# WWW2023

## 会议论文列表

本会议共有 682 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Dynamic Embedding-based Retrieval for Personalized Item Recommendations at Instacart](https://doi.org/10.1145/3543873.3587668) |  | 0 | Personalization is essential in e-commerce, with item recommendation as a critical task. In this paper, we describe a hybrid embedding-based retrieval system for real-time personalized item recommendations on Instacart. Our system addresses unique challenges in the multi-source retrieval system, and includes several key components to make it highly personalized and dynamic. Specifically, our system features a hybrid embedding model that includes a long-term user interests embedding model and a real-time session-based model, which are combined to capture users’ immediate intents and historical interactions. Additionally, we have developed a contextual bandit solution to dynamically adjust the number of candidates from each source and optimally allocate retrieval slots given a limited computational budget. Our modeling and system optimization efforts have enabled us to provide highly personalized item recommendations in real-time at scale to all our customers, including new and long-standing users. | Chuanwei Ruan, Allan Stewart, Han Li, Ryan Ye, David Vengerov, Haixun Wang | Instacart, USA |
| 2 |  |  [A Multi-Granularity Matching Attention Network for Query Intent Classification in E-commerce Retrieval](https://doi.org/10.1145/3543873.3584639) |  | 0 | Query intent classification, which aims at assisting customers to find desired products, has become an essential component of the e-commerce search. Existing query intent classification models either design more exquisite models to enhance the representation learning of queries or explore label-graph and multi-task to facilitate models to learn external information. However, these models cannot capture multi-granularity matching features from queries and categories, which makes them hard to mitigate the gap in the expression between informal queries and categories. This paper proposes a Multi-granularity Matching Attention Network (MMAN), which contains three modules: a self-matching module, a char-level matching module, and a semantic-level matching module to comprehensively extract features from the query and a query-category interaction matrix. In this way, the model can eliminate the difference in expression between queries and categories for query intent classification. We conduct extensive offline and online A/B experiments, and the results show that the MMAN significantly outperforms the strong baselines, which shows the superiority and effectiveness of MMAN. MMAN has been deployed in production and brings great commercial value for our company. | Chunyuan Yuan, Yiming Qiu, Mingming Li, Haiqing Hu, Songlin Wang, Sulong Xu | JD.com, Beijing, China, China |
| 3 |  |  [Divide and Conquer: Towards Better Embedding-based Retrieval for Recommender Systems from a Multi-task Perspective](https://doi.org/10.1145/3543873.3584629) |  | 0 | Embedding-based retrieval (EBR) methods are widely used in modern recommender systems thanks to its simplicity and effectiveness. However, along the journey of deploying and iterating on EBR in production, we still identify some fundamental issues in existing methods. First, when dealing with large corpus of candidate items, EBR models often have difficulties in balancing the performance on distinguishing highly relevant items (positives) from both irrelevant ones (easy negatives) and from somewhat related yet not competitive ones (hard negatives). Also, we have little control in the diversity and fairness of the retrieval results because of the \`\`greedy'' nature of nearest vector search. These issues compromise the performance of EBR methods in large-scale industrial scenarios. This paper introduces a simple and proven-in-production solution to overcome these issues. The proposed solution takes a divide-and-conquer approach: the whole set of candidate items are divided into multiple clusters and we run EBR to retrieve relevant candidates from each cluster in parallel; top candidates from each cluster are then combined by some controllable merging strategies. This approach allows our EBR models to only concentrate on discriminating positives from mostly hard negatives. It also enables further improvement from a multi-tasking learning (MTL) perspective: retrieval problems within each cluster can be regarded as individual tasks; inspired by recent successes in prompting and prefix-tuning, we propose an efficient task adaption technique further boosting the retrieval performance within each cluster with negligible overheads. | Yuan Zhang, Xue Dong, Weijie Ding, Biao Li, Peng Jiang, Kun Gai | Kuaishou Technology, China; Shandong University, China; Unaffiliated, China |
| 4 |  |  [Expressive user embedding from churn and recommendation multi-task learning](https://doi.org/10.1145/3543873.3587306) |  | 0 | In this paper, we present a Multi-Task model for Recommendation and Churn prediction (MT) in the retail banking industry. The model leverages a hard parameter-sharing framework and consists of a shared multi-stack encoder with multi-head self-attention and two fully connected task heads. It is trained to achieve two multi-class classification tasks: predicting product churn and identifying the next-best products (NBP) for users, individually. Our experiments demonstrate the superiority of the multi-task model compared to its single-task versions, reaching top-1 precision at 78.1% and 77.6%, for churn and NBP prediction respectively. Moreover, we find that the model learns a coherent and expressive high-level representation reflecting user intentions related to both tasks. There is a clear separation between users with acquisitions and users with churn. In addition, acquirers are more tightly clustered compared to the churners. The gradual separability of churning and acquiring users, who diverge in intent, is a desirable property. It provides a basis for model explainability, critical to industry adoption, and also enables other downstream applications. These potential additional benefits, beyond reducing customer attrition and increasing product use–two primary concerns of businesses, make such a model even more valuable. | Huajun Bai, Davide Liu, Thomas Hirtz, Alexandre Boulenger | Genify, United Arab Emirates; Genify, China; Tsinghua University, China |
| 5 |  |  [Continual Transfer Learning for Cross-Domain Click-Through Rate Prediction at Taobao](https://doi.org/10.1145/3543873.3584625) |  | 0 | As one of the largest e-commerce platforms in the world, Taobao's recommendation systems (RSs) serve the demands of shopping for hundreds of millions of customers. Click-Through Rate (CTR) prediction is a core component of the RS. One of the biggest characteristics in CTR prediction at Taobao is that there exist multiple recommendation domains where the scales of different domains vary significantly. Therefore, it is crucial to perform cross-domain CTR prediction to transfer knowledge from large domains to small domains to alleviate the data sparsity issue. However, existing cross-domain CTR prediction methods are proposed for static knowledge transfer, ignoring that all domains in real-world RSs are continually time-evolving. In light of this, we present a necessary but novel task named Continual Transfer Learning (CTL), which transfers knowledge from a time-evolving source domain to a time-evolving target domain. In this work, we propose a simple and effective CTL model called CTNet to solve the problem of continual cross-domain CTR prediction at Taobao, and CTNet can be trained efficiently. Particularly, CTNet considers an important characteristic in the industry that models has been continually well-trained for a very long time. So CTNet aims to fully utilize all the well-trained model parameters in both source domain and target domain to avoid losing historically acquired knowledge, and only needs incremental target domain data for training to guarantee efficiency. Extensive offline experiments and online A/B testing at Taobao demonstrate the efficiency and effectiveness of CTNet. CTNet is now deployed online in the recommender systems of Taobao, serving the main traffic of hundreds of millions of active users. | Lixin Liu, Yanling Wang, Tianming Wang, Dong Guan, Jiawei Wu, Jingxu Chen, Rong Xiao, Wenxiang Zhu, Fei Fang | Alibaba Group, China; Alibaba group, China; Renmin University of China, China |
| 6 |  |  [MAKE: Vision-Language Pre-training based Product Retrieval in Taobao Search](https://doi.org/10.1145/3543873.3584627) |  | 0 | Taobao Search consists of two phases: the retrieval phase and the ranking phase. Given a user query, the retrieval phase returns a subset of candidate products for the following ranking phase. Recently, the paradigm of pre-training and fine-tuning has shown its potential in incorporating visual clues into retrieval tasks. In this paper, we focus on solving the problem of text-to-multimodal retrieval in Taobao Search. We consider that users' attention on titles or images varies on products. Hence, we propose a novel Modal Adaptation module for cross-modal fusion, which helps assigns appropriate weights on texts and images across products. Furthermore, in e-commerce search, user queries tend to be brief and thus lead to significant semantic imbalance between user queries and product titles. Therefore, we design a separate text encoder and a Keyword Enhancement mechanism to enrich the query representations and improve text-to-multimodal matching. To this end, we present a novel vision-language (V+L) pre-training methods to exploit the multimodal information of (user query, product title, product image). Extensive experiments demonstrate that our retrieval-specific pre-training model (referred to as MAKE) outperforms existing V+L pre-training methods on the text-to-multimodal retrieval task. MAKE has been deployed online and brings major improvements on the retrieval system of Taobao Search. | Xiaoyang Zheng, Zilong Wang, Sen Li, Ke Xu, Tao Zhuang, Qingwen Liu, Xiaoyi Zeng | Alibaba Group, China |
| 7 |  |  [HAPENS: Hardness-Personalized Negative Sampling for Implicit Collaborative Filtering](https://doi.org/10.1145/3543873.3584631) |  | 0 | For training implicit collaborative filtering (ICF) models, hard negative sampling (HNS) has become a state-of-the-art solution for obtaining negative signals from massive uninteracted items. However, selecting appropriate hardness levels for personalized recommendations remains a fundamental, yet underexplored, problem. Previous HNS works have primarily adjusted the hardness level by tuning a single hyperparameter. However, applying the same hardness level to each user is unsuitable due to varying user behavioral characteristics, the quantity and quality of user records, and different consistencies of models’ inductive biases. Moreover, increasing the number of hyperparameters is not practical due to the massive number of users. To address this important and challenging problem, we propose a model-agnostic and practical approach called hardness-personalized negative sampling (HAPENS). HAPENS uses a two-stage approach: in stage one, it trains the ICF model with a customized objective function that optimizes its worst performance on each user’s interacted item set. In stage two, it utilizes these worst performances as personalized hardness levels with a well-designed sampling distribution, and trains the final model with the same architecture. We evaluated HAPENS on the collected Bing advertising dataset and one public dataset, and the comprehensive experimental results demonstrate its robustness and superiority. Moreover, HAPENS has delivered significant benefits to the Bing advertising system. To the best of our knowledge, we are the first to study this important and challenging problem. | Haoxin Liu, Pu Zhao, Si Qin, Yong Shi, Mirror Xu, Qingwei Lin, Dongmei Zhang | Microsoft Research, China; Microsoft Bing, China |
| 8 |  |  [Que2Engage: Embedding-based Retrieval for Relevant and Engaging Products at Facebook Marketplace](https://doi.org/10.1145/3543873.3584633) |  | 0 | Embedding-based Retrieval (EBR) in e-commerce search is a powerful search retrieval technique to address semantic matches between search queries and products. However, commercial search engines like Facebook Marketplace Search are complex multi-stage systems optimized for multiple business objectives. At Facebook Marketplace, search retrieval focuses on matching search queries with relevant products, while search ranking puts more emphasis on contextual signals to up-rank the more engaging products. As a result, the end-to-end searcher experience is a function of both relevance and engagement, and the interaction between different stages of the system. This presents challenges to EBR systems in order to optimize for better searcher experiences. In this paper we presents Que2Engage, a search EBR system built towards bridging the gap between retrieval and ranking for end-to-end optimizations. Que2Engage takes a multimodal & multitask approach to infuse contextual information into the retrieval stage and to balance different business objectives. We show the effectiveness of our approach via a multitask evaluation framework and thorough baseline comparisons and ablation studies. Que2Engage is deployed on Facebook Marketplace Search and shows significant improvements in searcher engagement in two weeks of A/B testing. | Yunzhong He, Yuxin Tian, Mengjiao Wang, Feier Chen, Licheng Yu, Maolong Tang, Congcong Chen, Ning Zhang, Bin Kuang, Arul Prakash | Meta, USA; University of California, Merced, USA |
| 9 |  |  [Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce Search](https://doi.org/10.1145/3543873.3584638) |  | 0 | Retrieving relevant items that match users' queries from billion-scale corpus forms the core of industrial e-commerce search systems, in which embedding-based retrieval (EBR) methods are prevailing. These methods adopt a two-tower framework to learn embedding vectors for query and item separately and thus leverage efficient approximate nearest neighbor (ANN) search to retrieve relevant items. However, existing EBR methods usually ignore inconsistent user behaviors in industrial multi-stage search systems, resulting in insufficient retrieval efficiency with a low commercial return. To tackle this challenge, we propose to improve EBR methods by learning Multi-level Multi-Grained Semantic Embeddings(MMSE). We propose the multi-stage information mining to exploit the ordered, clicked, unclicked and random sampled items in practical user behavior data, and then capture query-item similarity via a post-fusion strategy. We then propose multi-grained learning objectives that integrate the retrieval loss with global comparison ability and the ranking loss with local comparison ability to generate semantic embeddings. Both experiments on a real-world billion-scale dataset and online A/B tests verify the effectiveness of MMSE in achieving significant performance improvements on metrics such as offline recall and online conversion rate (CVR). | Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Sulong Xu, Bo Long, Weipeng Yan | JD.com, China |
| 10 |  |  [CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems](https://doi.org/10.1145/3543873.3584657) |  | 0 | Learning large-scale industrial recommender system models by fitting them to historical user interaction data makes them vulnerable to conformity bias. This may be due to a number of factors, including the fact that user interests may be difficult to determine and that many items are often interacted with based on ecosystem factors other than their relevance to the individual user. In this work, we introduce CAM2, a conformity-aware multi-task ranking model to serve relevant items to users on one of the largest industrial recommendation platforms. CAM2 addresses these challenges systematically by leveraging causal modeling to disentangle users' conformity to popular items from their true interests. This framework is generalizable and can be scaled to support multiple representations of conformity and user relevance in any large-scale recommender system. We provide deeper practical insights and demonstrate the effectiveness of the proposed model through improvements in offline evaluation metrics compared to our production multi-task ranking model. We also show through online experiments that the CAM2 model results in a significant 0.50% increase in aggregated user engagement, coupled with a 0.21% increase in daily active users on Facebook Watch, a popular video discovery and sharing platform serving billions of users. | Ameya Raul, Amey Porobo Dharwadker, Brad Schumitsch | Meta Inc., USA |
| 11 |  |  [A Deep Behavior Path Matching Network for Click-Through Rate Prediction](https://doi.org/10.1145/3543873.3584662) |  | 0 | User behaviors on an e-commerce app not only contain different kinds of feedback on items but also sometimes imply the cognitive clue of the user's decision-making. For understanding the psychological procedure behind user decisions, we present the behavior path and propose to match the user's current behavior path with historical behavior paths to predict user behaviors on the app. Further, we design a deep neural network for behavior path matching and solve three difficulties in modeling behavior paths: sparsity, noise interference, and accurate matching of behavior paths. In particular, we leverage contrastive learning to augment user behavior paths, provide behavior path self-activation to alleviate the effect of noise, and adopt a two-level matching mechanism to identify the most appropriate candidate. Our model shows excellent performance on two real-world datasets, outperforming the state-of-the-art CTR model. Moreover, our model has been deployed on the Meituan food delivery platform and has accumulated 1.6% improvement in CTR and 1.8% improvement in advertising revenue. | Jian Dong, Yisong Yu, Yapeng Zhang, Yimin Lv, Shuli Wang, Beihong Jin, Yongkang Wang, Xingxing Wang, Dong Wang | Meituan Ltd., China; Meituan Ltd, China; Institute of Software, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China |
| 12 |  |  [Cross-lingual Search for e-Commerce based on Query Translatability and Mixed-Domain Fine-Tuning](https://doi.org/10.1145/3543873.3587660) |  | 0 | Online stores in the US offer a unique scenario for Cross-Lingual Information Retrieval (CLIR) due to the mix of Spanish and English in user queries. Machine Translation (MT) provides an opportunity to lift relevance by translating the Spanish queries to English before delivering them to the search engine. However, polysemy-derived problems, high latency and context scarcity in product search, make generic MT an impractical solution. The wide diversity of products in marketplaces injects non-translatable entities, loanwords, ambiguous morphemes, cross-language ambiguity and a variety of Spanish dialects in the communication between buyers and sellers, posing a thread to the accuracy of MT. In this work, we leverage domain adaptation on a simplified architecture of Neural Machine Translation (NMT) to make both latency and accuracy suitable for e-commerce search. Our NMT model is fine-tuned on a mixed-domain corpus based on engagement data expanded with catalog back-translation techniques. Beyond accuracy, and given that translation is not the goal but the means to relevant results, the problem of Query Translatability is addressed by a classifier on whether the translation should be automatic or explicitly requested. We assembled these models into a query translation system that we tested and launched at Walmart.com , with a statistically significant lift in Spanish GMV and an nDCG gain for Spanish queries of +70%. | Jesus PerezMartin, Jorge GomezRobles, Asier GutiérrezFandiño, Pankaj Adsul, Sravanthi Rajanala, Leonardo Lezcano | Walmart Global Tech, USA |
| 13 |  |  [Enhancing User Personalization in Conversational Recommenders](https://doi.org/10.1145/3543507.3583192) |  | 0 | Conversational recommenders are emerging as a powerful tool to personalize a user's recommendation experience. Through a back-and-forth dialogue, users can quickly hone in on just the right items. Many approaches to conversational recommendation, however, only partially explore the user preference space and make limiting assumptions about how user feedback can be best incorporated, resulting in long dialogues and poor recommendation performance. In this paper, we propose a novel conversational recommendation framework with two unique features: (i) a greedy NDCG attribute selector, to enhance user personalization in the interactive preference elicitation process by prioritizing attributes that most effectively represent the actual preference space of the user; and (ii) a user representation refiner, to effectively fuse together the user preferences collected from the interactive elicitation process to obtain a more personalized understanding of the user. Through extensive experiments on four frequently used datasets, we find the proposed framework not only outperforms all the state-of-the-art conversational recommenders (in terms of both recommendation performance and conversation efficiency), but also provides a more personalized experience for the user under the proposed multi-groundtruth multi-round conversational recommendation setting. | Allen Lin, Ziwei Zhu, Jianling Wang, James Caverlee | Texas A&M University, USA; George Mason University, USA |
| 14 |  |  [Dual-interest Factorization-heads Attention for Sequential Recommendation](https://doi.org/10.1145/3543507.3583278) |  | 0 | Accurate user interest modeling is vital for recommendation scenarios. One of the effective solutions is the sequential recommendation that relies on click behaviors, but this is not elegant in the video feed recommendation where users are passive in receiving the streaming contents and return skip or no-skip behaviors. Here skip and no-skip behaviors can be treated as negative and positive feedback, respectively. With the mixture of positive and negative feedback, it is challenging to capture the transition pattern of behavioral sequence. To do so, FeedRec has exploited a shared vanilla Transformer, which may be inelegant because head interaction of multi-heads attention does not consider different types of feedback. In this paper, we propose Dual-interest Factorization-heads Attention for Sequential Recommendation (short for DFAR) consisting of feedback-aware encoding layer, dual-interest disentangling layer and prediction layer. In the feedback-aware encoding layer, we first suppose each head of multi-heads attention can capture specific feedback relations. Then we further propose factorization-heads attention which can mask specific head interaction and inject feedback information so as to factorize the relation between different types of feedback. Additionally, we propose a dual-interest disentangling layer to decouple positive and negative interests before performing disentanglement on their representations. Finally, we evolve the positive and negative interests by corresponding towers whose outputs are contrastive by BPR loss. Experiments on two real-world datasets show the superiority of our proposed method against state-of-the-art baselines. Further ablation study and visualization also sustain its effectiveness. We release the source code here: https://github.com/tsinghua-fib-lab/WWW2023-DFAR. | Guanyu Lin, Chen Gao, Yu Zheng, Jianxin Chang, Yanan Niu, Yang Song, Zhiheng Li, Depeng Jin, Yong Li | Department of Electronic Engineering, Tsinghua University, China; kuaishou, China; Tsinghua University, China |
| 15 |  |  [A Cross-Media Retrieval System for Web-SNS-Map Using Suggested Keywords Generating and Ranking Method Based on Search Characteristics](https://doi.org/10.1145/3543873.3587344) |  | 0 | The research on multimedia retrieval has lasted for several decades. However, past efforts generally focused on single-media retrieval, where the queries and retrieval results belong to the same media (platform) type, such as social media platforms or search engines. In single-media retrieval, users have to select search media or options based on search characteristics such as contents, time, or spatial distance, they might be unable to retrieve correct results mixed in other media if they carelessly forget to select. In this study, we propose a cross-media retrieval system using suggestion generation methods to integrate three search characteristics of the Web (textual content-based retrieval), SNS (timeliness), and map (spatial distance-aware retrieval). In our previous research, we attempted to improve search efficiency using clustering methods to provide search results to users through related terms, etc. In this paper, we focus on the search efficiency of multiple search media. We utilize Google search engine to obtain the retrieval content from the Web, Twitter to obtain timely information from SNSs, and Google Maps to get geographical information from maps. We apply the obtained retrieval results to analyze the similarities between them by clustering. Then, we generate relevant suggestions and provide them to users. Moreover, we validate the effectiveness of the search results generated by our proposed system. | Da Li, Masaki Sugihashi, Tadahiko Kumamoto, Yukiko Kawai | Kyoto Sangyo University, Japan; Fukuoka University, Japan; Chiba Institute of Technology, Japan |
| 16 |  |  [A Knowledge Enhanced Hierarchical Fusion Network for CTR Prediction under Account Search Scenario in WeChat](https://doi.org/10.1145/3543873.3584650) |  | 0 | Click-through rate (CTR) estimation plays as a pivotal function module in various online services. Previous studies mainly apply CTR models to the field of recommendation or online advertisement. Indeed, CTR is also critical in information retrieval, since the CTR probability can serve as a valuable feature for a query-document pair. In this paper, we study the CTR task under account search scenario in WeChat, where users search official accounts or mini programs corresponding to an organization. Despite the large number of CTR models, directly applying them to our task is inappropriate since the account retrieval task has a number of specific characteristics. E.g., different from traditional user-centric CTR models, in our task, CTR prediction is query-centric and does not model user information. In addition, queries and accounts are short texts, and heavily rely on prior knowledge and semantic understanding. These characteristics require us to specially design a CTR model for the task. To this end, we propose a novel CTR prediction model named Knowledge eNhanced hIerarchical Fusion nEtwork (KNIFE). Specifically, to tackle the prior information problem, we mine the knowledge graph of accounts as side information; to enhance the representations of queries, we construct a bipartite graph for queries and accounts. In addition, a hierarchical network structure is proposed to fuse the representations of different information in a fine-grained manner. Finally, the representations of queries and accounts are obtained from this hierarchical network and fed into the CTR model together with other features for prediction. We conduct extensive experiments against 12 existing models across two industrial datasets. Both offline and online A/B test results indicate the effectiveness of KNIFE. | Yuanzhou Yao, Zhao Zhang, Kaijia Yang, Huasheng Liang, Qiang Yan, Fuzheng Zhuang, Yongjun Xu, Boyu Diao, Chao Li | Zhejiang Lab, China; WeChat, Tencent, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Institute of Artificial Intelligence, Beihang University, China |
| 17 |  |  [Multi-Objective Ranking to Boost Navigational Suggestions in eCommerce AutoComplete](https://doi.org/10.1145/3543873.3584649) |  | 0 | Query AutoComplete (QAC) helps customers complete their search queries quickly by suggesting completed queries. QAC on eCommerce sites usually employ Learning to Rank (LTR) approaches based on customer behaviour signals such as clicks and conversion rates to optimize business metrics. However, they do not exclusively optimize for the quality of suggested queries which results in lack of navigational suggestions like product categories and attributes, e.g., "sports shoes" and "white shoes" for query "shoes". We propose to improve the quality of query suggestions by introducing navigational suggestions without impacting the business metrics. For this purpose, we augment the customer behaviour (CB) based objective with Query-Quality (QQ) objective and assemble them with trainable mixture weights to define multi-objective optimization function. We propose to optimize this multi-objective function by implementing ALMO algorithm to obtain a model robust against any mixture weight. We show that this formulation improves query relevance on an eCommerce QAC dataset by at least 13% over the baseline Deep Pairwise LTR (DeepPLTR) with minimal impact on MRR and results in a lift of 0.26% in GMV in an online A/B test. We also evaluated our approach on public search logs datasets and got improvement in query relevance by using query coherence as QQ objective. | Sonali Singh, Sachin Farfade, Prakash Mandayam Comar | Amazon, India |
| 18 |  |  [Personalization and Recommendations in Search](https://doi.org/10.1145/3543873.3589749) |  | 0 | The utility of a search system for its users can be further enhanced by providing personalized results and recommendations within the search context. However, the research discussions around these aspects of search remain fragmented across different conferences and workshops. Hence, this workshop aims to bring together researchers and practitioners from industry and academia to engage in the discussions of algorithmic and system challenges in search personalization and effectively recommending within search context. | Sudarshan Lamkhede, Anlei Dong, Moumita Bhattacharya, Hongning Wang | Microsoft Bing, USA; Dept. of Computer Science, University of Virginia, USA; Netflix Research, USA |
| 19 |  |  [Cooperative Retriever and Ranker in Deep Recommenders](https://doi.org/10.1145/3543507.3583422) |  | 0 | Deep recommender systems (DRS) are intensively applied in modern web services. To deal with the massive web contents, DRS employs a two-stage workflow: retrieval and ranking, to generate its recommendation results. The retriever aims to select a small set of relevant candidates from the entire items with high efficiency; while the ranker, usually more precise but time-consuming, is supposed to further refine the best items from the retrieved candidates. Traditionally, the two components are trained either independently or within a simple cascading pipeline, which is prone to poor collaboration effect. Though some latest works suggested to train retriever and ranker jointly, there still exist many severe limitations: item distribution shift between training and inference, false negative, and misalignment of ranking order. As such, it remains to explore effective collaborations between retriever and ranker. | Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, Enhong Chen | Microsoft Research Asia, China; University of Electronic Science and Technology of China, China; University of Science and Technology of China, China |
| 20 |  |  [Modeling Temporal Positive and Negative Excitation for Sequential Recommendation](https://doi.org/10.1145/3543507.3583463) |  | 0 | Sequential recommendation aims to predict the next item which interests users via modeling their interest in items over time. Most of the existing works on sequential recommendation model users’ dynamic interest in specific items while overlooking users’ static interest revealed by some static attribute information of items, e.g., category, brand. Moreover, existing works often only consider the positive excitation of a user’s historical interactions on his/her next choice on candidate items while ignoring the commonly existing negative excitation, resulting in insufficiently modeling dynamic interest. The overlook of static interest and negative excitation will lead to incomplete interest modeling and thus impedes the recommendation performance. To this end, in this paper, we propose modeling both static interest and negative excitation for dynamic interest to further improve the recommendation performance. Accordingly, we design a novel Static-Dynamic Interest Learning (SDIL) framework featured with a novel Temporal Positive and Negative Excitation Modeling (TPNE) module for accurate sequential recommendation. TPNE is specially designed for comprehensively modeling dynamic interest based on temporal positive and negative excitation learning. Extensive experiments on three real-world datasets show that SDIL can effectively capture both static and dynamic interest and outperforms state-of-the-art baselines. | Chengkai Huang, Shoujin Wang, Xianzhi Wang, Lina Yao | CSIRO's Data 61, Australia and The University of New South Wales, Australia; The University of New South Wales, Australia; University of Technology Sydney, Australia |
| 21 |  |  [Beyond Two-Tower: Attribute Guided Representation Learning for Candidate Retrieval](https://doi.org/10.1145/3543507.3583254) |  | 0 | Candidate retrieval is a key part of the modern search engines whose goal is to find candidate items that are semantically related to the query from a large item pool. The core difference against the later ranking stage is the requirement of low latency. Hence, two-tower structure with two parallel yet independent encoder for both query and item is prevalent in many systems. In these efforts, the semantic information of a query and a candidate item is fed into the corresponding encoder and then use their representations for retrieval. With the popularity of pre-trained semantic models, the state-of-the-art for semantic retrieval tasks has achieved the significant performance gain. However, the capacity of learning relevance signals is still limited by the isolation between the query and the item. The interaction-based modeling between the query and the item has been widely validated to be useful for the ranking stage, where more computation cost is affordable. Here, we are quite initerested in an demanding question: how to exploiting query-item interaction-based learning to enhance candidate retrieval and still maintain the low computation cost. Note that an item usually contain various heteorgeneous attributes which could help us understand the item characteristics more precisely. To this end, we propose a novel attribute guided representation learning framework (named AGREE) to enhance the candidate retrieval by exploiting query-attribute relevance. The key idea is to couple the query and item representation learning together during the training phase, but also enable easy decoupling for efficient inference. Specifically, we introduce an attribute fusion layer in the item side to identify most relevant item features for item representation. On the query side, an attribute-aware learning process is introduced to better infer the search intent also from these attributes. After model training, we then decouple the attribute information away from the query encoder, which guarantees the low latency for the inference phase. Extensive experiments over two real-world large-scale datasets demonstrate the superiority of the proposed AGREE against several state-of-the-art technical alternatives. Further online A/B test from AliPay search servise also show that AGREE achieves substantial performance gain over four business metrics. Currently, the proposed AGREE has been deployed online in AliPay for serving major traffic. | Hongyu Shan, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Chenliang Li | antgroup, China; Wuhan University, China |
| 22 |  |  [Improving Content Retrievability in Search with Controllable Query Generation](https://doi.org/10.1145/3543507.3583261) |  | 0 | An important goal of online platforms is to enable content discovery, i.e. allow users to find a catalog entity they were not familiar with. A pre-requisite to discover an entity, e.g. a book, with a search engine is that the entity is retrievable, i.e. there are queries for which the system will surface such entity in the top results. However, machine-learned search engines have a high retrievability bias, where the majority of the queries return the same entities. This happens partly due to the predominance of narrow intent queries, where users create queries using the title of an already known entity, e.g. in book search 'harry potter'. The amount of broad queries where users want to discover new entities, e.g. in music search 'chill lyrical electronica with an atmospheric feeling to it', and have a higher tolerance to what they might find, is small in comparison. We focus here on two factors that have a negative impact on the retrievability of the entities (I) the training data used for dense retrieval models and (II) the distribution of narrow and broad intent queries issued in the system. We propose CtrlQGen, a method that generates queries for a chosen underlying intent-narrow or broad. We can use CtrlQGen to improve factor (I) by generating training data for dense retrieval models comprised of diverse synthetic queries. CtrlQGen can also be used to deal with factor (II) by suggesting queries with broader intents to users. Our results on datasets from the domains of music, podcasts, and books reveal that we can significantly decrease the retrievability bias of a dense retrieval model when using CtrlQGen. First, by using the generated queries as training data for dense models we make 9% of the entities retrievable (go from zero to non-zero retrievability). Second, by suggesting broader queries to users, we can make 12% of the entities retrievable in the best case. | Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, Hugues Bouchard | Spotify, Netherlands; Spotify, Italy; Spotify, USA; Spotify, Spain |
| 23 |  |  [PIE: Personalized Interest Exploration for Large-Scale Recommender Systems](https://doi.org/10.1145/3543873.3584656) |  | 0 | Recommender systems are increasingly successful in recommending personalized content to users. However, these systems often capitalize on popular content. There is also a continuous evolution of user interests that need to be captured, but there is no direct way to systematically explore users' interests. This also tends to affect the overall quality of the recommendation pipeline as training data is generated from the candidates presented to the user. In this paper, we present a framework for exploration in large-scale recommender systems to address these challenges. It consists of three parts, first the user-creator exploration which focuses on identifying the best creators that users are interested in, second the online exploration framework and third a feed composition mechanism that balances explore and exploit to ensure optimal prevalence of exploratory videos. Our methodology can be easily integrated into an existing large-scale recommender system with minimal modifications. We also analyze the value of exploration by defining relevant metrics around user-creator connections and understanding how this helps the overall recommendation pipeline with strong online gains in creator and ecosystem value. In contrast to the regression on user engagement metrics generally seen while exploring, our method is able to achieve significant improvements of 3.50% in strong creator connections and 0.85% increase in novel creator connections. Moreover, our work has been deployed in production on Facebook Watch, a popular video discovery and sharing platform serving billions of users. | Khushhall Chandra Mahajan, Amey Porobo Dharwadker, Romil Shah, Simeng Qu, Gaurav Bang, Brad Schumitsch | Meta Inc., USA |
| 24 |  |  [Improving Product Search with Season-Aware Query-Product Semantic Similarity](https://doi.org/10.1145/3543873.3587625) |  | 0 | Product search for online shopping should be season-aware, i.e., presenting seasonally relevant products to customers. In this paper, we propose a simple yet effective solution to improve seasonal relevance in product search by incorporating seasonality into language models for semantic matching. We first identify seasonal queries and products by analyzing implicit seasonal contexts through time-series analysis over the past year. Then we introduce explicit seasonal contexts by enhancing the query representation with a season token according to when the query is issued. A new season-enhanced BERT model (SE-BERT) is also proposed to learn the semantic similarity between the resulting seasonal queries and products. SE-BERT utilizes Multi-modal Adaption Gate (MAG) to augment the season-enhanced semantic embedding with other contextual information such as product price and review counts for robust relevance prediction. To better align with the ranking objective, a listwise loss function (neural NDCG) is used to regularize learning. Experimental results validate the effectiveness of the proposed method, which outperforms existing solutions for query-product relevance prediction in terms of NDCG and Price Weighted Purchases (PWP). | Haoming Chen, Yetian Chen, Jingjing Meng, Yang Jiao, Yikai Ni, Yan Gao, Michinari Momma, Yi Sun | Amazon.com, USA; Harvard University, USA |
| 25 |  |  [Blend and Match: Distilling Semantic Search Models with Different Inductive Biases and Model Architectures](https://doi.org/10.1145/3543873.3587629) |  | 0 | Commercial search engines use different semantic models to augment lexical matches. These models provide candidate items for a user’s query from a target space of millions to billions of items. Models with different inductive biases provide relatively different predictions, making it desirable to launch multiple semantic models in production. However, latency and resource constraints make simultaneously deploying multiple models impractical. In this paper, we introduce a distillation approach, called Blend and Match (BM), to unify two different semantic search models into a single model. We use a Bi-encoder semantic matching model as our primary model and propose a novel loss function to incorporate eXtreme Multi-label Classification (XMC) predictions as the secondary model. Our experiments conducted on two large-scale datasets, collected from a popular e-commerce store, show that our proposed approach significantly improves the recall of the primary Bi-encoder model by 11% to 17% with a minimal loss in precision. We show that traditional knowledge distillation approaches result in a sub-optimal performance for our problem setting, and our BM approach yields comparable rankings with strong Rank Fusion (RF) methods used only if one could deploy multiple models. | Hamed Bonab, Ashutosh Joshi, Ravi Bhatia, Ankit Gandhi, Vijay Huddar, Juhi Naik, Mutasem AlDarabsah, Choon Hui Teo, Jonathan May, Tarun Agarwal, Vaclav Petricek | Amazon, India; Amazon, USA; Amazon, USA and USC Information Sciences Institute, USA |
| 26 |  |  [Joint Internal Multi-Interest Exploration and External Domain Alignment for Cross Domain Sequential Recommendation](https://doi.org/10.1145/3543507.3583366) |  | 0 | Sequential Cross-Domain Recommendation (CDR) has been popularly studied to utilize different domain knowledge and users’ historical behaviors for the next-item prediction. In this paper, we focus on the cross-domain sequential recommendation problem. This commonly exist problem is rather challenging from two perspectives, i.e., the implicit user historical rating sequences are difficult in modeling and the users/items on different domains are mostly non-overlapped. Most previous sequential CDR approaches cannot solve the cross-domain sequential recommendation problem well, since (1) they cannot sufficiently depict the users’ actual preferences, (2) they cannot leverage and transfer useful knowledge across domains. To tackle the above issues, we propose joint Internal multi-interest exploration and External domain alignment for cross domain Sequential Recommendation model (IESRec). IESRec includes two main modules, i.e., internal multi-interest exploration module and external domain alignment module. To reflect the users’ diverse characteristics with multi-interests evolution, we first propose internal temporal optimal transport method in the internal multi-interest exploration module. We further propose external alignment optimal transport method in the external domain alignment module to reduce domain discrepancy for the item embeddings. Our empirical studies on Amazon datasets demonstrate that IESRec significantly outperforms the state-of-the-art models. | Weiming Liu, Xiaolin Zheng, Chaochao Chen, Jiajie Su, Xinting Liao, Mengling Hu, Yanchao Tan | Zhejiang University, China; Fuzhou Univerisity, China |
| 27 |  |  [Latent User Intent Modeling for Sequential Recommenders](https://doi.org/10.1145/3543873.3584641) |  | 0 | Sequential recommender models are essential components of modern industrial recommender systems. These models learn to predict the next items a user is likely to interact with based on his/her interaction history on the platform. Most sequential recommenders however lack a higher-level understanding of user intents, which often drive user behaviors online. Intent modeling is thus critical for understanding users and optimizing long-term user experience. We propose a probabilistic modeling approach and formulate user intent as latent variables, which are inferred based on user behavior signals using variational autoencoders (VAE). The recommendation policy is then adjusted accordingly given the inferred user intent. We demonstrate the effectiveness of the latent user intent modeling via offline analyses as well as live experiments on a large-scale industrial recommendation platform. | Bo Chang, Alexandros Karatzoglou, Yuyan Wang, Can Xu, Ed H. Chi, Minmin Chen | Google, USA |
| 28 |  |  [Deep Neural Network with LinUCB: A Contextual Bandit Approach for Personalized Recommendation](https://doi.org/10.1145/3543873.3587684) |  | 0 | Recommender systems are widely used in many Web applications to recommend items which are relevant to a user’s preferences. However, focusing on exploiting user preferences while ignoring exploration will lead to biased feedback and hurt the user’s experience in the long term. The Mutli-Armed Bandit (MAB) is introduced to balance the tradeoff between exploitation and exploration. By utilizing context information in the reward function, contextual bandit algorithms lead to better performance compared to context-free bandit algorithms. However, existing contextual bandit algorithms either assume a linear relation between the expected reward and context features, whose representation power gets limited, or use a deep neural network in the reward function which is impractical in implementation. In this paper, we propose a new contextual bandit algorithm, DeepLinUCB, which leverages the representation power of deep neural network to transform the raw context features in the reward function. Specifically, this deep neural network is dedicated to the recommender system, which is efficient and practical in real-world applications. Furthermore, we conduct extensive experiments in our online recommender system using requests from real-world scenarios and show that DeepLinUCB is efficient and outperforms other bandit algorithms. | Qicai Shi, Feng Xiao, Douglas Pickard, Inga Chen, Liang Chen | Disneystreaming, USA; Disneystreaming, China |
| 29 |  |  [Contrastive Collaborative Filtering for Cold-Start Item Recommendation](https://doi.org/10.1145/3543507.3583286) |  | 0 | The cold-start problem is a long-standing challenge in recommender systems. As a promising solution, content-based generative models usually project a cold-start item's content onto a warm-start item embedding to capture collaborative signals from item content so that collaborative filtering can be applied. However, since the training of the cold-start recommendation models is conducted on warm datasets, the existent methods face the issue that the collaborative embeddings of items will be blurred, which significantly degenerates the performance of cold-start item recommendation. To address this issue, we propose a novel model called Contrastive Collaborative Filtering for Cold-start item Recommendation (CCFCRec), which capitalizes on the co-occurrence collaborative signals in warm training data to alleviate the issue of blurry collaborative embeddings for cold-start item recommendation. In particular, we devise a contrastive collaborative filtering (CF) framework, consisting of a content CF module and a co-occurrence CF module to generate the content-based collaborative embedding and the co-occurrence collaborative embedding for a training item, respectively. During the joint training of the two CF modules, we apply a contrastive learning between the two collaborative embeddings, by which the knowledge about the co-occurrence signals can be indirectly transferred to the content CF module, so that the blurry collaborative embeddings can be rectified implicitly by the memorized co-occurrence collaborative signals during the applying phase. Together with the sound theoretical analysis, the extensive experiments conducted on real datasets demonstrate the superiority of the proposed model. The codes and datasets are available on https://github.com/zzhin/CCFCRec. | Zhihui Zhou, Lilin Zhang, Ning Yang | Sichuan University, China |
| 30 |  |  [ColdNAS: Search to Modulate for User Cold-Start Recommendation](https://doi.org/10.1145/3543507.3583344) |  | 0 | Making personalized recommendation for cold-start users, who only have a few interaction histories, is a challenging problem in recommendation systems. Recent works leverage hypernetworks to directly map user interaction histories to user-specific parameters, which are then used to modulate predictor by feature-wise linear modulation function. These works obtain the state-of-the-art performance. However, the physical meaning of scaling and shifting in recommendation data is unclear. Instead of using a fixed modulation function and deciding modulation position by expertise, we propose a modulation framework called ColdNAS for user cold-start problem, where we look for proper modulation structure, including function and position, via neural architecture search. We design a search space which covers broad models and theoretically prove that this search space can be transformed to a much smaller space, enabling an efficient and robust one-shot search algorithm. Extensive experimental results on benchmark datasets show that ColdNAS consistently performs the best. We observe that different modulation functions lead to the best performance on different datasets, which validates the necessity of designing a searching-based method. Codes are available at https://github.com/LARS-research/ColdNAS. | Shiguang Wu, Yaqing Wang, Qinghe Jing, Daxiang Dong, Dejing Dou, Quanming Yao | Baidu Inc., China; Electronic Engineering, Tsinghua University, China |
| 31 |  |  [Improving the Relevance of Product Search for Queries with Negations](https://doi.org/10.1145/3543873.3587319) |  | 0 | Product search engines (PSEs) play an essential role in retail websites as they make it easier for users to retrieve relevant products within large catalogs. Despite the continuous progress that has led to increasingly accurate search engines, a limited focus has been given to their performance on queries with negations. Indeed, while we would expect to retrieve different products for the queries “iPhone 13 cover with ring” and “iPhone 13 cover without ring”, this does not happen in popular PSEs with the latter query containing results with the unwanted ring component. The limitation of modern PSEs in understanding negations motivates the need for further investigation. In this work, we start by defining the negation intent in users queries. Then, we design a transformer-based model, named Negation Detector for Queries (ND4Q), that reaches optimal performance in negation detection (+95% on accuracy metrics). Finally, having built the first negation detector for product search queries, we propose a negation-aware filtering strategy, named Filtering Irrelevant Products (FIP). The promising experimental results in improve the PSE relevance performance using FIP (+9.41% on [email protected] for queries where the negation starts with "without") pave the way to additional research effort towards negation-aware PSEs. | Felice Antonio Merra, Omar Zaidan, Fabricio de Sousa Nascimento | Amazon, Germany; Amazon, Japan |
| 32 |  |  [Movie Ticket, Popcorn, and Another Movie Next Weekend: Time-Aware Service Sequential Recommendation for User Retention](https://doi.org/10.1145/3543873.3584628) |  | 0 | When a customer sees a movie recommendation, she may buy the ticket right away, which is the immediate feedback that helps improve the recommender system. Alternatively, she may choose to come back later and this long-term feedback is also modeled to promote user retention. However, the long-term feedback comes with non-trivial challenges in understanding user retention: the complicated correlation between current demands and follow-up demands, coupled with the periodicity of services. For instance, before the movie, the customer buys popcorn through the App, which temporally correlates with the initial movie recommendation. Days later, she checks the App for new movies, as a weekly routine. To address this complexity in a more fine-grained revisit modeling, we propose Time Aware Service Sequential Recommendation (TASSR) for user retention, which is equipped with a multi-task design and an In-category TimeSeqBlock module. Large-scale online and offline experiments demonstrate its significant advantages over competitive baselines. | Xiaoyan Yang, Dong Wang, Binbin Hu, Dan Yang, Yue Shen, Jinjie Gu, Zhiqiang Zhang, Shiwei Lyu, Haipeng Zhang, Guannan Zhang | Ant Group, China; ShanghaiTech University, China |
| 33 |  |  [Unified Vision-Language Representation Modeling for E-Commerce Same-style Products Retrieval](https://doi.org/10.1145/3543873.3584632) |  | 0 | Same-style products retrieval plays an important role in e-commerce platforms, aiming to identify the same products which may have different text descriptions or images. It can be used for similar products retrieval from different suppliers or duplicate products detection of one supplier. Common methods use the image as the detected object, but they only consider the visual features and overlook the attribute information contained in the textual descriptions, and perform weakly for products in image less important industries like machinery, hardware tools and electronic component, even if an additional text matching module is added. In this paper, we propose a unified vision-language modeling method for e-commerce same-style products retrieval, which is designed to represent one product with its textual descriptions and visual contents. It contains one sampling skill to collect positive pairs from user click log with category and relevance constrained, and a novel contrastive loss unit to model the image, text, and image+text representations into one joint embedding space. It is capable of cross-modal product-to-product retrieval, as well as style transfer and user-interactive search. Offline evaluations on annotated data demonstrate its superior retrieval performance, and online testings show it can attract more clicks and conversions. Moreover, this model has already been deployed online for similar products retrieval in alibaba.com, the largest B2B e-commerce platform in the world. | Ben Chen, Linbo Jin, Xinxin Wang, Dehong Gao, Wen Jiang, Wei Ning | Alibaba Group, China; Aliaba Group, China |
| 34 |  |  [Task Adaptive Multi-learner Network for Joint CTR and CVR Estimation](https://doi.org/10.1145/3543873.3584653) |  | 0 | CTR and CVR are critical factors in personalized applications, and many methods jointly estimate them via multi-task learning to alleviate the ultra-sparsity of conversion behaviors. However, it is still difficult to predict CVR accurately and robustly due to the limited and even biased knowledge extracted by the single model tower optimized on insufficient conversion samples. In this paper, we propose a task adaptive multi-learner (TAML) framework for joint CTR and CVR prediction. We design a hierarchical task adaptive knowledge representation module with different experts to capture knowledge in different granularities, which can effectively exploit the commonalities between CTR and CVR estimation tasks meanwhile keeping their unique characteristics. We apply multiple learners to extract data knowledge from various views and fuse their predictions to obtain accurate and robust scores. To facilitate knowledge sharing across learners, we further perform self-distillation that uses the fused scores to teach different learners. Thorough offline and online experiments show the superiority of TAML in different Ad ranking tasks, and we have deployed it in Huawei’s online advertising platform to serve the main traffic. | Xiaofan Liu, Qinglin Jia, Chuhan Wu, Jingjie Li, Quanyu Dai, Lin Bo, Rui Zhang, Ruiming Tang | ruizhang.info, China; Huawei Noah's Ark Lab, China; Beijing University of Posts and Telecommunications, China; Renmin University of China, China |
| 35 |  |  [Deep Intention-Aware Network for Click-Through Rate Prediction](https://doi.org/10.1145/3543873.3584661) |  | 0 | E-commerce platforms provide entrances for customers to enter mini-apps that can meet their specific shopping requirements. Trigger items displayed on entrance icons can attract more entering. However, conventional Click-Through-Rate (CTR) prediction models, which ignore user instant interest in trigger item, fail to be applied to the new recommendation scenario dubbed Trigger-Induced Recommendation in Mini-Apps (TIRA). Moreover, due to the high stickiness of customers to mini-apps, we argue that existing trigger-based methods that over-emphasize the importance of trigger items, are undesired for TIRA, since a large portion of customer entries are because of their routine shopping habits instead of triggers. We identify that the key to TIRA is to extract customers' personalized entering intention and weigh the impact of triggers based on this intention. To achieve this goal, we convert CTR prediction for TIRA into a separate estimation form, and present Deep Intention-Aware Network (DIAN) with three key elements: 1) Intent Net that estimates user's entering intention, i.e., whether he/she is affected by the trigger or by the habits; 2) Trigger-Aware Net and 3) Trigger-Free Net that estimate CTRs given user's intention is to the trigger-item and the mini-app respectively. Following a joint learning way, DIAN can both accurately predict user intention and dynamically balance the results of trigger-free and trigger-based recommendations based on the estimated intention. Experiments show that DIAN advances state-of-the-art performance in a large real-world dataset, and brings a 9.39% lift of online Item Page View and 4.74% CTR for Juhuasuan, a famous mini-app of Taobao. | Yaxian Xia, Yi Cao, Sihao Hu, Tong Liu, Lingling Lu | Zhejiang University, China; Georgia Institute of Technology, USA; Alibaba Group, China |
| 36 |  |  [Search Personalization at Netflix](https://doi.org/10.1145/3543873.3587675) |  | 0 | At Netflix, personalization plays a key role in several aspects of our user experience, from ranking titles to constructing an optimal Homepage. Although personalization is a well established research field, its application to search presents unique problems and opportunities. In this paper, we describe the evolution of Search personalization at Netflix, its unique challenges, and provide a high level overview of relevant solutions. | Vito Ostuni, Christoph Kofler, Manjesh Nilange, Sudarshan Lamkhede, Dan Zylberglejd | Netflix Inc., USA |
| 37 |  |  [Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?](https://doi.org/10.1145/3543873.3587669) |  | 0 | The use of pretrained embeddings has become widespread in modern e-commerce machine learning (ML) systems. In practice, however, we have encountered several key issues when using pretrained embedding in a real-world production system, many of which cannot be fully explained by current knowledge. Unfortunately, we find that there is a lack of a thorough understanding of how pre-trained embeddings work, especially their intrinsic properties and interactions with downstream tasks. Consequently, it becomes challenging to make interactive and scalable decisions regarding the use of pre-trained embeddings in practice. Our investigation leads to two significant discoveries about using pretrained embeddings in e-commerce applications. Firstly, we find that the design of the pretraining and downstream models, particularly how they encode and decode information via embedding vectors, can have a profound impact. Secondly, we establish a principled perspective of pre-trained embeddings via the lens of kernel analysis, which can be used to evaluate their predictability, interactively and scalably. These findings help to address the practical challenges we faced and offer valuable guidance for successful adoption of pretrained embeddings in real-world production. Our conclusions are backed by solid theoretical reasoning, benchmark experiments, as well as online testings. | Da Xu, Bo Yang | LinkedIn, USA; Amazon, USA |
| 38 |  |  [GELTOR: A Graph Embedding Method based on Listwise Learning to Rank](https://doi.org/10.1145/3543507.3583193) |  | 0 | Similarity-based embedding methods have introduced a new perspective on graph embedding by conforming the similarity distribution of latent vectors in the embedding space to that of nodes in the graph; they show significant effectiveness over conventional embedding methods in various machine learning tasks. In this paper, we first point out the three drawbacks of existing similarity-based embedding methods: inaccurate similarity computation, conflicting optimization goal, and impairing in/out-degree distributions. Then, motivated by these drawbacks, we propose AdaSim\*, a novel similarity measure for graphs that is conducive to the similarity-based graph embedding. We finally propose GELTOR, an effective embedding method that employs AdaSim\* as a node similarity measure and the concept of learning-to-rank in the embedding process. Contrary to existing methods, GELTOR does not learn the similarity scores distribution; instead, for any target node, GELTOR conforms the ranks of its top-t similar nodes in the embedding space to their original ranks based on AdaSim\* scores. We conduct extensive experiments with six real-world datasets to evaluate the effectiveness of GELTOR in graph reconstruction, link prediction, and node classification tasks. Our experimental results show that (1) AdaSim\* outperforms AdaSim, RWR, and MCT in computing nodes similarity in graphs, (2) our GETLOR outperforms existing state-of-the-arts and conventional embedding methods in most cases of the above machine learning tasks, thereby implying that learning-to-rank is beneficial to graph embedding. | Masoud Reyhani Hamedani, JinSu Ryu, SangWook Kim | Hanyang University, Republic of Korea |
| 39 |  |  [On the Theories Behind Hard Negative Sampling for Recommendation](https://doi.org/10.1145/3543507.3583223) |  | 0 | Negative sampling has been heavily used to train recommender models on large-scale data, wherein sampling hard examples usually not only accelerates the convergence but also improves the model accuracy. Nevertheless, the reasons for the effectiveness of Hard Negative Sampling (HNS) have not been revealed yet. In this work, we fill the research gap by conducting thorough theoretical analyses on HNS. Firstly, we prove that employing HNS on the Bayesian Personalized Ranking (BPR) learner is equivalent to optimizing One-way Partial AUC (OPAUC). Concretely, the BPR equipped with Dynamic Negative Sampling (DNS) is an exact estimator, while with softmax-based sampling is a soft estimator. Secondly, we prove that OPAUC has a stronger connection with Top-K evaluation metrics than AUC and verify it with simulation experiments. These analyses establish the theoretical foundation of HNS in optimizing Top-K recommendation performance for the first time. On these bases, we offer two insightful guidelines for effective usage of HNS: 1) the sampling hardness should be controllable, e.g., via pre-defined hyper-parameters, to adapt to different Top-K metrics and datasets; 2) the smaller the $K$ we emphasize in Top-K evaluation metrics, the harder the negative samples we should draw. Extensive experiments on three real-world benchmarks verify the two guidelines. | Wentao Shi, Jiawei Chen, Fuli Feng, Jizhi Zhang, Junkang Wu, Chongming Gao, Xiangnan He | Zhejiang University, China; University of Science and Technology of China, China |
| 40 |  |  [A Counterfactual Collaborative Session-based Recommender System](https://doi.org/10.1145/3543507.3583321) |  | 0 | Most session-based recommender systems (SBRSs) focus on extracting information from the observed items in the current session of a user to predict a next item, ignoring the causes outside the session (called outer-session causes, OSCs) that influence the user's selection of items. However, these causes widely exist in the real world, and few studies have investigated their role in SBRSs. In this work, we analyze the causalities and correlations of the OSCs in SBRSs from the perspective of causal inference. We find that the OSCs are essentially the confounders in SBRSs, which leads to spurious correlations in the data used to train SBRS models. To address this problem, we propose a novel SBRS framework named COCO-SBRS (COunterfactual COllaborative Session-Based Recommender Systems) to learn the causality between OSCs and user-item interactions in SBRSs. COCO-SBRS first adopts a self-supervised approach to pre-train a recommendation model by designing pseudo-labels of causes for each user's selection of the item in data to guide the training process. Next, COCO-SBRS adopts counterfactual inference to recommend items based on the outputs of the pre-trained recommendation model considering the causalities to alleviate the data sparsity problem. As a result, COCO-SBRS can learn the causalities in data, preventing the model from learning spurious correlations. The experimental results of our extensive experiments conducted on three real-world datasets demonstrate the superiority of our proposed framework over ten representative SBRSs. | Wenzhuo Song, Shoujin Wang, Yan Wang, Kunpeng Liu, Xueyan Liu, Minghao Yin | Macquarie University, Australia; Jilin University, China; Northeast Normal University, China; University of Technology Sydney, Australia; Portland State University, USA |
| 41 |  |  [Debiased Contrastive Learning for Sequential Recommendation](https://doi.org/10.1145/3543507.3583361) |  | 0 | Current sequential recommender systems are proposed to tackle the dynamic user preference learning with various neural techniques, such as Transformer and Graph Neural Networks (GNNs). However, inference from the highly sparse user behavior data may hinder the representation ability of sequential pattern encoding. To address the label shortage issue, contrastive learning (CL) methods are proposed recently to perform data augmentation in two fashions: (i) randomly corrupting the sequence data (e.g. stochastic masking, reordering); (ii) aligning representations across pre-defined contrastive views. Although effective, we argue that current CL-based methods have limitations in addressing popularity bias and disentangling of user conformity and real interest. In this paper, we propose a new Debiased Contrastive learning paradigm for Recommendation (DCRec) that unifies sequential pattern encoding with global collaborative relation modeling through adaptive conformity-aware augmentation. This solution is designed to tackle the popularity bias issue in recommendation systems. Our debiased contrastive learning framework effectively captures both the patterns of item transitions within sequences and the dependencies between users across sequences. Our experiments on various real-world datasets have demonstrated that DCRec significantly outperforms state-of-the-art baselines, indicating its efficacy for recommendation. To facilitate reproducibility of our results, we make our implementation of DCRec publicly available at: https://github.com/HKUDS/DCRec. | Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, Kangyi Lin |  |
| 42 |  |  [Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders](https://doi.org/10.1145/3543507.3583434) |  | 0 | Recently, the generality of natural language text has been leveraged to develop transferable recommender systems. The basic idea is to employ pre-trained language models~(PLM) to encode item text into item representations. Despite the promising transferability, the binding between item text and item representations might be too tight, leading to potential problems such as over-emphasizing the effect of text features and exaggerating the negative impact of domain gap. To address this issue, this paper proposes VQ-Rec, a novel approach to learning Vector-Quantized item representations for transferable sequential Recommenders. The main novelty of our approach lies in the new item representation scheme: it first maps item text into a vector of discrete indices (called item code), and then employs these indices to lookup the code embedding table for deriving item representations. Such a scheme can be denoted as "text $\Longrightarrow$ code $\Longrightarrow$ representation". Based on this representation scheme, we further propose an enhanced contrastive pre-training approach, using semi-synthetic and mixed-domain code representations as hard negatives. Furthermore, we design a new cross-domain fine-tuning method based on a differentiable permutation-based network. Extensive experiments conducted on six public benchmarks demonstrate the effectiveness of the proposed approach, in both cross-domain and cross-platform settings. Code and pre-trained model are available at: https://github.com/RUCAIBox/VQ-Rec. | Yupeng Hou, Zhankui He, Julian J. McAuley, Wayne Xin Zhao | Beijing Key Laboratory of Big Data Management and Analysis Methods, Renmin University of China, China; UC San Diego, USA; Renmin University of China, China |
| 43 |  |  [KAE-Informer: A Knowledge Auto-Embedding Informer for Forecasting Long-Term Workloads of Microservices](https://doi.org/10.1145/3543507.3583288) |  | 0 | Accurately forecasting workloads in terms of throughput that is quantified as queries per second (QPS) is essential for microservices to elastically adjust their resource allocations. However, long-term QPS prediction is challenging in two aspects: 1) generality across various services with different temporal patterns, 2) characterization of intricate QPS sequences which are entangled by multiple components. In this paper, we propose a knowledge auto-embedding Informer network (KAE-Informer) for forecasting the long-term QPS sequences of microservices. By analyzing a large number of microservice traces, we discover that there are two main decomposable and predictable components in QPS sequences, namely global trend & dominant periodicity (TP) and low-frequency residual patterns with long-range dependencies. These two components are important for accurately forecasting long-term QPS. First, KAE-Informer embeds the knowledge of TP components through mathematical modeling. Second, KAE-Informer designs a convolution ProbSparse self-attention mechanism and a multi-layer event discrimination scheme to extract and embed the knowledge of local context awareness and event regression effect implied in residual components, respectively. We conduct experiments based on three real datasets including a QPS dataset collected from 40 microservices. The experiment results show that KAE-Informer achieves a reduction of MAPE, MAE and RMSE by about 16.6%, 17.6% and 23.1% respectively, compared to the state-of-the-art models. | Qin Hua, Dingyu Yang, Shiyou Qian, Hanwen Hu, Jian Cao, Guangtao Xue | Alibaba Group, China; Shanghai Jiao Tong University, China |
| 44 |  |  [Propaganda Política Pagada: Exploring U.S. Political Facebook Ads en Español](https://doi.org/10.1145/3543507.3583425) |  | 0 | In 2021, the U.S. Hispanic population totaled 62.5 million people, 68% of whom spoke Spanish in their homes. To date, it is unclear which political advertisers address this audience in their preferred language, and whether they do so differently than for English-speaking audiences. In this work, we study differences between political Facebook ads in English and Spanish during 2020, the latest U.S. presidential election. Political advertisers spent $ 1.48 B in English, but only $ 28.8 M in Spanish, disproportionately little compared to the share of Spanish speakers in the population. We further find a lower proportion of election-related advertisers (which additionally are more liberal-leaning than in the English set), and a higher proportion of government agencies in the set of Spanish ads. We perform multilingual topic classification, finding that the most common ad topics in English were also present in Spanish, but to a different extent, and with a different composition of advertisers. Thus, Spanish speakers are served different types of ads from different types of advertisers than English speakers, and in lower amounts; these results raise the question of whether political communication through Facebook ads may be inequitable and effectively disadvantaging the sizeable minority of Spanish speakers in the U.S. population. | Bruno Coelho, Tobias Lauinger, Laura Edelson, Ian Goldstein, Damon McCoy | New York University, USA |
| 45 |  |  [Learning Denoised and Interpretable Session Representation for Conversational Search](https://doi.org/10.1145/3543507.3583265) |  | 0 | Conversational search supports multi-turn user-system interactions to solve complex information needs. Compared with the traditional single-turn ad-hoc search, conversational search faces a more complex search intent understanding problem because a conversational search session is much longer and contains many noisy tokens. However, existing conversational dense retrieval solutions simply fine-tune the pre-trained ad-hoc query encoder on limited conversational search data, which are hard to achieve satisfactory performance in such a complex conversational search scenario. Meanwhile, the learned latent representation also lacks interpretability that people cannot perceive how the model understands the session. To tackle the above drawbacks, we propose a sparse Lexical-based Conversational REtriever (LeCoRE), which extends the SPLADE model with two well-matched multi-level denoising methods uniformly based on knowledge distillation and external query rewrites to generate denoised and interpretable lexical session representation. Extensive experiments on four public conversational search datasets in both normal and zero-shot evaluation settings demonstrate the strong performance of LeCoRE towards more effective and interpretable conversational search. | Kelong Mao, Hongjin Qian, Fengran Mo, Zhicheng Dou, Bang Liu, Xiaohua Cheng, Zhao Cao | Université de Montréal, Canada; RALI & Mila, Université de Montréal, Canada; Huawei Poisson Lab, China; Renmin University of China, China |
| 46 |  |  [Fairly Adaptive Negative Sampling for Recommendations](https://doi.org/10.1145/3543507.3583355) |  | 0 | Pairwise learning strategies are prevalent for optimizing recommendation models on implicit feedback data, which usually learns user preference by discriminating between positive (i.e., clicked by a user) and negative items (i.e., obtained by negative sampling). However, the size of different item groups (specified by item attribute) is usually unevenly distributed. We empirically find that the commonly used uniform negative sampling strategy for pairwise algorithms (e.g., BPR) can inherit such data bias and oversample the majority item group as negative instances, severely countering group fairness on the item side. In this paper, we propose a Fairly adaptive Negative sampling approach (FairNeg), which improves item group fairness via adaptively adjusting the group-level negative sampling distribution in the training process. In particular, it first perceives the model's unfairness status at each step and then adjusts the group-wise sampling distribution with an adaptive momentum update strategy for better facilitating fairness optimization. Moreover, a negative sampling distribution Mixup mechanism is proposed, which gracefully incorporates existing importance-aware sampling techniques intended for mining informative negative samples, thus allowing for achieving multiple optimization purposes. Extensive experiments on four public datasets show our proposed method's superiority in group fairness enhancement and fairness-utility tradeoff. | Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang, Qing Li |  |
| 47 |  |  [CNSVRE: A Query Reformulated Search System with Explainable Summarization for Virtual Research Environment](https://doi.org/10.1145/3543873.3587360) |  | 0 | Computational notebook environments have drawn broad attention in data-centric research applications, e.g., virtual research environment, for exploratory data analysis and algorithm prototyping. Vanilla computational notebook search solutions have been proposed but they do not pay much attention to the information needs of scientific researchers. Previous studies either treat computational notebook search as a code search problem or focus on content-based computational notebook search. The queries being considered are neither research-concerning nor diversified whereas researchers’ information needs are highly specialized and complex. Moreover, relevance evaluation for computational notebooks is tricky and unreliable since computational notebooks contain fragments of text and code and are usually poorly organized. To solve the above challenges, we propose a computational notebook search system for virtual research environment (VRE), i.e., CNSVRE, with scientific query reformulation and computational notebook summarization. We conduct a user study to demonstrate the effectiveness, efficiency, and satisfaction with the system. | Na Li, Yangjun Zhang, Zhiming Zhao | University of Amsterdam, Netherlands |
| 48 |  |  [Personalized style recommendation via reinforcement learning](https://doi.org/10.1145/3543873.3587367) |  | 0 | Pinterest fashion and home decor searchers often have different style tastes. Some existing work adopts users’ past engagement to infer style preference. These methods cannot help users discover new styles. Other work requires users to provide text or visual signals to describe their style preference, but users often are not familiar with style terms and do not have the right image to start with. In this paper, we propose a reinforcement learning (RL) method to help users explore and exploit style space without requiring extra user input. Experimental results show that our method improves the success rate of Pinterest fashion and home decor searches by 34.8%. | Jiyun Luo, Kurchi Subhra Hazra, Wenyu Huo, Rui Li, Abhijit Mahabal | Pinterest Inc., USA |
| 49 |  |  [HierCat: Hierarchical Query Categorization from Weakly Supervised Data at Facebook Marketplace](https://doi.org/10.1145/3543873.3584622) |  | 0 | Query categorization at customer-to-customer e-commerce platforms like Facebook Marketplace is challenging due to the vagueness of search intent, noise in real-world data, and imbalanced training data across languages. Its deployment also needs to consider challenges in scalability and downstream integration in order to translate modeling advances into better search result relevance. In this paper we present HierCat, the query categorization system at Facebook Marketplace. HierCat addresses these challenges by leveraging multi-task pre-training of dual-encoder architectures with a hierarchical inference step to effectively learn from weakly supervised training data mined from searcher engagement. We show that HierCat not only outperforms popular methods in offline experiments, but also leads to 1.4% improvement in NDCG and 4.3% increase in searcher engagement at Facebook Marketplace Search in online A/B testing. | Yunzhong He, Cong Zhang, Ruoyan Kong, Chaitanya Kulkarni, Qing Liu, Ashish Gandhe, Amit Nithianandan, Arul Prakash | University of Minnesota Twin Cities, USA; Meta, USA |
| 50 |  |  [Search-based Recommendation: the Case for Difficult Predictions](https://doi.org/10.1145/3543873.3587374) |  | 0 | Recommender systems have achieved impressive results on benchmark datasets. However, the numbers are often influenced by assumptions made on the data and evaluation mode. This work questions and revises these assumptions, to study and improve the quality, particularly for the difficult case of search-based recommendations. Users start with a personally liked item as a query and look for similar items that match their tastes. User satisfaction requires discovering truly unknown items: new authors of books rather than merely more books of known writers. We propose a unified system architecture that combines interaction-based and content-based signals and leverages language models for Transformer-powered predictions. We present new techniques for selecting negative training samples, and investigate their performance in the underexplored search-based evaluation mode. | Ghazaleh Haratinezhad Torbati, Gerhard Weikum, Andrew Yates | Max Planck Institute for Informatics, Germany; University of Amsterdam, Netherlands |
| 51 |  |  [Reweighting Clicks with Dwell Time in Recommendation](https://doi.org/10.1145/3543873.3584624) |  | 0 | The click behavior is the most widely-used user positive feedback in recommendation. However, simply considering each click equally in training may suffer from clickbaits and title-content mismatching, and thus fail to precisely capture users' real satisfaction on items. Dwell time could be viewed as a high-quality quantitative indicator of user preferences on each click, while existing recommendation models do not fully explore the modeling of dwell time. In this work, we focus on reweighting clicks with dwell time in recommendation. Precisely, we first define a new behavior named valid read, which helps to select high-quality click instances for different users and items via dwell time. Next, we propose a normalized dwell time function to reweight click signals in training for recommendation. The Click reweighting model achieves significant improvements on both offline and online evaluations in real-world systems. | Ruobing Xie, Lin Ma, Shaoliang Zhang, Feng Xia, Leyu Lin | WeChat, Tencent, China |
| 52 |  |  [Disentangled Causal Embedding With Contrastive Learning For Recommender System](https://doi.org/10.1145/3543873.3584637) |  | 0 | Recommender systems usually rely on observed user interaction data to build personalized recommendation models, assuming that the observed data reflect user interest. However, user interacting with an item may also due to conformity, the need to follow popular items. Most previous studies neglect user's conformity and entangle interest with it, which may cause the recommender systems fail to provide satisfying results. Therefore, from the cause-effect view, disentangling these interaction causes is a crucial issue. It also contributes to OOD problems, where training and test data are out-of-distribution. Nevertheless, it is quite challenging as we lack the signal to differentiate interest and conformity. The data sparsity of pure cause and the items' long-tail problem hinder disentangled causal embedding. In this paper, we propose DCCL, a framework that adopts contrastive learning to disentangle these two causes by sample augmentation for interest and conformity respectively. Futhermore, DCCL is model-agnostic, which can be easily deployed in any industrial online system. Extensive experiments are conducted over two real-world datasets and DCCL outperforms state-of-the-art baselines on top of various backbone models in various OOD environments. We also demonstrate the performance improvements by online A/B testing on Kuaishou, a billion-user scale short-video recommender system. | Weiqi Zhao, Dian Tang, Xin Chen, Dawei Lv, Daoli Ou, Biao Li, Peng Jiang, Kun Gai | Kuaishou Technology, China; Unaffiliated, China |
| 53 |  |  [Confidence Ranking for CTR Prediction](https://doi.org/10.1145/3543873.3584643) |  | 0 | Model evolution and data updating are two common phenomena in large-scale real-world machine learning applications, e.g. ads and recommendation systems. To adapt, the real-world system typically retrain with all available data and online learn with recently available data to update the models periodically with the goal of better serving performance. In this paper, we propose a novel framework, named Confidence Ranking, which designs the optimization objective as a ranking function with two different models. Our confidence ranking loss allows direct optimization of the logits output for different convex surrogate functions of metrics, e.g. AUC and Accuracy depending on the target task and dataset. Armed with our proposed methods, our experiments show that the introduction of confidence ranking loss can outperform all baselines on the CTR prediction tasks of public and industrial datasets. This framework has been deployed in the advertisement system of JD.com to serve the main traffic in the fine-rank stage. | Jian Zhu, Congcong Liu, Pei Wang, Xiwei Zhao, Zhangang Lin, Jingping Shao | JD.com, China |
| 54 |  |  [Personalised Search in E-Comm Groceries](https://doi.org/10.1145/3543873.3587588) |  | 0 | Personalized Search(henceforth called P10d Search) focuses to deliver user-specific search results based on the previous purchases. Search engine retrieves the result based on the defined relevancy algorithm. When a user searches a keyword, search engine constructs the search query based on the defined searchable fields/attributes along with configured relevancy algorithm. Position of the item retrieved in search results is determined by the search algorithm based on the search term. The results are further refined or ranked based on different click stream signals, product features, market data to provide much relevant results. Personalisation provides the ranked the list of items for a given user based on past purchases. Personalisation is agnostic of search query and takes user id, cart additions, site taxonomy and user’s shopping history as input signals. In summary, search engine queries data based on relevancy and personalisation engine retrieves based purely on purchases. Goal of personalised search is to enhance the search results by adding personalised results without affecting the search relevance. | Ramprabhu Murugesan, Anuja Sharan | Walmart labs, India; Walmart Labs, India |
| 55 |  |  [Graph Embedding for Mapping Interdisciplinary Research Networks](https://doi.org/10.1145/3543873.3587570) |  | 0 | Representation learning is the first step in automating tasks such as research paper recommendation, classification, and retrieval. Due to the accelerating rate of research publication, together with the recognised benefits of interdisciplinary research, systems that facilitate researchers in discovering and understanding relevant works from beyond their immediate school of knowledge are vital. This work explores different methods of research paper representation (or document embedding), to identify those methods that are capable of preserving the interdisciplinary implications of research papers in their embeddings. In addition to evaluating state of the art methods of document embedding in a interdisciplinary citation prediction task, we propose a novel Graph Neural Network architecture designed to preserve the key interdisciplinary implications of research articles in citation network node embeddings. Our proposed method outperforms other GNN-based methods in interdisciplinary citation prediction, without compromising overall citation prediction performance. | Eoghan Cunningham, Derek Greene | University College Dublin, Ireland |
| 56 |  |  [Deep Passage Retrieval in E-Commerce](https://doi.org/10.1145/3543873.3587624) |  | 0 | We have developed a conversational assistant called the Decision Assistant (DA) to help customers make purchase decisions. To answer customer queries successfully, we use a question and answering (QnA) system that retrieves data on product pages and extracts answers. With various data sources available on the product pages, we deal with unique challenges such as different terminologies and data formats for successful answer retrieval. In this paper, we propose two different bi-encoder architectures for retrieving data from each of the two data sources considered – product descriptions and specifications. The proposed architectures beat the baseline approaches while maintaining a high recall and low latency in production. We envision that the proposed approaches can be widely applicable to other e-commerce QnA systems. | Vinay Rao Dandin, Ozan Ersoy, Kyung Hyuk Kim | Flipkart US R&D Center, USA |
| 57 |  |  [Quantize Sequential Recommenders Without Private Data](https://doi.org/10.1145/3543507.3583351) |  | 0 | Deep neural networks have achieved great success in sequential recommendation systems. While maintaining high competence in user modeling and next-item recommendation, these models have long been plagued by the numerous parameters and computation, which inhibit them to be deployed on resource-constrained mobile devices. Model quantization, as one of the main paradigms for compression techniques, converts float parameters to low-bit values to reduce parameter redundancy and accelerate inference. To avoid drastic performance degradation, it usually requests a fine-tuning phase with an original dataset. However, the training set of user-item interactions is not always available due to transmission limits or privacy concerns. In this paper, we propose a novel framework to quantize sequential recommenders without access to any real private data. A generator is employed in the framework to synthesize fake sequence samples to feed the quantized sequential recommendation model and minimize the gap with a full-precision sequential recommendation model. The generator and the quantized model are optimized with a min-max game — alternating discrepancy estimation and knowledge transfer. Moreover, we devise a two-level discrepancy modeling strategy to transfer information between the quantized model and the full-precision model. The extensive experiments of various recommendation networks on three public datasets demonstrate the effectiveness of the proposed framework. | Lingfeng Shi, Yuang Liu, Jun Wang, Wei Zhang | East China Normal University, China |
| 58 |  |  [Adap-τ : Adaptively Modulating Embedding Magnitude for Recommendation](https://doi.org/10.1145/3543507.3583363) |  | 0 | Recent years have witnessed the great successes of embedding-based methods in recommender systems. Despite their decent performance, we argue one potential limitation of these methods — the embedding magnitude has not been explicitly modulated, which may aggravate popularity bias and training instability, hindering the model from making a good recommendation. It motivates us to leverage the embedding normalization in recommendation. By normalizing user/item embeddings to a specific value, we empirically observe impressive performance gains (9% on average) on four real-world datasets. Although encouraging, we also reveal a serious limitation when applying normalization in recommendation — the performance is highly sensitive to the choice of the temperature τ which controls the scale of the normalized embeddings. To fully foster the merits of the normalization while circumvent its limitation, this work studied on how to adaptively set the proper τ. Towards this end, we first make a comprehensive analyses of τ to fully understand its role on recommendation. We then accordingly develop an adaptive fine-grained strategy Adap-τ for the temperature with satisfying four desirable properties including adaptivity, personalized, efficiency and model-agnostic. Extensive experiments have been conducted to validate the effectiveness of the proposal. The code is available at https://github.com/junkangwu/Adap_tau. | Jiawei Chen, Junkang Wu, Jiancan Wu, Xuezhi Cao, Sheng Zhou, Xiangnan He | Zhejiang University, China; University of Science and Technology of China, China |
| 59 |  |  [Clustered Embedding Learning for Recommender Systems](https://doi.org/10.1145/3543507.3583362) |  | 0 | In recent years, recommender systems have advanced rapidly, where embedding learning for users and items plays a critical role. A standard method learns a unique embedding vector for each user and item. However, such a method has two important limitations in real-world applications: 1) it is hard to learn embeddings that generalize well for users and items with rare interactions on their own; and 2) it may incur unbearably high memory costs when the number of users and items scales up. Existing approaches either can only address one of the limitations or have flawed overall performances. In this paper, we propose Clustered Embedding Learning (CEL) as an integrated solution to these two problems. CEL is a plug-and-play embedding learning framework that can be combined with any differentiable feature interaction model. It is capable of achieving improved performance, especially for cold users and items, with reduced memory cost. CEL enables automatic and dynamic clustering of users and items in a top-down fashion, where clustered entities jointly learn a shared embedding. The accelerated version of CEL has an optimal time complexity, which supports efficient online updates. Theoretically, we prove the identifiability and the existence of a unique optimal number of clusters for CEL in the context of nonnegative matrix factorization. Empirically, we validate the effectiveness of CEL on three public datasets and one business dataset, showing its consistently superior performance against current state-of-the-art methods. In particular, when incorporating CEL into the business model, it brings an improvement of $+0.6\%$ in AUC, which translates into a significant revenue gain; meanwhile, the size of the embedding table gets $2650$ times smaller. | Yizhou Chen, Guangda Huzhang, Anxiang Zeng, Qingtao Yu, Hui Sun, HengYi Li, Jingyi Li, Yabo Ni, Han Yu, Zhiming Zhou | SCSE, Nanyang Technological University, Singapore; Shanghai University of Finance and Economics, China; Shopee Pte Ltd., Singapore |
| 60 |  |  [MMMLP: Multi-modal Multilayer Perceptron for Sequential Recommendations](https://doi.org/10.1145/3543507.3583378) |  | 0 | Sequential recommendation aims to offer potentially interesting products to users by capturing their historical sequence of interacted items. Although it has facilitated extensive physical scenarios, sequential recommendation for multi-modal sequences has long been neglected. Multi-modal data that depicts a user’s historical interactions exists ubiquitously, such as product pictures, textual descriptions, and interacted item sequences, providing semantic information from multiple perspectives that comprehensively describe a user’s preferences. However, existing sequential recommendation methods either fail to directly handle multi-modality or suffer from high computational complexity. To address this, we propose a novel Multi-Modal Multi-Layer Perceptron (MMMLP) for maintaining multi-modal sequences for sequential recommendation. MMMLP is a purely MLP-based architecture that consists of three modules - the Feature Mixer Layer, Fusion Mixer Layer, and Prediction Layer - and has an edge on both efficacy and efficiency. Extensive experiments show that MMMLP achieves state-of-the-art performance with linear complexity. We also conduct ablating analysis to verify the contribution of each component. Furthermore, compatible experiments are devised, and the results show that the multi-modal representation learned by our proposed model generally benefits other recommendation models, emphasizing our model’s ability to handle multi-modal information. We have made our code available online to ease reproducibility1. | Jiahao Liang, Xiangyu Zhao, Muyang Li, Zijian Zhang, Wanyu Wang, Haochen Liu, Zitao Liu | University of Sydney, Australia; Michigan State University, USA; Jilin University, China and City University of Hong Kong, Hong Kong; Guangdong Institute of Smart Education, Jinan University, China; City University of Hong Kong, Hong Kong |
| 61 |  |  [AutoMLP: Automated MLP for Sequential Recommendations](https://doi.org/10.1145/3543507.3583440) |  | 0 | Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through extensive experiments, we show that AutoMLP has competitive performance against state-of-the-art methods, while maintaining linear computational complexity. | Muyang Li, Zijian Zhang, Xiangyu Zhao, Wanyu Wang, Minghao Zhao, Runze Wu, Ruocheng Guo | Bytedance AI Lab UK, United Kingdom; Fuxi AI Lab, NetEase, China; City University of Hong Kong, Hong Kong and Jilin University, China; City University of Hong Kong, Hong Kong; City University of Hong Kong, Hong Kong and University of Sydney, Australia |
| 62 |  |  [NASRec: Weight Sharing Neural Architecture Search for Recommender Systems](https://doi.org/10.1145/3543507.3583446) |  | 0 | The rise of deep neural networks offers new opportunities in optimizing recommender systems. However, optimizing recommender systems using deep neural networks requires delicate architecture fabrication. We propose NASRec, a paradigm that trains a single supernet and efficiently produces abundant models/sub-architectures by weight sharing. To overcome the data multi-modality and architecture heterogeneity challenges in the recommendation domain, NASRec establishes a large supernet (i.e., search space) to search the full architectures. The supernet incorporates versatile choice of operators and dense connectivity to minimize human efforts for finding priors. The scale and heterogeneity in NASRec impose several challenges, such as training inefficiency, operator-imbalance, and degraded rank correlation. We tackle these challenges by proposing single-operator any-connection sampling, operator-balancing interaction modules, and post-training fine-tuning. Our crafted models, NASRecNet, show promising results on three Click-Through Rates (CTR) prediction benchmarks, indicating that NASRec outperforms both manually designed models and existing NAS methods with state-of-the-art performance. Our work is publicly available at https://github.com/facebookresearch/NasRec. | Tunhou Zhang, Dehua Cheng, Yuchen He, Zhengxing Chen, Xiaoliang Dai, Liang Xiong, Feng Yan, Hai Li, Yiran Chen, Wei Wen | University of Houston, USA; Duke University, USA; Meta AI, USA |
| 63 |  |  [Membership Inference Attacks Against Sequential Recommender Systems](https://doi.org/10.1145/3543507.3583447) |  | 0 | Recent studies have demonstrated the vulnerability of recommender systems to membership inference attacks, which determine whether a user’s historical data was utilized for model training, posing serious privacy leakage issues. Existing works assumed that member and non-member users follow different recommendation modes, and then infer membership based on the difference vector between the user’s historical behaviors and the recommendation list. The previous frameworks are invalid against inductive recommendations, such as sequential recommendations, since the disparities of difference vectors constructed by the recommendations between members and non-members become imperceptible. This motivates us to dig deeper into the target model. In addition, most MIA frameworks assume that they can obtain some in-distribution data from the same distribution of the target data, which is hard to gain in recommender system. To address these difficulties, we propose a Membership Inference Attack framework against sequential recommenders based on Model Extraction(ME-MIA). Specifically, we train a surrogate model to simulate the target model based on two universal loss functions. For a given behavior sequence, the loss functions ensure the recommended items and corresponding rank of the surrogate model are consistent with the target model’s recommendation. Due to the special training mode of the surrogate model, it is hard to judge which user is its member(non-member). Therefore, we establish a shadow model and use shadow model’s members(non-members) to train the attack model later. Next, we build a user feature generator to construct representative feature vectors from the shadow(surrogate) model. The crafting feature vectors are finally input into the attack model to identify users’ membership. Furthermore, to tackle the high cost of obtaining in-distribution data, we develop two variants of ME-MIA, realizing data-efficient and even data-free MIA by fabricating authentic in-distribution data. Notably, the latter is impossible in the previous works. Finally, we evaluate ME-MIA against multiple sequential recommendation models on three real-world datasets. Experimental results show that ME-MIA and its variants can achieve efficient extraction and outperform state-of-the-art algorithms in terms of attack performance. | Zhihao Zhu, Chenwang Wu, Rui Fan, Defu Lian, Enhong Chen | University of Science and Technology of China, China |
| 64 |  |  [Communicative MARL-based Relevance Discerning Network for Repetition-Aware Recommendation](https://doi.org/10.1145/3543507.3583459) |  | 0 | The repeated user-item interaction now is becoming a common phenomenon in the e-commerce scenario. Due to its potential economic profit, various models are emerging to predict which item will be re-interacted based on the user-item interactions. In this specific scenario, item relevance is a critical factor that needs to be concerned, which tends to have different effects on the succeeding re-interacted one (i.e., stimulating or delaying its emergence). It is necessary to make a detailed discernment of item relevance for a better repetition-aware recommendation. Unfortunately, existing works usually mixed all these types, which may disturb the learning process and result in poor performance. In this paper, we introduce a novel Communicative MARL-based Relevance Discerning Network (CARDfor short) to automatically discern the item relevance for a better repetition-aware recommendation. Specifically, CARDformalizes the item relevance discerning problem into a communication selection process in MARL. CARDtreats each unique interacted item as an agent and defines three different communication types over agents, which are stimulative, inhibitive, and noisy respectively. After this, CARDutilizes a Gumbel-enhanced classifier to distinguish the communication types among agents, and an attention-based Reactive Point Process is further designed to transmit the well-discerned stimulative and inhibitive incentives separately among all agents to make an effective collaboration for repetition decisions. Experimental results on two real-world e-commerce datasets show that our proposed method outperforms the state-of-the-art recommendation methods in terms of both sequential and repetition-aware recommenders. Furthermore, CARDis also deployed in the online sponsored search advertising system in Meituan, obtaining a performance improvement of over 1.5% and 1.2% in CTR and effective Cost Per Mille (eCPM) respectively, which is significant to the business. | Kaiyuan Li, Pengfei Wang, Haitao Wang, Qiang Liu, Xingxing Wang, Dong Wang, Shangguang Wang | Meituan, China; Beijing University of Posts and Telecommunications, China |
| 65 |  |  [Personalized Graph Signal Processing for Collaborative Filtering](https://doi.org/10.1145/3543507.3583466) |  | 0 | The collaborative filtering (CF) problem with only user-item interaction information can be solved by graph signal processing (GSP), which uses low-pass filters to smooth the observed interaction signals on the similarity graph to obtain the prediction signals. However, the interaction signal may not be sufficient to accurately characterize user interests and the low-pass filters may ignore the useful information contained in the high-frequency component of the observed signals, resulting in suboptimal accuracy. To this end, we propose a personalized graph signal processing (PGSP) method for collaborative filtering. Firstly, we design the personalized graph signal containing richer user information and construct an augmented similarity graph containing more graph topology information, to more effectively characterize user interests. Secondly, we devise a mixed-frequency graph filter to introduce useful information in the high-frequency components of the observed signals by combining an ideal low-pass filter that smooths signals globally and a linear low-pass filter that smooths signals locally. Finally, we combine the personalized graph signal, the augmented similarity graph and the mixed-frequency graph filter by proposing a pipeline consisting of three key steps: pre-processing, graph convolution and post-processing. Extensive experiments show that PGSP can achieve superior accuracy compared with state-of-the-art CF methods and, as a nonparametric method, PGSP has very high training efficiency. | Jiahao Liu, Dongsheng Li, Hansu Gu, Tun Lu, Peng Zhang, Li Shang, Ning Gu | Microsoft Research Asia, China; Amazon, USA; School of Computer Science, Fudan University, China and Shanghai Key Laboratory of Data Science, Fudan University, China |
| 66 |  |  [Multi-Task Recommendations with Reinforcement Learning](https://doi.org/10.1145/3543507.3583467) |  | 0 | In recent years, Multi-task Learning (MTL) has yielded immense success in Recommender System (RS) applications. However, current MTL-based recommendation models tend to disregard the session-wise patterns of user-item interactions because they are predominantly constructed based on item-wise datasets. Moreover, balancing multiple objectives has always been a challenge in this field, which is typically avoided via linear estimations in existing works. To address these issues, in this paper, we propose a Reinforcement Learning (RL) enhanced MTL framework, namely RMTL, to combine the losses of different recommendation tasks using dynamic weights. To be specific, the RMTL structure can address the two aforementioned issues by (i) constructing an MTL environment from session-wise interactions and (ii) training multi-task actor-critic network structure, which is compatible with most existing MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL loss function using the weights generated by critic networks. Experiments on two real-world public datasets demonstrate the effectiveness of RMTL with a higher AUC against state-of-the-art MTL-based recommendation models. Additionally, we evaluate and validate RMTL's compatibility and transferability across various MTL models. | Ziru Liu, Jiejie Tian, Qingpeng Cai, Xiangyu Zhao, Jingtong Gao, Shuchang Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, Kun Gai | Unaffiliated, China; Kuaishou, China; City University of Hong Kong, China |
| 67 |  |  [A Self-Correcting Sequential Recommender](https://doi.org/10.1145/3543507.3583479) |  | 0 | Sequential recommendations aim to capture users' preferences from their historical interactions so as to predict the next item that they will interact with. Sequential recommendation methods usually assume that all items in a user's historical interactions reflect her/his preferences and transition patterns between items. However, real-world interaction data is imperfect in that (i) users might erroneously click on items, i.e., so-called misclicks on irrelevant items, and (ii) users might miss items, i.e., unexposed relevant items due to inaccurate recommendations. To tackle the two issues listed above, we propose STEAM, a Self-correcTing sEquentiAl recoMmender. STEAM first corrects an input item sequence by adjusting the misclicked and/or missed items. It then uses the corrected item sequence to train a recommender and make the next item prediction.We design an item-wise corrector that can adaptively select one type of operation for each item in the sequence. The operation types are 'keep', 'delete' and 'insert.' In order to train the item-wise corrector without requiring additional labeling, we design two self-supervised learning mechanisms: (i) deletion correction (i.e., deleting randomly inserted items), and (ii) insertion correction (i.e., predicting randomly deleted items). We integrate the corrector with the recommender by sharing the encoder and by training them jointly. We conduct extensive experiments on three real-world datasets and the experimental results demonstrate that STEAM outperforms state-of-the-art sequential recommendation baselines. Our in-depth analyses confirm that STEAM benefits from learning to correct the raw item sequences. | Yujie Lin, Chenyang Wang, Zhumin Chen, Zhaochun Ren, Xin Xin, Qiang Yan, Maarten de Rijke, Xiuzhen Cheng, Pengjie Ren | Shandong University, China; University of Amsterdam, Netherlands; WeChat, Tencent, China |
| 68 |  |  [Confident Action Decision via Hierarchical Policy Learning for Conversational Recommendation](https://doi.org/10.1145/3543507.3583536) |  | 0 | Conversational recommender systems (CRS) aim to acquire a user’s dynamic interests for a successful recommendation. By asking about his/her preferences, CRS explore current needs of a user and recommend items of interest. However, previous works may not determine a proper action in a timely manner which leads to the insufficient information gathering and the waste of conversation turns. Since they learn a single decision policy, it is difficult for them to address the general decision problems in CRS. Besides, existing methods do not distinguish whether the past behaviors inferred from the historical interactions are closely related to the user’s current preference. To address these issues, we propose a novel Hierarchical policy learning based Conversational Recommendation framework (HiCR). HiCR formulates the multi-round decision making process as a hierarchical policy learning scheme, which consists of both a high-level policy and a low-level policy. In detail, the high-level policy aims to determine what type of action to take, such as a recommendation or a query, by observing the comprehensive conversation information. According to the decided action type, the low-level policy selects a specific action, such as which attribute to ask or which item to recommend. The hierarchical conversation policy enables CRS to decide an optimal action, resulting in reducing the unnecessary consumption of conversation turns and the continuous failure of recommendations. Furthermore, in order to filter out the unnecessary historical information when enriching the current user preference, we extract and utilize the informative past behaviors that are attentive to the current needs. Empirical experiments on four real-world datasets show the superiority of our approach against the current state-of-the-art methods. | Heeseon Kim, Hyeongjun Yang, KyongHo Lee | Department of Computer Science, Yonsei University, Republic of Korea |
| 69 |  |  [Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation](https://doi.org/10.1145/3543507.3583529) |  | 0 | Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy Measurement builds upon the 2-Wasserstein distance, which is more robust, more efficient in small batch sizes, and able to model the uncertainty of stochastic augmentation processes. We also propose a novel contrastive learning loss based on Wasserstein Discrepancy Measurement. Extensive experiments on four benchmark datasets demonstrate the effectiveness of MStein over baselines. More quantitative analyses show the robustness against perturbations and training efficiency in batch size. Finally, improvements analysis indicates better representations of popular users or items with significant uncertainty. The source code is at https://github.com/zfan20/MStein. | Ziwei Fan, Zhiwei Liu, Hao Peng, Philip S. Yu | University of Illinois Chicago, USA; Salesforce AI Research, USA; Beihang University, USA |
| 70 |  |  [Automatic Feature Selection By One-Shot Neural Architecture Search In Recommendation Systems](https://doi.org/10.1145/3543507.3583444) |  | 0 | Feature selection is crucial in large-scale recommendation system, which can not only reduce the computational cost, but also improve the recommendation efficiency. Most existing works rank the features and then select the top-k ones as the final feature subset. However, they assess feature importance individually and ignore the interrelationship between features. Consequently, multiple features with high relevance may be selected simultaneously, resulting in sub-optimal result. In this work, we solve this problem by proposing an AutoML-based feature selection framework that can automatically search the optimal feature subset. Specifically, we first embed the search space into a weight-sharing Supernet. Then, a two-stage neural architecture search method is employed to evaluate the feature quality. In the first stage, a well-designed sampling method considering feature convergence fairness is applied to train the Supernet. In the second stage, a reinforcement learning method is used to search for the optimal feature subset efficiently. The Experimental results on two real datasets demonstrate the superior performance of new framework over other solutions. Our proposed method obtain significant improvement with a 20% reduction in the amount of features on the Criteo. More validation experiments demonstrate the ability and robustness of the framework. | He Wei, Yuekui Yang, Haiyang Wu, Yangyang Tang, Meixi Liu, Jianfeng Li | Machine learning platform department, TEG, Tencent, China and Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, China; Machine learning platform department, TEG, Tencent, China |
| 71 |  |  [Catch: Collaborative Feature Set Search for Automated Feature Engineering](https://doi.org/10.1145/3543507.3583527) |  | 0 | Feature engineering often plays a crucial role in building mining systems for tabular data, which traditionally requires experienced human experts to perform. Thanks to the rapid advances in reinforcement learning, it has offered an automated alternative, i.e. automated feature engineering (AutoFE). In this work, through scrutiny of the prior AutoFE methods, we characterize several research challenges that remained in this regime, concerning system-wide efficiency, efficacy, and practicality toward production. We then propose Catch, a full-fledged new AutoFE framework that comprehensively addresses the aforementioned challenges. The core to Catch composes a hierarchical-policy reinforcement learning scheme that manifests a collaborative feature engineering exploration and exploitation grounded on the granularity of the whole feature set. At a higher level of the hierarchy, a decision-making module controls the post-processing of the attained feature engineering transformation. We extensively experiment with Catch on 26 academic standardized tabular datasets and 9 industrialized real-world datasets. Measured by numerous metrics and analyses, Catch establishes a new state-of-the-art, from perspectives performance, latency as well as its practicality towards production. Source code1 can be found at https://github.com/1171000709/Catch. | Guoshan Lu, Haobo Wang, Saisai Yang, Jing Yuan, Guozheng Yang, Cheng Zang, Gang Chen, Junbo Zhao | Zhejiang University, China; Zheshang Bank Co., Ltd., China; Institute of Computing Innovation, Zhejiang University, China |
| 72 |  |  [The Hitchhiker's Guide to Facebook Web Tracking with Invisible Pixels and Click IDs](https://doi.org/10.1145/3543507.3583311) |  | 0 | Over the past years, advertisement companies have used various tracking methods to persistently track users across the web. Such tracking methods usually include first and third-party cookies, cookie synchronization, as well as a variety of fingerprinting mechanisms. Facebook (FB) recently introduced a new tagging mechanism that attaches a one-time tag as a URL parameter (FBCLID) on outgoing links to other websites. Although such a tag does not seem to have enough information to persistently track users, we demonstrate that despite its ephemeral nature, when combined with FB Pixel, it can aid in persistently monitoring user browsing behavior across i) different websites, ii) different actions on each website, iii) time, i.e., both in the past as well as in the future. We refer to this online monitoring of users as FB web tracking. We find that FB Pixel tracks a wide range of user activities on websites with alarming detail, especially on websites classified as sensitive categories under GDPR. Also, we show how the FBCLID tag can be used to match, and thus de-anonymize, activities of online users performed in the distant past (even before those users had a FB account) tracked by FB Pixel. In fact, by combining this tag with cookies that have rolling expiration dates, FB can also keep track of users' browsing activities in the future as well. Our experimental results suggest that 23% of the 10k most popular websites have adopted this technology, and can contribute to this activity tracking on the web. Furthermore, our longitudinal study shows that this type of user activity tracking can go as far back as 2015. Simply said, if a user creates for the first time a FB account today, FB could, under some conditions, match their anonymously collected past web browsing activity to their newly created FB profile, from as far back as 2015 and continue tracking their activity in the future. | Paschalis Bekos, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis | University of Crete/FORTH, Greece; FORTH, Greece; Telefonica Research, Spain |
| 73 |  |  [Atrapos: Real-time Evaluation of Metapath Query Workloads](https://doi.org/10.1145/3543507.3583322) |  | 0 | Heterogeneous information networks (HINs) represent different types of entities and relationships between them. Exploring and mining HINs relies on metapath queries that identify pairs of entities connected by relationships of diverse semantics. While the real-time evaluation of metapath query workloads on large, web-scale HINs is highly demanding in computational cost, current approaches do not exploit interrelationships among the queries. In this paper, we present Atrapos, a new approach for the real-time evaluation of metapath query workloads that leverages a combination of efficient sparse matrix multiplication and intermediate result caching. Atrapos selects intermediate results to cache and reuse by detecting frequent sub-metapaths among workload queries in real time, using a tailor-made data structure, the Overlap Tree, and an associated caching policy. Our experimental study on real data shows that Atrapos accelerates exploratory data analysis and mining on HINs, outperforming off-the-shelf caching approaches and state-of-the-art research prototypes in all examined scenarios. | Serafeim Chatzopoulos, Thanasis Vergoulis, Dimitrios Skoutas, Theodore Dalamagas, Christos Tryfonopoulos, Panagiotis Karras | University of the Peloponnese, Greece; Aarhus University, Denmark; University of the Peloponnese, Greece and IMSI, Athena RC, Greece; IMSI, Athena RC, Greece |
| 74 |  |  [TRAVERS: A Diversity-Based Dynamic Approach to Iterative Relevance Search over Knowledge Graphs](https://doi.org/10.1145/3543507.3583429) |  | 0 | Relevance search over knowledge graphs seeks top-ranked answer entities that are most relevant to a query entity. Since the semantics of relevance varies with the user need and its formalization is difficult for non-experts, existing methods infer semantics from user-provided example answer entities. However, a user may provide very few examples, even none at the beginning of interaction, thereby limiting the effectiveness of such methods. In this paper, we vision a more practical scenario called labeling-based iterative relevance search: instead of effortfully inputting example answer entities, the user effortlessly (e.g., implicitly) labels current answer entities, and is rewarded with improved answer entities in the next iteration. To realize the scenario, our approach TRAVERS incorporates two rankers: a diversity-oriented ranker for supporting cold start and avoiding converging to sub-optimum caused by noisy labels, and a relevance-oriented ranker capable of handling unbalanced labels. Moreover, the two rankers and their combination dynamically evolve over iterations. TRAVERS outperformed a variety of baselines in experiments with simulated and real user behavior. | Ziyang Li, Yu Gu, Yulin Shen, Wei Hu, Gong Cheng | State Key Laboratory for Novel Software Technology, Nanjing University, China; Ohio State University, USA |
| 75 |  |  [Message Function Search for Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583546) |  | 0 | Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs. | Shimin Di, Lei Chen | The Hong Kong University of Science and Technology (Guangzhou), China; The Hong Kong University of Secience and Technology, China |
| 76 |  |  [FINGER: Fast Inference for Graph-based Approximate Nearest Neighbor Search](https://doi.org/10.1145/3543507.3583318) |  | 0 | Approximate K-Nearest Neighbor Search (AKNNS) has now become ubiquitous in modern applications, for example, as a fast search procedure with two tower deep learning models. Graph-based methods for AKNNS in particular have received great attention due to their superior performance. These methods rely on greedy graph search to traverse the data points as embedding vectors in a database. Under this greedy search scheme, we make a key observation: many distance computations do not influence search updates so these computations can be approximated without hurting performance. As a result, we propose FINGER, a fast inference method to achieve efficient graph search. FINGER approximates the distance function by estimating angles between neighboring residual vectors with low-rank bases and distribution matching. The approximated distance can be used to bypass unnecessary computations, which leads to faster searches. Empirically, accelerating a popular graph-based method named HNSW by FINGER is shown to outperform existing graph-based methods by 20%-60% across different benchmark datasets. | Patrick H. Chen, WeiCheng Chang, JyunYu Jiang, HsiangFu Yu, Inderjit S. Dhillon, ChoJui Hsieh |  |
| 77 |  |  [Match4Match: Enhancing Text-Video Retrieval by Maximum Flow with Minimum Cost](https://doi.org/10.1145/3543507.3583365) |  | 0 | With the explosive growth of video and text data on the web, text-video retrieval has become a vital task for online video platforms. Recently, text-video retrieval methods based on pre-trained models have attracted a lot of attention. However, existing methods cannot effectively capture the fine-grained information in videos, and typically suffer from the hubness problem where a collection of similar videos are retrieved by a large number of different queries. In this paper, we propose Match4Match, a new text-video retrieval method based on CLIP (Contrastive Language-Image Pretraining) and graph optimization theories. To balance calculation efficiency and model accuracy, Match4Match seamlessly supports three inference modes for different application scenarios. In fast vector retrieval mode, we embed texts and videos in the same space and employ a vector retrieval engine to obtain the top K videos. In fine-grained alignment mode, our method fully utilizes the pre-trained knowledge of the CLIP model to align words with corresponding video frames, and uses the fine-grained information to compute text-video similarity more accurately. In flow-style matching mode, to alleviate the detrimental impact of the hubness problem, we model the retrieval problem as a combinatorial optimization problem and solve it using maximum flow with minimum cost algorithm. To demonstrate the effectiveness of our method, we conduct experiments on five public text-video datasets. The overall performance of our proposed method outperforms state-of-the-art methods. Additionally, we evaluate the computational efficiency of Match4Match. Benefiting from the three flexible inference modes, Match4Match can respond to a large number of query requests with low latency or achieve high recall with acceptable time consumption. | Zhongjie Duan, Chengyu Wang, Cen Chen, Wenmeng Zhou, Jun Huang, Weining Qian | East China Normal University, China; Alibaba Group, China |
| 78 |  |  [Zero-shot Clarifying Question Generation for Conversational Search](https://doi.org/10.1145/3543507.3583420) |  | 0 | A long-standing challenge for search and conversational assistants is query intention detection in ambiguous queries. Asking clarifying questions in conversational search has been widely studied and considered an effective solution to resolve query ambiguity. Existing work have explored various approaches for clarifying question ranking and generation. However, due to the lack of real conversational search data, they have to use artificial datasets for training, which limits their generalizability to real-world search scenarios. As a result, the industry has shown reluctance to implement them in reality, further suspending the availability of real conversational search interaction data. The above dilemma can be formulated as a cold start problem of clarifying question generation and conversational search in general. Furthermore, even if we do have large-scale conversational logs, it is not realistic to gather training data that can comprehensively cover all possible queries and topics in open-domain search scenarios. The risk of fitting bias when training a clarifying question retrieval/generation model on incomprehensive dataset is thus another important challenge. In this work, we innovatively explore generating clarifying questions in a zero-shot setting to overcome the cold start problem and we propose a constrained clarifying question generation system which uses both question templates and query facets to guide the effective and precise question generation. The experiment results show that our method outperforms existing state-of-the-art zero-shot baselines by a large margin. Human annotations to our model outputs also indicate our method generates 25.2\% more natural questions, 18.1\% more useful questions, 6.1\% less unnatural and 4\% less useless questions. | Zhenduo Wang, Yuancheng Tu, Corby Rosset, Nick Craswell, Ming Wu, Qingyao Ai | GitHub Inc, USA; University of Utah, USA; Tsinghua University, China; Microsoft Corp, USA |
| 79 |  |  [Everything Evolves in Personalized PageRank](https://doi.org/10.1145/3543507.3583474) |  | 0 | Personalized PageRank, as a graphical model, has been proven as an effective solution in many applications such as web page search, recommendation, etc. However, in the real world, the setting of personalized PageRank is usually dynamic like the evolving World Wide Web. On the one hand, the outdated PageRank solution can be sub-optimal for ignoring the evolution pattern. On the other hand, solving the solution from the scratch at each timestamp causes costly computation complexity. Hence, in this paper, we aim to solve the Personalized PageRank effectively and efficiently in a fully dynamic setting, i.e., every component in the Personalized PageRank formula is dependent on time. To this end, we propose the EvePPR method that can track the exact personalized PageRank solution at each timestamp in the fully dynamic setting, and we theoretically and empirically prove the accuracy and time complexity of EvePPR. Moreover, we apply EvePPR to solve the dynamic knowledge graph alignment task, where a fully dynamic setting is necessary but complex. The experiments show that EvePPR outperforms the state-of-the-art baselines for similar nodes retrieval across graphs. | Zihao Li, Dongqi Fu, Jingrui He | University of Illinois at Urbana-Champaign, USA |
| 80 |  |  [Incorporating Explicit Subtopics in Personalized Search](https://doi.org/10.1145/3543507.3583488) |  | 0 | The key to personalized search is modeling user intents to tailor returned results for different users. Existing personalized methods mainly focus on learning implicit user interest vectors. In this paper, we propose ExpliPS, a personalized search model that explicitly incorporates query subtopics into personalization. It models the user’s current intent by estimating the user’s preference over the subtopics of the current query and personalizes the results over the weighted subtopics. We think that in such a way, personalized search could be more explainable and stable. Specifically, we first employ a semantic encoder to learn the representations of the user’s historical behaviours. Then with the historical behaviour representations, a subtopic preference encoder is devised to predict the user’s subtopic preferences on the current query. Finally, we rerank the candidates via a subtopic-aware ranker that prioritizes the documents relevant to the user-preferred subtopics. Experimental results show our model ExpliPS outperforms the state-of-the-art personalized web search models with explainable and stable results. | Shuting Wang, Zhicheng Dou, Jing Yao, Yujia Zhou, JiRong Wen | Social Computing Group, Microsoft Research Asia, China; Renmin University of China, China and Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Educationf Education, China; Renmin University of China, China |
| 81 |  |  [Optimizing Feature Set for Click-Through Rate Prediction](https://doi.org/10.1145/3543507.3583545) |  | 0 | Click-through prediction (CTR) models transform features into latent vectors and enumerate possible feature interactions to improve performance based on the input feature set. Therefore, when selecting an optimal feature set, we should consider the influence of both feature and its interaction. However, most previous works focus on either feature field selection or only select feature interaction based on the fixed feature set to produce the feature set. The former restricts search space to the feature field, which is too coarse to determine subtle features. They also do not filter useless feature interactions, leading to higher computation costs and degraded model performance. The latter identifies useful feature interaction from all available features, resulting in many redundant features in the feature set. In this paper, we propose a novel method named OptFS to address these problems. To unify the selection of feature and its interaction, we decompose the selection of each feature interaction into the selection of two correlated features. Such a decomposition makes the model end-to-end trainable given various feature interaction operations. By adopting feature-level search space, we set a learnable gate to determine whether each feature should be within the feature set. Because of the large-scale search space, we develop a learning-by-continuation training scheme to learn such gates. Hence, OptFS generates the feature set only containing features which improve the final prediction results. Experimentally, we evaluate OptFS on three public datasets, demonstrating OptFS can optimize feature sets which enhance the model performance and further reduce both the storage and computational cost. | Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, Xue Liu | Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), China; FiT, Tencent, China; McGill University, Canada |
| 82 |  |  [Filtered-DiskANN: Graph Algorithms for Approximate Nearest Neighbor Search with Filters](https://doi.org/10.1145/3543507.3583552) |  | 0 | As Approximate Nearest Neighbor Search (ANNS)-based dense retrieval becomes ubiquitous for search and recommendation scenarios, efficiently answering filtered ANNS queries has become a critical requirement. Filtered ANNS queries ask for the nearest neighbors of a query’s embedding from the points in the index that match the query’s labels such as date, price range, language. There has been little prior work on algorithms that use label metadata associated with vector data to build efficient indices for filtered ANNS queries. Consequently, current indices have high search latency or low recall which is not practical in interactive web-scenarios. We present two algorithms with native support for faster and more accurate filtered ANNS queries: one with streaming support, and another based on batch construction. Central to our algorithms is the construction of a graph-structured index which forms connections not only based on the geometry of the vector data, but also the associated label set. On real-world data with natural labels, both algorithms are an order of magnitude or more efficient for filtered queries than the current state of the art algorithms. The generated indices also be queried from an SSD and support thousands of queries per second at over [email protected] | Siddharth Gollapudi, Neel Karia, Varun Sivashankar, Ravishankar Krishnaswamy, Nikit Begwani, Swapnil Raz, Yiyong Lin, Yin Zhang, Neelam Mahapatro, Premkumar Srinivasan, Amit Singh, Harsha Vardhan Simhadri | Microsoft Research, USA; Columbia University, USA; Microsoft, India; Microsoft Research, India; Microsoft, USA |
| 83 |  |  [P-MMF: Provider Max-min Fairness Re-ranking in Recommender System](https://doi.org/10.1145/3543507.3583296) |  | 0 | In this paper, we address the issue of recommending fairly from the aspect of providers, which has become increasingly essential in multistakeholder recommender systems. Existing studies on provider fairness usually focused on designing proportion fairness (PF) metrics that first consider systematic fairness. However, sociological researches show that to make the market more stable, max-min fairness (MMF) is a better metric. The main reason is that MMF aims to improve the utility of the worst ones preferentially, guiding the system to support the providers in weak market positions. When applying MMF to recommender systems, how to balance user preferences and provider fairness in an online recommendation scenario is still a challenging problem. In this paper, we proposed an online re-ranking model named Provider Max-min Fairness Re-ranking (P-MMF) to tackle the problem. Specifically, P-MMF formulates provider fair recommendation as a resource allocation problem, where the exposure slots are considered the resources to be allocated to providers and the max-min fairness is used as the regularizer during the process. We show that the problem can be further represented as a regularized online optimizing problem and solved efficiently in its dual space. During the online re-ranking phase, a momentum gradient descent method is designed to conduct the dynamic re-ranking. Theoretical analysis showed that the regret of P-MMF can be bounded. Experimental results on four public recommender datasets demonstrated that P-MMF can outperformed the state-of-the-art baselines. Experimental results also show that P-MMF can retain small computationally costs on a corpus with the large number of items. | Chen Xu, Sirui Chen, Jun Xu, Weiran Shen, Xiao Zhang, Gang Wang, Zhenhua Dong |  |
| 84 |  |  [Dual Intent Enhanced Graph Neural Network for Session-based New Item Recommendation](https://doi.org/10.1145/3543507.3583526) |  | 0 | Recommender systems are essential to various fields, e.g., e-commerce, e-learning, and streaming media. At present, graph neural networks (GNNs) for session-based recommendations normally can only recommend items existing in users' historical sessions. As a result, these GNNs have difficulty recommending items that users have never interacted with (new items), which leads to a phenomenon of information cocoon. Therefore, it is necessary to recommend new items to users. As there is no interaction between new items and users, we cannot include new items when building session graphs for GNN session-based recommender systems. Thus, it is challenging to recommend new items for users when using GNN-based methods. We regard this challenge as '\textbf{G}NN \textbf{S}ession-based \textbf{N}ew \textbf{I}tem \textbf{R}ecommendation (GSNIR)'. To solve this problem, we propose a dual-intent enhanced graph neural network for it. Due to the fact that new items are not tied to historical sessions, the users' intent is difficult to predict. We design a dual-intent network to learn user intent from an attention mechanism and the distribution of historical data respectively, which can simulate users' decision-making process in interacting with a new item. To solve the challenge that new items cannot be learned by GNNs, inspired by zero-shot learning (ZSL), we infer the new item representation in GNN space by using their attributes. By outputting new item probabilities, which contain recommendation scores of the corresponding items, the new items with higher scores are recommended to users. Experiments on two representative real-world datasets show the superiority of our proposed method. The case study from the real-world verifies interpretability benefits brought by the dual-intent module and the new item reasoning module. The code is available at Github: https://github.com/Ee1s/NirGNN | Di Jin, Luzhi Wang, Yizhen Zheng, Guojie Song, Fei Jiang, Xiang Li, Wei Lin, Shirui Pan | Meituan, China; School of Information and Communication Technology, Griffith University, Australia; Professional, China; School of Intelligence Science and Technology, Peking University, China; College of Intelligence and Computing, Tianjin University, China; Department of Data Science and AI, Faculty of IT, Monash University, Australia |
| 85 |  |  [Cross-domain recommendation via user interest alignment](https://doi.org/10.1145/3543507.3583263) |  | 0 | Cross-domain recommendation aims to leverage knowledge from multiple domains to alleviate the data sparsity and cold-start problems in traditional recommender systems. One popular paradigm is to employ overlapping user representations to establish domain connections, thereby improving recommendation performance in all scenarios. Nevertheless, the general practice of this approach is to train user embeddings in each domain separately and then aggregate them in a plain manner, often ignoring potential cross-domain similarities between users and items. Furthermore, considering that their training objective is recommendation task-oriented without specific regularizations, the optimized embeddings disregard the interest alignment among user's views, and even violate the user's original interest distribution. To address these challenges, we propose a novel cross-domain recommendation framework, namely COAST, to improve recommendation performance on dual domains by perceiving the cross-domain similarity between entities and aligning user interests. Specifically, we first construct a unified cross-domain heterogeneous graph and redefine the message passing mechanism of graph convolutional networks to capture high-order similarity of users and items across domains. Targeted at user interest alignment, we develop deep insights from two more fine-grained perspectives of user-user and user-item interest invariance across domains by virtue of affluent unsupervised and semantic signals. We conduct intensive experiments on multiple tasks, constructed from two large recommendation data sets. Extensive results show COAST consistently and significantly outperforms state-of-the-art cross-domain recommendation algorithms as well as classic single-domain recommendation methods. | Chuang Zhao, Hongke Zhao, Ming HE, Jian Zhang, Jianping Fan | School of Cyberspace Security, Hangzhou Dianzi University, China; College of Management and Economics, Tianjin University, China; College of Management and Economics, Tianjin University, China and AI Lab at Lenovo Research, China; AI Lab at Lenovo Research, China |
| 86 |  |  [A Concept Knowledge Graph for User Next Intent Prediction at Alipay](https://doi.org/10.1145/3543873.3587308) |  | 0 | This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability. | Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, Yixin Ou, Ningyu Zhang | Ant Group, China; Zhejiang University, China |
| 87 |  |  [A Semantic Partitioning Method for Large-Scale Training of Knowledge Graph Embeddings](https://doi.org/10.1145/3543873.3587537) |  | 0 | In recent years, knowledge graph embeddings have achieved great success. Many methods have been proposed and achieved state-of-the-art results in various tasks. However, most of the current methods present one or more of the following problems: (i) They only consider fact triplets, while ignoring the ontology information of knowledge graphs. (ii) The obtained embeddings do not contain much semantic information. Therefore, using these embeddings for semantic tasks is problematic. (iii) They do not enable large-scale training. In this paper, we propose a new algorithm that incorporates the ontology of knowledge graphs and partitions the knowledge graph based on classes to include more semantic information for parallel training of large-scale knowledge graph embeddings. Our preliminary results show that our algorithm performs well on several popular benchmarks. | Yuhe Bai | Sorbonne University, France |
| 88 |  |  [Intent-Aware Propensity Estimation via Click Pattern Stratification](https://doi.org/10.1145/3543873.3587610) |  | 0 | Counterfactual learning to rank via inverse propensity weighting is the most popular approach to train ranking models using biased implicit user feedback from logged search data. Standard click propensity estimation techniques rely on simple models of user browsing behavior that primarily account for the attributes of the presentation context that affect whether the relevance of an item to the search context is observed. Most notably, the inherent effect of the listwise presentation of the items on users’ propensity for engagement is captured in the position of the presented items on the search result page. In this work, we enrich this position bias based click propensity model by proposing an observation model that further incorporates the underlying search intent, as reflected in the user’s click pattern in the search context. Our approach does not require an intent prediction model based on the content of the search context. Instead, we rely on a simple, yet effective, non-causal estimate of the user’s browsing intent from the number of click events in the search context. We empirically characterize the distinct rank decay patterns of the estimated click propensities in the characterized intent classes. In particular, we demonstrate a sharper decay of click propensities in top ranks for the intent class identified by sparse user clicks and the higher likelihood of observing clicks in lower ranks for the intent class identified by higher number of user clicks. We show that the proposed intent-aware propensity estimation technique helps with training ranking models with more effective personalization and generalization power through empirical results for a ranking task in a major e-commerce platform. | Ehsan Ebrahimzadeh, Alex Cozzi, Abraham Bagherjeiran | Search Ranking and Monetization, eBay, USA |
| 89 |  |  [Disentangling Degree-related Biases and Interest for Out-of-Distribution Generalized Directed Network Embedding](https://doi.org/10.1145/3543507.3583271) |  | 0 | The goal of directed network embedding is to represent the nodes in a given directed network as embeddings that preserve the asymmetric relationships between nodes. While a number of directed network embedding methods have been proposed, we empirically show that the existing methods lack out-of-distribution generalization abilities against degree-related distributional shifts. To mitigate this problem, we propose ODIN (Out-of-Distribution Generalized Directed Network Embedding), a new directed NE method where we model multiple factors in the formation of directed edges. Then, for each node, ODIN learns multiple embeddings, each of which preserves its corresponding factor, by disentangling interest factors and biases related to in- and out-degrees of nodes. Our experiments on four real-world directed networks demonstrate that disentangling multiple factors enables ODIN to yield out-of-distribution generalized embeddings that are consistently effective under various degrees of shifts in degree distributions. Specifically, ODIN universally outperforms 9 state-of-the-art competitors in 2 LP tasks on 4 real-world datasets under both identical distribution (ID) and non-ID settings. The code is available at https://github.com/hsyoo32/odin. | Hyunsik Yoo, YeonChang Lee, Kijung Shin, SangWook Kim | Georgia Institute of Technology, USA; Hanyang University, Republic of Korea; Korea Advanced Institute of Science and Technology, Republic of Korea |
| 90 |  |  [Fine-tuning Partition-aware Item Similarities for Efficient and Scalable Recommendation](https://doi.org/10.1145/3543507.3583240) |  | 0 | Collaborative filtering (CF) is widely searched in recommendation with various types of solutions. Recent success of Graph Convolution Networks (GCN) in CF demonstrates the effectiveness of modeling high-order relationships through graphs, while repetitive graph convolution and iterative batch optimization limit their efficiency. Instead, item similarity models attempt to construct direct relationships through efficient interaction encoding. Despite their great performance, the growing item numbers result in quadratic growth in similarity modeling process, posing critical scalability problems. In this paper, we investigate the graph sampling strategy adopted in latest GCN model for efficiency improving, and identify the potential item group structure in the sampled graph. Based on this, we propose a novel item similarity model which introduces graph partitioning to restrict the item similarity modeling within each partition. Specifically, we show that the spectral information of the original graph is well in preserving global-level information. Then, it is added to fine-tune local item similarities with a new data augmentation strategy acted as partition-aware prior knowledge, jointly to cope with the information loss brought by partitioning. Experiments carried out on 4 datasets show that the proposed model outperforms state-of-the-art GCN models with 10x speed-up and item similarity models with 95\% parameter storage savings. | Tianjun Wei, Jianghong Ma, Tommy W. S. Chow | Harbin Institute of Technology, China; City University of Hong Kong, Hong Kong |
| 91 |  |  [Multi-Behavior Recommendation with Cascading Graph Convolution Networks](https://doi.org/10.1145/3543507.3583439) |  | 0 | Multi-behavior recommendation, which exploits auxiliary behaviors (e.g., click and cart) to help predict users' potential interactions on the target behavior (e.g., buy), is regarded as an effective way to alleviate the data sparsity or cold-start issues in recommendation. Multi-behaviors are often taken in certain orders in real-world applications (e.g., click>cart>buy). In a behavior chain, a latter behavior usually exhibits a stronger signal of user preference than the former one does. Most existing multi-behavior models fail to capture such dependencies in a behavior chain for embedding learning. In this work, we propose a novel multi-behavior recommendation model with cascading graph convolution networks (named MB-CGCN). In MB-CGCN, the embeddings learned from one behavior are used as the input features for the next behavior's embedding learning after a feature transformation operation. In this way, our model explicitly utilizes the behavior dependencies in embedding learning. Experiments on two benchmark datasets demonstrate the effectiveness of our model on exploiting multi-behavior data. It outperforms the best baseline by 33.7% and 35.9% on average over the two datasets in terms of Recall@10 and NDCG@10, respectively. | Zhiyong Cheng, Sai Han, Fan Liu, Lei Zhu, Zan Gao, Yuxin Peng | Wangxuan Institute of Computer Technology, Peking University, China and Peng Cheng Laboratory, China; School of Information Science and Engineering, Shandong Normal University, China; Shandong Artificial Intelligence Institute, Qilu University of Technology (Shandong Academy of Sciences), China; School of Computing, National University of Singapore, Singapore |
| 92 |  |  [Cross-domain Recommendation with Behavioral Importance Perception](https://doi.org/10.1145/3543507.3583494) |  | 0 | Cross-domain recommendation (CDR) aims to leverage the source domain information to provide better recommendation for the target domain, which is widely adopted in recommender systems to alleviate the data sparsity and cold-start problems. However, existing CDR methods mostly focus on designing effective model architectures to transfer the source domain knowledge, ignoring the behavior-level effect during the loss optimization process, where behaviors regarding different aspects in the source domain may have different importance for the CDR model optimization. The ignorance of the behavior-level effect will cause the carefully designed model architectures ending up with sub-optimal parameters, which limits the recommendation performance. To tackle the problem, we propose a generic behavioral importance-aware optimization framework for cross-domain recommendation (BIAO). Specifically, we propose a behavioral perceptron which predicts the importance of each source behavior according to the corresponding item’s global impact and local user-specific impact. The joint optimization process of the CDR model and the behavioral perceptron is formulated as a bi-level optimization problem. In the lower optimization, only the CDR model is updated with weighted source behavior loss and the target domain loss, while in the upper optimization, the behavioral perceptron is updated with implicit gradient from a developing dataset obtained through the proposed reorder-and-reuse strategy. Extensive experiments show that our proposed optimization framework consistently improves the performance of different cross-domain recommendation models in 7 cross-domain scenarios, demonstrating that our method can serve as a generic and powerful tool for cross-domain recommendation1. | Hong Chen, Xin Wang, Ruobing Xie, Yuwei Zhou, Wenwu Zhu | WeChat Search Application Department, Tencent, China; Department of Computer Science and Technology, Tsinghua University, China |
| 93 |  |  [Multi-Lingual Multi-Partite Product Title Matching](https://doi.org/10.1145/3543873.3587322) |  | 0 | In a globalized marketplace, one could access products or services from almost anywhere. However, resolving which product in one language corresponds to another product in a different language remains an under-explored problem. We explore this from two perspectives. First, given two products of different languages, how to assess their similarity that could signal a potential match. Second, given products from various languages, how to arrive at a multi-partite clustering that respects cardinality constraints efficiently. We describe algorithms for each perspective and integrate them into a promising solution validated on real-world datasets. | HuanLin Tay, WeiJie Tay, Hady W. Lauw | Singapore Management University, Singapore |
| 94 |  |  [Multi-interest Recommendation on Shopping for Others](https://doi.org/10.1145/3543873.3587341) |  | 0 | Existing recommendation methods based on multi-interest frameworks effectively model users from multiple aspects to represent complex user interests. However, more research still needs to be done on the behavior of users shopping for others. We propose a Multi-Demander Recommendation (MDR) model to learn different people’s interests from a sequence of actions. We first decouple the feature embeddings of items to learn the static preferences of different demanders. Next, a weighted directed global graph is constructed to model the associations among item categories. We partition short sequences by time intervals and look up category embeddings from the graph to capture dynamic intents. Finally, preferences and intentions are combined with learning the interests of different demanders. The conducted experiments demonstrate that our model improves the accuracy of recommendations. | Shuang Li, Yaokun Liu, Xiaowang Zhang, Yuexian Hou, Zhiyong Feng | Tianjin University, China; Tianjin University, China, China |
| 95 |  |  [Explicit and Implicit Semantic Ranking Framework](https://doi.org/10.1145/3543873.3584621) |  | 0 | The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and has enabled 38.7% time reduction in composing messages in telemetry recorded since its general release in January 2021. In the ACI task, sRank selects relevant historical physician templates that serve as guidance for a text summarization model to generate higher quality medical notes. It achieves 35.5% top-one accuracy gain, along with 46% relative ROUGE-L gain in generated medical notes. | Xiaofeng Zhu, Thomas Lin, Vishal Anand, Matthew Calderwood, Eric ClausenBrown, Gord Lueck, Wenwai Yim, Cheng Wu | Microsoft Corporation, USA; Nuance Communications, USA |
| 96 |  |  [MPKGAC: Multimodal Product Attribute Completion in E-commerce](https://doi.org/10.1145/3543873.3584623) |  | 0 | Product attributes can display the selling points of products, helping users find their desired products in search results. However, product attributes are typically incomplete. In e-commerce, products have multimodal features, including original attributes, images, and texts. How to make full use of the multimodal data to complete the missing attributes is the key challenge. To this end, we propose MPKGAC, a powerful three-stream framework that handles multimodal product data for attribute completion. We build a multimodal product knowledge graph (KG) from the multimodal features, and then convert the attribute completion problem into a multimodal KG completion task. MPKGAC encodes each modality separately, fuses them adaptively, and integrates multimodal decoders for prediction. Experiments show that MPKGAC outperforms the best baseline by 6.2% in [email protected] MPKGAC is employed to enrich selling points of the women’s clothing industry at Alibaba.com.cn and improves the click-through rate (CTR) by a relative 2.14%. | Kai Wang, Jianzhi Shao, Tao Zhang, Qijin Chen, Chengfu Huo | Alibaba Group, China |
| 97 |  |  [Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching](https://doi.org/10.1145/3543873.3584626) |  | 0 | We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users. | Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, Yingchun Yang | Zhejiang University, China; Zhejiang University, China and NetEase Cloud Music, NetEase Inc., China; NetEase Cloud Music, NetEase Inc., China |
| 98 |  |  [Reinforcing User Retention in a Billion Scale Short Video Recommender System](https://doi.org/10.1145/3543873.3584640) |  | 0 | Recently, short video platforms have achieved rapid user growth by recommending interesting content to users. The objective of the recommendation is to optimize user retention, thereby driving the growth of DAU (Daily Active Users). Retention is a long-term feedback after multiple interactions of users and the system, and it is hard to decompose retention reward to each item or a list of items. Thus traditional point-wise and list-wise models are not able to optimize retention. In this paper, we choose reinforcement learning methods to optimize the retention as they are designed to maximize the long-term performance. We formulate the problem as an infinite-horizon request-based Markov Decision Process, and our objective is to minimize the accumulated time interval of multiple sessions, which is equal to improving the app open frequency and user retention. However, current reinforcement learning algorithms can not be directly applied in this setting due to uncertainty, bias, and long delay time incurred by the properties of user retention. We propose a novel method, dubbed RLUR, to address the aforementioned challenges. Both offline and live experiments show that RLUR can significantly improve user retention. RLUR has been fully launched in Kuaishou app for a long time, and achieves consistent performance improvement on user retention and DAU. | Qingpeng Cai, Shuchang Liu, Xueliang Wang, Tianyou Zuo, Wentao Xie, Bin Yang, Dong Zheng, Peng Jiang, Kun Gai | Kuaishou Technology, China; Unaffiliated, China |
| 99 |  |  [Jointly modeling products and resource pages for task-oriented recommendation](https://doi.org/10.1145/3543873.3584642) |  | 0 | Modeling high-level user intent in recommender systems can improve performance, although it is often difficult to obtain a ground truth measure of this intent. In this paper, we investigate a novel way to obtain such an intent signal by leveraging resource pages associated with a particular task. We jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users. Our experiments consider the domain of home improvement product recommendation, where resource pages are DIY (do-it-yourself) project pages from Lowes.com. Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers. We then extend a state-of-the-art system to incorporate this new intent data, and show a significant improvement in the ability of the system to recommend products. We further demonstrate that our system can be used to successfully recommend DIY project pages to users. We have taken initial steps towards deploying our method for project recommendation in production on the Lowe’s website and for recommendations through marketing emails. | Brendan Duncan, Surya Kallumadi, Taylor BergKirkpatrick, Julian J. McAuley | UC San Diego, USA; Lowe's Companies, Inc., USA |
| 100 |  |  [Meta-Generator Enhanced Multi-Domain Recommendation](https://doi.org/10.1145/3543873.3584652) |  | 0 | Large-scale e-commercial platforms usually contain multiple business fields, which require industrial algorithms to characterize user intents across multiple domains. Numerous efforts have been made in user multi-domain intent modeling to achieve state-of-the-art performance. However, existing methods mainly focus on the domains having rich user information, which makes implementation to domains with sparse or rare user behavior meet with mixed success. Hence, in this paper, we propose a novel method named Meta-generator enhanced multi-Domain model (MetaDomain) to address the above issue. MetaDomain mainly includes two steps, 1) users’ multi-domain intent representation and 2) users’ multi-domain intent fusion. Specifically, in users’ multi-domain intent representation, we use the gradient information from a domain intent extractor to train the domain intent meta-generator, where the domain intent extractor has the input of users’ sequence feature and domain meta-generator has the input of users’ basic feature, hence the capability of generating users’ intent with sparse behavior. Afterward, in users’ multi-domain intent fusion, a domain graph is used to represent the high-order multi-domain connectivity. Extensive experiments have been carried out under a real-world industrial platform named Meituan. Both offline and rigorous online A/B tests under the billion-level data scale demonstrate the superiority of the proposed MetaDomain method over the state-of-the-art baselines. Furthermore comparing with the method using multi-domain sequence features, MetaDomain can reduce the serving latency by 20%. Currently, MetaDomain has been deployed in Meituan one of the largest worldwide Online-to-Offline(O2O) platforms. | Yingyi Zhang, Xianneng Li, Yahe Yu, Jian Tang, Huanfang Deng, Junya Lu, Yeyin Zhang, Qiancheng Jiang, Yunsen Xian, Liqian Yu, Han Liu | Meituan, China; Meituan-Dianping Group, China; Dalian University of Technology, China |
| 101 |  |  [Integrated Ranking for News Feed with Reinforcement Learning](https://doi.org/10.1145/3543873.3584651) |  | 0 | With the development of recommender systems, it becomes an increasingly common need to mix multiple item sequences from different sources. Therefore, the integrated ranking stage is proposed to be responsible for this task with re-ranking models. However, existing methods ignore the relation between the sequences, thus resulting in local optimum over the interaction session. To resolve this challenge, in this paper, we propose a new model named NFIRank (News Feed Integrated Ranking with reinforcement learning) and formulate the whole interaction session as a MDP (Markov Decision Process). Sufficient offline experiments are provided to verify the effectiveness of our model. In addition, we deployed our model on Huawei Browser and gained 1.58% improvements in CTR compared with the baseline in online A/B test. Code will be available at https://gitee.com/mindspore/models/tree/master/research/recommend/NFIRank. | Menghui Zhu, Wei Xia, Weiwen Liu, Yifan Liu, Ruiming Tang, Weinan Zhang | Huawei Noah?s Ark Lab, China; Shanghai Jiao Tong University, China |
| 102 |  |  [Measuring e-Commerce Metric Changes in Online Experiments](https://doi.org/10.1145/3543873.3584654) |  | 0 | Digital technology organizations routinely use online experiments (e.g. A/B tests) to guide their product and business decisions. In e-commerce, we often measure changes to transaction- or item-based business metrics such as Average Basket Value (ABV), Average Basket Size (ABS), and Average Selling Price (ASP); yet it remains a common pitfall to ignore the dependency between the value/size of transactions/items during experiment design and analysis. We present empirical evidence on such dependency, its impact on measurement uncertainty, and practical implications on A/B test outcomes if left unmitigated. By making the evidence available, we hope to drive awareness of the pitfall among experimenters in e-commerce and hence encourage the adoption of established mitigation approaches. We also share lessons learned when incorporating selected mitigation approaches into our experimentation analysis platform currently in production. | C. H. Bryan Liu, Emma J. McCoy | ASOS.com, United Kingdom and Imperial College London, United Kingdom; London School of Economics and Political Science, United Kingdom |
| 103 |  |  [Improve retrieval of regional titles in streaming services with dense retrieval](https://doi.org/10.1145/3543873.3587619) |  | 0 | Customers search for movie and series titles released across the world on streaming services like primevideo.com (PV), netflix.com (Netflix). In non-English speaking countries like India, Nepal and many others, the regional titles are transliterated from native language to English and are being searched in English. Given that there can be multiple transliterations possible for almost all the titles, searching for a regional title can be a very frustrating customer experience if these nuances are not handled correctly by the search system. Typing errors make the problem even more challenging. Streaming services uses spell correction and auto-suggestions/auto-complete features to address this issue up to certain extent. Auto-suggest fails when user searches keywords not in scope of the auto-suggest. Spell correction is effective at correcting common typing errors but as these titles doesn’t follow strict grammar rules and new titles constantly added to the catalog, spell correction have limited success. With recent progress in deep learning (DL), embedding vectors based dense retrieval is being used extensively to retrieve semantically relevant documents for a given query. In this work, we have used dense retrieval to address the noise introduced by transliteration variations and typing errors to improve retrieval of regional media titles. In the absent of any relevant dataset to test our hypothesis, we created a new dataset of 40K query title pairs from PV search logs. We also created a baseline by bench-marking PV’s performance on test data. We present an extensive study on the impact of 1. pre-training, 2. data augmentation, 3. positive to negative sample ratio, and 4. choice of loss function on retrieval performance. Our best model has shown 51.24% improvement in [email protected] over PV baseline. | Bhargav Upadhyay, Tejas Khairnar, Anup Kotalwar | Amazon, India |
| 104 |  |  [hp-frac: An index to determine Awarded Researchers](https://doi.org/10.1145/3543873.3587597) |  | 0 | In order to advance academic research, it is important to assess and evaluate the academic influence of researchers and the findings they produce. Citation metrics are universally used methods to evaluate researchers. Amongst the several variations of citation metrics, the h-index proposed by Hirsch has become the leading measure. Recent work shows that h-index is not an effective measure to determine scientific impact - due to changing authorship patterns. This can be mitigated by using h-index of a paper to compute h-index of an author. We show that using fractional allocation of h-index gives better results. In this work, we reapply two indices based on the h-index of a single paper. The indices are referred to as: hp-index and hp-frac-index. We run large-scale experiments in three different fields with about a million publications and 3,000 authors. Our experiments show that hp-frac-index provides a unique ranking when compared to h-index. It also performs better than h-index in providing higher ranks to the awarded researcher. | Aashay Singhal, Kamalakar Karlapalem | International Institute of Information Technology, Hyderabad, India |
| 105 |  |  [Application of an ontology for model cards to generate computable artifacts for linking machine learning information from biomedical research](https://doi.org/10.1145/3543873.3587601) |  | 0 | Model card reports provide a transparent description of machine learning models which includes information about their evaluation, limitations, intended use, etc. Federal health agencies have expressed an interest in model cards report for research studies using machine-learning based AI. Previously, we have developed an ontology model for model card reports to structure and formalize these reports. In this paper, we demonstrate a Java-based library (OWL API, FaCT++) that leverages our ontology to publish computable model card reports. We discuss future directions and other use cases that highlight applicability and feasibility of ontology-driven systems to support FAIR challenges. | Muhammad Amith, Licong Cui, Kirk Roberts, Cui Tao | School of Biomedical Informatics, University of Texas Health Science Center at Houston, USA; School of Biomedical Informatics, The University of Texas Health Science Center at Houston, USA; Department of Information Science, University of North Texas, USA |
| 106 |  |  [Stance Inference in Twitter through Graph Convolutional Collaborative Filtering Networks with Minimal Supervision](https://doi.org/10.1145/3543873.3587640) |  | 0 | Social Media (SM) has become a stage for people to share thoughts, emotions, opinions, and almost every other aspect of their daily lives. This abundance of human interaction makes SM particularly attractive for social sensing. Especially during polarizing events such as political elections or referendums, users post information and encourage others to support their side, using symbols such as hashtags to represent their attitudes. However, many users choose not to attach hashtags to their messages, use a different language, or show their position only indirectly. Thus, automatically identifying their opinions becomes a more challenging task. To uncover these implicit perspectives, we propose a collaborative filtering model based on Graph Convolutional Networks that exploits the textual content in messages and the rich connections between users and topics. Moreover, our approach only requires a small annotation effort compared to state-of-the-art solutions. Nevertheless, the proposed model achieves competitive performance in predicting individuals' stances. We analyze users' attitudes ahead of two constitutional referendums in Chile in 2020 and 2022. Using two large Twitter datasets, our model achieves improvements of 3.4% in recall and 3.6% in accuracy over the baselines. | Zhiwei Zhou, Erick Elejalde | Leibniz Universität Hannover, L3S Research Center, Germany |
| 107 |  |  [Retrieving false claims on Twitter during the Russia-Ukraine conflict](https://doi.org/10.1145/3543873.3587571) |  | 0 | Nowadays, false and unverified information on social media sway individuals' perceptions during major geo-political events and threaten the quality of the whole digital information ecosystem. Since the Russian invasion of Ukraine, several fact-checking organizations have been actively involved in verifying stories related to the conflict that circulated online. In this paper, we leverage a public repository of fact-checked claims to build a methodological framework for automatically identifying false and unsubstantiated claims spreading on Twitter in February 2022. Our framework consists of two sequential models: First, the claim detection model identifies whether tweets incorporate a (false) claim among those considered in our collection. Then, the claim retrieval model matches the tweets with fact-checked information by ranking verified claims according to their relevance with the input tweet. Both models are based on pre-trained language models and fine-tuned to perform a text classification task and an information retrieval task, respectively. In particular, to validate the effectiveness of our methodology, we consider 83 verified false claims that spread on Twitter during the first week of the invasion, and manually annotate 5,872 tweets according to the claim(s) they report. Our experiments show that our proposed methodology outperforms standard baselines for both claim detection and claim retrieval. Overall, our results highlight how social media providers could effectively leverage semi-automated approaches to identify, track, and eventually moderate false information that spreads on their platforms. | Valerio La Gatta, Chiyu Wei, Luca Luceri, Francesco Pierri, Emilio Ferrara | Information Sciences Institute, University of Southern California, USA; Information Sciences Institute, University of Southern California, USA and University of Naples Federico II, Italy; Politecnico di Milano, Italy and Information Sciences Institute, University of Southern California, USA |
| 108 |  |  [Enhancing Hierarchy-Aware Graph Networks with Deep Dual Clustering for Session-based Recommendation](https://doi.org/10.1145/3543507.3583247) |  | 0 | Session-based Recommendation aims at predicting the next interacted item based on short anonymous behavior sessions. However, existing solutions neglect to model two inherent properties of sequential representing distributions, i.e., hierarchy structures resulted from item popularity and collaborations existing in both intra- and inter-session. Tackling with these two factors at the same time is challenging. On the one hand, traditional Euclidean space utilized in previous studies fails to capture hierarchy structures due to a restricted representation ability. On the other hand, the intuitive apply of hyperbolic geometry could extract hierarchical patterns but more emphasis on degree distribution weakens intra- and inter-session collaborations. To address the challenges, we propose a Hierarchy-Aware Dual Clustering Graph Network (HADCG) model for session-based recommendation. Towards the first challenge, we design the hierarchy-aware graph modeling module which converts sessions into hyperbolic session graphs, adopting hyperbolic geometry in propagation and attention mechanism so as to integrate chronological and hierarchical information. As for the second challenge, we introduce the deep dual clustering module which develops a two-level clustering strategy, i.e., information regularizer for intra-session clustering and contrastive learner for inter-session clustering, to enhance hyperbolic representation learning from collaborative perspectives and further promote recommendation performance. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed HADCG. | Jiajie Su, Chaochao Chen, Weiming Liu, Fei Wu, Xiaolin Zheng, Haoming Lyu | Zhejiang University, China |
| 109 |  |  [Intra and Inter Domain HyperGraph Convolutional Network for Cross-Domain Recommendation](https://doi.org/10.1145/3543507.3583402) |  | 0 | Cross-Domain Recommendation (CDR) aims to solve the data sparsity problem by integrating the strengths of different domains. Though researchers have proposed various CDR methods to effectively transfer knowledge across domains, they fail to address the following key issues, i.e., (1) they cannot model high-order correlations among users and items in every single domain to obtain more accurate representations; (2) they cannot model the correlations among items across different domains. To tackle the above issues, we propose a novel Intra and Inter Domain HyperGraph Convolutional Network (II-HGCN) framework, which includes two main layers in the modeling process, i.e., the intra-domain layer and the inter-domain layer. In the intra-domain layer, we design a user hypergraph and an item hypergraph to model high-order correlations inside every single domain. Thus we can address the data sparsity problem better and learn high-quality representations of users and items. In the inter-domain layer, we propose an inter-domain hypergraph structure to explore correlations among items from different domains based on their interactions with common users. Therefore we can not only transfer the knowledge of users but also combine embeddings of items across domains. Comprehensive experiments on three widely used benchmark datasets demonstrate that II-HGCN outperforms other state-of-the-art methods, especially when datasets are extremely sparse. | Zhongxuan Han, Xiaolin Zheng, Chaochao Chen, Wenjie Cheng, Yang Yao | Zhejiang University, China; Zhejiang Lab, China |
| 110 |  |  [Generating Counterfactual Hard Negative Samples for Graph Contrastive Learning](https://doi.org/10.1145/3543507.3583499) |  | 0 | Graph contrastive learning has emerged as a powerful tool for unsupervised graph representation learning. The key to the success of graph contrastive learning is to acquire high-quality positive and negative samples as contrasting pairs for the purpose of learning underlying structural semantics of the input graph. Recent works usually sample negative samples from the same training batch with the positive samples, or from an external irrelevant graph. However, a significant limitation lies in such strategies, which is the unavoidable problem of sampling false negative samples. In this paper, we propose a novel method to utilize \textbf{C}ounterfactual mechanism to generate artificial hard negative samples for \textbf{G}raph \textbf{C}ontrastive learning, namely \textbf{CGC}, which has a different perspective compared to those sampling-based strategies. We utilize counterfactual mechanism to produce hard negative samples, which ensures that the generated samples are similar to, but have labels that different from the positive sample. The proposed method achieves satisfying results on several datasets compared to some traditional unsupervised graph learning methods and some SOTA graph contrastive learning methods. We also conduct some supplementary experiments to give an extensive illustration of the proposed method, including the performances of CGC with different hard negative samples and evaluations for hard negative samples generated with different similarity measurements. | Haoran Yang, Hongxu Chen, Sixiao Zhang, Xiangguo Sun, Qian Li, Xiangyu Zhao, Guandong Xu | The Chinese University of Hong Kong, Hong Kong; City University of Hong Kong, Hong Kong; University of Technology Sydney, Australia; Curtin University, Australia |
| 111 |  |  [Toward Degree Bias in Embedding-Based Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583544) |  | 0 | A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. | Harry Shomer, Wei Jin, Wentao Wang, Jiliang Tang | Computer Science, Michigan State University, USA |
| 112 |  |  [LINet: A Location and Intention-Aware Neural Network for Hotel Group Recommendation](https://doi.org/10.1145/3543507.3583202) |  | 0 | Motivated by the collaboration with Fliggy1, a leading Online Travel Platform (OTP), we investigate an important but less explored research topic about optimizing the quality of hotel supply, namely selecting potential profitable hotels in advance to build up adequate room inventory. We formulate a WWW problem, i.e., within a specific time period (When) and potential travel area (Where), which hotels should be recommended to a certain group of users with similar travel intentions (Why). We identify three critical challenges in solving the WWW problem: user groups generation, travel data sparsity and utilization of hotel recommendation information (e.g., period, location and intention). To this end, we propose LINet, a Location and Intention-aware neural Network for hotel group recommendation. Specifically, LINet first identifies user travel intentions for user groups generalization, and then characterizes the group preferences by jointly considering historical user-hotel interaction and spatio-temporal features of hotels. For data sparsity, we develop a graph neural network, which employs long-term data, and further design an auxiliary loss function of location that efficiently exploits data within the same and across different locations. Both offline and online experiments demonstrate the effectiveness of LINet when compared with state-of-the-art methods. LINet has been successfully deployed on Fliggy to retrieve high quality hotels for business development, serving hundreds of hotel operation scenarios and thousands of hotel operators. | Ruitao Zhu, Detao Lv, Yao Yu, Ruihao Zhu, Zhenzhe Zheng, Ke Bu, Quan Lu, Fan Wu | Alibaba Group, China; Cornell University, USA; Shanghai Jiao Tong University, China |
| 113 |  |  [Distillation from Heterogeneous Models for Top-K Recommendation](https://doi.org/10.1145/3543507.3583209) |  | 0 | Recent recommender systems have shown remarkable performance by using an ensemble of heterogeneous models. However, it is exceedingly costly because it requires resources and inference latency proportional to the number of models, which remains the bottleneck for production. Our work aims to transfer the ensemble knowledge of heterogeneous teachers to a lightweight student model using knowledge distillation (KD), to reduce the huge inference costs while retaining high accuracy. Through an empirical study, we find that the efficacy of distillation severely drops when transferring knowledge from heterogeneous teachers. Nevertheless, we show that an important signal to ease the difficulty can be obtained from the teacher's training trajectory. This paper proposes a new KD framework, named HetComp, that guides the student model by transferring easy-to-hard sequences of knowledge generated from the teachers' trajectories. To provide guidance according to the student's learning state, HetComp uses dynamic knowledge construction to provide progressively difficult ranking knowledge and adaptive knowledge transfer to gradually transfer finer-grained ranking information. Our comprehensive experiments show that HetComp significantly improves the distillation quality and the generalization of the student model. | SeongKu Kang, Wonbin Kweon, Dongha Lee, Jianxun Lian, Xing Xie, Hwanjo Yu | Pohang University of Science and Technology, Republic of Korea; Yonsei University, Republic of Korea; Microsoft Research Asia, China |
| 114 |  |  [Exploration and Regularization of the Latent Action Space in Recommendation](https://doi.org/10.1145/3543507.3583244) |  | 0 | In recommender systems, reinforcement learning solutions have effectively boosted recommendation performance because of their ability to capture long-term user-system interaction. However, the action space of the recommendation policy is a list of items, which could be extremely large with a dynamic candidate item pool. To overcome this challenge, we propose a hyper-actor and critic learning framework where the policy decomposes the item list generation process into a hyper-action inference step and an effect-action selection step. The first step maps the given state space into a vectorized hyper-action space, and the second step selects the item list based on the hyper-action. In order to regulate the discrepancy between the two action spaces, we design an alignment module along with a kernel mapping function for items to ensure inference accuracy and include a supervision module to stabilize the learning process. We build simulated environments on public datasets and empirically show that our framework is superior in recommendation compared to standard RL baselines. | Shuchang Liu, Qingpeng Cai, Bowen Sun, Yuhao Wang, Ji Jiang, Dong Zheng, Peng Jiang, Kun Gai, Xiangyu Zhao, Yongfeng Zhang | Peking University, China; Kuaishou Technology, China; Rutgers University, USA; City University of Hong Kong, China |
| 115 |  |  [Compressed Interaction Graph based Framework for Multi-behavior Recommendation](https://doi.org/10.1145/3543507.3583312) |  | 0 | Multi-types of user behavior data (e.g., clicking, adding to cart, and purchasing) are recorded in most real-world recommendation scenarios, which can help to learn users' multi-faceted preferences. However, it is challenging to explore multi-behavior data due to the unbalanced data distribution and sparse target behavior, which lead to the inadequate modeling of high-order relations when treating multi-behavior data ''as features'' and gradient conflict in multitask learning when treating multi-behavior data ''as labels''. In this paper, we propose CIGF, a Compressed Interaction Graph based Framework, to overcome the above limitations. Specifically, we design a novel Compressed Interaction Graph Convolution Network (CIGCN) to model instance-level high-order relations explicitly. To alleviate the potential gradient conflict when treating multi-behavior data ''as labels'', we propose a Multi-Expert with Separate Input (MESI) network with separate input on the top of CIGCN for multi-task learning. Comprehensive experiments on three large-scale real-world datasets demonstrate the superiority of CIGF. Ablation studies and in-depth analysis further validate the effectiveness of our proposed model in capturing high-order relations and alleviating gradient conflict. The source code and datasets are available at https://github.com/MC-CV/CIGF. | Wei Guo, Chang Meng, Enming Yuan, Zhicheng He, Huifeng Guo, Yingxue Zhang, Bo Chen, Yaochen Hu, Ruiming Tang, Xiu Li, Rui Zhang | ruizhang.info, China; Huawei Technologies, Canada; Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Huawei Noah's Ark Lab, China; Shenzhen International Graduate School, Tsinghua University, China |
| 116 |  |  [Correlative Preference Transfer with Hierarchical Hypergraph Network for Multi-Domain Recommendation](https://doi.org/10.1145/3543507.3583331) |  | 0 | Advanced recommender systems usually involve multiple domains (such as scenarios or categories) for various marketing strategies, and users interact with them to satisfy diverse demands. The goal of multi-domain recommendation (MDR) is to improve the recommendation performance of all domains simultaneously. Conventional graph neural network based methods usually deal with each domain separately, or train a shared model to serve all domains. The former fails to leverage users' cross-domain behaviors, making the behavior sparseness issue a great obstacle. The latter learns shared user representation with respect to all domains, which neglects users' domain-specific preferences. In this paper we propose $\mathsf{H^3Trans}$, a hierarchical hypergraph network based correlative preference transfer framework for MDR, which represents multi-domain user-item interactions into a unified graph to help preference transfer. $\mathsf{H^3Trans}$ incorporates two hyperedge-based modules, namely dynamic item transfer (Hyper-I) and adaptive user aggregation (Hyper-U). Hyper-I extracts correlative information from multi-domain user-item feedbacks for eliminating domain discrepancy of item representations. Hyper-U aggregates users' scattered preferences in multiple domains and further exploits the high-order (not only pair-wise) connections to improve user representations. Experiments on both public and production datasets verify the superiority of $\mathsf{H^3Trans}$ for MDR. | Zixuan Xu, Penghui Wei, Shaoguo Liu, Weimin Zhang, Liang Wang, Bo Zheng | Alibaba Group, China |
| 117 |  |  [User Retention-oriented Recommendation with Decision Transformer](https://doi.org/10.1145/3543507.3583418) |  | 0 | Improving user retention with reinforcement learning~(RL) has attracted increasing attention due to its significant importance in boosting user engagement. However, training the RL policy from scratch without hurting users' experience is unavoidable due to the requirement of trial-and-error searches. Furthermore, the offline methods, which aim to optimize the policy without online interactions, suffer from the notorious stability problem in value estimation or unbounded variance in counterfactual policy evaluation. To this end, we propose optimizing user retention with Decision Transformer~(DT), which avoids the offline difficulty by translating the RL as an autoregressive problem. However, deploying the DT in recommendation is a non-trivial problem because of the following challenges: (1) deficiency in modeling the numerical reward value; (2) data discrepancy between the policy learning and recommendation generation; (3) unreliable offline performance evaluation. In this work, we, therefore, contribute a series of strategies for tackling the exposed issues. We first articulate an efficient reward prompt by weighted aggregation of meta embeddings for informative reward embedding. Then, we endow a weighted contrastive learning method to solve the discrepancy between training and inference. Furthermore, we design two robust offline metrics to measure user retention. Finally, the significant improvement in the benchmark datasets demonstrates the superiority of the proposed method. | Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, Dawei Yin | Wuhan University, China; Baidu Inc., China; City University of Hong Kong, Hong Kong |
| 118 |  |  [Balancing Unobserved Confounding with a Few Unbiased Ratings in Debiased Recommendations](https://doi.org/10.1145/3543507.3583495) |  | 0 | Recommender systems are seen as an effective tool to address information overload, but it is widely known that the presence of various biases makes direct training on large-scale observational data result in sub-optimal prediction performance. In contrast, unbiased ratings obtained from randomized controlled trials or A/B tests are considered to be the golden standard, but are costly and small in scale in reality. To exploit both types of data, recent works proposed to use unbiased ratings to correct the parameters of the propensity or imputation models trained on the biased dataset. However, the existing methods fail to obtain accurate predictions in the presence of unobserved confounding or model misspecification. In this paper, we propose a theoretically guaranteed model-agnostic balancing approach that can be applied to any existing debiasing method with the aim of combating unobserved confounding and model misspecification. The proposed approach makes full use of unbiased data by alternatively correcting model parameters learned with biased data, and adaptively learning balance coefficients of biased samples for further debiasing. Extensive real-world experiments are conducted along with the deployment of our proposal on four representative debiasing methods to demonstrate the effectiveness. | Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu | Peking University, China; University of Chinese Academy of Sciences, China; University of California, San Diego, USA; Beijing Technology and Business University, China |
| 119 |  |  [Denoising and Prompt-Tuning for Multi-Behavior Recommendation](https://doi.org/10.1145/3543507.3583513) |  | 0 | In practical recommendation scenarios, users often interact with items under multi-typed behaviors (e.g., click, add-to-cart, and purchase). Traditional collaborative filtering techniques typically assume that users only have a single type of behavior with items, making it insufficient to utilize complex collaborative signals to learn informative representations and infer actual user preferences. Consequently, some pioneer studies explore modeling multi-behavior heterogeneity to learn better representations and boost the performance of recommendations for a target behavior. However, a large number of auxiliary behaviors (i.e., click and add-to-cart) could introduce irrelevant information to recommenders, which could mislead the target behavior (i.e., purchase) recommendation, rendering two critical challenges: (i) denoising auxiliary behaviors and (ii) bridging the semantic gap between auxiliary and target behaviors. Motivated by the above observation, we propose a novel framework-Denoising and Prompt-Tuning (DPT) with a three-stage learning paradigm to solve the aforementioned challenges. In particular, DPT is equipped with a pattern-enhanced graph encoder in the first stage to learn complex patterns as prior knowledge in a data-driven manner to guide learning informative representation and pinpointing reliable noise for subsequent stages. Accordingly, we adopt different lightweight tuning approaches with effectiveness and efficiency in the following stages to further attenuate the influence of noise and alleviate the semantic gap among multi-typed behaviors. Extensive experiments on two real-world datasets demonstrate the superiority of DPT over a wide range of state-of-the-art methods. The implementation code is available online at https://github.com/zc-97/DPT. | Chi Zhang, Rui Chen, Xiangyu Zhao, Qilong Han, Li Li | City University of Hong Kong, Hong Kong; University of Delaware, USA; Harbin Engineering University, China |
| 120 |  |  [CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation](https://doi.org/10.1145/3543507.3583538) |  | 0 | Embedding-based methods currently achieved impressive success in recommender systems. However, such methods are more likely to suffer from bias in data distribution, especially the attribute bias problem. For example, when a certain type of user, like the elderly, occupies the mainstream, the recommendation results of minority users would be seriously affected by the mainstream users’ attributes. To address this problem, most existing methods are proposed from the perspective of fairness, which focuses on eliminating unfairness but deteriorates the recommendation performance. Unlike these methods, in this paper, we focus on improving the recommendation performance for minority users of biased attributes. Along this line, we propose a novel attribute-aware Counterfactual Augmentation framework for Minority Users(CAMUS). Specifically, the CAMUS consists of a counterfactual augmenter, a confidence estimator, and a recommender. The counterfactual augmenter conducts data augmentation for the minority group by utilizing the interactions of mainstream users based on a universal counterfactual assumption. Besides, a tri-training-based confidence estimator is applied to ensure the effectiveness of augmentation. Extensive experiments on three real-world datasets have demonstrated the superior performance of the proposed methods. Further case studies verify the universality of the proposed CAMUS framework on different data sparsity, attributes, and models. | Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, Hongwei Zheng | School of Computer Science and Engineering, Beihang University, China; Beijing Academy of Blockchain and Edge Computing, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Institute of Artificial Intelligence, Beihang University, China |
| 121 |  |  [Dynamically Expandable Graph Convolution for Streaming Recommendation](https://doi.org/10.1145/3543507.3583237) |  | 0 | Personalized recommender systems have been widely studied and deployed to reduce information overload and satisfy users' diverse needs. However, conventional recommendation models solely conduct a one-time training-test fashion and can hardly adapt to evolving demands, considering user preference shifts and ever-increasing users and items in the real world. To tackle such challenges, the streaming recommendation is proposed and has attracted great attention recently. Among these, continual graph learning is widely regarded as a promising approach for the streaming recommendation by academia and industry. However, existing methods either rely on the historical data replay which is often not practical under increasingly strict data regulations, or can seldom solve the \textit{over-stability} issue. To overcome these difficulties, we propose a novel \textbf{D}ynamically \textbf{E}xpandable \textbf{G}raph \textbf{C}onvolution (DEGC) algorithm from a \textit{model isolation} perspective for the streaming recommendation which is orthogonal to previous methods. Based on the motivation of disentangling outdated short-term preferences from useful long-term preferences, we design a sequence of operations including graph convolution pruning, refining, and expanding to only preserve beneficial long-term preference-related parameters and extract fresh short-term preferences. Moreover, we model the temporal user preference, which is utilized as user embedding initialization, for better capturing the individual-level preference shifts. Extensive experiments on the three most representative GCN-based recommendation models and four industrial datasets demonstrate the effectiveness and robustness of our method. | Bowei He, Xu He, Yingxue Zhang, Ruiming Tang, Chen Ma | Huawei Noah's Ark Lab Montreal, Canada; Huawei Noah's Ark Lab, China; Department of Computer Science, City University of Hong Kong, Hong Kong; City University of Hong Kong, Hong Kong |
| 122 |  |  [CTRLStruct: Dialogue Structure Learning for Open-Domain Response Generation](https://doi.org/10.1145/3543507.3583285) |  | 0 | Dialogue structure discovery is essential in dialogue generation. Well-structured topic flow can leverage background information and predict future topics to help generate controllable and explainable responses. However, most previous work focused on dialogue structure learning in task-oriented dialogue other than open-domain dialogue which is more complicated and challenging. In this paper, we present a new framework CTRLStruct for dialogue structure learning to effectively explore topic-level dialogue clusters as well as their transitions with unlabelled information. Precisely, dialogue utterances encoded by bi-directional Transformer are further trained through a special designed contrastive learning task to improve representation. Then we perform clustering to utterance-level representations and form topic-level clusters that can be considered as vertices in dialogue structure graph. The edges in the graph indicating transition probability between vertices are calculated by mimicking expert behavior in datasets. Finally, dialogue structure graph is integrated into dialogue model to perform controlled response generation. Experiments on two popular open-domain dialogue datasets show our model can generate more coherent responses compared to some excellent dialogue models, as well as outperform some typical sentence embedding methods in dialogue utterance representation. Code is available in GitHub. | Congchi Yin, Piji Li, Zhaochun Ren | Shandong University, China; Nanjing University of Aeronautics and Astronautics, China |
| 123 |  |  [BlinkViz: Fast and Scalable Approximate Visualization on Very Large Datasets using Neural-Enhanced Mixed Sum-Product Networks](https://doi.org/10.1145/3543507.3583411) |  | 0 | Web-based online interactive visual analytics enjoys popularity in recent years. Traditionally, visualizations are produced directly from querying the underlying data. However, for a very large dataset, this way is so time-consuming that it cannot meet the low-latency requirements of interactive visual analytics. In this paper, we propose a learning-based visualization approach called BlinkViz, which uses a learned model to produce approximate visualizations by leveraging mixed sum-product networks to learn the distribution of the original data. In such a way, it makes visualization faster and more scalable by decoupling visualization and data. In addition, to improve the accuracy of approximate visualizations, we propose an enhanced model by incorporating a neural network with residual structures, which can refine prediction results, especially for visual requests with low selectivity. Extensive experiments show that BlinkViz is extremely fast even on a large dataset with hundreds of millions of data records (over 30GB), responding in sub-seconds (from 2ms to less than 500ms for different requests) while keeping a low error rate. Furthermore, our approach remains scalable on latency and memory footprint size regardless of data size. | Yimeng Qiao, Yinan Jing, Hanbing Zhang, Zhenying He, Kai Zhang, X. Sean Wang | Shanghai Key Laboratory of Data Science, School of Software, Fudan University, China; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China |
| 124 |  |  [Semi-supervised Adversarial Learning for Complementary Item Recommendation](https://doi.org/10.1145/3543507.3583462) |  | 0 | Complementary item recommendations are a ubiquitous feature of modern e-commerce sites. Such recommendations are highly effective when they are based on collaborative signals like co-purchase statistics. In certain online marketplaces, however, e.g., on online auction sites, constantly new items are added to the catalog. In such cases, complementary item recommendations are often based on item side-information due to a lack of interaction data. In this work, we propose a novel approach that can leverage both item side-information and labeled complementary item pairs to generate effective complementary recommendations for cold items, i.e., for items for which no co-purchase statistics yet exist. Given that complementary items typically have to be of a different category than the seed item, we technically maintain a latent space for each item category. Simultaneously, we learn to project distributed item representations into these category spaces to determine suitable recommendations. The main learning process in our architecture utilizes labeled pairs of complementary items. In addition, we adopt ideas from Cycle Generative Adversarial Networks (CycleGAN) to leverage available item information even in case no labeled data exists for a given item and category. Experiments on three e-commerce datasets show that our method is highly effective. | Koby Bibas, Oren Sar Shalom, Dietmar Jannach | Meta, Israel; University of Klagenfurt, Austria; Amazon, Israel |
| 125 |  |  [MaSS: Model-agnostic, Semantic and Stealthy Data Poisoning Attack on Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583203) |  | 0 | Open-source knowledge graphs are attracting increasing attention. Nevertheless, the openness also raises the concern of data poisoning attacks, that is, the attacker could submit malicious facts to bias the prediction of knowledge graph embedding (KGE) models. Existing studies on such attacks adopt a clear-box setting and neglect the semantic information of the generated facts, making them fail to attack in real-world scenarios. In this work, we consider a more rigorous setting and propose a model-agnostic, semantic, and stealthy data poisoning attack on KGE models from a practical perspective. The main design of our work is to inject indicative paths to make the infected model predict certain malicious facts. With the aid of the proposed opaque-box path injection theory, we theoretically reveal that the attack success rate under the opaque-box setting is determined by the plausibility of triplets on the indicative path. Based on this, we develop a novel and efficient algorithm to search paths that maximize the attack goal, satisfy certain semantic constraints, and preserve certain stealthiness, i.e., the normal functionality of the target KGE will not be influenced although it predicts wrong facts given certain queries. Through extensive evaluation of benchmark datasets and 6 typical knowledge graph embedding models as the victims, we validate the effectiveness in terms of attack success rate (ASR) under opaque-box setting and stealthiness. For example, on FB15k-237, our attack achieves a ASR on DeepPath, with an average ASR over when attacking various KGE models under the opaque-box setting. | Xiaoyu You, Beina Sheng, Daizong Ding, Mi Zhang, Xudong Pan, Min Yang, Fuli Feng | University of Science and Technology of China, CCCD Key Lab of Ministry of Culture and Tourism, China; Fudan University, School of Computer Science, China |
| 126 |  |  [TaxoComplete: Self-Supervised Taxonomy Completion Leveraging Position-Enhanced Semantic Matching](https://doi.org/10.1145/3543507.3583342) |  | 0 | Taxonomies are used to organize knowledge in many applications, including recommender systems, content browsing, or web search. With the emergence of new concepts, static taxonomies become obsolete as they fail to capture up-to-date knowledge. Several approaches have been proposed to address the problem of maintaining taxonomies automatically. These approaches typically rely on a limited set of neighbors to represent a given node in the taxonomy. However, considering distant nodes could improve the representation of some portions of the taxonomy, especially for those nodes situated in the periphery or in sparse regions of the taxonomy. In this work, we propose TaxoComplete, a self-supervised taxonomy completion framework that learns the representation of nodes leveraging their position in the taxonomy. TaxoComplete uses a self-supervision generation process that selects some nodes and associates each of them with an anchor set, which is a set composed of nodes in the close and distant neighborhood of the selected node. Using self-supervision data, TaxoComplete learns a position-enhanced node representation using two components: (1) a query-anchor semantic matching mechanism, which encodes pairs of nodes and matches their semantic distance to their graph distance, such that nodes that are close in the taxonomy are placed closely in the shared embedding space while distant nodes are placed further apart; (2) a direction-aware propagation module, which embeds the direction of edges in node representation, such that we discriminate <node, parent> relation from other taxonomic relations. Our approach allows the representation of nodes to encapsulate information from a large neighborhood while being aware of the distance separating pairs of nodes in the taxonomy. Extensive experiments on four real-world and large-scale datasets show that TaxoComplete is substantially more effective than state-of-the-art methods (2x more effective in terms of [email protected] ). | Ines Arous, Ljiljana Dolamic, Philippe CudréMauroux | University of Fribourg, Switzerland; armasuisse, Switzerland |
| 127 |  |  [Bipartite Graph Convolutional Hashing for Effective and Efficient Top-N Search in Hamming Space](https://doi.org/10.1145/3543507.3583219) |  | 0 | Searching on bipartite graphs is basal and versatile to many real-world Web applications, e.g., online recommendation, database retrieval, and query-document searching. Given a query node, the conventional approaches rely on the similarity matching with the vectorized node embeddings in the continuous Euclidean space. To efficiently manage intensive similarity computation, developing hashing techniques for graph structured data has recently become an emerging research direction. Despite the retrieval efficiency in Hamming space, prior work is however confronted with catastrophic performance decay. In this work, we investigate the problem of hashing with Graph Convolutional Network on bipartite graphs for effective Top-N search. We propose an end-to-end Bipartite Graph Convolutional Hashing approach, namely BGCH, which consists of three novel and effective modules: (1) adaptive graph convolutional hashing, (2) latent feature dispersion, and (3) Fourier serialized gradient estimation. Specifically, the former two modules achieve the substantial retention of the structural information against the inevitable information loss in hash encoding; the last module develops Fourier Series decomposition to the hashing function in the frequency domain mainly for more accurate gradient estimation. The extensive experiments on six real-world datasets not only show the performance superiority over the competing hashing-based counterparts, but also demonstrate the effectiveness of all proposed model components contained therein. | Yankai Chen, Yixiang Fang, Yifei Zhang, Irwin King | The Chinese University of Hong Kong, Hong Kong; The Chinese University of Hong Kong, Shenzhen, China |
| 128 |  |  [LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval](https://doi.org/10.1145/3543507.3583294) |  | 0 | Retrieval models based on dense representations in semantic space have become an indispensable branch for first-stage retrieval. These retrievers benefit from surging advances in representation learning towards compressive global sequence-level embeddings. However, they are prone to overlook local salient phrases and entity mentions in texts, which usually play pivot roles in first-stage retrieval. To mitigate this weakness, we propose to make a dense retriever align a well-performing lexicon-aware representation model. The alignment is achieved by weakened knowledge distillations to enlighten the retriever via two aspects -- 1) a lexicon-augmented contrastive objective to challenge the dense encoder and 2) a pair-wise rank-consistent regularization to make dense model's behavior incline to the other. We evaluate our model on three public benchmarks, which shows that with a comparable lexicon-aware retriever as the teacher, our proposed dense one can bring consistent and significant improvements, and even outdo its teacher. In addition, we found our improvement on the dense retriever is complementary to the standard ranker distillation, which can further lift state-of-the-art performance. | Kai Zhang, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Binxing Jiao, Daxin Jiang | University of Technology Sydney, Australia; Microsoft, China; The Ohio State University, USA |
| 129 |  |  [A Passage-Level Reading Behavior Model for Mobile Search](https://doi.org/10.1145/3543507.3583343) |  | 0 | Reading is a vital and complex cognitive activity during users’ information-seeking process. Several studies have focused on understanding users’ reading behavior in desktop search. Their findings greatly contribute to the design of information retrieval models. However, little is known about how users read a result in mobile search, although search currently happens more frequently in mobile scenarios. In this paper, we conduct a lab-based user study to investigate users’ fine-grained reading behavior patterns in mobile search. We find that users’ reading attention allocation is strongly affected by several behavior biases, such as position and selection biases. Inspired by these findings, we propose a probabilistic generative model, the Passage-level Reading behavior Model (PRM), to model users’ reading behavior in mobile search. The PRM utilizes observable passage-level exposure and viewport duration events to infer users’ unobserved skimming event, reading event, and satisfaction perception during the reading process. Besides fitting the passage-level reading behavior, we utilize the fitted parameters of PRM to estimate the passage-level and document-level relevance. Experimental results show that PRM outperforms existing unsupervised relevance estimation models. PRM has strong interpretability and provides valuable insights into the understanding of how users seek and perceive useful information in mobile search. | Zhijing Wu, Jiaxin Mao, Kedi Xu, Dandan Song, Heyan Huang | School of Computer Science, Carnegie Mellon University, USA; School of Computer Science and Technology, Beijing Institute of Technology, China; Gaoling School of Artificial Intelligence, Renmin University of China, China |
| 130 |  |  [PROD: Progressive Distillation for Dense Retrieval](https://doi.org/10.1145/3543507.3583421) |  | 0 | Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released. | Zhenghao Lin, Yeyun Gong, Xiao Liu, Hang Zhang, Chen Lin, Anlei Dong, Jian Jiao, Jingwen Lu, Daxin Jiang, Rangan Majumder, Nan Duan | Microsoft Research Asia, China; Microsoft, USA; Microsoft, China; School of Informatics, Xiamen University, China |
| 131 |  |  [Ad Auction Design with Coupon-Dependent Conversion Rate in the Auto-bidding World](https://doi.org/10.1145/3543507.3583230) |  | 0 | Online advertising has become a dominant source of revenue of the Internet. In classic auction theory, only the auctioneer (i.e., the platform) and buyers (i.e., the advertisers) are involved, while the advertising audiences are ignored. For ecommerce advertising, however, the platform can provide coupons for the advertising audiences and nudge them into purchasing more products at lower prices (e.g., 2 dollars off the regular price). Such promotions can lead to an increase in amount and value of purchases. In this paper, we jointly design the coupon value computation, slot allocation, and payment of online advertising in an auto-bidding world. Firstly, we propose the auction mechanism, named CFA-auction (i.e., Coupon-For-the-Audiences-auction), which takes advertising audiences into account in the auction design. We prove the existence of pacing equilibrium, and show that CFA-auction satisfies the IC (incentive compatibility), IR (individual rationality) constraints. Then, we study the optimality of CFA-auction, and prove it can maintain an approximation of the optimal. Finally, experimental evaluation results on both offline dataset as well as online A/B test demonstrate the effectiveness of CFA-auction. | Bonan Ni, Xun Wang, Qi Zhang, Pingzhong Tang, Zhourong Chen, Tianjiu Yin, Liangni Lu, Xiaobing Liu, Kewu Sun, Zhe Ma | TuringSense, China and Institute for Interdisciplinary Information Sciences, Tsinghua University, China; Intelligent Science & Technology Academy of CASIC, China and Scientific Research Key Laboratory of Aerospace Defence Intelligent Systems and Technology, China; ByteDance, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, China |
| 132 |  |  [A Reference-Dependent Model for Web Search Evaluation: Understanding and Measuring the Experience of Boundedly Rational Users](https://doi.org/10.1145/3543507.3583551) |  | 0 | Previous researches demonstrate that users’ actions in search interaction are associated with relative gains and losses to reference points, known as the reference dependence effect. However, this widely confirmed effect is not represented in most user models underpinning existing search evaluation metrics. In this study, we propose a new evaluation metric framework, namely Reference Dependent Metric (ReDeM), for assessing query-level search by incorporating the effect of reference dependence into the modelling of user search behavior. To test the overall effectiveness of the proposed framework, (1) we evaluate the performance, in terms of correlation with user satisfaction, of ReDeMs built upon different reference points against that of the widely-used metrics on three search datasets; (2) we examine the performance of ReDeMs under different task states, like task difficulty and task urgency; and (3) we analyze the statistical reliability of ReDeMs in terms of discriminative power. Experimental results indicate that: (1) ReDeMs integrated with a proper reference point achieve better correlations with user satisfaction than most of the existing metrics, like Discounted Cumulative Gain (DCG) and Rank-Biased Precision (RBP), even though their parameters have already been well-tuned; (2) ReDeMs reach relatively better performance compared to existing metrics when the task triggers a high-level cognitive load; (3) the discriminative power of ReDeMs is far stronger than Expected Reciprocal Rank (ERR), slightly stronger than Precision and similar to DCG, RBP and INST. To our knowledge, this study is the first to explicitly incorporate the reference dependence effect into the user browsing model and offline evaluation metrics. Our work illustrates a promising approach to leveraging the insights about user biases from cognitive psychology in better evaluating user search experience and enhancing user models. | Nuo Chen, Jiqun Liu, Tetsuya Sakai | Waseda University, Japan; The University of Oklahoma, USA |
| 133 |  |  [Maximizing Submodular Functions for Recommendation in the Presence of Biases](https://doi.org/10.1145/3543507.3583195) |  | 0 | Subset selection tasks, arise in recommendation systems and search engines and ask to select a subset of items that maximize the value for the user. The values of subsets often display diminishing returns, and hence, submodular functions have been used to model them. If the inputs defining the submodular function are known, then existing algorithms can be used. In many applications, however, inputs have been observed to have social biases that reduce the utility of the output subset. Hence, interventions to improve the utility are desired. Prior works focus on maximizing linear functions -- a special case of submodular functions -- and show that fairness constraint-based interventions can not only ensure proportional representation but also achieve near-optimal utility in the presence of biases. We study the maximization of a family of submodular functions that capture functions arising in the aforementioned applications. Our first result is that, unlike linear functions, constraint-based interventions cannot guarantee any constant fraction of the optimal utility for this family of submodular functions. Our second result is an algorithm for submodular maximization. The algorithm provably outputs subsets that have near-optimal utility for this family under mild assumptions and that proportionally represent items from each group. In empirical evaluation, with both synthetic and real-world data, we observe that this algorithm improves the utility of the output subset for this family of submodular functions over baselines. | Anay Mehrotra, Nisheeth K. Vishnoi |  |
| 134 |  |  [Facility Relocation Search For Good: When Facility Exposure Meets User Convenience](https://doi.org/10.1145/3543507.3583859) |  | 0 | In this paper, we propose a novel facility relocation problem where facilities (and their services) are portable, which is a combinatorial search problem with many practical applications. Given a set of users, a set of existing facilities, and a set of potential sites, we decide which of the existing facilities to relocate to potential sites, such that two factors are satisfied: (1) facility exposure: facilities after relocation have balanced exposure, namely serving equivalent numbers of users; (2) user convenience: it is convenient for users to access the nearest facility, which provides services with shorter travel distance. This problem is motivated by applications such as dynamically redistributing vaccine resources to align supply with demand for different vaccination centers, and relocating the bike sharing sites daily to improve the transportation efficiency. We first prove that this problem is NP-hard, and then we propose two algorithms: a non-learning best response algorithm () and a reinforcement learning algorithm (). In particular, the best response algorithm finds a Nash equilibrium to balance the facility-related and the user-related goals. To avoid being confined to only one Nash equilibrium, as found in the method, we also propose the reinforcement learning algorithm for long-term benefits, where each facility is an agent and we determine whether a facility needs to be relocated or not. To verify the effectiveness of our methods, we adopt multiple metrics to evaluate not only our objective, but also several other facility exposure equity and user convenience metrics to understand the benefits after facility relocation. Finally, comprehensive experiments using real-world datasets provide insights into the effectiveness of the two algorithms in practice. | Hui Luo, Zhifeng Bao, J. Shane Culpepper, Mingzhao Li, Yanchang Zhao | CSIRO, Australia; RMIT University, Australia |
| 135 |  |  [Detecting and Limiting Negative User Experiences in Social Media Platforms](https://doi.org/10.1145/3543507.3583883) |  | 0 | Item ranking is important to a social media platform’s success. The order in which posts, videos, messages, comments, ads, used products, notifications are presented to a user greatly affects the time spent on the platform, how often they visit it, how much they interact with each other, and the quantity and quality of the content they post. To this end, item ranking algorithms use models that predict the likelihood of different events, e.g., the user liking, sharing, commenting on a video, clicking/converting on an ad, or opening the platform’s app from a notification. Unfortunately, by solely relying on such event-prediction models, social media platforms tend to over optimize for short-term objectives and ignore the long-term effects. In this paper, we propose an approach that aims at improving item ranking long-term impact. The approach primarily relies on an ML model that predicts negative user experiences. The model utilizes all available UI events: the details of an action can reveal how positive or negative the user experience has been; for example, a user writing a lengthy report asking for a given video to be taken down, likely had a very negative experience. Furthermore, the model takes into account detected integrity (e.g., hostile speech or graphic violence) and quality (e.g., click or engagement bait) issues with the content. Note that those issues can be perceived very differently from different users. Therefore, developing a personalized model, where a prediction refers to a specific user for a specific piece of content at a specific point in time, is a fundamental design choice in our approach. Besides the personalized ML model, our approach consists of two more pieces: (a) the way the personalized model is integrated with an item ranking algorithm and (b) the metrics, methodology, and success criteria for the long term impact of detecting and limiting negative user experiences. Our evaluation process uses extensive A/B testing on the Facebook platform: we compare the impact of our approach in treatment groups against production control groups. The AB test results indicate a 5% to 50% reduction in hides, reports, and submitted feedback. Furthermore, we compare against a baseline that does not include some of the crucial elements of our approach: the comparison shows our approach has a 100x to 30x lower False Positive Ratio than a baseline. Lastly, we present the results from a large scale survey, where we observe a statistically significant improvement of 3 to 6 percent in users’ sentiment regarding content suffering from nudity, clickbait, false / misleading, witnessing-hate, and violence issues. | Lluís Garcia Pueyo, Vinodh Kumar Sunkara, Prathyusha Senthil Kumar, Mohit Diwan, Qian Ge, Behrang Javaherian, Vasilis Verroios | Meta Platforms, Inc., USA |
| 136 |  |  [On Detecting Policy-Related Political Ads: An Exploratory Analysis of Meta Ads in 2022 French Election](https://doi.org/10.1145/3543507.3583875) |  | 0 | Online political advertising has become the cornerstone of political campaigns. The budget spent solely on political advertising in the U.S. has increased by more than 100% from \$700 million during the 2017-2018 U.S. election cycle to \$1.6 billion during the 2020 U.S. presidential elections. Naturally, the capacity offered by online platforms to micro-target ads with political content has been worrying lawmakers, journalists, and online platforms, especially after the 2016 U.S. presidential election, where Cambridge Analytica has targeted voters with political ads congruent with their personality To curb such risks, both online platforms and regulators (through the DSA act proposed by the European Commission) have agreed that researchers, journalists, and civil society need to be able to scrutinize the political ads running on large online platforms. Consequently, online platforms such as Meta and Google have implemented Ad Libraries that contain information about all political ads running on their platforms. This is the first step on a long path. Due to the volume of available data, it is impossible to go through these ads manually, and we now need automated methods and tools to assist in the scrutiny of political ads. In this paper, we focus on political ads that are related to policy. Understanding which policies politicians or organizations promote and to whom is essential in determining dishonest representations. This paper proposes automated methods based on pre-trained models to classify ads in 14 main policy groups identified by the Comparative Agenda Project (CAP). We discuss several inherent challenges that arise. Finally, we analyze policy-related ads featured on Meta platforms during the 2022 French presidential elections period. | Vera Sosnovik, Romaissa Kessi, Maximin Coavoux, Oana Goga | CNRS, France and LIG, Université Grenoble Alpes, Grenoble INP, France; CNRS, France and LIX, Inria, Ecole Polytechnique, Institut Polytechnique de Paris, France |
| 137 |  |  [A ML-based Approach for HTML-based Style Recommendation](https://doi.org/10.1145/3543873.3587300) |  | 0 | Given a large corpus of HTML-based emails (or websites, posters, documents) collected from the web, how can we train a model capable of learning from such rich heterogeneous data for HTML-based style recommendation tasks such as recommending useful design styles or suggesting alternative HTML designs? To address this new learning task, we first decompose each HTML document in the corpus into a sequence of smaller HTML fragments where each fragment may consist of a set of HTML entities such as buttons, images, textual content (titles, paragraphs) and stylistic entities such as background-style, font-style, button-style, among others. From these HTML fragments, we then derive a single large heterogeneous hypergraph that captures the higher-order dependencies between HTML fragments and entities in such fragments, both within the same HTML document as well as across the HTML documents in the corpus. We then formulate this new HTML style recommendation task as a hypergraph representation learning problem and propose an approach to solve it. Our approach is able to learn effective low-dimensional representations of the higher-order fragments that consist of sets of heterogeneous entities as well as low-dimensional representations of the individual entities themselves. We demonstrate the effectiveness of the approach across several design style recommendation tasks. To the best of our knowledge, this work is the first to develop an ML-based model for the task of HTML-based email style recommendation. | Ryan Aponte, Ryan A. Rossi, Shunan Guo, Jane Hoffswell, Nedim Lipka, Chang Xiao, Gromit YeukYin Chan, Eunyee Koh, Nesreen K. Ahmed | Adobe, USA; CMU, USA; Intel Labs, USA; Adobe Research, USA |
| 138 |  |  [Graph-Level Embedding for Time-Evolving Graphs](https://doi.org/10.1145/3543873.3587299) |  | 0 | Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph’s nodes. We then train a “document-level’’ language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly available datasets for the task of temporal graph similarity ranking, and our model outperforms baseline methods. Our experimental results demonstrate the effectiveness of our method in generating graph-level embeddings for dynamic networks. | Lili Wang, Chenghan Huang, Xinyuan Cao, Weicheng Ma, Soroush Vosoughi | Georgia Institute of Technology, USA; Jefferies Financial Group LLC, USA; Dartmouth College, USA |
| 139 |  |  [SpotLight: Visual Insight Recommendation](https://doi.org/10.1145/3543873.3587302) |  | 0 | Visualization recommendation systems make understanding data more accessible to users of all skill levels by automatically generating visualizations for users to explore. However, most existing visualization recommendation systems focus on ranking all possible visualizations based on the attributes or encodings, which makes it difficult to find the most relevant insights. We therefore introduce a novel class of insight-based visualization recommendation systems that automatically rank and recommend groups of related insights as well as the most important insights within each group. Our approach combines results from different learning-based methods to discover insights automatically and generalizes to a variety of attribute types (e.g., categorical, numerical, and temporal), including non-trivial combinations of these attribute types. To demonstrate the utility of this approach, we implemented a insight-centric visualization recommendation system, SpotLight, and conducted a user study with twelve participants, which showed that users are able to quickly find and understand relevant insights in unfamiliar data. | Camille Harris, Ryan A. Rossi, Sana Malik, Jane Hoffswell, Fan Du, Tak Yeon Lee, Eunyee Koh, Handong Zhao | KAIST, Republic of Korea; Adobe Research, USA; Georgia Tech, USA |
| 140 |  |  [DataExpo: A One-Stop Dataset Service for Open Science Research](https://doi.org/10.1145/3543873.3587305) |  | 0 | The large volumes of data on the Internet provides new opportunities for scientific discovery, especially promoting data-driven open science research. However, due to lack of accurate semantic markups, finding relevant data is still difficult. To address this problem, we develop a one-stop dataset service called DataExpo and propose a deep learning method for automatic metadata ingestion. In this demo paper, we describe the system architecture, and how DataExpo facilitates dataset discovery, search and recommendation. Up till now, DataExpo has indexed over 960,000 datasets from more than 27,000 repositories in the context of Deep-time Digital Earth Program. Demo visitors can explore our service via https://dataexpo.acemap.info. | Bin Lu, Lyuwen Wu, Lina Yang, Chenxing Sun, Wei Liu, Xiaoying Gan, Shiyu Liang, Luoyi Fu, Xinbing Wang, Chenghu Zhou | Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, China; Shanghai Jiao Tong University, China |
| 141 |  |  [Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization](https://doi.org/10.1145/3543873.3587309) |  | 0 | We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data. | Canwen Xu, Julian J. McAuley, Penghan Wang | UC San Diego, USA; Cisco, USA |
| 142 |  |  [Is the Impression Log Beneficial to Effective Model Training in News Recommender Systems? No, It's NOT](https://doi.org/10.1145/3543873.3587312) |  | 0 |  | Jeewon Ahn, HongKyun Bae, SangWook Kim |  |
| 143 |  |  [Incorporating Embedding to Topic Modeling for More Effective Short Text Analysis](https://doi.org/10.1145/3543873.3587316) |  | 0 | With the growing abundance of short text content on websites, analyzing and comprehending these short texts has become a crucial task. Topic modeling is a widely used technique for analyzing short text documents and uncovering the underlying topics. However, traditional topic models face difficulties in accurately extracting topics from short texts due to limited content and their sparse nature. To address these issues, we propose an Embedding-based topic modeling (EmTM) approach that incorporates word embedding and hierarchical clustering to identify significant topics. Experimental results demonstrate the effectiveness of EmTM on two datasets comprising web short texts, Snippet and News. The results indicate a superiority of EmTM over baseline topic models by its exceptional performance in both classification accuracy and topic coherence metrics. | Junaid Rashid, Jungeun Kim, Usman Naseem | Department of Software, Kongju National University, Cheonan, Republic of Korea, Republic of Korea; Department of Data Science, Sejong University, Seoul, Republic of Korea, Republic of Korea; School of Computer Science, The University of Sydney, Sydney, Australia, Australia |
| 144 |  |  [EnhancE: Enhanced Entity and Relation Embedding for Knowledge Hypergraph Link Prediction](https://doi.org/10.1145/3543873.3587326) |  | 0 | Knowledge Hypergraphs, as the generalization of knowledge graphs, have attracted increasingly widespread attention due to their friendly compatibility with real-world facts. However, link prediction in knowledge hypergraph is still an underexplored field despite the ubiquity of n-ary facts in the real world. Several recent representative embedding-based knowledge hypergraph link prediction methods have proven to be effective in a series of benchmarks, however, they only consider the position (or role) information, ignoring the neighborhood structure among entities and rich semantic information within each fact. To this end, we propose a model named EnhancE for effective link prediction in knowledge hypergraphs. On the one hand, a more expressive entity representation is obtained with both position and neighborhood information added to the initial embedding. On the other hand, rich semantic information of the involved entities within each tuple is incorporated into relation embedding for enhanced representation. Extensive experimental results over real datasets of both knowledge hypergraph and knowledge graph demonstrate the excellent performance of EnhancE compared with a variety of state-of-the-art baselines. | Chenxu Wang, Zhao Li, Xin Wang, Zirui Chen | Tianjin University, China |
| 145 |  |  [An Analogical Reasoning Method Based on Multi-task Learning with Relational Clustering](https://doi.org/10.1145/3543873.3587333) |  | 0 | Analogical QA task is a challenging natural language processing problem. When two word pairs are similar in their relationships, we refer to their relations as analogous. Although the analogy method based on word embedding is well developed, the analogy reasoning is far beyond this scope. At present, the methods based on pre-trained language models have explored only the tip of the iceberg. In this paper, we proposed a multi-task learning method for analogical QA task. First, we obtain word-pair representations by leveraging the output embeddings of the [MASK] token in the pre-trained language model. The representations are prepared for two tasks. The first task aims to train an analogical classifier by supervised learning. The second task is an auxiliary task based on relation clustering to generate relation pseudo-labels for word pairs and train relation classifier. Our method guides the model to analyze the relation similarity in analogical reasoning without relation labels. The experiments show that our method achieve excellent performance on four analogical reasoning datasets without the help of external corpus and knowledge. In the most difficult data set E-KAR, it has increased by at least 4%. | Shuyi Li, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng | College of Intelligence and Computing, Tianjin University, China |
| 146 |  |  [Templet: A Collaborative System for Knowledge Graph Question Answering over Wikidata](https://doi.org/10.1145/3543873.3587335) |  | 0 | We present Templet: an online question answering (QA) system for Wikidata. Templet is based on the collaboratively-edited repository QAWiki, which collects questions in multiple natural languages along with their corresponding structured queries. Templet generates templates from question–query pairs on QAWiki by replacing key entities with identifiers. Using autocompletion, the user can type a question in natural language, select a template, and again using autocompletion, select the entities they wish to insert into the template’s placeholders, generating a concrete question, query and results. The main objectives of Templet are: (i) to enable users to answer potentially complex questions over Wikidata using natural language templates and autocompletion; (ii) to encourage users to collaboratively create new templates via QAWiki, which in turn can benefit not only Templet, but other QA systems. | Francisca Suárez, Aidan Hogan | DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile |
| 147 |  |  [OptiRef: Query Optimization for Knowledge Bases](https://doi.org/10.1145/3543873.3587342) |  | 0 | Ontology-mediated query answering (OMQA) consists in asking database queries on a knowledge base (KB); a KB is a set of facts, the KB’s database, described by domain knowledge, the KB’s ontology. FOL-rewritability is the main OMQA technique: it reformulates a query w.r.t. the KB’s ontology so that the evaluation of the reformulated query on the KB’s database computes the correct answers. However, because this technique embeds the domain knowledge relevant to the query into the reformulated query, a reformulated query may be complex and its optimization is the crux of efficiency. We showcase OptiRef that implements a novel, general optimization framework for efficient query answering on datalog ±, description logic, existential rules, OWL and RDF/S KBs. OptiRef optimizes reformulated queries by rapidly computing, based on a KB’s database summary, simpler (contained) queries with the same answers. We demonstrate OptiRef’s effectiveness on well-established benchmarks: performance is significantly improved in general, up to several orders of magnitude in the best cases! | Wafaa El Husseini, Cheikh Brahim El Vaigh, François Goasdoué, Hélène Jaudoin | Univ. Rennes, France; Univ. Bourgogne, France |
| 148 |  |  [Learning Topical Structured Interfaces from Medical Research Literature](https://doi.org/10.1145/3543873.3587353) |  | 0 | Accessing large-scale structured datasets such as WDC or CORD-191 is very challenging. Even if one topic (e.g. COVID-19 vaccine efficacy) is of interest, all topical tables in different sources/papers have hundreds of different schemas, depending on the authors, which significantly complicates both finding and querying them. Here we demonstrate a scalable Meta-profiler system, capable of constructing a structured standardized interface to a topic of interest in large-scale (semi-)structured datasets. This interface, that we call Meta-profile represents a multi-dimensional meta-data summary for a selected topic of interest, accumulating all differently structured representations of the topical tables in the dataset. Such Meta-profiles can be used as a rich visualization as well as a robust structural query interface simplifying access to large-scale (semi-)structured data for different user segments, such as data scientists and end users. | Maitry Chauhan, Anna Pyayt, Michael N. Gubanov | University of South Florida, USA; Florida State University, USA |
| 149 |  |  [DGBCT: A Scalable Distributed Gradient Boosting Causal Tree at Alipay](https://doi.org/10.1145/3543873.3584645) |  | 0 | Causal effect estimation has been increasingly emphasized in the past few years. To handle this problem, tree-based causal methods have been widely used due to their robustness and explainability. However, most of the existing methods are limited to running on a single machine, making it difficult to scale up to hundreds of millions of data in typical industrial scenarios. This paper proposes DGBCT, a Distributed Gradient Boosting Causal Tree to tackle such problem, and the contribution of this paper is three folds. First, we extend the original GBCT method to a multi-treatment setting and take the monotonic constraints into consideration, so that more typical industrial necessities can be resolved with our framework. Moreover, we implement DGBCT based on the ‘Controller-Coordinator-Worker’ framework, in which dual failover mechanism is achieved, and commendable flexibility is ensured. In addition, empirical results show that DGBCT significantly outperforms the state-of-the-art causal trees, and has a near-linear speedup as the number of workers grows. The system is currently deployed in Alipay1 to support the daily business tasks that involve hundreds of millions of users. | Jun Zhou, Caizhi Tang, Qing Cui, Yi Ding, Longfei Li, Fei Wu | Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China; College of Computer Science and Technology, Zhejiang University, China |
| 150 |  |  [What Image do You Need? A Two-stage Framework for Image Selection in E-commerce](https://doi.org/10.1145/3543873.3584646) |  | 0 | In e-commerce, images are widely used to display more intuitive information about items. Image selection significantly affects the user’s click-through rate (CTR). Most existing work considers the CTR as the target to find an appropriate image. However, these methods are challenging to deploy online efficiently. Also, the selected images may not relate to the item but are profitable to CTR, resulting in the undesirable phenomenon of enticing users to click on the item. To address these issues, we propose a novel two-stage pipeline method with content-based recall model and CTR-based ranking model. The first is realized as a joint method based on the title-image matching model and multi-modal knowledge graph embedding learning model. The second is a CTR-based visually aware scoring model, incorporating the entity textual information and entity images. Experimental results show the effectiveness and efficiency of our method in offline evaluations. After a month of online A/B testing on a travel platform Fliggy, the relative improvement of our method is 5% with respect to seller selection on CTCVR in the searching scenario, and our method further improves pCTR from 3.48% of human pick to 3.53% in the recommendation scenario. | Sheng You, Chao Wang, Baohua Wu, Jingping Liu, Quan Lu, Guanzhou Han, Yanghua Xiao | Fudan University, China; Alibaba Group, China; Shanghai University, China; East China University of Science and Technology, China |
| 151 |  |  [Learning Geolocation by Accurately Matching Customer Addresses via Graph based Active Learning](https://doi.org/10.1145/3543873.3584647) |  | 0 | We propose a novel adaptation of graph-based active learning for customer address resolution or de-duplication, with the aim to determine if two addresses represent the same physical building or not. For delivery systems, improving address resolution positively impacts multiple downstream systems such as geocoding, route planning and delivery time estimations, leading to an efficient and reliable delivery experience, both for customers as well as delivery agents. Our proposed approach jointly leverages address text, past delivery information and concepts from graph theory to retrieve informative and diverse record pairs to label. We empirically show the effectiveness of our approach on manually curated dataset across addresses from India (IN) and United Arab Emirates (UAE). We achieved absolute improvement in recall on average across IN and UAE while preserving precision over the existing production system. We also introduce delivery point (DP) geocode learning for cold-start addresses as a downstream application of address resolution. In addition to offline evaluation, we also performed online A/B experiments which show that when the production model is augmented with active learnt record pairs, the delivery precision improved by and delivery defects reduced by on an average across shipments from IN and UAE. | Saket Maheshwary, Saurabh Sohoney | Amazon, India |
| 152 |  |  [CAViaR: Context Aware Video Recommendations](https://doi.org/10.1145/3543873.3584658) |  | 0 | Many recommendation systems rely on point-wise models, which score items individually. However, point-wise models generating scores for a video are unable to account for other videos being recommended in a query. Due to this, diversity has to be introduced through the application of heuristic-based rules, which are not able to capture user preferences, or make balanced trade-offs in terms of diversity and item relevance. In this paper, we propose a novel method which introduces diversity by modeling the impact of low diversity on user's engagement on individual items, thus being able to account for both diversity and relevance to adjust item scores. The proposed method is designed to be easily pluggable into existing large-scale recommender systems, while introducing minimal changes in the recommendations stack. Our models show significant improvements in offline metrics based on the normalized cross entropy loss compared to production point-wise models. Our approach also shows a substantial increase of 1.7% in topline engagements coupled with a 1.5% increase in daily active users in an A/B test with live traffic on Facebook Watch, which translates into an increase of millions in the number of daily active users for the product. | Khushhall Chandra Mahajan, Aditya Palnitkar, Ameya Raul, Brad Schumitsch | Meta Inc., USA |
| 153 |  |  [Towards Building a Mobile App for People on the Spectrum](https://doi.org/10.1145/3543873.3587533) |  | 0 | The inclusion of autistic people can be augmented by a mobile app that provides information without a human mediator making information perception more liberating for people in the spectrum. This paper is an overview of a doctoral work dedicated to the development of a web-based mobile tool for supporting the inclusion of people on the autism spectrum. The work includes UX/UI research conducted with psychiatry experts, web information retrieval study and neural question-answering research. Currently, the study results comprise several mobile app layouts, a retriever-reader model design and fine-tuned neural network for extractive question-answering. Source code and other resources are available at https://github.com/vifirsanova/empi. | Victoria Firsanova | Department of Mathematical Linguistics, Saint Petersburg State University, Russian Federation |
| 154 |  |  [Multi-turn mediated solutions for Conversational Artificial Intelligent systems leveraging graph-based techniques](https://doi.org/10.1145/3543873.3587540) |  | 0 | The current era is dominated by intelligent Question Answering (QA) systems that can instantly answer almost all their questions, saving users search time and increasing the throughput and precision in the applied domain. A vast amount of work is being carried out in QA systems to deliver better content satisfying users’ information needs [2]. Since QA systems are ascending the cycle of emerging technologies, there are potential research gaps that can be explored. QA systems form a significant part of Conversational Artificial Intelligent systems giving rise to a new research pathway, i.e., Conversational Question Answering (CQA) systems [32]. We propose to design and develop a CQA system leveraging Hypergraph-based techniques. The approach focuses on the multi-turn conversation and multi-context to gauge users’ exact information needs and deliver better answers. We further aim to address "supporting evidence-based retrieval" for fact-based responsible answer generation. Since the QA system requires a large amount of data and processing, we also intend to investigate hardware performance for effective system utilization. | Riya Naik | Computer Science & Information Systems, Birla Institute Of Technology And Science, Pilani, India |
| 155 |  |  [Graph and Embedding based Approach for Text Clustering: Topic Detection in a Large Multilingual Public Consultation](https://doi.org/10.1145/3543873.3587627) |  | 0 | We present a novel algorithm for multilingual text clustering built upon two well studied techniques: multilingual aligned embedding and community detection in graphs. The aim of our algorithm is to discover underlying topics in a multilingual dataset using clustering. We present both a numerical evaluation using silhouette and V-measure metrics, and a qualitative evaluation for which we propose a new systematic approach. Our algorithm presents robust overall performance and its results were empirically evaluated by an analyst. The work we present was done in the context of a large multilingual public consultation, for which our new algorithm was deployed and used on a daily basis. | Nicolas Stefanovitch, Guillaume Jacquet, Bertrand De Longueville | European Commission - Joint Research Centre, Italy |
| 156 |  |  [Dual-grained Text-Image Olfactory Matching Model with Mutual Promotion Stages](https://doi.org/10.1145/3543873.3587649) |  | 0 | Olfactory experience has great advantages in awakening human memories and emotions, which may even surpass vision in some cases. Studies have proved that olfactory scene descriptions in images and text content can also arouse human olfactory imagination, but there are still few studies on solving related problems from the perspective of computer vision and NLP. This paper proposes a multimodal model that can detect similar olfactory experience in paired text-image samples. The model builds two stages, coarse-grained and fine-grained. The model adopts the feature fusion method based on pre-trained CLIP for coarse-grained matching training to obtain a preliminary feature extractor to promote fine-grained matching training, and then uses the similarity calculation method based on stacked cross attention for fine-grained matching training to obtain the final feature extractor which in turn promotes coarse-grained matching training. Finally, we manually build an approximate olfactory nouns list during fine-grained matching training, which not only yields significantly better performance when fed back to the fine-grained matching process, but this noun list can be used for future research. Experiments on the MUSTI task dataset of MediaEval2022 prove that the coarse-grained and fine-grained matching stages in proposed model both perform well, and both F1 measures exceed the existing baseline models. | Yi Shao, Jiande Sun, Ye Jiang, Jing Li | Shandong Management University, China; Shandong Normal University, China; Qingdao University of Science and Technology, China |
| 157 |  |  [MEMER - Multimodal Encoder for Multi-signal Early-stage Recommendations](https://doi.org/10.1145/3543873.3587679) |  | 0 | Millions of content gets created daily on platforms like YouTube, Facebook, TikTok etc. Most of such large scale recommender systems are data demanding, thus taking substantial time for content embedding to mature. This problem is aggravated when there is no behavioral data available for new content. Poor quality recommendation for these items lead to user dissatisfaction and short content shelf-life. In this paper we propose a solution MEMER (Multimodal Encoder for Multi-signal Early-stage Recommendations), that utilises the multimodal semantic information of content and uses it to generate better quality embeddings for early-stage items. We demonstrate the flexibility of the framework by extending it to various explicit and implicit user actions. Using these learnt embeddings, we conduct offline and online experiments to verify its effectiveness. The predicted embeddings show significant gains in online early-stage experiments for both videos and images (videos: 44% relative gain in click through rate, 46% relative gain in explicit engagements, 9% relative gain in successful video play, 20% relative reduction in skips, images: 56% relative gain in explicit engagements). This also compares well against the performance of mature embeddings (83.3% RelaImpr (RI) [18] in Successful Video Play, 97.8% RelaImpr in Clicks). | Mohit Agarwal, Srijan Saket, Rishabh Mehrotra | ShareChat, India |
| 158 |  |  [Social Re-Identification Assisted RTO Detection for E-Commerce](https://doi.org/10.1145/3543873.3587620) |  | 0 | E-commerce features like easy cancellations, returns, and refunds can be exploited by bad actors or uninformed customers, leading to revenue loss for organization. One such problem faced by e-commerce platforms is Return To Origin (RTO), where the user cancels an order while it is in transit for delivery. In such a scenario platform faces logistics and opportunity costs. Traditionally, models trained on historical trends are used to predict the propensity of an order becoming RTO. Sociology literature has highlighted clear correlations between socio-economic indicators and users’ tendency to exploit systems to gain financial advantage. Social media profiles have information about location, education, and profession which have been shown to be an estimator of socio-economic condition. We believe combining social media data with e-commerce information can lead to improvements in a variety of tasks like RTO, recommendation, fraud detection, and credit modeling. In our proposed system, we find the public social profile of an e-commerce user and extract socio-economic features. Internal data fused with extracted social features are used to train a RTO order detection model. Our system demonstrates a performance improvement in RTO detection of 3.1% and 19.9% on precision and recall, respectively. Our system directly impacts the bottom line revenue and shows the applicability of social re-identification in e-commerce. | Hitkul Jangra, Abinaya K, Soham Saha, Satyajit Banerjee, Muthusamy Chelliah, Ponnurangam Kumaraguru | IIIT Hyderabad, India; IIIT Delhi, India; Flipkart, India |
| 159 |  |  [Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research](https://doi.org/10.1145/3543873.3587657) |  | 0 | Structured interviews are used in many settings, importantly in market research on topics such as brand perception, customer habits, or preferences, which are critical to product development, marketing, and e-commerce at large. Such interviews generally consist of a series of questions that are asked to a participant. These interviews are typically conducted by skilled interviewers, who interpret the responses from the participants and can adapt the interview accordingly. Using automated conversational agents to conduct such interviews would enable reaching a much larger and potentially more diverse group of participants than currently possible. However, the technical challenges involved in building such a conversational system are relatively unexplored. To learn more about these challenges, we convert a market research multiple-choice questionnaire to a conversational format and conduct a user study. We address the key task of conducting structured interviews, namely interpreting the participant's response, for example, by matching it to one or more predefined options. Our findings can be applied to improve response interpretation for the information elicitation phase of conversational recommender systems. | Harshita Sahijwani, Kaustubh D. Dhole, Ankur P. Purwar, Venugopal Vasudevan, Eugene Agichtein | Emory University, USA; Procter & Gamble, USA; Procter & Gamble, Singapore |
| 160 |  |  [Knowledge Graph-Enhanced Neural Query Rewriting](https://doi.org/10.1145/3543873.3587678) |  | 0 | The main task of an e-commerce search engine is to semantically match the user query to the product inventory and retrieve the most relevant items that match the user’s intent. This task is not trivial as often there can be a mismatch between the user’s intent and the product inventory for various reasons, the most prevalent being: (i) the buyers and sellers use different vocabularies, which leads to a mismatch; (ii) the inventory doesn’t contain products that match the user’s intent. To build a successful e-commerce platform it is of paramount importance to be able to address both of these challenges. To do so, query rewriting approaches are used, which try to bridge the semantic gap between the user’s intent and the available product inventory. Such approaches use a combination of query token dropping, replacement and expansion. In this work we introduce a novel Knowledge Graph-enhanced neural query rewriting in the e-commerce domain. We use a relationship-rich product Knowledge Graph to infuse auxiliary knowledge in a transformer-based query rewriting deep neural network. Experiments on two tasks, query pruning and complete query rewriting, show that our proposed approach significantly outperforms a baseline BERT-based query rewriting solution. | Shahla Farzana, Qunzhi Zhou, Petar Ristoski | eBay Inc, USA; University of Illinois Chicago, USA; eBay Inc., USA |
| 161 |  |  [Fairness-aware Differentially Private Collaborative Filtering](https://doi.org/10.1145/3543873.3587577) |  | 0 | Recently, there has been an increasing adoption of differential privacy guided algorithms for privacy-preserving machine learning tasks. However, the use of such algorithms comes with trade-offs in terms of algorithmic fairness, which has been widely acknowledged. Specifically, we have empirically observed that the classical collaborative filtering method, trained by differentially private stochastic gradient descent (DP-SGD), results in a disparate impact on user groups with respect to different user engagement levels. This, in turn, causes the original unfair model to become even more biased against inactive users. To address the above issues, we propose \textbf{DP-Fair}, a two-stage framework for collaborative filtering based algorithms. Specifically, it combines differential privacy mechanisms with fairness constraints to protect user privacy while ensuring fair recommendations. The experimental results, based on Amazon datasets, and user history logs collected from Etsy, one of the largest e-commerce platforms, demonstrate that our proposed method exhibits superior performance in terms of both overall accuracy and user group fairness on both shallow and deep recommendation models compared to vanilla DP-SGD. | Zhenhuan Yang, Yingqiang Ge, Congzhe Su, Dingxian Wang, Xiaoting Zhao, Yiming Ying | Rutgers University, USA; Etsy, USA; University at Albany, SUNY, USA |
| 162 |  |  [Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics](https://doi.org/10.1145/3543873.3587623) |  | 0 | We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's success in generating DIsorder-Specific Multi-Objective Policies (DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in providing personalized and efficient therapeutic recommendations. | Baihan Lin, Guillermo A. Cecchi, Djallel Bouneffouf | IBM TJ Watson Research Center, USA; Columbia University, USA |
| 163 |  |  [Investigating Action-Space Generalization in Reinforcement Learning for Recommendation Systems](https://doi.org/10.1145/3543873.3587661) |  | 0 | Recommender systems are used to suggest items to users based on the users’ preferences. Such systems often deal with massive item sets and incredibly sparse user-item interactions, which makes it very challenging to generate high-quality personalized recommendations. Reinforcement learning (RL) is a framework for sequential decision making and naturally formulates recommender-system tasks: recommending items as actions in different user and context states to maximize long-term user experience. We investigate two RL policy parameterizations that generalize sparse user-items interactions by leveraging the relationships between actions: parameterizing the policy over action features as a softmax or Gaussian distribution. Our experiments on synthetic problems suggest that the Gaussian parameterization—which is not commonly used on recommendation tasks—is more robust to the set of action features than the softmax parameterization. Based on these promising results, we propose a more thorough investigation of the theoretical properties and empirical benefits of the Gaussian parameterization for recommender systems. | Abhishek Naik, Bo Chang, Alexandros Karatzoglou, Martin Mladenov, Ed H. Chi, Minmin Chen | University of Alberta, Canada and Alberta Machine Intelligence Institute (Amii), Canada; Google Research, USA |
| 164 |  |  [Conversion of Legal Agreements into Smart Legal Contracts using NLP](https://doi.org/10.1145/3543873.3587554) |  | 0 | A Smart Legal Contract (SLC) is a specialized digital agreement comprising natural language and computable components. The Accord Project provides an open-source SLC framework containing three main modules: Cicero, Concerto, and Ergo. Currently, we need lawyers, programmers, and clients to work together with great effort to create a usable SLC using the Accord Project. This paper proposes a pipeline to automate the SLC creation process with several Natural Language Processing (NLP) models to convert law contracts to the Accord Project's Concerto model. After evaluating the proposed pipeline, we discovered that our NER pipeline accurately detects CiceroMark from Accord Project template text with an accuracy of 0.8. Additionally, our Question Answering method can extract one-third of the Concerto variables from the template text. We also delve into some limitations and possible future research for the proposed pipeline. Finally, we describe a web interface enabling users to build SLCs. This interface leverages the proposed pipeline to convert text documents to Smart Legal Contracts by using NLP models. | Eason Chen, Niall Roche, YuenHsien Tseng, Walter Hernández, Jiangbo Shangguan, Alastair Moore | National Taiwan Normal University, Taiwan; HSBC Business School, Peking University, United Kingdom; University College London, United Kingdom |
| 165 |  |  [Query-Driven Knowledge Graph Construction using Question Answering and Multimodal Fusion](https://doi.org/10.1145/3543873.3587567) |  | 0 | Over recent years, large knowledge bases have been constructed to store massive knowledge graphs. However, these knowledge graphs are highly incomplete. To solve this problem, we propose a web-based question answering system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge graph construction, we design multimodal features and question templates to extract missing facts, which can achieve good quality with very few questions. The question answering system also employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness, to help improve extraction quality. To improve system efficiency, we utilize a few query-driven techniques for web-based question answering to reduce the runtime and provide fast responses to user queries. Extensive experiments have been conducted to demonstrate the effectiveness and efficiency of our system. | Yang Peng | University of Florida, USA |
| 166 |  |  [Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models](https://doi.org/10.1145/3543873.3587655) |  | 0 | Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally different but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. These results reinforce that simple prompt structures are more effective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks. | Stephan Linzbach, Tim Tressel, Laura Kallmeyer, Stefan Dietze, Hajira Jabeen | GESIS Leibniz Institute for Social Sciences, Germany and Heinrich Heine University, Germany; Heinrich Heine University, Germany; GESIS Leibniz Institut für Sozialwissenschaften, Germany; GESIS Leibniz Institute for Social Sciences, Germany |
| 167 |  |  [CS-TGN: Community Search via Temporal Graph Neural Networks](https://doi.org/10.1145/3543873.3587654) |  | 0 | Searching for local communities is an important research challenge that allows for personalized community discovery and supports advanced data analysis in various complex networks, such as the World Wide Web, social networks, and brain networks. The evolution of these networks over time has motivated several recent studies to identify local communities in temporal networks. Given any query nodes, Community Search aims to find a densely connected subgraph containing query nodes. However, existing community search approaches in temporal networks have two main limitations: (1) they adopt pre-defined subgraph patterns to model communities, which cannot find communities that do not conform to these patterns in real-world networks, and (2) they only use the aggregation of disjoint structural information to measure quality, missing the dynamic of connections and temporal properties. In this paper, we propose a query-driven Temporal Graph Convolutional Network (CS-TGN) that can capture flexible community structures by learning from the ground-truth communities in a data-driven manner. CS-TGN first combines the local query-dependent structure and the global graph embedding in each snapshot of the network and then uses a GRU cell with contextual attention to learn the dynamics of interactions and update node embeddings over time. We demonstrate how this model can be used for interactive community search in an online setting, allowing users to evaluate the found communities and provide feedback. Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings. | Farnoosh Hashemi, Ali Behrouz, Milad Rezaei Hajidehi | University of British Columbia, Canada |
| 168 |  |  [Learned Temporal Aggregations for Fraud Classification on E-Commerce Platforms](https://doi.org/10.1145/3543873.3587632) |  | 0 | Fraud and other types of adversarial behavior are serious problems on customer-to-customer (C2C) e-commerce platforms, where harmful behaviors by bad actors erode user trust and safety. Many modern e-commerce integrity systems utilize machine learning (ML) to detect fraud and bad actors. We discuss the practical problems faced by integrity systems which utilize data associated with user interactions with the platform. Specifically, we focus on the challenge of representing the user interaction events, and aggregating their features. We compare the performance of two paradigms to handle the feature temporality when training the ML models: hand-engineered temporal aggregation and a learned aggregation using a sequence encoder. We show that a model which learns a time-aggregation using a sequence encoder outperforms models trained on handcrafted aggregations on the fraud classification task with a real-world dataset. | Xiao Ling, David Yan, Bilal Alsallakh, Ashutosh Pandey, Manan Bakshi, Pamela Bhattacharya | Meta, USA; North Carolina State University, USA; Meta, Canada; Voxel AI, USA |
| 169 |  |  [Decency and Decentralisation: Verifiable Decentralised Knowledge Graph Querying](https://doi.org/10.1145/3543873.3587635) |  | 0 | Increasing interest in decentralisation for data and processing on the Web brings with it the need to re-examine methods for verifying data and behaviour for scalable multi-party interactions. We consider factors relevant to verification of querying activity on knowledge graphs in a Trusted Decentralised Web, and set out ideas for future research in this area. | Aisling Third, John Domingue | Knowledge Media Institute, The Open University, United Kingdom |
| 170 |  |  [Towards a Decentralized Data Hub and Query System for Federated Dynamic Data Spaces](https://doi.org/10.1145/3543873.3587646) |  | 0 | This position paper proposes a hybrid architecture for secure and efficient data sharing and processing across dynamic data spaces. On the one hand, current centralized approaches are plagued by issues such as lack of privacy and control for users, high costs, and bad performance, making these approaches unsuitable for the decentralized data spaces prevalent in Europe and various industries (decentralized on the conceptual and physical levels while centralized in the underlying implementation). On the other hand, decentralized systems face challenges with limited knowledge of/control over the global system, fair resource utilization, and data provenance. Our proposed Semantic Data Ledger (SDL) approach combines the advantages of both architectures to overcome their limitations. SDL allows users to choose the best combination of centralized and decentralized features, providing a decentralized infrastructure for the publication of structured data with machine-readable semantics. It supports expressive structured queries, secure data sharing, and payment mechanisms based on an underlying autonomous ledger, enabling the implementation of economic models and fair-use strategies. | Danh Le Phuoc, Sonja Schimmler, Anh LeTuan, Uwe A. Kuehn, Manfred Hauswirth | Fraunhofer Institute for Open Communication Systems, Berlin, Germany; TU Berlin, Germany |
| 171 |  |  [What are "personal data spaces"?](https://doi.org/10.1145/3543873.3587656) |  | 0 | While the concept of “data spaces” is no longer new, its specific application to individuals and personal data management is still undeveloped. This short paper presents a vision for “personal data spaces” in the shape of a work-in-progress description of them and some of the conceptual and implementation features envisioned. It is offered for discussion, debate, and improvement by professionals, policymakers, and researchers operating in the intersection of data spaces and personal data management. | Viivi Lähteenoja | University of Helsinki, Finland and Aalto University, Finland |
| 172 |  |  [TAPP: Defining standard provenance information for clinical research data and workflows - Obstacles and opportunities](https://doi.org/10.1145/3543873.3587562) |  | 0 | Data provenance has raised much attention across disciplines lately, as it has been shown that enrichment of data with provenance information leads to better credibility, renders data more FAIR fostering data reuse. Also, the biomedical domain has recognised the potential of provenance capture. However, several obstacles prevent efficient, automated, and machine-interpretable enrichment of biomedical data with provenance information, such as data heterogeneity, complexity, and sensitivity. Here, we explain how in Germany clinical data are transferred from hospital information systems into a data integration centre to enable secondary use of patient data and how it can be reused as research data. Considering the complex data infrastructures in hospitals, we indicate obstacles and opportunities when collecting provenance information along heterogeneous data processing pipelines. To express provenance data, we indicate the usage of the Fast Healthcare Interoperability Resource (FHIR) provenance resource for healthcare data. In addition, we consider already existing approaches from other research fields and standard communities. As a solution towards high-quality standardised clinical research data, we propose to develop a ’MInimal Requirements for Automated Provenance Information Enrichment’ (MIRAPIE) guideline. As a community project, MIRAPIE should generalise provenance information concepts to allow its world-wide applicability, possibly beyond the health care sector. | Kerstin Gierend, Judith A. H. Wodke, Sascha Genehr, Robert Gött, Ron Henkel, Frank Krüger, Markus Mandalka, Lea Michaelis, Alexander Scheuerlein, Max Schröder, Atinkut Zeleke, Dagmar Waltemath | Core Unit Data Integration Center, University Medicine Greifswald, Germany; Institute of Communications Engineering, University of Rostock, Germany; Institute for Data Science, University of Greifswald, Germany; Medical Informatics Laboratory, University Medicine Greifswald, Germany; Medical Informatics Laboratory, MeDaX Group, University Medicine Greifswald, Germany; Faculty of Engineering, Wismar University of Applied Sciences, Germany; Department of Biomedical Informatics, Center for Preventive Medicine and Digital Health, Medical Faculty Mannheim, Heidelberg University, Germany; Rostock University Library, University of Rostock, Germany |
| 173 |  |  [ProSA: A provenance system for reproducing query results](https://doi.org/10.1145/3543873.3587563) |  | 0 | Good scientific work requires comprehensible, transparent and reproducible research. One way to ensure this is to include all data relevant to a study or evaluation when publishing an article. This data should be at least aggregated or anonymized, at best compact and complete, but always resilient. In this paper we present ProSA, a system for calculating the minimal necessary data set, called sub-database. For this, we combine the Chase — a set of algorithms for transforming databases — with additional provenance information. We display the implementation of provenance guided by the ProSA pipeline and show its use to generate an optimized sub-database. Furhter, we demonstrate how the ProSA GUI looks like and present some applications and extensions. | Tanja Auge | Faculty of Informatics and Data Science, University of Regensburg, Germany |
| 174 |  |  [Hybrid Query and Instance Explanations and Repairs](https://doi.org/10.1145/3543873.3587565) |  | 0 | Prior work on explaining missing (unexpected) query results identifies which parts of the query or data are responsible for the erroneous result or repairs the query or data to fix such errors. The problem of generating repairs is typically expressed as an optimization problem, i.e., a single repair is returned that is optimal wrt. to some criterion such as minimizing the repair’s side effects. However, such an optimization objective may not concretely model a user’s (often hard to formalize) notion of which repair is “correct”. In this paper, we motivate hybrid explanations and repairs, i.e., that fix both the query and the data. Instead of returning one “optimal” repair, we argue for an approach that empowers the user to explore the space of possible repairs effectively. We also present a proof-of-concept implementation and outline open research problems. | Seokki Lee, Boris Glavic, Adriane Chapman, Bertram Ludäscher | University of Illinois at Urbana-Champaign, USA; University of Southampton, United Kingdom; University of Cincinnati, USA; Illinois Institute of Technology, USA |
| 175 |  |  [Querying Container Provenance](https://doi.org/10.1145/3543873.3587568) |  | 0 | Containers are lightweight mechanisms for the isolation of operating system resources. They are realized by activating a set of namespaces. Given the use of containers in scientific computing, tracking and managing provenance within and across containers is becoming essential for debugging and reproducibility. In this work, we examine the properties of container provenance graphs that result from auditing containerized applications. We observe that the generated container provenance graphs are hypergraphs because one resource may belong to one or more namespaces. We examine the hierarchical behavior of PID, mount, and user namespaces, that are more commonly activated and show that even when represented as hypergraphs, the resulting container provenance graphs are acyclic. We experiment with recently published container logs and identify hypergraph properties. | Aniket Modi, Moaz Reyad, Tanu Malik, Ashish Gehani | College of Computing and Digital Media, DePaul University, USA and Department of Computer Science and Engineering, IIT Delhi, India; College of Computing and Digital Media, DePaul University, USA; SRI International, USA |
| 176 |  |  [Graph-less Collaborative Filtering](https://doi.org/10.1145/3543507.3583196) |  | 0 | Graph neural networks (GNNs) have shown the power in representation learning over graph-structured user-item interaction data for collaborative filtering (CF) task. However, with their inherently recursive message propagation among neighboring nodes, existing GNN-based CF models may generate indistinguishable and inaccurate user (item) representations due to the over-smoothing and noise effect with low-pass Laplacian smoothing operators. In addition, the recursive information propagation with the stacked aggregators in the entire graph structures may result in poor scalability in practical applications. Motivated by these limitations, we propose a simple and effective collaborative filtering model (SimRec) that marries the power of knowledge distillation and contrastive learning. In SimRec, adaptive transferring knowledge is enabled between the teacher GNN model and a lightweight student network, to not only preserve the global collaborative signals, but also address the over-smoothing issue with representation recalibration. Empirical results on public datasets show that SimRec archives better efficiency while maintaining superior recommendation performance compared with various strong baselines. Our implementations are publicly available at: https://github.com/HKUDS/SimRec. | Lianghao Xia, Chao Huang, Jiao Shi, Yong Xu | The University of Hong Kong, Hong Kong; South China University of Technology, China |
| 177 |  |  [Collaboration-Aware Graph Convolutional Network for Recommender Systems](https://doi.org/10.1145/3543507.3583229) |  | 0 | Graph Neural Networks (GNNs) have been successfully adopted in recommender systems by virtue of the message-passing that implicitly captures collaborative effect. Nevertheless, most of the existing message-passing mechanisms for recommendation are directly inherited from GNNs without scrutinizing whether the captured collaborative effect would benefit the prediction of user preferences. In this paper, we first analyze how message-passing captures the collaborative effect and propose a recommendation-oriented topological metric, Common Interacted Ratio (CIR), which measures the level of interaction between a specific neighbor of a node with the rest of its neighbors. After demonstrating the benefits of leveraging collaborations from neighbors with higher CIR, we propose a recommendation-tailored GNN, Collaboration-Aware Graph Convolutional Network (CAGCN), that goes beyond 1-Weisfeiler-Lehman(1-WL) test in distinguishing non-bipartite-subgraph-isomorphic graphs. Experiments on six benchmark datasets show that the best CAGCN variant outperforms the most representative GNN-based recommendation model, LightGCN, by nearly 10% in Recall@20 and also achieves around 80% speedup. Our code is publicly available at https://github.com/YuWVandy/CAGCN. | Yu Wang, Yuying Zhao, Yi Zhang, Tyler Derr | Vanderbilt university, USA |
| 178 |  |  [HyConvE: A Novel Embedding Model for Knowledge Hypergraph Link Prediction with Convolutional Neural Networks](https://doi.org/10.1145/3543507.3583256) |  | 0 | Knowledge hypergraph embedding, which projects entities and n-ary relations into a low-dimensional continuous vector space to predict missing links, remains a challenging area to be explored despite the ubiquity of n-ary relational facts in the real world. Currently, knowledge hypergraph link prediction methods are essentially simple extensions of those used in knowledge graphs, where n-ary relational facts are decomposed into different subelements. Convolutional neural networks have been shown to have remarkable information extraction capabilities in previous work on knowledge graph link prediction. In this paper, we propose a novel embedding-based knowledge hypergraph link prediction model named HyConvE, which exploits the powerful learning ability of convolutional neural networks for effective link prediction. Specifically, we employ 3D convolution to capture the deep interactions of entities and relations to efficiently extract explicit and implicit knowledge in each n-ary relational fact without compromising its translation property. In addition, appropriate relation and position-aware filters are utilized sequentially to perform two-dimensional convolution operations to capture the intrinsic patterns and position information in each n-ary relation, respectively. Extensive experimental results on real datasets of knowledge hypergraphs and knowledge graphs demonstrate the superior performance of HyConvE compared with state-of-the-art baselines. | Chenxu Wang, Xin Wang, Zhao Li, Zirui Chen, Jianxin Li | Tianjin University, China; Deakin University, Australia |
| 179 |  |  [Efficient Approximation Algorithms for the Diameter-Bounded Max-Coverage Group Steiner Tree Problem](https://doi.org/10.1145/3543507.3583257) |  | 0 | The Diameter-bounded max-Coverage Group Steiner Tree (DCGST) problem has recently been proposed as an expressive way of formulating keyword-based search and exploration of knowledge graphs. It aims at finding a diameter-bounded tree which covers the most given groups of vertices and has the minimum weight. In contrast to its specialization—the classic Group Steiner Tree (GST) problem which has been extensively studied, the emerging DCGST problem still lacks an efficient algorithm. In this paper, we propose Cba, the first approximation algorithm for the DCGST problem, and we prove its worst-case approximation ratio. Furthermore, we incorporate a best-first search strategy with two pruning methods into PrunedCBA, an improved approximation algorithm. Our extensive experiments on real and synthetic graphs demonstrate the effectiveness and efficiency of PrunedCBA. | Ke Zhang, Xiaoqing Wang, Gong Cheng | State Key Laboratory for Novel Software Technology, Nanjing University, China |
| 180 |  |  [ConsRec: Learning Consensus Behind Interactions for Group Recommendation](https://doi.org/10.1145/3543507.3583277) |  | 0 | Since group activities have become very common in daily life, there is an urgent demand for generating recommendations for a group of users, referred to as group recommendation task. Existing group recommendation methods usually infer groups' preferences via aggregating diverse members' interests. Actually, groups' ultimate choice involves compromises between members, and finally, an agreement can be reached. However, existing individual information aggregation lacks a holistic group-level consideration, failing to capture the consensus information. Besides, their specific aggregation strategies either suffer from high computational costs or become too coarse-grained to make precise predictions. To solve the aforementioned limitations, in this paper, we focus on exploring consensus behind group behavior data. To comprehensively capture the group consensus, we innovatively design three distinct views which provide mutually complementary information to enable multi-view learning, including member-level aggregation, item-level tastes, and group-level inherent preferences. To integrate and balance the multi-view information, an adaptive fusion component is further proposed. As to member-level aggregation, different from existing linear or attentive strategies, we design a novel hypergraph neural network that allows for efficient hypergraph convolutional operations to generate expressive member-level aggregation. We evaluate our ConsRec on two real-world datasets and experimental results show that our model outperforms state-of-the-art methods. An extensive case study also verifies the effectiveness of consensus modeling. | Xixi Wu, Yun Xiong, Yao Zhang, Yizhu Jiao, Jiawei Zhang, Yangyong Zhu, Philip S. Yu | University of Illinois at Urbana-Champaign, USA; University of Illinois at Chicago, USA; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University, China; IFM Lab, Department of Computer Science, University of California, Davis, USA |
| 181 |  |  [Graph Neural Networks with Diverse Spectral Filtering](https://doi.org/10.1145/3543507.3583324) |  | 0 | Spectral Graph Neural Networks (GNNs) have achieved tremendous success in graph machine learning, with polynomial filters applied for graph convolutions, where all nodes share the identical filter weights to mine their local contexts. Despite the success, existing spectral GNNs usually fail to deal with complex networks (e.g., WWW) due to such homogeneous spectral filtering setting that ignores the regional heterogeneity as typically seen in real-world networks. To tackle this issue, we propose a novel diverse spectral filtering (DSF) framework, which automatically learns node-specific filter weights to exploit the varying local structure properly. Particularly, the diverse filter weights consist of two components — A global one shared among all nodes, and a local one that varies along network edges to reflect node difference arising from distinct graph parts — to balance between local and global information. As such, not only can the global graph characteristics be captured, but also the diverse local patterns can be mined with awareness of different node positions. Interestingly, we formulate a novel optimization problem to assist in learning diverse filters, which also enables us to enhance any spectral GNNs with our DSF framework. We showcase the proposed framework on three state-of-the-arts including GPR-GNN, BernNet, and JacobiConv. Extensive experiments over 10 benchmark datasets demonstrate that our framework can consistently boost model performance by up to 4.92% in node classification tasks, producing diverse filters with enhanced interpretability. | Jingwei Guo, Kaizhu Huang, Xinping Yi, Rui Zhang | Duke Kunshan University, China; Xi'an Jiaotong-Liverpool University; The University of Liverpool, China; Xi'an Jiaotong-Liverpool University, China; The University of Liverpool, United Kingdom |
| 182 |  |  [Semi-decentralized Federated Ego Graph Learning for Recommendation](https://doi.org/10.1145/3543507.3583337) |  | 0 | Collaborative filtering (CF) based recommender systems are typically trained based on personal interaction data (e.g., clicks and purchases) that could be naturally represented as ego graphs. However, most existing recommendation methods collect these ego graphs from all users to compose a global graph to obtain high-order collaborative information between users and items, and these centralized CF recommendation methods inevitably lead to a high risk of user privacy leakage. Although recently proposed federated recommendation systems can mitigate the privacy problem, they either restrict the on-device local training to an isolated ego graph or rely on an additional third-party server to access other ego graphs resulting in a cumbersome pipeline, which is hard to work in practice. In addition, existing federated recommendation systems require resource-limited devices to maintain the entire embedding tables resulting in high communication costs. In light of this, we propose a semi-decentralized federated ego graph learning framework for on-device recommendations, named SemiDFEGL, which introduces new device-to-device collaborations to improve scalability and reduce communication costs and innovatively utilizes predicted interacted item nodes to connect isolated ego graphs to augment local subgraphs such that the high-order user-item collaborative information could be used in a privacy-preserving manner. Furthermore, the proposed framework is model-agnostic, meaning that it could be seamlessly integrated with existing graph neural network-based recommendation methods and privacy protection techniques. To validate the effectiveness of the proposed SemiDFEGL, extensive experiments are conducted on three public datasets, and the results demonstrate the superiority of the proposed SemiDFEGL compared to other federated recommendation methods. | Liang Qu, Ningzhi Tang, Ruiqi Zheng, Quoc Viet Hung Nguyen, Zi Huang, Yuhui Shi, Hongzhi Yin | The University of Queensland, Australia; Southern University of Science and Technology, China; Griffith University, Australia |
| 183 |  |  [SINCERE: Sequential Interaction Networks representation learning on Co-Evolving RiEmannian manifolds](https://doi.org/10.1145/3543507.3583353) |  | 0 | Sequential interaction networks (SIN) have been commonly adopted in many applications such as recommendation systems, search engines and social networks to describe the mutual influence between users and items/products. Efforts on representing SIN are mainly focused on capturing the dynamics of networks in Euclidean space, and recently plenty of work has extended to hyperbolic geometry for implicit hierarchical learning. Previous approaches which learn the embedding trajectories of users and items achieve promising results. However, there are still a range of fundamental issues remaining open. For example, is it appropriate to place user and item nodes in one identical space regardless of their inherent discrepancy? Instead of residing in a single fixed curvature space, how will the representation spaces evolve when new interaction occurs? To explore these issues for sequential interaction networks, we propose SINCERE, a novel method representing Sequential Interaction Networks on Co-Evolving RiEmannian manifolds. SIN- CERE not only takes the user and item embedding trajectories in respective spaces into account, but also emphasizes on the space evolvement that how curvature changes over time. Specifically, we introduce a fresh cross-geometry aggregation which allows us to propagate information across different Riemannian manifolds without breaking conformal invariance, and a curvature estimator which is delicately designed to predict global curvatures effectively according to current local Ricci curvatures. Extensive experiments on several real-world datasets demonstrate the promising performance of SINCERE over the state-of-the-art sequential interaction prediction methods. | Junda Ye, Zhongbao Zhang, Li Sun, Yang Yan, Feiyang Wang, Fuxin Ren | North China Electric Power University, China; Beijing University of Posts and Telecommunications, China |
| 184 |  |  [TIGER: Temporal Interaction Graph Embedding with Restarts](https://doi.org/10.1145/3543507.3583433) |  | 0 | Temporal interaction graphs (TIGs), consisting of sequences of timestamped interaction events, are prevalent in fields like e-commerce and social networks. To better learn dynamic node embeddings that vary over time, researchers have proposed a series of temporal graph neural networks for TIGs. However, due to the entangled temporal and structural dependencies, existing methods have to process the sequence of events chronologically and consecutively to ensure node representations are up-to-date. This prevents existing models from parallelization and reduces their flexibility in industrial applications. To tackle the above challenge, in this paper, we propose TIGER, a TIG embedding model that can restart at any timestamp. We introduce a restarter module that generates surrogate representations acting as the warm initialization of node representations. By restarting from multiple timestamps simultaneously, we divide the sequence into multiple chunks and naturally enable the parallelization of the model. Moreover, in contrast to previous models that utilize a single memory unit, we introduce a dual memory module to better exploit neighborhood information and alleviate the staleness problem. Extensive experiments on four public datasets and one industrial dataset are conducted, and the results verify both the effectiveness and the efficiency of our work. | Yao Zhang, Yun Xiong, Yongxiang Liao, Yiheng Sun, Yucheng Jin, Xuehao Zheng, Yangyong Zhu | Fudan University, China; Tencent Weixin Group, China |
| 185 |  |  [Expressive and Efficient Representation Learning for Ranking Links in Temporal Graphs](https://doi.org/10.1145/3543507.3583476) |  | 0 | Temporal graph representation learning (T-GRL) aims to learn representations that model how graph edges evolve over time. While recent works on T-GRL have improved link prediction accuracy in temporal settings, their methods optimize a point-wise loss function independently over future links rather than optimize jointly over a candidate set per node. In applications where resources (e.g., attention) are allocated based on ranking links by likelihood, the use of a ranking loss is preferred. However it is not straightforward to develop a T-GRL method to optimize a ranking loss due to a tradeoff between model expressivity and scalability. In this work, we address these issues and propose a Temporal Graph network for Ranking (TGRank), which significantly improves performance for link prediction tasks by (i) optimizing a list-wise loss for improved ranking, and (ii) incorporating a labeling approach designed to allow for efficient inference over the candidate set jointly, while provably boosting expressivity. We extensively evaluate TGRank over six real networks. TGRank outperforms the state-of-the-art baselines on average by 14.21%↑ (transductive) and 16.25% ↑ (inductive) in ranking metrics while being more efficient (up-to 65 × speed-up) to make inference on large networks. | Susheel Suresh, Mayank Shrivastava, Arko Mukherjee, Jennifer Neville, Pan Li | Purdue University, USA; Microsoft, USA; Microsoft Research, USA |
| 186 |  |  [Semi-Supervised Embedding of Attributed Multiplex Networks](https://doi.org/10.1145/3543507.3583485) |  | 0 | Complex information can be represented as networks (graphs) characterized by a large number of nodes, multiple types of nodes, and multiple types of relationships between them, i.e. multiplex networks. Additionally, these networks are enriched with different types of node features. We propose a Semi-supervised Embedding approach for Attributed Multiplex Networks (SSAMN), to jointly embed nodes, node attributes, and node labels of multiplex networks in a low dimensional space. Network embedding techniques have garnered research attention for real-world applications. However, most existing techniques solely focus on learning the node embeddings, and only a few learn class label embeddings. Our method assumes that we have different classes of nodes and that we know the class label of some, very few nodes for every class. Guided by this type of supervision, SSAMN learns a low-dimensional representation incorporating all information in a large labeled multiplex network. SSAMN integrates techniques from Spectral Embedding and Homogeneity Analysis to improve the embedding of nodes, node attributes, and node labels. Our experiments demonstrate that we only need very few labels per class in order to have a final embedding that preservers the information of the graph. To evaluate the performance of SSAMN, we run experiments on four real-world datasets. The results show that our approach outperforms state-of-the-art methods for downstream tasks such as semi-supervised node classification and node clustering. | Ylli Sadikaj, Justus Rass, Yllka Velaj, Claudia Plant | Faculty of Computer Science, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria and UniVie Doctoral School Computer Science, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria and ds:Univie, University of Vienna, Austria |
| 187 |  |  [Search to Capture Long-range Dependency with Stacking GNNs for Graph Classification](https://doi.org/10.1145/3543507.3583486) |  | 0 | In recent years, Graph Neural Networks (GNNs) have been popular in the graph classification task. Currently, shallow GNNs are more common due to the well-known over-smoothing problem facing deeper GNNs. However, they are sub-optimal without utilizing the information from distant nodes, i.e., the long-range dependencies. The mainstream methods in the graph classification task can extract the long-range dependencies either by designing the pooling operations or incorporating the higher-order neighbors, while they have evident drawbacks by modifying the original graph structure, which may result in information loss in graph structure learning. In this paper, by justifying the smaller influence of the over-smoothing problem in the graph classification task, we evoke the importance of stacking-based GNNs and then employ them to capture the long-range dependencies without modifying the original graph structure. To achieve this, two design needs are given for stacking-based GNNs, i.e., sufficient model depth and adaptive skip-connection schemes. By transforming the two design needs into designing data-specific inter-layer connections, we propose a novel approach with the help of neural architecture search (NAS), which is dubbed LRGNN (Long-Range Graph Neural Networks). Extensive experiments on five datasets show that the proposed LRGNN can achieve the best performance, and obtained data-specific GNNs with different depth and skip-connection schemes, which can better capture the long-range dependencies. | Lanning Wei, Zhiqiang He, Huan Zhao, Quanming Yao | Institute of Computing Technology, Chinese Academy of Science, China and Lenovo, China; Department of Electronic Engineering, Tsinghua University, China; Institute of Computing Technology, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China; 4Paradigm. Inc, China |
| 188 |  |  [Cut-matching Games for Generalized Hypergraph Ratio Cuts](https://doi.org/10.1145/3543507.3583539) |  | 0 | Hypergraph clustering is a basic algorithmic primitive for analyzing complex datasets and systems characterized by multiway interactions, such as group email conversations, groups of co-purchased retail products, and co-authorship data. This paper presents a practical $O(\log n)$-approximation algorithm for a broad class of hypergraph ratio cut clustering objectives. This includes objectives involving generalized hypergraph cut functions, which allow a user to penalize cut hyperedges differently depending on the number of nodes in each cluster. Our method is a generalization of the cut-matching framework for graph ratio cuts, and relies only on solving maximum s-t flow problems in a special reduced graph. It is significantly faster than existing hypergraph ratio cut algorithms, while also solving a more general problem. In numerical experiments on various types of hypergraphs, we show that it quickly finds ratio cut solutions within a small factor of optimality. | Nate Veldt | Texas A&M University, USA |
| 189 |  |  [ApeGNN: Node-Wise Adaptive Aggregation in GNNs for Recommendation](https://doi.org/10.1145/3543507.3583530) |  | 0 | In recent years, graph neural networks (GNNs) have made great progress in recommendation. The core mechanism of GNNs-based recommender system is to iteratively aggregate neighboring information on the user-item interaction graph. However, existing GNNs treat users and items equally and cannot distinguish diverse local patterns of each node, which makes them suboptimal in the recommendation scenario. To resolve this challenge, we present a node-wise adaptive graph neural network framework ApeGNN. ApeGNN develops a node-wise adaptive diffusion mechanism for information aggregation, in which each node is enabled to adaptively decide its diffusion weights based on the local structure (e.g., degree). We perform experiments on six widely-used recommendation datasets. The experimental results show that the proposed ApeGNN is superior to the most advanced GNN-based recommender methods (up to 48.94%), demonstrating the effectiveness of node-wise adaptive aggregation. | Dan Zhang, Yifan Zhu, Yuxiao Dong, Yuandong Wang, Wenzheng Feng, Evgeny Kharlamov, Jie Tang | Bosch Center for Artificial Intelligence, Germany; Tsinghua University, China |
| 190 |  |  [Multi-Modal Self-Supervised Learning for Recommendation](https://doi.org/10.1145/3543507.3583206) |  | 0 | The online emergence of multi-modal sharing platforms (eg, TikTok, Youtube) is powering personalized recommender systems to incorporate various modalities (eg, visual, textual and acoustic) into the latent user representations. While existing works on multi-modal recommendation exploit multimedia content features in enhancing item embeddings, their model representation capability is limited by heavy label reliance and weak robustness on sparse user behavior data. Inspired by the recent progress of self-supervised learning in alleviating label scarcity issue, we explore deriving self-supervision signals with effectively learning of modality-aware user preference and cross-modal dependencies. To this end, we propose a new Multi-Modal Self-Supervised Learning (MMSSL) method which tackles two key challenges. Specifically, to characterize the inter-dependency between the user-item collaborative view and item multi-modal semantic view, we design a modality-aware interactive structure learning paradigm via adversarial perturbations for data augmentation. In addition, to capture the effects that user's modality-aware interaction pattern would interweave with each other, a cross-modal contrastive learning approach is introduced to jointly preserve the inter-modal semantic commonality and user preference diversity. Experiments on real-world datasets verify the superiority of our method in offering great potential for multimedia recommendation over various state-of-the-art baselines. The implementation is released at: https://github.com/HKUDS/MMSSL. | Wei Wei, Chao Huang, Lianghao Xia, Chuxu Zhang | The University of Hong Kong, Hong Kong; Brandeis University, USA; University of Hong Kong, Hong Kong |
| 191 |  |  [Bootstrap Latent Representations for Multi-modal Recommendation](https://doi.org/10.1145/3543507.3583251) |  | 0 | This paper studies the multi-modal recommendation problem, where the item multi-modality information (e.g., images and textual descriptions) is exploited to improve the recommendation accuracy. Besides the user-item interaction graph, existing state-of-the-art methods usually use auxiliary graphs (e.g., user-user or item-item relation graph) to augment the learned representations of users and/or items. These representations are often propagated and aggregated on auxiliary graphs using graph convolutional networks, which can be prohibitively expensive in computation and memory, especially for large graphs. Moreover, existing multi-modal recommendation methods usually leverage randomly sampled negative examples in Bayesian Personalized Ranking (BPR) loss to guide the learning of user/item representations, which increases the computational cost on large graphs and may also bring noisy supervision signals into the training process. To tackle the above issues, we propose a novel self-supervised multi-modal recommendation model, dubbed BM3, which requires neither augmentations from auxiliary graphs nor negative samples. Specifically, BM3 first bootstraps latent contrastive views from the representations of users and items with a simple dropout augmentation. It then jointly optimizes three multi-modal objectives to learn the representations of users and items by reconstructing the user-item interaction graph and aligning modality features under both inter- and intra-modality perspectives. BM3 alleviates both the need for contrasting with negative examples and the complex graph augmentation from an additional target network for contrastive view generation. We show BM3 outperforms prior recommendation models on three datasets with number of nodes ranging from 20K to 200K, while achieving a 2-9X reduction in training time. Our code is available at https://github.com/enoche/BM3. | Xin Zhou, Hongyu Zhou, Yong Liu, Zhiwei Zeng, Chunyan Miao, Pengwei Wang, Yuan You, Feijun Jiang | Alibaba, China; Nanyang Technological University, Singapore |
| 192 |  |  [Recommendation with Causality enhanced Natural Language Explanations](https://doi.org/10.1145/3543507.3583260) |  | 0 | Explainable recommendation has recently attracted increasing attention from both academic and industry communities. Among different explainable strategies, generating natural language explanations is an important method, which can deliver more informative, flexible and readable explanations to facilitate better user decisions. Despite the effectiveness, existing models are mostly optimized based on the observed datasets, which can be skewed due to the selection or exposure bias. To alleviate this problem, in this paper, we formulate the task of explainable recommendation with a causal graph, and design a causality enhanced framework to generate unbiased explanations. More specifically, we firstly define an ideal unbiased learning objective, and then derive a tractable loss for the observational data based on the inverse propensity score (IPS), where the key is a sample re-weighting strategy for equalizing the loss and ideal objective in expectation. Considering that the IPS estimated from the sparse and noisy recommendation datasets can be inaccurate, we introduce a fault tolerant mechanism by minimizing the maximum loss induced by the sample weights near the IPS. For more comprehensive modeling, we further analyze and infer the potential latent confounders induced by the complex and diverse user personalities. We conduct extensive experiments by comparing with the state-of-the-art methods based on three real-world datasets to demonstrate the effectiveness of our method. | Jingsen Zhang, Xu Chen, Jiakai Tang, Weiqi Shao, Quanyu Dai, Zhenhua Dong, Rui Zhang | Huawei Noah's Ark Lab, China; www.ruizhang.info, China; Renmin University of China, China |
| 193 |  |  [Two-Stage Constrained Actor-Critic for Short Video Recommendation](https://doi.org/10.1145/3543507.3583259) |  | 0 | The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including watch time and various types of interactions with multiple videos. One the one hand, the platforms aims at optimizing the users' cumulative watch time (main goal) in long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also needs to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such like, follow, share etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms can not work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual policies to optimize each auxiliary signal. At stage two, we learn a policy to (i) optimize the main signal and (ii) stay close to policies learned at the first stage, which effectively guarantees the performance of this main policy on the auxiliaries. Through extensive offline evaluations, we demonstrate effectiveness of our method over alternatives in both optimizing the main goal as well as balancing the others. We further show the advantage of our method in live experiments of short video recommendations, where it significantly outperforms other baselines in terms of both watch time and interactions. Our approach has been fully launched in the production system to optimize user experiences on the platform. | Qingpeng Cai, Zhenghai Xue, Chi Zhang, Wanqi Xue, Shuchang Liu, Ruohan Zhan, Xueliang Wang, Tianyou Zuo, Wentao Xie, Dong Zheng, Peng Jiang, Kun Gai | Kuaishou Technology, China; Hong Kong University of Science and Technology, China; Unaffiliated, China |
| 194 |  |  [Robust Recommendation with Adversarial Gaussian Data Augmentation](https://doi.org/10.1145/3543507.3583273) |  | 0 | Recommender system holds the promise of accurately understanding and estimating the user preferences. However, due to the extremely sparse user-item interactions, the learned recommender models can be less robust and sensitive to the highly dynamic user preferences and easily changed recommendation environments. To alleviate this problem, in this paper, we propose a simple yet effective robust recommender framework by generating additional samples from the Gaussian distributions. In specific, we design two types of data augmentation strategies. For the first one, we directly produce the data based on the original samples, where we simulate the generation process in the latent space. For the second one, we firstly change the original samples towards the direction of maximizing the loss function, and then produce the data based on the altered samples to make more effective explorations. Based on both of the above strategies, we leverage adversarial training to optimize the recommender model with the generated data which can achieve the largest losses. In addition, we theoretically analyze our framework, and find that the above two data augmentation strategies equal to impose a gradient based regularization on the original recommender models. We conduct extensive experiments based on six real-world datasets to demonstrate the effectiveness of our framework. | Zhenlei Wang, Xu Chen | Gaoling School of Artificial Intelligence, Renmin university of China, China |
| 195 |  |  [Anti-FakeU: Defending Shilling Attacks on Graph Neural Network based Recommender Model](https://doi.org/10.1145/3543507.3583289) |  | 0 | Graph neural network (GNN) based recommendation models are observed to be more vulnerable against carefully-designed malicious records injected into the system, i.e., shilling attacks, which manipulate the recommendation to common users and therefore impair user trust. In this paper, we for the first time conduct a systematic study on the vulnerability of GNN based recommendation model against the shilling attack. With the aid of theoretical analysis, we attribute the root cause of the vulnerability to its neighborhood aggregation mechanism, which could make the negative impact of attacks propagate rapidly in the system. To restore the robustness of GNN based recommendation model, the key factor lies in detecting malicious records in the system and preventing the propagation of misinformation. To this end, we construct a user-user graph to capture the patterns of malicious behaviors and design a novel GNN based detector to identify fake users. Furthermore, we develop a data augmentation strategy and a joint learning paradigm to train the recommender model and the proposed detector. Extensive experiments on benchmark datasets validate the enhanced robustness of the proposed method in resisting various types of shilling attacks and identifying fake users, e.g., our proposed method fully mitigating the impact of popularity attacks on target items up to , and improving the accuracy of detecting fake users on the Gowalla dataset by . | Xiaoyu You, Chi Li, Daizong Ding, Mi Zhang, Fuli Feng, Xudong Pan, Min Yang | University of Science and Technology of China, CCCD Key Lab of Ministry of Culture and Tourism, China; Fudan University, School of Computer Science, China |
| 196 |  |  [Automated Self-Supervised Learning for Recommendation](https://doi.org/10.1145/3543507.3583336) |  | 0 | Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm for collaborative filtering (CF). To improve the representation quality over limited labeled data, contrastive learning has attracted attention in recommendation and benefited graph-based CF model recently. However, the success of most contrastive methods heavily relies on manually generating effective contrastive views for heuristic-based data augmentation. This does not generalize across different datasets and downstream recommendation tasks, which is difficult to be adaptive for data augmentation and robust to noise perturbation. To fill this crucial gap, this work proposes a unified Automated Collaborative Filtering (AutoCF) to automatically perform data augmentation for recommendation. Specifically, we focus on the generative self-supervised learning framework with a learnable augmentation paradigm that benefits the automated distillation of important self-supervised signals. To enhance the representation discrimination ability, our masked graph autoencoder is designed to aggregate global information during the augmentation via reconstructing the masked subgraph structures. Experiments and ablation studies are performed on several public datasets for recommending products, venues, and locations. Results demonstrate the superiority of AutoCF against various baseline methods. We release the model implementation at https://github.com/HKUDS/AutoCF. | Lianghao Xia, Chao Huang, Chunzhen Huang, Kangyi Lin, Tao Yu, Ben Kao | The University of Hong Kong, Hong Kong; Tencent, China |
| 197 |  |  [AutoDenoise: Automatic Data Instance Denoising for Recommendations](https://doi.org/10.1145/3543507.3583339) |  | 0 | Historical user-item interaction datasets are essential in training modern recommender systems for predicting user preferences. However, the arbitrary user behaviors in most recommendation scenarios lead to a large volume of noisy data instances being recorded, which cannot fully represent their true interests. While a large number of denoising studies are emerging in the recommender system community, all of them suffer from highly dynamic data distributions. In this paper, we propose a Deep Reinforcement Learning (DRL) based framework, AutoDenoise, with an Instance Denoising Policy Network, for denoising data instances with an instance selection manner in deep recommender systems. To be specific, AutoDenoise serves as an agent in DRL to adaptively select noise-free and predictive data instances, which can then be utilized directly in training representative recommendation models. In addition, we design an alternate two-phase optimization strategy to train and validate the AutoDenoise properly. In the searching phase, we aim to train the policy network with the capacity of instance denoising; in the validation phase, we find out and evaluate the denoised subset of data instances selected by the trained policy network, so as to validate its denoising ability. We conduct extensive experiments to validate the effectiveness of AutoDenoise combined with multiple representative recommender system models. | Weilin Lin, Xiangyu Zhao, Yejing Wang, Yuanshao Zhu, Wanyu Wang | City University of Hong Kong, Hong Kong and Southern University of Science and Technology, China; City University of Hong Kong, Hong Kong |
| 198 |  |  [AutoS2AE: Automate to Regularize Sparse Shallow Autoencoders for Recommendation](https://doi.org/10.1145/3543507.3583349) |  | 0 | The Embarrassingly Shallow Autoencoders (EASE and SLIM) are strong recommendation methods based on implicit feedback, compared to competing methods like iALS and VAE-CF. However, EASE suffers from several major shortcomings. First, the training and inference of EASE can not scale with the increasing number of items since it requires storing and inverting a large dense matrix; Second, though its optimization objective – the square loss– can yield a closed-form solution, it is not consistent with recommendation goal – predicting a personalized ranking on a set of items, so that its performance is far from optimal w.r.t ranking-oriented recommendation metrics. Finally, the regularization coefficients are sensitive w.r.t recommendation accuracy and vary a lot across different datasets, so the fine-tuning of these parameters is important yet time-consuming. To improve training and inference efficiency, we propose a Similarity-Structure Aware Shallow Autoencoder on top of three similarity structures, including Co-Occurrence, KNN and NSW. We then optimize the model with a weighted square loss, which is proven effective for ranking-based recommendation but still capable of deriving closed-form solutions. However, the weight in the loss can not be learned in the training set and is similarly sensitive w.r.t the accuracy to regularization coefficients. To automatically tune the hyperparameters, we design two validation losses on the validation set for guidance, and update the hyperparameters with the gradient of the validation losses. We finally evaluate the proposed method on multiple real-world datasets and show that it outperforms seven competing baselines remarkably, and verify the effectiveness of each part in the proposed method. | Rui Fan, Yuanhao Pu, Jin Chen, Zhihao Zhu, Defu Lian, Enhong Chen | School of Data Science, University of Science and Technology of China, China; School of Computer Science, University of Science and Technology of China, China; School of Computer Science, School of Data Science, University of Science and Technology of China, China and State Key Laboratory of Cognitive Intelligence, China; University of Electronic Science and Technology of China, China |
| 199 |  |  [Improving Recommendation Fairness via Data Augmentation](https://doi.org/10.1145/3543507.3583341) |  | 0 | Collaborative filtering based recommendation learns users' preferences from all users' historical behavior data, and has been popular to facilitate decision making. R Recently, the fairness issue of recommendation has become more and more essential. A recommender system is considered unfair when it does not perform equally well for different user groups according to users' sensitive attributes~(e.g., gender, race). Plenty of methods have been proposed to alleviate unfairness by optimizing a predefined fairness goal or changing the distribution of unbalanced training data. However, they either suffered from the specific fairness optimization metrics or relied on redesigning the current recommendation architecture. In this paper, we study how to improve recommendation fairness from the data augmentation perspective. The recommendation model amplifies the inherent unfairness of imbalanced training data. We augment imbalanced training data towards balanced data distribution to improve fairness. The proposed framework is generally applicable to any embedding-based recommendation, and does not need to pre-define a fairness metric. Extensive experiments on two real-world datasets clearly demonstrate the superiority of our proposed framework. We publish the source code at https://github.com/newlei/FDA. | Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, Meng Wang | Ant Group, China; University of Science and Technology of China, China; Hefei University of Technology, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China; Hefei University of Technology, China |
| 200 |  |  [Interaction-level Membership Inference Attack Against Federated Recommender Systems](https://doi.org/10.1145/3543507.3583359) |  | 0 | The marriage of federated learning and recommender system (FedRec) has been widely used to address the growing data privacy concerns in personalized recommendation services. In FedRecs, users' attribute information and behavior data (i.e., user-item interaction data) are kept locally on their personal devices, therefore, it is considered a fairly secure approach to protect user privacy. As a result, the privacy issue of FedRecs is rarely explored. Unfortunately, several recent studies reveal that FedRecs are vulnerable to user attribute inference attacks, highlighting the privacy concerns of FedRecs. In this paper, we further investigate the privacy problem of user behavior data (i.e., user-item interactions) in FedRecs. Specifically, we perform the first systematic study on interaction-level membership inference attacks on FedRecs. An interaction-level membership inference attacker is first designed, and then the classical privacy protection mechanism, Local Differential Privacy (LDP), is adopted to defend against the membership inference attack. Unfortunately, the empirical analysis shows that LDP is not effective against such new attacks unless the recommendation performance is largely compromised. To mitigate the interaction-level membership attack threats, we design a simple yet effective defense method to significantly reduce the attacker's inference accuracy without losing recommendation performance. Extensive experiments are conducted with two widely used FedRecs (Fed-NCF and Fed-LightGCN) on three real-world recommendation datasets (MovieLens-100K, Steam-200K, and Amazon Cell Phone), and the experimental results show the effectiveness of our solutions. | Wei Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Lizhen Cui, Tieke He, Hongzhi Yin | The University of Queensland, Australia; Shandong University, China; Griffith University, Australia; Nanjing University, China |
| 201 |  |  [Robust Preference-Guided Denoising for Graph based Social Recommendation](https://doi.org/10.1145/3543507.3583374) |  | 0 | Graph Neural Network(GNN) based social recommendation models improve the prediction accuracy of user preference by leveraging GNN in exploiting preference similarity contained in social relations. However, in terms of both effectiveness and efficiency of recommendation, a large portion of social relations can be redundant or even noisy, e.g., it is quite normal that friends share no preference in a certain domain. Existing models do not fully solve this problem of relation redundancy and noise, as they directly characterize social influence over the full social network. In this paper, we instead propose to improve graph based social recommendation by only retaining the informative social relations to ensure an efficient and effective influence diffusion, i.e., graph denoising. Our designed denoising method is preference-guided to model social relation confidence and benefits user preference learning in return by providing a denoised but more informative social graph for recommendation models. Moreover, to avoid interference of noisy social relations, it designs a self-correcting curriculum learning module and an adaptive denoising strategy, both favoring highly-confident samples. Experimental results on three public datasets demonstrate its consistent capability of improving two state-of-the-art social recommendation models by robustly removing 10-40% of original relations. We release the source code at https://github.com/tsinghua-fib-lab/Graph-Denoising-SocialRec. | Yuhan Quan, Jingtao Ding, Chen Gao, Lingling Yi, Depeng Jin, Yong Li | Tencent, China; Tsinghua University, China |
| 202 |  |  [Few-shot News Recommendation via Cross-lingual Transfer](https://doi.org/10.1145/3543507.3583383) |  | 0 | The cold-start problem has been commonly recognized in recommendation systems and studied by following a general idea to leverage the abundant interaction records of warm users to infer the preference of cold users. However, the performance of these solutions is limited by the amount of records available from warm users to use. Thus, building a recommendation system based on few interaction records from a few users still remains a challenging problem for unpopular or early-stage recommendation platforms. This paper focuses on solving the few-shot recommendation problem for news recommendation based on two observations. First, news at diferent platforms (even in diferent languages) may share similar topics.Second, the user preference over these topics is transferable across diferent platforms. Therefore, we propose to solve the few-shot news recommendation problem by transferring the user-news preference from a many-shot source domain to a few-shot target domain. To bridge two domainsthat are even in diferent languages and without any overlapping users and news, we propose a novel unsupervised cross-lingual transfer model as the news encoder that aligns semantically similar news in two domains. A user encoder is constructed on top of the aligned news encoding and transfers the user preference from the source to target domain. Experimental results on two real-world news recommendation datasets show the superior performance of our proposed method on addressing few-shot news recommendation, comparing to the baselines. The source code can be found at https://github.com/taichengguo/Few-shot-NewsRec . | Taicheng Guo, Lu Yu, Basem Shihada, Xiangliang Zhang |  |
| 203 |  |  [Show Me The Best Outfit for A Certain Scene: A Scene-aware Fashion Recommender System](https://doi.org/10.1145/3543507.3583435) |  | 0 | Fashion recommendation (FR) has received increasing attention in the research of new types of recommender systems. Existing fashion recommender systems (FRSs) typically focus on clothing item suggestions for users in three scenarios: 1) how to best recommend fashion items preferred by users; 2) how to best compose a complete outfit, and 3) how to best complete a clothing ensemble. However, current FRSs often overlook an important aspect when making FR, that is, the compatibility of the clothing item or outfit recommendations is highly dependent on the scene context. To this end, we propose the scene-aware fashion recommender system (SAFRS), which uncovers a hitherto unexplored avenue where scene information is taken into account when constructing the FR model. More specifically, our SAFRS addresses this problem by encoding scene and outfit information in separation attention encoders and then fusing the resulting feature embeddings via a novel scene-aware compatibility score function. Extensive qualitative and quantitative experiments are conducted to show that our SAFRS model outperforms all baselines for every evaluated metric. | Tangwei Ye, Liang Hu, Qi Zhang, Zhong Yuan Lai, Usman Naseem, Dora D. Liu | DeepBlue Academy of Sciences, China; University of Sydney, Australia; University of Technology Sydney, Australia and DeepBlue Academy of Sciences, China; Tongji University, China and DeepBlue Academy of Sciences, China; DeepBlue Academy of Sciences, China and BirenTech Research, China |
| 204 |  |  [Invariant Collaborative Filtering to Popularity Distribution Shift](https://doi.org/10.1145/3543507.3583461) |  | 0 | Collaborative Filtering (CF) models, despite their great success, suffer from severe performance drops due to popularity distribution shifts, where these changes are ubiquitous and inevitable in real-world scenarios. Unfortunately, most leading popularity debiasing strategies, rather than tackling the vulnerability of CF models to varying popularity distributions, require prior knowledge of the test distribution to identify the degree of bias and further learn the popularity-entangled representations to mitigate the bias. Consequently, these models result in significant performance benefits in the target test set, while dramatically deviating the recommendation from users' true interests without knowing the popularity distribution in advance. In this work, we propose a novel learning framework, Invariant Collaborative Filtering (InvCF), to discover disentangled representations that faithfully reveal the latent preference and popularity semantics without making any assumption about the popularity distribution. At its core is the distillation of unbiased preference representations (i.e., user preference on item property), which are invariant to the change of popularity semantics, while filtering out the popularity feature that is unstable or outdated. Extensive experiments on five benchmark datasets and four evaluation settings (i.e., synthetic long-tail, unbiased, temporal split, and out-of-distribution evaluations) demonstrate that InvCF outperforms the state-of-the-art baselines in terms of popularity generalization ability on real recommendations. Visualization studies shed light on the advantages of InvCF for disentangled representation learning. Our codes are available at https://github.com/anzhang314/InvCF. | An Zhang, Jingnan Zheng, Xiang Wang, Yancheng Yuan, TatSeng Chua | The Hong Kong Polytechnic University, Hong Kong; National University of Singapore, Singapore; University of Science and Technology of China, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China; Sea-NExT Joint Lab, National University of Singapore, Singapore |
| 205 |  |  [Code Recommendation for Open Source Software Developers](https://doi.org/10.1145/3543507.3583503) |  | 0 | Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers' interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. Considering the complex interactions among multiple parties within the system, we propose CODER, a novel graph-based code recommendation framework for open source software developers. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect the project hierarchy. Moreover, due to the lack of reliable benchmarks, we construct three large-scale datasets to facilitate future research in this direction. Extensive experiments show that our CODER framework achieves superior performance under various experimental settings, including intra-project, cross-project, and cold-start recommendation. We will release all the datasets, code, and utilities for data retrieval upon the acceptance of this work. | Yiqiao Jin, Yunsheng Bai, Yanqiao Zhu, Yizhou Sun, Wei Wang | Georgia Institute of Technology, USA; University of California, Los Angeles, USA |
| 206 |  |  [pFedPrompt: Learning Personalized Prompt for Vision-Language Models in Federated Learning](https://doi.org/10.1145/3543507.3583518) |  | 0 | Pre-trained vision-language models like CLIP show great potential in learning representations that capture latent characteristics of users. A recently proposed method called Contextual Optimization (CoOp) introduces the concept of training prompt for adapting pre-trained vision-language models. Given the lightweight nature of this method, researchers have migrated the paradigm from centralized to decentralized system to innovate the collaborative training framework of Federated Learning (FL). However, current prompt training in FL mainly focuses on modeling user consensus and lacks the adaptation to user characteristics, leaving the personalization of prompt largely under-explored. Researches over the past few years have applied personalized FL (pFL) approaches to customizing models for heterogeneous users. Unfortunately, we find that with the variation of modality and training behavior, directly applying the pFL methods to prompt training leads to insufficient personalization and performance. To bridge the gap, we present pFedPrompt, which leverages the unique advantage of multimodality in vision-language models by learning user consensus from linguistic space and adapting to user characteristics in visual space in a non-parametric manner. Through this dual collaboration, the learned prompt will be fully personalized and aligned to the user’s local characteristics. We conduct extensive experiments across various datasets under the FL setting with statistical heterogeneity. The results demonstrate the superiority of our pFedPrompt against the alternative approaches with robust performance. | Tao Guo, Song Guo, Junxiao Wang | The Hong Kong Polytechnic University, Hong Kong |
| 207 |  |  [Word Sense Disambiguation by Refining Target Word Embedding](https://doi.org/10.1145/3543507.3583191) |  | 0 | Word Sense Disambiguation (WSD) which aims to identify the correct sense of a target word appearing in a specific context is essential for web text analysis. The use of glosses has been explored as a means for WSD. However, only a few works model the correlation between the target context and gloss. We add to the body of literature by presenting a model that employs a multi-head attention mechanism on deep contextual features of the target word and candidate glosses to refine the target word embedding. Furthermore, to encourage the model to learn the relevant part of target features that align with the correct gloss, we recursively alternate attention on target word features and that of candidate glosses to gradually extract the relevant contextual features of the target word, refining its representation and strengthening the final disambiguation results. Empirical studies on the five most commonly used benchmark datasets show that our proposed model is effective and achieves state-of-the-art results. | Xuefeng Zhang, Richong Zhang, Xiaoyang Li, Fanshuang Kong, Junfan Chen, Samuel Mensah, Yongyi Mao | SKLSDE, School of Computer Science and Engineering, Beihang University, China; University of Ottawa, Canada; The University of Sheffield, United Kingdom |
| 208 |  |  [Dual Policy Learning for Aggregation Optimization in Graph Neural Network-based Recommender Systems](https://doi.org/10.1145/3543507.3583241) |  | 0 | Graph Neural Networks (GNNs) provide powerful representations for recommendation tasks. GNN-based recommendation systems capture the complex high-order connectivity between users and items by aggregating information from distant neighbors and can improve the performance of recommender systems. Recently, Knowledge Graphs (KGs) have also been incorporated into the user-item interaction graph to provide more abundant contextual information; they are exploited to address cold-start problems and enable more explainable aggregation in GNN-based recommender systems (GNN-Rs). However, due to the heterogeneous nature of users and items, developing an effective aggregation strategy that works across multiple GNN-Rs, such as LightGCN and KGAT, remains a challenge. In this paper, we propose a novel reinforcement learning-based message passing framework for recommender systems, which we call DPAO (Dual Policy framework for Aggregation Optimization). This framework adaptively determines high-order connectivity to aggregate users and items using dual policy learning. Dual policy learning leverages two Deep-Q-Network models to exploit the user- and item-aware feedback from a GNN-R and boost the performance of the target GNN-R. Our proposed framework was evaluated with both non-KG-based and KG-based GNN-R models on six real-world datasets, and their results show that our proposed framework significantly enhances the recent base model, improving nDCG and Recall by up to 63.7% and 42.9%, respectively. Our implementation code is available at https://github.com/steve30572/DPAO/. | Heesoo Jung, Sangpil Kim, Hogun Park | Dept. of Artificial Intelligence, Korea University, Republic of Korea; Dept. of Electrical and Computer Engineering, Sungkyunkwan University, Republic of Korea and Dept. of Artificial Intelligence, Sungkyunkwan University, Republic of Korea; Dept. of Artificial Intelligence, Sungkyunkwan University, Republic of Korea |
| 209 |  |  [Addressing Heterophily in Graph Anomaly Detection: A Perspective of Graph Spectrum](https://doi.org/10.1145/3543507.3583268) |  | 0 | Graph anomaly detection (GAD) suffers from heterophily — abnormal nodes are sparse so that they are connected to vast normal nodes. The current solutions upon Graph Neural Networks (GNNs) blindly smooth the representation of neiboring nodes, thus undermining the discriminative information of the anomalies. To alleviate the issue, recent studies identify and discard inter-class edges through estimating and comparing the node-level representation similarity. However, the representation of a single node can be misleading when the prediction error is high, thus hindering the performance of the edge indicator. In graph signal processing, the smoothness index is a widely adopted metric which plays the role of frequency in classical spectral analysis. Considering the ground truth Y to be a signal on graph, the smoothness index is equivalent to the value of the heterophily ratio. From this perspective, we aim to address the heterophily problem in the spectral domain. First, we point out that heterophily is positively associated with the frequency of a graph. Towards this end, we could prune inter-class edges by simply emphasizing and delineating the high-frequency components of the graph. Recall that graph Laplacian is a high-pass filter, we adopt it to measure the extent of 1-hop label changing of the center node and indicate high-frequency components. As GAD can be formulated as a semi-supervised binary classification problem, only part of the nodes are labeled. As an alternative, we use the prediction of the nodes to estimate it. Through our analysis, we show that prediction errors are less likely to affect the identification process. Extensive empirical evaluations on four benchmarks demonstrate the effectiveness of the indicator over popular homophilic, heterophilic, and tailored fraud detection methods. Our proposed indicator can effectively reduce the heterophily degree of the graph, thus boosting the overall GAD performance. Codes are open-sourced in https://github.com/blacksingular/GHRN. | Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang | Zhejiang University, China; University of Science and Technology of China, China; Beijing Electronic Science And Technology Institute, China |
| 210 |  |  [Ginver: Generative Model Inversion Attacks Against Collaborative Inference](https://doi.org/10.1145/3543507.3583306) |  | 0 | Deep Learning (DL) has been widely adopted in almost all domains, from threat recognition to medical diagnosis. Albeit its supreme model accuracy, DL imposes a heavy burden on devices as it incurs overwhelming system overhead to execute DL models, especially on Internet-of-Things (IoT) and edge devices. Collaborative inference is a promising approach to supporting DL models, by which the data owner (the victim) runs the first layers of the model on her local device and then a cloud provider (the adversary) runs the remaining layers of the model. Compared to offloading the entire model to the cloud, the collaborative inference approach is more data privacy-preserving as the owner’s model input is not exposed to outsiders. However, we show in this paper that the adversary can restore the victim’s model input by exploiting the output of the victim’s local model. Our attack is dubbed Ginver 1: Generative model inversion attacks against collaborative inference. Once trained, Ginver can infer the victim’s unseen model inputs without remaking the inversion attack model and thus has the generative capability. We extensively evaluate Ginver under different settings (e.g., white-box and black-box of the victim’s local model) and applications (e.g., CIFAR10 and FaceScrub datasets). The experimental results show that Ginver recovers high-quality images from the victims. | Yupeng Yin, Xianglong Zhang, Huanle Zhang, Feng Li, Yue Yu, Xiuzhen Cheng, Pengfei Hu | School of Computer Science and Technology, Shandong University, China; National University of Defense Technology, China and Peng Cheng Laboratory, China |
| 211 |  |  [All Your Shops Are Belong to Us: Security Weaknesses in E-commerce Platforms](https://doi.org/10.1145/3543507.3583319) |  | 0 | Software as a Service (SaaS) e-commerce platforms for merchants allow individual business owners to set up their online stores almost instantly. Prior work has shown that the checkout flows and payment integration of some e-commerce applications are vulnerable to logic bugs with serious financial consequences, e.g., allowing “shopping for free”. Apart from checkout and payment integration, vulnerabilities in other e-commerce operations have remained largely unexplored, even though they can have far more serious consequences, e.g., enabling “store takeover”. In this work, we design and implement a security evaluation framework to uncover security vulnerabilities in e-commerce operations beyond checkout/payment integration. We use this framework to analyze 32 representative e-commerce platforms, including web services of 24 commercial SaaS platforms and 15 associated Android apps, and 8 open source platforms; these platforms host over 10 million stores as approximated through Google dorks. We uncover several new vulnerabilities with serious consequences, e.g., allowing an attacker to take over all stores under a platform, and listing illegal products at a victim’s store—in addition to “shopping for free” bugs, without exploiting the checkout/payment process. We found 12 platforms vulnerable to store takeover (affecting 41000+ stores) and 6 platforms vulnerable to shopping for free (affecting 19000+ stores, approximated via Google dorks on Oct. 8, 2022). We have responsibly disclosed the vulnerabilities to all affected parties, and requested four CVEs (three assigned, and one is pending review). | Rohan Pagey, Mohammad Mannan, Amr M. Youssef | Concordia Institute for Information Systems Engineering, Concordia University, Canada |
| 212 |  |  [An Empirical Study of the Usage of Checksums for Web Downloads](https://doi.org/10.1145/3543507.3583326) |  | 0 | Checksums, typically provided on webpages and generated from cryptographic hash functions (e.g., MD5, SHA256) or signature schemes (e.g., PGP), are commonly used on websites to enable users to verify that the files they download have not been tampered with when stored on possibly untrusted servers. In this paper, we elucidate the current practices regarding the usage of checksums for web downloads (hash functions used, visibility and validity of checksums, type of websites and files, etc.), as this has been mostly overlooked so far. Using a snowball-sampling strategy for the 200000 most popular domains of the Web, we first crawled a dataset of 8.5M webpages, from which we built, through an active-learning approach, a unique dataset of 277 diverse webpages that contain checksums. Our analysis of these webpages reveals interesting findings about the usage of checksums. For instance, it shows that checksums are used mostly to verify program files, that weak hash functions are frequently used, and that a non-negligible proportion of the checksums provided on webpages do not match that of their associated files. Finally, we complement our analysis with a survey of the webmasters of the considered webpages (N = 26), thus shedding light on the reasons behind the checksum-related choices they make. | Gaël Bernard, Rémi Coudert, Bertil Chapuis, Kévin Huguenin | University of Applied Sciences Western Switzerland, Switzerland; EPFL, Switzerland; Department of Information Systems, University of Lausanne, Switzerland |
| 213 |  |  [Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding](https://doi.org/10.1145/3543507.3583450) |  | 0 | Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients' sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from $83.1\%$ to $59.4\%$ on average with only a modest utility decrease. | Yuke Hu, Wei Liang, Ruofan Wu, Kai Xiao, Weiqiang Wang, Xiaochen Li, Jinfei Liu, Zhan Qin | Ant Group, China; Zhejiang University, China and HIC-ZJU, China |
| 214 |  |  [Sanitizing Sentence Embeddings (and Labels) for Local Differential Privacy](https://doi.org/10.1145/3543507.3583512) |  | 0 | Differentially private (DP) learning, notably DP stochastic gradient descent (DP-SGD), has limited applicability in fine-tuning gigantic pre-trained language models (LMs) for natural language processing tasks. The culprit is the perturbation of gradients (as gigantic as entire models), leading to significant efficiency and accuracy drops. We show how to achieve metric-based local DP (LDP) by sanitizing (high-dimensional) sentence embedding, extracted by LMs and much smaller than gradients. For potential utility improvement, we impose a consistency constraint on the sanitization. We explore two approaches: One is brand new and can directly output consistent noisy embeddings; the other is an upgradation with post-processing. To further mitigate “the curse of dimensionality,” we introduce two trainable linear maps for mediating dimensions without hurting privacy or utility. Our protection can effectively defend against privacy threats on embeddings. It also naturally extends to inference. Our experiments1 show that we reach the non-private accuracy under properly configured parameters, e.g., 0.92 for SST-2 with a privacy budget ϵ = 10 and the reduced dimension as 16. We also sanitize the label for LDP (with another small privacy budget) with limited accuracy losses to fully protect every sequence-label pair. | Minxin Du, Xiang Yue, Sherman S. M. Chow, Huan Sun | Department of Information Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Ohio State University, USA |
| 215 |  |  [Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning](https://doi.org/10.1145/3543507.3583305) |  | 0 | Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting. | Xiangrong Zhu, Guangyao Li, Wei Hu | State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China and National Institute of Healthcare Data Science, Nanjing University, China |
| 216 |  |  [A Single Vector Is Not Enough: Taxonomy Expansion via Box Embeddings](https://doi.org/10.1145/3543507.3583310) |  | 0 | Taxonomies, which organize knowledge hierarchically, support various practical web applications such as product navigation in online shopping and user profile tagging on social platforms. Given the continued and rapid emergence of new entities, maintaining a comprehensive taxonomy in a timely manner through human annotation is prohibitively expensive. Therefore, expanding a taxonomy automatically with new entities is essential. Most existing methods for expanding taxonomies encode entities into vector embeddings (i.e., single points). However, we argue that vectors are insufficient to model the “is-a” hierarchy in taxonomy (asymmetrical relation), because two points can only represent pairwise similarity (symmetrical relation). To this end, we propose to project taxonomy entities into boxes (i.e., hyperrectangles). Two boxes can be "contained", "disjoint" and "intersecting", thus naturally representing an asymmetrical taxonomic hierarchy. Upon box embeddings, we propose a novel model BoxTaxo for taxonomy expansion. The core of BoxTaxo is to learn boxes for entities to capture their child-parent hierarchies. To achieve this, BoxTaxo optimizes the box embeddings from a joint view of geometry and probability. BoxTaxo also offers an easy and natural way for inference: examine whether the box of a given new entity is fully enclosed inside the box of a candidate parent from the existing taxonomy. Extensive experiments on two benchmarks demonstrate the effectiveness of BoxTaxo compared to vector based models. | Song Jiang, Qiyue Yao, Qifan Wang, Yizhou Sun | Meta AI, USA; University of California, Los Angeles, USA |
| 217 |  |  [Knowledge Graph Question Answering with Ambiguous Query](https://doi.org/10.1145/3543507.3583316) |  | 0 | Knowledge graph question answering aims to identify answers of the query according to the facts in the knowledge graph. In the vast majority of the existing works, the input queries are considered perfect and can precisely express the user’s query intention. However, in reality, input queries might be ambiguous and elusive which only contain a limited amount of information. Directly answering these ambiguous queries may yield unwanted answers and deteriorate user experience. In this paper, we propose PReFNet which focuses on answering ambiguous queries with pseudo relevance feedback on knowledge graphs. In order to leverage the hidden (pseudo) relevance information existed in the results that are initially returned from a given query, PReFNet treats the top-k returned candidate answers as a set of most relevant answers, and uses variational Bayesian inference to infer user’s query intention. To boost the quality of the inferred queries, a neighborhood embedding based VGAE model is used to prune inferior inferred queries. The inferred high quality queries will be returned to the users to help them search with ease. Moreover, all the high-quality candidate nodes will be re-ranked according to the inferred queries. The experiment results show that our proposed method can recommend high-quality query graphs to users and improve the question answering accuracy. | Lihui Liu, Yuzhong Chen, Mahashweta Das, Hao Yang, Hanghang Tong | visa research, USA; Department of Computer Science, University of Illinois at Urbana Champaign, USA |
| 218 |  |  [Hierarchical Self-Attention Embedding for Temporal Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583397) |  | 0 | Temporal Knowledge Graph (TKG) is composed of a series of facts related to timestamps in the real world and has become the basis of many artificial intelligence applications. However, the existing TKG is usually incomplete. It has become a hot research task to infer missing facts based on existing facts in a TKG; namely, Temporal Knowledge Graph Completion (TKGC). The current mainstream TKGC models are embedded models that predict missing facts by representing entities, relations and timestamps as low-dimensional vectors. In order to deal with the TKG structure information, there are some models that try to introduce attention mechanism into the embedding process. But they only consider the structure information of entities or relations, and ignore the structure information of the whole TKG. Moreover, most of them usually treat timestamps as a general feature and cannot take advantage of the potential time series information of the timestamp. To solve these problems, wo propose a new Hierarchical Self-Attention Embedding (HSAE) model which inspired by self-attention mechanism and diachronic embedding technique. For structure information of the whole TKG, we divide the TKG into two layers: entity layer and relation layer, and then apply the self-attention mechanism to the entity layer and relation layer respectively to capture the structure information. For time series information of the timestamp, we capture them by combining positional encoding and diachronic embedding technique into the above two self-attention layers. Finally, we can get the embedded representation vectors of entities, relations and timestamps, which can be combined with other models for better results. We evaluate our model on three TKG datasets: ICEWS14, ICEWS05-15 and GDELT. Experimental results on the TKGC (interpolation) task demonstrate that our model achieves state-of-the-art results. | Xin Ren, Luyi Bai, Qianwen Xiao, Xiangxi Meng | Northeastern University, China |
| 219 |  |  [Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models](https://doi.org/10.1145/3543507.3583358) |  | 0 | Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to predict new links, scoring them according to geometric criteria. Relations in the graph may follow patterns that can be learned, e.g., some relations might be symmetric and others might be hierarchical. However, the learning capability of different embedding models varies for each pattern and, so far, no single model can learn all patterns equally well. In this paper, we combine the query representations from several models in a unified one to incorporate patterns that are independently captured by each model. Our combination uses attention to select the most suitable model to answer each query. The models are also mapped onto a non-Euclidean manifold, the Poincar\'e ball, to capture structural patterns, such as hierarchies, besides relational patterns, such as symmetry. We prove that our combination provides a higher expressiveness and inference power than each model on its own. As a result, the combined model can learn relational and structural patterns. We conduct extensive experimental analysis with various link prediction benchmarks showing that the combined model outperforms individual models, including state-of-the-art approaches. | Cosimo Gregucci, Mojtaba Nayyeri, Daniel Hernández, Steffen Staab | University of Stuttgart, Germany and University of Southampton, United Kingdom; University of Stuttgart, Germany |
| 220 |  |  [SeqCare: Sequential Training with External Medical Knowledge Graph for Diagnosis Prediction in Healthcare Data](https://doi.org/10.1145/3543507.3583543) |  | 0 | Deep learning techniques are capable of capturing complex input-output relationships, and have been widely applied to the diagnosis prediction task based on web-based patient electronic health records (EHR) data. To improve the prediction and interpretability of pure data-driven deep learning with only a limited amount of labeled data, a pervasive trend is to assist the model training with knowledge priors from online medical knowledge graphs. However, they marginally investigated the label imbalance and the task-irrelevant noise in the external knowledge graph. The imbalanced label distribution would bias the learning and knowledge extraction towards the majority categories. The task-irrelevant noise introduces extra uncertainty to the model performance. To this end, aiming at by-passing the bias-variance trade-off dilemma, we introduce a new sequential learning framework, dubbed SeqCare, for diagnosis prediction with online medical knowledge graphs. Concretely, in the first step, SeqCare learns a bias-reduced space through a self-supervised graph contrastive learning task. Secondly, SeqCare reduces the learning uncertainty by refining the supervision signal and the graph structure of the knowledge graph simultaneously. Lastly, SeqCare trains the model in the bias-variance reduced space with a self-distillation to further filter out irrelevant information in the data. Experimental evaluations on two real-world datasets show that SeqCare outperforms state-of-the-art approaches. Case studies exemplify the interpretability of SeqCare. Moreover, the medical findings discovered by SeqCare are consistent with experts and medical literature. | Yongxin Xu, Xu Chu, Kai Yang, Zhiyuan Wang, Peinie Zou, Hongxin Ding, Junfeng Zhao, Yasha Wang, Bing Xie | Zhongguancun Laboratory, China; Tsinghua University, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, China and Peking University, China |
| 221 |  |  [The Thin Ideology of Populist Advertising on Facebook during the 2019 EU Elections](https://doi.org/10.1145/3543507.3583267) |  | 0 | Social media has been an important tool in the expansion of the populist message, and it is thought to have contributed to the electoral success of populist parties in the past decade. This study compares how populist parties advertised on Facebook during the 2019 European Parliamentary election. In particular, we examine commonalities and differences in which audiences they reach and on which issues they focus. By using data from Meta (previously Facebook) Ad Library, we analyze 45k ad campaigns by 39 parties, both populist and mainstream, in Germany, United Kingdom, Italy, Spain, and Poland. While populist parties represent just over 20% of the total expenditure on political ads, they account for 40% of the total impressions$\unicode{x2013}$most of which from Eurosceptic and far-right parties$\unicode{x2013}$thus hinting at a competitive advantage for populist parties on Facebook. We further find that ads posted by populist parties are more likely to reach male audiences, and sometimes much older ones. In terms of issues, populist politicians focus on monetary policy, state bureaucracy and reforms, and security, while the focus on EU and Brexit is on par with non-populist, mainstream parties. However, issue preferences are largely country-specific, thus supporting the view in political science that populism is a "thin ideology", that does not have a universal, coherent policy agenda. This study illustrates the usefulness of publicly available advertising data for monitoring the populist outreach to, and engagement with, millions of potential voters, while outlining the limitations of currently available data. | Arthur Capozzi, Gianmarco De Francisci Morales, Yelena Mejova, Corrado Monti, André Panisson | ISI Foundation, Italy; Computer Science, Universita' di Torino, Italy; Centai, Italy |
| 222 |  |  [FlexiFed: Personalized Federated Learning for Edge Clients with Heterogeneous Model Architectures](https://doi.org/10.1145/3543507.3583347) |  | 0 | Mobile and Web-of-Things (WoT) devices at the network edge account for more than half of the world’s web traffic, making a great data source for various machine learning (ML) applications, particularly federated learning (FL) which offers a promising solution to privacy-preserving ML feeding on these data. FL allows edge mobile and WoT devices to train a shared global ML model under the orchestration of a central parameter server. In the real world, due to resource heterogeneity, these edge devices often train different versions of models (e.g., VGG-16 and VGG-19) or different ML models (e.g., VGG and ResNet) for the same ML task (e.g., computer vision and speech recognition). Existing FL schemes have assumed that participating edge devices share a common model architecture, and thus cannot facilitate FL across edge devices with heterogeneous ML model architectures. We explored this architecture heterogeneity challenge and found that FL can and should accommodate these edge devices to improve model accuracy and accelerate model training. This paper presents our findings and FlexiFed, a novel scheme for FL across edge devices with heterogeneous model architectures, and three model aggregation strategies for accommodating architecture heterogeneity under FlexiFed. Experiments with four widely-used ML models on four public datasets demonstrate 1) the usefulness of FlexiFed; and 2) that compared with the state-of-the-art FL scheme, FlexiFed improves model accuracy by 2.6%-9.7% and accelerates model convergence by 1.24 × -4.04 ×. | Kaibin Wang, Qiang He, Feifei Chen, Chunyang Chen, Faliang Huang, Hai Jin, Yun Yang | Nanning Normal University, China; Swinburne University of Technology, Australia; Huazhong University of Science and Technology, China; Deakin University, Australia; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Monash University, Australia |
| 223 |  |  [PipeEdge: A Trusted Pipelining Collaborative Edge Training based on Blockchain](https://doi.org/10.1145/3543507.3583413) |  | 0 | Powered by the massive data generated by the blossom of mobile and Web-of-Things (WoT) devices, Deep Neural Networks (DNNs) have developed both in accuracy and size in recent years. Conventional cloud-based DNN training incurs rapidly-increasing data and model transmission overheads as well as privacy issues. Mobile edge computing (MEC) provides a promising solution by facilitating DNN model training on edge servers at the network edge. However, edge servers often suffer from constrained resources and need to collaborate on DNN training. Unfortunately, managed by different telecoms, edge servers cannot properly collaborate with each other without incentives and trust. In this paper, we introduce PipeEdge, a scheme that promotes collaborative edge training between edge servers by introducing incentives and trust based on blockchain. Under the PipeEdge scheme, edge servers can hire trustworthy workers for pipelined DNN training tasks based on model parallelism. We implement PipeEdge and evaluate it comprehensively with four different DNN models. The results show that it outperforms state-of-the-art schemes by up to 173.98% with negligible overheads. | Liang Yuan, Qiang He, Feifei Chen, Ruihan Dou, Hai Jin, Yun Yang | Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; Deakin University, Australia; University of Waterloo, Canada; Huazhong University of Science and Technology, China |
| 224 |  |  [ELASTIC: Edge Workload Forecasting based on Collaborative Cloud-Edge Deep Learning](https://doi.org/10.1145/3543507.3583436) |  | 0 | With the rapid development of edge computing in the post-COVID19 pandemic period, precise workload forecasting is considered the basis for making full use of the edge limited resources, and both edge service providers (ESPs) and edge service consumers (ESCs) can benefit significantly from it. Existing paradigms of workload forecasting (i.e., edge-only or cloud-only) are improper, due to failing to consider the inter-site correlations and might suffer from significant data transmission delays. With the increasing adoption of edge platforms by web services, it is critical to balance both accuracy and efficiency in workload forecasting. In this paper, we propose ELASTIC, which is the first study that leverages a cloud-edge collaborative paradigm for edge workload forecasting with multi-view graphs. Specifically, at the global stage, we design a learnable aggregation layer on each edge site to reduce the time consumption while capturing the inter-site correlation. Additionally, at the local stage, we design a disaggregation layer combining both the intra-site correlation and inter-site correlation to improve the prediction accuracy. Extensive experiments on realistic edge workload datasets collected from China’s largest edge service provider show that ELASTIC outperforms state-of-the-art methods, decreases time consumption, and reduces communication cost. | Yanan Li, Haitao Yuan, Zhe Fu, Xiao Ma, Mengwei Xu, Shangguang Wang | Nanyang Technological University, Singapore; Beijing University of Posts and Telecommunications, China; Tsinghua University, China |
| 225 |  |  [DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization](https://doi.org/10.1145/3543507.3583451) |  | 0 | Device Model Generalization (DMG) is a practical yet under-investigated research topic for on-device machine learning applications. It aims to improve the generalization ability of pre-trained models when deployed on resource-constrained devices, such as improving the performance of pre-trained cloud models on smart mobiles. While quite a lot of works have investigated the data distribution shift across clouds and devices, most of them focus on model fine-tuning on personalized data for individual devices to facilitate DMG. Despite their promising, these approaches require on-device re-training, which is practically infeasible due to the overfitting problem and high time delay when performing gradient calculation on real-time data. In this paper, we argue that the computational cost brought by fine-tuning can be rather unnecessary. We consequently present a novel perspective to improving DMG without increasing computational cost, i.e., device-specific parameter generation which directly maps data distribution to parameters. Specifically, we propose an efficient Device-cloUd collaborative parametErs generaTion framework DUET. DUET is deployed on a powerful cloud server that only requires the low cost of forwarding propagation and low time delay of data transmission between the device and the cloud. By doing so, DUET can rehearse the device-specific model weight realizations conditioned on the personalized real-time data for an individual device. Importantly, our DUET elegantly connects the cloud and device as a 'duet' collaboration, frees the DMG from fine-tuning, and enables a faster and more accurate DMG paradigm. We conduct an extensive experimental study of DUET on three public datasets, and the experimental results confirm our framework's effectiveness and generalisability for different DMG tasks. | Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, Fei Wu | Zhejiang University, China; National University of Singapore, Singapore; Alibaba Group, China |
| 226 |  |  [RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems](https://doi.org/10.1145/3543507.3583313) |  | 0 | Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiveness of their approaches. This paper proposes a Reinforcement Learning (RL) based Multi-Phase Computation Allocation approach (RL-MPCA), which aims to maximize the total business revenue under the limitation of CRs. RL-MPCA formulates the CR allocation problem as a Weakly Coupled MDP problem and solves it with an RL-based approach. Specifically, RL-MPCA designs a novel deep Q-network to adapt to various CR allocation scenarios, and calibrates the Q-value by introducing multiple adaptive Lagrange multipliers (adaptive-λ) to avoid violating the global CR constraints. Finally, experiments on the offline simulation environment and online real-world recommender system validate the effectiveness of our approach. | Jiahong Zhou, Shunhui Mao, Guoliang Yang, Bo Tang, Qianlong Xie, Lebin Lin, Xingxing Wang, Dong Wang | Meituan, China |
| 227 |  |  [Learning To Rank Resources with GNN](https://doi.org/10.1145/3543507.3583360) |  | 0 | As the content on the Internet continues to grow, many new dynamically changing and heterogeneous sources of data constantly emerge. A conventional search engine cannot crawl and index at the same pace as the expansion of the Internet. Moreover, a large portion of the data on the Internet is not accessible to traditional search engines. Distributed Information Retrieval (DIR) is a viable solution to this as it integrates multiple shards (resources) and provides a unified access to them. Resource selection is a key component of DIR systems. There is a rich body of literature on resource selection approaches for DIR. A key limitation of the existing approaches is that they primarily use term-based statistical features and do not generally model resource-query and resource-resource relationships. In this paper, we propose a graph neural network (GNN) based approach to learning-to-rank that is capable of modeling resource-query and resource-resource relationships. Specifically, we utilize a pre-trained language model (PTLM) to obtain semantic information from queries and resources. Then, we explicitly build a heterogeneous graph to preserve structural information of query-resource relationships and employ GNN to extract structural information. In addition, the heterogeneous graph is enriched with resource-resource type of edges to further enhance the ranking accuracy. Extensive experiments on benchmark datasets show that our proposed approach is highly effective in resource selection. Our method outperforms the state-of-the-art by 6.4% to 42% on various performance metrics. | Ulugbek Ergashev, Eduard C. Dragut, Weiyi Meng | Computer Science, Binghamton University, USA; Computer and Information Sciences, Temple University, USA |
| 228 |  |  [CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval](https://doi.org/10.1145/3543507.3583369) |  | 0 | Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iteration of the deep hashing network, they are adopted to guide the adversarial training process. On the one hand, CgAT generates the worst adversarial examples as augmented data by maximizing the Hamming distance between the hash codes of the adversarial examples and the center codes. On the other hand, CgAT learns to mitigate the effects of adversarial samples by minimizing the Hamming distance to the center codes. Extensive experiments on the benchmark datasets demonstrate the effectiveness of our adversarial training algorithm in defending against adversarial attacks for deep hashing-based retrieval. Compared with the current state-of-the-art defense method, we significantly improve the defense performance by an average of 18.61\%, 12.35\%, and 11.56\% on FLICKR-25K, NUS-WIDE, and MS-COCO, respectively. The code is available at https://github.com/xunguangwang/CgAT. | Xunguang Wang, Yiqun Lin, Xiaomeng Li | The Hong Kong University of Science and Technology, China; The Hong Kong University of Science and Technology, China and The Hong Kong University of Science and Technology Shenzhen Research Institute, China |
| 229 |  |  [Algorithmic Vibe in Information Retrieval](https://doi.org/10.1145/3543507.3583384) |  | 0 | When information retrieval systems return a ranked list of results in response to a query, they may be choosing from a large set of candidate results that are equally useful and relevant. This means we might be able to identify a difference between rankers A and B, where ranker A systematically prefers a certain type of relevant results. Ranker A may have this systematic difference (different “vibe”) without having systematically better or worse results according to standard information retrieval metrics. We first show that a vibe difference can exist, comparing two publicly available rankers, where the one that is trained on health-related queries will systematically prefer health-related results, even for non-health queries. We define a vibe metric that lets us see the words that a ranker prefers. We investigate the vibe of search engine clicks vs. human labels. We perform an initial study into correcting for vibe differences to make ranker A more like ranker B via changes in negative sampling during training. | Ali Montazeralghaem, Nick Craswell, Ryen W. White, Ahmed Hassan Awadallah, Byungki Byun | University of Massachusetts Amherst, USA; Microsoft, USA |
| 230 |  |  [Geographic Information Retrieval Using Wikipedia Articles](https://doi.org/10.1145/3543507.3583469) |  | 0 | Assigning semantically relevant, real-world locations to documents opens new possibilities to perform geographic information retrieval. We propose a novel approach to automatically determine the latitude-longitude coordinates of appropriate Wikipedia articles with high accuracy, leveraging both text and metadata in the corpus. By examining articles whose base-truth coordinates are known, we show that our method attains a substantial improvement over state of the art works. We subsequently demonstrate how our approach could yield two benefits: (1) detecting significant geolocation errors in Wikipedia; and (2) proposing approximated coordinates for hundreds of thousands of articles which are not traditionally considered to be locations (such as events, ideas or people), opening new possibilities for conceptual geographic retrievals over Wikipedia. | Amir Krause, Sara Cohen | The Rachel and Selim Benin School of Computer Science and Engineering, The Hebrew University, Israel |
| 231 |  |  [Optimizing Guided Traversal for Fast Learned Sparse Retrieval](https://doi.org/10.1145/3543507.3583497) |  | 0 | Recent studies show that BM25-driven dynamic index skipping can greatly accelerate MaxScore-based document retrieval based on the learned sparse representation derived by DeepImpact. This paper investigates the effectiveness of such a traversal guidance strategy during top k retrieval when using other models such as SPLADE and uniCOIL, and finds that unconstrained BM25-driven skipping could have a visible relevance degradation when the BM25 model is not well aligned with a learned weight model or when retrieval depth k is small. This paper generalizes the previous work and optimizes the BM25 guided index traversal with a two-level pruning control scheme and model alignment for fast retrieval using a sparse representation. Although there can be a cost of increased latency, the proposed scheme is much faster than the original MaxScore method without BM25 guidance while retaining the relevance effectiveness. This paper analyzes the competitiveness of this two-level pruning scheme, and evaluates its tradeoff in ranking relevance and time efficiency when searching several test datasets. | Yifan Qiao, Yingrui Yang, Haixin Lin, Tao Yang | Department of Computer Science, University of California, Santa Barbara, USA; Department of Computer Science, University of California at Santa Barbara, USA |
| 232 |  |  [Learning with Exposure Constraints in Recommendation Systems](https://doi.org/10.1145/3543507.3583320) |  | 0 | Recommendation systems are dynamic economic systems that balance the needs of multiple stakeholders. A recent line of work studies incentives from the content providers' point of view. Content providers, e.g., vloggers and bloggers, contribute fresh content and rely on user engagement to create revenue and finance their operations. In this work, we propose a contextual multi-armed bandit setting to model the dependency of content providers on exposure. In our model, the system receives a user context in every round and has to select one of the arms. Every arm is a content provider who must receive a minimum number of pulls every fixed time period (e.g., a month) to remain viable in later rounds; otherwise, the arm departs and is no longer available. The system aims to maximize the users' (content consumers) welfare. To that end, it should learn which arms are vital and ensure they remain viable by subsidizing arm pulls if needed. We develop algorithms with sub-linear regret, as well as a lower bound that demonstrates that our algorithms are optimal up to logarithmic factors. | Omer BenPorat, Rotem Torkan | Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Israel |
| 233 |  |  [Stability and Efficiency of Personalised Cultural Markets](https://doi.org/10.1145/3543507.3583315) |  | 0 | This work is concerned with the dynamics of online cultural markets, namely, attention allocation of many users on a set of digital goods with infinite supply. Such dynamic is important in shaping processes and outcomes in society, from trending items in entertainment, collective knowledge creation, to election outcomes. The outcomes of online cultural markets are susceptible to intricate social influence dynamics, particularly so when the community comprises consumers with heterogeneous interests. This has made formal analysis of these markets improbable. In this paper, we remedy this by establishing robust connections between influence dynamics and optimization processes, in trial-offer markets where the consumer preferences are modelled by multinomial logit. Among other results, we show that the proportional-response-esque influence dynamic is equivalent to stochastic mirror descent on a convex objective function, thus leading to a stable and predictable outcome. When all consumers are homogeneous, the objective function has a natural interpretation as a weighted sum of efficiency and diversity of the culture market. In simulations driven by real-world preferences collected from a large-scale recommender system, we observe that ranking strategies aligned with the underlying heterogeneous preferences are more stable, and achieves higher efficiency and diversity. In simulations driven by real-world preferences collected from a large-scale recommender system, we observe that ranking strategies aligned with the underlying heterogeneous preferences are more stable, and achieves higher efficiency and diversity. | Haiqing Zhu, Yun Kuen Cheung, Lexing Xie | Royal Holloway University of London, United Kingdom; Australian National University, Australia |
| 234 |  |  [Eligibility Mechanisms: Auctions Meet Information Retrieval](https://doi.org/10.1145/3543507.3583478) |  | 0 | The design of internet advertisement systems is both an auction design problem and an information retrieval (IR) problem. As an auction, the designer needs to take the participants incentives into account. As an information retrieval problem, it needs to identify the ad that it is the most relevant to a user out of an enormous set of ad candidates. Those aspects are combined by first having an IR system narrow down the initial set of ad candidates to a manageable size followed by an auction that ranks and prices those candidates. If the IR system uses information about bids, agents could in principle manipulate the system by manipulating the IR stage even when the subsequent auction is truthful. In this paper we investigate the design of truthful IR mechanisms, which we term eligibility mechanisms. We model it as a truthful version of the stochastic probing problem. We show that there is a constant gap between the truthful and non-truthful versions of the stochastic probing problem and exhibit a constant approximation algorithm. En route, we also characterize the set of eligibility mechanisms, which provides necessary and sufficient conditions for an IR system to be truthful. | Gagan Goel, Renato Paes Leme, Jon Schneider, David Thompson, Hanrui Zhang | Carnegie Mellon University, USA; Google, USA |
| 235 |  |  [Scoping Fairness Objectives and Identifying Fairness Metrics for Recommender Systems: The Practitioners' Perspective](https://doi.org/10.1145/3543507.3583204) |  | 0 | Measuring and assessing the impact and “fairness’’ of recommendation algorithms is central to responsible recommendation efforts. However, the complexity of fairness definitions and the proliferation of fairness metrics in research literature have led to a complex decision-making space. This environment makes it challenging for practitioners to operationalize and pick metrics that work within their unique context. This suggests that practitioners require more decision-making support, but it is not clear what type of support would be beneficial. We conducted a literature review of 24 papers to gather metrics introduced by the research community for measuring fairness in recommendation and ranking systems. We organized these metrics into a ‘decision-tree style’ support framework designed to help practitioners scope fairness objectives and identify fairness metrics relevant to their recommendation domain and application context. To explore the feasibility of this approach, we conducted 15 semi-structured interviews using this framework to assess which challenges practitioners may face when scoping fairness objectives and metrics for their system, and which further support may be needed beyond such tools. | Jessie J. Smith, Lex Beattie, Henriette Cramer | Spotify, USA; University of Colorado, Boulder, USA |
| 236 |  |  [Same Same, But Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection](https://doi.org/10.1145/3543507.3583290) |  | 0 | Algorithmic bias often arises as a result of differential subgroup validity, in which predictive relationships vary across groups. For example, in toxic language detection, comments targeting different demographic groups can vary markedly across groups. In such settings, trained models can be dominated by the relationships that best fit the majority group, leading to disparate performance. We propose framing toxicity detection as multi-task learning (MTL), allowing a model to specialize on the relationships that are relevant to each demographic group while also leveraging shared properties across groups. With toxicity detection, each task corresponds to identifying toxicity against a particular demographic group. However, traditional MTL requires labels for all tasks to be present for every data point. To address this, we propose Conditional MTL (CondMTL), wherein only training examples relevant to the given demographic group are considered by the loss function. This lets us learn group specific representations in each branch which are not cross contaminated by irrelevant labels. Results on synthetic and real data show that using CondMTL improves predictive recall over various baselines in general and for the minority demographic group in particular, while having similar overall accuracy. | Soumyajit Gupta, Sooyong Lee, Maria DeArteaga, Matthew Lease |  |
| 237 |  |  [Towards Explainable Collaborative Filtering with Taste Clusters Learning](https://doi.org/10.1145/3543507.3583303) |  | 0 | Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decisionmaking process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements. In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model's explanations should truly reflect its decision-making process, not generated from post-hoc methods. The core of ECF is mining taste clusters from user-item interactions and item profiles.We map each user and item to a sparse set of taste clusters, and taste clusters are distinguished by a few representative tags. The user-item preference, users/items' cluster affiliations, and the generation of taste clusters are jointly optimized in an end-to-end manner. Additionally, we introduce a forest mechanism to ensure the model's accuracy, explainability, and diversity. To comprehensively evaluate the explainability quality of taste clusters, we design several quantitative metrics, including in-cluster item coverage, tag utilization, silhouette, and informativeness. Our model's effectiveness is demonstrated through extensive experiments on three real-world datasets. | Yuntao Du, Jianxun Lian, Jing Yao, Xiting Wang, Mingqi Wu, Lu Chen, Yunjun Gao, Xing Xie |  |
| 238 |  |  [Towards Fair Allocation in Social Commerce Platforms](https://doi.org/10.1145/3543507.3583398) |  | 0 | Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network. Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them. The enormous product space in such platforms prohibits manual search, and motivates the need for recommendation algorithms to effectively allocate product exposure and, consequently, earning opportunities. In this work, we focus on the fairness of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products. Our work systematically explores various well-studied benchmarks of fairness—including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1)—from both theoretical and experimental perspectives. We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model. To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets. Overall, our work takes the first step towards achieving provable fairness alongside reasonable revenue guarantees on social commerce platforms. | Anjali Gupta, Shreyans J. Nagori, Abhijnan Chakraborty, Rohit Vaish, Sayan Ranu, Prajit Prashant Sinai Nadkarni, Narendra Varma Dasararaju, Muthusamy Chelliah | Flipkart Internet Pvt. Ltd., India; Indian Institute of Technology Delhi, India |
| 239 |  |  [Fairness-Aware Clique-Preserving Spectral Clustering of Temporal Graphs](https://doi.org/10.1145/3543507.3583423) |  | 0 | With the widespread development of algorithmic fairness, there has been a surge of research interest that aims to generalize the fairness notions from the attributed data to the relational data (graphs). The vast majority of existing work considers the fairness measure in terms of the low-order connectivity patterns (e.g., edges), while overlooking the higher-order patterns (e.g., k-cliques) and the dynamic nature of real-world graphs. For example, preserving triangles from graph cuts during clustering is the key to detecting compact communities; however, if the clustering algorithm only pays attention to triangle-based compactness, then the returned communities lose the fairness guarantee for each group in the graph. Furthermore, in practice, when the graph (e.g., social networks) topology constantly changes over time, one natural question is how can we ensure the compactness and demographic parity at each timestamp efficiently. To address these problems, we start from the static setting and propose a spectral method that preserves clique connections and incorporates demographic fairness constraints in returned clusters at the same time. To make this static method fit for the dynamic setting, we propose two core techniques, Laplacian Update via Edge Filtering and Searching and Eigen-Pairs Update with Singularity Avoided. Finally, all proposed components are combined into an end-to-end clustering framework named F-SEGA, and we conduct extensive experiments to demonstrate the effectiveness, efficiency, and robustness of F-SEGA. | Dongqi Fu, Dawei Zhou, Ross Maciejewski, Arie Croitoru, Marcus Boyd, Jingrui He | University of Illinois at Urbana-Champaign, USA; Virginia Tech, USA; Arizona State University, USA; University of Maryland, College Park, USA; George Mason University, USA |
| 240 |  |  [HybridEval: A Human-AI Collaborative Approach for Evaluating Design Ideas at Scale](https://doi.org/10.1145/3543507.3583496) |  | 0 | Evaluating design ideas is necessary to predict their success and assess their impact early on in the process. Existing methods rely either on metrics computed by systems that are effective but subject to errors and bias, or experts’ ratings, which are accurate but expensive and long to collect. Crowdsourcing offers a compelling way to evaluate a large number of design ideas in a short amount of time while being cost-effective. Workers’ evaluation is, however, less reliable and might substantially differ from experts’ evaluation. In this work, we investigate workers’ rating behavior and compare it with experts. First, we instrument a crowdsourcing study where we asked workers to evaluate design ideas from three innovation challenges. We show that workers share similar insights with experts but tend to rate more generously and weigh certain criteria more importantly. Next, we develop a hybrid human-AI approach that combines a machine learning model with crowdsourcing to evaluate ideas. Our approach models workers’ reliability and bias while leveraging ideas’ textual content to train a machine learning model. It is able to incorporate experts’ ratings whenever available, to supervise the model training and infer worker performance. Results show that our framework outperforms baseline methods and requires significantly less training data from experts, thus providing a viable solution for evaluating ideas at scale. | Sepideh Mesbah, Ines Arous, Jie Yang, Alessandro Bozzon | University of Fribourg, Switzerland; Booking, Netherlands; Delft University of Technology, Netherlands |
| 241 |  |  [A Multi-task Model for Emotion and Offensive Aided Stance Detection of Climate Change Tweets](https://doi.org/10.1145/3543507.3583860) |  | 0 | In this work, we address the United Nations Sustainable Development Goal 13: Climate Action by focusing on identifying public attitudes toward climate change on social media platforms such as Twitter. Climate change is threatening the health of the planet and humanity. Public engagement is critical to address climate change. However, climate change conversations on Twitter tend to polarize beliefs, leading to misinformation and fake news that influence public attitudes, often dividing them into climate change believers and deniers. Our paper proposes an approach to classify the attitude of climate change tweets (believe/deny/ambiguous) to identify denier statements on Twitter. Most existing approaches for detecting stances and classifying climate change tweets either overlook deniers’ tweets or do not have a suitable architecture. The relevant literature suggests that emotions and higher levels of toxicity are prevalent in climate change Twitter conversations, leading to a delay in appropriate climate action. Therefore, our work focuses on learning stance detection (main task) while exploiting the auxiliary tasks of recognizing emotions and offensive utterances. We propose a multimodal multitasking framework MEMOCLiC that captures the input data using different embedding techniques and attention frameworks, and then incorporates the learned emotional and offensive expressions to obtain an overall representation of the features relevant to the stance of the input tweet. Extensive experiments conducted on a novel curated climate change dataset and two benchmark stance detection datasets (SemEval-2016 and ClimateStance-2022) demonstrate the effectiveness of our approach. | Apoorva Upadhyaya, Marco Fisichella, Wolfgang Nejdl | L3S Research Center, Leibniz University Hannover, Germany |
| 242 |  |  [Cross-center Early Sepsis Recognition by Medical Knowledge Guided Collaborative Learning for Data-scarce Hospitals](https://doi.org/10.1145/3543507.3583989) |  | 0 | There are significant regional inequities in health resources around the world. It has become one of the most focused topics to improve health services for data-scarce hospitals and promote health equity through knowledge sharing among medical institutions. Because electronic medical records (EMRs) contain sensitive personal information, privacy protection is unavoidable and essential for multi-hospital collaboration. In this paper, for a common disease in ICU patients, sepsis, we propose a novel cross-center collaborative learning framework guided by medical knowledge, SofaNet, to achieve early recognition of this disease. The Sepsis-3 guideline, published in 2016, defines that sepsis can be diagnosed by satisfying both suspicion of infection and Sequential Organ Failure Assessment (SOFA) greater than or equal to 2. Based on this knowledge, SofaNet adopts a multi-channel GRU structure to predict SOFA values of different systems, which can be seen as an auxiliary task to generate better health status representations for sepsis recognition. Moreover, we only achieve feature distribution alignment in the hidden space during cross-center collaborative learning, which ensures secure and compliant knowledge transfer without raw data exchange. Extensive experiments on two open clinical datasets, MIMIC-III and Challenge, demonstrate that SofaNet can benefit early sepsis recognition when hospitals only have limited EMRs. | Ruiqing Ding, Fangjie Rong, Xiao Han, Leye Wang | Peking University, China; Shanghai University of Finance and Economics, China |
| 243 |  |  [Breaking Filter Bubble: A Reinforcement Learning Framework of Controllable Recommender System](https://doi.org/10.1145/3543507.3583856) |  | 0 | In the information-overloaded era of the Web, recommender systems that provide personalized content filtering are now the mainstream portal for users to access Web information. Recommender systems deploy machine learning models to learn users’ preferences from collected historical data, leading to more centralized recommendation results due to the feedback loop. As a result, it will harm the ranking of content outside the narrowed scope and limit the options seen by users. In this work, we first conduct data analysis from a graph view to observe that the users’ feedback is restricted to limited items, verifying the phenomenon of centralized recommendation. We further develop a general simulation framework to derive the procedure of the recommender system, including data collection, model learning, and item exposure, which forms a loop. To address the filter bubble issue under the feedback loop, we then propose a general and easy-to-use reinforcement learning-based method, which can adaptively select few but effective connections between nodes from different communities as the exposure list. We conduct extensive experiments in the simulation framework based on large-scale real-world datasets. The results demonstrate that our proposed reinforcement learning-based control method can serve as an effective solution to alleviate the filter bubble and the separated communities induced by it. We believe the proposed framework of controllable recommendation in this work can inspire not only the researchers of recommender systems, but also a broader community concerned with artificial intelligence algorithms’ impact on humanity, especially for those vulnerable populations on the Web. | Zhenyang Li, Yancheng Dong, Chen Gao, Yizhou Zhao, Dong Li, Jianye Hao, Kai Zhang, Yong Li, Zhi Wang | Carnegie Mellon University, USA; Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen International Graduate School, China and Peng Cheng Laboratory, China; Tsinghua University, China and Huawei Noah's Ark Lab, China; Tsinghua Shenzhen International Graduate School, Tsinghua University, China and Research Institute of Tsinghua, Pearl River Delta, China; Tsinghua University, China; Huawei Noah's Ark Lab, China |
| 244 |  |  [CollabEquality: A Crowd-AI Collaborative Learning Framework to Address Class-wise Inequality in Web-based Disaster Response](https://doi.org/10.1145/3543507.3583871) |  | 0 | Web-based disaster response (WebDR) is emerging as a pervasive approach to acquire real-time situation awareness of disaster events by collecting timely observations from the Web (e.g., social media). This paper studies a class-wise inequality problem in WebDR applications where the objective is to address the limitation of current WebDR solutions that often have imbalanced classification performance across different classes. To address such a limitation, this paper explores the collaborative strengths of the diversified yet complementary biases of AI and crowdsourced human intelligence to ensure a more balanced and accurate performance for WebDR applications. However, two critical challenges exist: 1) it is difficult to identify the imbalanced AI results without knowing the ground-truth WebDR labels a priori; ii) it is non-trivial to address the class-wise inequality problem using potentially imperfect crowd labels. To address the above challenges, we develop CollabEquality, an inequality-aware crowd-AI collaborative learning framework that carefully models the inequality bias of both AI and human intelligence from crowdsourcing systems into a principled learning framework. Extensive experiments on two real-world WebDR applications demonstrate that CollabEquality consistently outperforms the state-of-the-art baselines by significantly reducing class-wise inequality while improving the WebDR classification accuracy. | Yang Zhang, Lanyu Shang, Ruohan Zong, Huimin Zeng, Zhenrui Yue, Dong Wang | School of Information Sciences, University of Illinois Urbana-Champaign, USA |
| 245 |  |  [MoleRec: Combinatorial Drug Recommendation with Substructure-Aware Molecular Representation Learning](https://doi.org/10.1145/3543507.3583872) |  | 0 | Combinatorial drug recommendation involves recommending a personalized combination of medication (drugs) to a patient over his/her longitudinal history, which essentially aims at solving a combinatorial optimization problem that pursues high accuracy under the safety constraint. Among existing learning-based approaches, the association between drug substructures (i.e., a sub-graph of the molecule that contributes to certain chemical effect) and the target disease is largely overlooked, though the function of drugs in fact exhibits strong relevance with particular substructures. To address this issue, we propose a molecular substructure-aware encoding method entitled MoleRec that entails a hierarchical architecture aimed at modeling inter-substructure interactions and individual substructures’ impact on patient’s health condition, in order to identify those substructures that really contribute to healing patients. Specifically, MoleRec learns to attentively pooling over substructure representations which will be element-wisely re-scaled by the model’s inferred relevancy with a patient’s health condition to obtain a prior-knowledge-informed drug representation. We further design a weight annealing strategy for drug-drug-interaction (DDI) objective to adaptively control the balance between accuracy and safety criteria throughout training. Experiments on the MIMIC-III dataset demonstrate that our approach achieves new state-of-the-art performance w.r.t. four accuracy and safety metrics. Our source code is publicly available at https://github.com/yangnianzu0515/MoleRec. | Nianzu Yang, Kaipeng Zeng, Qitian Wu, Junchi Yan | Shanghai Jiao Tong University, China |
| 246 |  |  [Moral Narratives Around the Vaccination Debate on Facebook](https://doi.org/10.1145/3543507.3583865) |  | 0 | Vaccine hesitancy is a complex issue with psychological, cultural, and even societal factors entangled in the decision-making process. The narrative around this process is captured in our everyday interactions; social media data offer a direct and spontaneous view of peoples' argumentation. Here, we analysed more than 500,000 public posts and comments from Facebook Pages dedicated to the topic of vaccination to study the role of moral values and, in particular, the understudied role of the Liberty moral foundation from the actual user-generated text. We operationalise morality by employing the Moral Foundations Theory, while our proposed framework is based on recurrent neural network classifiers with a short memory and entity linking information. Our findings show that the principal moral narratives around the vaccination debate focus on the values of Liberty, Care, and Authority. Vaccine advocates urge compliance with the authorities as prosocial behaviour to protect society. On the other hand, vaccine sceptics mainly build their narrative around the value of Liberty, advocating for the right to choose freely whether to adhere or not to the vaccination. We contribute to the automatic understanding of vaccine hesitancy drivers emerging from user-generated text, providing concrete insights into the moral framing around vaccination decision-making. Especially in emergencies such as the Covid-19 pandemic, contrary to traditional surveys, these insights can be provided contemporary to the event, helping policymakers craft communication campaigns that adequately address the concerns of the hesitant population. | Mariano Gastón Beiró, Jacopo D'Ignazi, Victoria Perez Bustos, Maria Florencia Prado, Kyriaki Kalimeri | ISI Foundation, Italy; Universidad de Buenos Aires. Facultad de Ingeniería, Paseo Colón 850, C1063ACV, Argentina and CONICET, Universidad de Buenos Aires, INTECIN, Paseo Colón 850, C1063ACV, Argentina |
| 247 |  |  [Exploration of Framing Biases in Polarized Online Content Consumption](https://doi.org/10.1145/3543873.3587534) |  | 0 | The study of framing bias on the Web is crucial in our digital age, as the framing of information can influence human behavior and decision on critical issues such as health or politics. Traditional frame analysis requires a curated set of frames derived from manual content analysis by domain experts. In this work, we introduce a frame analysis approach based on pretrained Transformer models that let us capture frames in an exploratory manner beyond predefined frames. In our experiments on two public online news and social media datasets, we show that our approach lets us identify underexplored conceptualizations, such as that health-related content is framed in terms of beliefs for conspiracy media, while mainstream media is instead concerned with science. We anticipate our work to be a starting point for further research on exploratory computational framing analysis using pretrained Transformers. | Markus ReiterHaas | Institute of Interactive Systems and Data Science, Graz University of Technology, Austria |
| 248 |  |  [On Modeling Long-Term User Engagement from Stochastic Feedback](https://doi.org/10.1145/3543873.3587626) |  | 0 | An ultimate goal of recommender systems (RS) is to improve user engagement. Reinforcement learning (RL) is a promising paradigm for this goal, as it directly optimizes overall performance of sequential recommendation. However, many existing RL-based approaches induce huge computational overhead, because they require not only the recommended items but also all other candidate items to be stored. This paper proposes an efficient alternative that does not require the candidate items. The idea is to model the correlation between user engagement and items directly from data. Moreover, the proposed approach consider randomness in user feedback and termination behavior, which are ubiquitous for RS but rarely discussed in RL-based prior work. With online A/B experiments on real-world RS, we confirm the efficacy of the proposed approach and the importance of modeling the two types of randomness. | Guoxi Zhang, Xing Yao, Xuanji Xiao | Graduate School of Informatics, Kyoto University, Japan; China Central Depository & Clearing Co., Ltd., China; Shopee Inc., China |
| 249 |  |  [CaML: Carbon Footprinting of Household Products with Zero-Shot Semantic Text Similarity](https://doi.org/10.1145/3543507.3583882) |  | 0 | Products contribute to carbon emissions in each phase of their life cycle, from manufacturing to disposal. Estimating the embodied carbon in products is a key step towards understanding their impact, and undertaking mitigation actions. Precise carbon attribution is challenging at scale, requiring both domain expertise and granular supply chain data. As a first-order approximation, standard reports use Economic Input-Output based Life Cycle Assessment (EIO-LCA) which estimates carbon emissions per dollar at an industry sector level using transactions between different parts of the economy. EIO-LCA models map products to an industry sector, and uses the corresponding carbon per dollar estimates to calculate the embodied carbon footprint of a product. An LCA expert needs to map each product to one of upwards of 1000 potential industry sectors. To reduce the annotation burden, the standard practice is to group products by categories, and map categories to their corresponding industry sector. We present CaML, an algorithm to automate EIO-LCA using semantic text similarity matching by leveraging the text descriptions of the product and the industry sector. CaML uses a pre-trained sentence transformer model to rank the top-5 matches, and asks a human to check if any of them are a good match. We annotated 40K products with non-experts. Our results reveal that pre-defined product categories are heterogeneous with respect to EIO-LCA industry sectors, and lead to a large mean absolute percentage error (MAPE) of 51% in kgCO2e/$. CaML outperforms the previous manually intensive method, yielding a MAPE of 22% with no domain labels (zero-shot). We compared annotations of a small sample of 210 products with LCA experts, and find that CaML accuracy is comparable to that of annotations by non-experts. | Bharathan Balaji, Venkata Sai Gargeya Vunnava, Geoffrey Guest, Jared Kramer | Amazon, USA |
| 250 |  |  [RDF Playground: An Online Tool for Learning about the Semantic Web](https://doi.org/10.1145/3543873.3587325) |  | 0 | We present RDF Playground: a web-based tool to assist those who wish to learn or teach about the Semantic Web. The tool integrates functionalities relating to the key features of RDF, allowing users to specify an RDF graph in Turtle syntax, visualise it as an interactive graph, query it using SPARQL, reason over it using OWL 2 RL, and to validate it using SHACL or ShEx. The tool further provides the ability to import and explore data from the Web through a graph-based Linked Data browser. We discuss the design and functionality of the tool, its implementation, and the results of a usability study considering students from a Web of Data course that used it for lab assignments. We conclude with a discussion of these results, as well as future directions that we envisage for improving the tool. | Bastián Inostroza, Raúl Cid, Aidan Hogan | DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile |
| 251 |  |  [Locating Faulty Applications via Semantic and Topology Estimation](https://doi.org/10.1145/3543873.3584660) |  | 0 | With the explosion of Internet product users, how to locate the faulty ones from numerous back-end applications after a customer complaint has become an essential issue in improving user experience. However, existing solutions mostly rely on manual testing to infer the fault, severely limiting their efficiency. In this paper, we transform the problem of locating faulty applications into two subproblems and propose a fully automated framework. We design a scorecard model in one stage to evaluate the semantic relevance between applications and customer complaints. Then in the other stage, topology graphs that reflect the actual calling relationship and engineering connection relationship between applications are utilized to evaluate the topology relevance between applications. Specifically, we employ a multi-graph co-learning framework constrained by consistency-independence loss and an engineering-theory-driven clustering strategy for the unsupervised learning of graphs. With semantic and topology relevance, we can comprehensively locate relevant faulty applications. Experiments on the Alipay dataset show that our method gains significant improvements in both model performance and efficiency. | Shuyi Niu, Jiawei Jin, Xiutian Huang, Yonggeng Wang, Wenhao Xu, Youyong Kong | Ant Group, China; Southeast University, China |
| 252 |  |  [Analyzing COVID-Related Social Discourse on Twitter using Emotion, Sentiment, Political Bias, Stance, Veracity and Conspiracy Theories](https://doi.org/10.1145/3543873.3587622) |  | 0 | Online misinformation has become a major concern in recent years, and it has been further emphasized during the COVID-19 pandemic. Social media platforms, such as Twitter, can be serious vectors of misinformation online. In order to better understand the spread of these fake-news, lies, deceptions, and rumours, we analyze the correlations between the following textual features in tweets: emotion, sentiment, political bias, stance, veracity and conspiracy theories. We train several transformer-based classifiers from multiple datasets to detect these textual features and identify potential correlations using conditional distributions of the labels. Our results show that the online discourse regarding some topics, such as COVID-19 regulations or conspiracy theories, is highly controversial and reflects the actual U.S. political landscape. | Youri Peskine, Raphaël Troncy, Paolo Papotti | EURECOM, France |
| 253 |  |  [Machine Learning for Streaming Media](https://doi.org/10.1145/3543873.3589751) |  | 0 | Streaming media has become a popular medium for consumers of all ages, with people spending several hours a day streaming videos, games, music, or podcasts across devices. Most global streaming services have introduced Machine Learning (ML) into their operations to personalize consumer experience, improve content, and further enhance the value proposition of streaming services. Despite the rapid growth, there is a need to bridge the gap between academic research and industry requirements and build connections between researchers and practitioners in the field. This workshop aims to provide a unique forum for practitioners and researchers interested in Machine Learning to get together, exchange ideas and get a pulse for the state of the art in research and burning issues in the industry. | Sudarshan Lamkhede, Praveen Chandar, Vladan Radosavljevic, Amit Goyal, Lan Luo | Amazon Music, USA; Spotify, USA; Netflix Research, USA; University of Southern California, USA |
| 254 |  |  [Interleaved Online Testing in Large-Scale Systems](https://doi.org/10.1145/3543873.3587572) |  | 0 | Online testing is indispensable in decision making for information retrieval systems. Interleaving emerges as an online testing method with orders of magnitude higher sensitivity than the pervading A/B testing. It merges the compared results into a single interleaved result to show to users, and attributes user actions back to the systems being tested. However, its pairwise design also brings practical challenges to real-world systems, in terms of effectively comparing multiple (more than two) systems and interpreting the magnitude of raw interleaving measurement. We present two novel methods to address these challenges that make interleaving practically applicable. The first method infers the ordering of multiple systems based on interleaving pairwise results with false discovery control. The second method estimates A/B effect size based on interleaving results using a weighted linear model that adjust for uncertainties of different measurements. We showcase the effectiveness of our methods in large-scale e-commerce experiments, reporting as many as 75 interleaving results, and provide extensive evaluations of their underlying assumptions. | Nan Bi, Bai Li, Ruoyuan Gao, Graham Edge, Sachin Ahuja | Amazon, USA |
| 255 |  |  [Impact of COVID-19 Pandemic on Cultural Products Interests](https://doi.org/10.1145/3543873.3587594) |  | 0 | The COVID-19 pandemic has had a significant impact on human behaviors and how it influenced peoples’ interests in cultural products is an unsolved problem. While prior studies mostly adopt subjective surveys to find an answer, these methods are always suffering from high cost, limited size, and subjective bias. Inspired by the rich user-oriented data over the Internet, this work explores the possibility to leverage users’ search logs to reflect humans’ underlying cultural product interests. To further examine how the COVID-19 mobility policy might influence cultural interest changes, we propose a new regression discontinuity design that has the additional potential to predict the recovery phase of peoples’ cultural product interests. By analyzing the 1592 search interest time series in 6 countries, we found different patterns of change in interest in movies, music, and art during the COVID-19 pandemic, but a clear overall incremental increase. Across the six countries we studied, we found that changes in interest in cultural products were found to be strongly correlated with mobility and that as mobility declined, interest in movies, music, and art increased by an average of 35, 27 and 20, respectively, with these changes lasting at least eight weeks. | Ke Li, Zhiwen Yu, Ying Zhang, Bin Guo | School of Computer Science, Northwestern Polytechnical University, China; School of Computer Science, Northwestern Polytechnical University, China and Harbin Engineering University, China |
| 256 |  |  [Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective](https://doi.org/10.1145/3543873.3587658) |  | 0 | Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing metadata and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be significantly improved. This involves automatically generating and updating metadata, which results in a more flexible vocabulary that can accommodate the diverse terminologies used by different sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community. | Zeyd Boukhers, Christoph Lange, Oya Beyan | Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany and Fraunhofer Institute for Applied Information Technology, Germany; Fraunhofer Institute for Applied Information Technology, Germany and Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany; Fraunhofer Institute for Applied Information Technology, Germany and RWTH Aachen University, Germany |
| 257 |  |  [The PLASMA Framework: Laying the Path to Domain-Specific Semantics in Dataspaces](https://doi.org/10.1145/3543873.3587662) |  | 0 | Modern data management is evolving from centralized integration-based solutions to a non-integration-based process of finding, accessing and processing data, as observed within dataspaces. Common reference dataspace architectures assume that sources publish their own domain-specific schema. These schemas, also known as semantic models, can only be partially created automatically and require oversight and refinement by human modellers. Non-expert users, such as mechanical engineers or municipal workers, often have difficulty building models because they are faced with multiple ontologies, classes, and relations, and existing tools are not designed for non-expert users. The PLASMA framework consists of a platform and auxiliary services that focus on providing non-expert users with an accessible way to create and edit semantic models, combining automation approaches and support systems such as a recommendation engine. It also provides data conversion from raw data to RDF. In this paper we highlight the main features, like the modeling interface and the data conversion engine. We discuss how PLASMA as a tool is suitable for building semantic models by non-expert users in the context of dataspaces and show some applications where PLASMA has already been used in data management projects. | Alexander Paulus, André Pomp, Tobias Meisen | Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany |
| 258 |  |  [Pairwise-interactions-based Bayesian Inference of Network Structure from Information Cascades](https://doi.org/10.1145/3543507.3583231) |  | 0 | An explicit network structure plays an important role when analyzing and understanding diffusion processes. In many scenarios, however, the interactions between nodes in an underlying network are unavailable. Although many methods for inferring a network structure from observed cascades have been proposed, they did not perceive the relationship between pairwise interactions in a cascade. Therefore, this paper proposes a Pairwise-interactions-based Bayesian Inference method (named PBI) to infer the underlying diffusion network structure. More specifically, to get more accurate inference results, we measure the weights of each candidate pairwise interaction in different cascades and add them to the likelihood of a contagion process. In addition, a pre-pruning work is introduced for candidate edges to further improve the inference efficiency. Experiments on synthetic and real-world networks show that PBI achieves significantly better results. | Chao Gao, Yuchen Wang, Zhen Wang, Xianghua Li, Xuelong Li | School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, China; Northwestern Polytechnical University, China |
| 259 |  |  [Graph Neural Network with Two Uplift Estimators for Label-Scarcity Individual Uplift Modeling](https://doi.org/10.1145/3543507.3583368) |  | 0 | Uplift modeling aims to measure the incremental effect, which we call uplift, of a strategy or action on the users from randomized experiments or observational data. Most existing uplift methods only use individual data, which are usually not informative enough to capture the unobserved and complex hidden factors regarding the uplift. Furthermore, uplift modeling scenario usually has scarce labeled data, especially for the treatment group, which also poses a great challenge for model training. Considering that the neighbors’ features and the social relationships are very informative to characterize a user’s uplift, we propose a graph neural network-based framework with two uplift estimators, called GNUM, to learn from the social graph for uplift estimation. Specifically, we design the first estimator based on a class-transformed target. The estimator is general for all types of outcomes, and is able to comprehensively model the treatment and control group data together to approach the uplift. When the outcome is discrete, we further design the other uplift estimator based on our defined partial labels, which is able to utilize more labeled data from both the treatment and control groups, to further alleviate the label scarcity problem. Comprehensive experiments on a public dataset and two industrial datasets show a superior performance of our proposed framework over state-of-the-art methods under various evaluation metrics. The proposed algorithms have been deployed online to serve real-world uplift estimation scenarios. | Dingyuan Zhu, Daixin Wang, Zhiqiang Zhang, Kun Kuang, Yan Zhang, Yulin Kang, Jun Zhou | Ant Group, China; Zhejiang university, China |
| 260 |  |  [Multi-head Variational Graph Autoencoder Constrained by Sum-product Networks](https://doi.org/10.1145/3543507.3583517) |  | 0 | Variational graph autoencoder (VGAE) is a promising deep probabilistic model in graph representation learning. However, most existing VGAEs adopt the mean-field assumption, and cannot characterize the graphs with noise well. In this paper, we propose a novel deep probabilistic model for graph analysis, termed Multi-head Variational Graph Autoencoder Constrained by Sum-product Networks (named SPN-MVGAE), which helps to relax the mean-field assumption and learns better latent representation with fault tolerance. Our proposed model SPN-MVGAE uses conditional sum-product networks as constraints to learn the dependencies between latent factors in an end-to-end manner. Furthermore, we introduce the superposition of the latent representations learned by multiple variational networks to represent the final latent representations of nodes. Our model is the first use sum-product networks for graph representation learning, extending the scope of sum-product networks applications. Experimental results show that compared with other baseline methods, our model has competitive advantages in link prediction, fault tolerance, node classification, and graph visualization on real datasets. | Riting Xia, Yan Zhang, Chunxu Zhang, Xueyan Liu, Bo Yang | Jilin University, China |
| 261 |  |  [Interactive Log Parsing via Light-weight User Feedback](https://doi.org/10.1145/3543507.3583456) |  | 0 | Template mining is one of the foundational tasks to support log analysis, which supports the diagnosis and troubleshooting of large scale Web applications. This paper develops a human-in-the-loop template mining framework to support interactive log analysis, which is highly desirable in real-world diagnosis or troubleshooting of Web applications but yet previous template mining algorithms fails to support it. We formulate three types of light-weight user feedbacks and based on them we design three atomic human-in-the-loop template mining algorithms. We derive mild conditions under which the outputs of our proposed algorithms are provably correct. We also derive upper bounds on the computational complexity and query complexity of each algorithm. We demonstrate the versatility of our proposed algorithms by combining them to improve the template mining accuracy of five representative algorithms over sixteen widely used benchmark datasets. | Liming Wang, Hong Xie, Ye Li, Jian Tan, John C. S. Lui | Alibaba, China; College of Computer Science, Chongqing University, China; Alibaba, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong |
| 262 |  |  [Misbehavior and Account Suspension in an Online Financial Communication Platform](https://doi.org/10.1145/3543507.3583385) |  | 0 | The expanding accessibility and appeal of investing have attracted millions of new retail investors. As such, investment discussion boards became the de facto communities where traders create, disseminate, and discuss investing ideas. These communities, which can provide useful information to support investors, have anecdotally also attracted a wide range of misbehavior – toxicity, spam/fraud, and reputation manipulation. This paper is the first comprehensive analysis of online misbehavior in the context of investment communities. We study TradingView, the largest online communication platform for financial trading. We collect 2.76M user profiles with their corresponding social graphs, 4.2M historical article posts, and 5.3M comments, including information on nearly 4 000 suspended accounts and 17 000 removed comments. Price fluctuations seem to drive abuse across the platform and certain types of assets, such as “meme” stocks, attract disproportionate misbehavior. Suspended user accounts tend to form more closely-knit communities than those formed by non-suspended accounts; and paying accounts are less likely to be suspended than free accounts even when posting similar levels of content violating platform policies. We conclude by offering guidelines on how to adapt content moderation efforts to fit the particularities of online investment communities. | Taro Tsuchiya, Alejandro Cuevas, Thomas Magelinski, Nicolas Christin | Carnegie Mellon University, USA |
| 263 |  |  [BiSR: Bidirectionally Optimized Super-Resolution for Mobile Video Streaming](https://doi.org/10.1145/3543507.3583519) |  | 0 | The user experience of mobile web video streaming is often impacted by insufficient and dynamic network bandwidth. In this paper, we design Bidirectionally Optimized Super-Resolution (BiSR) to improve the quality of experience (QoE) for mobile web users under limited bandwidth. BiSR exploits a deep neural network (DNN)-based model to super-resolve key frames efficiently without changing the inter-frame spatial-temporal information. We then propose a downscaling DNN and a mobile-specific optimized lightweight super-resolution DNN to enhance the performance. Finally, a novel reinforcement learning-based adaptive bitrate (ABR) algorithm is proposed to verify the performance of BiSR on real network traces. Our evaluation, using a full system implementation, shows that BiSR saves 26% of bitrate compared to the traditional H.264 codec and improves the SSIM of video by 3.7% compared to the prior state-of-the-art. Overall, BiSR enhances the user-perceived quality of experience by up to 30.6%. | Qian Yu, Qing Li, Rui He, Gareth Tyson, Wanxin Shi, Jianhui Lv, Zhenhui Yuan, Peng Zhang, Yulong Lan, Zhicheng Li | SUSTech, China and Peng Cheng Laboratory, China; Northumbria University, United Kingdom; Peng Cheng Laboratory, China; International Graduate School, Tsinghua University, China; Tencent, China; Hong Kong University of Science and Technology(GZ), China |
| 264 |  |  [Autobidding Auctions in the Presence of User Costs](https://doi.org/10.1145/3543507.3583234) |  | 0 | We study autobidding ad auctions with user costs, where each bidder is value-maximizing subject to a return-over-investment (ROI) constraint, and the seller aims to maximize the social welfare taking into consideration the user's cost of viewing an ad. We show that in the worst case, the approximation ratio of social welfare by running the vanilla VCG auctions with user costs could as bad as 0. To improve the performance of VCG, We propose a new variant of VCG based on properly chosen cost multipliers, and prove that there exist auction-dependent and bidder-dependent cost multipliers that guarantee approximation ratios of 1/2 and 1/4 respectively in terms of the social welfare. | Yuan Deng, Jieming Mao, Vahab Mirrokni, Hanrui Zhang, Song Zuo | Carnegie Mellon University, USA; Google Research, USA |
| 265 |  |  [Online Bidding Algorithms for Return-on-Spend Constrained Advertisers✱](https://doi.org/10.1145/3543507.3583491) |  | 0 | Online advertising has recently grown into a highly competitive and complex multi-billion-dollar industry, with advertisers bidding for ad slots at large scales and high frequencies. This has resulted in a growing need for efficient "auto-bidding" algorithms that determine the bids for incoming queries to maximize advertisers' targets subject to their specified constraints. This work explores efficient online algorithms for a single value-maximizing advertiser under an increasingly popular constraint: Return-on-Spend (RoS). We quantify efficiency in terms of regret relative to the optimal algorithm, which knows all queries a priori. We contribute a simple online algorithm that achieves near-optimal regret in expectation while always respecting the specified RoS constraint when the input sequence of queries are i.i.d. samples from some distribution. We also integrate our results with the previous work of Balseiro, Lu, and Mirrokni [BLM20] to achieve near-optimal regret while respecting both RoS and fixed budget constraints. Our algorithm follows the primal-dual framework and uses online mirror descent (OMD) for the dual updates. However, we need to use a non-canonical setup of OMD, and therefore the classic low-regret guarantee of OMD, which is for the adversarial setting in online learning, no longer holds. Nonetheless, in our case and more generally where low-regret dynamics are applied in algorithm design, the gradients encountered by OMD can be far from adversarial but influenced by our algorithmic choices. We exploit this key insight to show our OMD setup achieves low regret in the realm of our algorithm. | Zhe Feng, Swati Padmanabhan, Di Wang | University of Washington, Seattle, USA; Google Research, USA |
| 266 |  |  [EDNet: Attention-Based Multimodal Representation for Classification of Twitter Users Related to Eating Disorders](https://doi.org/10.1145/3543507.3583863) |  | 0 | Social media platforms provide rich data sources in several domains. In mental health, individuals experiencing an Eating Disorder (ED) are often hesitant to seek help through conventional healthcare services. However, many people seek help with diet and body image issues on social media. To better distinguish at-risk users who may need help for an ED from those who are simply commenting on ED in social environments, highly sophisticated approaches are required. Assessment of ED risks in such a situation can be done in various ways, and each has its own strengths and weaknesses. Hence, there is a need for and potential benefit of a more complex multimodal approach. To this end, we collect historical tweets, user biographies, and online behaviours of relevant users from Twitter, and generate a reasonably large labelled benchmark dataset. Thereafter, we develop an advanced multimodal deep learning model called EDNet using these data to identify the different types of users with ED engagement (e.g., potential ED sufferers, healthcare professionals, or communicators) and distinguish them from those not experiencing EDs on Twitter. EDNet consists of five deep neural network layers. With the help of its embedding, representation and behaviour modeling layers, it effectively learns the multimodalities of social media. In our experiments, EDNet consistently outperforms all the baseline techniques by significant margins. It achieves an accuracy of up to 94.32% and F1 score of up to 93.91% F1 score. To the best of our knowledge, this is the first such study to propose a multimodal approach for user-level classification according to their engagement with ED content on social media. | Mohammad Abuhassan, Tarique Anwar, Chengfei Liu, Hannah K. Jarman, Matthew FullerTyszkiewicz | Swinburne University of Technology, Australia; Deakin University, Australia; University of York, United Kingdom |
| 267 |  |  [C-Affinity: A Novel Similarity Measure for Effective Data Clustering](https://doi.org/10.1145/3543873.3587307) |  | 0 | Clustering is widely employed in various applications as it is one of the most useful data mining techniques. In performing clustering, a similarity measure, which defines how similar a pair of data objects are, plays an important role. A similarity measure is employed by considering a target dataset’s characteristics. Current similarity measures (or distances) do not reflect the distribution of data objects in a dataset at all. From the clustering point of view, this fact may limit the clustering accuracy. In this paper, we propose c-affinity, a new notion of a similarity measure that reflects the distribution of objects in the given dataset from a clustering point of view. We design c-affinity between any two objects to have a higher value as they are more likely to belong to the same cluster by learning the data distribution. We use random walk with restart (RWR) on the k-nearest neighbor graph of the given dataset to measure (1) how similar a pair of objects are and (2) how densely other objects are distributed between them. Via extensive experiments on sixteen synthetic and real-world datasets, we verify that replacing the existing similarity measure with our c-affinity improves the clustering accuracy significantly. | Jiwon Hong, SangWook Kim | Hanyang University, Republic of Korea |
| 268 |  |  [Knowledge Distillation on Cross-Modal Adversarial Reprogramming for Data-Limited Attribute Inference](https://doi.org/10.1145/3543873.3587313) |  | 0 | Social media generates a rich source of text data with intrinsic user attributes (e.g., age, gender), where different parties benefit from disclosing them. Attribute inference can be cast as a text classification problem, which, however, suffers from labeled data scarcity. To address this challenge, we propose a data-limited learning model to distill knowledge on adversarial reprogramming of a visual transformer (ViT) for attribute inferences. Not only does this novel cross-modal model transfers the powerful learning capability from ViT, but also leverages unlabeled texts to reduce the demand on labeled data. Experiments on social media datasets demonstrate the state-of-the-art performance of our model on data-limited attribute inferences. | Quan Li, Lingwei Chen, Shixiong Jing, Dinghao Wu | Wright State University, USA; Pennsylvania State University, USA |
| 269 |  |  [Copyright Protection and Accountability of Generative AI: Attack, Watermarking and Attribution](https://doi.org/10.1145/3543873.3587321) |  | 0 |  | Haonan Zhong, Jiamin Chang, Ziyue Yang, Tingmin Wu, Pathum Chamikara Mahawaga Arachchige, Chehara Pathmabandu, Minhui Xue |  |
| 270 |  |  [How Streaming Can Improve the World (Wide Web)](https://doi.org/10.1145/3543873.3587332) |  | 0 | Since its beginnings, web pages have been based on files. This means that HTML, CSS, and JavaScript are transferred from server to client as files, which by default need to be fully loaded before the web page is displayed. This render-blocking procedure increases loading times significantly, leading to reduced user satisfaction and revenue loss due to lower conversion rates. We present a full implementation of a new approach for loading web pages by splitting up every component and loading the page via a text-based stream. Such a modification aligns with current trends of the HTTP protocol, which has been using streams internally since HTTP/2. It significantly improves loading times, independent of the total page size. | Lucas Vogel, Thomas Springer | TU Dresden, Germany |
| 271 |  |  [PyPoll: A python library automating mining of networks, discussions and polarization on Twitter](https://doi.org/10.1145/3543873.3587349) |  | 0 | Today online social networks have a high impact in our society as more and more people use them for communicating with each other, express their opinions, participating in public discussions, etc. In particular, Twitter is one of the most popular social network platforms people mainly use for political discussions. This attracted the interest of many research studies that analyzed social phenomena on Twitter, by collecting data, analysing communication patterns, and exploring the structure of user networks. While previous works share many common methodologies for data collection and analysis, these are mainly re-implemented every time by researchers in a custom way. In this paper, we introduce PyPoll an open-source Python library that operationalizes common analysis tasks for Twitter discussions. With PyPoll users can perform Twitter graph mining, calculate the polarization index and generate interactive visualizations without needing third-party tools. We believe that PyPoll can help researchers automate their tasks by giving them methods that are easy to use. Also, we demonstrate the use of the library by presenting two use cases; the PyPoll visualization app, an online application for graph visualizing and sharing, and the Political Lighthouse, a Web portal for displaying the polarization in various political topics on Twitter. | Dimitrios Panteleimon Giakatos, Pavlos Sermpezis, Athena Vakali | Aristotle University of Thessaloniki, Greece |
| 272 |  |  [WebSHAP: Towards Explaining Any Machine Learning Models Anywhere](https://doi.org/10.1145/3543873.3587362) |  | 0 | As machine learning (ML) is increasingly integrated into our everyday Web experience, there is a call for transparent and explainable web-based ML. However, existing explainability techniques often require dedicated backend servers, which limit their usefulness as the Web community moves toward in-browser ML for lower latency and greater privacy. To address the pressing need for a client-side explainability solution, we present WebSHAP, the first in-browser tool that adapts the state-of-the-art model-agnostic explainability technique SHAP to the Web environment. Our open-source tool is developed with modern Web technologies such as WebGL that leverage client-side hardware capabilities and make it easy to integrate into existing Web ML applications. We demonstrate WebSHAP in a usage scenario of explaining ML-based loan approval decisions to loan applicants. Reflecting on our work, we discuss the opportunities and challenges for future research on transparent Web ML. WebSHAP is available at https://github.com/poloclub/webshap. | Zijie J. Wang, Duen Horng Chau | Georgia Institute of Technology, USA |
| 273 |  |  [Privacy-Preserving Online Content Moderation: A Federated Learning Use Case](https://doi.org/10.1145/3543873.3587604) |  | 0 | Users are daily exposed to a large volume of harmful content on various social network platforms. One solution is developing online moderation tools using Machine Learning techniques. However, the processing of user data by online platforms requires compliance with privacy policies. Federated Learning (FL) is an ML paradigm where the training is performed locally on the users' devices. Although the FL framework complies, in theory, with the GDPR policies, privacy leaks can still occur. For instance, an attacker accessing the final trained model can successfully perform unwanted inference of the data belonging to the users who participated in the training process. In this paper, we propose a privacy-preserving FL framework for online content moderation that incorporates Differential Privacy (DP). To demonstrate the feasibility of our approach, we focus on detecting harmful content on Twitter - but the overall concept can be generalized to other types of misbehavior. We simulate a text classifier - in FL fashion - which can detect tweets with harmful content. We show that the performance of the proposed FL framework can be close to the centralized approach - for both the DP and non-DP FL versions. Moreover, it has a high performance even if a small number of clients (each with a small number of data points) are available for the FL training. When reducing the number of clients (from 50 to 10) or the data points per client (from 1K to 0.1K), the classifier can still achieve ~81% AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC). Finally, we explore the overhead on the users' devices during the FL training phase and show that the local training does not introduce excessive CPU utilization and memory consumption overhead. | Pantelitsa Leonidou, Nicolas Kourtellis, Nikos Salamanos, Michael Sirivianos | Telefonica Research, Spain; Cyprus University of Technology, Cyprus |
| 274 |  |  [Intent-based Web Page Summarization with Structure-Aware Chunking and Generative Language Models](https://doi.org/10.1145/3543873.3587372) |  | 0 | This paper introduces a structure-aware method to segment web pages into chunks based on their web structures. We utilize large language models to select chunks correspond to a given intent and generate the abstractive summary. Experiments on a food pantry dataset developed for mitigating food insecurity show that the proposed framework is promising. | HuanYuan Chen, Hong Yu | School of Computer and Information Sciences, University of Massachusetts Lowell, USA; College of Information and Computer Sciences, University of Massachusetts Amherst, USA |
| 275 |  |  [Measuring and Detecting Virality on Social Media: The Case of Twitter's Viral Tweets Topic](https://doi.org/10.1145/3543873.3587373) |  | 0 | Social media posts may go viral and reach large numbers of people within a short period of time. Such posts may threaten the public dialogue if they contain misleading content, making their early detection highly crucial. Previous works proposed their own metrics to annotate if a tweet is viral or not in order to automatically detect them later. However, such metrics may not accurately represent viral tweets or may introduce too many false positives. In this work, we use the ground truth data provided by Twitter's "Viral Tweets" topic to review the current metrics and also propose our own metric. We find that a tweet is more likely to be classified as viral by Twitter if the ratio of retweets to its author's followers exceeds some threshold. We found this threshold to be 2.16 in our experiments. This rule results in less false positives although it favors smaller accounts. We also propose a transformers-based model to early detect viral tweets which reports an F1 score of 0.79. The code and the tweet ids are publicly available at: https://github.com/tugrulz/ViralTweets | Tugrulcan Elmas, Stephane Selim, Célia Houssiaux | EPFL, Switzerland; Indiana University Bloomington, USA |
| 276 |  |  [Anytime-Valid Confidence Sequences in an Enterprise A/B Testing Platform](https://doi.org/10.1145/3543873.3584635) |  | 0 | A/B tests are the gold standard for evaluating digital experiences on the web. However, traditional "fixed-horizon" statistical methods are often incompatible with the needs of modern industry practitioners as they do not permit continuous monitoring of experiments. Frequent evaluation of fixed-horizon tests ("peeking") leads to inflated type-I error and can result in erroneous conclusions. We have released an experimentation service on the Adobe Experience Platform based on anytime-valid confidence sequences, allowing for continuous monitoring of the A/B test and data-dependent stopping. We demonstrate how we adapted and deployed asymptotic confidence sequences in a full featured A/B testing platform, describe how sample size calculations can be performed, and how alternate test statistics like "lift" can be analyzed. On both simulated data and thousands of real experiments, we show the desirable properties of using anytime-valid methods instead of traditional approaches. | Akash Maharaj, Ritwik Sinha, David Arbour, Ian WaudbySmith, Simon Z. Liu, Moumita Sinha, Raghavendra Addanki, Aaditya Ramdas, Manas Garg, Viswanathan Swaminathan | Carnegie Mellon University, USA; Adobe, USA; Adobe Research, USA |
| 277 |  |  [Contrastive Fine-tuning on Few Shot Intent Detection with Topological Intent Tree](https://doi.org/10.1145/3543873.3584648) |  | 0 | We present a few-shot intent detection model for an enterprise’s conversational dialogue system. The model uses an intent topological tree to guide the search for the user intent using large language models (LLMs). The intents are resolved based on semantic similarities between user utterances and the text descriptions of the internal nodes of the intent tree or the intent examples in the leaf nodes of the tree. Our results show that an off-the-shelf language model can work reasonably well in a large enterprise deployment without fine-tuning, and its performance can be further improved with fine-tuning as more domain-specific data becomes available. We also show that the fine-tuned language model meets and outperforms the state-of-the-art (SOTA) results in resolving conversation intents without training classifiers. With the use of a topological intent tree, our model provides more interpretability to cultivate people’s trust in their decisions. | Wei Yuan, Martin Dimkovski, Aijun An | Department of Electrical Engineering and Computer Science, York University, Canada; Intact Financial Corporation, Canada |
| 278 |  |  [Visual Item Selection With Voice Assistants: A systems perspective](https://doi.org/10.1145/3543873.3584655) |  | 0 | Interacting with voice assistants, such as Amazon Alexa to aid in day-to-day tasks has become a ubiquitous phenomenon in modern-day households. These voice assistants often have screens to provide visual content (e.g., images, videos) to their users. There is an increasing trend of users shopping or searching for products using these devices, yet, these voice assistants do not support commands or queries that contain visual references to the content shown on screen (e.g., “blue one”, “red dress”). We introduce a novel multi-modal visual shopping experience where the voice assistant is aware of the visual content shown on the screen and assists the user in item selection using natural language multi-modal interactions. We detail a practical, lightweight end-to-end system architecture spanning from model fine-tuning, deployment, to skill invocation on an Amazon Echo family device with a screen. We also define a niche “Visual Item Selection” task and evaluate whether we can effectively leverage publicly available multi-modal models, and embeddings produced from these models for the task. We show that open source contrastive embeddings like CLIP [30] and ALBEF [24] have zero-shot accuracy above for the “Visual Item Selection” task on an internally collected visual shopping dataset. By further fine-tuning the embeddings, we obtain further gains of 8.6% to 24.0% in relative accuracy improvement over a baseline. The technology that enables our visual shopping assistant is available as an Alexa Skill in the Alexa Skills store. | Prashan Wanigasekara, Rafid AlHumaimidi, Turan Gojayev, Niloofar Gheissari, Achal Dave, Stephen Rawls, Fan Yang, Kechen Qin, Nalin Gupta, Spurthi Sandiri, Chevanthie Dissanayake, Zeynab Raeesy, Emre Barut, Chengwei Su | Amazon, Germany; Amazon, Canada; Amazon, USA |
| 279 |  |  [Multi-Source Domain Adaptation via Latent Domain Reconstruction](https://doi.org/10.1145/3543873.3584659) |  | 0 | Multi-Source Domain Adaptation (MSDA) is widely used in various machine learning scenarios for domain shifts between labeled source domains and unlabeled target domains. Conventional MSDA methods are built on a strong hypothesis that data samples from the same source belong to the same domain with the same latent distribution. However, in practice sources and their latent domains are not necessarily one-to-one correspondence. To tackle this problem, a novel Multi-source Reconstructed Domain Adaptation (MRDA) framework for MSDA is proposed. We use an Expectation-Maximization (EM) mechanism that iteratively reconstructs the source domains to recover the latent domains and performs domain adaptation on the reconstructed domains. Specifically, in the E-step, we cluster the samples from multiple sources into different latent domains, and a soft assignment strategy is proposed to avoid cluster imbalance. In the M-step, we freeze the latent domains clustered in the E-step and optimize the objective function for domain adaptation, and a global-specific feature extractor is used to capture both domain-invariant and domain-specific features. Extensive experiments demonstrate that our approach can reconstruct source domains and perform domain adaptation on the reconstructed domains effectively, thus significantly outperforming state-of-the-art (SOTA) baselines (e.g., 1% to 3.1% absolute improvement in AUC). | Jun Zhou, Chilin Fu, Xiaolu Zhang | Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China |
| 280 |  |  [Human Dimensions of Animal Exploitation: Towards Understanding the International Wildlife Trade and Selfie-Tourism on Twitter](https://doi.org/10.1145/3543873.3587538) |  | 0 | This study investigates statements of participation in an exploitative animal activity on social media website Twitter. The data include social posts (tweets) related to two exploited species - the sloth (N=32,119), and the elephant (N=15,160). Tweets for each of these case studies were examined and labeled. The initial results reveal several features of interaction with exploited species. Namely, there are a high number of tweets indicating that individuals participated in exploited species activities during vacations in destinations that double as native countries for the exploited species. The data also indicate that a large number of exploited species activities take place at fairs, carnivals, and circuses. These initial results shed light on the trends in human participation in activities with exploited species. These findings will offer insight to stakeholders seeking to bolster education programs and quantify the level of animal exploitation. | Sean P. Rogers, Jeremiah Onaolapo | University of Vermont, USA |
| 281 |  |  [A Bridge over the Troll: Non-Complementary Activism Online](https://doi.org/10.1145/3543873.3587541) |  | 0 | Previous research has identified phenomena such as cyberbystander intervention and various other forms of responses to aggressive or hateful behaviours online. In the online media ecosystem, some people from marginalized communities and their allies have attempted to enhance organic engagement by participating in organized activism, which is sometimes characterized as "non-complementary" or "indirect". This paper attempts to identify, recognize, and label this phenomenon, as well as provide suggestions for further research in this area. | Emyn Dean | Knowledge Media Institute (KMi), Open University, United Kingdom |
| 282 |  |  [The DEEP Sensorium: a multidimensional approach to sensory domain labelling](https://doi.org/10.1145/3543873.3587631) |  | 0 | In this paper, we describe our intuitions about how language technologies can contribute to create new ways to enhance the accessibility of exhibits in cultural contexts by exploiting the knowledge about the history of our senses and the link between perception and language. We evaluate the performance of five multi-class classification models for the task of sensory recognition and introduce the DEEP Sensorium (Deep Engaging Experiences and Practices - Sensorium), a multidimensional dataset that combines cognitive and affective features to inform systematic methodologies for augmenting exhibits with multi-sensory stimuli. For each model, using different feature sets, we show that the features expressing the affective dimension of words combined with sub-lexical features perform better than uni-dimensional training sets. | Simona Corciulo, Livio Bioglio, Valerio Basile, Viviana Patti, Rossana Damiano | Dipartimento di Informatica, University of Turin, Italy; Dipartimento di Studi Umanistici, University of Turin, Italy |
| 283 |  |  [A Survey of General Ontologies for the Cross-Industry Domain of Circular Economy](https://doi.org/10.1145/3543873.3587613) |  | 0 | Circular Economy has the goal to reduce value loss and avoid waste by extending the life span of materials and products, including circulating materials or product parts before they become waste. Circular economy models (e.g., circular value networks) are typically complex and networked, involving different cross-industry domains. In the context of a circular value network, multiple actors, such as suppliers, manufacturers, recyclers, and product end-users, may be involved. In addition, there may be various flows of resources, energy, information and value throughout the network. This means that we face the challenge that the data and information from cross-industry domains in a circular economy model are not built on common ground, and as a result are difficult to understand and use for both humans and machines. Using ontologies to represent domain knowledge can enable actors and stakeholders from different industries in the circular economy to communicate using a common language. The knowledge domains involved include circular economy, sustainability, materials, products, manufacturing, and logistics. The objective of this paper is to investigate the landscape of current ontologies for these domains. This will enable us to in the future explore what existing knowledge can be adapted or used to develop ontologies for circular value networks. | Huanyu Li, Mina Abd Nikooie Pour, Ying Li, Mikael Lindecrantz, Eva Blomqvist, Patrick Lambrix | Linköping University, Sweden and University of Gävle, Sweden; Linköping University, Sweden; Ragn-Sells AB, Sweden |
| 284 |  |  [Improving Netflix Video Quality with Neural Networks](https://doi.org/10.1145/3543873.3587553) |  | 0 | Video downscaling is an important component of adaptive video streaming, which tailors streaming to screen resolutions of different devices and optimizes picture quality under varying network conditions. With video downscaling, a high-resolution input video is downscaled into multiple lower-resolution videos. This is typically done by a conventional resampling filter like Lanczos. In this talk, we describe how we improved Netflix video quality by developing neural networks for video downscaling and deploying them at scale. | Christos G. Bampis, LiHeng Chen, Zhi Li | Netflix, USA |
| 285 |  |  [Graph2Feat: Inductive Link Prediction via Knowledge Distillation](https://doi.org/10.1145/3543873.3587596) |  | 0 | Link prediction between two nodes is a critical task in graph machine learning. Most approaches are based on variants of graph neural networks (GNNs) that focus on transductive link prediction and have high inference latency. However, many real-world applications require fast inference over new nodes in inductive settings where no information on connectivity is available for these nodes. Thereby, node features provide an inevitable alternative in the latter scenario. To that end, we propose Graph2Feat, which enables inductive link prediction by exploiting knowledge distillation (KD) through the Student-Teacher learning framework. In particular, Graph2Feat learns to match the representations of a lightweight student multi-layer perceptron (MLP) with a more expressive teacher GNN while learning to predict missing links based on the node features, thus attaining both GNN’s expressiveness and MLP’s fast inference. Furthermore, our approach is general; it is suitable for transductive and inductive link predictions on different types of graphs regardless of them being homogeneous or heterogeneous, directed or undirected. We carry out extensive experiments on seven real-world datasets including homogeneous and heterogeneous graphs. Our experiments demonstrate that Graph2Feat significantly outperforms SOTA methods in terms of AUC and average precision in homogeneous and heterogeneous graphs. Finally, Graph2Feat has the minimum inference time compared to the SOTA methods, and 100x acceleration compared to GNNs. The code and datasets are available on GitHub1. | Ahmed E. Samy, Zekarias T. Kefato, Sarunas Girdzijauskas | KTH, Royal Institue of Technology, Sweden; KTH, Royal Institute of Technology, Sweden |
| 286 |  |  [Universal Model in Online Customer Service](https://doi.org/10.1145/3543873.3587630) |  | 0 | Building machine learning models can be a time-consuming process that often takes several months to implement in typical business scenarios. To ensure consistent model performance and account for variations in data distribution, regular retraining is necessary. This paper introduces a solution for improving online customer service in e-commerce by presenting a universal model for predicting labels based on customer questions, without requiring training. Our novel approach involves using machine learning techniques to tag customer questions in transcripts and create a repository of questions and corresponding labels. When a customer requests assistance, an information retrieval model searches the repository for similar questions, and statistical analysis is used to predict the corresponding label. By eliminating the need for individual model training and maintenance, our approach reduces both the model development cycle and costs. The repository only requires periodic updating to maintain accuracy. | ShuTing Pi, ChengPing Hsieh, Qun Liu, Yuying Zhu | Amazon, USA |
| 287 |  |  [Robust Stochastic Multi-Armed Bandits with Historical Data](https://doi.org/10.1145/3543873.3587653) |  | 0 | We consider the problem of Stochastic Contextual Multi-Armed Bandits (CMABs) initialised with Historical data. Initialisation with historical data is an example of data-driven regularisation which should, in theory, accelerate the convergence of CMABs. However, in practice, we have little to no control over the underlying generation process of such data, which may exhibit some pathologies, possibly impeding the convergence and the stability of the algorithm. In this paper, we focus on two main challenges: bias selection and data corruption. We propose two new algorithms to solve these specific issues: LinUCB with historical data and offline balancing (OB-HLinUCB) and Robust LinUCB with corrupted historical data (R-HLinUCB). We derive their theoretical regret bounds and discuss their computational performance using real-world datasets. | Sarah Boufelja Yacobi, Djallel Bouneffouf | IBM Research, USA; Imperial College London, United Kingdom |
| 288 |  |  [Skill Graph Construction From Semantic Understanding](https://doi.org/10.1145/3543873.3587667) |  | 0 | LinkedIn is building a skill graph to power a skill-first talent marketplace. Constructing a skill graph from a flat list is not an trivial task, especially by human curation. In this paper, we leverage the pre-trained large language model BERT to achieve this through semantic understanding on synthetically generated texts as training data. We automatically create positive and negative labels from the seed skill graph. The training data are encoded by pre-trained language models into embeddings and they are consumed by the downstream classification module to classify the relationships between skill pairs. | Shiyong Lin, Yiping Yuan, Carol Jin, Yi Pan | LinkedIn, USA |
| 289 |  |  [Cultural Differences in Signed Ego Networks on Twitter: An Investigatory Analysis](https://doi.org/10.1145/3543873.3587641) |  | 0 | Human social behaviour has been observed to adhere to certain structures. One such structure, the Ego Network Model (ENM), has been found almost ubiquitously in human society. Recently, this model has been extended to include signed connections. While the unsigned ENM has been rigorously observed for decades, the signed version is still somewhat novel and lacks the same breadth of observation. Therefore, the main aim of this paper is to examine this signed structure across various categories of individuals from a swathe of culturally distinct regions. Minor differences in the distribution of signs across the SENM can be observed between cultures. However, these can be overwhelmed when the network is centred around a specific topic. Indeed, users who are engaged with specific themes display higher levels of negativity in their networks. This effect is further supported by a significant negative correlation between the number of "general" topics discussed in a network and that network’s percentage of negative connections. These findings suggest that the negativity of communications and relationships on Twitter are very dependent on the topics being discussed and, furthermore, these relationships are more likely to be negative when they are based around a specific topic. | Jack Tacchi, Chiara Boldrini, Andrea Passarella, Marco Conti | Istituto di Informatica e Telematica - Consiglio Nazionale delle Ricerche, Italy and Scuola Normale Superiore, Italy; Istituto di Informatica e Telematica - Consiglio Nazionale delle Ricerche, Italy |
| 290 |  |  [Don't Trust, Verify: The Case of Slashing from a Popular Ethereum Explorer](https://doi.org/10.1145/3543873.3587555) |  | 0 | Blockchain explorers are important tools for quick look-ups of on-chain activities. However, as centralized data providers, their reliability remains under-studied. As a case study, we investigate Beaconcha.in , a leading explorer serving Ethereum’s proof-of-stake (PoS) update. According to the explorer, we find that more than 75% of slashable Byzantine actions were not slashed. Since Ethereum relies on the “stake-and-slash" mechanism to align incentives, this finding would at its face value cause concern over Ethereum’s security. However, further investigation reveals that all the apparent unslashed incidents were erroneously recorded due to the explorer’s mishandling of consensus edge cases. Besides the usual message of using caution with centralized information providers, our findings also call for attention to improving the monitoring of blockchain systems that support high-value applications. | Zhiguo He, Jiasun Li, Zhengxun Wu | University of Chicago and NBER, USA; Independent, USA; George Mason University, USA |
| 291 |  |  [An Exploration on Cryptocurrency Corporations' Fiscal Opportunities](https://doi.org/10.1145/3543873.3587603) |  | 0 | As the decentralized finance industry gains traction, governments worldwide are creating or modifying legislations to regulate such financial activities. To avoid these new legislations, decentralized finance enterprises may shop for fiscally advantageous jurisdictions. This study explores global tax evasion opportunities for decentralized finance enterprises. Opportunities are identified by considering various jurisdictions’ tax laws on cryptocurrencies along with their corporate income tax rates, corporate capital gains tax rates, level of financial development and level of cryptocurrency adoption. They are visualized with the manifold approximation and projection for dimension reduction (UMAP) technique. The study results show that there exist a substantial number of tax evasion opportunities for decentralized finance enterprises through both traditional offshore jurisdictions and crypto-advantageous jurisdictions. The latter jurisdictions are usually considered high-tax fiscal regimes; but, given that they do not apply tax laws, tax evasion opportunities arise, especially in jurisdictions that have high financial development and high cryptocurrency adoption. Further research should investigate these new opportunities and how they are evolving. Understanding the global landscape surrounding tax evasion opportunities in decentralized finance represents a first step at preventing corporate capital flight of cryptocurrencies. | Thomas Charest, Masarah PaquetClouston | School of Criminology, University of Montreal, Canada |
| 292 |  |  [Automated Ontology Evaluation: Evaluating Coverage and Correctness using a Domain Corpus](https://doi.org/10.1145/3543873.3587617) |  | 0 | Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology’s concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology’s relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art. | Antonio Zaitoun, Tomer Sagi, Katja Hose | Aalborg University, Denmark; University of Haifa, Israel; Aalborg University, Denmark and TU Wien, Austria |
| 293 |  |  [Improving the Exploration/Exploitation Trade-Off in Web Content Discovery](https://doi.org/10.1145/3543873.3587574) |  | 0 | New web content is published constantly, and although protocols such as RSS can notify subscribers of new pages, they are not always implemented or actively maintained. A more reliable way to discover new content is to periodically re-crawl the target sites. Designing such “content discovery crawlers” has important applications, for example, in web search, digital assistants, business, humanitarian aid, and law enforcement. Existing approaches assume that each site of interest has a relatively small set of unknown “source pages” that, when refreshed, frequently provide hyperlinks to the majority of new content. The state of the art (SOTA) uses ideas from the multi-armed bandit literature to explore candidate sources while simultaneously exploiting known good sources. We observe, however, that the SOTA uses a sub-optimal algorithm for balancing exploration and exploitation. We trace this back to a mismatch between the space of actions that the SOTA algorithm models and the space of actions that the crawler must actually choose from. Our proposed approach, the Thompson crawler (named after the Thompson sampler that drives its refresh decisions), addresses this shortcoming by more faithfully modeling the action space. On a dataset of 4,070 source pages drawn from 53 news domains over a period of 7 weeks, we show that, on average, the Thompson crawler discovers 20% more new pages, finds pages 6 hours earlier, and uses 14 fewer refreshes per 100 pages discovered than the SOTA. | Peter Schulam, Ion Muslea | Amazon Alexa, USA |
| 294 |  |  [SoarGraph: Numerical Reasoning over Financial Table-Text Data via Semantic-Oriented Hierarchical Graphs](https://doi.org/10.1145/3543873.3587598) |  | 0 | Towards the intelligent understanding of table-text data in the finance domain, previous research explores numerical reasoning over table-text content with Question Answering (QA) tasks. A general framework is to extract supporting evidence from the table and text and then perform numerical reasoning over extracted evidence for inferring the answer. However, existing models are vulnerable to missing supporting evidence, which limits their performance. In this work, we propose a novel Semantic-Oriented Hierarchical Graph (SoarGraph) that models the semantic relationships and dependencies among the different elements (e.g., question, table cells, text paragraphs, quantities, and dates) using hierarchical graphs to facilitate supporting evidence extraction and enhance numerical reasoning capability. We conduct our experiments on two popular benchmarks, FinQA and TAT-QA datasets, and the results show that our SoarGraph significantly outperforms all strong baselines, demonstrating remarkable effectiveness. | Fengbin Zhu, Moxin Li, Junbin Xiao, Fuli Feng, Chao Wang, TatSeng Chua | 6ESTATES PTE LTD, Singapore; National University of Singapore, Singapore; University of Science and Technology of China, China |
| 295 |  |  [Online to Offline Crossover of White Supremacist Propaganda](https://doi.org/10.1145/3543873.3587569) |  | 0 | White supremacist extremist groups are a significant domestic terror threat in many Western nations. These groups harness the Internet to spread their ideology via online platforms: blogs, chat rooms, forums, and social media, which can inspire violence offline. In this work, we study the persistence and reach of white supremacist propaganda in both online and offline environments. We also study patterns in narratives that crossover from online to offline environments, or vice versa. From a geospatial analysis, we find that offline propaganda is geographically widespread in the United States, with a slight tendency toward Northeastern states. Propaganda that spreads the farthest and lasts the longest has a patriotic framing and is short, memorable, and repeatable. Through text comparison methods, we illustrate that online propaganda typically leads the appearance of the same propaganda in offline flyers, banners, and graffiti. We hope that this study sheds light on the characteristics of persistent white supremacist narratives both online and offline. | Ahmad Diab, BolorErdene Jagdagdorj, Lynnette Hui Xian Ng, YuRu Lin, Michael Miller Yoder | Carnegie Mellon University, USA; University of Pittsburgh, USA |
| 296 |  |  [Privacy-Preserving Online Content Moderation with Federated Learning](https://doi.org/10.1145/3543873.3587366) |  | 0 | Users are daily exposed to a large volume of harmful content on various social network platforms. One solution is developing online moderation tools using Machine Learning techniques. However, the processing of user data by online platforms requires compliance with privacy policies. Federated Learning (FL) is an ML paradigm where the training is performed locally on the users' devices. Although the FL framework complies, in theory, with the GDPR policies, privacy leaks can still occur. For instance, an attacker accessing the final trained model can successfully perform unwanted inference of the data belonging to the users who participated in the training process. In this paper, we propose a privacy-preserving FL framework for online content moderation that incorporates Differential Privacy (DP). To demonstrate the feasibility of our approach, we focus on detecting harmful content on Twitter - but the overall concept can be generalized to other types of misbehavior. We simulate a text classifier - in FL fashion - which can detect tweets with harmful content. We show that the performance of the proposed FL framework can be close to the centralized approach - for both the DP and non-DP FL versions. Moreover, it has a high performance even if a small number of clients (each with a small number of data points) are available for the FL training. When reducing the number of clients (from 50 to 10) or the data points per client (from 1K to 0.1K), the classifier can still achieve ~81% AUC. Furthermore, we extend the evaluation to four other Twitter datasets that capture different types of user misbehavior and still obtain a promising performance (61% - 80% AUC). Finally, we explore the overhead on the users' devices during the FL training phase and show that the local training does not introduce excessive CPU utilization and memory consumption overhead. | Pantelitsa Leonidou, Nicolas Kourtellis, Nikos Salamanos, Michael Sirivianos | Telefonica Research, Spain; Cyprus University of Technology, Cyprus |
| 297 |  |  [Graph-based Approach for Studying Spread of Radical Online Sentiment](https://doi.org/10.1145/3543873.3587634) |  | 0 | The spread of radicalization through the Internet is a growing problem. We are witnessing a rise in online hate groups, inspiring the impressionable and vulnerable population towards extreme actions in the real world. In this paper, we study the spread of hate sentiments in online forums by collecting 1,973 long comment threads (30+ comments per thread) posted on dark-web forums and containing a combination of benign posts and radical comments on the Islamic religion. This framework allows us to leverage network analysis tools to investigate sentiment propagation through a social network. By combining sentiment analysis, social network analysis, and graph theory, we aim to shed light on the propagation of hate speech in online forums and the extent to which such speech can influence individuals. The results of the intra-thread analysis suggests that sentiment tends to cluster within comment threads, with around 75% of connected members sharing similar sentiments. They also indicate that online forums can act as echo chambers where people with similar views reinforce each other’s beliefs and opinions. On the other hand, the inter-thread shows that 64% of connected threads share similar sentiments, suggesting similarities between the ideologies present in different threads and that there likely is a wider network of individuals spreading hate speech across different forums. Finally, we plan to study this work with a larger dataset, which could provide further insights into the spread of hate speech in online forums and how to mitigate it. | Le Nguyen, Nidhi Rastogi | Golisano College of Computing and Information Science, Department of Software Engineering, Rochester Institute of Technology, USA |
| 298 |  |  [Swinging in the States: Does disinformation on Twitter mirror the US presidential election system?](https://doi.org/10.1145/3543873.3587638) |  | 0 | For more than a decade scholars have been investigating the disinformation flow on social media contextually to societal events, like, e.g., elections. In this paper, we analyze the Twitter traffic related to the US 2020 pre-election debate and ask whether it mirrors the electoral system. The U.S. electoral system provides that, regardless of the actual vote gap, the premier candidate who received more votes in one state \`takes' that state. Criticisms of this system have pointed out that election campaigns can be more intense in particular key states to achieve victory, so-called {\it swing states}. Our intuition is that election debate may cause more traffic on Twitter-and probably be more plagued by misinformation-when associated with swing states. The results mostly confirm the intuition. About 88\% of the entire traffic can be associated with swing states, and links to non-trustworthy news are shared far more in swing-related traffic than the same type of news in safe-related traffic. Considering traffic origin instead, non-trustworthy tweets generated by automated accounts, so-called social bots, are mostly associated with swing states. Our work sheds light on the role an electoral system plays in the evolution of online debates, with, in the spotlight, disinformation and social bots. | Manuel Pratelli, Marinella Petrocchi, Fabio Saracco, Rocco De Nicola | “Enrico Fermi” Research Center (CREF), Italy and IMT - School for Advanced Studies Lucca, Italy; IMT - School for Advanced Studies Lucca, Italy and Istituto di Informatica e Telematica (IIT) CNR, Italy; IMT - School for Advanced Studies Lucca, Italy; Istituto di Informatica e Telematica (IIT) CNR, Italy and IMT - School for Advanced Studies Lucca, Italy |
| 299 |  |  [Analyzing Activity and Suspension Patterns of Twitter Bots Attacking Turkish Twitter Trends by a Longitudinal Dataset](https://doi.org/10.1145/3543873.3587650) |  | 0 | Twitter bots amplify target content in a coordinated manner to make them appear popular, which is an astroturfing attack. Such attacks promote certain keywords to push them to Twitter trends to make them visible to a broader audience. Past work on such fake trends revealed a new astroturfing attack named ephemeral astroturfing that employs a very unique bot behavior in which bots post and delete generated tweets in a coordinated manner. As such, it is easy to mass-annotate such bots reliably, making them a convenient source of ground truth for bot research. In this paper, we detect and disclose over 212,000 such bots targeting Turkish trends, which we name astrobots. We also analyze their activity and suspension patterns. We found that Twitter purged those bots en-masse 6 times since June 2018. However, the adversaries reacted quickly and deployed new bots that were created years ago. We also found that many such bots do not post tweets apart from promoting fake trends, which makes it challenging for bot detection methods to detect them. Our work provides insights into platforms' content moderation practices and bot detection research. The dataset is publicly available at https://github.com/tugrulz/EphemeralAstroturfing. | Tugrulcan Elmas | Indiana University Bloomington, USA |
| 300 |  |  [Socio-Emotional Computational Analysis of Propaganda Campaigns on Social Media Users in the Middle East](https://doi.org/10.1145/3543873.3587677) |  | 0 | Society has been significantly impacted by social media platforms in almost every aspect of their life. This impact has been effectively formulating people’s global mindsets and opinions on political, economic, and social events. Such waves of opinion formation are referred to as propagandas and misinformation. Online propaganda influences the emotional and psychological orientation of people. The remarkable leaps in Machine Learning models and Natural Language Processing have helped in analyzing the emotional and psychological effects of cyber social threats such as propaganda campaigns on different nations, specifically in the Middle East, where rates of disputes have risen after the Arab Spring and the ongoing crises. In this paper, we present an approach to detect propagandas and the associated emotional and psychological aspects from social media news headlines that contain such a contextualized cyber social attack. We created a new dataset of headlines containing propaganda tweets and another dataset of potential emotions that the audience might endure when being exposed to such propaganda headlines. We believe that this is the first research to address the detection of emotional reactions linked to propaganda types on social media in the Middle East. | Zain A. Halloush, Ahmed Aleroud, Craig Albert | Pamplin College of Arts, Humanities, and Social Sciences, Augusta University, USA; School of Computer and Cyber Sciences, Augusta University, Augusta University, USA |
| 301 |  |  [Towards a Semantic Approach for Linked Dataspace, Model and Data Cards](https://doi.org/10.1145/3543873.3587659) |  | 0 | The vast majority of artificial intelligence practitioners overlook the importance of documentation when building and publishing models and datasets. However, due to the recent trend in the explainability and fairness of AI models, several frameworks have been proposed such as Model Cards, and Data Cards, among others, to help in the appropriate re-usage of those models and datasets. In addition, because of the introduction of the dataspace concept for similar datasets in one place, there is potential that similar Model Cards, Data Cards, Service Cards, and Dataspace Cards can be linked to extract helpful information for better decision-making about which model and data can be used for a specific application. This paper reviews the case for considering a Semantic Web approach for exchanging Model/Data Cards as Linked Data or knowledge graphs in a dataspace, making them machine-readable. We discuss the basic concepts and propose a schema for linking Data Cards and Model Cards within a dataspace. In addition, we introduce the concept of a dataspace card which can be a starting point for extracting knowledge about models and datasets in a dataspace. This helps in building trust and reuse of models and data among companies and individuals participating as publishers or consumers of such assets. | Andy Donald, Apostolos Galanopoulos, Edward Curry, Emir Muñoz, Ihsan Ullah, M. A. Waskow, Maciej Dabrowski, Manan Kalra | Insight SFI Centre for Data Analytics, Data Science Institute, University of Galway, Galway, Ireland, Ireland; Genesys Cloud Services Inc., Bonham Quay, Galway, Ireland, Ireland |
| 302 |  |  [Semantics in Dataspaces: Origin and Future Directions](https://doi.org/10.1145/3543873.3587689) |  | 0 | The term dataspace was coined two decades ago [12] and has evolved since then. Definitions range from (i) an abstraction for data management in an identifiable scope [15] over (iii) a multi-sided data platform connecting participants in an ecosystem [21] to (iii) interlinking data towards loosely connected (global) information [17]. Many implementations and scientific notions follow different interpretations of the term dataspace, but agree on some use of semantic technologies. For example, dataspaces such as the European Open Science Cloud and the German National Research Data Infrastructure are committed to applying the FAIR principles [11, 16]. Dataspaces built on top of Gaia-X are using semantic methods for service Self-Descriptions [13]. This paper investigates ongoing dataspace efforts and aims to provide insights on the definition of the term dataspace, the usage of semantics and FAIR principles, and future directions for the role of semantics in dataspaces. | Johannes TheissenLipp, Max Kocher, Christoph Lange, Stefan Decker, Alexander Paulus, André Pomp, Edward Curry | University of Galway, Ireland; RWTH Aachen University, Germany and Fraunhofer Institute for Applied Information Technology FIT, Germany; RWTH Aachen University, Germany; University of Wuppertal, Germany |
| 303 |  |  [Efficient Sampling for Big Provenance](https://doi.org/10.1145/3543873.3587556) |  | 0 | Provenance has been studied extensively to explain existing and missing results for many applications while focusing on scalability and usability challenges. Recently, techniques that efficiently compute a compact representation of provenance have been introduced. In this work, we introduce a practical solution that computes a sample of provenance for existing results without computing full provenance. Our technique computes a sample of provenance based on the distribution of provenance wrt the query result that is estimated from the distribution of input data while considering the correlation among the data. The preliminary evaluation demonstrates that comparing to the naive approach our method efficiently computes a sample of (large size of) provenance with low errors. | Sara Moshtaghi Largani, Seokki Lee | University of Cincinnati, USA |
| 304 |  |  [Provenance Tracking for End-to-End Machine Learning Pipelines](https://doi.org/10.1145/3543873.3587557) |  | 0 | No abstract available. | Stefan Grafberger, Paul Groth, Sebastian Schelter | University of Amsterdam, Netherlands |
| 305 |  |  [SeeGera: Self-supervised Semi-implicit Graph Variational Auto-encoders with Masking](https://doi.org/10.1145/3543507.3583245) |  | 0 | Generative graph self-supervised learning (SSL) aims to learn node representations by reconstructing the input graph data. However, most existing methods focus on unsupervised learning tasks only and very few work has shown its superiority over the state-of-the-art graph contrastive learning (GCL) models, especially on the classification task. While a very recent model has been proposed to bridge the gap, its performance on unsupervised learning tasks is still unknown. In this paper, to comprehensively enhance the performance of generative graph SSL against other GCL models on both unsupervised and supervised learning tasks, we propose the SeeGera model, which is based on the family of self-supervised variational graph auto-encoder (VGAE). Specifically, SeeGera adopts the semi-implicit variational inference framework, a hierarchical variational framework, and mainly focuses on feature reconstruction and structure/feature masking. On the one hand, SeeGera co-embeds both nodes and features in the encoder and reconstructs both links and features in the decoder. Since feature embeddings contain rich semantic information on features, they can be combined with node embeddings to provide fine-grained knowledge for feature reconstruction. On the other hand, SeeGera adds an additional layer for structure/feature masking to the hierarchical variational framework, which boosts the model generalizability. We conduct extensive experiments comparing SeeGera with 9 other state-of-the-art competitors. Our results show that SeeGera can compare favorably against other state-of-the-art GCL methods in a variety of unsupervised and supervised learning tasks. | Xiang Li, Tiandi Ye, Caihua Shan, Dongsheng Li, Ming Gao | East China Normal University, China; Microsoft Research Asia, China |
| 306 |  |  [Lightweight source localization for large-scale social networks](https://doi.org/10.1145/3543507.3583299) |  | 0 | The rapid diffusion of hazardous information in large-flow-based social media causes great economic losses and potential threats to society. It is crucial to infer the inner information source as early as possible to prevent further losses. However, existing localization methods wait until all deployed sensors obtain propagation information before starting source inference within a network, and hence the best opportunity to control propagation is missed. In this paper, we propose a new localization strategy based on finite deployed sensors, named Greedy-coverage-based Rapid Source Localization (GRSL), to rapidly, flexibly and accurately infer the source in the early propagation stage of large-scale networks. There are two phases in GRSL. In the first phase, the Greedy-based Strategy (GS) greedily deploys sensors to rapidly achieve wide area coverage at a low cost. In the second phase, when a propagation event within a network is observed by a part of the sensors, the Inference Strategy (IS) with an earlier response mechanism begins executing the source inference task in an earlier small infected area. Comprehensive experiments with the SOTA methods demonstrate the superior performance and robustness of GRSL in various application scenarios. | Zhen Wang, Dongpeng Hou, Chao Gao, Xiaoyu Li, Xuelong Li | School of Artificial Intelligence, Optics and Electronics (iOPEN), Northwestern Polytechnical University, China; Northwestern Polytechnical University, China |
| 307 |  |  [xGCN: An Extreme Graph Convolutional Network for Large-scale Social Link Prediction](https://doi.org/10.1145/3543507.3583340) |  | 0 | Graph neural networks (GNNs) have seen widespread usage across multiple real-world applications, yet in transductive learning, they still face challenges in accuracy, efficiency, and scalability, due to the extensive number of trainable parameters in the embedding table and the paradigm of stacking neighborhood aggregations. This paper presents a novel model called xGCN for large-scale network embedding, which is a practical solution for link predictions. xGCN addresses these issues by encoding graph-structure data in an extreme convolutional manner, and has the potential to push the performance of network embedding-based link predictions to a new record. Specifically, instead of assigning each node with a directly learnable embedding vector, xGCN regards node embeddings as static features. It uses a propagation operation to smooth node embeddings and relies on a Refinement neural Network (RefNet) to transform the coarse embeddings derived from the unsupervised propagation into new ones that optimize a training objective. The output of RefNet, which are well-refined embeddings, will replace the original node embeddings. This process is repeated iteratively until the model converges to a satisfying status. Experiments on three social network datasets with link prediction tasks show that xGCN not only achieves the best accuracy compared with a series of competitive baselines but also is highly efficient and scalable. | Xiran Song, Jianxun Lian, Hong Huang, Zihan Luo, Wei Zhou, Xue Lin, Mingqi Wu, Chaozhuo Li, Xing Xie, Hai Jin | National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; Microsoft Research Asia, China; Microsoft Gaming, USA |
| 308 |  |  [GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks](https://doi.org/10.1145/3543507.3583386) |  | 0 | Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks(GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily rely on a large amount of task-specific supervision. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-train model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt. | Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang | National University of Singapore, Singapore; Singapore Management University, Singapore; University of Science and Technology of China, China |
| 309 |  |  [FedACK: Federated Adversarial Contrastive Knowledge Distillation for Cross-Lingual and Cross-Model Social Bot Detection](https://doi.org/10.1145/3543507.3583500) |  | 0 | Social bot detection is of paramount importance to the resilience and security of online social platforms. The state-of-the-art detection models are siloed and have largely overlooked a variety of data characteristics from multiple cross-lingual platforms. Meanwhile, the heterogeneity of data distribution and model architecture makes it intricate to devise an efficient cross-platform and cross-model detection framework. In this paper, we propose FedACK, a new federated adversarial contrastive knowledge distillation framework for social bot detection. We devise a GAN-based federated knowledge distillation mechanism for efficiently transferring knowledge of data distribution among clients. In particular, a global generator is used to extract the knowledge of global data distribution and distill it into each client's local model. We leverage local discriminator to enable customized model design and use local generator for data enhancement with hard-to-decide samples. Local training is conducted as multi-stage adversarial and contrastive learning to enable consistent feature spaces among clients and to constrain the optimization direction of local models, reducing the divergences between local and global models. Experiments demonstrate that FedACK outperforms the state-of-the-art approaches in terms of accuracy, communication efficiency, and feature space consistency. | Yingguang Yang, Renyu Yang, Hao Peng, Yangyang Li, Tong Li, Yong Liao, Pengyuan Zhou | NERC-RPP, CAEIT, China; Beihang University, China; University of Science and Technology of China, China; University of Leeds, United Kingdom; Tsinghua University, China |
| 310 |  |  [Self-training through Classifier Disagreement for Cross-Domain Opinion Target Extraction](https://doi.org/10.1145/3543507.3583325) |  | 0 | Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental task in opinion mining that aims to extract the targets (or aspects) on which opinions have been expressed. Recent work focus on cross-domain OTE, which is typically encountered in real-world scenarios, where the testing and training distributions differ. Most methods use domain adversarial neural networks that aim to reduce the domain gap between the labelled source and unlabelled target domains to improve target domain performance. However, this approach only aligns feature distributions and does not account for class-wise feature alignment, leading to suboptimal results. Semi-supervised learning (SSL) has been explored as a solution, but is limited by the quality of pseudo-labels generated by the model. Inspired by the theoretical foundations in domain adaptation [2], we propose a new SSL approach that opts for selecting target samples whose model output from a domain-specific teacher and student network disagree on the unlabelled target data, in an effort to boost the target domain performance. Extensive experiments on benchmark cross-domain OTE datasets show that this approach is effective and performs consistently well in settings with large domain shifts. | Kai Sun, Richong Zhang, Samuel Mensah, Nikolaos Aletras, Yongyi Mao, Xudong Liu | School of Electrical Engineering and Computer Science, University of Ottawa, Canada; SKLSDE, School of Computer Science and Engineering, Beihang University, China; Computer Science Department, University of Sheffield, UK, United Kingdom |
| 311 |  |  [Fast and Multi-aspect Mining of Complex Time-stamped Event Streams](https://doi.org/10.1145/3543507.3583370) |  | 0 | Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalies and identify the types of anomalies that occur in practice. Our proposed method has the following properties: (a) Effective: it captures dynamical multi-aspect patterns, i.e., regimes and components, and statistically summarizes all the events; (b) General: it is practical for successful application to data compression, pattern discovery, and anomaly detection on various types of tensor streams; (c) Scalable: our algorithm does not depend on the length of the data stream and its dimensionality. Extensive experiments on real datasets demonstrate that CubeScope finds meaningful patterns and anomalies correctly, and consistently outperforms the state-of-the-art methods as regards accuracy and execution speed. | Kota Nakamura, Yasuko Matsubara, Koki Kawabata, Yuhei Umeda, Yuichiro Wada, Yasushi Sakurai | SANKEN, Osaka University, Japan; AI Lab., Fujitsu, Japan; AI Lab., Fujitsu, Japan and AIP, RIKEN, Japan |
| 312 |  |  [PDSum: Prototype-driven Continuous Summarization of Evolving Multi-document Sets Stream](https://doi.org/10.1145/3543507.3583371) |  | 0 | Summarizing text-rich documents has been long studied in the literature, but most of the existing efforts have been made to summarize a static and predefined multi-document set. With the rapid development of online platforms for generating and distributing text-rich documents, there arises an urgent need for continuously summarizing dynamically evolving multi-document sets where the composition of documents and sets is changing over time. This is especially challenging as the summarization should be not only effective in incorporating relevant, novel, and distinctive information from each concurrent multi-document set, but also efficient in serving online applications. In this work, we propose a new summarization problem, Evolving Multi-Document sets stream Summarization (EMDS), and introduce a novel unsupervised algorithm PDSum with the idea of prototype-driven continuous summarization. PDSum builds a lightweight prototype of each multi-document set and exploits it to adapt to new documents while preserving accumulated knowledge from previous documents. To update new summaries, the most representative sentences for each multi-document set are extracted by measuring their similarities to the prototypes. A thorough evaluation with real multi-document sets streams demonstrates that PDSum outperforms state-of-the-art unsupervised multi-document summarization algorithms in EMDS in terms of relevance, novelty, and distinctiveness and is also robust to various evaluation settings. | Susik Yoon, Hou Pong Chan, Jiawei Han | University of Illinois at Urbana-Champaign, USA; University of Macau, Macao |
| 313 |  |  [Learning Disentangled Representation via Domain Adaptation for Dialogue Summarization](https://doi.org/10.1145/3543507.3583389) |  | 0 | Dialogue summarization, which aims to generate a summary for an input dialogue, plays a vital role in intelligent dialogue systems. The end-to-end models have achieved satisfactory performance in summarization, but the success is built upon enough annotated data, which is costly to obtain, especially in the dialogue summarization. To leverage the rich external data, previous works first pre-train the model on the other domain data (e.g., the news domain), and then fine-tune it directly on the dialogue domain. The data from different domains are equally treated during the training process, while the vast differences between dialogues (usually informal, repetitive, and with multiple speakers) and conventional articles (usually formal and concise) are neglected. In this work, we propose to use a disentangled representation method to reduce the deviation between data in different domains, where the input data is disentangled into domain-invariant and domain-specific representations. The domain-invariant representation carries context information that is supposed to be the same across domains (e.g., news, dialogue) and the domain-specific representation indicates the input data belongs to a particular domain. We use adversarial learning and contrastive learning to constrain the disentangled representations to the target space. Furthermore, we propose two novel reconstruction strategies, namely backtracked and cross-track reconstructions, which aim to reduce the domain characteristics of out-of-domain data and mitigate the domain bias of the model. Experimental results on three public datasets show that our model significantly outperforms the strong baselines. | Jinpeng Li, Yingce Xia, Xin Cheng, Dongyan Zhao, Rui Yan | Microsoft Research, China; Wangxuan Institute of Computer Technology, Peking University, China and National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence, China; Wangxuan Institute of Computer Technology, Peking University, China; Gaoling School of Artificial Intelligence, Renmin University of China, China |
| 314 |  |  [Towards Understanding Consumer Healthcare Questions on the Web with Semantically Enhanced Contrastive Learning](https://doi.org/10.1145/3543507.3583449) |  | 0 | In recent years, seeking health information on the web has become a preferred way for healthcare consumers to support their information needs. Generally, healthcare consumers use long and detailed questions with several peripheral details to express their healthcare concerns, contributing to natural language understanding challenges. One way to address this challenge is by summarizing the questions. However, most of the existing abstractive summarization systems generate impeccably fluent yet factually incorrect summaries. In this paper, we present a semantically-enhanced contrastive learning-based framework for generating abstractive question summaries that are faithful and factually correct. We devised multiple strategies based on question semantics to generate the erroneous (negative) summaries, such that the model has the understanding of plausible and incorrect perturbations of the original summary. Our extensive experimental results on two benchmark consumer health question summarization datasets confirm the effectiveness of our proposed method by achieving state-of-the-art performance and generating factually correct and fluent summaries, as measured by human evaluation. | Shweta Yadav, Stefan Cobeli, Cornelia Caragea | University of Illinois at Chicago, USA |
| 315 |  |  [Modeling Dynamic Interactions over Tensor Streams](https://doi.org/10.1145/3543507.3583458) |  | 0 | Many web applications, such as search engines and social network services, are continuously producing a huge number of events with a multi-order tensor form, {count;query, location, …, timestamp}, and so how can we discover important trends to enables us to forecast long-term future events? Can we interpret any relationships between events that determine the trends from multi-aspect perspectives? Real-world online activities can be composed of (1) many time-changing interactions that control trends, for example, competition/cooperation to gain user attention, as well as (2) seasonal patterns that covers trends. To model the shifting trends via interactions, namely dynamic interactions over tensor streams, in this paper, we propose a streaming algorithm, DISMO, that we designed to discover Dynamic Interactions and Seasonality in a Multi-Order tensor. Our approach has the following properties. (a) Interpretable: it incorporates interpretable non-linear differential equations in tensor factorization so that it can reveal latent interactive relationships and thus generate future events effectively; (b) Dynamic: it can be aware of shifting trends by switching multi-aspect factors while summarizing their characteristics incrementally; and (c) Automatic: it finds every factor automatically without losing forecasting accuracy. Extensive experiments on real datasets demonstrate that our algorithm extracts interpretable interactions between data attributes, while simultaneously providing improved forecasting accuracy and a great reduction in computational time. | Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai | SANKEN, Osaka University, Japan |
| 316 |  |  [Constrained Subset Selection from Data Streams for Profit Maximization](https://doi.org/10.1145/3543507.3583490) |  | 0 | The problem of constrained subset selection from a large data stream for profit maximization has many applications in web data mining and machine learning, such as social advertising, team formation and recommendation systems. Such a problem can be formulated as maximizing a regularized submodular function under certain constraints. In this paper, we consider a generalized k-system constraint, which captures various requirements in real-world applications. For this problem, we propose the first streaming algorithm with provable performance bounds, leveraging a novel multitudinous distorted filter framework. The empirical performance of our algorithm is extensively evaluated in several applications including web data mining and recommendation systems, and the experimental results demonstrate the superiorities of our algorithm in terms of both effectiveness and efficiency. | Shuang Cui, Kai Han, Jing Tang, He Huang | School of Computer Science and Technology, Soochow University, China; The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, China; School of Computer Science and Technology, University of Science and Technology of China, China |
| 317 |  |  [SCStory: Self-supervised and Continual Online Story Discovery](https://doi.org/10.1145/3543507.3583507) |  | 0 | We present a framework SCStory for online story discovery, that helps people digest rapidly published news article streams in real-time without human annotations. To organize news article streams into stories, existing approaches directly encode the articles and cluster them based on representation similarity. However, these methods yield noisy and inaccurate story discovery results because the generic article embeddings do not effectively reflect the story-indicative semantics in an article and cannot adapt to the rapidly evolving news article streams. SCStory employs self-supervised and continual learning with a novel idea of story-indicative adaptive modeling of news article streams. With a lightweight hierarchical embedding module that first learns sentence representations and then article representations, SCStory identifies story-relevant information of news articles and uses them to discover stories. The embedding module is continuously updated to adapt to evolving news streams with a contrastive learning objective, backed up by two unique techniques, confidence-aware memory replay and prioritized-augmentation, employed for label absence and data scarcity problems. Thorough experiments on real and the latest news data sets demonstrate that SCStory outperforms existing state-of-the-art algorithms for unsupervised online story discovery. | Susik Yoon, Yu Meng, Dongha Lee, Jiawei Han | University of Illinois at Urbana-Champaign, USA; Yonsei University, Republic of Korea |
| 318 |  |  [Know Your Transactions: Real-time and Generic Transaction Semantic Representation on Blockchain & Web3 Ecosystem](https://doi.org/10.1145/3543507.3583537) |  | 0 | Web3, based on blockchain technology, is the evolving next generation Internet of value. Massive active applications on Web3, e.g. DeFi and NFT, usually rely on blockchain transactions to achieve value transfer as well as complex and diverse custom logic and intentions. Various risky or illegal behaviors such as financial fraud, hacking, money laundering are currently rampant in the blockchain ecosystem, and it is thus important to understand the intent behind the pseudonymous transactions. To reveal the intent of transactions, much effort has been devoted to extracting some particular transaction semantics through specific expert experiences. However, the limitations of existing methods in terms of effectiveness and generalization make it difficult to extract diverse transaction semantics in the rapidly growing and evolving Web3 ecosystem. In this paper, we propose the Motif-based Transaction Semantics representation method (MoTS), which can capture the transaction semantic information in the real-time transaction data workflow. To the best of our knowledge, MoTS is the first general semantic extraction method in Web3 blockchain ecosystem. Experimental results show that MoTS can effectively distinguish different transaction semantics in real-time, and can be used for various downstream tasks, giving new insights to understand the Web3 blockchain ecosystem. Our codes are available at https://github.com/wuzhy1ng/MoTS. | Zhiying Wu, Jieli Liu, Jiajing Wu, Zibin Zheng, Xiapu Luo, Ting Chen | Hong Kong Polytechnic University, China; Sun Yat-sen University, China; University of Electronic Science and Technologyo of China, China |
| 319 |  |  [Toward Open-domain Slot Filling via Self-supervised Co-training](https://doi.org/10.1145/3543507.3583541) |  | 0 | Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called SCot, that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised cotraining mechanism, where both models automatically select highconfidence soft labels to further improve the performance of the other in an iterative fashion. Our thorough evaluations show that SCot outperforms state-of-the-art models by 45.57% and 37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed framework SCot achieves comparable performance when compared to state-of-the-art fully supervised models. | Adib Mosharrof, Moghis Fereidouni, A. B. Siddique | University of Kentucky, USA |
| 320 |  |  [Measuring and Evading Turkmenistan's Internet Censorship: A Case Study in Large-Scale Measurements of a Low-Penetration Country](https://doi.org/10.1145/3543507.3583189) |  | 0 | Since 2006, Turkmenistan has been listed as one of the few Internet enemies by Reporters without Borders due to its extensively censored Internet and strictly regulated information control policies. Existing reports of filtering in Turkmenistan rely on a small number of vantage points or test a small number of websites. Yet, the country's poor Internet adoption rates and small population can make more comprehensive measurement challenging. With a population of only six million people and an Internet penetration rate of only 38%, it is challenging to either recruit in-country volunteers or obtain vantage points to conduct remote network measurements at scale. We present the largest measurement study to date of Turkmenistan's Web censorship. To do so, we developed TMC, which tests the blocking status of millions of domains across the three foundational protocols of the Web (DNS, HTTP, and HTTPS). Importantly, TMC does not require access to vantage points in the country. We apply TMC to 15.5M domains, our results reveal that Turkmenistan censors more than 122K domains, using different blocklists for each protocol. We also reverse-engineer these censored domains, identifying 6K over-blocking rules causing incidental filtering of more than 5.4M domains. Finally, we use Geneva, an open-source censorship evasion tool, to discover five new censorship evasion strategies that can defeat Turkmenistan's censorship at both transport and application layers. We will publicly release both the data collected by TMC and the code for censorship evasion. | Sadia Nourin, Van Tran, Xi Jiang, Kevin Bock, Nick Feamster, Nguyen Phong Hoang, Dave Levin | University of Maryland, USA; University of Chicago, USA |
| 321 |  |  [On How Zero-Knowledge Proof Blockchain Mixers Improve, and Worsen User Privacy](https://doi.org/10.1145/3543507.3583217) |  | 0 | Zero-knowledge proof (ZKP) mixers are one of the most widely-used blockchain privacy solutions, operating on top of smart contract-enabled blockchains. We find that ZKP mixers are tightly intertwined with the growing number of Decentralized Finance (DeFi) attacks and Blockchain Extractable Value (BEV) extractions. Through coin flow tracing, we discover that 205 blockchain attackers and 2,595 BEV extractors leverage mixers as their source of funds, while depositing a total attack revenue of 412.87M USD. Moreover, the US OFAC sanctions against the largest ZKP mixer, Tornado.Cash, have reduced the mixer's daily deposits by more than 80%. Further, ZKP mixers advertise their level of privacy through a so-called anonymity set size, which similarly to k-anonymity allows a user to hide among a set of k other users. Through empirical measurements, we, however, find that these anonymity set claims are mostly inaccurate. For the most popular mixers on Ethereum (ETH) and Binance Smart Chain (BSC), we show how to reduce the anonymity set size on average by 27.34% and 46.02% respectively. Our empirical evidence is also the first to suggest a differing privacy-predilection of users on ETH and BSC. State-of-the-art ZKP mixers are moreover interwoven with the DeFi ecosystem by offering anonymity mining (AM) incentives, i.e., users receive monetary rewards for mixing coins. However, contrary to the claims of related work, we find that AM does not necessarily improve the quality of a mixer's anonymity set. Our findings indicate that AM attracts privacy-ignorant users, who then do not contribute to improving the privacy of other mixer users. | Zhipeng Wang, Stefanos Chaliasos, Kaihua Qin, Liyi Zhou, Lifeng Gao, Pascal Berrang, Benjamin Livshits, Arthur Gervais | UCL, United Kingdom and UC Berkeley, USA; University of Birmingham, United Kingdom; Imperial College London, United Kingdom |
| 322 |  |  [NetGuard: Protecting Commercial Web APIs from Model Inversion Attacks using GAN-generated Fake Samples](https://doi.org/10.1145/3543507.3583224) |  | 0 | Recently more and more cloud service providers (e.g., Microsoft, Google, and Amazon) have commercialized their well-trained deep learning models by providing limited access via web API interfaces. However, it is shown that these APIs are susceptible to model inversion attacks, where attackers can recover the training data with high fidelity, which may cause serious privacy leakage.Existing defenses against model inversion attacks, however, hinder the model performance and are ineffective for more advanced attacks, e.g., Mirror [4]. In this paper, we proposed NetGuard, a novel utility-aware defense methodology against model inversion attacks (MIAs). Unlike previous works that perturb prediction outputs of the victim model, we propose to mislead the MIA effort by inserting engineered fake samples during the training process. A generative adversarial network (GAN) is carefully built to construct fake training samples to mislead the attack model without degrading the performance of the victim model. Besides, we adopt continual learning to further improve the utility of the victim model. Extensive experiments on CelebA, VGG-Face, and VGG-Face2 datasets show that NetGuard is superior to existing defenses, including DP [37] and Ad-mi [32] on state-of-the-art model inversion attacks, i.e., DMI [8], Mirror [4], Privacy [12], and Alignment [34]. | Xueluan Gong, Ziyao Wang, Yanjiao Chen, Qian Wang, Cong Wang, Chao Shen | Zhejiang University, China; City University of Hong Kong, China; Xi'an Jiaotong University, China; Wuhan University, China |
| 323 |  |  [Meteor: Improved Secure 3-Party Neural Network Inference with Reducing Online Communication Costs](https://doi.org/10.1145/3543507.3583272) |  | 0 | Secure neural network inference has been a promising solution to private Deep-Learning-as-a-Service, which enables the service provider and user to execute neural network inference without revealing their private inputs. However, the expensive overhead of current schemes is still an obstacle when applied in real applications. In this work, we present Meteor, an online communication-efficient and fast secure 3-party computation neural network inference system aginst semi-honest adversary in honest-majority. The main contributions of Meteor are two-fold: i) We propose a new and improved 3-party secret sharing scheme stemming from the linearity of replicated secret sharing, and design efficient protocols for the basic cryptographic primitives, including linear operations, multiplication, most significant bit extraction, and multiplexer. ii) Furthermore, we build efficient and secure blocks for the widely used neural network operators such as Matrix Multiplication, ReLU, and Maxpool, along with exploiting several specific optimizations for better efficiency. Our total communication with the setup phase is a little larger than SecureNN (PoPETs’19) and Falcon (PoPETs’21), two state-of-the-art solutions, but the gap is not significant when the online phase must be optimized as a priority. Using Meteor, we perform extensive evaluations on various neural networks. Compared to SecureNN and Falcon, we reduce the online communication costs by up to 25.6 × and 1.5 ×, and improve the running-time by at most 9.8 × (resp. 8.1 ×) and 1.5 × (resp. 2.1 ×) in LAN (resp. WAN) for the online inference. | Ye Dong, Xiaojun Chen, Weizhan Jing, Kaiyun Li, Weiping Wang | Institute of Information Engineering,Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China; Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China |
| 324 |  |  [IRWArt: Levering Watermarking Performance for Protecting High-quality Artwork Images](https://doi.org/10.1145/3543507.3583489) |  | 0 | Increasing artwork plagiarism incidents underscores the urgent need for reliable copyright protection for high-quality artwork images. Although watermarking is helpful to this issue, existing methods are limited in imperceptibility and robustness. To provide high-level protection for valuable artwork images, we propose a novel invisible robust watermarking framework, dubbed as IRWArt. In our architecture, the embedding and recovery of the watermark are treated as a pair of image transformations’ inverse problems, and can be implemented through the forward and backward processes of an invertible neural networks (INN), respectively. For high visual quality, we embed the watermark in high-frequency domains with minimal impact on artwork and supervise image reconstruction using a human visual system(HVS)-consistent deep perceptual loss. For strong plagiarism-resistant, we construct a quality enhancement module for the embedded image against possible distortions caused by plagiarism actions. Moreover, the two-stagecontrastive training strategy enables the simultaneous realization of the above two goals. Experimental results on 4 datasets demonstrate the superiority of our IRWArt over other state-of-the-art watermarking methods. Code: https://github.com/1024yy/IRWArt. | Yuanjing Luo, Tongqing Zhou, Fang Liu, Zhiping Cai | National University of Defense Technology, China; Hunan University, China |
| 325 |  |  [CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal Pre-trained Knowledge](https://doi.org/10.1145/3543507.3583232) |  | 0 | Automatically generating textual descriptions for massive unlabeled images on the web can greatly benefit realistic web applications, e.g. multimodal retrieval and recommendation. However, existing models suffer from the problem of generating \`\`over-generic'' descriptions, such as their tendency to generate repetitive sentences with common concepts for different images. These generic descriptions fail to provide sufficient textual semantics for ever-changing web images. Inspired by the recent success of Vision-Language Pre-training (VLP) models that learn diverse image-text concept alignment during pretraining, we explore leveraging their cross-modal pre-trained knowledge to automatically enrich the textual semantics of image descriptions. With no need for additional human annotations, we propose a plug-and-play framework, i.e CapEnrich, to complement the generic image descriptions with more semantic details. Specifically, we first propose an automatic data-building strategy to get desired training sentences, based on which we then adopt prompting strategies, i.e. learnable and template prompts, to incentivize VLP models to generate more textual details. For learnable templates, we fix the whole VLP model and only tune the prompt vectors, which leads to two advantages: 1) the pre-training knowledge of VLP models can be reserved as much as possible to describe diverse visual concepts; 2) only lightweight trainable parameters are required, so it is friendly to low data resources. Extensive experiments show that our method significantly improves the descriptiveness and diversity of generated sentences for web images. The code is available at https://github.com/yaolinli/CapEnrich. | Linli Yao, Weijing Chen, Qin Jin | School of Information, Renmin University of China, China |
| 326 |  |  [MLN4KB: an efficient Markov logic network engine for large-scale knowledge bases and structured logic rules](https://doi.org/10.1145/3543507.3583248) |  | 0 | Markov logic network (MLN) is a powerful statistical modeling framework for probabilistic logic reasoning. Despite the elegancy and effectiveness of MLN, the inference of MLN is known to suffer from an efficiency issue. Even the state-of-the-art MLN engines can not scale to medium-size real-world knowledge bases in the open-world setting, i.e., all unobserved facts in the knowledge base need predictions. In this work, by focusing on a certain class of first-order logic rules that are sufficiently expressive, we develop a highly efficient MLN inference engine called MLN4KB that can leverage the sparsity of knowledge bases. MLN4KB enjoys quite strong theoretical properties; its space and time complexities can be exponentially smaller than existing MLN engines. Experiments on both synthetic and real-world knowledge bases demonstrate the effectiveness of the proposed method. MLN4KB is orders of magnitudes faster (more than 103 times faster on some datasets) than existing MLN engines in the open-world setting. Without any approximation tricks, MLN4KB can scale to real-world knowledge bases including WN-18 and YAGO3-10 and achieve decent prediction accuracy without bells and whistles. We implement MLN4KB as a Julia package called MLN4KB.jl. The package supports both maximum a posteriori (MAP) inference and learning the weights of rules. MLN4KB.jl is public available at https://github.com/baidu-research/MLN4KB . | Huang Fang, Yang Liu, Yunfeng Cai, Mingming Sun | Baidu, China |
| 327 |  |  [Learning Long- and Short-term Representations for Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3543507.3583242) |  | 0 | Temporal Knowledge graph (TKG) reasoning aims to predict missing facts based on historical TKG data. Most of the existing methods are incapable of explicitly modeling the long-term time dependencies from history and neglect the adaptive integration of the long- and short-term information. To tackle these problems, we propose a novel method that utilizes a designed Hierarchical Relational Graph Neural Network to learn the Long- and Short-term representations for TKG reasoning, namely HGLS. Specifically, to explicitly associate entities in different timestamps, we first transform the TKG into a global graph. Based on the built graph, we design a Hierarchical Relational Graph Neural Network that executes in two levels: The sub-graph level is to capture the semantic dependencies within concurrent facts of each KG. And the global-graph level aims to model the temporal dependencies between entities. Furthermore, we design a module to extract the long- and short-term information from the output of these two levels. Finally, the long- and short-term representations are fused into a unified one by Gating Integration for entity prediction. Extensive experiments on four datasets demonstrate the effectiveness of HGLS. | Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, Liang Wang | School of Artificial Intelligence, University of Chinese Academy of Sciences, China and CRIPAC,MAIS, Institute of Automation, Chinese Academy of Sciences, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, China and CRIPAC, MAIS, Institute of Automation, Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China and Institute of Information Engineering, Chinese Academy of Sciences, China |
| 328 |  |  [Mutually-paced Knowledge Distillation for Cross-lingual Temporal Knowledge Graph Reasoning](https://doi.org/10.1145/3543507.3583407) |  | 0 | This paper investigates cross-lingual temporal knowledge graph reasoning problem, which aims to facilitate reasoning on Temporal Knowledge Graphs (TKGs) in low-resource languages by transfering knowledge from TKGs in high-resource ones. The cross-lingual distillation ability across TKGs becomes increasingly crucial, in light of the unsatisfying performance of existing reasoning methods on those severely incomplete TKGs, especially in low-resource languages. However, it poses tremendous challenges in two aspects. First, the cross-lingual alignments, which serve as bridges for knowledge transfer, are usually too scarce to transfer sufficient knowledge between two TKGs. Second, temporal knowledge discrepancy of the aligned entities, especially when alignments are unreliable, can mislead the knowledge distillation process. We correspondingly propose a mutually-paced knowledge distillation model MP-KD, where a teacher network trained on a source TKG can guide the training of a student network on target TKGs with an alignment module. Concretely, to deal with the scarcity issue, MP-KD generates pseudo alignments between TKGs based on the temporal information extracted by our representation module. To maximize the efficacy of knowledge transfer and control the noise caused by the temporal knowledge discrepancy, we enhance MP-KD with a temporal cross-lingual attention mechanism to dynamically estimate the alignment strength. The two procedures are mutually paced along with model training. Extensive experiments on twelve cross-lingual TKG transfer tasks in the EventKG benchmark demonstrate the effectiveness of the proposed MP-KD method. | Ruijie Wang, Zheng Li, Jingfeng Yang, Tianyu Cao, Chao Zhang, Bing Yin, Tarek F. Abdelzaher | Amazon.com Inc, USA; School of Computational Science and Engineering, Georgia Institute of Technology, USA; Department of Computer Science, University of Illinois Urbana-Champaign, USA |
| 329 |  |  [Large-Scale Analysis of New Employee Network Dynamics](https://doi.org/10.1145/3543507.3583400) |  | 0 | The COVID-19 pandemic has accelerated digital transformations across industries, but also introduced new challenges into workplaces, including the difficulties of effectively socializing with colleagues when working remotely. This challenge is exacerbated for new employees who need to develop workplace networks from the outset. In this paper, by analyzing a large-scale telemetry dataset of more than 10,000 Microsoft employees who joined the company in the first three months of 2022, we describe how new employees interact and telecommute with their colleagues during their \`\`onboarding'' period. Our results reveal that although new hires are gradually expanding networks over time, there still exists significant gaps between their network statistics and those of tenured employees even after the six-month onboarding phase. We also observe that heterogeneity exists among new employees in how their networks change over time, where employees whose job tasks do not necessarily require extensive and diverse connections could be at a disadvantaged position in this onboarding process. By investigating how web-based people recommendations in organizational knowledge base facilitate new employees naturally expand their networks, we also demonstrate the potential of web-based applications for addressing the aforementioned socialization challenges. Altogether, our findings provide insights on new employee network dynamics in remote and hybrid work environments, which may help guide organizational leaders and web application developers on quantifying and improving the socialization experiences of new employees in digital workplaces. | Yulin Yu, Longqi Yang, Siân Lindley, Mengting Wan | Microsoft Research, United Kingdom; School of Information, University of Michigan, USA; Microsoft, USA |
| 330 |  |  [MassNE: Exploring Higher-Order Interactions with Marginal Effect for Massive Battle Outcome Prediction](https://doi.org/10.1145/3543507.3583390) |  | 0 | In online games, predicting massive battle outcomes is a fundamental task of many applications, such as team optimization and tactical formulation. Existing works do not pay adequate attention to the massive battle. They either seek to evaluate individuals in isolation or mine simple pair-wise interactions between individuals, neither of which effectively captures the intricate interactions between massive units (e.g., individuals). Furthermore, as the team size increases, the phenomenon of diminishing marginal utility of units emerges. Such a diminishing pattern is rarely noticed in previous work, and how to capture it from data remains a challenge. To this end, we propose a novel Massive battle outcome predictor with margiNal Effect modules, namely MassNE, which comprehensively incorporates individual effects, cooperation effects (i.e., intra-team interactions) and suppression effects (i.e., inter-team interactions) for predicting battle outcomes. Specifically, we design marginal effect modules to learn how units’ marginal utility changing respect to their number, where the monotonicity assumption is applied to ensure rationality. In addition, we evaluate the current classical models and provide mathematical proofs that MassNE is able to generalize several earlier works in massive settings. Massive battle datasets generated by StarCraft II APIs are adopted to evaluate the performances of MassNE. Extensive experiments empirically demonstrate the effectiveness of MassNE, and MassNE can reveal reasonable cooperation effects, suppression effects, and marginal utilities of combat units from the data. | Yin Gu, Kai Zhang, Qi Liu, Xin Lin, Zhenya Huang, Enhong Chen | Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, China; Anhui Province Key Laboratory of Big Data Analysis and Application, University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China |
| 331 |  |  [Online Advertising in Ukraine and Russia During the 2022 Russian Invasion](https://doi.org/10.1145/3543507.3583484) |  | 0 | Online ads are a major source of information on the web. The mass reach of online advertising is often leveraged for information dissemination, at times with an objective to influence public opinion (e.g., election misinformation). We hypothesized that online advertising, due to its reach and potential, might have been used to spread information around the 2022 Russian invasion of Ukraine. Thus, to understand the online ad ecosystem during this conflict, we conducted a five-month long large-scale measurement study of online advertising in Ukraine, Russia, and the US. We studied advertising trends of ad platforms that delivered ads in Ukraine, Russia, and the US and conducted an in-depth qualitative analysis of the conflict-related ad content. We found that prominent US-based advertisers continued to support Russian websites, and a portion of online ads were used to spread conflict-related information, including protesting the invasion, and spreading awareness, which might have otherwise potentially been censored in Russia. | Christina Yeung, Umar Iqbal, Yekaterina Tsipenyuk O'Neil, Tadayoshi Kohno, Franziska Roesner | Security and Privacy Research Lab, University of Washington, USA; Micro Focus, USA |
| 332 |  |  [Understanding the Behaviors of Toxic Accounts on Reddit](https://doi.org/10.1145/3543507.3583522) |  | 0 | Toxic comments are the top form of hate and harassment experienced online. While many studies have investigated the types of toxic comments posted online, the effects that such content has on people, and the impact of potential defenses, no study has captured the behaviors of the accounts that post toxic comments or how such attacks are operationalized. In this paper, we present a measurement study of 929K accounts that post toxic comments on Reddit over an 18 month period. Combined, these accounts posted over 14 million toxic comments that encompass insults, identity attacks, threats of violence, and sexual harassment. We explore the impact that these accounts have on Reddit, the targeting strategies that abusive accounts adopt, and the distinct patterns that distinguish classes of abusive accounts. Our analysis informs the nuanced interventions needed to curb unwanted toxic behaviors online. | Deepak Kumar, Jeff T. Hancock, Kurt Thomas, Zakir Durumeric | Stanford University, USA; Google, USA |
| 333 |  |  [Online Reviews Are Leading Indicators of Changes in K-12 School Attributes](https://doi.org/10.1145/3543507.3583531) |  | 0 | School rating websites are increasingly used by parents to assess the quality and fit of U.S. K-12 schools for their children. These online reviews often contain detailed descriptions of a school’s strengths and weaknesses, which both reflect and inform perceptions of a school. Existing work on these text reviews has focused on finding words or themes that underlie these perceptions, but has stopped short of using the textual reviews as leading indicators of school performance. In this paper, we investigate to what extent the language used in online reviews of a school is predictive of changes in the attributes of that school, such as its socio-economic makeup and student test scores. Using over 300K reviews of 70K U.S. schools from a popular ratings website, we apply language processing models to predict whether schools will significantly increase or decrease in an attribute of interest over a future time horizon. We find that using the text improves predictive performance significantly over a baseline model that does not include text but only the historical time-series of the indicators themselves, suggesting that the review text carries predictive power. A qualitative analysis of the most predictive terms and phrases used in the text reviews indicates a number of topics that serve as leading indicators, such as diversity, changes in school leadership, a focus on testing, and school safety. | Linsen Li, Aron Culotta, Douglas N. Harris, Nicholas Mattei | Department of Economics, Tulane University, USA; Department of Computer Science, Tulane University, USA |
| 334 |  |  [Beyond Fine-Tuning: Efficient and Effective Fed-Tuning for Mobile/Web Users](https://doi.org/10.1145/3543507.3583212) |  | 0 | Fine-tuning is a typical mechanism to achieve model adaptation for mobile/web users, where a model trained by the cloud is further retrained to fit the target user task. While traditional fine-tuning has been proved effective, it only utilizes local data to achieve adaptation, failing to take advantage of the valuable knowledge from other mobile/web users. In this paper, we attempt to extend the local-user fine-tuning to multi-user fed-tuning with the help of Federated Learning (FL). Following the new paradigm, we propose EEFT, a framework aiming to achieve Efficient and Effective Fed-Tuning for mobile/web users. The key idea is to introduce lightweight but effective adaptation modules to the pre-trained model, such that we can freeze the pre-trained model and just focus on optimizing the modules to achieve cost reduction and selective task cooperation. Extensive experiments on our constructed benchmark demonstrate the effectiveness and efficiency of the proposed framework. | Bingyan Liu, Yifeng Cai, Hongzhe Bi, Ziqi Zhang, Ding Li, Yao Guo, Xiangqun Chen | Peking University, China; Beijing University of Posts and Telecommunications, China |
| 335 |  |  [Automated WebAssembly Function Purpose Identification With Semantics-Aware Analysis](https://doi.org/10.1145/3543507.3583235) |  | 0 | WebAssembly is a recent web standard built for better performance in web applications. The standard defines a binary code format to use as a compilation target for a variety of languages, such as C, C++, and Rust. The standard also defines a text representation for readability, although, WebAssembly modules are difficult to interpret by human readers, regardless of their experience level. This makes it difficult to understand and maintain any existing WebAssembly code. As a result, third-party WebAssembly modules need to be implicitly trusted by developers as verifying the functionality themselves may not be feasible. To this end, we construct WASPur, a tool to automatically identify the purposes of WebAssembly functions. To build this tool, we first construct an extensive collection of WebAssembly samples that represent the state of WebAssembly. Second, we analyze the dataset and identify the diverse use cases of the collected WebAssembly modules. We leverage the dataset of WebAssembly modules to construct semantics-aware intermediate representations (IR) of the functions in the modules. We encode the function IR for use in a machine learning classifier, and we find that this classifier can predict the similarity of a given function against known named functions with an accuracy rate of 88.07%. We hope our tool will enable inspection of optimized and minified WebAssembly modules that remove function names and most other semantic identifiers. | Alan Romano, Weihang Wang | University of Southern California, USA |
| 336 |  |  [SCTAP: Supporting Scenario-Centric Trigger-Action Programming based on Software-Defined Physical Environments](https://doi.org/10.1145/3543507.3583293) |  | 0 | The physical world we live in is accelerating digitalization with the vigorous development of Internet of Things (IoT). Following this trend, Web of Things (WoT) further enables fast and efficient creation of various applications that perceive and act on the physical world using standard Web technologies. A popular way for creating WoT applications is Trigger-Action Programming (TAP), which allows users to orchestrate the capabilities of IoT devices in the form of “if trigger, then action”. However, existing TAP approaches don’t support scenario-centric WoT applications which involve abstract modeling of physical environments and complex spatio-temporal dependencies between events and actions. In this paper, we propose an approach called SCTAP which supports Scenario-Centric Trigger-Action Programming based on software-defined physical environments. SCTAP defines a structured and conceptual representation for physical environments, which provides the required programming abstractions for WoT applications. Based on the representation, SCTAP defines a grammar for specifying scenario-centric WoT applications with spatio-temporal dependencies. Furthermore, we design a service-based architecture for SCTAP which supports the integration of device access, event perception, environment representation, and rule execution in a loosely-coupled and extensible way. We implement SCTAP as a WoT infrastructure and evaluate it with two case studies including a smart laboratory and a smart coffee house. The results confirm the usability, feasibility and efficiency of SCTAP and its implementation. | Bingkun Sun, Liwei Shen, Xin Peng, Ziming Wang | School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China |
| 337 |  |  [DeeProphet: Improving HTTP Adaptive Streaming for Low Latency Live Video by Meticulous Bandwidth Prediction](https://doi.org/10.1145/3543507.3583364) |  | 0 | The performance of HTTP adaptive streaming (HAS) depends heavily on the prediction of end-to-end network bandwidth. The increasingly popular low latency live streaming (LLLS) faces greater challenges since it requires accurate, short-term bandwidth prediction, compared with VOD streaming which needs long-term bandwidth prediction and has good tolerance against prediction error. Part of the challenges comes from the fact that short-term bandwidth experiences both large abrupt changes and uncertain fluctuations. Additionally, it is hard to obtain valid bandwidth measurement samples in LLLS due to its inter-chunk and intra-chunk sending idleness. In this work, we present DeeProphet, a system for accurate bandwidth prediction in LLLS to improve the performance of HAS. DeeProphet overcomes the above challenges by collecting valid measurement samples using fine-grained TCP state information to identify the packet bursting intervals, and by combining the time series model and learning-based model to predict both large change and uncertain fluctuations. Experiment results show that DeeProphet improves the overall QoE by 17.7%-359.2% compared with state-of-the-art LLLS ABR algorithms, and reduces the median bandwidth prediction error to 2.7%. | Kefan Chen, Bo Wang, Wufan Wang, Xiaoyu Li, Fengyuan Ren | Beijing Institute of Technology, China; Tsinghua University, China and Zhongguancun Laboratory, China; Tsinghua University, China |
| 338 |  |  [Is IPFS Ready for Decentralized Video Streaming?](https://doi.org/10.1145/3543507.3583404) |  | 0 | InterPlanetary File System (IPFS) is a peer-to-peer protocol for decentralized content storage and retrieval. The IPFS platform has the potential to help users evade censorship and avoid a central point of failure. IPFS is seeing increasing adoption for distributing various kinds of files, including video. However, the performance of video streaming on IPFS has not been well-studied. We conduct a measurement study with over 28,000 videos hosted on the IPFS network and find that video streaming experiences high stall rates due to relatively high Round Trip Times (RTT). Further, videos are encoded using a single static quality, because of which streaming cannot adapt to different network conditions. A natural approach is to use adaptive bitrate (ABR) algorithms for streaming, which encode videos in multiple qualities and streams according to the throughput available. However, traditional ABR algorithms perform poorly on IPFS because the throughput cannot be estimated correctly. The main problem is that video segments can be retrieved from multiple sources, making it difficult to estimate the throughput. To overcome this issue, we have designed Telescope, an IPFS-aware ABR system. We conduct experiments on the IPFS network, where IPFS video providers are geographically distributed across the globe. Our results show that Telescope significantly improves the Quality of Experience (QoE) of videos, for a diverse set of network and cache conditions, compared to traditional ABR. | Zhengyu Wu, ChengHao Ryan Yang, Santiago Vargas, Aruna Balasubramanian | Computer Science, Stony Brook University, USA; Computer Science, Northeastern University, USA |
| 339 |  |  [SISSI: An Architecture for Semantic Interoperable Self-Sovereign Identity-based Access Control on the Web](https://doi.org/10.1145/3543507.3583409) |  | 0 | We present an architecture for authentication and authorization on the Web that is based on the Self-Sovereign Identity paradigm. Using our architecture, we aim to achieve semantic interoperability across different approaches to SSI. We build on the underlying RDF data model of the W3C’s recommendation for Verifiable Credentials and specify semantic access control rules using SHACL. Our communication protocol for an authorization process is based on Decentralised Identifiers and extends the Hyperledger Aries Present Proof protocol. We propose a modular architecture that allows for flexible extension, e. g., for supporting more signature schemes or Decentralised Identifier Methods. For evaluation, we implemented a Proof-of-Concept: We show that a Web-based approach to SSI outperfoms a blockchain-based approach to SSI in terms of End-to-End execution time. | Christoph H.J. Braun, Vasil Papanchev, Tobias Käfer | Karlsruhe Institute of Technology, Germany |
| 340 |  |  [To Store or Not? Online Data Selection for Federated Learning with Limited Storage](https://doi.org/10.1145/3543507.3583426) |  | 0 | Machine learning models have been deployed in mobile networks to deal with massive data from different layers to enable automated network management and intelligence on devices. To overcome high communication cost and severe privacy concerns of centralized machine learning, federated learning (FL) has been proposed to achieve distributed machine learning among networked devices. While the computation and communication limitation has been widely studied, the impact of on-device storage on the performance of FL is still not explored. Without an effective data selection policy to filter the massive streaming data on devices, classical FL can suffer from much longer model training time ($4\times$) and significant inference accuracy reduction ($7\%$), observed in our experiments. In this work, we take the first step to consider the online data selection for FL with limited on-device storage. We first define a new data valuation metric for data evaluation and selection in FL with theoretical guarantees for speeding up model convergence and enhancing final model accuracy, simultaneously. We further design {\ttfamily ODE}, a framework of \textbf{O}nline \textbf{D}ata s\textbf{E}lection for FL, to coordinate networked devices to store valuable data samples. Experimental results on one industrial dataset and three public datasets show the remarkable advantages of {\ttfamily ODE} over the state-of-the-art approaches. Particularly, on the industrial dataset, {\ttfamily ODE} achieves as high as $2.5\times$ speedup of training time and $6\%$ increase in inference accuracy, and is robust to various factors in practical environments. | Chen Gong, Zhenzhe Zheng, Fan Wu, Yunfeng Shao, Bingshuai Li, Guihai Chen | Department of Computer Science and Engineering, Shanghai Jiao Tong University, China |
| 341 |  |  [Detecting Socially Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks](https://doi.org/10.1145/3543507.3583452) |  | 0 | With the rapid development of Internet of Things technologies, the next generation traffic monitoring infrastructures are connected via the web, to aid traffic data collection and intelligent traffic management. One of the most important tasks in traffic is anomaly detection, since abnormal drivers can reduce traffic efficiency and cause safety issues. This work focuses on detecting abnormal driving behaviors from trajectories produced by highway video surveillance systems. Most of the current abnormal driving behavior detection methods focus on a limited category of abnormal behaviors that deal with a single vehicle without considering vehicular interactions. In this work, we consider the problem of detecting a variety of socially abnormal driving behaviors, i.e., behaviors that do not conform to the behavior of other nearby drivers. This task is complicated by the variety of vehicular interactions and the spatial-temporal varying nature of highway traffic. To solve this problem, we propose an autoencoder with a Recurrent Graph Attention Network that can capture the highway driving behaviors contextualized on the surrounding cars, and detect anomalies that deviate from learned patterns. Our model is scalable to large freeways with thousands of cars. Experiments on data generated from traffic simulation software show that our model is the only one that can spot the exact vehicle conducting socially abnormal behaviors, among the state-of-the-art anomaly detection models. We further show the performance on real world HighD traffic dataset, where our model detects vehicles that violate the local driving norms. | Yue Hu, Yuhang Zhang, Yanbing Wang, Daniel B. Work | Vanderbilt University, USA |
| 342 |  |  [GROUP: An End-to-end Multi-step-ahead Workload Prediction Approach Focusing on Workload Group Behavior](https://doi.org/10.1145/3543507.3583460) |  | 0 | Accurately forecasting workloads can enable web service providers to achieve proactive runtime management for applications and ensure service quality and cost efficiency. For cloud-native applications, multiple containers collaborate to handle user requests, making each container’s workload changes influenced by workload group behavior. However, existing approaches mainly analyze the individual changes of each container and do not explicitly model the workload group evolution of containers, resulting in sub-optimal results. Therefore, we propose a workload prediction method, GROUP, which implements the shifts of workload prediction focus from individual to group, workload group behavior representation from data similarity to data correlation, and workload group behavior evolution from implicit modeling to explicit modeling. First, we model the workload group behavior and its evolution from multiple perspectives. Second, we propose a container correlation calculation algorithm that considers static and dynamic container information to represent the workload group behavior. Third, we propose an end-to-end multi-step-ahead prediction method that explicitly portrays the complex relationship between the evolution of workload group behavior and the workload changes of each container. Lastly, enough experiments on public datasets show the advantages of GROUP, which provides an effective solution to achieve workload prediction for cloud-native applications. | Binbin Feng, Zhijun Ding | Tongji University, China |
| 343 |  |  [FANS: Fast Non-Autoregressive Sequence Generation for Item List Continuation](https://doi.org/10.1145/3543507.3583430) |  | 0 | User-curated item lists, such as video-based playlists on Youtube and book-based lists on Goodreads, have become prevalent for content sharing on online platforms. Item list continuation is proposed to model the overall trend of a list and predict subsequent items. Recently, Transformer-based models have shown promise in comprehending contextual information and capturing item relationships in a list. However, deploying them in real-time industrial applications is challenging, mainly because the autoregressive generation mechanism used in them is time-consuming. In this paper, we propose a novel fast non-autoregressive sequence generation model, namely FANS, to enhance inference efficiency and quality for item list continuation. First, we use a non-autoregressive generation mechanism to decode next $K$ items simultaneously instead of one by one in existing models. Then, we design a two-stage classifier to replace the vanilla classifier used in current transformer-based models to further reduce the decoding time. Moreover, to improve the quality of non-autoregressive generation, we employ a curriculum learning strategy to optimize training. Experimental results on four real-world item list continuation datasets including Zhihu, Spotify, AotM, and Goodreads show that our FANS model can significantly improve inference efficiency (up to 8.7x) while achieving competitive or better generation quality for item list continuation compared with the state-of-the-art autoregressive models. We also validate the efficiency of FANS in an industrial setting. Our source code and data will be available at MindSpore/models and Github. | Qijiong Liu, Jieming Zhu, Jiahao Wu, Tiandeng Wu, Zhenhua Dong, XiaoMing Wu | The Hong Kong Polytechnic University, Hong Kong; Huawei Noah's Ark Lab, China; Huawei Technolologies Co., Ltd, China |
| 344 |  |  [DANCE: Learning A Domain Adaptive Framework for Deep Hashing](https://doi.org/10.1145/3543507.3583445) |  | 0 | This paper studies unsupervised domain adaptive hashing, which aims to transfer a hashing model from a label-rich source domain to a label-scarce target domain. Current state-of-the-art approaches generally resolve the problem by integrating pseudo-labeling and domain adaptation techniques into deep hashing paradigms. Nevertheless, they usually suffer from serious class imbalance in pseudo-labels and suboptimal domain alignment caused by the neglection of the intrinsic structures of two domains. To address this issue, we propose a novel method named unbiaseD duAl hashiNg Contrastive lEarning (DANCE) for domain adaptive image retrieval. The core of our DANCE is to perform contrastive learning on hash codes from both instance level and prototype level. To begin, DANCE utilizes label information to guide instance-level hashing contrastive learning in the source domain. To generate unbiased and reliable pseudo-labels for semantic learning in the target domain, we uniformly select samples around each label embedding in the Hamming space. A momentum-update scheme is also utilized to smooth the optimization process. Additionally, we measure the semantic prototype representations in both source and target domains and incorporate them into a domain-aware prototype-level contrastive learning paradigm, which enhances domain alignment in the Hamming space while maximizing the model capacity. Experimental results on a number of well-known domain adaptive retrieval benchmarks validate the effectiveness of our proposed DANCE compared to a variety of competing baselines in different settings. | Haixin Wang, Jinan Sun, Xiang Wei, Shikun Zhang, Chong Chen, XianSheng Hua, Xiao Luo | Peking University, China; Department of Computer Science, UCLA, USA; BIGO Inc., Singapore; Zhejiang University, China |
| 345 |  |  [Differentiable Optimized Product Quantization and Beyond](https://doi.org/10.1145/3543507.3583482) |  | 0 | Vector quantization techniques, such as Product Quantization (PQ), play a vital role in approximate nearest neighbor search (ANNs) and maximum inner product search (MIPS) owing to their remarkable search and storage efficiency. However, the indexes in vector quantization cannot be trained together with the inference models since data indexing is not differentiable. To this end, differentiable vector quantization approaches, such as DiffPQ and DeepPQ, have been recently proposed, but existing methods have two drawbacks. First, they do not impose any constraints on codebooks, such that the resultant codebooks lack diversity, leading to limited retrieval performance. Second, since data indexing resorts to operator, differentiability is usually achieved by either relaxation or Straight-Through Estimation (STE), which leads to biased gradient and slow convergence. To address these problems, we propose a Differentiable Optimized Product Quantization method (DOPQ) and beyond in this paper. Particularly, each data is projected into multiple orthogonal spaces, to generate multiple views of data. Thus, each codebook is learned with one view of data, guaranteeing the diversity of codebooks. Moreover, instead of simple differentiable relaxation, DOPQ optimizes the loss based on direct loss minimization, significantly reducing the gradient bias problem. Finally, DOPQ is evaluated with seven datasets of both recommendation and image search tasks. Extensive experimental results show that DOPQ outperforms state-of-the-art baselines by a large margin. | Zepu Lu, Defu Lian, Jin Zhang, Zaixi Zhang, Chao Feng, Hao Wang, Enhong Chen | University of Science and Technology of China, School of Data Science, China; University of Science and Technology of China, School of Computer Science, School of Data Science, China and State Key Laboratory of Cognitive Intelligence, China; University of Science and Technology of China, School of Computer Science, China |
| 346 |  |  [Auctions without commitment in the auto-bidding world](https://doi.org/10.1145/3543507.3583416) |  | 0 | Advertisers in online ad auctions are increasingly using auto-bidding mechanisms to bid into auctions instead of directly bidding their value manually. One prominent auto-bidding format is the target cost-per-acquisition (tCPA) which maximizes the volume of conversions subject to a return-of-investment constraint. From an auction theoretic perspective however, this trend seems to go against foundational results that postulate that for profit-maximizing bidders, it is optimal to use a classic bidding system like marginal CPA (mCPA) bidding rather than using strategies like tCPA. In this paper we rationalize the adoption of such seemingly sub-optimal bidding within the canonical quasi-linear framework. The crux of the argument lies in the notion of commitment. We consider a multi-stage game where first the auctioneer declares the auction rules; then bidders select either the tCPA or mCPA bidding format and then, if the auctioneer lacks commitment, it can revisit the rules of the auction (e.g., may readjust reserve prices depending on the observed bids). Our main result is that so long as a bidder believes that the auctioneer lacks commitment to follow the rule of the declared auction then the bidder will make a higher profit by choosing the tCPA format over the mCPA format. We then explore the commitment consequences for the auctioneer. In a simplified version of the model where there is only one bidder, we show that the tCPA subgame admits a credible equilibrium while the mCPA format does not. That is, when the bidder chooses the tCPA format the auctioneer can credibly implement the auction rules announced at the beginning of the game. We also show that, under some mild conditions, the auctioneer's revenue is larger when the bidder uses the tCPA format rather than mCPA. We further quantify the value for the auctioneer to be able to commit to the declared auction rules. | Andrés Perlroth, Aranyak Mehta |  |
| 347 |  |  [Online resource allocation in Markov Chains](https://doi.org/10.1145/3543507.3583428) |  | 0 | A large body of work in Computer Science and Operations Research study online algorithms for stochastic resource allocation problems. The most common assumption is that the online requests have randomly generated i.i.d. types. This assumption is well justified for static markets and/or relatively short time periods. We consider dynamic markets, whose states evolve as a random walk in a market-specific Markov Chain. This is a new model that generalizes previous i.i.d. settings. We identify important parameters of the Markov chain that is crucial for obtaining good approximation guarantees to the expected value of the optimal offline algorithm which knows realizations of all requests in advance. We focus on a stylized single-resource setting and: (i) generalize the well-known Prophet Inequality from the optimal stopping theory (single-unit setting) to Markov Chain setting; (ii) in multi-unit setting, design a simple algorithm that is asymptotically optimal under mild assumptions on the underlying Markov chain. | Jianhao Jia, Hao Li, Kai Liu, Ziqi Liu, Jun Zhou, Nikolai Gravin, Zhihao Gavin Tang | Shanghai University of Finance and Economics, China; Ant Group, China |
| 348 |  |  [Worst-Case Welfare of Item Pricing in the Tollbooth Problem](https://doi.org/10.1145/3543507.3583432) |  | 0 | We study the worst-case welfare of item pricing in the \emph{tollbooth problem}. The problem was first introduced by Guruswami et al, and is a special case of the combinatorial auction in which (i) each of the $m$ items in the auction is an edge of some underlying graph; and (ii) each of the $n$ buyers is single-minded and only interested in buying all edges of a single path. We consider the competitive ratio between the hindsight optimal welfare and the optimal worst-case welfare among all item-pricing mechanisms, when the order of the arriving buyers is adversarial. We assume that buyers own the \emph{tie-breaking} power, i.e. they can choose whether or not to buy the demand path at 0 utility. We prove a tight competitive ratio of $3/2$ when the underlying graph is a single path (also known as the \emph{highway} problem), whereas item-pricing can achieve the hindsight optimal if the seller is allowed to choose a proper tie-breaking rule to maximize the welfare. Moreover, we prove an $O(1)$ upper bound of competitive ratio when the underlying graph is a tree. For general graphs, we prove an $\Omega(m^{1/8})$ lower bound of the competitive ratio. We show that an $m^{\Omega(1)}$ competitive ratio is unavoidable even if the graph is a grid, or if the capacity of every edge is augmented by a constant factor $c$. The results hold even if the seller has tie-breaking power. | Zihan Tan, Yifeng Teng, Mingfei Zhao | Center for Discrete Mathematics and Theoretical Computer Science, Rutgers University, USA; Google Research, USA |
| 349 |  |  [Learning to Bid in Contextual First Price Auctions✱](https://doi.org/10.1145/3543507.3583427) |  | 0 | In this paper, we investigate the problem about how to bid in repeated contextual first price auctions. We consider a single bidder (learner) who repeatedly bids in the first price auctions: at each time $t$, the learner observes a context $x_t\in \mathbb{R}^d$ and decides the bid based on historical information and $x_t$. We assume a structured linear model of the maximum bid of all the others $m_t = \alpha_0\cdot x_t + z_t$, where $\alpha_0\in \mathbb{R}^d$ is unknown to the learner and $z_t$ is randomly sampled from a noise distribution $\mathcal{F}$ with log-concave density function $f$. We consider both \emph{binary feedback} (the learner can only observe whether she wins or not) and \emph{full information feedback} (the learner can observe $m_t$) at the end of each time $t$. For binary feedback, when the noise distribution $\mathcal{F}$ is known, we propose a bidding algorithm, by using maximum likelihood estimation (MLE) method to achieve at most $\widetilde{O}(\sqrt{\log(d) T})$ regret. Moreover, we generalize this algorithm to the setting with binary feedback and the noise distribution is unknown but belongs to a parametrized family of distributions. For the full information feedback with \emph{unknown} noise distribution, we provide an algorithm that achieves regret at most $\widetilde{O}(\sqrt{dT})$. Our approach combines an estimator for log-concave density functions and then MLE method to learn the noise distribution $\mathcal{F}$ and linear weight $\alpha_0$ simultaneously. We also provide a lower bound result such that any bidding policy in a broad class must achieve regret at least $\Omega(\sqrt{T})$, even when the learner receives the full information feedback and $\mathcal{F}$ is known. | Ashwinkumar Badanidiyuru, Zhe Feng, Guru Guruganesh | Google, USA |
| 350 |  |  [Efficiency of Non-Truthful Auctions in Auto-bidding: The Power of Randomization](https://doi.org/10.1145/3543507.3583492) |  | 0 | Auto-bidding is now widely adopted as an interface between advertisers and internet advertising as it allows advertisers to specify high-level goals, such as maximizing value subject to a value-per-spend constraint. Prior research has mainly focused on auctions that are truthful (such as a second-price auction) because these auctions admit simple (uniform) bidding strategies and are thus simpler to analyze. The main contribution of this paper is to characterize the efficiency across the spectrum of all auctions, including non-truthful auctions for which optimal bidding may be complex. For deterministic auctions, we show a dominance result: any uniform bidding equilibrium of a second-price auction (SPA) can be mapped to an equilibrium of any other auction – for example, first price auction (FPA) – with identical outcomes. In this sense, SPA with uniform bidding is an instance-wise optimal deterministic auction. Consequently, the price of anarchy (PoA) of any deterministic auction is at least the PoA of SPA with uniform bidding, which is known to be 2. We complement this by showing that the PoA of FPA without uniform bidding is 2. Next, we show, surprisingly, that truthful pricing is not dominant in the randomized setting. There is a randomized version of FPA that achieves a strictly smaller price of anarchy than its truthful counterpart when there are two bidders per query. Furthermore, this randomized FPA achieves the best-known PoA for two bidders, thus showing the power of non-truthfulness when combined with randomization. Finally, we show that no prior-free auction (even randomized, non-truthful) can improve on a PoA bound of 2 when there are a large number of advertisers per auction. These results should be interpreted qualitatively as follows. When the auction pressure is low, randomization and non-truthfulness is beneficial. On the other hand, if the auction pressure is intense, the benefits diminishes and it is optimal to implement a second-price auction. | Christopher Liaw, Aranyak Mehta, Andrés Perlroth | Google, USA |
| 351 |  |  [Platform Behavior under Market Shocks: A Simulation Framework and Reinforcement-Learning Based Study](https://doi.org/10.1145/3543507.3583523) |  | 0 | We study the behavior of an economic platform (e.g., Amazon, Uber Eats, Instacart) under shocks, such as COVID-19 lockdowns, and the effect of different regulation considerations. To this end, we develop a multi-agent simulation environment of a platform economy in a multi-period setting where shocks may occur and disrupt the economy. Buyers and sellers are heterogeneous and modeled as economically-motivated agents, choosing whether or not to pay fees to access the platform. We use deep reinforcement learning to model the fee-setting and matching behavior of the platform, and consider two major types of regulation frameworks: (1) taxation policies and (2) platform fee restrictions. We offer a number of simulated experiments that cover different market settings and shed light on regulatory tradeoffs. Our results show that while many interventions are ineffective with a sophisticated platform actor, we identify a particular kind of regulation—fixing fees to the optimal, no-shock fees while still allowing a platform to choose how to match buyers and sellers—as holding promise for promoting the efficiency and resilience of the economic system. | Xintong Wang, Gary Qiurui Ma, Alon Eden, Clara Li, Alexander Trott, Stephan Zheng, David C. Parkes | Hebrew University of Jerusalem, Israel; Salesforce Research, USA; Harvard University, USA |
| 352 |  |  [Near-Optimal Experimental Design Under the Budget Constraint in Online Platforms](https://doi.org/10.1145/3543507.3583528) |  | 0 | A/B testing, or controlled experiments, is the gold standard approach to causally compare the performance of algorithms on online platforms. However, conventional Bernoulli randomization in A/B testing faces many challenges such as spillover and carryover effects. Our study focuses on another challenge, especially for A/B testing on two-sided platforms -- budget constraints. Buyers on two-sided platforms often have limited budgets, where the conventional A/B testing may be infeasible to be applied, partly because two variants of allocation algorithms may conflict and lead some buyers to exceed their budgets if they are implemented simultaneously. We develop a model to describe two-sided platforms where buyers have limited budgets. We then provide an optimal experimental design that guarantees small bias and minimum variance. Bias is lower when there is more budget and a higher supply-demand rate. We test our experimental design on both synthetic data and real-world data, which verifies the theoretical results and shows our advantage compared to Bernoulli randomization. | Yongkang Guo, Yuan Yuan, Jinshan Zhang, Yuqing Kong, Zhihua Zhu, Zheng Cai | Peking University, China; Purdue University, USA; Tencent Technology (Shenzhen) Co., Ltd., China; Zhejiang University, China |
| 353 |  |  [A Method to Assess and Explain Disparate Impact in Online Retailing](https://doi.org/10.1145/3543507.3583270) |  | 0 | This paper presents a method for assessing whether algorithmic decision making induces disparate impact in online retailing. The proposed method specifies a statistical design, a sampling algorithm, and a technological setup for data collection through web crawling. The statistical design reduces the dimensionality of the problem and ensures that the data collected are representative, variation-rich, and suitable for the investigation of the causes behind any observed disparities. Implementations of the method can collect data on algorithmic decisions, such as price, recommendations, and delivery fees that can be matched to website visitor demographic data from established sources such as censuses and large scale surveys. The combined data can be used to investigate the presence and causes of disparate impact, potentially helping online retailers audit their algorithms without collecting or holding the demographic data of their users. The proposed method is illustrated in the context of the automated pricing decisions of a leading retailer in the United States. A custom-built platform implemented the method to collect data for nearly 20,000 different grocery products at more than 3,000 randomly-selected zip codes. The data collected indicates that prices are higher for locations with high proportions of minority households. Although these price disparities can be partly attributed to algorithmic biases, they are mainly explained by local factors and therefore can be regarded as business necessities. | Rafael BecerrilArreola | University of South Carolina, USA |
| 354 |  |  [Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection](https://doi.org/10.1145/3543507.3583214) |  | 0 | Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules -- shallow decision trees trained on a small number of features -- achieve near-state-of-the-art performance on most available datasets and that bot detection datasets, even when combined together, do not generalize well to out-of-sample datasets. Our findings reveal that predictions are highly dependent on each dataset's collection and labeling procedures rather than fundamental differences between bots and humans. These results have important implications for both transparency in sampling and labeling procedures and potential biases in research using existing bot detection tools for pre-processing. | Chris Hays, Zachary Schutzman, Manish Raghavan, Erin Walk, Philipp Zimmer | Massachusetts Institute of Technology, USA |
| 355 |  |  [A Dataset on Malicious Paper Bidding in Peer Review](https://doi.org/10.1145/3543507.3583424) |  | 0 | In conference peer review, reviewers are often asked to provide "bids" on each submitted paper that express their interest in reviewing that paper. A paper assignment algorithm then uses these bids (along with other data) to compute a high-quality assignment of reviewers to papers. However, this process has been exploited by malicious reviewers who strategically bid in order to unethically manipulate the paper assignment, crucially undermining the peer review process. For example, these reviewers may aim to get assigned to a friend's paper as part of a quid-pro-quo deal. A critical impediment towards creating and evaluating methods to mitigate this issue is the lack of any publicly-available data on malicious paper bidding. In this work, we collect and publicly release a novel dataset to fill this gap, collected from a mock conference activity where participants were instructed to bid either honestly or maliciously. We further provide a descriptive analysis of the bidding behavior, including our categorization of different strategies employed by participants. Finally, we evaluate the ability of each strategy to manipulate the assignment, and also evaluate the performance of some simple algorithms meant to detect malicious bidding. The performance of these detection algorithms can be taken as a baseline for future research on detecting malicious bidding. | Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B. Shah, Fei Fang | Carnegie Mellon University, USA |
| 356 |  |  [Exploring Social Media for Early Detection of Depression in COVID-19 Patients](https://doi.org/10.1145/3543507.3583867) |  | 0 | The COVID-19 pandemic has caused substantial damage to global health. Even though three years have passed, the world continues to struggle with the virus. Concerns are growing about the impact of COVID-19 on the mental health of infected individuals, who are more likely to experience depression, which can have long-lasting consequences for both the affected individuals and the world. Detection and intervention at an early stage can reduce the risk of depression in COVID-19 patients. In this paper, we investigated the relationship between COVID-19 infection and depression through social media analysis. Firstly, we managed a dataset of COVID-19 patients that contains information about their social media activity both before and after infection. Secondly,We conducted an extensive analysis of this dataset to investigate the characteristic of COVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep neural network for early prediction of depression risk. This model considers daily mood swings as a psychiatric signal and incorporates textual and emotional characteristics via knowledge distillation. Experimental results demonstrate that our proposed framework outperforms baselines in detecting depression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has the potential to enable public health organizations to initiate prompt intervention with high-risk patients | Jiageng Wu, Xian Wu, Yining Hua, Shixu Lin, Yefeng Zheng, Jie Yang | Zhejiang University, China; Tencent Jarvis Lab, China; Harvard University, USA |
| 357 |  |  [Identifying Checkworthy CURE Claims on Twitter](https://doi.org/10.1145/3543507.3583870) |  | 0 | Medical claims on social media, if left unchecked, have the potential to directly affect the well-being of consumers of online health information. However, existing studies on claim detection do not specifically focus on medical cure aspects, neither do they address if a cure claim is “checkworthy", an indicator of whether a claim is potentially beneficial or harmful, if unchecked. In this paper, we address these limitations by compiling CW-CURE, a novel dataset of CURE tweets, namely tweets containing claims on prevention, diagnoses, risks, treatments, and cures of medical conditions. CW-CURE contains tweets on four major health conditions, namely, Alzheimer’s disease, Cancer, Diabetes, and Depression annotated for claims, their “checkworthiness", as well as the different types of claims such as quantitative claim, correlation/causation, personal experience, and future prediction. We describe our processing pipeline for compiling CW-CURE and present classification results on CURE tweets using transformer-based models. In particular, we harness claim-type information obtained with zero-shot learning to show significant improvements in checkworthiness identification. Through CW-CURE, we hope to enable research on models for effective identification and flagging of impactful CURE content, to safeguard the public’s consumption of medical content online. | Sujatha Das Gollapalli, Mingzhe Du, SeeKiong Ng | Institute of Data Science, National University of Singapore, Singapore and Centre for Trusted Internet and Community, National University of Singapore, Singapore; Institute of Data Science, National University of Singapore, Singapore and Nanyang Technological University, Singapore |
| 358 |  |  [The Impact of Covid-19 on Online Discussions: the Case Study of the Sanctioned Suicide Forum](https://doi.org/10.1145/3543507.3583879) |  | 0 | The COVID-19 pandemic has been at the center of the lives of many of us for at least a couple of years, during which periods of isolation and lockdowns were common. How all that affected our mental well-being, especially the ones’ who were already in distress? To investigate the matter we analyse the online discussions on Sanctioned Suicide, a forum where users discuss suicide-related topics freely. We collected discussions starting from March 2018 (before pandemic) up to July 2022, for a total of 53K threads with 700K comments and 16K users. We investigate the impact of COVID-19 on the discussions in the forum. The data show that covid, while being present in the discussions, especially during the first lockdown, has not been the main reason why new users registered to the forum. However, covid appears to be indirectly connected to other causes of distress for the users, i.e. anxiety for the economy. | Elisa Sartori, Luca Pajola, Giovanni Da San Martino, Mauro Conti | University of Padua, Italy |
| 359 |  |  [Learning like human annotators: Cyberbullying detection in lengthy social media sessions](https://doi.org/10.1145/3543507.3583873) |  | 0 | The inherent characteristic of cyberbullying of being a recurrent attitude calls for the investigation of the problem by looking at social media sessions as a whole, beyond just isolated social media posts. However, the lengthy nature of social media sessions challenges the applicability and performance of session-based cyberbullying detection models. This is especially true when one aims to use state-of-the-art Transformer-based pre-trained language models, which only take inputs of a limited length. In this paper, we address this limitation of transformer models by proposing a conceptually intuitive framework called LS-CB, which enables cyberbullying detection from lengthy social media sessions. LS-CB relies on the intuition that we can effectively aggregate the predictions made by transformer models on smaller sliding windows extracted from lengthy social media sessions, leading to an overall improved performance. Our extensive experiments with six transformer models on two session-based datasets show that LS-CB consistently outperforms three types of competitive baselines including state-of-the-art cyberbullying detection models. In addition, we conduct a set of qualitative analyses to validate the hypotheses that cyberbullying incidents can be detected through aggregated analysis of smaller chunks derived from lengthy social media sessions (H1), and that cyberbullying incidents can occur at different points of the session (H2), hence positing that frequently used text truncation strategies are suboptimal compared to relying on holistic views of sessions. Our research in turn opens an avenue for fine-grained cyberbullying detection within sessions in future work. | Peiling Yi, Arkaitz Zubiaga | Queen Mary University of London, United Kingdom |
| 360 |  |  [Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks](https://doi.org/10.1145/3543507.3583874) |  | 0 | Collaboration between healthcare institutions can significantly lessen the imbalance in medical resources across various geographic areas. However, directly sharing diagnostic information between institutions is typically not permitted due to the protection of patients' highly sensitive privacy. As a novel privacy-preserving machine learning paradigm, federated learning (FL) makes it possible to maximize the data utility among multiple medical institutions. These feature-enrichment FL techniques are referred to as vertical FL (VFL). Traditional VFL can only benefit multi-parties' shared samples, which strongly restricts its application scope. In order to improve the information-sharing capability and innovation of various healthcare-related institutions, and then to establish a next-generation open medical collaboration network, we propose a unified framework for vertical federated knowledge transfer mechanism (VFedTrans) based on a novel cross-hospital representation distillation component. Specifically, our framework includes three steps. First, shared samples' federated representations are extracted by collaboratively modeling multi-parties' joint features with current efficient vertical federated representation learning methods. Second, for each hospital, we learn a local-representation-distilled module, which can transfer the knowledge from shared samples' federated representations to enrich local samples' representations. Finally, each hospital can leverage local samples' representations enriched by the distillation module to boost arbitrary downstream machine learning tasks. The experiments on real-life medical datasets verify the knowledge transfer effectiveness of our framework. | Chungju Huang, Leye Wang, Xiao Han | School of Information Management and Engineering, Shanghai University of Finance and Economics, China; School of Computer Science, Peking University, China |
| 361 |  |  [On Graph Time-Series Representations for Temporal Networks](https://doi.org/10.1145/3543873.3587301) |  | 0 | Representations of temporal networks arising from a stream of edges lie at the heart of models learned on it and its performance on downstream applications. While previous work on dynamic modeling and embedding have focused on representing a stream of timestamped edges using a time-series of graphs based on a specific time-scale τ (e.g., 1 month), we introduce the notion of an ϵ -graph time-series that uses a fixed number of edges for each graph, and show its effectiveness in capturing fundamental structural graph statistics over time. The results indicate that the ϵ -graph time-series representation effectively captures the structural properties of the graphs across time whereas the commonly used τ -graph time-series representation captures the frequency of edges and temporal patterns with respect to their arrival in the application time. These results have many important implications especially on the design of new GNN-based models for temporal networks as well as for understanding existing models and their limitations. | Ryan A. Rossi, Nesreen K. Ahmed, Namyong Park | Meta, USA; Intel Labs, USA; Adobe Research, USA |
| 362 |  |  [Lower Risks, Better Choices: Stock Correlation Based Portfolio Selection in Stock Markets](https://doi.org/10.1145/3543873.3587298) |  | 0 | Over the past few years, we’ve seen a huge interest in applying AI techniques to develop investment strategies both in academia and the finance industry. However, we note that generating returns is not always the sole investment objective. Take large pension funds for example, they are considerably more risk-averse as opposed to profit-seeking. With this observation, we propose a Risk-balanced Deep Portfolio Constructor (RDPC) that takes risk into explicit consideration. RDPC is an end-to-end reinforcement learning-based transformer trained to optimize both returns and risk, with a hard attention mechanism that learns the relationship between asset pairs, imitating the powerful pairs trading strategy widely adopted by many investors. Experiments on real-world data show that RDPC achieves state-of-the-art performance not just on risk metrics such as maximum drawdown, but also on risk-adjusted returns metrics including Sharpe ratio and Calmar ratio. | Di Luo, Weiheng Liao, Rui Yan | Made by Data, United Kingdom and Renmin University of China, China; Renmin University of China, China |
| 363 |  |  [Creation and Analysis of a Corpus of Scam Emails Targeting Universities](https://doi.org/10.1145/3543873.3587303) |  | 0 | Email-based scams pose a threat to the personally identifiable information and financial safety of all email users. Within a university environment, the risks are potentially greater: traditional students (i.e., within an age range typical of college students) often lack the experience and knowledge of older email users. By understanding the topics, temporal trends, and other patterns of scam emails targeting universities, these institutions can be better equipped to reduce this threat by improving their filtering methods and educating their users. While anecdotal evidence suggests common topics and trends in these scams, the empirical evidence is limited. Observing that large universities are uniquely positioned to gather and share information about email scams, we built a corpus of 5,155 English language scam emails scraped from information security websites of five large universities in the United States. We use Latent Dirichlet Allocation (LDA) topic modelling to assess the landscape and trends of scam emails sent to university addresses. We examine themes chronologically and observe that topics vary over time, indicating changes in scammer strategies. For example, scams targeting students with disabilities have steadily risen in popularity since they first appeared in 2015, while password scams experienced a boom in 2016 but have lessened in recent years. To encourage further research to mitigate the threat of email scams, we release this corpus for others to study. | Grace Ciambrone, Shomir Wilson | Human Language Technologies Lab, Pennsylvania State University, USA |
| 364 |  |  [STRUM: Extractive Aspect-Based Contrastive Summarization](https://doi.org/10.1145/3543873.3587304) |  | 0 | Comparative decisions, such as picking between two cars or deciding between two hiking trails, require the users to visit multiple webpages and contrast the choices along relevant aspects. Given the impressive capabilities of pre-trained large language models [4, 11], we ask whether they can help automate such analysis. We refer to this task as extractive aspect-based contrastive summarization which involves constructing a structured summary that compares the choices along relevant aspects. In this paper, we propose a novel method called STRUM for this task that can generalize across domains without requiring any human-written summaries or fixed aspect list as supervision. Given a set of relevant input webpages, STRUM solves this problem using two pre-trained T5-based [11] large language models: first one fine-tuned for aspect and value extraction [14], and second one fine-tuned for natural language inference [13]. We showcase the abilities of our method across different domains, identify shortcomings, and discuss questions that we believe will be critical in this new line of research. | Beliz Gunel, Sandeep Tata, Marc Najork | Google, USA |
| 365 |  |  [gDoc: Automatic Generation of Structured API Documentation](https://doi.org/10.1145/3543873.3587310) |  | 0 | Generating and maintaining API documentation with integrity and consistency can be time-consuming and expensive for evolving APIs. To solve this problem, several approaches have been proposed to automatically generate high-quality API documentation based on a combination of knowledge from different web sources. However, current researches are weak in handling unpopular APIs and cannot generate structured API documentation. Hence, in this poster, we propose a hybrid technique(namely \textit{gDoc}) for the automatic generation of structured API documentation. We first present a fine-grained search-based strategy to generate the description for partial API parameters via computing the relevance between various APIs, ensuring the consistency of API documentation. Then, we employ the cross-modal pretraining Seq2Seq model M6 to generate a structured API document for each API, which treats the document generation problem as a translation problem. Finally, we propose a heuristic algorithm to extract practical parameter examples from API request logs. The experiments evaluated on the online system show that this work's approach significantly improves the effectiveness and efficiency of API document generation. | Shujun Wang, Yongqiang Tian, Dengcheng He | Alibaba Group, China |
| 366 |  |  [Reduce API Debugging Overhead via Knowledge Prepositioning](https://doi.org/10.1145/3543873.3587311) |  | 0 | OpenAPI indicates a behavior where producers offer Application Programming Interfaces (APIs) to help end-users access their data, resources, and services. Generally, API has many parameters that need to be entered. However, it is challenging for users to understand and document these parameters correctly. This paper develops an API workbench to help users learn and debug APIs. Based on this workbench, much exploratory work has been proposed to reduce the overhead of learning and debugging APIs. We explore the knowledge, such as parameter characteristics (e.g., enumerability) and constraints (e.g., maximum/minimum value), from the massive API call logs to narrow the range of parameter values. Then, we propose a fine-grained approach to enrich the API documentation by extracting dependency knowledge between APIs. Finally, we present a learning-based prediction method to predict API execution results before the API is called, significantly reducing user debugging cycles. The experiments evaluated on the online system show that this work's approach substantially improves the user experience of debugging OpenAPIs. | Shujun Wang, Yongqiang Tian, Dengcheng He | Alibaba Group, China |
| 367 |  |  [Augmenting Visualizations with Predictive and Investigative Insights to Facilitate Decision Making](https://doi.org/10.1145/3543873.3587317) |  | 0 | Many people find it difficult to comprehend basic charts on the web, let alone make effective decisions from them. To address this gap, several ML models aim to automatically detect useful insights from charts and narrate them in a simpler textual format. However, most of these solutions can only detect basic factual insights (a.k.a. descriptive insights) that are already present in the chart, which may help with chart comprehension, but not decision-making. In this work, we study whether more advanced predictive and investigative insights can help users understand what will happen next and what actions they should take. These advanced insights can help decision-makers better understand the reasons behind anomaly events, predict future unfolding trends, and recommend possible actions for optimizing business outcomes. Through a study with 18 participants, we found that predictive and investigative insights lead to more insights recorded by users on average and better effectiveness ratings. | Md Main Uddin Rony, Fan Du, Ryan A. Rossi, Jane Hoffswell, Niyati Chhaya, Iftikhar Ahamath Burhanuddin, Eunyee Koh | University of Maryland, USA; Adobe Research, India; Adobe Research, USA |
| 368 |  |  [OntoEval: an Automated Ontology Evaluation System](https://doi.org/10.1145/3543873.3587318) |  | 0 | Developing semantically-aware web services requires comprehensive and accurate ontologies. Evaluating an existing ontology or adapting it is a labor-intensive and complex task for which no automated tools exist. Nevertheless, in this paper we propose a tool that aims at making this vision come true, i.e., we present a tool for the automated evaluation of ontologies that allows one to rapidly assess an ontology’s coverage of a domain and identify specific problems in the ontology’s structure. The tool evaluates the domain coverage and correctness of parent-child relations of a given ontology based on domain information derived from a text corpus representing the domain. The tool provides both overall statistics and detailed analysis of sub-graphs of the ontology. In the demo, we show how these features can be used for the iterative improvement of an ontology. | Antonio Zaitoun, Tomer Sagi, Katja Hose | Aalborg University, Denmark; University of Haifa, Israel; Aalborg University, Denmark and TU Wien, Austria |
| 369 |  |  [Public Spot Instance Dataset Archive Service](https://doi.org/10.1145/3543873.3587314) |  | 0 | Spot instances offered by major cloud vendors allow users to use cloud instances cost-effectively but with the risk of sudden instance interruption. To enable efficient use of spot instances by users, cloud vendors provide various datasets that reflect the current status of spot instance services, such as savings ratio, interrupt ratio, and instant availability. However, this information is scattered, and they require distinct access mechanisms and pose query constraints. Hence, ordinary users find it difficult to use the dataset to optimize spot instance usage. To resolve this issue, we propose a multi-cloud spot instance dataset service that is publicly available. This will help cloud users and system researchers to use spot instances from multiple cloud vendors to build a cost-efficient and reliable environment expediting cloud system research. | Kyunghwan Kim, Subin Park, Jaeil Hwang, Hyeonyoung Lee, Seokhyeon Kang, Kyungyong Lee | Distributed Data Processing System Lab, CS Department, Kookmin University, Republic of Korea |
| 370 |  |  [Chain of Explanation: New Prompting Method to Generate Quality Natural Language Explanation for Implicit Hate Speech](https://doi.org/10.1145/3543873.3587320) |  | 0 | Recent studies have exploited advanced generative language models to generate Natural Language Explanations (NLE) for why a certain text could be hateful. We propose the Chain of Explanation (CoE) Prompting method, using the heuristic words and target group, to generate high-quality NLE for implicit hate speech. We improved the BLUE score from 44.0 to 62.3 for NLE generation by providing accurate target information. We then evaluate the quality of generated NLE using various automatic metrics and human annotations of informativeness and clarity scores. | Fan Huang, Haewoon Kwak, Jisun An | Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA |
| 371 |  |  [Towards Deeper Graph Neural Networks via Layer-Adaptive](https://doi.org/10.1145/3543873.3587323) |  | 0 | Graph neural networks have achieved state-of-the-art performance on graph-related tasks. Previous methods observed that GNNs’ performance degrades as the number of layers increases and attributed this phenomenon to over-smoothing caused by the stacked propagation. However, we proved experimentally and theoretically that it is overfitting rather than propagation that causes performance degradation. We propose a novel framework: layer-adaptive GNN (LAGNN) consisting of two modules: adaptive layer selection and random Droplayer, which can adaptively determine the number of layers and thus alleviate overfitting. We attached this general framework to two representative GNNs and achieved consistency improvements on six representative datasets. | Bingbing Xu, Bin Xie, Huawei Shen | Tiangong University, China; Institute of Computing Technology, Chinese Academy of Sciences, China |
| 372 |  |  [qEndpoint: A Wikidata SPARQL endpoint on commodity hardware](https://doi.org/10.1145/3543873.3587327) |  | 0 | In this work, we demonstrate how to setup a Wikidata SPARQL endpoint on commodity hardware resources. We achieve this by using a novel triple store called qEndpoint, which uses a read-only partition based on HDT and a write partition based on RDF4J. We show that qEndpoint can index and query the entire Wikidata dump (currently 17 billion triples) on a machine with 600GB SSD, 10 cores and 10GB of RAM, while keeping the query performance comparable with other SPARQL endpoints indexing Wikidata. In a nutshell, we present the first SPARQL endpoint over Wikidata that can run on commodity hardware while preserving the query run time of existing implementations. Our work goes in the direction of democratizing the access to Wikidata as well as to other large-scale Knowledge Graphs published on the Web. The source code of qEndpoint along with the query workloads are publicly available. | Antoine Willerval, Angela Bonifati, Dennis Diefenbach | LIRIS, University of Lyon 1, France; LIRIS, University of Lyon 1, France and The QA Company, France; The QA Company, France |
| 373 |  |  [Metadatamatic: A Web application to Create a Dataset Description](https://doi.org/10.1145/3543873.3587328) |  | 0 | This article introduces Metadatamatic, an open-source, online, user-friendly tool for generating the description of a knowledge base. It supports the description of any RDF dataset via a user-friendly web form that does not require prior knowledge of the vocabularies begin used, and can enrich the description with automatically generated statistics if the dataset is accessible from a public SPARQL endpoint. We discuss the models and methods behind the tool, and present some initial results suggesting that Metadatamatic can help in increasing the visibility of public knowledge bases. | Pierre Maillot, Olivier Corby, Catherine Faron, Fabien Gandon, Franck Michel | I3S, Univ. Cote d'Azur, Inria, CNRS, France; I3S, Univ. Cote d'Azur, CNRS, Inria, France |
| 374 |  |  [What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis](https://doi.org/10.1145/3543873.3587324) |  | 0 | Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation. | Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky | Google Research, USA; The Ohio State University, USA |
| 375 |  |  [GAT-DNS: DNS Multivariate Time Series Prediction Model Based on Graph Attention Network](https://doi.org/10.1145/3543873.3587329) |  | 0 | As one of the most basic services of the Internet, DNS has suffered a lot of attacks. Existing attack detection methods rely on the learning of malicious samples, so it is difficult to detect new attacks and long-period attacks. This paper transforms the DNS data flow into time series, and proposes a DNS anomaly detection method based on graph attention network and graph embedding (GAT-DNS). GAT-DNS establishes a multivariate time series model to depict the DNS service status. When the actual flow of a feature exceeds the predicted range, it is considered that abnormal DNS behavior is found. In this paper, vertex dependency is proposed to describe the dependency between features. The features with high vertex dependency values are deleted to achieve model compression. This improves the system efficiency. Experiments on open data sets show that compared with the latest AD-Bop and QLAD methods, GAT-DNS method not only improves the precision, recall and F1 value, but also improves the time efficiency of the model. | Xiaofeng Lu, Xiaoyu Zhang, Pietro Lió | University of Cambridge, United Kingdom; Beijing University of Posts and Telecommunications, China |
| 376 |  |  [Weedle: Composable Dashboard for Data-Centric NLP in Computational Notebooks](https://doi.org/10.1145/3543873.3587330) |  | 0 | Data-centric NLP is a highly iterative process requiring careful exploration of text data throughout entire model development lifecycle. Unfortunately, existing data exploration tools are not suitable to support data-centric NLP because of workflow discontinuity and lack of support for unstructured text. In response, we propose Weedle, a seamless and customizable exploratory text analysis system for data-centric NLP. Weedle is equipped with built-in text transformation operations and a suite of visual analysis features. With its widget, users can compose customizable dashboards interactively and programmatically in computational notebooks. | Nahyun Kwon, Hannah Kim, Sajjadur Rahman, Dan Zhang, Estevam Hruschka | Texas A&M University, USA; Megagon Labs, USA |
| 377 |  |  [The Web Data Commons Schema.org Data Set Series](https://doi.org/10.1145/3543873.3587331) |  | 0 | Millions of websites have started to annotate structured data within their HTML pages using the schema.org vocabulary. Popular entity types annotated with schema.org terms are products, local businesses, events, and job postings. The Web Data Commons project has been extracting schema.org data from the Common Crawl every year since 2013 and offers the extracted data for public download in the form of the schema.org data set series. The latest release in the series consists of 106 billion RDF quads describing 3.1 billion entities. The entity descriptions originate from 12.8 million different websites. From a Web Science perspective, the data set series lays the foundation for analyzing the adoption process of schema.org annotations on the Web over the past decade. From a machine learning perspective, the annotations provide a large pool of training data for tasks such as product matching, product or job categorization, information extraction, or question answering. This poster gives an overview of the content of the Web Data Commons schema.org data set series. It highlights trends in the adoption of schema.org annotations on the Web and discusses how the annotations are being used as training data for machine learning applications. | Alexander Brinkmann, Anna Primpeli, Christian Bizer | University of Mannheim, Germany |
| 378 |  |  [Class Cardinality Comparison as a Fermi Problem](https://doi.org/10.1145/3543873.3587334) |  | 0 | Questions on class cardinality comparisons are quite tricky to answer and come with its own challenges. They require some kind of reasoning since web documents and knowledge bases, indispensable sources of information, rarely store direct answers to questions, such as, \`\`Are there more astronauts or Physics Nobel Laureates?'' We tackle questions on class cardinality comparison by tapping into three sources for absolute cardinalities as well as the cardinalities of orthogonal subgroups of the classes. We propose novel techniques for aggregating signals with partial coverage for more reliable estimates and evaluate them on a dataset of 4005 class pairs, achieving an accuracy of 83.7%. | Shrestha Ghosh, Simon Razniewski, Gerhard Weikum | Max Planck Institute for Informatics, Germany; Max Planck Institute for Informatics, Germany and Saarland University, Germany |
| 379 |  |  [Towards a Critical Open-Source Software Database](https://doi.org/10.1145/3543873.3587336) |  | 0 | Open-source software (OSS) plays a vital role in the modern software ecosystem. However, the maintenance and sustainability of OSS projects can be challenging. In this paper, we present the CrOSSD project, which aims to build a database of OSS projects and measure their current project "health" status. In the project, we will use both quantitative and qualitative metrics to evaluate the health of OSS projects. The quantitative metrics will be gathered through automated crawling of meta information such as the number of contributors, commits and lines of code. Qualitative metrics will be gathered for selected "critical" projects through manual analysis and automated tools, including aspects such as sustainability, funding, community engagement and adherence to security policies. The results of the analysis will be presented on a user-friendly web platform, which will allow users to view the health of individual OSS projects as well as the overall health of the OSS ecosystem. With this approach, the CrOSSD project provides a comprehensive and up-to-date view of the health of OSS projects, making it easier for developers, maintainers and other stakeholders to understand the health of OSS projects and make informed decisions about their use and maintenance. | Tobias Dam, Lukas Daniel Klausner, Sebastian Neumaier | St. Pölten University of Applied Sciences, Austria |
| 380 |  |  [Task-Specific Data Augmentation for Zero-shot and Few-shot Stance Detection](https://doi.org/10.1145/3543873.3587337) |  | 0 | Various targets keep coming up on social media, and most of them lack labeled data. In this paper, we focus on zero-shot and few-shot stance detection, which aims to identify stances with few or even no training instances. In order to solve the lack of labeled data and implicit stance expression, we propose a self-supervised data augment approach based on coreference resolution. The method is specific for stance detection to generate more stable data and reduce the variance within and between classes to achieve a balance between validity and robustness. Considering the diversity of comments, we propose a novel multi-task stance detection framework of target-related fragment extraction and stance detection, which can enhance attention on target-related fragments and reduce the noise of other fragments. Experiments show that the proposed approach achieves state-of-the-art performance in zero-shot and few-shot stance detection. | Jiarui Zhang, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng | College of Intelligence and Computing, Tianjin University, China |
| 381 |  |  [Measuring Potential Performance Gains of Python Web Applications with pyUpgradeSim](https://doi.org/10.1145/3543873.3587338) |  | 0 | Python is a popular programming language for web development. However, optimizing the performance of Python web applications is a challenging task for developers. This paper presents a new approach to measuring the potential performance gains of upgraded Python web applications. Our approach is based on the provision of an interactive service that assists developers in optimizing their Python code through changes to the underlying system. The service uses profiling and visualization techniques to identify performance bottlenecks. We demonstrate and evaluate the effectiveness of our approach through a series of experiments on real-world Python web applications, measuring performance differences in between versions and the benefits of migrating at a reduced cost. The results show promising improvement in performance without any required code changes. | Anthony Shaw, Amin Beheshti | School of Computing, Macquarie University, Australia |
| 382 |  |  [mStore: Schema Mining based-RDF Data Storage](https://doi.org/10.1145/3543873.3587339) |  | 0 | The relationship-based approach is an efficient solution strategy for distributed RDF data management. The schema of tables can directly affect the system’s storage efficiency and query performance. Most current approaches are based on fixed schema(e.g., VP, ExtVP). When facing large-scale RDF datasets and complex SPARQL queries requiring many joins, such methods suffer from problems such as long pre-processing time and poor query performance. Schemas with Pareto Optimality between the system’s space consumption and query efficiency are needed but also hard to find. Therefore, we propose mStore, a prototype system with flexible schemas based on schema mining. The intuition behind our approach is that we want to divide the combinations of predicates with higher relevance into the same schema, which can replace costly joins with low-cost selects, improving the query performance. The results show that our system performs better on complex query workloads while reducing the pre-processing time overhead compared to systems with fixed schema partitioning strategies. | Guopeng Zheng, Tenglong Ren, Lulu Yang, Xiaowang Zhang, Zhiyong Feng | College of Intelligence and Computing, Tianjin University, China |
| 383 |  |  [Identifying Topic and Cause for Sarcasm: An Unsupervised Knowledge-enhanced Prompt Method](https://doi.org/10.1145/3543873.3587343) |  | 0 | Sarcasm is usually emotional and topical. Mining the characteristics of sarcasm semantics in different emotional tendencies and topic expressions helps gain insight into the sarcasm cause. Most of the existing work detect sarcasm or topic label based on a supervised learning framework, which requires heavy data annotation work. To overcome the above challenges, inspired by the multi-task learning framework, this paper proposes an unsupervised knowledge-enhanced prompt method. This method uses the similarity interaction mechanism to mine the hidden relationship between the sarcasm cause and topic, which integrates external knowledge, such as syntax and emotion, into the prompting and generation process. Additionally, it identifies the sarcasm cause and topic simultaneously. Experimental results on a real-world dataset verify the effectiveness of the proposed model. | Minjie Yuan, Qiudan Li, Xue Mao, Daniel Dajun Zeng | State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Science, China; State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Science, China and School of Artificial Intelligence, University of Chinese Academy of Sciences, China |
| 384 |  |  [The Community Notes Observatory: Can Crowdsourced Fact-Checking be Trusted in Practice?](https://doi.org/10.1145/3543873.3587340) |  | 0 | Fact-checking is an important tool in fighting online misinformation. However, it requires expert human resources, and thus does not scale well on social media because of the flow of new content. Crowdsourcing has been proposed to tackle this challenge, as it can scale with a smaller cost, but it has always been studied in controlled environments. In this demo, we present the Community Notes Observatory, an online system to evaluate the first large-scale effort of crowdsourced fact-checking deployed in practice. We let demo attendees search and analyze tweets that are fact-checked by Community Notes users and compare the crowd’s activity against professional fact-checkers. The attendees will explore evidence of i) differences in how the crowd and experts select content to be checked, ii) how the crowd and the experts retrieve different resources to fact-check, and iii) the edge the crowd shows in fact-checking scalability and efficiency as compared to expert checkers. | Luca Righes, Mohammed Saeed, Gianluca Demartini, Paolo Papotti | The University of Queensland, Australia; EURECOM, France |
| 385 |  |  [EasySpider: A No-Code Visual System for Crawling the Web](https://doi.org/10.1145/3543873.3587345) |  | 0 | The web is a treasure trove for data that is increasingly used by computer scientists for building large machine learning models as well as non-computer scientists for social studies or marketing analyses. As such, web-crawling is an essential tool for both computational and non-computational scientists to conduct research. However, most of the existing web crawler frameworks and software products either require professional coding skills without an easy-to-use graphic user interface or are expensive and limited in features. They are thus not friendly to newbies and inconvenient for complicated web-crawling tasks. In this paper, we present an easy-to-use visual web crawler system, EasySpider, for designing and executing web crawling tasks without coding. The workflow of a new web crawling task can be visually programmed by following EasySpider’s visual wizard on the target webpages using an intuitive point-and-click interface. The generated crawler task can then be easily invoked locally or as a web service. Our EasySpider is cross-platform and flexible to adapt to different web-resources. It also supports advanced configuration for complicated tasks and extension. The whole system is open-sourced and transparent for free-access at GitHub 1, which avoids possible privacy leakage. | Naibo Wang, Wenjie Feng, Jianwei Yin, SeeKiong Ng | Zhejiang University, China; National University of Singapore, Singapore |
| 386 |  |  [Persona Consistent Dialogue Generation via Contrastive Learning](https://doi.org/10.1145/3543873.3587346) |  | 0 | The inclusion of explicit personas in generation models has gained significant attention as a means of developing intelligent dialogue agents. However, large pretrained generation models often produce inconsistent responses with persona. We investigate the model generation behavior to identify signs of inconsistency and observe inconsistent behavior patterns. In this work, we introduce contrastive learning into persona consistent dialogue generation, building on the idea that humans learn not just from positive feedback, but also from identifying and correcting undesirable behaviors. According to the inconsistent patterns, we design two strategies to construct high-quality negative samples, which are critical for contrastive learning efficacy. Experimental results demonstrate that our method can effectively improve the consistency of the responses while improving its dialogue quality on both automatic and human evaluation. | Zhenfeng Han, Sai Zhang, Xiaowang Zhang | College of Intelligence and Computing, Tianjin University, China |
| 387 |  |  [TML: A Temporal-aware Multitask Learning Framework for Time-sensitive Question Answering](https://doi.org/10.1145/3543873.3587347) |  | 0 | Many facts change over time, Time-sensitive Question Answering(TSQA) answers questions about time evolution facts to test the model’s ability in the dimension of the time. The existing methods obtain the representations of questions and documents and then compute their similarity to find the answer spans. These methods perform well in simple moment questions, but they are difficult to solve hard duration problems that need temporal relations and temporal numeric comparisons. In this paper, we propose Temporal-aware Multitask Learning (TML) with three auxiliary tasks to tackle with them. First, we propose a temporal-aware sequence labeling task to help the model distinguish the temporal expressions by detecting temporal types of tokens in the document. Then a temporal-aware masked language modeling task is used to capture the temporal relation between events based on the context. Furthermore, temporal-aware order learning is proposed to inject the ability of numeric comparison into the model. We carried out comprehensive experiments on the TimeQA benchmark, aiming to evaluate the performance of our proposed methodology in handling temporal question answering. TML significantly outperforms the baselines by a relative 10% on the two splits of the dataset. | Ziqiang Chen, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng | College of Intelligence and Computing, Tianjin University, China |
| 388 |  |  [Addressing socially destructive disinformation on the web with advanced AI tools: Russia as a case study](https://doi.org/10.1145/3543873.3587348) |  | 0 | Today, disinformation (i.e. deliberate misinformation) is omnipresent in all web communication channels. There is a developing explosion in this socially disruptive mode of web-based information. Increasingly, we have seen various countries developing advanced methods to spread their targeted disinformation. To address this flood of disinformation will require refined strategies to capture and evaluate the messages. In this paper, we present both a quantitative and a qualitative analysis of online social and information networks to better evaluate the characteristics of the disinformation campaigns. We focus on the case of Russian-generated disinformation, which has been developed to an elevated level. We demonstrate an effective approach based on a new dataset to study the Russian campaign composed of 14497 cases of dis-information and the corresponding counter-dis-information. Although this case is of high current relevance, there is very limited published evaluation. We provide a novel analysis and present a methodology to characterize this disinformation. We based our investigation on a Spherical k-means algorithm to determine the main topics of the disinformation and to discover the key trends. We employ distilBERT algorithm and achieve a high accuracy F1-score of 98.8 demonstrating good quantitative capabilities. We propose the methodology as a template for further exploration and analysis. | Florian Barbaro, Andy Skumanich | Innov8ai, USA |
| 389 |  |  [Katti: An Extensive and Scalable Tool for Website Analyses](https://doi.org/10.1145/3543873.3587351) |  | 0 | Research on web security and privacy frequently relies on tools that analyze a set of websites. One major obstacle to the judicious analysis is the employment of a rock-solid and feature-rich web crawler. For example, the automated analysis of ad-malware campaigns on websites requests crawling a vast set of domains on multiple real web browsers, while simultaneously mitigating bot detections and applying user interactions on websites. Further, the ability to attach various threat analysis frameworks lacks current tooling efforts in web crawling and analyses. In this paper we introduce Katti, which overcomes several of today’s technical hurdles in web crawling. Our tool employs a distributed task queue that efficiently and reliably handles both large crawling and threat analyses requests. Katti extensively collects all available web data through an integrated person-in-the-middle proxy. Moreover, Katti is not limited to a specific use case, allowing users to easily customize our tool to their individual research intends. | Florian Nettersheim, Stephan Arlt, Michael Rademacher, Florian Dehling | Federal Office for Information Security, Germany; Fraunhofer FKIE, Germany; University of Applied Sciences Bonn-Rhein-Sieg, Germany |
| 390 |  |  [Graph Induced Transformer Network for Detection of Politeness and Formality in Text](https://doi.org/10.1145/3543873.3587352) |  | 0 | Formality and politeness are two of the most commonly studied stylistic dimensions of language that convey, authority, amount of shared context, and social distances among the communicators and are known to affect user behavior significantly. Formality in the text refers to the type of language used in situations when the speaker is very careful about the choice of words and sentence structure. In this paper, we propose a graph-induced transformer network (GiTN) to detect formality and politeness in text automatically. The proposed model exploits the latent linguistic features present in the text to identify the aforementioned stylistic factors. The proposed model is evaluated with multiple datasets across domains. We found that the proposed model’s performance surpasses most baseline systems. | Tirthankar Dasgupta, Manjira Sinha, Chundru Geetha Praveen | IIT Kharagpur, India; Indian Institute of Technology Kharagpur, India; Tata Consultancy Services, India |
| 391 |  |  [Visualizing How-Provenance Explanations for SPARQL Queries](https://doi.org/10.1145/3543873.3587350) |  | 0 | Knowledge graphs (KGs) are vast collections of machine-readable information, usually modeled in RDF and queried with SPARQL. KGs have opened the door to a plethora of applications such as Web search or smart assistants that query and process the knowledge contained in those KGs. An important, but often disregarded, aspect of querying KGs is query provenance: explanations of the data sources and transformations that made a query result possible. In this article we demonstrate, through a Web application, the capabilities of SPARQLprov, an engine-agnostic method that annotates query results with how-provenance annotations. To this end, SPARQLprov resorts to query rewriting techniques, which make it applicable to already deployed SPARQL endpoints. We describe the principles behind SPARQLprov and discuss perspectives on visualizing how-provenance explanations for SPARQL queries. | Luis Galárraga, Daniel Hernández, Anas Katim, Katja Hose | INSA Rouen, France; University of Stuttgart, Germany; IRISA, Inria, France; Aalborg University, Denmark and TU Wien, Austria |
| 392 |  |  [Counterfactual Reasoning for Decision Model Fairness Assessment](https://doi.org/10.1145/3543873.3587354) |  | 0 | The increasing application of Artificial Intelligence and Machine Learning models poses potential risks of unfair behaviour and, in the light of recent regulations, has attracted the attention of the research community. Several researchers focused on seeking new fairness definitions or developing approaches to identify biased predictions. These approaches focus solely on a discrete and limited space; only a few analyze the minimum variations required in the user characteristics to ensure a positive outcome for the individuals (counterfactuals). In that direction, the methodology proposed in this paper aims to unveil unfair model behaviors using counterfactual reasoning in the case of fairness under unawareness. The method also proposes two new metrics that analyse the (estimated) sensitive information of counterfactual samples with the help of an external oracle. Experimental results on three data sets show the effectiveness of our approach for disclosing unfair behaviour of state-of-the-art Machine Learning and debiasing models. Source code is available at https://github.com/giandos200/WWW-23-Counterfactual-Fair-Opportunity-Poster-. | Giandomenico Cornacchia, Vito Walter Anelli, Fedelucio Narducci, Azzurra Ragone, Eugenio Di Sciascio | University of Bari, Italy; Politecnico di Bari, Italy |
| 393 |  |  [Wikidata Atlas: Putting Wikidata on the Map](https://doi.org/10.1145/3543873.3587356) |  | 0 | Wikidata Atlas is an online system that allows users to explore Wikidata items on an interactive global map; for example, users can explore the global distribution of all lighthouses described by Wikidata. Designing such a system poses challenges in terms of scalability, where some classes have hundreds of thousands of instances; efficiency, where visualisations are generated live; freshness, where we want changes on Wikidata to be reflected as they happen in the system; and usability, where we aim for the system to be accessible for a broad audience. Herein we describe the design and implementation of the system in light of these challenges. | Benjamín Del Pino, Aidan Hogan | DCC, Universidad de Chile, Chile; DCC, Universidad de Chile, Chile and Instituto Milenio Fundamentos de los Datos (IMFD), Chile |
| 394 |  |  [How Algorithm Awareness Impacts Algospeak Use on TikTok](https://doi.org/10.1145/3543873.3587355) |  | 0 | Algospeak refers to social media users intentionally altering or substituting words when creating or sharing online content, for example, using ‘le$bean’ for ‘lesbian’. This study discusses the characteristics of algospeak as a computer-mediated language phenomenon on TikTok with regards to users’ algorithmic literacy and their awareness of how the platform’s algorithms work. We then present results from an interview study with TikTok creators on their motivations to utilize algospeak. Our results indicate that algospeak is used to oppose TikTok’s algorithmic moderation system in order to prevent unjust content violations and shadowbanning when posting about benign yet seemingly unwanted subjects on TikTok. In this, we find that although algospeak helps to prevent consequences, it often impedes the creation of quality content. We provide an adapted definition of algospeak and new insights into user-platform interactions in the context of algorithmic systems and algorithm awareness. | Daniel Klug, Ella Steen, Kathryn Yurechko | Software and Societal Systems Department, Carnegie Mellon University, USA; Gordon College, USA; Washington and Lee University, USA |
| 395 |  |  [Computing and Visualizing Agro-Meteorological Parameters based on an Observational Weather Knowledge Graph](https://doi.org/10.1145/3543873.3587357) |  | 0 | Linked-data principles are more and more adopted to integrate and publish semantically described open data using W3C standards resulting in a large amount of available resources [7]. In particular, meteorological sensor data have been uplifted into public RDF graphs, such as WeKG-MF which offers access to a large set of meteorological variables described through spatial and temporal dimensions. Nevertheless, these resources include huge numbers of raw observations that are tedious to be explored and reused by lay users. In this paper, we leverage WeKG-MF to compute important agro-meteorological parameters and views with SPARQL queries. As a result, we deployed a LOD platform as a web application to allow users to navigate, consume and produce linked datasets of agro-meterological parameters calculated on-the-fly. | Nadia Yacoubi Ayadi, Catherine Faron, Franck Michel, Fabien Gandon, Olivier Corby | Université Côte d'Azur, CNRS, I3S (UMR 7271), France, France; Université Côte d'Azur, CNRS, I3S (UMR 7271),France, France |
| 396 |  |  [Injecting data into ODRL privacy policies dynamically with RDF mappings](https://doi.org/10.1145/3543873.3587358) |  | 0 | The privacy of the data provided by available sources is one of the major concerns of our era. In order to address this challenge, the W3C has promoted recommendations to allow expressing privacy policies. One of these recommendations is the Open Digital Rights Language (ODRL) vocabulary. Although this standard has wide adoption, it is not suitable in domains such as IoT, Ubiquitous and Mobile Computing, or discovery. The reason behind is the fact that ODRL privacy policies are not able to cope with dynamic information that may come from external sources of data and, therefore, these policies can not define privacy restrictions upon data that is not already written in the policy beforehand. In this demo paper, a solution to this challenge is presented. It is shown how ODRL policies can overcome the aforementioned limitation by being combined with a mapping language for RDF materialisation. The article shows how ODRL policies are able to consider data coming from an external data source when they are solved, in particular, a weather forecast API that provides temperature values. The demonstration defines an ODRL policy that grants access to a resource only when the temperature of the API is above a certain value. | Juan CanoBenito, Andrea Cimmino, Raúl GarcíaCastro | Universidad Politécnica de Madrid, Spain |
| 397 |  |  [MediSage: An AI Assistant for Healthcare via Composition of Neural-Symbolic Reasoning Operators](https://doi.org/10.1145/3543873.3587361) |  | 0 | We introduce MediSage, an AI decision support assistant for medical professionals and caregivers that simplifies the way in which they interact with different modalities of electronic health records (EHRs) through a conversational interface. It provides step-by-step reasoning support to an end-user to summarize patient health, predict patient outcomes and provide comprehensive and personalized healthcare recommendations. MediSage provides these reasoning capabilities by using a knowledge graph that combines general purpose clinical knowledge resources with recent-most information from the EHR data. By combining the structured representation of knowledge with the predictive power of neural models trained over both EHR and knowledge graph data, MediSage brings explainability by construction and represents a stepping stone into the future through further integration with biomedical language models. | Sutanay Choudhury, Khushbu Agarwal, Colby Ham, Suzanne Tamang | Stanford University, USA; Pacific Northwest National Laboratory, USA |
| 398 |  |  [Depicting Vocabulary Summaries with Devos](https://doi.org/10.1145/3543873.3587359) |  | 0 | Communicating ontologies to potential users is still a difficult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams together with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on different strategies for summarizing the ontology. | Ahmad Alobaid, Jhon Toledo, Óscar Corcho, María PovedaVillalón | Universidad Politécnica de Madrid, Spain |
| 399 |  |  [Child Sexual Abuse Awareness and Support Seeking on Reddit: A thematic Analysis](https://doi.org/10.1145/3543873.3587363) |  | 0 | Child sexual abuse (CSA) is a pervasive issue in both online and physical contexts. Social media has grown in popularity as a platform for offering awareness, support, and community for those seeking help or advice regarding CSA. One popular social media platform in which such communities has formed is Reddit. In this study, we use both LDA and a reflexive thematic analysis to understand the types of engagements users have with subreddits aimed at CSA awareness. Through the reflexive thematic analysis, we identified six themes including strong negative emotions and phrasing, seeking help, personal experiences and their impact, measurement strategies to prevent abuse, provisioning of support, and the problematic nature of the Omegle platform. This research has implications for those creating awareness materials around CSA safety as well as for child advocacy groups. | Siva Sahitya Simhadri, Tatiana Ringenberg | Purdue University, USA; Computer and Information Technology, Purdue University, USA |
| 400 |  |  [Violentometer: measuring violence on the Web in real time](https://doi.org/10.1145/3543873.3587364) |  | 0 | This paper describes a system for monitoring in real time the level of violence on Web platforms through the use of an artificial intelligence model to classify textual data according to their content. The system was successfully implemented and tested during the electoral campaign period of the Brazilian 2022 elections by using it to monitor the attacks directed to thousands of candidates on Twitter. We show that, despite an accurate and absolute quantification of violence is not feasible, the system yields differential measures of violence levels that can be useful for understanding human behavior online. | Henrique S. Xavier | Ceweb, NIC.br, Brazil |
| 401 |  |  [Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://doi.org/10.1145/3543873.3587368) |  | 0 | Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research. | Fan Huang, Haewoon Kwak, Jisun An | Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, USA |
| 402 |  |  [RealGraph+: A High-Performance Single-Machine-Based Graph Engine that Utilizes IO Bandwidth Effectively](https://doi.org/10.1145/3543873.3587365) |  | 0 | This paper proposes RealGraph+, an improved version of RealGraph that processes large-scale real-world graphs efficiently in a single machine. Via a preliminary analysis, we observe that the original RealGraph does not fully utilize the IO bandwidth provided by NVMe SSDs, a state-of-the-art storage device. In order to increase the IO bandwidth, we equip RealGraph+ with three optimization strategies to issue more-frequent IO requests: (1) User-space IO, (2) Asynchronous IO, and (3) SIMD processing. Via extensive experiments with four graph algorithms and six real-world datasets, we show that (1) each of our strategies is effective in increasing the IO bandwidth, thereby reducing the execution time; (2) RealGraph+ with all of our strategies improves the original RealGraph significantly; (3) RealGraph+ outperforms state-of-the-art single-machine-based graph engines dramatically; (4) it shows performance comparable to or even better than those of other distributed-system-based graph engines. | MyungHwan Jang, JeongMin Park, Ikhyeon Jo, DuckHo Bae, SangWook Kim | Department of Computer Science, Hanyang University, Republic of Korea; Samsung Electronics, Republic of Korea |
| 403 |  |  [Hierarchical Deep Neural Network Inference for Device-Edge-Cloud Systems](https://doi.org/10.1145/3543873.3587370) |  | 0 | Edge computing and cloud computing have been utilized in many AI applications in various fields, such as computer vision, NLP, autonomous driving, and smart cities. To benefit from the advantages of both paradigms, we introduce HiDEC, a hierarchical deep neural network (DNN) inference framework with three novel features. First, HiDEC enables the training of a resource-adaptive DNN through the injection of multiple early exits. Second, HiDEC provides a latency-aware inference scheduler, which determines which input samples should exit locally on an edge device based on the exit scores, enabling inference on edge devices with insufficient resources to run the full model. Third, we introduce a dual thresholding approach allowing both easy and difficult samples to exit early. Our experiments on image and text classification benchmarks show that HiDEC significantly outperforms existing solutions. | Fatih Ilhan, Selim Furkan Tekin, Sihao Hu, Tiansheng Huang, Ka Ho Chow, Ling Liu | Georgia Institute of Technology, USA |
| 404 |  |  [A Detection System for Comfortable Locations Based on Facial Expression Analysis While Riding Bicycles](https://doi.org/10.1145/3543873.3587371) |  | 0 | In recent years, the use of bicycle as a healthy and economical means of transportation has been promoted worldwide. In addition, with the increase in bicycle commuting due to the COVID-19, the use of bicycles are attracting attention as a last-mile means of transportation in Mobility as a Service(MaaS). To help ensure a safe and comfortable ride using a smartphone mounted on a bicycle, this study focuses on analyzing facial expressions while riding to determine potential comfort along the route with the surrounding environment and to provide a map that users can explicitly feedback(FB) after riding. Combining the emotions of facial expressions while riding and FB, we annotate comfort to different locations. Afterwards, we verify the relationship between locations with high level of comfort based on the acquired data and the surrounding environment of those locations using Google Street View(GSV). | Ryuta Yamaguchi, Panote Siriaraya, Tomoki Yoshihisa, Shinji Shimojo, Yukiko Kawai | Kyoto Sangyo University, Japan; Osaka University, Japan; Kyoto Institute of Technology, Japan |
| 405 |  |  [Efficient Fair Graph Representation Learning Using a Multi-level Framework](https://doi.org/10.1145/3543873.3587369) |  | 0 | Graph representation learning models have demonstrated great capability in many real-world applications. Nevertheless, prior research reveals that these models can learn biased representations leading to unfair outcomes. A few works have been proposed to mitigate the bias in graph representations. However, most existing works require exceptional time and computing resources for training and fine-tuning. In this demonstration, we propose a framework FairMILE for efficient fair graph representation learning. FairMILE allows the user to efficiently learn fair graph representations while preserving utility. In addition, FairMILE can work in conjunction with any unsupervised embedding approach based on the user’s preference and accommodate various fairness constraints. The demonstration will introduce the methodology of FairMILE, showcase how to set up and run this framework, and demonstrate our effectiveness and efficiency to the audience through both quantitative metrics and visualization. | Yuntian He, Saket Gurukar, Srinivasan Parthasarathy | The Ohio State University, USA |
| 406 |  |  [Simple Multi-view Can Bring Powerful Graph Neural Network](https://doi.org/10.1145/3543873.3587375) |  | 0 | Graph neural networks have achieved state-of-the-art performance on graph-related tasks through layer-wise neighborhood aggregation. Previous works aim to achieve powerful capability via designing injective neighborhood aggregation functions in each layer, which is difficult to determine and numerous additional parameters make it difficult to train these models. It is the input space and the aggregation function that achieve powerful capability at the same time. Instead of designing complexity aggregation functions, we propose a simple and effective framework, namely MV-GNN, to improve the model expressive power via constructing the new input space. Precisely, MV-GNN samples multi-view subgraphs for each node, and any GNN model can be applied to these views. The representation of target node is finally obtained via aggregating all views injectively. Two typical GNNs (i.e., GCN and GAT) are adopted as base models in the proposed framework, and we demonstrate the effectiveness of MV-GNN through extensive experiments. | Bingbing Xu, Yang Li, Qi Cao, Huawei Shen | Institute of Computing Technology, Chinese Academy of Sciences, China |
| 407 |  |  [EDITS: An Easy-to-difficult Training Strategy for Cloud Failure Prediction](https://doi.org/10.1145/3543873.3584630) |  | 0 | Cloud failures have been a major threat to the reliability of cloud services. Many failure prediction approaches have been proposed to predict cloud failures before they actually occur, so that proactive actions can be taken to ensure service reliability. In industrial practice, existing failure prediction approaches mainly focus on utilizing state-of-the-art time series models to enhance the performance of failure prediction but neglect the training strategy. However, as curriculum learning points out, models perform better when they are trained with data in an order of easy-to-difficult. In this paper, we propose EDITS, a novel training strategy for cloud failure prediction, which greatly improves the performance of the existing cloud failure prediction models. Our experimental results on industrial and public datasets show that EDITS can obviously enhance the performance of cloud failure prediction model. In addition, EDITS also outperforms other curriculum learning methods. More encouragingly, our proposed EDITS has been successfully applied to Microsoft 365 and Azure online service systems, and has obviously reduced financial losses caused by cloud failures. | Qingwei Lin, Tianci Li, Pu Zhao, Yudong Liu, Minghua Ma, Lingling Zheng, Murali Chintalapati, Bo Liu, Paul Wang, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang | Microsoft Research, China; Microsoft 365, China; Peking University, Microsoft Research, China; Microsoft 365, USA; Microsoft Azure, USA; Chongqing University, The University of Newcastle, China |
| 408 |  |  [Multi-Agent Reinforcement Learning with Shared Policy for Cloud Quota Management Problem](https://doi.org/10.1145/3543873.3584634) |  | 0 | Quota is often used in resource allocation and management scenarios to prevent abuse of resource and increase the efficiency of resource utilization. Quota management is usually fulfilled with a set of rules maintained by the system administrator. However, maintaining these rules usually needs deep domain knowledge. Moreover, arbitrary rules usually cannot guarantee both high resource utilization and fairness at the same time. In this paper, we propose a reinforcement learning framework to automatically respond to quota requests in cloud computing platforms with distinctive usage characteristics for users. Extensive experimental results have demonstrated the superior performance of our framework on achieving a great trade-off between efficiency and fairness. | Tong Cheng, Hang Dong, Lu Wang, Bo Qiao, Si Qin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Thomas Moscibroda | Microsoft Research, China; Microsoft 365, USA; Microsoft Azure, USA |
| 409 |  |  [Job Type Extraction for Service Businesses](https://doi.org/10.1145/3543873.3584636) |  | 0 | Google My Business (GMB) is a platform that hosts business profiles, which will be displayed when a user issues a relevant query on Google Search or Google Maps. GMB businesses provide a wide variety of services, from home cleaning and repair, to legal consultation. However, the exact details of the service provided (a.k.a. job types), are often missing in business profiles. This places the burden of finding these details on the users. To alleviate this burden, we built a pipeline to automatically extract the job types from business websites. We share the various challenges we faced while developing this pipeline, and how we effectively addressed these challenges by (1) utilizing structured content to tackle the cold start problem for dataset collection; (2) exploiting context information to improve model performance without hurting scalability; and (3) formulating the extraction problem as a retrieval task to improve both generalizability, efficiency, and coverage. The pipeline has been deployed for over a year and is scalable enough to be periodically refreshed. The extracted job types are serving users of Google Search and Google Maps, with significant improvements in both precision and coverage. | Cheng Li, Yaping Qi, Hayk Zakaryan, Mingyang Zhang, Michael Bendersky, Yonghua Wu, Marc Najork | Google, USA |
| 410 |  |  [A Practical Rule Learning Framework for Risk Management](https://doi.org/10.1145/3543873.3584644) |  | 0 | Identifying the fraud risk of applications on the web platform is a critical challenge with both requirements of effectiveness and interpretability. In these high-stakes web applications especially in financial scenarios, decision rules have been extensively used due to the rising requirements for explainable artificial intelligence (XAI). In this work, we develop a rule learning framework with rule mining and rule refining modules for addressing the learning efficiency and class imbalance issues while making the decision rules more broadly and simply applicable to risk management scenarios. On four benchmark data-sets and two large-scale data-sets, the classification performance, interpretability, and scalability of the framework have been proved, achieving at least a 26.2% relative improvement over the state-of-the-art (SOTA) models. The system is currently being used by hundreds of millions of users and dealing with an enormous number of transactions in Ant Group, which is one of the largest mobile payment platforms in the world. | Jun Zhou, Meng Li, Lu Yu, Longfei Li, Fei Wu | Ant Group, China; College of Computer Science and Technology, Zhejiang University, China and Ant Group, China; College of Computer Science and Technology, Zhejiang University, China |
| 411 |  |  [WAM-studio, a Digital Audio Workstation (DAW) for the Web](https://doi.org/10.1145/3543873.3587987) |  | 0 | This paper presents WAM Studio, an open source, online Digital Audio Workstation (DAW) that takes advantages of several W3C Web APIs, such as Web Audio, Web Assembly, Web Components, Web Midi, Media Devices etc. It also uses the Web Audio Modules proposal that has been designed to facilitate the development of inter-operable audio plugins (effects, virtual instruments, virtual piano keyboards as controllers etc.) and host applications. DAWs are feature-rich software and therefore particularly complex to develop in terms of design, implementation, performances and ergonomics. Very few commercial online DAWs exist today and the only open-source examples lack features (no support for inter-operable plugins, for example) and do not take advantage of the recent possibilities offered by modern W3C APIs (e.g. AudioWorklets/Web Assembly). WAM Studio was developed as an open-source technology demonstrator with the aim of showcasing the potential of the web platform, made possible by these APIs. The paper highlights some of the difficulties we encountered (i.e limitations due to the sandboxed and constrained environments that are Web browsers, latency compensation etc.). An online demo, as well as a GitHub repository for the source code are available. | Michel Buffa, Antoine VidalMazuy | Université Côte d'Azur / I3S / INRIA, France; University Côte d'Azur / I3S Laboratory / INRIA, France |
| 412 |  |  [The Capable Web](https://doi.org/10.1145/3543873.3587988) |  | 0 | In this paper, I discuss arguments in favor and in disfavor of building for the Web. I look at three extraordinary examples of apps built for the Web, and analyze reasons their creators provided for doing so. In continuation, I look at the decline of interest in cross-platform app frameworks with the exception of Flutter, which leads me to the two research questions RQ1 "Why do people not fully bet on PWA" and RQ2 "Why is Flutter so popular". My hypothesis for why developers don’t more frequently set on the Web is that in many cases they (or their non-technical reporting lines) don’t realize how powerful it has become. To counter that, I introduce a Web app and a browser extension that demonstrate the Web’s capabilities. | Thomas Steiner | Google Germany GmbH, Germany |
| 413 |  |  [MetaCrimes: Criminal accountability for conducts in the Metaverse](https://doi.org/10.1145/3543873.3587535) |  | 0 | The research addresses a topic whose precise boundaries are yet to be defined: the criminal accountability for conducts committed in the Metaverse. Following a short introduction motivating the reason why this issue has to be considered as pivotal both for the Web and for the society, the main problem raised by the research will be identified, namely, whether an action taken against a person, that in real-life would be a criminal conduct, is considerable as a crime in the Metaverse as well. A short assessment of the (very little so far) current state of the art, as well as the proposed approach and methodology will be then overviewed; finally, the contribution shows its current results, and concludes stating that countries are highly encouraged to shape respective criminal frameworks when applied to the Metaverse, and that the international community should consider the topic as a priority in its agenda. Nevertheless, further experimental and research work has still to be made. | Gian Marco Bovenzi | Center for Higher Defence Studies, University of Turin, Italy |
| 414 |  |  [SEKA: Seeking Knowledge Graph Anomalies](https://doi.org/10.1145/3543873.3587536) |  | 0 | Knowledge Graphs (KGs) form the backbone of many knowledge dependent applications such as search engines and digital personal assistants. KGs are generally constructed either manually or automatically using a variety of extraction techniques applied over multiple data sources. Due to the diverse quality of these data sources, there are likely anomalies introduced into any KG. Hence, it is unrealistic to expect a perfect archive of knowledge. Given how large KGs can be, manual validation is impractical, necessitating an automated approach for anomaly detection in KGs. To improve KG quality, and to identify interesting and abnormal triples (edges) and entities (nodes) that are worth investigating, we introduce SEKA, a novel unsupervised approach to detect anomalous triples and entities in a KG using both the structural characteristics and the content of edges and nodes of the graph. While an anomaly can be an interesting or unusual discovery, such as a fraudulent transaction requiring human intervention, anomaly detection can also identify potential errors. We propose a novel approach named Corroborative Path Algorithm to generate a matrix of semantic features, which we then use to train a one-class Support Vector Machine to identify abnormal triples and entities with no dependency on external sources. We evaluate our approach on four real-world KGs demonstrating the ability of SEKA to detect anomalies, and to outperform comparative baselines. | Asara Senaratne | School of Computing, The Australian National University, Australia |
| 415 |  |  [Detecting Cross-Lingual Information Gaps in Wikipedia](https://doi.org/10.1145/3543873.3587539) |  | 0 | An information gap exists across Wikipedia’s language editions, with a considerable proportion of articles available in only a few languages. As an illustration, it has been observed that 10 languages possess half of the available Wikipedia articles, despite the existence of 330 Wikipedia language editions. To address this issue, this study presents an approach to identify the information gap between the different language editions of Wikipedia. The proposed approach employs Latent Dirichlet Allocation (LDA) to analyze linked entities in a cross-lingual knowledge graph in order to determine topic distributions for Wikipedia articles in 28 languages. The distance between paired articles across language editions is then calculated. The potential applications of the proposed algorithm to detecting sources of information disparity in Wikipedia are discussed, and directions for future research are put forward. | Vahid Ashrafimoghari | Stevens Institute of Technology, USA |
| 416 |  |  [Caught in the Game: On the History and Evolution of Web Browser Gaming](https://doi.org/10.1145/3543873.3585572) |  | 0 | Web browsers have come a long way since their inception, evolving from a simple means of displaying text documents over the network to complex software stacks with advanced graphics and network capabilities. As personal computers grew in popularity, developers jumped at the opportunity to deploy cross-platform games with centralized management and a low barrier to entry. Simply going to the right address is now enough to start a game. From text-based to GPU-powered 3D games, browser gaming has evolved to become a strong alternative to traditional console and mobile-based gaming, targeting both casual and advanced gamers. Browser technology has also evolved to accommodate more demanding applications, sometimes even supplanting functions typically left to the operating system. Today, websites display rich, computationally intensive, hardware-accelerated graphics, allowing developers to build ever-more impressive applications and games.In this paper, we present the evolution of browser gaming and the technologies that enabled it, from the release of the first text-based games in the early 1990s to current open-world and game-engine-powered browser games. We discuss the societal impact of browser gaming and how it has allowed a new target audience to accessdigital gaming. Finally, we review the potential future evolution ofthe browser gaming industry. | Naif Mehanna, Walter Rudametkin | Univ Rennes, Institut Universitaire de France (IUF), CNRS, Inria, UMR 6074 IRISA, France; Univ Lille, CNRS, Inria, UMR 9189 CRIStAL, France |
| 417 |  |  [Identifying Stable States of Large Signed Graphs](https://doi.org/10.1145/3543873.3587544) |  | 0 | Signed network graphs provide a way to model complex relationships and interdependencies between entities: negative edges allow for a deeper study of social dynamics. One approach to achieving balance in a network is to model the sources of conflict through structural balance. Current methods focus on computing the frustration index or finding the largest balanced clique, but these do not account for multiple ways to reach a consensus or scale well for large, sparse networks. In this paper, we propose an expansion of the frustration cloud computation and compare various tree-sampling algorithms that can discover a high number of diverse balanced states. Then, we compute and compare the frequencies of balanced states produced by each. Finally, we investigate these techniques’ impact on the consensus feature space. | Muhieddine Shebaro, Jelena Tesic | Department of Computer Science, Texas State University, USA |
| 418 |  |  [Those who are left behind: A chronicle of internet access in Cuba](https://doi.org/10.1145/3543873.3585573) |  | 0 | This paper presents a personal chronicle of internet access in Cuba from the perspective of a visitor to the island. It is told across three time periods: 1997, 2010, and 2021. The story describes how the island first connected to the internet in the 90s, how internet access evolved throughout the 2000s, and ends in the role the internet played in the government protests on July 11, 2021. The article analyzes how internet access in Cuba has changed over the decades and its effects on civil society. It discusses issues such as Cuba’s technological infrastructure, internet censorship, and free expression. | Brenda Reyes Ayala | University of Alberta, Canada |
| 419 |  |  [Reflex-in: Generate Music on the Web with Real-time Brain Wave](https://doi.org/10.1145/3543873.3587315) |  | 0 | Reflex-in is a sound installation that uses brain-wave streams to create music composition within the Web environment in real time. The work incorporates various state-of-the-art Web technologies, including Web Audio, WebSocket, WebAssembly, and WebGL. The music generated from the algorithm - mapping brain wave signal to musical events - aims to produce a form of furniture music that is relaxing and meditative, possibly therapeutic. This effect can be further enhanced through binaural beats or other forms of auditory stimulation, also known as “digital drugs,” which can be enabled through the user interface. The system represents a potential avenue for the development of closed-loop brain-computer interfaces by using the listener’s own brain waves as the source of musical stimuli, which can be used for therapeutic or medical purposes. | Shihong Ren, Michel Buffa, Laurent Pottier, Yang Yu, Gerwin Schalk | Shanghai Key Laboratory for Music Acoustic, Shanghai Conservatory of Music, China and Laboratoire d'Études du Contemporain en Littératures, Langues, Arts, Université Jean Monnet, France; Frontier Lab for Applied Neurotechnology, Tianqiao and Chrissy Chen Institute, China; Laboratoire d'Informatique, Signaux et Systèmes de Sophia Antipolis, Université Côte d'Azur, France; Laboratoire d'Études du Contemporain en Littératures, Langues, Arts, Université Jean Monnet, France; Shanghai Key Laboratory for Music Acoustic, Shanghai Conservatory of Music, China |
| 420 |  |  [Wikidata: The Making Of](https://doi.org/10.1145/3543873.3585579) |  | 0 | Wikidata, now a decade old, is the largest public knowledge graph, with data on more than 100 million concepts contributed by over 560,000 editors. It is widely used in applications and research. At its launch in late 2012, however, it was little more than a hopeful new Wikimedia project, with no content, almost no community, and a severely restricted platform. Seven years earlier still, in 2005, it was merely a rough idea of a few PhD students, a conceptual nucleus that had yet to pick up many important influences from others to turn into what is now called Wikidata. In this paper, we try to recount this remarkable journey, and we review what has been accomplished, what has been given up on, and what is yet left to do for the future. | Denny Vrandecic, Lydia Pintscher, Markus Krötzsch | Wikimedia Foundation, USA; Wikimedia Deutschland, Germany; TU Dresden, Germany |
| 421 |  |  [A History of Diversity in The Web (Conference)](https://doi.org/10.1145/3543873.3585576) |  | 0 | The Web has grown considerably since its inception and opened up a multitude of opportunities for people all around the world for work, leisure, and learning. These opportunities were limited to western audiences earlier on, but globalization has now put almost the entire world online. While there is a growing social understanding and acknowledgment of various gender and ethnic groups in society, we still have a long way to go toward achieving equity in gender and ethnic representations, especially in the workplace. In this paper, we attempt to quantify the diversity and evenness in terms of gender and ethnicity of The WebConference participants over its 30 year history. The choice is motivated by the monumental contribution of this conference to the evolution of the web. In particular, we study the gender and ethnicity of program committee members, authors and other speakers at the conference between 1994-2022. We also generate the co-speaker network over the three decades to study how closely the speakers work with each other. Our findings show that we still have a long way to go before achieving fair representation at The WebConference, especially for female participants and individuals from non-White, non-Asian ethnicities. | Siddharth D. Jaiswal, Animesh Mukherjee | Indian Institute of Technology, Kharagpur, India |
| 422 |  |  [Why are Hyperlinks Blue?: A deep dive into browser hyperlink color history](https://doi.org/10.1145/3543873.3587714) |  | 0 | The internet has ingrained itself into every aspect of our lives, but there's one aspect of the digital world that some take for granted. Did you ever notice that many links, specifically hyperlinks, are blue? When a coworker casually asked me why links are blue, I was stumped. As a user experience designer who has created websites since 2001, I've always made my links blue. I have advocated for the specific shade of blue, and for the consistent application of blue, yes, but I've never stopped and wondered, why are links blue? It was just a fact of life. Grass is green and hyperlinks are blue. Culturally, we associate links with the color blue so much that in 2016, when Google changed its links to black, it created quite a disruption [1]. But now, I find myself all consumed by the question, WHY are links blue? WHO decided to make them blue? WHEN was this decision made, and HOW has this decision made such a lasting impact? Mosaic, an early browser released by Marc Andreessen and Eric Bina on January 23, 1993 [2], had blue hyperlinks. To truly understand the origin and evolution of hyperlinks, I took a journey through technology history and interfaces to explore how links were handled before color monitors, and how interfaces and hyperlinks rapidly evolved once color monitors became an option. | Elise Blanchard | Mozilla, USA |
| 423 |  |  [The One Hundred Year Web](https://doi.org/10.1145/3543873.3585578) |  | 0 | The year 2023 marks the thirty-second anniversary of the World Wide Web being announced. In the intervening years, the web has become an essential part of the fabric of society. Part of that is that huge amounts of information that used to be available (only) on paper is now available (only) electronically. One of the dangers of this is that owners of information often treat the data as ephemeral, and delete old information once it becomes out of date. As a result society is at risk of losing large parts of its history. So it is time to assess how we use the web, how it has been designed, and what we should do to ensure that in one hundred years time (and beyond) we will still be able to access, and read, what we are now producing. We can still read 100 year-old books; that should not be any different for the web. This paper takes a historical view of the web, and discusses the web from its early days: why it was successful compared with other similar systems emerging at the time, the things it did right, the mistakes that were made, and how it has developed to the web we know today, to what extent it meets the requirements needed for such an essential part of society's infrastructure, and what still needs to be done. | Steven Pemberton | CWI, Netherlands |
| 424 |  |  [Followers Tell Who an Influencer Is](https://doi.org/10.1145/3543873.3587576) |  | 0 | Influencers are followed by a relatively smaller group of people on social media platforms under a common theme. Unlike the global celebrities, it is challenging to categorize influencers into general categories of fame (e.g., Politics, Religion, Entertainment, etc.) because of their overlapping and narrow reach to people interested in these categories. In this paper, we focus on categorizing influencers based on their followers. We exploit the top-1K Twitter celebrities to identify the common interest among the followers of an influencer as his/her category. We annotate the top one thousand celebrities in multiple categories of popularity, language, and locations. Such categorization is essential for targeted marketing, recommending experts, etc. We define a novel FollowerSimilarity between the set of followers of an influencer and a celebrity. We propose an inverted index to calculate similarity values efficiently. We exploit the similarity score in a K-Nearest Neighbor classifier and visualize the top celebrities over a neighborhood-embedded space. | Dheeman Saha, Md Rashidul Hasan, Abdullah Mueen | Department of Mathematics and Statistics, University of New Mexico, USA; Department of Computer Science, University of New Mexico, USA |
| 425 |  |  [Smart Cities as Hubs: a use case from Biotechnology](https://doi.org/10.1145/3543873.3587582) |  | 0 | Smart city platforms operate as central points of access for locally collected data. The smart city hub (SCHub) introduces a new concept that aims to homogenize data, service, human and material flows in cities. A proof of concept is based on non-typical data flows, like the ones that are collected by biotechnological activities, like Diet-related non-communicable diseases (NCDs). NCDs are responsible for 1 in 5 deaths globally. Most of the diet related NCDs are related to the gut microbiome, the microbial community that resides in our gastrointestinal tract. The imbalance or loss of microbiome diversity is one of the main factors leading to NCDs by affecting various functions, including energy metabolism, intestinal permeability, and brain function. Gut dysbiosis is reflected in altered concentrations of Short Chain Fatty Acids (SCFAs), produced by the gut microbiota. A microcapsule system can play the role of a sensor that collects data from the local community and transmits it to the SCHub in order for the doctors to receive the appropriate patients information and define the appropriate treatment method; for the city to process anonymized information and measure community's health in diet terms. A prototype with a biosensor that correlates the amount of gut SCFAs with gut microbiome functional capacities is presented in this paper, together with the use-case scenario that engages the SCHub. | Tsapadikou Asteria, Leonidas G. Anthopoulos | Department of Biochemistry and Biotechnology, University of Thessaly, Greece; Department of Business Administration, University of Thessaly, Greece |
| 426 |  |  [Do bridges dream of water pollutants? Towards DreamsKG, a knowledge graph to make digital access for sustainable environmental assessment come true](https://doi.org/10.1145/3543873.3587590) |  | 0 | An environmental assessment (EA) report describes and assesses the environmental impact of a series of activities involved in the development of a project. As such, EA is a key tool for sustainability. Improving information access to EA reporting is a billion-euro untapped business opportunity to build an engaging, efficient digital experience for EA. We aim to become a landmark initiative in making this experience come true, by transforming the traditional manual assessment of numerous heterogeneous reports by experts into a computer-assisted approach. Specifically, a knowledge graph that represents and stores facts about EA practice allows for what it is so far only accessible manually to become machine-readable, and by this, to enable downstream information access services. This paper describes the ongoing process of building DreamsKG, a knowledge graph that stores relevant data- and expert-driven EA reporting and practicing in Denmark. Representation of cause-effect relations in EA and integration of Sustainable Developmental Goals (SDGs) are among its prominent features. | Darío Garigliotti, Johannes Bjerva, Finn Årup Nielsen, Annika Butzbach, Ivar Lyhne, Lone Kørnøv, Katja Hose | Aalborg University, Denmark; Technical University of Denmark, Denmark; Aalborg University, Denmark and TU Wien, Austria |
| 427 |  |  [Towards High Resolution Urban Heat Analysis: Incorporating Thermal Drones to Enhance Satellite Based Urban Heatmaps](https://doi.org/10.1145/3543873.3587682) |  | 0 | As remote-sensing becomes more actively utilized in the environmental sciences, our research continues the efforts in adapting smart cities by using civilian UAVs and drones for land surface temperature (LST) analysis. Given the increased spatial resolution that this technology provides as compared to standard satellite measurements, we sought to further study the urban heat island (UHI) effect – specifically when it comes to heterogeneous and dynamic landscapes such as the Charleston peninsula. Furthermore, we sought to develop a method to enhance the spatial resolution of publicly available LST temperature data (such as those measured from the Landsat satellites) by building a machine learning model utilizing remote-sensed data from drones. While we found a high correlation and an accurate degree of prediction for areas of open water and vegetation (respectively), our model struggled when it came to areas containing highly impervious surfaces. We believe, however, that these findings further illustrate the discrepancy between high and medium spatial resolutions, and demonstrate how urban environments specifically are prone to inaccurate LST measurements and are uniquely in need of an industry pursuit of higher spatial resolution for hyperlocal environmental sciences and urban analysis. | Bryan Rickens, Navid Hashemi Tonekaboni | Department of Computer Science, College of Charleston, USA |
| 428 |  |  [Towards a Sustainability Index Calculator for Smart Cities](https://doi.org/10.1145/3543873.3587683) |  | 0 | In this era of rapid urbanization and our endeavors to create more smart cities, it's crucial to keep track of how our society and neighborhood are getting impacted. It is important to make conscious decisions to keep harmony in sustainability. There are multiple frameworks to evaluate how sustainability is measured and to understand how sustainable a place is, be it a city or a region, and one such framework is the Circles of Sustainability. Though these frameworks offer good solutions, it is a challenge to collect relevant data to make the framework widely usable. This paper focuses on this specific issue by utilizing the methodology introduced in the framework and applying it practically to better understand how sustainable our cities and society are. We present a unique web-based application which utilizes publicly accessible data to compute sustainability scores and rank for every city and presents the results in an intelligent and easy to comprehend visual interface. The paper also discusses the technical difficulties associated with creating such an application, including data collection, data processing, data integration, and scoring algorithm. The paper concludes by discussing the needs for such practical solutions for promoting sustainable urban development. | Ramesh Gorantla, Srividya Bansal | Arizona State University, USA |
| 429 |  |  [Sustainable Grain Transportation in Ukraine Amidst War Utilizing KNARM and KnowWhereGraph](https://doi.org/10.1145/3543873.3587618) |  | 0 | In this work, we propose a sustainable path-finding application for grain transportation during the ongoing Russian military invasion in Ukraine. This application is to build a suite of algorithms to find possible optimal paths for transporting grain that remains in Ukraine. The application uses the KNowledge Acquisition and Representation Methodology(KNARM) and the KnowWhereGraph to achieve this goal. Currently, we are working towards creating an ontology that will allow for a more effective heuristic approach by incorporating the lessons learned from the KnowWhereGraph. The aim is to enhance the path-finding process and provide more accurate and efficient results. In the future, we will continue exploring and implementing new techniques that can further improve the sustainability of the path-finding applications with a knowledge graph backend for grain transportation through hazardous and adversarial environments. The code is available upon reviewer’s request. It can not be made public due to the sensitive nature of the data. | Yinglun Zhang, Antonina Broyaka, Jude Kastens, Allen M. Featherstone, Cogan Shimizu, Pascal Hitzler, Hande KüçükMcGinty | Kansas State Univerisity, USA; Wright State University, USA; University of Kansas, USA; Department of Agricultural Economics, Kansas State Univerisity, USA; Kansas State University, USA |
| 430 |  |  [Entity and Event Topic Extraction from Podcast Episode Title and Description Using Entity Linking](https://doi.org/10.1145/3543873.3587648) |  | 0 | To improve Amazon Music podcast services and customer engagements, we introduce Entity-Linked Topic Extraction (ELTE) to identify well-known entity and event topics from podcast episodes. An entity can be a person, organization, work-of-art, etc., while an event, such as the Opioid epidemic, occurs at specific point(s) in time. ELTE first extracts key-phrases from episode title and description metadata. It then uses entity linking to canonicalize them against Wikipedia knowledge base (KB), ensuring that the topics exist in the real world. ELTE also models NIL-predictions for entity or event topics that are not in the KB, as well as topics that are not of entity or event type. To test the model, we construct a podcast topic database of 1166 episodes from various categories. Each episode comes with a Wiki-link annotated main topic or NIL-prediction. ELTE produces the best overall Exact Match EM score of .84, with by-far the best EM of .89 among the entity or event type episodes, as well as NIL-predictions for episodes without entity or event main topic (EM score of .86). | Christian Siagian, Amina Shabbeer | Amazon, USA |
| 431 |  |  [NASA Science Mission Directorate Knowledge Graph Discovery](https://doi.org/10.1145/3543873.3587585) |  | 0 | The size of the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) is growing exponentially, allowing researchers to make discoveries. However, making discoveries is challenging and time-consuming due to the size of the data catalogs, and as many concepts and data are indirectly connected. This paper proposes a pipeline to generate knowledge graphs (KGs) representing different NASA SMD domains. These KGs can be used as the basis for dataset search engines, saving researchers time and supporting them in finding new connections. We collected textual data and used several modern natural language processing (NLP) methods to create the nodes and the edges of the KGs. We explore the cross-domain connections, discuss our challenges, and provide future directions to inspire researchers working on similar challenges. | Roelien C. Timmer, Megan Mark, Fech Scen Khoo, Marcella Scoczynski Ribeiro Martins, Anamaria Berea, Gregory Renard, Kaylin M. Bugbee | The Applied AI Company (AAICO), USA; George Mason University, USA; Federal University of Technology, Brazil; NASA Marshall Space Flight Center, USA; The University of New South Wales, Australia |
| 432 |  |  [Scientific Data Extraction from Oceanographic Papers](https://doi.org/10.1145/3543873.3587595) |  | 0 | Scientific data collected in the oceanographic domain is invaluable to researchers when performing meta-analyses and examining changes over time in oceanic environments. However, many of the data samples and subsequent analyses published by researchers are not uploaded to a repository leaving the scientific paper as the only available source. Automated extraction of scientific data is, therefore, a valuable tool for such researchers. Specifically, much of the most valuable data in scientific papers are structured as tables, making these a prime target for information extraction research. Using the data relies on an additional step where the concepts mentioned in the tables, such as names of measures, units, and biological species, are identified within a domain ontology. Unfortunately, state-of-the-art table extraction leaves much to be desired and has not been attempted on a large scale on oceanographic papers. Furthermore, while entity linking in the context of a full paragraph of text has been heavily researched, it is still lacking in this harder task of linking single concepts. In this work, we present an annotated benchmark dataset of data tables from oceanographic papers. We further present the result of an evaluation on the extraction of these tables and the linking of the contained entities to the domain and general-purpose knowledge bases using the current state of the art. We highlight the challenges and quantify the performance of current tools for table extraction and table-concept linking. | Bartal Eyðfinsson Veyhe, Tomer Sagi, Katja Hose | Computer Science, Aalborg Universitet, Denmark; Computer Science, Aalborg University, Denmark; Aalborg University, Denmark and TU Wien, Austria |
| 433 |  |  [Cross-Team Collaboration and Diversity in the Bridge2AI Project](https://doi.org/10.1145/3543873.3587579) |  | 0 | The Bridge2AI project, funded by the National Institutes of Health, involves researchers from different disciplines and backgrounds to develop well-curated AI health data and tools. Understanding cross-disciplinary and cross-organizational collaboration at the individual, team, and project levels is critical. In this paper, we matched Bridge2AI team members to the PubMed Knowledge dataset to get their health-related publications. We built the collaboration network for Bridge2AI members and all of their collaborators and sorted out researchers with the largest degree of centrality and betweenness centrality. Our finding suggests that Bridge2AI members need to strengthen internal collaborations and boost mutual understanding in this project. We also applied machine learning methods to cluster all the researchers and labeled publication topics in different clusters. Finally, by identifying the gender/racial diversity of researchers, we found that teams with higher racial diversity receive more citations, and individuals with diverse gender collaborators publish more papers. | Huimin Xu, Chitrank Gupta, Zhandos Sembay, Swathi Thaker, Pamela PayneFoster, Jake Chen, Ying Ding | The University of Texas at Austin, USA; University of Alabama at Birmingham, USA; University of Alabama, USA |
| 434 |  |  [A New Annotation Method and Dataset for Layout Analysis of Long Documents](https://doi.org/10.1145/3543873.3587609) |  | 0 | Parsing long documents, such as books, theses, and dissertations, is an important component of information extraction from scholarly documents. Layout analysis methods based on object detection have been developed in recent years to help with PDF document parsing. However, several challenges hinder the adoption of such methods for scholarly documents such as theses and dissertations. These include (a) the manual effort and resources required to annotate training datasets, (b) the scanned nature of many documents and the inherent noise present resulting from the capture process, and (c) the imbalanced distribution of various types of elements in the documents. In this paper, we address some of the challenges related to object detection based layout analysis for scholarly long documents. First, we propose an AI-aided annotation method to help develop training datasets for object detection based layout analysis. This leverages the knowledge of existing trained models to help human annotators, thus reducing the time required for annotation. It also addresses the class imbalance problem, guiding annotators to focus on labeling instances of rare classes. We also introduce ETD-ODv2, a novel dataset for object detection on electronic theses and dissertations (ETDs). In addition to the page images included in ETD-OD [1], our dataset consists of more than 16K manually annotated page images originating from 100 scanned ETDs, along with annotations for 20K page images primarily consisting of rare classes that were labeled using the proposed framework. The new dataset thus covers a diversity of document types, viz., scanned and born-digital, and is better balanced in terms of training samples from different object categories. | Aman Ahuja, Kevin Dinh, Brian Dinh, William A. Ingram, Edward A. Fox | Virginia Tech, USA; Department of Computer Science, Virginia Tech, USA |
| 435 |  |  [Towards InnoGraph: A Knowledge Graph for AI Innovation](https://doi.org/10.1145/3543873.3587614) |  | 0 | Researchers seeking to comprehend the state-of-the-art innovations in a particular field of study must examine recent patents and scientific articles in that domain. Innovation ecosystems consist of interconnected information about entities such as researchers, institutions, projects, products, and technologies. However, representing such information in a machine-readable format is challenging because concepts like "knowledge" are not easily represented. Nonetheless, even a partial representation of innovation ecosystems provides valuable insights. Therefore, representing innovation ecosystems as knowledge graphs (KGs) would enable advanced data analysis and generate new insights. To this end, we propose InnoGraph, a framework that integrates multiple heterogeneous data sources to build a Knowledge Graph of the worldwide AI innovation ecosystem. | M. Besher Massri, Blerina Spahiu, Marko Grobelnik, Vladimir Alexiev, Matteo Palmonari, Dumitru Roman | Department of Artificial Intelligence, Jozef Stefan Institute, Slovenia; University of Milano-Bicocca, Italy; Ontotext (Sirma AI), Bulgaria; SINTEF AS, Norway; Department of Artificial Intelligence, Jozef Stefan Institute, Slovenia and Jozef Stefan International Postgraduate School, Slovenia |
| 436 |  |  [Assessing Scientific Contributions in Data Sharing Spaces](https://doi.org/10.1145/3543873.3587608) |  | 0 | In the present academic landscape, the process of collecting data is slow, and the lax infrastructures for data collaborations lead to significant delays in coming up with and disseminating conclusive findings. Therefore, there is an increasing need for a secure, scalable, and trustworthy data-sharing ecosystem that promotes and rewards collaborative data-sharing efforts among researchers, and a robust incentive mechanism is required to achieve this objective. Reputation-based incentives, such as the h-index, have historically played a pivotal role in the academic community. However, the h-index suffers from several limitations. This paper introduces the SCIENCE-index, a blockchain-based metric measuring a researcher's scientific contributions. Utilizing the Microsoft Academic Graph and machine learning techniques, the SCIENCE-index predicts the progress made by a researcher over their career and provides a soft incentive for sharing their datasets with peer researchers. To incentivize researchers to share their data, the SCIENCE-index is augmented to include a data-sharing parameter. DataCite, a database of openly available datasets, proxies this parameter, which is further enhanced by including a researcher's data-sharing activity. Our model is evaluated by comparing the distribution of its output for geographically diverse researchers to that of the h-index. We observe that it results in a much more even spread of evaluations. The SCIENCE-index is a crucial component in constructing a decentralized protocol that promotes trust-based data sharing, addressing the current inequity in dataset sharing. The work outlined in this paper provides the foundation for assessing scientific contributions in future data-sharing spaces powered by decentralized applications. | Kacy Adams, Fernando Spadea, Conor Flynn, Oshani Seneviratne |  |
| 437 |  |  [Promoting Inactive Members in Edge-Building Marketplace](https://doi.org/10.1145/3543873.3587647) |  | 0 | Social networks are platforms where content creators and consumers share and consume content. The edge recommendation system, which determines who a member should connect with, significantly impacts the reach and engagement of the audience on such networks. This paper emphasizes improving the experience of inactive members (IMs) who do not have a large connection network by recommending better connections. To that end, we propose a multi-objective linear optimization framework and solve it using accelerated gradient descent. We report our findings regarding key business metrics related to user engagement on LinkedIn, a professional network with over 850 million members. | Ayan Acharya, Siyuan Gao, Borja Ocejo, Kinjal Basu, Ankan Saha, Sathiya Keerthi Selvaraj, Rahul Mazumder, Parag Agrawal, Aman Gupta | LinkedIn Corporation, USA; LinkedIn Inc., USA; linkedin corporation, USA; LinkedIn Corporation, USA and Massachusetts Institute of Technology, USA; LinkedIn, USA |
| 438 |  |  [CLIME: Completeness-Constrained LIME](https://doi.org/10.1145/3543873.3587652) |  | 0 | We evaluate two popular local explainability techniques, LIME and SHAP, on a movie recommendation task. We discover that the two methods behave very differently depending on the sparsity of the data set, where sparsity is defined by the amount of historical viewing data available to explain a movie recommendation for a particular data instance. We find that LIME does better than SHAP in dense segments of the data set and SHAP does better in sparse segments. We trace this difference to the differing bias-variance characteristics of the underlying estimators of LIME and SHAP. We find that SHAP exhibits lower variance in sparse segments of the data compared to LIME. We attribute this lower variance to the completeness constraint property inherent in SHAP and missing in LIME. This constraint acts as a regularizer and therefore increases the bias of the SHAP estimator but decreases its variance, leading to a favorable bias-variance trade-off especially in high sparsity data settings. With this insight, we introduce the same constraint into LIME and formulate a novel local explainabilty framework called Completeness-Constrained LIME (CLIME) that is superior to LIME and much faster than SHAP. | Claudia V. Roberts, Ehtsham Elahi, Ashok Chandrashekar | WarnerMedia, USA; Netflix, Inc., USA; Princeton University, USA |
| 439 |  |  [DeepPvisit: A Deep Survival Model for Notification Management](https://doi.org/10.1145/3543873.3587666) |  | 0 | Notification is a core feature of mobile applications. They inform users about a variety of events happening in the communities. Users may take immediate action to visit the app or ignore the notifications depending on the timing and the relevance of a notification to the user. In this paper, we present the design, implementation, and evaluation of DeepPvisit, a novel probabilistic deep learning survival method for modeling interactions between a user visit and a mobile notification decision, targeting notification volume and delivery time optimization, and driving long-term user engagements. Offline evaluations and online A/B test experiments show DeepPvisit outperforms the existing survival regression model and the other baseline models and delivers better business metrics online. | Guangyu Yang, Efrem Ghebreab, Jiaxi Xu, Xianen Qiu, Yiping Yuan, Wensheng Sun | LinkedIn Corporation, USA |
| 440 |  |  [Digital Twins for Radiation Oncology](https://doi.org/10.1145/3543873.3587688) |  | 0 | Digital twin technology has revolutionized the state-of-the-art practice in many industries, and digital twins have a natural application to modeling cancer patients. By simulating patients at a more fundamental level than conventional machine learning models, digital twins can provide unique insights by predicting each patient's outcome trajectory. This has numerous associated benefits, including patient-specific clinical decision-making support and the potential for large-scale virtual clinical trials. Historically, it has not been feasible to use digital twin technology to model cancer patients because of the large number of variables that impact each patient's outcome trajectory, including genotypic, phenotypic, social, and environmental factors. However, the path to digital twins in radiation oncology is becoming possible due to recent progress, such as multiscale modeling techniques that estimate patient-specific cellular, molecular, and histological distributions, and modern cryptographic techniques that enable secure and efficient centralization of patient data across multiple institutions. With these and other future scientific advances, digital twins for radiation oncology will likely become feasible. This work discusses the likely generalized architecture of patient-specific digital twins and digital twin networks, as well as the benefits, existing barriers, and potential gateways to the application of digital twin technology in radiation oncology. | James Jensen, Jun Deng | Department of Therapeutic Radiology, Yale University, USA |
| 441 |  |  [Graph-Based Hierarchical Attention Network for Suicide Risk Detection on Social Media](https://doi.org/10.1145/3543873.3587587) |  | 0 | The widespread use of social media for expressing personal thoughts and emotions makes it a valuable resource for identifying individuals at risk of suicide. Existing sequential learning-based methods have shown promising results. However, these methods may fail to capture global features. Due to its inherent ability to learn interconnected data, graph-based methods can address this gap. In this paper, we present a new graph-based hierarchical attention network (GHAN) that uses a graph convolutional neural network with an ordinal loss to improve suicide risk identification on social media. Specifically, GHAN first captures global features by constructing three graphs to capture semantic, syntactic, and sequential contextual information. Then encoded textual features are fed to attentive transformers’ encoder and optimized to factor in the increasing suicide risk levels using an ordinal classification layer hierarchically for suicide risk detection. Experimental results show that the proposed GHAN outperformed state-of-the-art methods on a public Reddit dataset. | Usman Naseem, Jinman Kim, Matloob Khushi, Adam G. Dunn | School of Medical Sciences, The University of Sydney, Australia; Department of Computer Science, Brunel University, United Kingdom; School of Computer Science, The University of Sydney, Australia |
| 442 |  |  [I'm out of breath from laughing! I think? A dataset of COVID-19 Humor and its toxic variants](https://doi.org/10.1145/3543873.3587591) |  | 0 | Humor is a cognitive construct that predominantly evokes the feeling of mirth. During the COVID-19 pandemic, the situations that arouse out of the pandemic were so incongruous to the world we knew that even factual statements often had a humorous reaction. In this paper, we present a dataset of 2510 samples hand-annotated with labels such as humor style, type, theme, target and stereotypes formed or exploited while creating the humor in addition to 909 memes. Our dataset comprises Reddit posts, comments, Onion news headlines, real news headlines, and tweets. We evaluate the task of humor detection and maladaptive humor detection on state-of-the-art models namely RoBERTa and GPT-3. The finetuned models trained on our dataset show significant gains over zero-shot models including GPT-3 when detecting humor. Even though GPT-3 is good at generating meaningful explanations, we observed that it fails to detect maladaptive humor due to the absence of overt targets and profanities. We believe that the presented dataset will be helpful in designing computational methods for topical humor processing as it provides a unique sample set to study the theory of incongruity in a post-pandemic world. The data is available to research community at https://github.com/smritae01/Covid19_Humor. | Neha Reddy Bogireddy, Smriti Suresh, Sunny Rai | Boston University, USA; University of Pennsylvania, USA; George Mason University, USA |
| 443 |  |  [LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models](https://doi.org/10.1145/3543873.3587605) |  | 0 | Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. In this work, we conduct a case study approaching this problem with semi-supervised learning using a large language model (LLM). We select Reddit as the target social media platform due to its broad coverage of topics and content types. Our pipeline first generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while training the student model using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation. | Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, Michael Bendersky | Google, USA; The Ohio State University, USA |
| 444 |  |  [Forecasting COVID-19 Vaccination Rates using Social Media Data](https://doi.org/10.1145/3543873.3587639) |  | 0 | The COVID-19 pandemic has had a profound impact on the global community, and vaccination has been recognized as a crucial intervention. To gain insight into public perceptions of COVID-19 vaccines, survey studies and the analysis of social media platforms have been conducted. However, existing methods lack consideration of individual vaccination intentions or status and the relationship between public perceptions and actual vaccine uptake. To address these limitations, this study proposes a text classification approach to identify tweets indicating a user’s intent or status on vaccination. A comparative analysis between the proportions of tweets from different categories and real-world vaccination data reveals notable alignment, suggesting that tweets may serve as a precursor to actual vaccination status. Further, regression analysis and time series forecasting were performed to explore the potential of tweet data, demonstrating the significance of incorporating tweet data in predicting future vaccination status. Finally, clustering was applied to the tweet sets with positive and negative labels to gain insights into underlying focuses of each stance. | Xintian Li, Aron Culotta | Department of Computer Science, Tulane University, USA |
| 445 |  |  [A Cross-Modal Study of Pain Across Communities in the United States](https://doi.org/10.1145/3543873.3587642) |  | 0 | Pain is one of the most prevalent reasons for seeking medical attention in the United States. Understanding how different communities report and express pain can aid in directing medical efforts and in advancing precision pain management. Using a large-scale self-report survey data set on pain from Gallup (2.5 million surveys) and social media posts from Twitter (1.8 million tweets), we investigate a) if Twitter posts could predict community-level pain and b) how expressions of pain differ across communities in the United States. Beyond observing an improvement of over 9% (in Pearson r) when using Twitter language over demographics to predict community-level pain, our study reveals that the discourse on pain varied significantly across communities in the United States. Evangelical Hubs frequently post about God, lessons from struggle, and prayers when expressing pain, whereas Working Class Country posts about regret and extreme endurance. Academic stresses, injuries, painkillers, and surgeries were the most commonly discussed pain themes in College Towns; Graying America discussed therapy, used emotional language around empathy and anger, and posted about chronic pain treatment; the African American South posted about struggles, patience, and faith when talking about pain. Our study demonstrates the efficacy of using Twitter to predict survey-based self-reports of pain across communities and has implications in aiding community-focused pain management interventions. | Arnav Aggarwal, Sunny Rai, Salvatore Giorgi, Shreya Havaldar, Garrick Sherman, Juhi Mittal, Sharath Chandra Guntuku | University of Pennsylvania, USA |
| 446 |  |  [Claim Extraction and Dynamic Stance Detection in COVID-19 Tweets](https://doi.org/10.1145/3543873.3587643) |  | 0 | The information ecosystem today is noisy, and rife with messages that contain a mix of objective claims and subjective remarks or reactions. Any automated system that intends to capture the social, cultural, or political zeitgeist, must be able to analyze the claims as well as the remarks. Due to the deluge of such messages on social media, and their tremendous power to shape our perceptions, there has never been a greater need to automate these analyses, which play a pivotal role in fact-checking, opinion mining, understanding opinion trends, and other such downstream tasks of social consequence. In this noisy ecosystem, not all claims are worth checking for veracity. Such a check-worthy claim, moreover, must be accurately distilled from subjective remarks surrounding it. Finally, and especially for understanding opinion trends, it is important to understand the stance of the remarks or reactions towards that specific claim. To this end, we introduce a COVID-19 Twitter dataset, and present a three-stage process to (i) determine whether a given Tweet is indeed check-worthy, and if so, (ii) which portion of the Tweet ought to be checked for veracity, and finally, (iii) determine the author’s stance towards the claim in that Tweet, thus introducing the novel task of topic-agnostic stance detection. | Noushin Salek Faramarzi, Fateme Hashemi Chaleshtori, Hossein Shirazi, Indrakshi Ray, Ritwik Banerjee | Colorado State University, USA; San Diego State University, USA; Stony Brook University, USA |
| 447 |  |  [Self-supervised Pre-training and Semi-supervised Learning for Extractive Dialog Summarization](https://doi.org/10.1145/3543873.3587680) |  | 0 | Language model pre-training has led to state-of-the-art performance in text summarization. While a variety of pre-trained transformer models are available nowadays, they are mostly trained on documents. In this study we introduce self-supervised pre-training to enhance the BERT model’s semantic and structural understanding of dialog texts from social media. We also propose a semi-supervised teacher-student learning framework to address the common issue of limited available labels in summarization datasets. We empirically evaluate our approach on extractive summarization task with the TWEETSUMM corpus, a recently introduced dialog summarization dataset from Twitter customer care conversations and demonstrate that our self-supervised pre-training and semi-supervised teacher-student learning are both beneficial in comparison to other pre-trained models. Additionally, we compare pre-training and teacher-student learning in various low data-resource settings, and find that pre-training outperforms teacher-student learning and the differences between the two are more significant when the available labels are scarce. | Yingying Zhuang, Jiecheng Song, Narayanan Sadagopan, Anurag Beniwal | Amazon, USA |
| 448 |  |  [Ready, Aim, Snipe! Analysis of Sniper Bots and their Impact on the DeFi Ecosystem](https://doi.org/10.1145/3543873.3587612) |  | 0 | In the world of cryptocurrencies, public listing of a new token often generates significant hype, in many cases causing its price to skyrocket in a few seconds. In this scenario, timing is crucial to determine the success or failure of an investment opportunity. In this work, we present an in-depth analysis of sniper bots, automated tools designed to buy tokens as soon as they are listed on the market. We leverage GitHub open-source repositories of sniper bots to analyze their features and how they are implemented. Then, we build a dataset of Ethereum and BNB Smart Chain (BSC) liquidity pools to identify addresses that serially take advantage of sniper bots. Our findings reveal 14,029 sniping operations on Ethereum and 1,395,042 in BSC that bought tokens for a total of $10,144,808 dollars and $18,720,447, respectively. We find that Ethereum operations have a higher success rate but require a larger investment. Finally, we analyze token smart contracts to identify mechanisms that can hinder sniper bots. | Federico Cernera, Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini, Francesco Sassi | Sapienza University of Rome, Italy |
| 449 |  |  [Regime-based Implied Stochastic Volatility Model for Crypto Option Pricing](https://doi.org/10.1145/3543873.3587621) |  | 0 | The increasing adoption of Digital Assets (DAs), such as Bitcoin (BTC), rises the need for accurate option pricing models. Yet, existing methodologies fail to cope with the volatile nature of the emerging DAs. Many models have been proposed to address the unorthodox market dynamics and frequent disruptions in the microstructure caused by the non-stationarity, and peculiar statistics, in DA markets. However, they are either prone to the curse of dimensionality, as additional complexity is required to employ traditional theories, or they overfit historical patterns that may never repeat. Instead, we leverage recent advances in market regime (MR) clustering with the Implied Stochastic Volatility Model (ISVM). Time-regime clustering is a temporal clustering method, that clusters the historic evolution of a market into different volatility periods accounting for non-stationarity. ISVM can incorporate investor expectations in each of the sentiment-driven periods by using implied volatility (IV) data. In this paper, we applied this integrated time-regime clustering and ISVM method (termed MR-ISVM) to high-frequency data on BTC options at the popular trading platform Deribit. We demonstrate that MR-ISVM contributes to overcome the burden of complex adaption to jumps in higher order characteristics of option pricing models. This allows us to price the market based on the expectations of its participants in an adaptive fashion. | Danial Saef, Yuanrong Wang, Tomaso Aste | Humboldt University Berlin, Germany; University College London, United Kingdom |
| 450 |  |  [NLP4KGC: Natural Language Processing for Knowledge Graph Construction](https://doi.org/10.1145/3543873.3589746) |  | 0 | No abstract available. | Edlira Vakaj, Sanju Tiwari, Nandana Mihindukulasooriya, Fernando OrtizRodríguez, Ryan McGranaghan | NASA Jet Propulsion Laboratory, USA; IBM Research, Ireland; Computing and Data Science/Natural Language Processing Lab, Birmingham City University, United Kingdom; Universidad Autónoma de Tamaulipas, Mexico |
| 451 |  |  [GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering](https://doi.org/10.1145/3543873.3587651) |  | 0 | Commonsense question-answering (QA) methods combine the power of pre-trained Language Models (LM) with the reasoning provided by Knowledge Graphs (KG). A typical approach collects nodes relevant to the QA pair from a KG to form a Working Graph (WG) followed by reasoning using Graph Neural Networks(GNNs). This faces two major challenges: (i) it is difficult to capture all the information from the QA in the WG, and (ii) the WG contains some irrelevant nodes from the KG. To address these, we propose GrapeQA with two simple improvements on the WG: (i) Prominent Entities for Graph Augmentation identifies relevant text chunks from the QA pair and augments the WG with corresponding latent representations from the LM, and (ii) Context-Aware Node Pruning removes nodes that are less relevant to the QA pair. We evaluate our results on OpenBookQA, CommonsenseQA and MedQA-USMLE and see that GrapeQA shows consistent improvements over its LM + KG predecessor (QA-GNN in particular) and large improvements on OpenBookQA. | Dhaval Taunk, Lakshya Khanna, Siri Venkata Pavan Kumar Kandru, Vasudeva Varma, Charu Sharma, Makarand Tapaswi | International Institute of Information Technology, India |
| 452 |  |  [Federated Learning for Metaverse: A Survey](https://doi.org/10.1145/3543873.3587584) |  | 0 | The metaverse, which is at the stage of innovation and exploration, faces the dilemma of data collection and the problem of private data leakage in the process of development. This can seriously hinder the widespread deployment of the metaverse. Fortunately, federated learning (FL) is a solution to the above problems. FL is a distributed machine learning paradigm with privacy-preserving features designed for a large number of edge devices. Federated learning for metaverse (FL4M) will be a powerful tool. Because FL allows edge devices to participate in training tasks locally using their own data, computational power, and model-building capabilities. Applying FL to the metaverse not only protects the data privacy of participants but also reduces the need for high computing power and high memory on servers. Until now, there have been many studies about FL and the metaverse, respectively. In this paper, we review some of the early advances of FL4M, which will be a research direction with unlimited development potential. We first introduce the concepts of metaverse and FL, respectively. Besides, we discuss the convergence of key metaverse technologies and FL in detail, such as big data, communication technology, the Internet of Things, edge computing, blockchain, and extended reality. Finally, we discuss some key challenges and promising directions of FL4M in detail. In summary, we hope that our up-to-date brief survey can help people better understand FL4M and build a fair, open, and secure metaverse. | Yao Chen, Shan Huang, Wensheng Gan, Gengsen Huang, Yongdong Wu | Jinan University, China |
| 453 |  |  [Understanding the Impact of Label Skewness and Optimization on Federated Learning for Text Classification](https://doi.org/10.1145/3543873.3587599) |  | 0 | Federated Learning (FL), also known as collaborative learning, is a distributed machine learning approach that collaboratively learns a shared prediction model without explicitly sharing private data. When dealing with sensitive data, privacy measures need to be carefully considered. Optimizers have a massive role in accelerating the learning process given the high dimensionality and non-convexity of the search space. The data partitioning in FL can be assumed to be either IID (independent and identically distributed) or non-IID. In this paper, we experiment with the impact of applying different adaptive optimization methods for FL frameworks in both IID and non-IID setups. We analyze the effects of label and quantity skewness, learning rate, and local client training on the learning process of optimizers as well as the overall performance of the global model. We evaluate the FL hyperparameter settings on biomedical text classification tasks on two datasets ADE V2 (Adverse Drug Effect: 2 classes) and Clinical-Trials (Reasons to stop trials: 17 classes). | Sumam Francis, Kanimozhi Uma, MarieFrancine Moens | LIIR lab, Department of Computer Science, KU Leuven, Belgium |
| 454 |  |  [A Survey of Trustworthy Federated Learning with Perspectives on Security, Robustness and Privacy](https://doi.org/10.1145/3543873.3587681) |  | 0 | Trustworthy artificial intelligence (AI) technology has revolutionized daily life and greatly benefited human society. Among various AI technologies, Federated Learning (FL) stands out as a promising solution for diverse real-world scenarios, ranging from risk evaluation systems in finance to cutting-edge technologies like drug discovery in life sciences. However, challenges around data isolation and privacy threaten the trustworthiness of FL systems. Adversarial attacks against data privacy, learning algorithm stability, and system confidentiality are particularly concerning in the context of distributed training in federated learning. Therefore, it is crucial to develop FL in a trustworthy manner, with a focus on security, robustness, and privacy. In this survey, we propose a comprehensive roadmap for developing trustworthy FL systems and summarize existing efforts from three key aspects: security, robustness, and privacy. We outline the threats that pose vulnerabilities to trustworthy federated learning across different stages of development, including data processing, model training, and deployment. To guide the selection of the most appropriate defense methods, we discuss specific technical solutions for realizing each aspect of Trustworthy FL (TFL). Our approach differs from previous work that primarily discusses TFL from a legal perspective or presents FL from a high-level, non-technical viewpoint. | Yifei Zhang, Dun Zeng, Jinglong Luo, Zenglin Xu, Irwin King | The Chinese University of Hong Kong, Hong Kong; University of Electronic Science and Technology of China and Peng Cheng Lab, China; Harbin Institute of Technology and Peng Cheng Lab, China |
| 455 |  |  [A Federated Learning Benchmark for Drug-Target Interaction](https://doi.org/10.1145/3543873.3587687) |  | 0 | Aggregating pharmaceutical data in the drug-target interaction (DTI) domain has the potential to deliver life-saving breakthroughs. It is, however, notoriously difficult due to regulatory constraints and commercial interests. This work proposes the application of federated learning, which we argue to be reconcilable with the industry's constraints, as it does not require sharing of any information that would reveal the entities' data or any other high-level summary of it. When used on a representative GraphDTA model and the KIBA dataset it achieves up to 15% improved performance relative to the best available non-privacy preserving alternative. Our extensive battery of experiments shows that, unlike in other domains, the non-IID data distribution in the DTI datasets does not deteriorate FL performance. Additionally, we identify a material trade-off between the benefits of adding new data, and the cost of adding more clients. | Gianluca Mittone, Filip Svoboda, Marco Aldinucci, Nicholas D. Lane, Pietro Lió | Department of Computer Science, University of Turin, Italy; University of Cambridge, United Kingdom |
| 456 |  |  [Towards Timeline Generation with Abstract Meaning Representation](https://doi.org/10.1145/3543873.3587670) |  | 0 | Timeline summarization (TLS) is a challenging research task that requires researchers to distill extensive and intricate temporal data into a concise and easily comprehensible representation. This paper proposes a novel approach to timeline summarization using Abstract Meaning Representations (AMRs), a graphical representation of the text where the nodes are semantic concepts and the edges denote relationships between concepts. With AMR, sentences with different wordings, but similar semantics, have similar representations. To make use of this feature for timeline summarization, a two-step sentence selection method that leverages features extracted from both AMRs and the text is proposed. First, AMRs are generated for each sentence. Sentences are then filtered out by removing those with no named-entities and keeping the ones with the highest number of named-entities. In the next step, sentences to appear in the timeline are selected based on two scores: Inverse Document Frequency (IDF) of AMR nodes combined with the score obtained by applying a keyword extraction method to the text. Our experimental results on the TLS-Covid19 test collection demonstrate the potential of the proposed approach. | Behrooz Mansouri, Ricardo Campos, Adam Jatowt | Department of Computer Science, University of Southern Maine, USA; University of Innsbruck, Austria; Ci2 - Polytechnic Institute of Tomar; INESC TEC, Portugal |
| 457 |  |  [Gone, Gone, but Not Really, and Gone, But Not forgotten: A Typology of Website Recoverability](https://doi.org/10.1145/3543873.3587671) |  | 0 | This paper presents a qualitative analysis of the recoverability of various webpages on the live web, using their archived counterparts as a baseline. We used a heterogeneous dataset consisting of four web archive collections, each with varying degrees of content drift. We were able to recover a small number of webpages previously thought to have been lost and analyzed their content and evolution. Our analysis yielded three types of lost webpages: 1) those that are not recoverable (with three subtypes), 2) those that are fully recoverable, and 3) those that are partially recoverable. The analysis presented here attempts to establish clear definitions and boundaries between the different degrees of webpage recoverabilty. By using a few simple methods, web archivists could discover the new locations of web content that was previously deemed lost, and include them in future crawling efforts, and lead to more complete web archives with less content drift. | Brenda Reyes Ayala | University of Alberta, Canada |
| 458 |  |  [Detecting the Hidden Dynamics of Networked Actors Using Temporal Correlations](https://doi.org/10.1145/3543873.3587672) |  | 0 | Influence campaigns pose a threat to fact-based reasoning, erode trust in institutions, and tear at the fabric of our society. In the 21st century, influence campaigns have rapidly evolved, taking on new online identities. Many of these propaganda campaigns are persistent and well-resourced, making their identification and removal both hard and expensive. Social media companies have predominantly aimed to counter the threat of online propaganda by prioritizing the moderation of "coordinated inauthentic behavior". This strategy focuses on identifying orchestrated campaigns explicitly intended to deceive, rather than individual social media accounts or posts. In this paper, we study the Twitter footprint of a multi-year influence campaign linked to the Russian government. Drawing from the influence model, a generative model that describes the interactions between networked Markov chains, we demonstrate how temporal correlations in the sequential decision processes of individual social media accounts can reveal coordinated inauthentic activity. | Keeley Erhardt, Dina Albassam | Massachusetts Institute of Technology, USA; King Abdulaziz City for Science and Technology, Saudi Arabia |
| 459 |  |  [The Age of Snippet Programming: Toward Understanding Developer Communities in Stack Overflow and Reddit](https://doi.org/10.1145/3543873.3587673) |  | 0 | Today, coding skills are among the most required competencies worldwide, often also for non-computer scientists. Because of this trend, community contribution-based, question-and-answer (Q&A) platforms became prominent for finding the proper solution to all programming issues. Stack Overflow has been the most popular platform for technical-related questions for years. Still, recently, some programming-related subreddits of Reddit have become a standing stone for questions and discussions. This work investigates the developers’ behavior and community formation around the twenty most popular programming languages. We examined two consecutive years of programming-related questions from Stack Overflow and Reddit, performing a longitudinal study on users’ posting activity and their high-order interaction patterns abstracted via hypergraphs. Our analysis highlighted crucial differences in how these Q&A platforms are utilized by their users. In line with previous literature, it emphasized the constant decline of Stack Overflow in favor of more community-friendly platforms, such as Reddit, which has been growing rapidly lately. | Alessia Antelmi, Gennaro Cordasco, Daniele De Vinco, Carmine Spagnuolo | Department of Computer Science, Università degli Studi di Salerno, Italy; Department of Psychology, Università della Campania, Italy |
| 460 |  |  [Temporal Ordinance Mining for Event-Driven Social Media Reaction Analytics](https://doi.org/10.1145/3543873.3587674) |  | 0 | As a growing number of policies are adopted to address the substantial rise in urbanization, there is a significant push for smart governance, endowing transparency in decision-making and enabling greater public involvement. The thriving concept of smart governance goes beyond just cities, ultimately aiming at a smart planet. Ordinances (local laws) affect our life with regard to health, business, etc. This is particularly notable during major events such as the recent pandemic, which may lead to rapid changes in ordinances, pertaining for instance to public safety, disaster management, and recovery phases. However, many citizens view ordinances as impervious and complex. This position paper proposes a research agenda enabling novel forms of ordinance content analysis over time and temporal web question answering (QA) for both legislators and the broader public. Along with this, we aim to analyze social media posts so as to track the public opinion before and after the introduction of ordinances. Challenges include addressing concepts changing over time and infusing subtle human reasoning in mining, which we aim to address by harnessing terminology evolution methods and commonsense knowledge sources, respectively. We aim to make the results of the historical ordinance mining and event-driven analysis seamlessly accessible, relying on a robust semantic understanding framework to flexibly support web QA. | Aparna S. Varde, Gerard de Melo, Boxiang Dong | Dept. of Computer Science; Clean Energy & Sustainability Analytics Center, Montclair State University, USA; AI & Intelligent Systems, HPI, University of Potsdam, Germany; Dept. of Computer Science, Montclair State University, USA |
| 461 |  |  [A Chinese Fine-grained Financial Event Extraction Dataset](https://doi.org/10.1145/3543873.3587578) |  | 0 | The existing datasets are mostly composed of official documents, statements, news articles, and so forth. So far, only a little attention has been paid to the numerals in financial social comments. Therefore, this paper presents CFinNumAttr, a financial numeral attribute dataset in Chinese via annotating the stock reviews and comments collected from social networking platform. We also conduct several experiments on the CFinNumAttr dataset with state-of-the-art methods to discover the importance of the financial numeral attributes. The experimental results on the CFinNumAttr dataset show that the numeral attributes in social reviews or comments contain rich semantic information, and the numeral clue extraction and attribute classification tasks can make a great improvement in financial text understanding. | Mengjie Wu, Maofu Liu, Luyao Wang, Huijun Hu | School of Computer Science and Technology, Hubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial System,Wuhan University of Science and Technology, China |
| 462 |  |  [Financial Technology on the Web](https://doi.org/10.1145/3543873.3589738) |  | 0 | This paper shares our observations based on our three-year experience organizing the FinWeb workshop series. In addition to the widely-discussed topic, content analysis, we notice two tendencies for FinTech applications: customers’ behavior analysis and finance-oriented LegalTech. We also briefly share our idea on the research direction about reliable and trustworthy FinWeb from the investment perspective. | ChungChi Chen, HenHsen Huang, Hiroya Takamura, HsinHsi Chen | National Taiwan University, Taiwan; National Institute of Advanced Industrial Science and Technology, Japan; Institute of Information Science, Academia Sinica, Taiwan |
| 463 |  |  [Aspect-based Summarization of Legal Case Files using Sentence Classification](https://doi.org/10.1145/3543873.3587611) |  | 0 | Aspect-based summarization of a legal case file related to regulating bodies allows different stakeholders to consume information of interest therein efficiently. In this paper, we propose a multi-step process to achieve the same. First, we explore the semantic sentence segmentation of SEBI case files via classification. We also propose a dataset of Indian legal adjudicating orders which contain tags from carefully crafted domain-specific sentence categories with the help of legal experts. We experiment with various machine learning and deep learning methods for this multi-class classification. Then, we examine the performance of numerous summarization methods on the segmented document to generate persona-specific summaries. Finally, we develop a pipeline making use of the best methods in both sub-tasks to achieve high recall. | Nikhil E, Anshul Padhi, Pulkit Parikh, Swati Kanwal, Kamalakar Karlapalem, Natraj Raman | JP Morgan AI Research, United Kingdom; IIIT Hyderabad, India; IIIT Hyderabad, Canada |
| 464 |  |  [Exploiting graph metrics to detect anomalies in cross-country money transfer temporal networks](https://doi.org/10.1145/3543873.3587602) |  | 0 | During the last decades, Anti-Financial Crime (AFC) entities and Financial Institutions have put a constantly increasing effort to reduce financial crime and detect fraudulent activities, that are changing and developing in extremely complex ways. We propose an anomaly detection approach based on network analysis to help AFC officers navigating through the high load of information that is typical of AFC data-driven scenarios. By experimenting on a large financial dataset of more than 80M cross-country wire transfers, we leverage on the properties of complex networks to develop a tool for explainable anomaly detection, that can help in identifying outliers that could be engaged in potentially malicious activities according to financial regulations. We identify a set of network metrics that provide useful insights on individual nodes; by keeping track of the evolution over time of the metric-based node rankings, we are able to highlight sudden and unexpected changes in the roles of individual nodes that deserve further attention by AFC officers. Such changes can hardly be noticed by means of current AFC practices, that sometimes can lack a higher-level, global vision of the system. This approach represents a preliminary step in the automation of AFC and AML processes, serving the purpose of facilitating the work of AFC officers by providing them with a top-down view of the picture emerging from financial data. | Salvatore Vilella, Arthur Thomas Edward Capozzi Lupi, Giancarlo Ruffo, Marco Fornasiero, Dario Moncalvo, Valeria Ricci, Silvia Ronchiadin | Anti Financial Crime Digital Hub, Italy; Department of Computer Science, University of Turin, Italy; Università degli Studi del Piemonte Orientale, Italy |
| 465 |  |  [Multiple-Agent Deep Reinforcement Learning for Avatar Migration in Vehicular Metaverses](https://doi.org/10.1145/3543873.3587573) |  | 0 | Vehicular Metaverses are widely considered as the next Internet revolution to build a 3D virtual world with immersive virtual-real interaction for passengers and drivers. In vehicular Metaverse applications, avatars are digital representations of on-board users to obtain and manage immersive vehicular services (i.e., avatar tasks) in Metaverses and the data they generate. However, traditional Internet of Vehicles (IoV) data management solutions have serious data security risks and privacy protection. Fortunately, blockchain-based Web 3.0 enables avatars to have an ownership identity to securely manage the data owned by users in a decentralized and transparent manner. To ensure users’ immersive experiences and securely manage their data, avatar tasks often require significant computing resources. Therefore, it is impractical for the vehicles to process avatar tasks locally, massive computation resources are needed to support the avatar tasks. To this end, offloading avatar tasks to nearby RoadSide Units (RSUs) is a promising solution to avoid computation overload. To ensure real-time and continuous Metaverse services, the avatar tasks should be migrated among the RSUs when the vehicle navigation. It is challenging for the vehicles to independently decide whether migrate or not according to current and future avatar states. Therefore, in this paper, we propose a new avatar task migration framework for vehicular Metaverses. We then formulate the avatar task migration problem as a Partially Observable Markov Decision Process (POMDP), and apply a Multi-Agent Deep Reinforcement Learning (MADRL) algorithm to dynamically make migration decisions for avatar tasks. Numerous results show that our proposed algorithm outperforms existing baselines for avatar task migration and enables immersive vehicular Metaverse services. | Junlong Chen, Jiangtian Nie, Minrui Xu, Lingjuan Lyu, Zehui Xiong, Jiawen Kang, Yongju Tong, Wenchao Jiang | Sony(Japan), Japan; Singapore University of Technology and Design, Singapore; Guangdong University of Technology, China; Nanyang Technological University, Singapore |
| 466 |  |  [China's First Natural Language-based AI ChatBot Trader](https://doi.org/10.1145/3543873.3587633) |  | 0 | Repo (repurchase agreement) trading provides easy access to short-term financing secured by a pledge of collateral and plays an important role in the global financial system. However, repo traders face many tough challenges in their job, from managing complex financial transactions to keeping up with changing market trends and regulations in the complex financial transactions involved. Besides the difficult and tedious processes that take a lot of time and energy, repo traders need to keep up to date with various laws, regulations, and financial trends that may affect their job, worsened by the exposure to a variety of market risks. As the leader of the FinTech industry, Ant Group launched a new initiative to alleviate the affliction of the repo traders at MyBank1. By leveraging many existing platform technologies, such as AI ChatBot and forecasting platforms, and with the collective work of various engineering groups, we are able to create a ChatBot that communicates with other human traders in natural language and create electronic contracts based on the negotiated terms, equipped with proper trading strategies based on forecasting results. The fully automatic workflow not only frees our trader from tedious routines, but also reduces potential human errors. At the same time, it enables refined portfolio and risk management, while opening up the possibility to apply neural network-based trading strategies, and yielding greater returns comparing to traditional workflow reliant on human experiences. Our system has evolved beyond just providing services to our own traders, to now a fully commercialized product, covering other types of interbank trading. | James Y. Zhang, Zhi Li, Hao Fang, Jun Wu, Zhongnan Shen, Jing Zheng, Wei Chu, Weiping Duan, Peng Xu | MyBank, China; Ant Group, USA |
| 467 |  |  [Web 3.0: The Future of Internet](https://doi.org/10.1145/3543873.3587583) |  | 0 | With the rapid growth of the Internet, human daily life has become deeply bound to the Internet. To take advantage of massive amounts of data and information on the internet, the Web architecture is continuously being reinvented and upgraded. From the static informative characteristics of Web 1.0 to the dynamic interactive features of Web 2.0, scholars and engineers have worked hard to make the internet world more open, inclusive, and equal. Indeed, the next generation of Web evolution (i.e., Web 3.0) is already coming and shaping our lives. Web 3.0 is a decentralized Web architecture that is more intelligent and safer than before. The risks and ruin posed by monopolists or criminals will be greatly reduced by a complete reconstruction of the Internet and IT infrastructure. In a word, Web 3.0 is capable of addressing web data ownership according to distributed technology. It will optimize the internet world from the perspectives of economy, culture, and technology. Then it promotes novel content production methods, organizational structures, and economic forms. However, Web 3.0 is not mature and is now being disputed. Herein, this paper presents a comprehensive survey of Web 3.0, with a focus on current technologies, challenges, opportunities, and outlook. This article first introduces a brief overview of the history of World Wide Web as well as several differences among Web 1.0, Web 2.0, Web 3.0, and Web3. Then, some technical implementations of Web 3.0 are illustrated in detail. We discuss the revolution and benefits that Web 3.0 brings. Finally, we explore several challenges and issues in this promising area. | Wensheng Gan, Zhenqiang Ye, Shicheng Wan, Philip S. Yu | Jinan University, China; University of Illinois at Chicago, USA; Guangdong University of Technology, China |
| 468 |  |  [DSNet: Efficient Lightweight Model for Video Salient Object Detection for IoT and WoT Applications](https://doi.org/10.1145/3543873.3587592) |  | 0 | The most challenging aspects of deploying deep models in IoT and embedded systems are extensive computational complexity and large training and inference time. Although various lightweight versions of state-of-the-art models are also being designed, maintaining the performance of such models is difficult. To overcome these problems, an efficient, lightweight, Deformable Separable Network (DSNet) is proposed for video salient object detection tasks, mainly for mobile and embedded vision applications. DSNet is equipped with a Deformable Convolution Network (DeCNet), Separable Convolution Network (SCNet), and Depth-wise Attention Response Propagation (DARP) module, which makes it maintain the trade-off between accuracy and latency. The proposed model generates saliency maps considering both the background and foreground simultaneously, making it perform better in unconstrained scenarios (such as partial occlusion, deformable background/objects, and illumination effect). The extensive experiments conducted on six benchmark datasets demonstrate that the proposed model outperforms state-of-art approaches in terms of computational complexity, number of parameters, and latency measures. | Hemraj Singh, Mridula Verma, Ramalingaswamy Cheruku | National Institute of Technology Warangal, India; Institute for Development and Research in Banking Technology, India |
| 469 |  |  [Weighted Statistically Significant Pattern Mining](https://doi.org/10.1145/3543873.3587586) |  | 0 | Pattern discovery (aka pattern mining) is a fundamental task in the field of data science. Statistically significant pattern mining (SSPM) is the task of finding useful patterns that statistically occur more often from databases for one class than for another. The existing SSPM task does not consider the weight of each item. While in the real world, the significant level of different items/objects is various. Therefore, in this paper, we introduce the Weighted Statistically Significant Patterns Mining (WSSPM) problem and propose a novel WSSpm algorithm to successfully solve it. We present a new framework that effectively mines weighted statistically significant patterns by combining the weighted upper-bound model and the multiple hypotheses test. We also propose a new weighted support threshold that can satisfy the demand of WSSPM and prove its correctness and completeness. Besides, our weighted support threshold and modified weighted upper-bound can effectively shrink the mining range. Finally, experimental results on several real datasets show that the WSSpm algorithm performs well in terms of execution time and memory storage. | Tingfu Zhou, Zhenlian Qi, Wensheng Gan, Shicheng Wan, Guoting Chen | Jinan University, China; Guangdong University of Technology, China; Harbin Institute of Technology, China; Guangdong Eco-Engineering Polytechnic, China |
| 470 |  |  [The Human-Centric Metaverse: A Survey](https://doi.org/10.1145/3543873.3587593) |  | 0 | In the era of the Web of Things, the Metaverse is expected to be the landing site for the next generation of the Internet, resulting in the increased popularity of related technologies and applications in recent years and gradually becoming the focus of Internet research. The Metaverse, as a link between the real and virtual worlds, can provide users with immersive experiences. As the concept of the Metaverse grows in popularity, many scholars and developers begin to focus on the Metaverse's ethics and core. This paper argues that the Metaverse should be centered on humans. That is, humans constitute the majority of the Metaverse. As a result, we begin this paper by introducing the Metaverse's origins, characteristics, related technologies, and the concept of the human-centric Metaverse (HCM). Second, we discuss the manifestation of human-centric in the Metaverse. Finally, we discuss some current issues in the construction of HCM. In this paper, we provide a detailed review of the applications of human-centric technologies in the Metaverse, as well as the relevant HCM application scenarios. We hope that this paper can provide researchers and developers with some directions and ideas for human-centric Metaverse construction. | Riyan Yang, Lin Li, Wensheng Gan, Zefeng Chen, Zhenlian Qi | Jinan University, China; Guangdong Eco-Engineering Polytechnic, China |
| 471 |  |  [Can Deepfakes be created on a whim?](https://doi.org/10.1145/3543873.3587581) |  | 0 | Recent advancements in machine learning and computer vision have led to the proliferation of Deepfakes. As technology democratizes over time, there is an increasing fear that novice users can create Deepfakes, to discredit others and undermine public discourse. In this paper, we conduct user studies to understand whether participants with advanced computer skills and varying level of computer science expertise can create Deepfakes of a person saying a target statement using limited media files. We conduct two studies; in the first study (n = 39) participants try creating a target Deepfake in a constrained time frame using any tool they desire. In the second study (n = 29) participants use pre-specified deep learning based tools to create the same Deepfake. We find that for the first study, of the participants successfully created complete Deepfakes with audio and video, whereas for the second user study, of the participants were successful in stitching target speech to the target video. We further use Deepfake detection software tools as well as human examiner-based analysis, to classify the successfully generated Deepfake outputs as fake, suspicious, or real. The software detector classified of the Deepfakes as fake, whereas the human examiners classified of the videos as fake. We conclude that creating Deepfakes is a simple enough task for a novice user given adequate tools and time; however, the resulting Deepfakes are not sufficiently real-looking and are unable to completely fool detection software as well as human examiners. | Pulak Mehta, Gauri Jagatap, Kevin Gallagher, Brian Timmerman, Progga Deb, Siddharth Garg, Rachel Greenstadt, Brendan DolanGavitt | New York University, USA; NOVA LINCS & Universidade NOVA de Lisboa, Portugal |
| 472 |  |  [On Cohesively Polarized Communities in Signed Networks](https://doi.org/10.1145/3543873.3587698) |  | 0 | Locating and characterizing polarization is one of the most important issues to enable a healthier web ecosystem. Finding groups of nodes that form strongly stable agreements and participate in collective conflicts with other groups is an important problem in this context. Previous works approach this problem by finding balanced subgraphs, in which the polarity measure is optimized, that result in large subgraphs without a clear notion of agreement or conflict. In real-world signed networks, balanced subgraphs are often not polarized as in the case of a subgraph with only positive edges. To remedy this issue, we leverage the notion of cohesion — we find pairs of cohesively polarized communities where each node in a community is positively connected to nodes in the same community and negatively connected to nodes in the other community. To capture the cohesion along with the polarization, we define a new measure, dichotomy. We leverage the balanced triangles, which model the cohesion and polarization at the same time, to design a heuristic that results in good seedbeds for polarized communities in real-world signed networks. Then, we introduce the electron decomposition which finds cohesively polarized communities with high dichotomy score. In an extensive experimental evaluation, we show that our method finds cohesively polarized communities and outperforms the state-of-the-art methods with respect to several measures. Moreover, our algorithm is more efficient than the existing methods and practical for large-scale networks. | Jason Niu, Ahmet Erdem Sariyüce | University at Buffalo, USA |
| 473 |  |  [Towards Automated Detection of Risky Images Shared by Youth on Social Media](https://doi.org/10.1145/3543873.3587607) |  | 0 | With the growing ubiquity of the Internet and access to media-based social media platforms, the risks associated with media content sharing on social media and the need for safety measures against such risks have grown paramount. At the same time, risk is highly contextualized, especially when it comes to media content youth share privately on social media. In this work, we conducted qualitative content analyses on risky media content flagged by youth participants and research assistants of similar ages to explore contextual dimensions of youth online risks. The contextual risk dimensions were then used to inform semi- and self-supervised state-of-the-art vision transformers to automate the process of identifying risky images shared by youth. We found that vision transformers are capable of learning complex image features for use in automated risk detection and classification. The results of our study serve as a foundation for designing contextualized and youth-centered machine-learning methods for automated online risk detection. | Jinkyung Park, Joshua Gracie, Ashwaq Alsoubai, Gianluca Stringhini, Vivek K. Singh, Pamela J. Wisniewski | University of Central Florida, USA; Rutgers University, USA; Vanderbilt University, USA; Boston University, USA |
| 474 |  |  [Detecting Social Media Manipulation in Low-Resource Languages](https://doi.org/10.1145/3543873.3587615) |  | 0 | Social media have been deliberately used for malicious purposes, including political manipulation and disinformation. Most research focuses on high-resource languages. However, malicious actors share content across countries and languages, including low-resource ones. Here, we investigate whether and to what extent malicious actors can be detected in low-resource language settings. We discovered that a high number of accounts posting in Tagalog were suspended as part of Twitter's crackdown on interference operations after the 2016 US Presidential election. By combining text embedding and transfer learning, our framework can detect, with promising accuracy, malicious users posting in Tagalog without any prior knowledge or training on malicious content in that language. We first learn an embedding model for each language, namely a high-resource language (English) and a low-resource one (Tagalog), independently. Then, we learn a mapping between the two latent spaces to transfer the detection model. We demonstrate that the proposed approach significantly outperforms state-of-the-art models, including BERT, and yields marked advantages in settings with very limited training data -- the norm when dealing with detecting malicious activity in online platforms. | Samar Haider, Luca Luceri, Ashok Deb, Adam Badawy, Nanyun Peng, Emilio Ferrara | University of Southern California, USA |
| 475 |  |  [Text Mining-based Social-Psychological Vulnerability Analysis of Potential Victims To Cybergrooming: Insights and Lessons Learned](https://doi.org/10.1145/3543873.3587636) |  | 0 | Cybergrooming is a serious cybercrime that primarily targets youths through online platforms. Although reactive predator detection methods have been studied, proactive victim protection and crime prevention can also be achieved through vulnerability analysis of potential youth victims. Despite its significance, vulnerability analysis has not been thoroughly studied in the data science literature, while several social science studies used survey-based methods. To address this gap, we investigate humans’ social-psychological traits and quantify key vulnerability factors to cybergrooming by analyzing text features in the Linguistic Inquiry and Word Count (LIWC). Through pairwise correlation studies, we demonstrate the degrees of key vulnerability dimensions to cybergrooming from youths’ conversational features. Our findings reveal that victims have negative correlations with family and community traits, contrasting with previous social survey studies that indicated family relationships or social support as key vulnerability factors. We discuss the current limitations of text mining analysis and suggest cross-validation methods to increase the validity of research findings. Overall, this study provides valuable insights into understanding the vulnerability factors to cybergrooming and highlights the importance of adopting multidisciplinary approaches. | Zhen Guo, Pei Wang, JinHee Cho, Lifu Huang | Computer Science, Virginia Tech, USA; Microsoft, USA |
| 476 |  |  [Evaluating the Emergence of Collective Identity using Socio-Computational Techniques](https://doi.org/10.1145/3543873.3587637) |  | 0 | Social media platforms provide fertile ground for investigating the processes of identity creation and communication that shape individual and public opinion. The computational methods used in social network analysis have opened the way for new approaches to be used to understand the psychological and social processes that occur when users take part in online social movements or digital activism. The research in this paper takes an interdisciplinary approach bridging social identity and deindividuation theories to show how shared, individual social identities merge into a collective identity using computational techniques. We demonstrate a novel approach to evaluating the emergence of collective identity by measuring: 1) the statistical similarity of discussion topics within online communities and 2) the strength of these communities by examining network modularity and assortative properties of the network. To accomplish this, we examined the online connective action campaign of the #stopthesteal movement that emerged during the 2020 U.S. Presidential Election. Our dataset consisted of 838,395 tweets posted by 178,296 users collected from January 04, 2020, to January 31, 2021. The results show that the network becomes more cohesive and topic similarity increases within communities leading up to and just after the elections (event 1) and the U.S. Capitol riot (event 2). Taking this multi-method approach of measuring content and network structure over time helps researchers and social scientists understand the emergence of a collective community as it is being constructed. The use of computational methods to study collective identity formation can help researchers identify the behaviors and social dynamics emerging from this type of cyber-collective movement that often serve as catalysts for these types of events. Finally, this research offers a new way to assess the psycho-social drivers of participant behaviors in cyber collective action. | Billy Spann, Nitin Agarwal, David Stafford, Obianuju Okeke | Collaboratorium for Social Media and Online Behavioral Studies (COSMOS) - University of Arkansas at Little Rock, USA |
| 477 |  |  [Trusting Decentralised Knowledge Graphs and Web Data at the Web Conference](https://doi.org/10.1145/3543873.3589756) |  | 0 | Knowledge Graphs have become a foundation for sharing data on the web and building intelligent services across many sectors and also within some of the most successful corporations in the world. The over centralisation of data on the web, however, has been raised as a concern by a number of prominent researchers in the field. For example, at the beginning of 2022 a €2.7B civil lawsuit was launched against Meta on the basis that it has abused its market dominance to impose unfair terms and conditions on UK users in order to exploit their personal data. Data centralisation can lead to a number of problems including: lock-in/siloing effects, lack of user control over their personal data, limited incentives and opportunities for interoperability and openness, and the resulting detrimental effects on privacy and innovation. A number of diverse approaches and technologies exist for decentralising data, such as federated querying and distributed ledgers. The main question is, though, what does decentralisation really mean for web data and Knowledge Graphs? What are the main issues and tradeoffs involved? These questions and others are addressed in this workshop. | John Domingue, Aisling Third, MariaEsther Vidal, Philipp D. Rohde, Juan Cano, Andrea Cimmino, Ruben Verborgh | Ghent University, Belgium; Leibniz University Hannover and TIB Leibniz Information Centre for Science and Technology, Germany; The Open University, United Kingdom; Universidad Politécnica de Madrid, Spain |
| 478 |  |  [A Decentralised Persistent Identification Layer for DCAT Datasets](https://doi.org/10.1145/3543873.3587589) |  | 0 | The Data Catalogue Vocabulary (DCAT) standard is a popular RDF vocabulary for publishing metadata about data catalogs and a valuable foundation for creating Knowledge Graphs. It has widespread application in the (Linked) Open Data and scientific communities. However, DCAT does not specify a robust mechanism to create and maintain persistent identifiers for the datasets. It relies on Internationalized Resource Identifiers (IRIs), that are not necessarily unique, resolvable and persistent. This impedes findability, citation abilities, and traceability of derived and aggregated data artifacts. As a remedy, we propose a decentralized identifier registry where persistent identifiers are managed by a set of collaborative distributed nodes. Every node gives full access to all identifiers, since an unambiguous state is shared across all nodes. This facilitates a common view on the identifiers without the need for a (virtually) centralized directory. To support this architecture, we propose a data model and network methodology based on a distributed ledger and the W3C recommendation for Decentralized Identifiers (DID). We implemented our approach as a working prototype on a five-peer test network based on Hyperledger Fabric. | Fabian Kirstein, Anton Altenbernd, Sonja Schimmler, Manfred Hauswirth | Fraunhofer FOKUS, Germany; Fraunhofer FOKUS, Germany and Weizenbaum Institute, Germany; TU Berlin, Open Distributed Systems, Germany and Weizenbaum Institute, Germany |
| 479 |  |  [Practical challenges of ODRL and potential courses of action](https://doi.org/10.1145/3543873.3587628) |  | 0 | The Open Digital Rights Language (ODRL) is a standard widely adopted to express privacy policies. This article presents several challenges identified in the context of the European project AURORAL in which ODRL is used to express privacy policies for Smart Communities and Rural Areas. The article presents that some challenges should be addressed directly by the ODRL standardisation group to achieve the best course of action, although others exists. For others, the authors have presented a potential solution, in particular, for considering dynamic values coming from external data sources into privacy policies. Finally, the last challenge is an open research question, since it revolves around the interoperability of privacy policies that belong to different systems and that are expressed with different privacy languages. | Andrea Cimmino, Juan CanoBenito, Raúl GarcíaCastro | UPM, Spain; Universidad Politécnica de Madrid, Spain |
| 480 |  |  [The Web and Linked Data as a Solid Foundation for Dataspaces](https://doi.org/10.1145/3543873.3587616) |  | 0 | The concepts for dataspaces range from database management systems to cross-company platforms for data and applications. In this short paper, we present the “Solid Data Space” (SDS), a concept for dataspaces that build on top of the (Semantic) Web and Social Linked Data (Solid). Existing Web technologies and Linked Data principles form the foundation for open, decentralized networks for sovereign data exchange between citizens, organizations and companies. Domain-specific dataspace implementations can extend the agreements for communication and collaboration to enable specific functionality. We compare the SDS with principles and components of the emerging International Data Spaces to identify similarities and point out technological differences. | Sascha Meckler, Rene Dorsch, Daniel Henselmann, Andreas Harth | Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Germany |
| 481 |  |  [Extending Actor Models in Data Spaces](https://doi.org/10.1145/3543873.3587645) |  | 0 | In today’s internet almost any party can share sets of data with each other. However, creating frameworks and regulated realms for the sharing of data is very complex when multiple parties are involved and complicated regulation comes into play. As solution data spaces were introduced to enable participating parties to share data among themselves in an organized, regulated and standardized way. However, contract data processors, acting as data space participants, are currently unable to execute data requests on behalf of their contract partners. Here we show that an on-behalf-of actor model can be easily added to existing data spaces. We demonstrate how this extension can be realized using verifiable credentials. We provide a sample use case, a detailed sequence diagram and discuss necessary architectural adaptations and additions to established protocols. Using the extensions explained in this work numerous real life use cases which previously could technically not be realized can now be covered. This enables future data spaces to provide more dynamic and complex real world use cases. | Hendrik Meyer zum Felde, Maarten Kollenstart, Thomas Bellebaum, Simon Dalmolen, Gerd Brost | Fraunhofer AISEC, Germany; TNO, Netherlands |
| 482 |  |  [Towards Decentralised Learning Analytics (Positioning Paper)](https://doi.org/10.1145/3543873.3587644) |  | 0 | When students interact with an online course, the routes they take when navigating through the course can be captured. Learning Analytics is the process of measuring, collecting, recording, and analysing this Student Activity Data. Predictive Learning Analytics, a sub-field of Learning Analytics, can help to identify students who are at risk of dropping out or failing, as well as students who are close to a grade boundary. Course tutors can use the insights provided by the analyses to offer timely assistance to these students. Despite its usefulness, there are privacy and ethical issues with the typically centralised approach to Predictive Learning Analytics. In this positioning paper, it is proposed that the issues associated with Predictive Learning Analytics can be alleviated, in a framework called EMPRESS, by combining 1) self-sovereign data, where data owners control who legitimately has access to data pertaining to them, 2) Federated Learning, where the data remains on the data owner’s device and/or the data is processed by the data owners themselves, and 3) Graph Convolutional Networks for Heterogeneous graphs, which are examples of knowledge graphs. | Audrey Ekuban, John Domingue | The Open University, United Kingdom |
| 483 |  |  [Analyzing Distributed Medical Data in FAIR Data Spaces](https://doi.org/10.1145/3543873.3587663) |  | 0 | The exponential growth in data production has led to increasing demand for high-quality data-driven services. Additionally, the benefits of data-driven analysis are vast and have significantly propelled research in many fields. Data sharing benefits scientific advancement, as it promotes transparency, and collaboration, accelerates research and aids in making informed decisions. The European strategy for data aims to create a single data market that ensures Europe’s global competitiveness and data sovereignty. Common European Data Spaces ensure that data from different sources are available in the economy and society, while data providers (e.g., hospitals and scientists) control data access. The National Research Data Infrastructure for Personal Health Data (NFDI4Health) initiative is a prime example of an effort focused on data from clinical trials and public health studies. Collecting and analyzing this data is essential to developing novel therapies, comprehensive care approaches, and preventive measures in modern healthcare systems. This work describes distributed data analysis services and components that adhere to the FAIR data principles (Findable, Accessible, Interoperable, and Reusable) within the data space environment. We focus on distributed analytics functionality in Gaia-X-based data spaces. Gaia-X offers a trustworthy federation of data infrastructure and service providers for European countries. | Mehrshad Jaberansary, Macedo Maia, Yeliz Ucer Yediel, Oya Beyan, Toralf Kirsten | Institute for Medical Informatics, Statistics and Epidemiology (IMISE), Leipzig University, Germany; Institute for Biomedical Informatics, Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany and Fraunhofer Institute for Applied Information Techniques (FIT), Germany; Fraunhofer Institute for Applied Information Technology (FIT), Germany and Information Systems and Database Technology Research Group, RWTH Aachen University, 52062 Aachen, Germany, Germany; Institute for Biomedical Informatics, Faculty of Medicine and University Hospital Cologne, University of Cologne, Germany |
| 484 |  |  [Requirements and Building Blocks for Manufacturing Dataspaces](https://doi.org/10.1145/3543873.3587664) |  | 0 | With the advent and pervasiveness of the Internet of Things (IoT), big data and cloud computing technologies, digitalization in enterprises and factories has rapidly increased in the last few years. Digital platforms have emerged as an effective mechanism for enabling the management and sharing of data from various companies. To enable sharing of data beyond the platform boundaries, definition of new platform federation approaches is on the rise. This makes enabling the federation of digital platforms a key requirement for large-scale dataspaces. Therefore, the identification of platform federation requirements and building blocks for such dataspaces needs to be systematically addressed. In this paper, we try to systematically explore the high-level requirements for enabling a federation of digital platforms in the manufacturing domain and identify a set of building blocks. We integrate the requirements and building blocks into the notion of dataspaces. The identified requirements and building blocks act as a blueprint for designing and instantiating new dataspaces, thereby speeding up the development process and reducing costs. We present a case study to illustrate how the use of common building blocks can act as common guiding principles and result in more complete and interoperable implementations. | Rohit A. Deshmukh, Sisay Adugna Chala, Christoph Lange | Fraunhofer Institute for Applied Information Technology FIT, 53754 Sankt Augustin, Germany and Chair of Databases and Information Systems (i5), RWTH Aachen University, 52074 Aachen, Germany; Fraunhofer Institute for Applied Information Technology FIT, 53754 Sankt Augustin, Germany |
| 485 |  |  [Towards Multimodal Knowledge Graphs for Data Spaces](https://doi.org/10.1145/3543873.3587665) |  | 0 | Multimodal knowledge graphs have the potential to enhance data spaces by providing a unified and semantically grounded structured representation of multimodal data produced by multiple sources. With the ability to integrate and analyze data in real-time, multimodal knowledge graphs offer a wealth of insights for smart city applications, such as monitoring traffic flow, air quality, public safety, and identifying potential hazards. Knowledge enrichment can enable a more comprehensive representation of multimodal data and intuitive decision-making with improved expressiveness and generalizability. However, challenges remain in effectively modelling the complex relationships between and within different types of modalities in data spaces and infusing common sense knowledge from external sources. This paper reviews the related literature and identifies major challenges and key requirements for effectively developing multimodal knowledge graphs for data spaces, and proposes an ontology for their construction. | Atiya Usmani, Muhammad Jaleed Khan, John G. Breslin, Edward Curry | Insight SFI Research Centre for Data Analytics, Data Science Institute, University of Galway, Ireland; SFI Centre for Research Training in Artificial Intelligence, Data Science Institute, University of Galway, Ireland |
| 486 |  |  [SPACE_DS: Towards a Circular Economy Data Space](https://doi.org/10.1145/3543873.3587685) |  | 0 | The circular economy (CE) is essential to achieving a sustainable future through resource conservation and climate protection. Efficient use of materials and products over time is a critical aspect of CE, helping to reduce CO2 emissions, waste and resource consumption. The Digital Product Passport (DPP) is a CE-specific approach that contains information about components and their origin, and can also provide environmental and social impact assessments. However, creating a DPP requires collecting and analyzing data from many different stakeholders along the supply chain and even throughout the product lifecycle. In this paper, we present a concept for the SPACE_DS, which is a data space for circular economy data. A key point here is that the SPACE_DS enables the creation of DPPs by especially considering privacy and security concerns of data providers. | André Pomp, Maike Jansen, Holger Berg, Tobias Meisen | Wuppertal Institute for Climate, Environment and Energy, Germany; Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany |
| 487 |  |  [Towards a Data Space for Interoperability of Analytic Provenance](https://doi.org/10.1145/3543873.3587686) |  | 0 | Capturing, visualizing and analyzing provenance data to better understand and support analytic reasoning processes is a rapidly growing research field named analytic provenance. Provenance data includes the state of a visualization within a tool as well as the user’s interactions performed while interacting with the tool. Research in this field has produced in many new approaches that generate data for specific tools and use cases. However, since a variety of tools are used and analytic tasks are performed in real analysis use cases there is a problem in building an interoperable baseline data corpus for investigation of the transferability of different approaches. In this paper, we present a visionary data space architecture for integrating and processing analytic provenance data in a unified way using semantic modeling. We discuss emerging challenges and research opportunities to realize such a vision using semantic models in data spaces to enable analytic provenance data interoperability. | Tristan Langer, André Pomp, Tobias Meisen | Institute for Technologies and Management of Digital Transformation, University of Wuppertal, Germany |
| 488 |  |  [Provenance for Lattice QCD workflows](https://doi.org/10.1145/3543873.3587559) |  | 0 | We present a provenance model for the generic workflow of numerical Lattice Quantum Chromodynamics (QCD) calculations, which constitute an important component of particle physics research. These calculations are carried out on the largest supercomputers worldwide with data in the multi-PetaByte range being generated and analyzed. In the Lattice QCD community, a custom metadata standard (QCDml) that includes certain provenance information already exists for one part of the workflow, the so-called generation of configurations. In this paper, we follow the W3C PROV standard and formulate a provenance model that includes both the generation part and the so-called measurement part of the Lattice QCD workflow. We demonstrate the applicability of this model and show how the model can be used to answer some provenance-related research questions. However, many important provenance questions in the Lattice QCD community require extensions of this provenance model. To this end, we propose a multi-layered provenance approach that combines prospective and retrospective elements. | Tanja Auge, Gunnar Bali, Meike Klettke, Bertram Ludäscher, Wolfgang Söldner, Simon Weishäupl, Tilo Wettig | Faculty of Computer Science and Data Science, University of Regensburg, Germany; Department of Physics, University of Regensburg, Germany; School of Information Sciences, University of Illinois at Urbana-Champaign, USA |
| 489 |  |  [Implementing an Environmental Management System Using Provenance-By-Design](https://doi.org/10.1145/3543873.3587560) |  | 0 | Organisations have to comply with environmental regulations to protect the environment and meet internationally agreed climate change targets. To assist organisations, processes and standards are being defined to manage these compliance obligations. They typically rely on a notion of Environmental Management System (EMS), defined as a reflective framework allowing organisations to set and manage their goals, and demonstrate they follow due processes in order to comply with prevailing regulations. The importance of these obligations can be highlighted by the fact that failing to comply may lead to significant liabilities for organisations. An EMS framework, typically structured as a set of documents and spreadsheets, contains a record of continuously evolving regulations, teams, stakeholders, actions and updates. However, the maintainance of an EMS is often human driven, and therefore is error prone despite the meticulousness of environmental officers, and further requires external human auditing to check their validity. To avoid green washing, but also to contain the burden and cost of compliance, it is desirable for these claims to be checked by trusted automated means. Provenance is ideally suited to track the changes occurring in an EMS, allowing queries to determine precisely which compliance objective is prevailing at any point in time, whether it is being met, and who is responsible for it. Thus, this paper has a dual aim: first, it investigates the benefits of provenance for EMS, second, it presents the application of an emerging approach “Provenance-By-Design”, which automatically converts a specification of an EMS data model and its provenance to a data backend, a service for processing and querying of EMS provenance data, a client-side library to interact with such a service, and a simple user interface allowing developers to navigate the provenance. The application of a Provenance-By-Design approach to EMS applications results in novel opportunities for a provenance-based EMS; we present our preliminary reflection on their potential. | Luc Moreau, Nicola Hogan, Nick O'Donnell | Department of Informatics, King's College London, United Kingdom; Estates and Facilities, King's College London, United Kingdom |
| 490 |  |  [Trust the Process: Analyzing Prospective Provenance for Data Cleaning](https://doi.org/10.1145/3543873.3587558) |  | 0 | In the field of data-driven research and analysis, the quality of results largely depends on the quality of the data used. Data cleaning is a crucial step in improving the quality of data. Still, it is equally important to document the steps made during the data cleaning process to ensure transparency and enable others to assess the quality of the resulting data. While provenance models such as W3C PROV have been introduced to track changes and events related to any entity, their use in documenting the provenance of data-cleaning workflows can be challenging, particularly when mixing different types of documents or entities in the model. To address this, we propose a conceptual model and analysis that breaks down data-cleaning workflows into process abstraction and workflow recipes, refining operations to the column level. This approach provides users with detailed provenance information, enabling transparency, auditing, and support for data cleaning workflow improvements. Our model has several features that allow static analysis, e.g., to determine the minimal input schema and expected output schema for running a recipe, to identify which steps violate the column schema requirement constraint, and to assess the reusability of a recipe on a new dataset. We hope that our model and analysis will contribute to making data processing more transparent, accessible, and reusable. | Nikolaus Nova Parulian, Bertram Ludäscher | School of Information Sciences, University of Illinois at Urbana Champaign, USA |
| 491 |  |  [Deep Learning Provenance Data Integration: a Practical Approach](https://doi.org/10.1145/3543873.3587561) |  | 0 | A Deep Learning (DL) life cycle involves several data transformations, such as performing data pre-processing, defining datasets to train and test a deep neural network (DNN), and training and evaluating the DL model. Choosing a final model requires DL model selection, which involves analyzing data from several training configurations (e.g. hyperparameters and DNN architectures). Tracing training data back to pre-processing operations can provide insights into the model selection step. Provenance is a natural solution to represent data derivation of the whole DL life cycle. However, there are challenges in providing an integration of the provenance of these different steps. There are a few approaches to capturing and integrating provenance data from the DL life cycle, but they require that the same provenance capture solution is used along all the steps, which can limit interoperability and flexibility when choosing the DL environment. Therefore, in this work, we present a prototype for provenance data integration using different capture solutions. We show use cases where the integrated provenance from pre-processing and training steps can show how data pre-processing decisions influenced the model selection. Experiments were performed using real-world datasets to train a DNN and provided evidence of the integration between the considered steps, answering queries such as how the data used to train a model that achieved a specific result was processed. | Débora B. Pina, Adriane Chapman, Daniel de Oliveira, Marta Mattoso | University of Southampton, United Kingdom; Federal University of Rio de Janeiro (UFRJ), Brazil; Fluminense Federal University (UFF), Brazil |
| 492 |  |  [Providing Data on Financial Results of Public Companies Enriched with Provenance for OBInvest](https://doi.org/10.1145/3543873.3587566) |  | 0 | Financial Literacy (FL) initiatives, aimed at young people in formal or informal learning spaces, are defended and implemented in several countries, being encouraged since 2005 by the Organization for Economic Co-operation and Development (OECD). In Brazil, the teaching and learning process in several areas has been stimulated through Academic Competitions generally called Knowledge Olympics, which are essentially student contests that aim to encourage, find talent and awaken interest in the field knowledge presented in the competition. It was precisely for this purpose that the Brazilian Investment Olympics (OBInvest) was born, aiming to democratize access to education and promote reflections on economic and financial issues, through a FL perspective for high school students from all over the country. One of OBInvest’s objectives is to help boosting the development of computational tools, aiming to provide easier access to fundamental data for decision-making in the field of finance. However, from the tools developed by OBInvest, it was noted that the creation of new educational tools would be enhanced through the use of datasets enriched with provenance and aligned with FAIR principles. This work aims to offer a computational strategy based on data science techniques, which is easy to use and also provides curated data series through a reproducible pipeline, using open data on financial reports from publicly listed Brazilian companies, provided by the Brazilian Security and Exchange Commission, called Comissão de Valores Mobiliarios (CMV). During the exploration of related works, we found just a few academic works that use CVM data with little expressive results, which motivated the development of a tool called DRE-CVM, that was supported by computational tools, with a focus on the Python language, Pandas library, the KNIME workflow platform, and Jupyter integrated development environments, running on the Anaconda3 platform over a Docker container. It’s also possible run this experiment in the Google Colabotory cloud environment. This processing it’s capable of executing reproducible pipelines and using curated, fairified, and annotated data with the retrospective source metadata of the financial statements of publicly traded Brazilian companies. The artifact uses pipelines that can be reused by students and other interested parties in finance to study the behaviors of a company’s time series results and thus introduce research on predicting future results. The last executable version of the DRE-CVM experiment can be accessed through Zenodo website at https://doi.org/10.5281/zenodo.7110653 and can be reproduced using a Docker Container available on DockerHub repository. Some improvements can be incorporated into the presented work, the main suggestions for future work are: (i) Perform more substantial analyses on the created dataset, such as predicting results based on the history of demonstration results; (ii) Recover other types of information made available by CVM, to be used during the activities of the Brazilian Investment Olympics; (iii) Adapt the docker image so that it can be executed in the My Binder cloud environment, aiming to improve reproducibility issues. | Saulo Almeida, Gilberto Passos, Valquire Jesus, Sérgio Manuel Serra da Cruz, Jorge Zavaleta | UFRJ, Brazil |
| 493 |  |  [Using diversity as a source of scientific innovation for the Web](https://doi.org/10.1145/3543507.3593046) |  | 0 | The Web has become a resource that allows us to make sense of social phenomena around the world. This started the moment users became content creators, and has grown with the emergence of social platforms tailored to our need to connect and share with others. Throughout my work, I’ve come to appreciate how social media has democratized access to real-world news and social sentiment, while also witnessing the loss of trust created by fake information. As a computer scientist from Chile in Latin America, I have worked on a range of problems that were driven by local needs. Many times, I have tried to apply state of the art solutions to well-known problems, only to find that these don’t work outside of their initial evaluation dataset. In this talk, I’ll discuss how geographical, language, and social diversity have opened new avenues for innovation and better understanding the social Web. I’ll also show that to truly create useful technological solutions, we must develop inclusive research and resources. | Barbara Poblete | Department of Computer Science, University of Chile, Chile |
| 494 |  |  [Concept Regulation in the Social Sciences](https://doi.org/10.1145/3543507.3593050) |  | 0 | The sciences, notably biology and medicine, operate with highly regulated taxonomies and ontologies. The Social Sciences, on the other hand, muddle through in a proverbial tower of Babel. There may be some real benefits to an undisciplined set of ideas, but also some real costs. Over the last ten years, political scientists have attempted to get their semantic act by cooperating to formalize their vocabulary. The result has been a dramatic improvement in how scholars diagnose and treat problems of democracy, as well as a set of web applications that have changed the way countries write constitutions. Nevertheless, these methods of semantic cooperation have exposed some persistent challenges of “social engineering,” ones that may have tractable web solutions. | Zachary Elkins | University of Texas at Austin, USA |
| 495 |  |  [GNNs and Graph Generative models for biomedical applications](https://doi.org/10.1145/3543507.3593049) |  | 0 | Graph generative models are recently gaining significant interest in current application domains. They are commonly used to model social networks, knowledge graphs, and protein-protein interaction networks. In this talk we will present the potential of graph generative models and our recent relevant efforts in the biomedical domain. More specifically we present a novel architecture that generates medical records as graphs with privacy guarantees. We capitalize and modify the graph Variational autoencoders (VAEs) architecture. We train the generative model with the well known MIMIC medical database and achieve generated data that are very similar to the real ones yet provide privacy guarantees. We also develop new GNNs for predicting antibiotic resistance and other protein related downstream tasks such as enzymes classifications and Gene Ontology classification. We achieve there as well promising results with potential for future application in broader biomedical related tasks. Finally we present future research directions for multi modal generative models involving graphs. | Michalis Vazirgiannis | Ecole Polytechnique de France, France |
| 496 |  |  [Decolonizing Creative Labor in the age of AI](https://doi.org/10.1145/3543507.3593047) |  | 0 | Creative AI has got us asking existential questions of what makes us human. To crack the code, you need to crack the culture that makes us who we are. Who and what is creative remains largely disconnected from diverse and global cultural norms, rendering existing technology suboptimal and even unusable to the world’s majority. Creativity has long been dictated by the aesthetic taste, values, needs, concerns, and aspirations of the West. Today, India and China alone account for the majority of the world’s users. The Global South are fast shaping data systems in ways that remain underexamined and siloed as “Rest of World” among industry and government folks. With the rise of the creator economy across sectors, questions abound on creative rights, provenance, fairness, labor, and representation. This talk discusses concerns around digital labor, data materiality, media literacies, creative value, and online expression. In doing so, it sets a pathway towards designing inclusive and intersectional systems that transcend borders. | Payal Arora | Erasmus University, Netherlands |
| 497 |  |  [Connectivity](https://doi.org/10.1145/3543507.3593048) |  | 0 | WikiPathways (https://www.wikipathways.org) is a biological pathway database known for its collaborative nature and open science approaches. With the core idea of the scientific community developing and curating biological knowledge in pathway models, WikiPathways lowers all barriers for accessing and using its content. Increasingly more content creators, initiatives, projects and tools have started using WikiPathways. Central in this growth and increased use of WikiPathways are the various communities that focus on particular subsets of molecular pathways such as for rare diseases and lipid metabolism. Knowledge from published pathway figures helps prioritize pathway development, using optical character and named entity recognition. We show the growth of WikiPathways over the last three years, highlight the new communities and collaborations of pathway authors and curators, and describe various technologies to connect to external resources and initiatives. The road toward a sustainable, community-driven pathway database goes through integration with other resources such as Wikidata and allowing more use, curation and redistribution of WikiPathways content. | Robert Melancton Metcalfe | Gladstone Inst, Inst Data Sci & Biotechnol, San Francisco, CA 94158 USA; Univ Fed Minas Gerais, Dept Bioquim & Imunol, Inst Ciencias Biol, BR-31270901 Belo Horizonte, MG, Brazil; Micelio, B-2180 Antwerp, Belgium; Maastricht Univ, NUTRIM, Dept Bioinformat BiGCaT, NL-6229 ER Maastricht, Netherlands; Univ Vienna, Dept Pharmaceut Chem, Pharmacoinformat Res Grp, A-1090 Vienna, Austria |
| 498 |  |  [Fair Graph Representation Learning via Diverse Mixture-of-Experts](https://doi.org/10.1145/3543507.3583207) |  | 0 | Graph Neural Networks (GNNs) have demonstrated a great representation learning capability on graph data and have been utilized in various downstream applications. However, real-world data in web-based applications (e.g., recommendation and advertising) always contains bias, preventing GNNs from learning fair representations. Although many works were proposed to address the fairness issue, they suffer from the significant problem of insufficient learnable knowledge with limited attributes after debiasing. To address this problem, we develop Graph-Fairness Mixture of Experts (G-Fame), a novel plug-and-play method to assist any GNNs to learn distinguishable representations with unbiased attributes. Furthermore, based on G-Fame, we propose G-Fame++, which introduces three novel strategies to improve the representation fairness from node representations, model layer, and parameter redundancy perspectives. In particular, we first present the embedding diversified method to learn distinguishable node representations. Second, we design the layer diversified strategy to maximize the output difference of distinct model layers. Third, we introduce the expert diversified method to minimize expert parameter similarities to learn diverse and complementary representations. Extensive experiments demonstrate the superiority of G-Fame and G-Fame++ in both accuracy and fairness, compared to state-of-the-art methods across multiple graph datasets. | Zheyuan Liu, Chunhui Zhang, Yijun Tian, Erchi Zhang, Chao Huang, Yanfang Ye, Chuxu Zhang | University of Notre Dame, USA; Brandeis University, USA; University of Hong Kong, Hong Kong |
| 499 |  |  [Multi-Aspect Heterogeneous Graph Augmentation](https://doi.org/10.1145/3543507.3583208) |  | 0 | Data augmentation has been widely studied as it can be used to improve the generalizability of graph representation learning models. However, existing works focus only on the data augmentation on homogeneous graphs. Data augmentation for heterogeneous graphs remains under-explored. Considering that heterogeneous graphs contain different types of nodes and links, ignoring the type information and directly applying the data augmentation methods of homogeneous graphs to heterogeneous graphs will lead to suboptimal results. In this paper, we propose a novel Multi-Aspect Heterogeneous Graph Augmentation framework named MAHGA. Specifically, MAHGA consists of two core augmentation strategies: structure-level augmentation and metapath-level augmentation. Structure-level augmentation pays attention to network schema aspect and designs a relation-aware conditional variational auto-encoder that can generate synthetic features of neighbors to augment the nodes and the node types with scarce links. Metapath-level augmentation concentrates on metapath aspect, which constructs metapath reachable graphs for different metapaths and estimates the graphons of them. By sampling and mixing up based on the graphons, MAHGA yields intra-metapath and inter-metapath augmentation. Finally, we conduct extensive experiments on multiple benchmarks to validate the effectiveness of MAHGA. Experimental results demonstrate that our method improves the performances across a set of heterogeneous graph learning models and datasets. | Yuchen Zhou, Yanan Cao, Yongchao Liu, Yanmin Shang, Peng Zhang, Zheng Lin, Yun Yue, Baokun Wang, Xing Fu, Weiqiang Wang | Ant Group, China; Cyberspace Institute of Advanced Technology, Guangzhou University, China; Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China; School of Cyber Security, University of Chinese Academy of Sciences, China and Institute of Information Engineering, Chinese Academy of Sciences, China |
| 500 |  |  [NeuKron: Constant-Size Lossy Compression of Sparse Reorderable Matrices and Tensors](https://doi.org/10.1145/3543507.3583226) |  | 0 | Many real-world data are naturally represented as a sparse reorderable matrix, whose rows and columns can be arbitrarily ordered (e.g., the adjacency matrix of a bipartite graph). Storing a sparse matrix in conventional ways requires an amount of space linear in the number of non-zeros, and lossy compression of sparse matrices (e.g., Truncated SVD) typically requires an amount of space linear in the number of rows and columns. In this work, we propose NeuKron for compressing a sparse reorderable matrix into a constant-size space. NeuKron generalizes Kronecker products using a recurrent neural network with a constant number of parameters. NeuKron updates the parameters so that a given matrix is approximated by the product and reorders the rows and columns of the matrix to facilitate the approximation. The updates take time linear in the number of non-zeros in the input matrix, and the approximation of each entry can be retrieved in logarithmic time. We also extend NeuKron to compress sparse reorderable tensors (e.g. multi-layer graphs), which generalize matrices. Through experiments on ten real-world datasets, we show that NeuKron is (a) Compact: requiring up to five orders of magnitude less space than its best competitor with similar approximation errors, (b) Accurate: giving up to 10x smaller approximation error than its best competitors with similar size outputs, and (c) Scalable: successfully compressing a matrix with over 230 million non-zero entries. | Taehyung Kwon, Jihoon Ko, Jinhong Jung, Kijung Shin | Department of Computer Science & Engineering, Jeonbuk National University, Republic of Korea; Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology, Republic of Korea |
| 501 |  |  [Testing Cluster Properties of Signed Graphs](https://doi.org/10.1145/3543507.3583213) |  | 0 | This work initiates the study of property testing in signed graphs, where every edge has either a positive or a negative sign. We show that there exist sublinear query and time algorithms for testing three key properties of signed graphs: balance (or 2-clusterability), clusterability and signed triangle freeness. We consider both the dense graph model, where one queries the adjacency matrix entries of a signed graph, and the bounded-degree model, where one queries for the neighbors of a node and the sign of the connecting edge. Our algorithms use a variety of tools from unsigned graph property testing, as well as reductions from one setting to the other. Our main technical contribution is a sublinear algorithm for testing clusterability in the bounded-degree model. This contrasts with the property of k-clusterability in unsigned graphs, which is not testable with a sublinear number of queries in the bounded-degree model. We experimentally evaluate the complexity and usefulness of several of our testers on real-life and synthetic datasets. | Florian Adriaens, Simon Apers | University of Helsinki, Finland; Université de Paris, CNRS, IRIF, France |
| 502 |  |  [RSGNN: A Model-agnostic Approach for Enhancing the Robustness of Signed Graph Neural Networks](https://doi.org/10.1145/3543507.3583221) |  | 0 | Signed graphs model complex relations using both positive and negative edges. Signed graph neural networks (SGNN) are powerful tools to analyze signed graphs. We address the vulnerability of SGNN to potential edge noise in the input graph. Our goal is to strengthen existing SGNN allowing them to withstand edge noises by extracting robust representations for signed graphs. First, we analyze the expressiveness of SGNN using an extended Weisfeiler-Lehman (WL) graph isomorphism test and identify the limitations to SGNN over triangles that are unbalanced. Then, we design some structure-based regularizers to be used in conjunction with an SGNN that highlight intrinsic properties of a signed graph. The tools and insights above allow us to propose a novel framework, Robust Signed Graph Neural Network (RSGNN), which adopts a dual architecture that simultaneously denoises the graph while learning node representations. We validate the performance of our model empirically on four real-world signed graph datasets, i.e., Bitcoin_OTC, Bitcoin_Alpha, Epinion and Slashdot, RSGNN can clearly improve the robustness of popular SGNN models. When the signed graphs are affected by random noise, our method outperforms baselines by up to 9.35% Binary-F1 for link sign prediction. Our implementation is available in PyTorch1. | Zeyu Zhang, Jiamou Liu, Xianda Zheng, Yifei Wang, Pengqian Han, Yupan Wang, Kaiqi Zhao, Zijian Zhang | Beijing Institute of Technology, China; The University of Auckland, New Zealand |
| 503 |  |  [Multi-aspect Diffusion Network Inference](https://doi.org/10.1145/3543507.3583228) |  | 0 | To learn influence relationships between nodes in a diffusion network, most existing approaches resort to precise timestamps of historical node infections. The target network is customarily assumed as an one-aspect diffusion network, with homogeneous influence relationships. Nonetheless, tracing node infection timestamps is often infeasible due to high cost, and the type of influence relationships may be heterogeneous because of the diversity of propagation media. In this work, we study how to infer a multi-aspect diffusion network with heterogeneous influence relationships, using only node infection statuses that are more readily accessible in practice. Equipped with a probabilistic generative model, we iteratively conduct a posteriori, quantitative analysis on historical diffusion results of the network, and infer the structure and strengths of homogeneous influence relationships in each aspect. Extensive experiments on both synthetic and real-world networks are conducted, and the results verify the effectiveness and efficiency of our approach. | Hao Huang, Keqi Han, Beicheng Xu, Ting Gan | Wuhan University, China |
| 504 |  |  [Encoding Node Diffusion Competence and Role Significance for Network Dismantling](https://doi.org/10.1145/3543507.3583233) |  | 0 | Percolation theory shows that removing a small fraction of critical nodes can lead to the disintegration of a large network into many disconnected tiny subnetworks. The network dismantling task focuses on how to efficiently select the least such critical nodes. Most existing approaches focus on measuring nodes' importance from either functional or topological viewpoint. Different from theirs, we argue that nodes' importance can be measured from both of the two complementary aspects: The functional importance can be based on the nodes' competence in relaying network information; While the topological importance can be measured from nodes' regional structural patterns. In this paper, we propose an unsupervised learning framework for network dismantling, called DCRS, which encodes and fuses both node diffusion competence and role significance. Specifically, we propose a graph diffusion neural network which emulates information diffusion for competence encoding; We divide nodes with similar egonet structural patterns into a few roles, and construct a role graph on which to encode node role significance. The DCRS converts and fuses the two encodings to output a final ranking score for selecting critical nodes. Experiments on both real-world networks and synthetic networks demonstrate that our scheme significantly outperforms the state-of-the-art competitors for its mostly requiring much fewer nodes to dismantle a network. | Jiazheng Zhang, Bang Wang | School of Electronic Information and Communications, Huazhong University of Science and Technology (HUST), Wuhan, China, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, China |
| 505 |  |  [Hierarchical Knowledge Graph Learning Enabled Socioeconomic Indicator Prediction in Location-Based Social Network](https://doi.org/10.1145/3543507.3583239) |  | 0 | Socioeconomic indicators reflect location status from various aspects such as demographics, economy, crime and land usage, which play an important role in the understanding of location-based social networks (LBSNs). Especially, several existing works leverage multi-source data for socioeconomic indicator prediction in LBSNs, which however fail to capture semantic information as well as distil comprehensive knowledge therein. On the other hand, knowledge graph (KG), which distils semantic knowledge from multi-source data, has been popular in recent LBSN research, which inspires us to introduce KG for socioeconomic indicator prediction in LBSNs. Specifically, we first construct a location-based KG (LBKG) to integrate various kinds of knowledge from heterogeneous LBSN data, including locations and other related elements like point of interests (POIs), business areas as well as various relationships between them, such as spatial proximity and functional similarity. Then we propose a hierarchical KG learning model to capture both global knowledge from LBKG and domain knowledge from several sub-KGs. Extensive experiments on three datasets demonstrate our model’s superiority over state-of-the-art methods in socioeconomic indicators prediction. Our code is released at: https://github.com/tsinghua-fib-lab/KG-socioeconomic-indicator-prediction. | Zhilun Zhou, Yu Liu, Jingtao Ding, Depeng Jin, Yong Li | Tsinghua University, China |
| 506 |  |  [Opinion Maximization in Social Networks via Leader Selection](https://doi.org/10.1145/3543507.3583243) |  | 0 | We study a leader selection problem for the DeGroot model of opinion dynamics in a social network with n nodes and m edges, in the presence of s0 = O(1) leaders with opinion 0. Concretely, we consider the problem of maximizing the average opinion in equilibrium by selecting k = O(1) leaders with opinion 1 from the remaining n − s0 nodes, which was previously proved to be NP-hard. A deterministic greedy algorithm was also proposed to approximately solve the problem, which has an approximation factor (1 − 1/e) and time complexity O(n3), and thus does not apply to large networks. In this paper, we first give an interpretation for the opinion of each node in equilibrium and the disagreement of the model from the perspective of resistor networks. We then develop a fast randomized greedy algorithm to solve the problem. To this end, we express the average opinion in terms of the pseudoinverse and Schur complement of Laplacian matrix for . The key ingredients of our randomized algorithm are Laplacian solvers and node sparsifiers, where the latter can preserve pairwise effective resistance by viewing Schur complement as random walks with average length l. For any error parameter ϵ > 0, at each iteration, the randomized algorithm selects a node that deviates from the local optimum marginal gain at most ϵ. The time complexity of the fast algorithm is O(mkllog nϵ− 2). Extensive experiments on various real networks show that the effectiveness of our randomized algorithm is similar to that of the deterministic algorithm, both of which are better than several baseline algorithms, and that our randomized algorithm is more efficient and scalable to large graphs with more than one million nodes. | Xiaotian Zhou, Zhongzhi Zhang | Fudan University, China |
| 507 |  |  [Graph Self-supervised Learning with Augmentation-aware Contrastive Learning](https://doi.org/10.1145/3543507.3583246) |  | 0 | Graph self-supervised learning aims to mine useful information from unlabeled graph data, and has been successfully applied to pre-train graph representations. Many existing approaches use contrastive learning to learn powerful embeddings by learning contrastively from two augmented graph views. However, none of these graph contrastive methods fully exploits the diversity of different augmentations, and hence is prone to overfitting and limited generalization ability of learned representations. In this paper, we propose a novel Graph Self-supervised Learning method with Augmentation-aware Contrastive Learning. Our method is based on the finding that the pre-trained model after adding augmentation diversity can achieve better generalization ability. To make full use of the information from the diverse augmentation method, this paper constructs new augmentation-aware prediction task which complementary with the contrastive learning task. Similar to how pre-training requires fast adaptation to different downstream tasks, we simulate train-test adaptation on the constructed tasks for further enhancing the learning ability; this strategy can be deemed as a form of meta-learning. Experimental results show that our method outperforms previous methods and learns better representations for a variety of downstream tasks. | Dong Chen, Xiang Zhao, Wei Wang, Zhen Tan, Weidong Xiao | College of Systems Engineering, National University of Defense Technology, China; Laboratory for Big Data and Decision, National University of Defense Technology, China; Data Science and Analytics Thrust, Information Hub, The Hong Kong University of Science and Technology (Guangzhou), China |
| 508 |  |  [Unifying and Improving Graph Convolutional Neural Networks with Wavelet Denoising Filters](https://doi.org/10.1145/3543507.3583253) |  | 0 | Graph convolutional neural network (GCN) is a powerful deep learning framework for network data. However, variants of graph neural architectures can lead to drastically different performance on different tasks. Model comparison calls for a unifying framework with interpretability and principled experimental procedures. Based on the theories from graph signal processing (GSP), we show that GCN’s capability is fundamentally limited by the uncertainty principle, and wavelets provide a controllable trade-off between local and global information. We adapt wavelet denoising filters to the graph domain, unifying popular variants of GCN under a common interpretable mathematical framework. Furthermore, we propose WaveThresh and WaveShrink which are novel GCN models based on proven denoising filters from the signal processing literature. Empirically, we evaluate our models and other popular GCNs under a more principled procedure and analyze how trade-offs between local and global graph signals can lead to better performance in different datasets. | Liangtian Wan, Xiaona Li, Huijin Han, Xiaoran Yan, Lu Sun, Zhaolong Ning, Feng Xia | Research Center of Big Data Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, China; Department of Communication Engineering, Institute of Information Science Technology, Dalian Maritime University, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing University of Posts and Telecommunications, China; School of Computing Technologies, RMIT University, Australia; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, China |
| 509 |  |  [Neighborhood Structure Configuration Models](https://doi.org/10.1145/3543507.3583266) |  | 0 | We develop a new method to efficiently sample synthetic networks that preserve the d-hop neighborhood structure of a given network for any given d. The proposed algorithm trades off the diversity in network samples against the depth of the neighborhood structure that is preserved. Our key innovation is to employ a colored Configuration Model with colors derived from iterations of the so-called Color Refinement algorithm. We prove that with increasing iterations the preserved structural information increases: the generated synthetic networks and the original network become more and more similar, and are eventually indistinguishable in terms of centrality measures such as PageRank, HITS, Katz centrality and eigenvector centrality. Our work enables to efficiently generate samples with a precisely controlled similarity to the original network, especially for large networks. | Felix I. Stamm, Michael Scholkemper, Michael T. Schaub, Markus Strohmaier | University of Mannheim & GESIS, Germany; RWTH Aachen University, Germany |
| 510 |  |  [CurvDrop: A Ricci Curvature Based Approach to Prevent Graph Neural Networks from Over-Smoothing and Over-Squashing](https://doi.org/10.1145/3543507.3583269) |  | 0 | Graph neural networks (GNNs) are powerful models to handle graph data and can achieve state-of-the-art in many critical tasks including node classification and link prediction. However, existing graph neural networks still face both challenges of over-smoothing and over-squashing based on previous literature. To this end, we propose a new Curvature-based topology-aware Dropout sampling technique named CurvDrop, in which we integrate the Discrete Ricci Curvature into graph neural networks to enable more expressive graph models. Also, this work can improve graph neural networks by quantifying connections in graphs and using structural information such as community structures in graphs. As a result, our method can tackle the both challenges of over-smoothing and over-squashing with theoretical justification. Also, numerous experiments on public datasets show the effectiveness and robustness of our proposed method. The code and data are released in https://github.com/liu-yang-maker/Curvature-based-Dropout. | Yang Liu, Chuan Zhou, Shirui Pan, Jia Wu, Zhao Li, Hongyang Chen, Peng Zhang | Research Center for Graph Computing, Zhejiang Lab, China; Cyberspace Institute of Advanced Technology, Guangzhou University, China; Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China; School of Information and Communication Technology, Griffith University, Australia; AMSS, Chinese Academy of Science, China and School of Cyber Security, University of Chinese Academy of Science, China; Hangzhou link2do Technology, China; School of Computing, Macquarie University, Australia |
| 511 |  |  [A Post-Training Framework for Improving Heterogeneous Graph Neural Networks](https://doi.org/10.1145/3543507.3583282) |  | 0 | Recent years have witnessed the success of heterogeneous graph neural networks (HGNNs) in modeling heterogeneous information networks (HINs). In this paper, we focus on the benchmark task of HGNNs, i.e., node classification, and empirically find that typical HGNNs are not good at predicting the label of a test node whose receptive field (1) has few training nodes from the same category or (2) has multiple training nodes from different categories. A possible explanation is that their message passing mechanisms may involve noises from different categories, and cannot fully explore task-specific knowledge such as the label dependency between distant nodes. Therefore, instead of introducing a new HGNN model, we propose a general post-training framework that can be applied on any pretrained HGNNs to further inject task-specific knowledge and enhance their prediction performance. Specifically, we first design an auxiliary system that estimates node labels based on (1) a global inference module of multi-channel label propagation and (2) a local inference module of network schema-aware prediction. The mechanism of our auxiliary system can complement the pretrained HGNNs by providing extra task-specific knowledge. During the post-training process, we will strengthen both system-level and module-level consistencies to encourage the cooperation between a pretrained HGNN and our auxiliary system. In this way, both systems can learn from each other for better performance. In experiments, we apply our framework to four typical HGNNs. Experimental results on three benchmark datasets show that compared with pretrained HGNNs, our post-training framework can enhance Micro-F1 by a relative improvement of 3.9% on average. Code, data and appendix are available at https://github.com/GXM1141/HGPF. | Cheng Yang, Xumeng Gong, Chuan Shi, Philip S. Yu | UNIVERSITY OF ILLINOIS AT CHICAGO, USA; Beijing University of Posts and Telecommunications, China |
| 512 |  |  [Link Prediction on Latent Heterogeneous Graphs](https://doi.org/10.1145/3543507.3583284) |  | 0 | On graph data, the multitude of node or edge types gives rise to heterogeneous information networks (HINs). To preserve the heterogeneous semantics on HINs, the rich node/edge types become a cornerstone of HIN representation learning. However, in real-world scenarios, type information is often noisy, missing or inaccessible. Assuming no type information is given, we define a so-called latent heterogeneous graph (LHG), which carries latent heterogeneous semantics as the node/edge types cannot be observed. In this paper, we study the challenging and unexplored problem of link prediction on an LHG. As existing approaches depend heavily on type-based information, they are suboptimal or even inapplicable on LHGs. To address the absence of type information, we propose a model named LHGNN, based on the novel idea of semantic embedding at node and path levels, to capture latent semantics on and between nodes. We further design a personalization function to modulate the heterogeneous contexts conditioned on their latent semantics w.r.t. the target node, to enable finer-grained aggregation. Finally, we conduct extensive experiments on four benchmark datasets, and demonstrate the superior performance of LHGNN. | TrungKien Nguyen, Zemin Liu, Yuan Fang | School of Computing & Information Systems, Singapore Management University, Singapore; School of Computing, National University of Singapore, Singapore |
| 513 |  |  [Predicting the Silent Majority on Graphs: Knowledge Transferable Graph Neural Network](https://doi.org/10.1145/3543507.3583287) |  | 0 | Graphs consisting of vocal nodes ("the vocal minority") and silent nodes ("the silent majority"), namely VS-Graph, are ubiquitous in the real world. The vocal nodes tend to have abundant features and labels. In contrast, silent nodes only have incomplete features and rare labels, e.g., the description and political tendency of politicians (vocal) are abundant while not for ordinary people (silent) on the twitter's social network. Predicting the silent majority remains a crucial yet challenging problem. However, most existing message-passing based GNNs assume that all nodes belong to the same domain, without considering the missing features and distribution-shift between domains, leading to poor ability to deal with VS-Graph. To combat the above challenges, we propose Knowledge Transferable Graph Neural Network (KT-GNN), which models distribution shifts during message passing and representation learning by transferring knowledge from vocal nodes to silent nodes. Specifically, we design the domain-adapted "feature completion and message passing mechanism" for node representation learning while preserving domain difference. And a knowledge transferable classifier based on KL-divergence is followed. Comprehensive experiments on real-world scenarios (i.e., company financial risk assessment and political elections) demonstrate the superior performance of our method. Our source code has been open sourced. | Wendong Bi, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen, Xueqi Cheng | Institute of Computing Technology, University of Chinese Academy of Sciences, China; Institute of Computing Technology, Chinese Academy of Sciences, China |
| 514 |  |  [Automated Spatio-Temporal Graph Contrastive Learning](https://doi.org/10.1145/3543507.3583304) |  | 0 | Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region dependencies with respect to POI semantics, mobility flow patterns and geographical positions. To improve the robustness of our GNN encoder against data noise and distribution issues, we design an automated spatio-temporal augmentation scheme with a parameterized contrastive view generator. AutoST can adapt to the spatio-temporal heterogeneous graph with multi-view semantics well preserved. Extensive experiments for three downstream spatio-temporal mining tasks on several real-world datasets demonstrate the significant performance gain achieved by our \model\ over a variety of baselines. The code is publicly available at https://github.com/HKUDS/AutoST. | Qianru Zhang, Chao Huang, Lianghao Xia, Zheng Wang, Zhonghang Li, SiuMing Yiu | The University of Hong Kong, Hong Kong; Huawei Singapore Research Center, Singapore; South China University of Technology, China |
| 515 |  |  [Characterization of Simplicial Complexes by Counting Simplets Beyond Four Nodes](https://doi.org/10.1145/3543507.3583332) |  | 0 | Simplicial complexes are higher-order combinatorial structures which have been used to represent real-world complex systems. In this paper, we concentrate on the local patterns in simplicial complexes called simplets, a generalization of graphlets. We formulate the problem of counting simplets of a given size in a given simplicial complex. For this problem, we extend a sampling algorithm based on color coding from graphs to simplicial complexes, with essential technical novelty. We theoretically analyze our proposed algorithm named SC3, showing its correctness, unbiasedness, convergence, and time/space complexity. Through the extensive experiments on sixteen real-world datasets, we show the superiority of SC3 in terms of accuracy, speed, and scalability, compared to the baseline methods. Finally, we use the counts given by SC3 for simplicial complex analysis, especially for characterization, which is further used for simplicial complex clustering, where SC3 shows a strong ability of characterization with domain-based similarity. | Hyunju Kim, Jihoon Ko, Fanchen Bu, Kijung Shin | Kim Jaechul Graduate School of AI, Korea Advanced Institute of Science and Technology, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Republic of Korea |
| 516 |  |  [Robust Mid-Pass Filtering Graph Convolutional Networks](https://doi.org/10.1145/3543507.3583335) |  | 0 | Graph convolutional networks (GCNs) are currently the most promising paradigm for dealing with graph-structure data, while recent studies have also shown that GCNs are vulnerable to adversarial attacks. Thus developing GCN models that are robust to such attacks become a hot research topic. However, the structural purification learning-based or robustness constraints-based defense GCN methods are usually designed for specific data or attacks, and introduce additional objective that is not for classification. Extra training overhead is also required in their design. To address these challenges, we conduct in-depth explorations on mid-frequency signals on graphs and propose a simple yet effective Mid-pass filter GCN (Mid-GCN). Theoretical analyses guarantee the robustness of signals through the mid-pass filter, and we also shed light on the properties of different frequency signals under adversarial attacks. Extensive experiments on six benchmark graph data further verify the effectiveness of our designed Mid-GCN in node classification accuracy compared to state-of-the-art GCNs under various adversarial attack strategies. | Jincheng Huang, Lun Du, Xu Chen, Qiang Fu, Shi Han, Dongmei Zhang | Microsoft Research Aisa, China; Microsoft Research Asia, China; School of Computer Science, Southwest Petroleum University, China |
| 517 |  |  [PARROT: Position-Aware Regularized Optimal Transport for Network Alignment](https://doi.org/10.1145/3543507.3583357) |  | 0 | Network alignment is a critical steppingstone behind a variety of multi-network mining tasks. Most of the existing methods essentially optimize a Frobenius-like distance or ranking-based loss, ignoring the underlying geometry of graph data. Optimal transport (OT), together with Wasserstein distance, has emerged to be a powerful approach accounting for the underlying geometry explicitly. Promising as it might be, the state-of-the-art OT-based alignment methods suffer from two fundamental limitations, including (1) effectiveness due to the insufficient use of topology and consistency information and (2) scalability due to the non-convex formulation and repeated computationally costly loss calculation. In this paper, we propose a position-aware regularized optimal transport framework for network alignment named PARROT. To tackle the effectiveness issue, the proposed PARROT captures topology information by random walk with restart, with three carefully designed consistency regularization terms. To tackle the scalability issue, the regularized OT problem is decomposed into a series of convex subproblems and can be efficiently solved by the proposed constrained proximal point method with guaranteed convergence. Extensive experiments show that our algorithm achieves significant improvements in both effectiveness and scalability, outperforming the state-of-the-art network alignment methods and speeding up existing OT-based methods by up to 100 times. | Zhichen Zeng, Si Zhang, Yinglong Xia, Hanghang Tong | Computer Science, University of Illinois at Urbana-Champaign, USA; Meta, USA; University of Illinois at Urbana-Champaign, USA |
| 518 |  |  [Label Information Enhanced Fraud Detection against Low Homophily in Graphs](https://doi.org/10.1145/3543507.3583373) |  | 0 | Node classification is a substantial problem in graph-based fraud detection. Many existing works adopt Graph Neural Networks (GNNs) to enhance fraud detectors. While promising, currently most GNN-based fraud detectors fail to generalize to the low homophily setting. Besides, label utilization has been proved to be significant factor for node classification problem. But we find they are less effective in fraud detection tasks due to the low homophily in graphs. In this work, we propose GAGA, a novel Group AGgregation enhanced TrAnsformer, to tackle the above challenges. Specifically, the group aggregation provides a portable method to cope with the low homophily issue. Such an aggregation explicitly integrates the label information to generate distinguishable neighborhood information. Along with group aggregation, an attempt towards end-to-end trainable group encoding is proposed which augments the original feature space with the class labels. Meanwhile, we devise two additional learnable encodings to recognize the structural and relational context. Then, we combine the group aggregation and the learnable encodings into a Transformer encoder to capture the semantic information. Experimental results clearly show that GAGA outperforms other competitive graph-based fraud detectors by up to 24.39% on two trending public datasets and a real-world industrial dataset from Anonymous. Even more, the group aggregation is demonstrated to outperform other label utilization methods (e.g., C&S, BoT/UniMP) in the low homophily setting. | Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng Ma, Yu Sun, Dianhai Yu, Fang Dong, Jiahui Jin, Beilun Wang, Junzhou Luo | Southeast University, China; Baidu Inc., China |
| 519 |  |  [An Attentional Multi-scale Co-evolving Model for Dynamic Link Prediction](https://doi.org/10.1145/3543507.3583396) |  | 0 | Dynamic link prediction is essential for a wide range of domains, including social networks, bioinformatics, knowledge bases, and recommender systems. Existing works have demonstrated that structural information and temporal information are two of the most important information for this problem. However, existing works either focus on modeling them independently or modeling the temporal dynamics of a single structural scale, neglecting the complex correlations among them. This paper proposes to model the inherent correlations among the evolving dynamics of different structural scales for dynamic link prediction. Following this idea, we propose an Attentional Multi-scale Co-evolving Network (AMCNet). Specifically, We model multi-scale structural information by a motif-based graph neural network with multi-scale pooling. Then, we design a hierarchical attention-based sequence-to-sequence model for learning the complex correlations among the evolution dynamics of different structural scales. Extensive experiments on four real-world datasets with different characteristics demonstrate that AMCNet significantly outperforms the state-of-the-art in both single-step and multi-step dynamic link prediction tasks. | Guozhen Zhang, Tian Ye, Depeng Jin, Yong Li | Department of Electronic Engineering, Tsinghua University, China; Tsinghua University, China |
| 520 |  |  [Robust Graph Representation Learning for Local Corruption Recovery](https://doi.org/10.1145/3543507.3583399) |  | 0 | The performance of graph representation learning is affected by the quality of graph input. While existing research usually pursues a globally smoothed graph embedding, we believe the rarely observed anomalies are as well harmful to an accurate prediction. This work establishes a graph learning scheme that automatically detects (locally) corrupted feature attributes and recovers robust embedding for prediction tasks. The detection operation leverages a graph autoencoder, which does not make any assumptions about the distribution of the local corruptions. It pinpoints the positions of the anomalous node attributes in an unbiased mask matrix, where robust estimations are recovered with sparsity promoting regularizer. The optimizer approaches a new embedding that is sparse in the framelet domain and conditionally close to input observations. Extensive experiments are provided to validate our proposed model can recover a robust graph representation from black-box poisoning and achieve excellent performance. | Bingxin Zhou, Yuanhong Jiang, Yuguang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang | Shanghai Jiao Tong University, China; The University of Sydney, Australia; Shanghai Jiao Tong University, China and Shanghai Artificial Intelligence Laboratory, China; Griffith University, Australia; Shanghai Jiao Tong University, China and The University of Sydney, Australia |
| 521 |  |  [Hyperbolic Geometric Graph Representation Learning for Hierarchy-imbalance Node Classification](https://doi.org/10.1145/3543507.3583403) |  | 0 | Learning unbiased node representations for imbalanced samples in the graph has become a more remarkable and important topic. For the graph, a significant challenge is that the topological properties of the nodes (e.g., locations, roles) are unbalanced (topology-imbalance), other than the number of training labeled nodes (quantity-imbalance). Existing studies on topology-imbalance focus on the location or the local neighborhood structure of nodes, ignoring the global underlying hierarchical properties of the graph, i.e., hierarchy. In the real-world scenario, the hierarchical structure of graph data reveals important topological properties of graphs and is relevant to a wide range of applications. We find that training labeled nodes with different hierarchical properties have a significant impact on the node classification tasks and confirm it in our experiments. It is well known that hyperbolic geometry has a unique advantage in representing the hierarchical structure of graphs. Therefore, we attempt to explore the hierarchy-imbalance issue for node classification of graph neural networks with a novelty perspective of hyperbolic geometry, including its characteristics and causes. Then, we propose a novel hyperbolic geometric hierarchy-imbalance learning framework, named HyperIMBA, to alleviate the hierarchy-imbalance issue caused by uneven hierarchy-levels and cross-hierarchy connectivity patterns of labeled nodes.Extensive experimental results demonstrate the superior effectiveness of HyperIMBA for hierarchy-imbalance node classification tasks. | Xingcheng Fu, Yuecen Wei, Qingyun Sun, Haonan Yuan, Jia Wu, Hao Peng, Jianxin Li | School of Computer Science and Engineering, Beihang University, China and Zhongguancun Lab, China; School of Cyber Science and Technology, Beihang University, China; Guangxi Key Lab of Multi-source Information Mining Security, Guangxi Normal University, China; School of Computer Science and Engineering, Beihang University, China; School of Computing, Macquarie University, Australia |
| 522 |  |  [Graph Neural Networks without Propagation](https://doi.org/10.1145/3543507.3583419) |  | 0 | Due to the simplicity, intuition and explanation, most Graph Neural Networks (GNNs) are proposed by following the pipeline of message passing. Although they achieve superior performances in many tasks, propagation-based GNNs possess three essential drawbacks. Firstly, the propagation tends to produce smooth effect, which meets the inductive bias of homophily, and causes two serious issues: over-smoothing issue and performance drop on networks with heterophily. Secondly, the propagations to each node are irrelevant, which prevents GNNs from modeling high-order relation, and cause the GNNs fragile to the attributes noises. Thirdly, propagation-based GNNs may be fragile to topology noise, since they heavily relay on propagation over the topology. Therefore, the propagation, as the key component of most GNNs, may be the essence of some serious issues in GNNs. To get to the root of these issue, this paper attempts to replace the propagation with a novel local operation. Quantitative experimental analysis reveals: 1) the existence of low-rank characteristic in the node attributes from ego-networks and 2) the performance improvement by reducing its rank. Motivated by this finding, this paper propose the Low-Rank GNNs, whose key component is the low-rank attribute matrix approximation in ego-network. The graph topology is employed to construct the ego-networks instead of message propagation, which is sensitive to topology noises. The proposed Low-Rank GNNs posses some attractive characteristics, including robust to topology and attribute noises, parameter-free and parallelizable. Experimental evaluations demonstrate the superior performance, robustness to noises and universality of the proposed Low-Rank GNNs. | Liang Yang, Qiuliang Zhang, Runjie Shi, Wenmiao Zhou, Bingxin Niu, Chuan Wang, Xiaochun Cao, Dongxiao He, Zhen Wang, Yuanfang Guo | Tianjin University, China; Chinese Academy of Sciences, China; Northwestern Polytechnical University, China; Beihang University, China; Hebei University of Technology, China; Sun Yat-sen University, China |
| 523 |  |  [Self-Supervised Teaching and Learning of Representations on Graphs](https://doi.org/10.1145/3543507.3583441) |  | 0 | Recent years have witnessed significant advances in graph contrastive learning (GCL), while most GCL models use graph neural networks as encoders based on supervised learning. In this work, we propose a novel graph learning model called GraphTL, which explores self-supervised teaching and learning of representations on graphs. One critical objective of GCL is to retain original graph information. For this purpose, we design an encoder based on the idea of unsupervised dimensionality reduction of locally linear embedding (LLE). Specifically, we map one iteration of the LLE to one layer of the network. To guide the encoder to better retain the original graph information, we propose an unbalanced contrastive model consisting of two views, which are the learning view and the teaching view, respectively. Furthermore, we consider the nodes that are identical in muti-views as positive node pairs, and design the node similarity scorer so that the model can select positive samples of a target node. Extensive experiments have been conducted over multiple datasets to evaluate the performance of GraphTL in comparison with baseline models. Results demonstrate that GraphTL can reduce distances between similar nodes while preserving network topological and feature information, yielding better performance in node classification. | Liangtian Wan, Zhenqiang Fu, Lu Sun, Xianpeng Wang, Gang Xu, Xiaoran Yan, Feng Xia | Research Center of Big Data Intelligence, Research Institute of Artificial Intelligence, Zhejiang Lab, China; Department of Communication Engineering, Institute of Information Science Technology, Dalian Maritime University, China; State Key Laboratory of Marine Resource Utilization in South China Sea, School of Information and Communication Engineering, Hainan University, China; School of Computing Technologies, RMIT University, Australia; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, China; State Key Laboratory of Millimeter Waves, School of Information Science and Engineering, Southeast University, China |
| 524 |  |  [SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization](https://doi.org/10.1145/3543507.3583453) |  | 0 | Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger uncertainty in lower-level communities. SE-GSL is compatible with various GNN models and enhances the robustness towards noisy and heterophily structures. Extensive experiments show significant improvements in the effectiveness and robustness of structure learning and node representation learning. | Dongcheng Zou, Hao Peng, Xiang Huang, Renyu Yang, Jianxin Li, Jia Wu, Chunyang Liu, Philip S. Yu | Macquarie University, Australia; University of Illinois Chicago, USA; Didi Chuxing, China; Beihang University, China |
| 525 |  |  [Homophily-oriented Heterogeneous Graph Rewiring](https://doi.org/10.1145/3543507.3583454) |  | 0 | With the rapid development of the World Wide Web (WWW), heterogeneous graphs (HG) have explosive growth. Recently, heterogeneous graph neural network (HGNN) has shown great potential in learning on HG. Current studies of HGNN mainly focus on some HGs with strong homophily properties (nodes connected by meta-path tend to have the same labels), while few discussions are made in those that are less homophilous. Recently, there have been many works on homogeneous graphs with heterophily. However, due to heterogeneity, it is non-trivial to extend their approach to deal with HGs with heterophily. In this work, based on empirical observations, we propose a meta-path-induced metric to measure the homophily degree of a HG. We also find that current HGNNs may have degenerated performance when handling HGs with less homophilous properties. Thus it is essential to increase the generalization ability of HGNNs on non-homophilous HGs. To this end, we propose HDHGR, a homophily-oriented deep heterogeneous graph rewiring approach that modifies the HG structure to increase the performance of HGNN. We theoretically verify HDHGR. In addition, experiments on real-world HGs demonstrate the effectiveness of HDHGR, which brings at most more than 10% relative gain. | Jiayan Guo, Lun Du, Wendong Bi, Qiang Fu, Xiaojun Ma, Xu Chen, Shi Han, Dongmei Zhang, Yan Zhang | School of Intelligence Science and Technology, Peking University, China; Institute of Computing Technology, Chinese Academy of Sciences, China; Microsoft Research Asia, China |
| 526 |  |  [HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction](https://doi.org/10.1145/3543507.3583455) |  | 0 | Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the other hand, the internal order of causal correlation between historical states is captured by hyperbolic dilated causal convolution (HDCC) modules. The whole model is built upon the hyperbolic spaces to preserve the hierarchical structural information in the entire data flow. To prove the superiority of HGWaveNet, extensive experiments are conducted on six real-world graph datasets and the results show a relative improvement by up to 6.67% on AUC for temporal link prediction over SOTA methods. | Qijie Bai, Changli Nie, Haiwei Zhang, Dongming Zhao, Xiaojie Yuan | China Mobile Communication Group Tianjin Co., Ltd, China; College of CS, TJ Key Lab of NDST, Nankai University, China |
| 527 |  |  [Rethinking Structural Encodings: Adaptive Graph Transformer for Node Classification Task](https://doi.org/10.1145/3543507.3583464) |  | 0 | Graph Transformers have proved their advantages in graph data mining with elaborate Positional Encodings, especially in graph-level tasks. However, their application in the node classification task has not been fully exploited yet. In the node classification task, existing Graph Transformers with Positional Encodings are limited by the following issues: (i) PEs describing the node’s positional identities are insufficient for the node classification task on complex graphs, where a full portrayal of the local node property is needed. (ii) PEs for graphs are integrated with Transformers in a constant schema, resulting in the ignorance of local patterns that may vary among different nodes. In this paper, we propose Adaptive Graph Transformer (AGT) to tackle above issues. AGT consists of a Learnable Centrality Encoding and a Kernelized Local Structure Encoding. The two modules extract structural patterns from centrality and subgraph views in a learnable and scalable manner. Further, we design the Adaptive Transformer Block to adaptively integrate the attention scores and Structural Encodings in a node-specific manner. AGT achieves state-of-the-art performances on nine real-world web graphs (up to 1.6 million nodes). Furthermore, AGT shows outstanding results on two series of synthetic graphs with ranges of heterophily and noise ratios. | Xiaojun Ma, Qin Chen, Yi Wu, Guojie Song, Liang Wang, Bo Zheng | Peking University, China; Alibaba Group, China; Microsoft, China |
| 528 |  |  [Federated Node Classification over Graphs with Latent Link-type Heterogeneity](https://doi.org/10.1145/3543507.3583471) |  | 0 | Federated learning (FL) aims to train powerful and generalized global models without putting distributed data together, which has been shown effective in various domains of machine learning. The non-IIDness of data across local clients has been a major challenge for FL. In graphs, one specifically important perspective of non-IIDness is manifested in the link-type heterogeneity underlying homogeneous graphs– the seemingly uniform links captured in most real-world networks can carry different levels of homophily or semantics of relations, while the exact sets and distributions of such latent link-types can further differ across local clients. Through our preliminary data analysis, we are motivated to design a new graph FL framework that can simultaneously discover latent link-types and model message-passing w.r.t. the discovered link-types through the collaboration of distributed local clients. Specifically, we propose a framework FedLit that can dynamically detect the latent link-types during FL via an EM-based clustering algorithm and differentiate the message-passing through different types of links via multiple convolution channels. For experiments, we synthesize multiple realistic datasets of graphs with latent heterogeneous link-types from real-world data, and partition them with different levels of link-type heterogeneity. Comprehensive experimental results and in-depth analysis have demonstrated both superior performance and rational behaviors of our proposed techniques. | Han Xie, Li Xiong, Carl Yang | Emory University, USA |
| 529 |  |  [CMINet: a Graph Learning Framework for Content-aware Multi-channel Influence Diffusion](https://doi.org/10.1145/3543507.3583465) |  | 0 | The phenomena of influence diffusion on social networks have received tremendous research interests in the past decade. While most prior works mainly focus on predicting the total influence spread on a single network, a marketing campaign that exploits influence diffusion often involves multiple channels with various information disseminated on different media. In this paper, we introduce a new influence estimation problem, namely Content-aware Multi-channel Influence Diffusion (CMID), and accordingly propose CMINet to predict newly influenced users, given a set of seed users with different multimedia contents. In CMINet, we first introduce DiffGNN to encode the influencing power of users (nodes) and Influence-aware Optimal Transport (IOT) to align the embeddings to address the distribution shift across different diffusion channels. Then, we transform CMID into a node classification problem and propose Social-based Multimedia Feature Extractor (SMFE) and Content-aware Multi-channel Influence Propagation (CMIP) to jointly learn the user preferences on multimedia contents and predict the susceptibility of users. Furthermore, we prove that CMINet preserves monotonicity and submodularity, thus enabling (1 − 1/e)-approximate solutions for influence maximization. Experimental results manifest that CMINet outperforms eleven baselines on three public datasets. | HsiWen Chen, DeNian Yang, WangChien Lee, Philip S. Yu, MingSyan Chen | University of Illinois Chicago, USA; National Taiwan University, Taiwan and Academia Sinica, Taiwan; Academia Sinica, Taiwan; National Taiwan University, Taiwan; Pennsylvania State University, USA |
| 530 |  |  [Auto-HeG: Automated Graph Neural Network on Heterophilic Graphs](https://doi.org/10.1145/3543507.3583498) |  | 0 | Graph neural architecture search (NAS) has gained popularity in automatically designing powerful graph neural networks (GNNs) with relieving human efforts. However, existing graph NAS methods mainly work under the homophily assumption and overlook another important graph property, i.e., heterophily, which exists widely in various real-world applications. To date, automated heterophilic graph learning with NAS is still a research blank to be filled in. Due to the complexity and variety of heterophilic graphs, the critical challenge of heterophilic graph NAS mainly lies in developing the heterophily-specific search space and strategy. Therefore, in this paper, we propose a novel automated graph neural network on heterophilic graphs, namely Auto-HeG, to automatically build heterophilic GNN models with expressive learning abilities. Specifically, Auto-HeG incorporates heterophily into all stages of automatic heterophilic graph learning, including search space design, supernet training, and architecture selection. Through the diverse message-passing scheme with joint micro-level and macro-level designs, we first build a comprehensive heterophilic GNN search space, enabling Auto-HeG to integrate complex and various heterophily of graphs. With a progressive supernet training strategy, we dynamically shrink the initial search space according to layer-wise variation of heterophily, resulting in a compact and efficient supernet. Taking a heterophily-aware distance criterion as the guidance, we conduct heterophilic architecture selection in the leave-one-out pattern, so that specialized and expressive heterophilic GNN architectures can be derived. Extensive experiments illustrate the superiority of Auto-HeG in developing excellent heterophilic GNNs to human-designed models and graph NAS models. | Xin Zheng, Miao Zhang, Chunyang Chen, Qin Zhang, Chuan Zhou, Shirui Pan | Shenzhen University, China; Chinese Academy of Sciences, China; Harbin Institute of Technology (Shenzhen), China; Griffith University, Australia; Monash University, Australia |
| 531 |  |  [HINormer: Representation Learning On Heterogeneous Information Networks with Graph Transformer](https://doi.org/10.1145/3543507.3583493) |  | 0 | Recent studies have highlighted the limitations of message-passing based graph neural networks (GNNs), e.g., limited model expressiveness, over-smoothing, over-squashing, etc. To alleviate these issues, Graph Transformers (GTs) have been proposed which work in the paradigm that allows message passing to a larger coverage even across the whole graph. Hinging on the global range attention mechanism, GTs have shown a superpower for representation learning on homogeneous graphs. However, the investigation of GTs on heterogeneous information networks (HINs) is still under-exploited. In particular, on account of the existence of heterogeneity, HINs show distinct data characteristics and thus require different treatment. To bridge this gap, in this paper we investigate the representation learning on HINs with Graph Transformer, and propose a novel model named HINormer, which capitalizes on a larger-range aggregation mechanism for node representation learning. In particular, assisted by two major modules, i.e., a local structure encoder and a heterogeneous relation encoder, HINormer can capture both the structural and heterogeneous information of nodes on HINs for comprehensive node representations. We conduct extensive experiments on four HIN benchmark datasets, which demonstrate that our proposed model can outperform the state-of-the-art. | Qiheng Mao, Zemin Liu, Chenghao Liu, Jianling Sun | Salesforce Research Asia, Singapore; National University of Singapore, Singapore; Zhejiang University, China and Alibaba-Zhejiang University Joint Institute of Frontier Technologies, China |
| 532 |  |  [Minimum Topology Attacks for Graph Neural Networks](https://doi.org/10.1145/3543507.3583509) |  | 0 | With the great popularity of Graph Neural Networks (GNNs), their robustness to adversarial topology attacks has received significant attention. Although many attack methods have been proposed, they mainly focus on fixed-budget attacks, aiming at finding the most adversarial perturbations within a fixed budget for target node. However, considering the varied robustness of each node, there is an inevitable dilemma caused by the fixed budget, i.e., no successful perturbation is found when the budget is relatively small, while if it is too large, the yielding redundant perturbations will hurt the invisibility. To break this dilemma, we propose a new type of topology attack, named minimum-budget topology attack, aiming to adaptively find the minimum perturbation sufficient for a successful attack on each node. To this end, we propose an attack model, named MiBTack, based on a dynamic projected gradient descent algorithm, which can effectively solve the involving non-convex constraint optimization on discrete topology. Extensive results on three GNNs and four real-world datasets show that MiBTack can successfully lead all target nodes misclassified with the minimum perturbation edges. Moreover, the obtained minimum budget can be used to measure node robustness, so we can explore the relationships of robustness, topology, and uncertainty for nodes, which is beyond what the current fixed-budget topology attacks can offer. | Mengmei Zhang, Xiao Wang, Chuan Shi, Lingjuan Lyu, Tianchi Yang, Junping Du | Sony AI, China; Beijing University of Posts and Telecommunications, China |
| 533 |  |  [Learning Mixtures of Markov Chains with Quality Guarantees](https://doi.org/10.1145/3543507.3583524) |  | 0 | A large number of modern applications ranging from listening songs online and browsing the Web to using a navigation app on a smartphone generate a plethora of user trails. Clustering such trails into groups with a common sequence pattern can reveal significant structure in human behavior that can lead to improving user experience through better recommendations, and even prevent suicides [LMCR14]. One approach to modeling this problem mathematically is as a mixture of Markov chains. Recently, Gupta, Kumar and Vassilvitski [GKV16] introduced an algorithm (GKV-SVD) based on the singular value decomposition (SVD) that under certain conditions can perfectly recover a mixture of L chains on n states, given only the distribution of trails of length 3 (3-trail). In this work we contribute to the problem of unmixing Markov chains by highlighting and addressing two important constraints of the GKV-SVD algorithm [GKV16]: some chains in the mixture may not even be weakly connected, and secondly in practice one does not know beforehand the true number of chains. We resolve these issues in the Gupta et al. paper [GKV16]. Specifically, we propose an algebraic criterion that enables us to choose a value of L efficiently that avoids overfitting. Furthermore, we design a reconstruction algorithm that outputs the true mixture in the presence of disconnected chains and is robust to noise. We complement our theoretical results with experiments on both synthetic and real data, where we observe that our method outperforms the GKV-SVD algorithm. Finally, we empirically observe that combining an EM-algorithm with our method performs best in practice, both in terms of reconstruction error with respect to the distribution of 3-trails and the mixture of Markov Chains. | Fabian Spaeh, Charalampos E. Tsourakakis | Department of Computer Science, Boston University, USA |
| 534 |  |  [GIF: A General Graph Unlearning Strategy via Influence Function](https://doi.org/10.1145/3543507.3583521) |  | 0 | With the greater emphasis on privacy and security in our society, the problem of graph unlearning -- revoking the influence of specific data on the trained GNN model, is drawing increasing attention. However, ranging from machine unlearning to recently emerged graph unlearning methods, existing efforts either resort to retraining paradigm, or perform approximate erasure that fails to consider the inter-dependency between connected neighbors or imposes constraints on GNN structure, therefore hard to achieve satisfying performance-complexity trade-offs. In this work, we explore the influence function tailored for graph unlearning, so as to improve the unlearning efficacy and efficiency for graph unlearning. We first present a unified problem formulation of diverse graph unlearning tasks \wrt node, edge, and feature. Then, we recognize the crux to the inability of traditional influence function for graph unlearning, and devise Graph Influence Function (GIF), a model-agnostic unlearning method that can efficiently and accurately estimate parameter changes in response to a $\epsilon$-mass perturbation in deleted data. The idea is to supplement the objective of the traditional influence function with an additional loss term of the influenced neighbors due to the structural dependency. Further deductions on the closed-form solution of parameter changes provide a better understanding of the unlearning mechanism. We conduct extensive experiments on four representative GNN models and three benchmark datasets to justify the superiority of GIF for diverse graph unlearning tasks in terms of unlearning efficacy, model utility, and unlearning efficiency. Our implementations are available at \url{https://github.com/wujcan/GIF-torch/}. | Jiancan Wu, Yi Yang, Yuchun Qian, Yongduo Sui, Xiang Wang, Xiangnan He | University of Science and Technology of China, China |
| 535 |  |  [INCREASE: Inductive Graph Representation Learning for Spatio-Temporal Kriging](https://doi.org/10.1145/3543507.3583525) |  | 0 | Spatio-temporal kriging is an important problem in web and social applications, such as Web or Internet of Things, where things (e.g., sensors) connected into a web often come with spatial and temporal properties. It aims to infer knowledge for (the things at) unobserved locations using the data from (the things at) observed locations during a given time period of interest. This problem essentially requires \emph{inductive learning}. Once trained, the model should be able to perform kriging for different locations including newly given ones, without retraining. However, it is challenging to perform accurate kriging results because of the heterogeneous spatial relations and diverse temporal patterns. In this paper, we propose a novel inductive graph representation learning model for spatio-temporal kriging. We first encode heterogeneous spatial relations between the unobserved and observed locations by their spatial proximity, functional similarity, and transition probability. Based on each relation, we accurately aggregate the information of most correlated observed locations to produce inductive representations for the unobserved locations, by jointly modeling their similarities and differences. Then, we design relation-aware gated recurrent unit (GRU) networks to adaptively capture the temporal correlations in the generated sequence representations for each relation. Finally, we propose a multi-relation attention mechanism to dynamically fuse the complex spatio-temporal information at different time steps from multiple relations to compute the kriging output. Experimental results on three real-world datasets show that our proposed model outperforms state-of-the-art methods consistently, and the advantage is more significant when there are fewer observed locations. Our code is available at https://github.com/zhengchuanpan/INCREASE. | Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, Jianzhong Qi, Chaochao Chen, Longbiao Chen | Zhejiang University, China; University of Melbourne, Australia; School of Informatics, Xiamen University, China |
| 536 |  |  [Unlearning Graph Classifiers with Limited Data Resources](https://doi.org/10.1145/3543507.3583547) |  | 0 | As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearning mechanism, which is hard to replicate for deep neural networks. Our third contribution are extensive simulation results which show that, compared to complete retraining of GNNs after each removal request, the new GST-based approach offers, on average, a 10.38x speed-up and leads to a 2.6% increase in test accuracy during unlearning of 90 out of 100 training graphs from the IMDB dataset (10% training ratio). Our implementation is available online at https://doi.org/10.5281/zenodo.7613150. | Chao Pan, Eli Chien, Olgica Milenkovic | University of Illinois, Urbana-Champaign, USA |
| 537 |  |  [GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner](https://doi.org/10.1145/3543507.3583379) |  | 0 | Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)--one type of generative method--have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)--that are randomly masked from the input--with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to regularize the feature reconstruction. The multi-view random re-mask decoding is to introduce randomness into reconstruction in the feature space, while the latent representation prediction is to enforce the reconstruction in the embedding space. Extensive experiments show that GraphMAE2 can consistently generate top results on various public datasets, including at least 2.45% improvements over state-of-the-art baselines on ogbn-Papers100M with 111M nodes and 1.6B edges. | Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, Jie Tang | Beijing Institute of Technology, China; Bosch Center for Artificial Intelligence, Germany; Tsinghua University, China |
| 538 |  |  [KGTrust: Evaluating Trustworthiness of SIoT via Knowledge Enhanced Graph Neural Networks](https://doi.org/10.1145/3543507.3583549) |  | 0 | Social Internet of Things (SIoT), a promising and emerging paradigm that injects the notion of social networking into smart objects (i.e., things), paving the way for the next generation of Internet of Things. However, due to the risks and uncertainty, a crucial and urgent problem to be settled is establishing reliable relationships within SIoT, that is, trust evaluation. Graph neural networks for trust evaluation typically adopt a straightforward way such as one-hot or node2vec to comprehend node characteristics, which ignores the valuable semantic knowledge attached to nodes. Moreover, the underlying structure of SIoT is usually complex, including both the heterogeneous graph structure and pairwise trust relationships, which renders hard to preserve the properties of SIoT trust during information propagation. To address these aforementioned problems, we propose a novel knowledge-enhanced graph neural network (KGTrust) for better trust evaluation in SIoT. Specifically, we first extract useful knowledge from users' comment behaviors and external structured triples related to object descriptions, in order to gain a deeper insight into the semantics of users and objects. Furthermore, we introduce a discriminative convolutional layer that utilizes heterogeneous graph structure, node semantics, and augmented trust relationships to learn node embeddings from the perspective of a user as a trustor or a trustee, effectively capturing multi-aspect properties of SIoT trust during information propagation. Finally, a trust prediction layer is developed to estimate the trust relationships between pairwise nodes. Extensive experiments on three public datasets illustrate the superior performance of KGTrust over state-of-the-art methods. | Zhizhi Yu, Di Jin, Cuiying Huo, Zhiqiang Wang, Xiulong Liu, Heng Qi, Jia Wu, Lingfei Wu | School of Computer Science and Technology, Dalian University of Technology, China; School of Computing, Macquarie University, Australia; Content and Knowledge Graph, Pinterest, USA; College of Intelligence and Computing, Tianjin University, China |
| 539 |  |  [CogDL: A Comprehensive Library for Graph Deep Learning](https://doi.org/10.1145/3543507.3583472) |  | 0 | Graph neural networks (GNNs) have attracted tremendous attention from the graph learning community in recent years. It has been widely adopted in various real-world applications from diverse domains, such as social networks and biological graphs. The research and applications of graph deep learning present new challenges, including the sparse nature of graph data, complicated training of GNNs, and non-standard evaluation of graph tasks. To tackle the issues, we present CogDL, a comprehensive library for graph deep learning that allows researchers and practitioners to conduct experiments, compare methods, and build applications with ease and efficiency. In CogDL, we propose a unified design for the training and evaluation of GNN models for various graph tasks, making it unique among existing graph learning libraries. By utilizing this unified trainer, CogDL can optimize the GNN training loop with several training techniques, such as mixed precision training. Moreover, we develop efficient sparse operators for CogDL, enabling it to become the most competitive graph library for efficiency. Another important CogDL feature is its focus on ease of use with the aim of facilitating open and reproducible research of graph learning. We leverage CogDL to report and maintain benchmark results on fundamental graph tasks, which can be reproduced and directly used by the community. | Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Zhongming Yu, Hengrui Zhang, Xingcheng Yao, Aohan Zeng, Shiguang Guo, Yuxiao Dong, Yang Yang, Peng Zhang, Guohao Dai, Yu Wang, Chang Zhou, Hongxia Yang, Jie Tang | Zhejiang University, China; Alibaba Group, China; Zhipu AI, China; Tsinghua University, China |
| 540 |  |  [Tracing Knowledge Instead of Patterns: Stable Knowledge Tracing with Diagnostic Transformer](https://doi.org/10.1145/3543507.3583255) |  | 0 | Knowledge Tracing (KT) aims at tracing the evolution of the knowledge states along the learning process of a learner. It has become a crucial task for online learning systems to model the learning process of their users, and further provide their users a personalized learning guidance. However, recent developments in KT based on deep neural networks mostly focus on increasing the accuracy of predicting the next performance of students. We argue that current KT modeling, as well as training paradigm, can lead to models tracing patterns of learner’s learning activities, instead of their evolving knowledge states. In this paper, we propose a new architecture, Diagnostic Transformer (DTransformer), along with a new training paradigm, to tackle this challenge. With DTransformer, we build the architecture from question-level to knowledge-level, explicitly diagnosing learner’s knowledge proficiency from each question mastery states. We also propose a novel training algorithm based on contrastive learning that focuses on maintaining the stability of the knowledge state diagnosis. Through extensive experiments, we will show that with its understanding of knowledge state evolution, DTransformer achieves a better performance prediction accuracy and more stable knowledge state tracing results. We will also show that DTransformer is less sensitive to specific patterns with case study. We open-sourced our code and data at https://github.com/yxonic/DTransformer. | Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, Enhong Chen, Xin Li | Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology & School of Data Science, University of Science and Technology of China, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, China; Anhui Province Key Laboratory of Big Data Analysis and Application, School of Data Science, University of Science and Technology of China, China; University of Science and Technology of China, China and iFLYTEK Co., Ltd, China |
| 541 |  |  [Learning to Simulate Daily Activities via Modeling Dynamic Human Needs](https://doi.org/10.1145/3543507.3583276) |  | 0 | Daily activity data that records individuals' various types of activities in daily life are widely used in many applications such as activity scheduling, activity recommendation, and policymaking. Though with high value, its accessibility is limited due to high collection costs and potential privacy issues. Therefore, simulating human activities to produce massive high-quality data is of great importance to benefit practical applications. However, existing solutions, including rule-based methods with simplified assumptions of human behavior and data-driven methods directly fitting real-world data, both cannot fully qualify for matching reality. In this paper, motivated by the classic psychological theory, Maslow's need theory describing human motivation, we propose a knowledge-driven simulation framework based on generative adversarial imitation learning. To enhance the fidelity and utility of the generated activity data, our core idea is to model the evolution of human needs as the underlying mechanism that drives activity generation in the simulation model. Specifically, this is achieved by a hierarchical model structure that disentangles different need levels, and the use of neural stochastic differential equations that successfully captures piecewise-continuous characteristics of need dynamics. Extensive experiments demonstrate that our framework outperforms the state-of-the-art baselines in terms of data fidelity and utility. Besides, we present the insightful interpretability of the need modeling. The code is available at https://github.com/tsinghua-fib-lab/SAND. | Yuan Yuan, Huandong Wang, Jingtao Ding, Depeng Jin, Yong Li | Department of Electronic Engineering, Tsinghua University, China |
| 542 |  |  [Controllable Universal Fair Representation Learning](https://doi.org/10.1145/3543507.3583307) |  | 0 | Learning fair and transferable representations of users that can be used for a wide spectrum of downstream tasks (specifically, machine learning models) has great potential in fairness-aware Web services. Existing studies focus on debiasing w.r.t. a small scale of (one or a handful of) fixed pre-defined sensitive attributes. However, in real practice, downstream data users can be interested in various protected groups and these are usually not known as prior. This requires the learned representations to be fair w.r.t. all possible sensitive attributes. We name this task universal fair representation learning, in which an exponential number of sensitive attributes need to be dealt with, bringing the challenges of unreasonable computational cost and un-guaranteed fairness constraints. To address these problems, we propose a controllable universal fair representation learning (CUFRL) method. An effective bound is first derived via the lens of mutual information to guarantee parity of the universal set of sensitive attributes while maintaining the accuracy of downstream tasks. We also theoretically establish that the number of sensitive attributes that need to be processed can be reduced from exponential to linear. Experiments on two public real-world datasets demonstrate CUFRL can achieve significantly better accuracy-fairness trade-off compared with baseline approaches. | Yue Cui, Ma Chen, Kai Zheng, Lei Chen, Xiaofang Zhou | University of Electronic Science and Technology of China, China; The Hong Kong University of Science and Technology, Hong Kong; City University of Hong Kong, Hong Kong |
| 543 |  |  [Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling](https://doi.org/10.1145/3543507.3583380) |  | 0 | Virtual Mental Health Assistants (VMHAs) have become a prevalent method for receiving mental health counseling in the digital healthcare space. An assistive counseling conversation commences with natural open-ended topics to familiarize the client with the environment and later converges into more fine-grained domain-specific topics. Unlike other conversational systems, which are categorized as open-domain or task-oriented systems, VMHAs possess a hybrid conversational flow. These counseling bots need to comprehend various aspects of the conversation, such as dialogue-acts, intents, etc., to engage the client in an effective conversation. Although the surge in digital health research highlights applications of many general-purpose response generation systems, they are barely suitable in the mental health domain -- the prime reason is the lack of understanding in mental health counseling. Moreover, in general, dialogue-act guided response generators are either limited to a template-based paradigm or lack appropriate semantics. To this end, we propose READER -- a REsponse-Act guided reinforced Dialogue genERation model for the mental health counseling conversations. READER is built on transformer to jointly predict a potential dialogue-act d(t+1) for the next utterance (aka response-act) and to generate an appropriate response u(t+1). Through the transformer-reinforcement-learning (TRL) with Proximal Policy Optimization (PPO), we guide the response generator to abide by d(t+1) and ensure the semantic richness of the responses via BERTScore in our reward computation. We evaluate READER on HOPE, a benchmark counseling conversation dataset and observe that it outperforms several baselines across several evaluation metrics -- METEOR, ROUGE, and BERTScore. We also furnish extensive qualitative and quantitative analyses on results, including error analysis, human evaluation, etc. | Aseem Srivastava, Ishan Pandey, Md. Shad Akhtar, Tanmoy Chakraborty | IIT Delhi, India; IIIT Delhi, India |
| 544 |  |  [Offline Policy Evaluation in Large Action Spaces via Outcome-Oriented Action Grouping](https://doi.org/10.1145/3543507.3583448) |  | 0 | Offline policy evaluation (OPE) aims to accurately estimate the performance of a hypothetical policy using only historical data, which has drawn increasing attention in a wide range of applications including recommender systems and personalized medicine. With the presence of rising granularity of consumer data, many industries started exploring larger action candidate spaces to support more precise personalized action. While inverse propensity score (IPS) is a standard OPE estimator, it suffers from more severe variance issues with increasing action spaces. To address this issue, we theoretically prove that the estimation variance can be reduced by merging actions into groups while the distinction among these action effects on the outcome can induce extra bias. Motivated by these, we propose a novel IPS estimator with outcome-oriented action Grouping (GroupIPS), which leverages a Lipschitz regularized network to measure the distance of action effects in the embedding space and merges nearest action neighbors. This strategy enables more robust estimation by achieving smaller variances while inducing minor additional bias. Empirically, extensive experiments on both synthetic and real world datasets demonstrate the effectiveness of our proposed method. | Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, Peng Cui | Meituan, China; Duke University, USA; Tsinghua University, China |
| 545 |  |  [Web Table Formatting Affects Readability on Mobile Devices](https://doi.org/10.1145/3543507.3583506) |  | 0 | Reading large tables on small mobile screens presents serious usability challenges that can be addressed, in part, by better table formatting. However, there are few evidenced-based guidelines for formatting mobile tables to improve readability. For this work, we first conducted a survey to investigate how people interact with tables on mobile devices and conducted a study with designers to identify which design considerations are most critical. Based on these findings, we designed and conducted three large scale studies with remote crowdworker participants. Across the studies, we analyze over 14,000 trials from 590 participants who each viewed and answered questions about 28 diverse tables rendered in different formats. We find that smaller cell padding and frozen headers lead to faster task completion, and that while zebra striping and row borders do not speed up tasks, they are still subjectively preferred by participants. | Christopher Tensmeyer, Zoya Bylinskii, Tianyuan Cai, Dave Miller, Ani Nenkova, Aleena Gertrudes Niklaus, Shaun Wallace | Brown University, USA; Adobe, USA; Tufts, USA; Adobe Research, USA |
| 546 |  |  [Web Structure Derived Clustering for Optimised Web Accessibility Evaluation](https://doi.org/10.1145/3543507.3583508) |  | 0 | Web accessibility evaluation is a costly and complex process due to limited time, resources and ambiguity. To optimise the accessibility evaluation process, we aim to reduce the number of pages auditors must review by employing statistically representative pages, reducing a site of thousands of pages to a manageable review of archetypal pages. Our paper focuses on representativeness, one of six proposed metrics that form our methodology, to address the limitations we have identified with the W3C Website Accessibility Conformance Evaluation Methodology (WCAG-EM). These include the evaluative scope, the non-probabilistic sampling approach, and the potential for bias within the selected sample. Representativeness, in particular, is a metric to assess the quality and coverage of sampling. To measure this, we systematically evaluate five web page representations with a website of 388 pages, including tags, structure, the DOM tree, content, and a mixture of structure and content. Our findings highlight the importance of including structural components in representations. We validate our conclusions using the same methodology for three additional random sites of 500 pages. As an exclusive attribute, we find that features derived from web content are suboptimal and can lead to lower quality and more disparate clustering for optimised accessibility evaluation. | Alexander Hambley, Yeliz Yesilada, Markel Vigo, Simon Harper | Middle East Technical University Northern Cyprus Campus, Turkey; University of Manchester, United Kingdom |
| 547 |  |  [Hashtag-Guided Low-Resource Tweet Classification](https://doi.org/10.1145/3543507.3583194) |  | 0 | Social media classification tasks (e.g., tweet sentiment analysis, tweet stance detection) are challenging because social media posts are typically short, informal, and ambiguous. Thus, training on tweets is challenging and demands large-scale human-annotated labels, which are time-consuming and costly to obtain. In this paper, we find that providing hashtags to social media tweets can help alleviate this issue because hashtags can enrich short and ambiguous tweets in terms of various information, such as topic, sentiment, and stance. This motivates us to propose a novel Hashtag-guided Tweet Classification model (HashTation), which automatically generates meaningful hashtags for the input tweet to provide useful auxiliary signals for tweet classification. To generate high-quality and insightful hashtags, our hashtag generation model retrieves and encodes the post-level and entity-level information across the whole corpus. Experiments show that HashTation achieves significant improvements on seven low-resource tweet classification tasks, in which only a limited amount of training data is provided, showing that automatically enriching tweets with model-generated hashtags could significantly reduce the demand for large-scale human-labeled data. Further analysis demonstrates that HashTation is able to generate high-quality hashtags that are consistent with the tweets and their labels. The code is available at https://github.com/shizhediao/HashTation. | Shizhe Diao, Sedrick Scott Keh, Liangming Pan, Zhiliang Tian, Yan Song, Tong Zhang | University of Science and Technology of China, China; The Hong Kong University of Science and Technology, Hong Kong; University of California, Santa Barbara, USA; Carnegie Mellon University, USA |
| 548 |  |  [FormerTime: Hierarchical Multi-Scale Representations for Multivariate Time Series Classification](https://doi.org/10.1145/3543507.3583205) |  | 0 | Deep learning-based algorithms, e.g., convolutional networks, have significantly facilitated multivariate time series classification (MTSC) task. Nevertheless, they suffer from the limitation in modeling long-range dependence due to the nature of convolution operations. Recent advancements have shown the potential of transformers to capture long-range dependence. However, it would incur severe issues, such as fixed scale representations, temporal-invariant and quadratic time complexity, with transformers directly applicable to the MTSC task because of the distinct properties of time series data. To tackle these issues, we propose FormerTime, an hierarchical representation model for improving the classification capacity for the MTSC task. In the proposed FormerTime, we employ a hierarchical network architecture to perform multi-scale feature maps. Besides, a novel transformer encoder is further designed, in which an efficient temporal reduction attention layer and a well-informed contextual positional encoding generating strategy are developed. To sum up, FormerTime exhibits three aspects of merits: (1) learning hierarchical multi-scale representations from time series data, (2) inheriting the strength of both transformers and convolutional networks, and (3) tacking the efficiency challenges incurred by the self-attention mechanism. Extensive experiments performed on $10$ publicly available datasets from UEA archive verify the superiorities of the FormerTime compared to previous competitive baselines. | Mingyue Cheng, Qi Liu, Zhiding Liu, Zhi Li, Yucong Luo, Enhong Chen | University of Science and Technology of China, China and State Key Laboratory of Cognitive Intelligence, China; Tsinghua University, China |
| 549 |  |  [HISum: Hyperbolic Interaction Model for Extractive Multi-Document Summarization](https://doi.org/10.1145/3543507.3583197) |  | 0 | Extractive summarization helps provide a short description or a digest of news or other web texts. It enhances the reading experience of users, especially when they are reading on small displays (e.g., mobile phones). Matching-based methods are recently proposed for the extractive summarization task, which extracts a summary from a global view via a document-summary matching framework. However, these methods only calculate similarities between candidate summaries and the entire document embeddings, insufficiently capturing interactions between different contextual information in the document to accurately estimate the importance of candidates. In this paper, we propose a new hyperbolic interaction model for extractive multi-document summarization (HISum). Specifically, HISum first learns document and candidate summary representations in the same hyperbolic space to capture latent hierarchical structures and then estimates the importance scores of candidates by jointly modeling interactions between each candidate and the document from global and local views. Finally, the importance scores are used to rank and extract the best candidate as the extracted summary. Experimental results on several benchmarks show that HISum outperforms the state-of-the-art extractive baselines1. | Mingyang Song, Yi Feng, Liping Jing | Beijing Jiaotong University, China |
| 550 |  |  [Descartes: Generating Short Descriptions of Wikipedia Articles](https://doi.org/10.1145/3543507.3583220) |  | 0 | Wikipedia is one of the richest knowledge sources on the Web today. In order to facilitate navigating, searching, and maintaining its content, Wikipedia's guidelines state that all articles should be annotated with a so-called short description indicating the article's topic (e.g., the short description of beer is "Alcoholic drink made from fermented cereal grains"). Nonetheless, a large fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no short description yet, with detrimental effects for millions of Wikipedia users. Motivated by this problem, we introduce the novel task of automatically generating short descriptions for Wikipedia articles and propose Descartes, a multilingual model for tackling it. Descartes integrates three sources of information to generate an article description in a target language: the text of the article in all its language versions, the already-existing descriptions (if any) of the article in other languages, and semantic type information obtained from a knowledge graph. We evaluate a Descartes model trained for handling 25 languages simultaneously, showing that it beats baselines (including a strong translation-based baseline) and performs on par with monolingual models tailored for specific languages. A human evaluation on three languages further shows that the quality of Descartes's descriptions is largely indistinguishable from that of human-written descriptions; e.g., 91.3% of our English descriptions (vs. 92.1% of human-written descriptions) pass the bar for inclusion in Wikipedia, suggesting that Descartes is ready for production, with the potential to support human editors in filling a major gap in today's Wikipedia across languages. | Marija Sakota, Maxime Peyrard, Robert West | EPFL, Switzerland |
| 551 |  |  [A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking](https://doi.org/10.1145/3543507.3583238) |  | 0 | Dialogue state tracking (DST) module is an important component for task-oriented dialog systems to understand users' goals and needs. Collecting dialogue state labels including slots and values can be costly, especially with the wide application of dialogue systems in more and more new-rising domains. In this paper, we focus on how to utilize the language understanding and generation ability of pre-trained language models for DST. We design a dual prompt learning framework for few-shot DST. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two prompts are designed based on such a dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. Experimental results on two task-oriented dialogue datasets show that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from PLM and utilized to address low-resource DST efficiently with the help of prompt learning. | Yuting Yang, Wenqiang Lei, Pei Huang, Juan Cao, Jintao Li, TatSeng Chua | Stanford University, USA; National University of Singapore, Singapore; Institute of Computing Technology, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China; Sichuan University, China; Institute of Computing Technology, Chinese Academy of Sciences, China |
| 552 |  |  [TTS: A Target-based Teacher-Student Framework for Zero-Shot Stance Detection](https://doi.org/10.1145/3543507.3583250) |  | 0 | The goal of zero-shot stance detection (ZSSD) is to identify the stance (in favor of, against, or neutral) of a text towards an unseen target in the inference stage. In this paper, we explore this problem from a novel angle by proposing a Target-based Teacher-Student learning (TTS) framework. Specifically, we first augment the training set by extracting diversified targets that are unseen during training with a keyphrase generation model. Then, we develop a teacher-student framework which effectively utilizes the augmented data. Extensive experiments show that our model significantly outperforms state-of-the-art ZSSD baselines on the available benchmark dataset for this task by 8.9% in macro-averaged F1. In addition, previous ZSSD requires human-annotated targets and labels during training, which may not be available in real-world applications. Therefore, we go one step further by proposing a more challenging open-world ZSSD task: identifying the stance of a text towards an unseen target without human-annotated targets and stance labels. We show that our TTS can be easily adapted to the new task. Remarkably, TTS without human-annotated targets and stance labels even significantly outperforms previous state-of-the-art ZSSD baselines trained with human-annotated data. We publicly release our code 1 to facilitate future research. | Yingjie Li, Chenye Zhao, Cornelia Caragea | Computer Science, University of Illinois at Chicago, USA |
| 553 |  |  [CL-WSTC: Continual Learning for Weakly Supervised Text Classification on the Internet](https://doi.org/10.1145/3543507.3583249) |  | 0 | Continual text classification is an important research direction in Web mining. Existing works are limited to supervised approaches relying on abundant labeled data, but in the open and dynamic environment of Internet, involving constant semantic change of known topics and the appearance of unknown topics, text annotations are hard to access in time for each period. That calls for the technique of weakly supervised text classification (WSTC), which requires just seed words for each category and has succeed in static text classification tasks. However, there are still no studies of applying WSTC methods in a continual learning paradigm to actually accommodate the open and evolving Internet. In this paper, we tackle this problem for the first time and propose a framework, named Continual Learning for Weakly Supervised Text Classification (CL-WSTC), which can take any WSTC method as base model. It consists of two modules, classification decision with delay and seed word updating. In the former, the probability threshold for each category in each period is adaptively learned to determine the acceptance/rejection of texts. In the latter, with candidate words output by the base model, seed words are added and deleted via reinforcement learning with immediate rewards, according to an empirically certified unsupervised measure. Extensive experiments show that our approach has strong universality and can achieve a better trade-off between classification accuracy and decision timeliness compared to non-continual counterparts, with intuitively interpretable updating of seed words. | Miaomiao Li, Jiaqi Zhu, Xin Yang, Yi Yang, Qiang Gao, Hongan Wang | Institute of Software, Chinese Academy of Sciences, China; Southwestern University of Finance and Economics, China; Institute of Software, Chinese Academy of Sciences, China and University of Chinese Academy of Sciences, China |
| 554 |  |  [Learning Robust Multi-Modal Representation for Multi-Label Emotion Recognition via Adversarial Masking and Perturbation](https://doi.org/10.1145/3543507.3583258) |  | 0 | Recognizing emotions from multi-modal data is an emotion recognition task that requires strong multi-modal representation ability. The general approach to this task is to naturally train the representation model on training data without intervention. However, such natural training scheme is prone to modality bias of representation (i.e., tending to over-encode some informative modalities while neglecting other modalities) and data bias of training (i.e., tending to overfit training data). These biases may lead to instability (e.g., performing poorly when the neglected modality is dominant for recognition) and weak generalization (e.g., performing poorly when unseen data is inconsistent with overfitted data) of the model on unseen data. To address these problems, this paper presents two adversarial training strategies to learn more robust multi-modal representation for multi-label emotion recognition. Firstly, we propose an adversarial temporal masking strategy, which can enhance the encoding of other modalities by masking the most emotion-related temporal units (e.g., words for text or frames for video) of the informative modality. Secondly, we propose an adversarial parameter perturbation strategy, which can enhance the generalization of the model by adding the adversarial perturbation to the parameters of model. Both strategies boost model performance on the benchmark MMER datasets CMU-MOSEI and NEMu. Experimental results demonstrate the effectiveness of the proposed method compared with the previous state-of-the-art method. Code will be released at https://github.com/ShipingGe/MMER. | Shiping Ge, Zhiwei Jiang, Zifeng Cheng, Cong Wang, Yafeng Yin, Qing Gu | State Key Laboratory for Novel Software Technology, Nanjing University, China |
| 555 |  |  [Continual Few-shot Learning with Transformer Adaptation and Knowledge Regularization](https://doi.org/10.1145/3543507.3583262) |  | 0 | Continual few-shot learning, as a paradigm that simultaneously solves continual learning and few-shot learning, has become a challenging problem in machine learning. An eligible continual few-shot learning model is expected to distinguish all seen classes upon new categories arriving, where each category only includes very few labeled data. However, existing continual few-shot learning methods only consider the visual modality, where the distributions of new categories often indistinguishably overlap with old categories, thus resulting in the severe catastrophic forgetting problem. To tackle this problem, in this paper we study continual few-shot learning with the assistance of semantic knowledge by simultaneously taking both visual modality and semantic concepts of categories into account. We propose a Continual few-shot learning algorithm with Semantic knowledge Regularization (CoSR) for adapting to the distribution changes of visual prototypes through a Transformer-based prototype adaptation mechanism. Specifically, the original visual prototypes from the backbone are fed into the well-designed Transformer with corresponding semantic concepts, where the semantic concepts are extracted from all categories. The semantic-level regularization forces the categories with similar semantics to be closely distributed, while the opposite ones are constrained to be far away from each other. The semantic regularization improves the model’s ability to distinguish between new and old categories, thus significantly mitigating the catastrophic forgetting problem in continual few-shot learning. Extensive experiments on CIFAR100, miniImageNet, CUB200 and an industrial dataset with long-tail distribution demonstrate the advantages of our CoSR model compared with state-of-the-art methods. | Xin Wang, Yue Liu, Jiapei Fan, Weigao Wen, Hui Xue, Wenwu Zhu | Alibaba Group, China; Department of Computer Science and Technology, Tsinghua University, China |
| 556 |  |  [Open-World Social Event Classification](https://doi.org/10.1145/3543507.3583291) |  | 0 | With the rapid development of Internet and the expanding scale of social media, social event classification has attracted increasing attention. The key to social event classification is effectively leveraging the visual and textual semantics for classification. However, most of the existing approaches may suffer from the following limitations: (1) Most of them just simply concatenate the image features and text features to get the multimodal features and ignore the fine-grained semantic relationship between modalities. (2) The majority of them hold the closed-world assumption that all classes in test are already seen in training, while this assumption can be easily broken in real-world applications. In practice, new events on Internet may not belong to any existing/seen class, and therefore cannot be correctly identified by closed-world learning algorithms. To tackle these challenges, we propose an Open-World Social Event Classifier (OWSEC) model in this paper. Firstly, we design a multimodal mask transformer network to capture cross-modal semantic relations and fuse fine-grained multimodal features of social events while masking redundant information. Secondly, we design an open-world classifier and propose a cross-modal event mixture mechanism with a novel open-world classification loss to capture the potential distribution space of the unseen class. Extensive experiments on two public datasets demonstrate the superiority of our proposed OWSEC model for open-world social event classification. | Shengsheng Qian, Hong Chen, Dizhan Xue, Quan Fang, Changsheng Xu | Henan Institute of Advanced Technology, Zhengzhou University, China; Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China and Peng Cheng Laboratory, China; Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences, China |
| 557 |  |  [KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction](https://doi.org/10.1145/3543507.3583300) |  | 0 | The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierarchical attention networks (HAN) to learn the relationships among words and sentences in three different levels and (2) knowledge encoding (KE) to incorporate external knowledge for real-world entities into the process of political stance prediction. Also, to take into account the subtle and important difference between opposite political stances, we build two independent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by ourselves and learn to fuse the different political knowledge. Through extensive evaluations on three real-world datasets, we demonstrate the superiority of DASH in terms of (1) accuracy, (2) efficiency, and (3) effectiveness. | YunYong Ko, Seongeun Ryu, Soeun Han, Youngseung Jeon, Jaehoon Kim, Sohyun Park, Kyungsik Han, Hanghang Tong, SangWook Kim | University of Illinois at Urbana-Champaign, USA; Hanyang University, Republic of Korea; Ajou University, Republic of Korea; University of California, Los Angeles, USA |
| 558 |  |  [Improving (Dis)agreement Detection with Inductive Social Relation Information From Comment-Reply Interactions](https://doi.org/10.1145/3543507.3583314) |  | 0 | (Dis)agreement detection aims to identify the authors' attitudes or positions (\textit{{agree, disagree, neutral}}) towards a specific text. It is limited for existing methods merely using textual information for identifying (dis)agreements, especially for cross-domain settings. Social relation information can play an assistant role in the (dis)agreement task besides textual information. We propose a novel method to extract such relation information from (dis)agreement data into an inductive social relation graph, merely using the comment-reply pairs without any additional platform-specific information. The inductive social relation globally considers the historical discussion and the relation between authors. Textual information based on a pre-trained language model and social relation information encoded by pre-trained RGCN are jointly considered for (dis)agreement detection. Experimental results show that our model achieves state-of-the-art performance for both the in-domain and cross-domain tasks on the benchmark -- DEBAGREEMENT. We find social relations can boost the performance of the (dis)agreement detection model, especially for the long-token comment-reply pairs, demonstrating the effectiveness of the social relation graph. We also explore the effect of the knowledge graph embedding methods, the information fusing method, and the time interval in constructing the social relation graph, which shows the effectiveness of our model. | Yun Luo, Zihan Liu, Stan Z. Li, Yue Zhang | Westlake university, China; Westlake University, China |
| 559 |  |  [Dynalogue: A Transformer-Based Dialogue System with Dynamic Attention](https://doi.org/10.1145/3543507.3583330) |  | 0 | Businesses face a range of cyber risks, both external threats and internal vulnerabilities that continue to evolve over time. As cyber attacks continue to increase in complexity and sophistication, more organisations will experience them. For this reason, it is important that organisations seek timely consultancy from cyber professionals so that they can respond to and recover from cyber attacks as quickly as possible. However, huge surges in cyber attacks have long left cyber professionals short of what is required to cover the security needs. This problem is getting worse when an increasing number of people choose to work from home during the pandemic because this situation usually yields extra communication cost. In this paper, we propose to develop a cybersecurity-oriented dialogue system, called Dynalogue1, which can provide consultancy online as a cyber professional. For the first time, Dynalogue provides a promising solution to mitigate the need for cyber professionals via automatically generating problem-targeted conversions to victims of cyber attacks. In spite of many dialogue systems developed in the past, Dynalogue provides a distinct capability of handling long and complicated sentences that are common in cybersecurity-related conversations. It is challenging to have this capability because limited memory in dialogue systems can be hard to accommodate sufficient key information of long sentences. To overcome this challenge, Dynalogue utilises an attention mechanism that dynamically captures key semantics within a sentence instead of using fix window to cut off the sentence. To evaluate Dynalogue, we collect 67K real-world conversations (0.6M utterances) from Bleeping Computer2, which is one of the most popular cybersecurity consultancy websites in the world. The results suggest that Dynalogue outperforms all the existing dialogue systems with 1% ∼ 9% improvements on all different metrics. We further run Dynalogue on the public dataset WikiHow to validate its compatibility in other domains where conversations are also long and complicated. Dynalogue also outperforms all the other methods with at most 2.4% improvement. | Rongjunchen Zhang, Tingmin Wu, Xiao Chen, Sheng Wen, Surya Nepal, Cécile Paris, Yang Xiang | CSIRO's Data61, Australia; Swinburne University of Technology, Australia; Monash University, Australia |
| 560 |  |  [Active Learning from the Web](https://doi.org/10.1145/3543507.3583346) |  | 0 | Labeling data is one of the most costly processes in machine learning pipelines. Active learning is a standard approach to alleviating this problem. Pool-based active learning first builds a pool of unlabelled data and iteratively selects data to be labeled so that the total number of required labels is minimized, keeping the model performance high. Many effective criteria for choosing data from the pool have been proposed in the literature. However, how to build the pool is less explored. Specifically, most of the methods assume that a task-specific pool is given for free. In this paper, we advocate that such a task-specific pool is not always available and propose the use of a myriad of unlabelled data on the Web for the pool for which active learning is applied. As the pool is extremely large, it is likely that relevant data exist in the pool for many tasks, and we do not need to explicitly design and build the pool for each task. The challenge is that we cannot compute the acquisition scores of all data exhaustively due to the size of the pool. We propose an efficient method, Seafaring, to retrieve informative data in terms of active learning from the Web using a user-side information retrieval algorithm. In the experiments, we use the online Flickr environment as the pool for active learning. This pool contains more than ten billion images and is several orders of magnitude larger than the existing pools in the literature for active learning. We confirm that our method performs better than existing approaches of using a small unlabelled pool. | Ryoma Sato | Kyoto University, Japan and RIKEN AIP, Japan |
| 561 |  |  [The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study](https://doi.org/10.1145/3543507.3583354) |  | 0 | Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore their performance change in scientific literature tagging when metadata are fed to the classifiers as additional features. We observe some ubiquitous patterns of metadata's effects across all fields (e.g., venues are consistently beneficial to paper tagging in almost all cases), as well as some unique patterns in fields other than computer science and biomedicine, which are not explored in previous studies. | Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, Jiawei Han | University of Illinois at Urbana-Champaign, USA |
| 562 |  |  ["Why is this misleading?": Detecting News Headline Hallucinations with Explanations](https://doi.org/10.1145/3543507.3583375) |  | 0 | Automatic headline generation enables users to comprehend ongoing news events promptly and has recently become an important task in web mining and natural language processing. With the growing need for news headline generation, we argue that the hallucination issue, namely the generated headlines being not supported by the original news stories, is a critical challenge for the deployment of this feature in web-scale systems Meanwhile, due to the infrequency of hallucination cases and the requirement of careful reading for raters to reach the correct consensus, it is difficult to acquire a large dataset for training a model to detect such hallucinations through human curation. In this work, we present a new framework named ExHalder to address this challenge for headline hallucination detection. ExHalder adapts the knowledge from public natural language inference datasets into the news domain and learns to generate natural language sentences to explain the hallucination detection results. To evaluate the model performance, we carefully collect a dataset with more than six thousand labeled <article, headline> pairs. Extensive experiments on this dataset and another six public ones demonstrate that ExHalder can identify hallucinated headlines accurately and justifies its predictions with human-readable natural language explanations. | Jiaming Shen, Jialu Liu, Daniel Finnie, Negar Rahmati, Mike Bendersky, Marc Najork | Google, USA |
| 563 |  |  [DIWIFT: Discovering Instance-wise Influential Features for Tabular Data](https://doi.org/10.1145/3543507.3583382) |  | 0 | Tabular data is one of the most common data storage formats behind many real-world web applications such as retail, banking, and e-commerce. The success of these web applications largely depends on the ability of the employed machine learning model to accurately distinguish influential features from all the predetermined features in tabular data. Intuitively, in practical business scenarios, different instances should correspond to different sets of influential features, and the set of influential features of the same instance may vary in different scenarios. However, most existing methods focus on global feature selection assuming that all instances have the same set of influential features, and few methods considering instance-wise feature selection ignore the variability of influential features in different scenarios. In this paper, we first introduce a new perspective based on the influence function for instance-wise feature selection, and give some corresponding theoretical insights, the core of which is to use the influence function as an indicator to measure the importance of an instance-wise feature. We then propose a new solution for discovering instance-wise influential features in tabular data (DIWIFT), where a self-attention network is used as a feature selection model and the value of the corresponding influence function is used as an optimization objective to guide the model. Benefiting from the advantage of the influence function, i.e., its computation does not depend on a specific architecture and can also take into account the data distribution in different scenarios, our DIWIFT has better flexibility and robustness. Finally, we conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our DIWIFT. | Dugang Liu, Pengxiang Cheng, Hong Zhu, Xing Tang, Yanyu Chen, Xiaoting Wang, Weike Pan, Zhong Ming, Xiuqiang He | College of Computer Science and Software Engineering, Shenzhen University, China; Shenzhen University, China; Fit, Tencent, China; FIT, Tencent, China; Huawei Technologies Co Ltd, China; Tsinghua-Berkeley Shenzhen Institute, China |
| 564 |  |  [XWikiGen: Cross-lingual Summarization for Encyclopedic Text Generation in Low Resource Languages](https://doi.org/10.1145/3543507.3583405) |  | 0 | Lack of encyclopedic text contributors, especially on Wikipedia, makes automated text generation for low resource (LR) languages a critical problem. Existing work on Wikipedia text generation has focused on English only where English reference articles are summarized to generate English Wikipedia pages. But, for low-resource languages, the scarcity of reference articles makes monolingual summarization ineffective in solving this problem. Hence, in this work, we propose XWikiGen, which is the task of cross-lingual multi-document summarization of text from multiple reference articles, written in various languages, to generate Wikipedia-style text. Accordingly, we contribute a benchmark dataset, XWikiRef, spanning ~69K Wikipedia articles covering five domains and eight languages. We harness this dataset to train a two-stage system where the input is a set of citations and a section title and the output is a section-specific LR summary. The proposed system is based on a novel idea of neural unsupervised extractive summarization to coarsely identify salient information followed by a neural abstractive model to generate the section-specific text. Extensive experiments show that multi-domain training is better than the multi-lingual setup on average. | Dhaval Taunk, Shivprasad Sagare, Anupam Patil, Shivansh Subramanian, Manish Gupta, Vasudeva Varma | International Institute of Information Technology, Hyderabad, India; International Institute of Information Technology, Hyderabad, India and Microsoft India, India; SCTR's Pune Institute of Computer Technology, India |
| 565 |  |  [Learning Structural Co-occurrences for Structured Web Data Extraction in Low-Resource Settings](https://doi.org/10.1145/3543507.3583387) |  | 0 | Extracting structured information from all manner of webpages is an important problem with the potential to automate many real-world applications. Recent work has shown the effectiveness of leveraging DOM trees and pre-trained language models to describe and encode webpages. However, they typically optimize the model to learn the semantic co-occurrence of elements and labels in the same webpage, thus their effectiveness depends on sufficient labeled data, which is labor-intensive. In this paper, we further observe structural co-occurrences in different webpages of the same website: the same position in the DOM tree usually plays the same semantic role, and the DOM nodes in this position also share similar surface forms. Motivated by this, we propose a novel method, Structor, to effectively incorporate the structural co-occurrences over DOM tree and surface form into pre-trained language models. Such structural co-occurrences help the model learn the task better under low-resource settings, and we study two challenging experimental scenarios: website-level low-resource setting and webpage-level low-resource setting, to evaluate our approach. Extensive experiments on the public SWDE dataset show that Structor significantly outperforms the state-of-the-art models in both settings, and even achieves three times the performance of the strong baseline model in the case of extreme lack of training data. | Zhenyu Zhang, Bowen Yu, Tingwen Liu, Tianyun Liu, Yubin Wang, Li Guo | Institute of Information Engineering, Chinese Academy of Sciences, China and School of Cyber Security, University of Chinese Academy of Sciences, China |
| 566 |  |  [TMMDA: A New Token Mixup Multimodal Data Augmentation for Multimodal Sentiment Analysis](https://doi.org/10.1145/3543507.3583406) |  | 0 | Existing methods for Multimodal Sentiment Analysis (MSA) mainly focus on integrating multimodal data effectively on limited multimodal data. Learning more informative multimodal representation often relies on large-scale labeled datasets, which are difficult and unrealistic to obtain. To learn informative multimodal representation on limited labeled datasets as more as possible, we proposed TMMDA for MSA, a new Token Mixup Multimodal Data Augmentation, which first generates new virtual modalities from the mixed token-level representation of raw modalities, and then enhances the representation of raw modalities by utilizing the representation of the generated virtual modalities. To preserve semantics during virtual modality generation, we propose a novel cross-modal token mixup strategy based on the generative adversarial network. Extensive experiments on two benchmark datasets, i.e., CMU-MOSI and CMU-MOSEI, verify the superiority of our model compared with several state-of-the-art baselines. The code is available at https://github.com/xiaobaicaihhh/TMMDA. | Xianbing Zhao, Yixin Chen, Sicen Liu, Xuan Zang, Yang Xiang, Buzhou Tang | Peng Cheng Laboratory, Shenzhen, China, China; Harbin Institute of Technology (Shenzhen), China |
| 567 |  |  [Node-wise Diffusion for Scalable Graph Learning](https://doi.org/10.1145/3543507.3583408) |  | 0 | Graph Neural Networks (GNNs) have shown superior performance for semi-supervised learning of numerous web applications, such as classification on web services and pages, analysis of online social networks, and recommendation in e-commerce. The state of the art derives representations for all nodes in graphs following the same diffusion (message passing) model without discriminating their uniqueness. However, (i) labeled nodes involved in model training usually account for a small portion of graphs in the semi-supervised setting, and (ii) different nodes locate at different graph local contexts and it inevitably degrades the representation qualities if treating them undistinguishedly in diffusion. To address the above issues, we develop NDM, a universal node-wise diffusion model, to capture the unique characteristics of each node in diffusion, by which NDM is able to yield high-quality node representations. In what follows, we customize NDM for semi-supervised learning and design the NIGCN model. In particular, NIGCN advances the efficiency significantly since it (i) produces representations for labeled nodes only and (ii) adopts well-designed neighbor sampling techniques tailored for node representation generation. Extensive experimental results on various types of web datasets, including citation, social and co-purchasing graphs, not only verify the state-of-the-art effectiveness of NIGCN but also strongly support the remarkable scalability of NIGCN. In particular, NIGCN completes representation generation and training within 10 seconds on the dataset with hundreds of millions of nodes and billions of edges, up to orders of magnitude speedups over the baselines, while achieving the highest F1-scores on classification. | Keke Huang, Jing Tang, Juncheng Liu, Renchi Yang, Xiaokui Xiao | [email protected]; National University of Singapore, Singapore; Hong Kong Baptist University, Hong Kong; The Hong Kong University of Science and Technology (Guangzhou), China and The Hong Kong Uni. of Sci. and Tech., Hong Kong |
| 568 |  |  [MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters](https://doi.org/10.1145/3543507.3583417) |  | 0 | State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g.\ the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with {\em novel} campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce \textit{campaign-specific} transformer adapters to MetaTroll to \`\`memorise'' campaign-specific knowledge so as to tackle catastrophic forgetting, where a model \`\`forgets'' how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git. | Lin Tian, Xiuzhen Zhang, Jey Han Lau | The University of Melbourne, Australia; RMIT University, Australia |
| 569 |  |  [EmpMFF: A Multi-factor Sequence Fusion Framework for Empathetic Response Generation](https://doi.org/10.1145/3543507.3583438) |  | 0 | Empathy is one of the fundamental abilities of dialog systems. In order to build more intelligent dialogue systems, it’s important to learn how to demonstrate empathy toward others. Existing studies focus on identifying and leveraging the user’s coarse emotion to generate empathetic responses. However, human emotion and dialog act (e.g., intent) evolve as the talk goes along in an empathetic dialogue. This leads to the generated responses with very different intents from the human responses. As a result, empathy failure is ultimately caused. Therefore, using fine-grained emotion and intent sequential data on conversational emotions and dialog act is crucial for empathetic response generation. On the other hand, existing empathy models overvalue the empathy of responses while ignoring contextual relevance, which results in repetitive model-generated responses. To address these issues, we propose a Multi-Factor sequence Fusion framework (EmpMFF) based on conditional variational autoencoder. To generate empathetic responses, the proposed EmpMFF encodes a combination of contextual, emotion, and intent information into a continuous latent variable, which is then fed into the decoder. Experiments on the EmpatheticDialogues benchmark dataset demonstrate that EmpMFF exhibits exceptional performance in both automatic and human evaluations. | Xiaobing Pang, Yequan Wang, Siqi Fan, Lisi Chen, Shuo Shang, Peng Han | University Of Electronic Science And Technology Of China, China; Beijing Academy of Artificial Intelligence, China |
| 570 |  |  [CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering](https://doi.org/10.1145/3543507.3583457) |  | 0 | Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contrastive Clustering (CDCC) algorithm. After strict data filtering and aggregation processes, samples with clean category labels are retrieved, which serve as supervision information to update the language model with the classification objective via a prompt learning approach. Finally, the updated language model with improved representation ability is used to enhance clustering in the next iteration. Extensive experiments demonstrate that the CEIL framework significantly improves the clustering performance over iterations, and is generally effective on various clustering algorithms. Moreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art clustering performance on a wide range of short text clustering benchmarks outperforming other strong baseline methods. | Mingjun Zhao, Mengzhen Wang, Yinglong Ma, Di Niu, Haijiang Wu | Electrical & Computer Engineering, University of Alberta, Canada; DiDi Global, China; North China Electric Power University, China |
| 571 |  |  [Interval-censored Transformer Hawkes: Detecting Information Operations using the Reaction of Social Systems](https://doi.org/10.1145/3543507.3583481) |  | 0 | Social media is being increasingly weaponized by state-backed actors to elicit reactions, push narratives and sway public opinion. These are known as Information Operations (IO). The covert nature of IO makes their detection difficult. This is further amplified by missing data due to the user and content removal and privacy requirements. This work advances the hypothesis that the very reactions that Information Operations seek to elicit within the target social systems can be used to detect them. We propose an Interval-censored Transformer Hawkes (IC-TH) architecture and a novel data encoding scheme to account for both observed and missing data. We derive a novel log-likelihood function that we deploy together with a contrastive learning procedure. We showcase the performance of IC-TH on three real-world Twitter datasets and two learning tasks: future popularity prediction and item category prediction. The latter is particularly significant. Using the retweeting timing and patterns solely, we can predict the category of YouTube videos, guess whether news publishers are reputable or controversial and, most importantly, identify state-backed IO agent accounts. Additional qualitative investigations uncover that the automatically discovered clusters of Russian-backed agents appear to coordinate their behavior, activating simultaneously to push specific narratives. | Quyu Kong, Pio Calderon, Rohit Ram, Olga Boichak, MarianAndrei Rizoiu | University of Technology, Sydney, Australia; The University of Sydney, Australia; Alibaba Group, China and University of Technology, Sydney, Australia |
| 572 |  |  [Towards Model Robustness: Generating Contextual Counterfactuals for Entities in Relation Extraction](https://doi.org/10.1145/3543507.3583504) |  | 0 | The goal of relation extraction (RE) is to extract the semantic relations between/among entities in the text. As a fundamental task in information systems, it is crucial to ensure the robustness of RE models. Despite the high accuracy current deep neural models have achieved in RE tasks, they are easily affected by spurious correlations. One solution to this problem is to train the model with counterfactually augmented data (CAD) such that it can learn the causation rather than the confounding. However, no attempt has been made on generating counterfactuals for RE tasks. In this paper, we formulate the problem of automatically generating CAD for RE tasks from an entity-centric viewpoint, and develop a novel approach to derive contextual counterfactuals for entities. Specifically, we exploit two elementary topological properties, i.e., the centrality and the shortest path, in syntactic and semantic dependency graphs, to first identify and then intervene on the contextual causal features for entities. We conduct a comprehensive evaluation on four RE datasets by combining our proposed approach with a variety of RE backbones. Results prove that our approach not only improves the performance of the backbones but also makes them more robust in the out-of-domain test 1. | Mi Zhang, Tieyun Qian, Ting Zhang, Xin Miao | School of Computer Science, Wuhan University, China |
| 573 |  |  [CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization](https://doi.org/10.1145/3543507.3583505) |  | 0 | Citation graphs can be helpful in generating high-quality summaries of scientific papers, where references of a scientific paper and their correlations can provide additional knowledge for contextualising its background and main contributions. Despite the promising contributions of citation graphs, it is still challenging to incorporate them into summarization tasks. This is due to the difficulty of accurately identifying and leveraging relevant content in references for a source paper, as well as capturing their correlations of different intensities. Existing methods either ignore references or utilize only abstracts indiscriminately from them, failing to tackle the challenge mentioned above. To fill that gap, we propose a novel citation-aware scientific paper summarization framework based on citation graphs, able to accurately locate and incorporate the salient contents from references, as well as capture varying relevance between source papers and their references. Specifically, we first build a domain-specific dataset PubMedCite with about 192K biomedical scientific papers and a large citation graph preserving 917K citation relationships between them. It is characterized by preserving the salient contents extracted from full texts of references, and the weighted correlation between the salient contents of references and the source paper. Based on it, we design a self-supervised citation-aware summarization framework (CitationSum) with graph contrastive learning, which boosts the summarization generation by efficiently fusing the salient information in references with source paper contents under the guidance of their correlations. Experimental results show that our model outperforms the state-of-the-art methods, due to efficiently leveraging the information of references and citation correlations. | Zheheng Luo, Qianqian Xie, Sophia Ananiadou | The University of Manchester, United Kingdom; University of Manchester, United Kingdom |
| 574 |  |  [Set in Stone: Analysis of an Immutable Web3 Social Media Platform](https://doi.org/10.1145/3543507.3583510) |  | 0 | There has been growing interest in the so-called “Web3” movement. This loosely refers to a mix of decentralized technologies, often underpinned by blockchain technologies. Among these, Web3 social media platforms have begun to emerge. These store all social interaction data (e.g., posts) on a public ledger, removing the need for centralized data ownership and management. But this comes at a cost, which some argue is prohibitively expensive. As an exemplar within this growing ecosytem, we explore memo.cash, a microblogging service built on the Bitcoin Cash (BCH) blockchain. We gather data for 24K users, 317K posts, 2.57M user actions, which have facilitated $6.75M worth of transactions. A particularly unique feature is that users must pay BCH tokens for each interaction (e.g., posting, following). We study how this may impact the social makeup of the platform. We therefore study memo.cash as both a social network and a transaction platform. | Wenrui Zuo, Aravindh Raman, Raul J. Mondragón, Gareth Tyson | Hong Kong University of Science and Technology, Hong Kong; Queen Mary University of London, United Kingdom; Telefónica Research, Spain |
| 575 |  |  [Show me your NFT and I tell you how it will perform: Multimodal representation learning for NFT selling price prediction](https://doi.org/10.1145/3543507.3583520) |  | 0 | Non-Fungible Tokens (NFTs) represent deeds of ownership, based on blockchain technologies and smart contracts, of unique crypto assets on digital art forms (e.g., artworks or collectibles). In the spotlight after skyrocketing in 2021, NFTs have attracted the attention of crypto enthusiasts and investors intent on placing promising investments in this profitable market. However, the NFT financial performance prediction has not been widely explored to date. In this work, we address the above problem based on the hypothesis that NFT images and their textual descriptions are essential proxies to predict the NFT selling prices. To this purpose, we propose MERLIN, a novel multimodal deep learning framework designed to train Transformer-based language and visual models, along with graph neural network models, on collections of NFTs' images and texts. A key aspect in MERLIN is its independence on financial features, as it exploits only the primary data a user interested in NFT trading would like to deal with, i.e., NFT images and textual descriptions. By learning dense representations of such data, a price-category classification task is performed by MERLIN models, which can also be tuned according to user preferences in the inference phase to mimic different risk-return investment profiles. Experimental evaluation on a publicly available dataset has shown that MERLIN models achieve significant performances according to several financial assessment criteria, fostering profitable investments, and also beating baseline machine-learning classifiers based on financial features. | Davide Costa, Lucio La Cava, Andrea Tagarelli | DIMES - Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering, University of Calabria, Italy |
| 576 |  |  [CoTel: Ontology-Neural Co-Enhanced Text Labeling](https://doi.org/10.1145/3543507.3583533) |  | 0 | The success of many web services relies on the large-scale domain-specific high-quality labeled dataset. Insufficient public datasets motivate us to reduce the cost of data labeling while maintaining high accuracy in support of intelligent web applications. The rule-based method and the learning-based method are common techniques for labeling. In this work, we study how to utilize the rule-based and learning-based methods for resource-effective text labeling. We propose CoTel, the first ontology-neural co-enhanced framework for text labeling. We propose critical ontology extraction in the rule-based module and ontology-enhanced loss prediction in the learning-based module. CoTel can integrate explicit labeling rules and implicit labeling models and make them help each other to improve resource efficiency in text labeling tasks. We evaluate CoTel on both public datasets and real applications with three different tasks. Compared with the baseline, CoTel can reduce the time cost by 64.75% (a 2.84× speedup) and the number of labeling by 62.07%. | MiaoHui Song, Lan Zhang, Mu Yuan, Zichong Li, Qi Song, Yijun Liu, Guidong Zheng | University of Science and Technology of China, China and Institute of Artificial Intelligence, Hefei Comprehensive National Science Center, China; University of Science and Technology of China, China; China Merchants Bank, China |
| 577 |  |  [Extracting Cultural Commonsense Knowledge at Scale](https://doi.org/10.1145/3543507.3583535) |  | 0 | Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents CANDLE, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. CANDLE extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). CANDLE includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the CANDLE CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and data can be accessed at https://candle.mpi-inf.mpg.de/. | TuanPhong Nguyen, Simon Razniewski, Aparna S. Varde, Gerhard Weikum | Max Planck Institute for Informatics, Germany; Montclair State University, USA |
| 578 |  |  [Unsupervised Event Chain Mining from Multiple Documents](https://doi.org/10.1145/3543507.3583295) |  | 0 | Massive and fast-evolving news articles keep emerging on the web. To effectively summarize and provide concise insights into real-world events, we propose a new event knowledge extraction task Event Chain Mining in this paper. Given multiple documents about a super event, it aims to mine a series of salient events in temporal order. For example, the event chain of super event Mexico Earthquake in 2017 is {earthquake hit Mexico, destroy houses, kill people, block roads}. This task can help readers capture the gist of texts quickly, thereby improving reading efficiency and deepening text comprehension. To address this task, we regard an event as a cluster of different mentions of similar meanings. In this way, we can identify the different expressions of events, enrich their semantic knowledge and replenish relation information among them. Taking events as the basic unit, we present a novel unsupervised framework, EMiner. Specifically, we extract event mentions from texts and merge them with similar meanings into a cluster as a single event. By jointly incorporating both content and commonsense, essential events are then selected and arranged chronologically to form an event chain. Meanwhile, we annotate a multi-document benchmark to build a comprehensive testbed for the proposed task. Extensive experiments are conducted to verify the effectiveness of EMiner in terms of both automatic and human evaluations. | Yizhu Jiao, Ming Zhong, Jiaming Shen, Yunyi Zhang, Chao Zhang, Jiawei Han | Georgia Institute of Technology, USA; University of Illinois Urbana-Champaign, USA; Google Research, USA |
| 579 |  |  [A Multi-view Meta-learning Approach for Multi-modal Response Generation](https://doi.org/10.1145/3543507.3583548) |  | 0 | As massive conversation examples are easily accessible on the Internet, we are now able to organize large-scale conversation corpora to build chatbots in a data-driven manner. Multi-modal social chatbots produce conversational utterances according to both textual utterances and vision signals. Due to the difficulty of bridging different modalities, the dialogue generation model of chatbots falls into local minima that only capture the mapping between textual input and textual output, as a result, it almost ignores the non-textual signals. Further, similar to the dialogue model with plain text as input and output, the generated responses from multi-modal dialogue also lack diversity and informativeness. In this paper, to address the above issues, we propose a Multi-View Meta-Learning (MultiVML) algorithm that groups samples in multiple views and customizes generation models to different groups. We employ a multi-view clustering to group the training samples so as to attend more to the unique information in non-textual modality. Tailoring different sets of model parameters for each group boosts the genereation diversity via meta-learning. We evaluate MultiVML on two variants of the OpenViDial benchmark datasets. The experiments show that our model not only better explore the information from multiple modalities, but also excels baselines in both quality and diversity. | Zhiliang Tian, Zheng Xie, Fuqiang Lin, Yiping Song | National University of Defense Technology, China |
| 580 |  |  [Provenance of Training without Training Data: Towards Privacy-Preserving DNN Model Ownership Verification](https://doi.org/10.1145/3543507.3583198) |  | 0 | In the era of deep learning, it is critical to protect the intellectual property of high-performance deep neural network (DNN) models. Existing proposals, however, are subject to adversarial ownership forgery (e.g., methods based on watermarks or fingerprints) or require full access to the original training dataset for ownership verification (e.g., methods requiring the replay of the learning process). In this paper, we propose a novel Provenance of Training (PoT) scheme, the first empirical study towards verifying DNN model ownership without accessing any original dataset while being robust against existing attacks. At its core, PoT relies on a coherent model chain built from the intermediate checkpoints saved during model training to serve as the ownership certificate. Through an in-depth analysis of model training, we propose six key properties that a legitimate model chain shall naturally hold. In contrast, it is difficult for the adversary to forge a model chain that satisfies these properties simultaneously without performing actual training. We systematically analyze PoT’s robustness against various possible attacks, including the adaptive attacks that are designed given the full knowledge of PoT’s design, and further perform extensive empirical experiments to demonstrate our security analysis. | Yunpeng Liu, Kexin Li, Zhuotao Liu, Bihan Wen, Ke Xu, Weiqiang Wang, Wenbiao Zhao, Qi Li | Tsinghua University, China and Zhongguancun Laboratory, China; Nanyang Technological University, Singapore; Ant Group, China; University of Toronto, Canada; Tsinghua University, China |
| 581 |  |  [Efficient and Low Overhead Website Fingerprinting Attacks and Defenses based on TCP/IP Traffic](https://doi.org/10.1145/3543507.3583200) |  | 0 | Website fingerprinting attack is an extensively studied technique used in a web browser to analyze traffic patterns and thus infer confidential information about users. Several website fingerprinting attacks based on machine learning and deep learning tend to use the most typical features to achieve a satisfactory performance of attacking rate. However, these attacks suffer from several practical implementation factors, such as a skillfully pre-processing step or a clean dataset. To defend against such attacks, random packet defense (RPD) with a high cost of excessive network overhead is usually applied. In this work, we first propose a practical filter-assisted attack against RPD, which can filter out the injected noises using the statistical characteristics of TCP/IP traffic. Then, we propose a list-assisted defensive mechanism to defend the proposed attack method. To achieve a configurable trade-off between the defense and the network overhead, we further improve the list-based defense by a traffic splitting mechanism, which can combat the mentioned attacks as well as save a considerable amount of network overhead. In the experiments, we collect real-life traffic patterns using three mainstream browsers, i.e., Microsoft Edge, Google Chrome, and Mozilla Firefox, and extensive results conducted on the closed and open-world datasets show the effectiveness of the proposed algorithms in terms of defense accuracy and network efficiency. | Guodong Huang, Chuan Ma, Ming Ding, Yuwen Qian, Chunpeng Ge, Liming Fang, Zhe Liu | Shandong Univerisity, China; Nanjing University of Science and Technology, China; Zhejiang Lab, China; Data 61, CSIRO, Sydney, Australia; Nanjing University of Aeronautics and Astronautics, China |
| 582 |  |  [Curriculum Graph Poisoning](https://doi.org/10.1145/3543507.3583211) |  | 0 | Despite the success of graph neural networks (GNNs) over the Web in recent years, the typical transductive learning setting for node classification requires GNNs to be retrained frequently, making them vulnerable to poisoning attacks by corrupting the training graph. Poisoning attacks on graphs are, however, non-trivial as the attack space is potentially large, and the discrete graph structure makes the poisoning function non-differentiable. In this paper, we revisit the bi-level optimization problem in graph poisoning and propose a novel graph poisoning method, termed Curriculum Graph Poisoning (CuGPo), inspired by curriculum learning. In contrast to other poisoning attacks that use heuristics or directly optimize the graph, our method learns to generate poisoned graphs from basic adversarial knowledge first and advanced knowledge later. Specifically, for the outer optimization, we utilize the slightly perturbed graphs which represent the easy poisoning task at the beginning, and then enlarge the attack space until the final; for the inner optimization, we firstly exploit the knowledge from the clean graph and then adapt quickly to perturbed graphs to obtain the adversarial knowledge. Extensive experiments demonstrate that CuGPo achieves state-of-the-art performance in graph poisoning attacks. | Hanwen Liu, Peilin Zhao, Tingyang Xu, Yatao Bian, Junzhou Huang, Yuesheng Zhu, Yadong Mu | Peking University, China; Tencent AI Lab, China; University of Texas at Arlington, USA |
| 583 |  |  [Transferring Audio Deepfake Detection Capability across Languages](https://doi.org/10.1145/3543507.3583222) |  | 0 | The proliferation of deepfake content has motivated a surge of detection studies. However, existing detection methods in the audio area exclusively work in English, and there is a lack of data resources in other languages. Cross-lingual deepfake detection, a critical but rarely explored area, urges more study. This paper conducts the first comprehensive study on the cross-lingual perspective of deepfake detection. We observe that English data enriched in deepfake algorithms can teach a detector the knowledge of various spoofing artifacts, contributing to performing detection across language domains. Based on the observation, we first construct a first-of-its-kind cross-lingual evaluation dataset including heterogeneous spoofed speech uttered in the two most widely spoken languages, then explored domain adaptation (DA) techniques to transfer the artifacts detection capability and propose effective and practical DA strategies fitting the cross-lingual scenario. Our adversarial-based DA paradigm teaches the model to learn real/fake knowledge while losing language dependency. Extensive experiments over 137-hour audio clips validate the adapted models can detect fake audio generated by unseen algorithms in the new domain. | Zhongjie Ba, Qing Wen, Peng Cheng, Yuwei Wang, Feng Lin, Li Lu, Zhenguang Liu | Zhejiang University, China and ZJU-Hangzhou Global Scientific and Technological Innovation Center, China |
| 584 |  |  [Web Photo Source Identification based on Neural Enhanced Camera Fingerprint](https://doi.org/10.1145/3543507.3583225) |  | 0 | With the growing popularity of smartphone photography in recent years, web photos play an increasingly important role in all walks of life. Source camera identification of web photos aims to establish a reliable linkage from the captured images to their source cameras, and has a broad range of applications, such as image copyright protection, user authentication, investigated evidence verification, etc. This paper presents an innovative and practical source identification framework that employs neural-network enhanced sensor pattern noise to trace back web photos efficiently while ensuring security. Our proposed framework consists of three main stages: initial device fingerprint registration, fingerprint extraction and cryptographic connection establishment while taking photos, and connection verification between photos and source devices. By incorporating metric learning and frequency consistency into the deep network design, our proposed fingerprint extraction algorithm achieves state-of-the-art performance on modern smartphone photos for reliable source identification. Meanwhile, we also propose several optimization sub-modules to prevent fingerprint leakage and improve accuracy and efficiency. Finally for practical system design, two cryptographic schemes are introduced to reliably identify the correlation between registered fingerprint and verified photo fingerprint, i.e. fuzzy extractor and zero-knowledge proof (ZKP). The codes for fingerprint extraction network and benchmark dataset with modern smartphone cameras photos are all publicly available at https://github.com/PhotoNecf/PhotoNecf. | Feng Qian, Sifeng He, Honghao Huang, Huanyu Ma, Xiaobo Zhang, Lei Yang | Ant Group, China |
| 585 |  |  [TFE-GNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification](https://doi.org/10.1145/3543507.3583227) |  | 0 | Encrypted traffic classification is receiving widespread attention from researchers and industrial companies. However, the existing methods only extract flow-level features, failing to handle short flows because of unreliable statistical properties, or treat the header and payload equally, failing to mine the potential correlation between bytes. Therefore, in this paper, we propose a byte-level traffic graph construction approach based on point-wise mutual information (PMI), and a model named Temporal Fusion Encoder using Graph Neural Networks (TFE-GNN) for feature extraction. In particular, we design a dual embedding layer, a GNN-based traffic graph encoder as well as a cross-gated feature fusion mechanism, which can first embed the header and payload bytes separately and then fuses them together to obtain a stronger feature representation. The experimental results on two real datasets demonstrate that TFE-GNN outperforms multiple state-of-the-art methods in fine-grained encrypted traffic classification tasks. | Haozhen Zhang, Le Yu, Xi Xiao, Qing Li, Francesco Mercaldo, Xiapu Luo, Qixu Liu | Peng Cheng Laboratory, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Institute of Information Engineering, Chinese Academy of Sciences, China; University of Molise, Italy and IIT-CNR, Italy; Shenzhen International Graduate School, Tsinghua University, China |
| 586 |  |  [Time-manipulation Attack: Breaking Fairness against Proof of Authority Aura](https://doi.org/10.1145/3543507.3583252) |  | 0 | As blockchain-based commercial projects and startups flourish, efficiency becomes one of the critical metrics in designing blockchain systems. Due to its high efficiency, Proof of Authority (PoA) Aura has become one of the most widely adopted consensus solutions for blockchains. Our research finds over 4,000 projects have used Aura and its variants. In this paper, we provide a rigorous analysis of Aura. We propose three types of time-manipulation attacks, where a malicious leader simply needs to modify the timestamp in its proposed block or delay it to extract extra benefits. These attacks can easily break the legal leader election, thus directly harming the fairness of the block proposal. We apply our attacks to a mature Aura project called OpenEthereum. By repeatedly conducting our attacks1 over 15 days, we find that an adversary can gain on average 200% mining rewards of their fair shares. Furthermore, such attacks can even indirectly break the finality of blocks and the safety of the system. Based on the deployment of Aura as of September 2022, the potentially affected market cap is up to 2.13 billion USD. As a by-product, we further discuss solutions to mitigate such issues and report our observations to official teams. | Xinrui Zhang, Rujia Li, Qin Wang, Qi Wang, Sisi Duan | Institute for Advanced Study, Tsinghua University, China; CSIRO Data61, Australia; Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China; Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China and School of Computer Science, The University of Sydney, Australia; Research Institute of Trustworthy Autonomous Systems, Department of Computer Science and Engineering, Southern University of Science and Technology, China and Institute for Advanced Study, Tsinghua University, China |
| 587 |  |  [Do NFTs' Owners Really Possess their Assets? A First Look at the NFT-to-Asset Connection Fragility](https://doi.org/10.1145/3543507.3583281) |  | 0 | NFTs (Non-Fungible Tokens) have experienced an explosive growth and their record-breaking prices have been witnessed. Typically, the assets that NFTs represent are stored off-chain with a pointer, e.g., multi-hop URLs, due to the costly on-chain storage. Hence, this paper aims to answer the question: Is the NFT-to-Asset connection fragile? This paper makes a first step towards this end by characterizing NFT-to-Asset connections of 12,353 Ethereum NFT Contracts (6,234,141 NFTs in total) from three perspectives, storage, accessibility and duplication. In order to overcome challenges of affecting the measurement accuracy, e.g., IPFS instability and the changing availability of both IPFS and servers' data, we propose to leverage multiple gateways to enlarge the data coverage and extend a longer measurement period with non-trivial efforts. Results of our extensive study show that such connection is very fragile in practice. The loss, unavailability, or duplication of off-chain assets could render value of NFTs worthless. For instance, we find that assets of 25.24% of Ethereum NFT contracts are not accessible, and 21.48% of Ethereum NFT contracts include duplicated assets. Our work sheds light on the fragility along the NFT-to-Asset connection, which could help the NFT community to better enhance the trust of off-chain assets. | Ziwei Wang, Jiashi Gao, Xuetao Wei | Southern University of Science and Technology, China |
| 588 |  |  [Preserving Missing Data Distribution in Synthetic Data](https://doi.org/10.1145/3543507.3583297) |  | 0 | Data from Web artifacts and from the Web is often sensitive and cannot be directly shared for data analysis. Therefore, synthetic data generated from the real data is increasingly used as a privacy-preserving substitute. In many cases, real data from the web has missing values where the missingness itself possesses important informational content, which domain experts leverage to improve their analysis. However, this information content is lost if either imputation or deletion is used before synthetic data generation. In this paper, we propose several methods to generate synthetic data that preserve both the observable and the missing data distributions. An extensive empirical evaluation over a range of carefully fabricated and real world datasets demonstrates the effectiveness of our approach. | Xinyue Wang, Hafiz Salman Asif, Jaideep Vaidya | Rutgers University, USA |
| 589 |  |  [Not Seen, Not Heard in the Digital World! Measuring Privacy Practices in Children's Apps](https://doi.org/10.1145/3543507.3583327) |  | 0 | The digital age has brought a world of opportunity to children. Connectivity can be a game-changer for some of the world's most marginalized children. However, while legislatures around the world have enacted regulations to protect children's online privacy, and app stores have instituted various protections, privacy in mobile apps remains a growing concern for parents and wider society. In this paper, we explore the potential privacy issues and threats that exist in these apps. We investigate 20,195 mobile apps from the Google Play store that are designed particularly for children (Family apps) or include children in their target user groups (Normal apps). Using both static and dynamic analysis, we find that 4.47% of Family apps request location permissions, even though collecting location information from children is forbidden by the Play store, and 81.25% of Family apps use trackers (which are not allowed in children's apps). Even major developers with 40+ kids apps on the Play store use ad trackers. Furthermore, we find that most permission request notifications are not well designed for children, and 19.25% apps have inconsistent content age ratings across the different protection authorities. Our findings suggest that, despite significant attention to children's privacy, a large gap between regulatory provisions, app store policies, and actual development practices exist. Our research sheds light for government policymakers, app stores, and developers. | Ruoxi Sun, Minhui Xue, Gareth Tyson, Shuo Wang, Seyit Camtepe, Surya Nepal | Hong Kong University of Science and Technology (GZ), China; University of Adelaide, Australia and CSIRO's Data61, Australia; CSIRO's Data61, Australia and Cybersecurity CRC, Australia |
| 590 |  |  [Automatic Discovery of Emerging Browser Fingerprinting Techniques](https://doi.org/10.1145/3543507.3583333) |  | 0 | With the progression of modern browsers, online tracking has become the most concerning issue for preserving privacy on the web. As major browser vendors plan to or already ban third-party cookies, trackers have to shift towards browser fingerprinting by incorporating novel browser APIs into their tracking arsenal. Understanding how new browser APIs are abused in browser fingerprinting techniques is a significant step toward ensuring protection from online tracking. In this paper, we propose a novel hybrid system, named BFAD, that automatically identifies previously unknown browser fingerprinting APIs in the wild. The system combines dynamic and static analysis to accurately reveal browser API usage and automatically infer browser fingerprinting behavior. Based on the observation that a browser fingerprint is constructed by pulling information from multiple APIs, we leverage dynamic analysis and a locality-based algorithm to discover all involved APIs and static analysis on the dataflow of fingerprinting information to accurately associate them together. Our system discovers 231 fingerprinting APIs in Alexa top 10K domains, starting with only 35 commonly known fingerprinting APIs and 17 data transmission APIs. Out of 231 APIs, 161 of them are not identified by state-of-the-art detection systems. Since our approach is fully automated, we repeat our experiments 11 months later and discover 18 new fingerprinting APIs that were not discovered in our previous experiment. We present with case studies the fingerprinting ability of a total of 249 detected APIs. | Junhua Su, Alexandros Kapravelos | Department of Computer Science, North Carolina State University, USA |
| 591 |  |  [BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection](https://doi.org/10.1145/3543507.3583345) |  | 0 | As various forms of fraud proliferate on Ethereum, it is imperative to safeguard against these malicious activities to protect susceptible users from being victimized. While current studies solely rely on graph-based fraud detection approaches, it is argued that they may not be well-suited for dealing with highly repetitive, skew-distributed and heterogeneous Ethereum transactions. To address these challenges, we propose BERT4ETH, a universal pre-trained Transformer encoder that serves as an account representation extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features the superior modeling capability of Transformer to capture the dynamic sequential patterns inherent in Ethereum transactions, and addresses the challenges of pre-training a BERT model for Ethereum with three practical and effective strategies, namely repetitiveness reduction, skew alleviation and heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH outperforms state-of-the-art methods with significant enhancements in terms of the phishing account detection and de-anonymization tasks. The code for BERT4ETH is available at: https://github.com/git-disl/BERT4ETH. | Sihao Hu, Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He, Ling Liu | Georgia Institute of Technology, USA; National University of Singapore, Singapore; National University of Singapore, Singapore and Georgia Institute of Technology, USA |
| 592 |  |  [Training-free Lexical Backdoor Attacks on Language Models](https://doi.org/10.1145/3543507.3583348) |  | 0 | Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack. | Yujin Huang, Terry Yue Zhuo, Qiongkai Xu, Han Hu, Xingliang Yuan, Chunyang Chen | The University of Melbourne, Australia; Monash University, Australia; Monash University, Australia and CSIRO's Data61, Australia |
| 593 |  |  [The Benefits of Vulnerability Discovery and Bug Bounty Programs: Case Studies of Chromium and Firefox](https://doi.org/10.1145/3543507.3583352) |  | 0 | Recently, bug-bounty programs have gained popularity and become a significant part of the security culture of many organizations. Bug-bounty programs enable organizations to enhance their security posture by harnessing the diverse expertise of crowds of external security experts (i.e., bug hunters). Nonetheless, quantifying the benefits of bug-bounty programs remains elusive, which presents a significant challenge for managing them. Previous studies focused on measuring their benefits in terms of the number of vulnerabilities reported or based on the properties of the reported vulnerabilities, such as severity or exploitability. However, beyond these inherent properties, the value of a report also depends on the probability that the vulnerability would be discovered by a threat actor before an internal expert could discover and patch it. In this paper, we present a data-driven study of the Chromium and Firefox vulnerability-reward programs. First, we estimate the difficulty of discovering a vulnerability using the probability of rediscovery as a novel metric. Our findings show that vulnerability discovery and patching provide clear benefits by making it difficult for threat actors to find vulnerabilities; however, we also identify opportunities for improvement, such as incentivizing bug hunters to focus more on development releases. Second, we compare the types of vulnerabilities that are discovered internally vs. externally and those that are exploited by threat actors. We observe significant differences between vulnerabilities found by external bug hunters, internal security teams, and external threat actors, which indicates that bug-bounty programs provide an important benefit by complementing the expertise of internal teams, but also that external hunters should be incentivized more to focus on the types of vulnerabilities that are likely to be exploited by threat actors. | Soodeh Atefi, Amutheezan Sivagnanam, Afiya Ayman, Jens Grossklags, Aron Laszka | Technical University of Munich, Germany; Pennsylvania State University, USA; University of Houston, USA |
| 594 |  |  [Net-track: Generic Web Tracking Detection Using Packet Metadata](https://doi.org/10.1145/3543507.3583372) |  | 0 | While third-party trackers breach users’ privacy by compiling large amounts of personal data through web tracking techniques, combating these trackers is still left at the hand of each user. Although network operators may attempt a network-wide detection of trackers through inspecting all web traffic inside the network, their methods are not only privacy-intrusive but of limited accuracy as these are susceptible to domain changes or ineffective against encrypted traffic. To this end, in this paper, we propose Net-track, a novel approach to managing a secure web environment through platform-independent, encryption-agnostic detection of trackers. Utilizing only side-channel data from network traffic that are still available when encrypted, Net-track accurately detects trackers network-wide, irrespective of user’s browsers or devices without looking into packet payloads or resources fetched from the web server. This prevents user data from leaking to tracking servers in a privacy-preserving manner. By measuring statistics from traffic traces and their similarities, we show distinctions between benign traffic and tracker traffic in their traffic patterns and build Net-track based on the features that fully capture trackers’ distinctive characteristics. Evaluation results show that Net-track is able to detect trackers with 94.02% accuracy and can even discover new trackers yet unrecognized by existing filter lists. Furthermore, Net-track shows its potential for real-time detection, maintaining its performance when using only a portion of each traffic trace. | Dongkeun Lee, Minwoo Joo, Wonjun Lee | Korea University, Republic of Korea; Samsung Research, Republic of Korea |
| 595 |  |  [Cross-Modality Mutual Learning for Enhancing Smart Contract Vulnerability Detection on Bytecode](https://doi.org/10.1145/3543507.3583367) |  | 0 | Over the past couple of years, smart contracts have been plagued by multifarious vulnerabilities, which have led to catastrophic financial losses. Their security issues, therefore, have drawn intense attention. As countermeasures, a family of tools has been developed to identify vulnerabilities in smart contracts at the source-code level. Unfortunately, only a small fraction of smart contracts is currently open-sourced. Another spectrum of work is presented to deal with pure bytecode, but most such efforts still suffer from relatively low performance due to the inherent difficulty in restoring abundant semantics in the source code from the bytecode. This paper proposes a novel cross-modality mutual learning framework for enhancing smart contract vulnerability detection on bytecode. Specifically, we engage in two networks, a student network as the primary network and a teacher network as the auxiliary network. takes two modalities, i.e., source code and its corresponding bytecode as inputs, while is fed with only bytecode. By learning from , is trained to infer the missed source code embeddings and combine both modalities to approach precise vulnerability detection. To further facilitate mutual learning between and , we present a cross-modality mutual learning loss and two transfer losses. As a side contribution, we construct and release a labeled smart contract dataset that concerns four types of common vulnerabilities. Experimental results show that our method significantly surpasses state-of-the-art approaches. | Peng Qian, Zhenguang Liu, Yifang Yin, Qinming He | Zhejiang University, China; Institute for Infocomm Research, A\*STAR, Singapore |
| 596 |  |  [The Chameleon on the Web: an Empirical Study of the Insidious Proactive Web Defacements](https://doi.org/10.1145/3543507.3583377) |  | 0 | Web defacement is one of the major promotional channels for online underground economies. It regularly compromises benign websites and injects fraudulent content to promote illicit goods and services. It inflicts significant harm to websites’ reputations and revenues and may lead to legal ramifications. In this paper, we uncover proactive web defacements, where the involved web pages (i.e., landing pages) proactively deface themselves within browsers using JavaScript (i.e., control scripts). Proactive web defacements have not yet received attention from research communities, anti-hacking organizations, or law-enforcement officials. To detect proactive web defacements, we designed a practical tool, PACTOR. It runs in the browser and intercepts JavaScript API calls that manipulate web page content. It takes snapshots of the rendered HTML source code immediately before and after the intercepted API calls and detects proactive web defacements by visually comparing every two consecutive snapshots. Our two-month empirical study, using PACTOR, on 2,454 incidents of proactive web defacements shows that they can evade existing URL safety-checking tools and effectively promote the ranking of their landing pages using legitimate content/keywords. We also investigated the vendor network of proactive web defacements and reported all the involved domains to law-enforcement officials and URL-safety checking tools. | Rui Zhao | University of Nebraska at Omaha, USA |
| 597 |  |  [Shield: Secure Allegation Escrow System with Stronger Guarantees](https://doi.org/10.1145/3543507.3583391) |  | 0 | The rising issues of harassment, exploitation, corruption and other forms of abuse have led victims to seek comfort by acting in unison against common perpetrators. This is corroborated by the widespread #MeToo movement, which was explicitly against sexual harassment. Installation of escrow systems has allowed victims to report such incidents. The escrows are responsible for identifying the perpetrator and taking the necessary action to bring justice to all its victims. However, users hesitate to participate in these systems due to the fear of such sensitive reports being leaked to perpetrators, who may further misuse them. Thus, to increase trust in the system, cryptographic solutions are being designed to realize web-based secure allegation escrow (SAE) systems. While the work of Arun et al. (NDSS’20) presents the state-of-the-art solution, we identify attacks that can leak sensitive information and compromise victim privacy. We also report issues present in prior works that were left unidentified. Having identified the attacks and issues in all prior works, we put forth an SAE system that overcomes these while retaining all the existing salient features. The cryptographic technique of secure multiparty computation (MPC) serves as the primary underlying tool in designing our system. At the heart of our system lies a new duplicity check protocol and an improved matching protocol. We also provide essential features such as allegation modification and deletion, which were absent in the state of the art. To demonstrate feasibility, we benchmark the proposed system with state-of-the-art MPC protocols and report the cost of processing an allegation. Different settings that affect system performance are analyzed, and the reported values showcase the practicality of our solution. | Nishat Koti, Varsha Bhat Kukkala, Arpita Patra, Bhavish Raj Gopal | Indian Institute of Science, India |
| 598 |  |  [Unnoticeable Backdoor Attacks on Graph Neural Networks](https://doi.org/10.1145/3543507.3583392) |  | 0 | Graph Neural Networks (GNNs) have achieved promising results in various tasks such as node classification and graph classification. Recent studies find that GNNs are vulnerable to adversarial attacks. However, effective backdoor attacks on graphs are still an open problem. In particular, backdoor attack poisons the graph by attaching triggers and the target class label to a set of nodes in the training graph. The backdoored GNNs trained on the poisoned graph will then be misled to predict test nodes to target class once attached with triggers. Though there are some initial efforts in graph backdoor attacks, our empirical analysis shows that they may require a large attack budget for effective backdoor attacks and the injected triggers can be easily detected and pruned. Therefore, in this paper, we study a novel problem of unnoticeable graph backdoor attacks with limited attack budget. To fully utilize the attack budget, we propose to deliberately select the nodes to inject triggers and target class labels in the poisoning phase. An adaptive trigger generator is deployed to obtain effective triggers that are difficult to be noticed. Extensive experiments on real-world datasets against various defense strategies demonstrate the effectiveness of our proposed method in conducting effective unnoticeable backdoor attacks. | Enyan Dai, Minhua Lin, Xiang Zhang, Suhang Wang | Pennsylvania State University, USA |
| 599 |  |  [Bad Apples: Understanding the Centralized Security Risks in Decentralized Ecosystems](https://doi.org/10.1145/3543507.3583393) |  | 0 | The blockchain-powered decentralized applications and systems have been widely deployed in recent years. The decentralization feature promises users anonymity, security, and non-censorship, which is especially welcomed in the areas of decentralized finance and digital assets. From the perspective of most common users, a decentralized ecosystem means every service follows the principle of decentralization. However, we find that the services in a decentralized ecosystem still may contain centralized components or scenarios, like third-party SDKs and privileged operations, which violate the promise of decentralization and may cause a series of centralized security risks. In this work, we systematically study the centralized security risks existing in decentralized ecosystems. Specifically, we identify seven centralized security risks in the deployment of two typical decentralized services – crypto wallets and DApps, such as anonymity loss and overpowered owner. Also, to measure these risks in the wild, we designed an automated detection tool called Naga and carried out large-scale experiments. Based on the measurement of 28 Ethereum crypto wallets (Android version) and 110,506 on-chain smart contracts, the result shows that the centralized security risks are widespread. Up to 96.4% of wallets and 83.5% of contracts exist at least one security risk, including 260 well-known tokens with a total market cap of over $98 billion. | Kailun Yan, Jilian Zhang, Xiangyu Liu, Wenrui Diao, Shanqing Guo | Jinan University, China; Alibaba Group, China; Shandong University, China |
| 600 |  |  [Scan Me If You Can: Understanding and Detecting Unwanted Vulnerability Scanning](https://doi.org/10.1145/3543507.3583394) |  | 0 | Web vulnerability scanners (WVS) are an indispensable tool for penetration testers and developers of web applications, allowing them to identify and fix low-hanging vulnerabilities before they are discovered by attackers. Unfortunately, malicious actors leverage the very same tools to identify and exploit vulnerabilities in third-party websites. Existing research in the WVS space is largely concerned with how many vulnerabilities these tools can discover, as opposed to trying to identify the tools themselves when they are used illicitly. In this work, we design a testbed to characterize web vulnerability scanners using browser-based and network-based fingerprinting techniques. We conduct a measurement study over 12 web vulnerability scanners as well as 159 users who were recruited to interact with the same web applications that were targeted by the evaluated WVSs. By contrasting the traffic and behavior of these two groups, we discover tool-specific and type-specific behaviors in WVSs that are absent from regular users. Based on these observations, we design and build ScannerScope, a machine-learning-based, web vulnerability scanner detection system. ScannerScope consists of a transparent reverse proxy that injects fingerprinting modules on the fly without the assistance (or knowledge) of the protected web applications. Our evaluation results show that ScannerScope can effectively detect WVSs and protect web applications against unwanted vulnerability scanning, with a detection accuracy of over 99% combined with near-zero false positives on human-visitor traffic. Finally, we show that the asynchronous design of ScannerScope results in a negligible impact on server performance and demonstrate that its classifier can resist adversarial ML attacks launched by sophisticated adversaries. | Xigao Li, Babak Amin Azad, Amir Rahmati, Nick Nikiforakis | Computer Science, Stony Brook University, USA |
| 601 |  |  [The More Things Change, the More They Stay the Same: Integrity of Modern JavaScript](https://doi.org/10.1145/3543507.3583395) |  | 0 | The modern web is a collection of remote resources that are identified by their location and composed of interleaving networks of trust. Supply chain attacks compromise the users of a target domain by leveraging its often large set of trusted third parties who provide resources such as JavaScript. The ubiquity of JavaScript, paired with its ability to execute arbitrary code on client machines, makes this particular web resource an ideal vector for supply chain attacks. Currently, there exists no robust method for users browsing the web to verify that the script content they receive from a third party is the expected content. In this paper, we present key insights to inform the design of robust integrity mechanisms, derived from our large-scale analyses of the 6M scripts we collected while crawling 44K domains every day for 77 days. We find that scripts that frequently change should be considered first-class citizens in the modern web ecosystem, and that the ways in which scripts change remain constant over time. Furthermore, we present analyses on the use of strict integrity verification (e.g., Subresource Integrity) at the granularity of the script providers themselves, offering a more complete perspective and demonstrating that the use of strict integrity alone cannot provide satisfactory security guarantees. We conclude that it is infeasible for a client to distinguish benign changes from malicious ones without additional, external knowledge, motivating the need for a new protocol to provide clients the necessary context to assess the potential ramifications of script changes. | Johnny So, Michael Ferdman, Nick Nikiforakis | Stony Brook University, USA |
| 602 |  |  [AppSniffer: Towards Robust Mobile App Fingerprinting Against VPN](https://doi.org/10.1145/3543507.3583473) |  | 0 | Application fingerprinting is a useful data analysis technique for network administrators, marketing agencies, and security analysts. For example, an administrator can adopt application fingerprinting techniques to determine whether a user’s network access is allowed. Several mobile application fingerprinting techniques (e.g., FlowPrint, AppScanner, and ET-BERT) were recently introduced to identify applications using the characteristics of network traffic. However, we find that the performance of the existing mobile application fingerprinting systems significantly degrades when a virtual private network (VPN) is used. To address such a shortcoming, we propose a framework dubbed AppSniffer that uses a two-stage classification process for mobile app fingerprinting. In the first stage, we distinguish VPN traffic from normal traffic; in the second stage, we use the optimal model for each traffic type. Specifically, we propose a stacked ensemble model using Light Gradient Boosting Machine (LightGBM) and a FastAI library-based neural network model to identify applications’ traffic when a VPN is used. To show the feasibility of AppSniffer, we evaluate the detection accuracy of AppSniffer for 150 popularly used Android apps. Our experimental results show that AppSniffer effectively identifies mobile applications over VPNs with F1-scores between 84.66% and 95.49% across four different VPN protocols. In contrast, the best state-of-the-art method (i.e., AppScanner) demonstrates significantly lower F1-scores between 25.63% and 47.56% in the same settings. Overall, when normal traffic and VPN traffic are mixed, AppSniffer achieves an F1-score of 90.63%, which is significantly better than AppScanner that shows an F1-score of 70.36%. | Sanghak Oh, Minwook Lee, Hyunwoo Lee, Elisa Bertino, Hyoungshick Kim | Korea Institute of Energy Technology, Republic of Korea; Purdue University, USA; Sungkyunkwan University, Republic of Korea |
| 603 |  |  [RICC: Robust Collective Classification of Sybil Accounts](https://doi.org/10.1145/3543507.3583475) |  | 0 | A Sybil attack is a critical threat that undermines the trust and integrity of web services by creating and exploiting a large number of fake (i.e., Sybil) accounts. To mitigate this threat, previous studies have proposed leveraging collective classification to detect Sybil accounts. Recently, researchers have demonstrated that state-of-the-art adversarial attacks are able to bypass existing collective classification methods, posing a new security threat. To this end, we propose RICC, the first robust collective classification framework, designed to identify adversarial Sybil accounts created by adversarial attacks. RICC leverages the novel observation that these adversarial attacks are highly tailored to a target collective classification model to optimize the attack budget. Owing to this adversarial strategy, the classification results for adversarial Sybil accounts often significantly change when deploying a new training set different from the original training set used for assigning prior reputation scores to user accounts. Leveraging this observation, RICC achieves robustness in collective classification by stabilizing classification results across different training sets randomly sampled in each round. RICC achieves false negative rates of 0.01, 0.11, 0.00, and 0.01 in detecting adversarial Sybil accounts for the Enron, Facebook, Twitter_S, and Twitter_L datasets, respectively. It also attains respective AUCs of 0.99, 1.00, 0.89, and 0.74 for these datasets, achieving high performance on the original task of detecting Sybil accounts. RICC significantly outperforms all existing Sybil detection methods, demonstrating superior robustness and efficacy in the collective classification of Sybil accounts. | Dongwon Shin, Suyoung Lee, Sooel Son | School of Computing, KAIST, Republic of Korea |
| 604 |  |  [ZTLS: A DNS-based Approach to Zero Round Trip Delay in TLS handshake](https://doi.org/10.1145/3543507.3583516) |  | 0 | Establishing secure connections fast to end-users is crucial to online services. However, when a client sets up a TLS session with a server, the TLS handshake needs one round trip time (RTT) to negotiate a session key. Additionally, establishing a TLS session also requires a DNS lookup (e.g., the A record lookup to fetch the IP address of the server) and a TCP handshake. In this paper, we propose ZTLS to eliminate the 1-RTT latency for the TLS handshake by leveraging the DNS. In ZTLS, a server distributes TLS handshake-related data (i.e., Diffie-Hellman elements), dubbed Z-data, as DNS records. A ZTLS client can fetch Z-data by DNS lookups and derive a session key. With the session key, the client can send encrypted data along with its ClientHello, achieving 0-RTT. ZTLS supports incremental deployability on the current TLS-based infrastructure. Our prototype-based experiments show that ZTLS is 1-RTT faster than TLS in terms of the first response time. | Sangwon Lim, Hyeonmin Lee, Hyunsoo Kim, Hyunwoo Lee, Ted Taekyoung Kwon | Korea Institute of Energy Technology, Republic of Korea; Seoul National University, Republic of Korea |
| 605 |  |  [AgrEvader: Poisoning Membership Inference against Byzantine-robust Federated Learning](https://doi.org/10.1145/3543507.3583542) |  | 0 | The Poisoning Membership Inference Attack (PMIA) is a newly emerging privacy attack that poses a significant threat to federated learning (FL). An adversary conducts data poisoning (i.e., performing adversarial manipulations on training examples) to extract membership information by exploiting the changes in loss resulting from data poisoning. The PMIA significantly exacerbates the traditional poisoning attack that is primarily focused on model corruption. However, there has been a lack of a comprehensive systematic study that thoroughly investigates this topic. In this work, we conduct a benchmark evaluation to assess the performance of PMIA against the Byzantine-robust FL setting that is specifically designed to mitigate poisoning attacks. We find that all existing coordinate-wise averaging mechanisms fail to defend against the PMIA, while the detect-then-drop strategy was proven to be effective in most cases, implying that the poison injection is memorized and the poisonous effect rarely dissipates. Inspired by this observation, we propose AgrEvader, a PMIA that maximizes the adversarial impact on the victim samples while circumventing the detection by Byzantine-robust mechanisms. AgrEvader significantly outperforms existing PMIAs. For instance, AgrEvader achieved a high attack accuracy of between 72.78% (on CIFAR-10) to 97.80% (on Texas100), which is an average accuracy increase of 13.89% compared to the strongest PMIA reported in the literature. We evaluated AgrEvader on five datasets across different domains, against a comprehensive list of threat models, which included black-box, gray-box and white-box models for targeted and non-targeted scenarios. AgrEvader demonstrated consistent high accuracy across all settings tested. The code is available at: https://github.com/PrivSecML/AgrEvader. | Yanjun Zhang, Guangdong Bai, Mahawaga Arachchige Pathum Chamikara, Mengyao Ma, Liyue Shen, Jingwei Wang, Surya Nepal, Minhui Xue, Long Wang, Joseph K. Liu | Intelligent Engine Department, Ant Group, MYBank, China; CSIRO's Data61, Australia; Deakin University, Australia; The University of Queensland, Australia; Monash University, Australia |
| 606 |  |  [Event Prediction using Case-Based Reasoning over Knowledge Graphs](https://doi.org/10.1145/3543507.3583201) |  | 0 | Applying link prediction (LP) methods over knowledge graphs (KG) for tasks such as causal event prediction presents an exciting opportunity. However, typical LP models are ill-suited for this task as they are incapable of performing inductive link prediction for new, unseen event entities and they require retraining as knowledge is added or changed in the underlying KG. We introduce a case-based reasoning model, EvCBR, to predict properties about new consequent events based on similar cause-effect events present in the KG. EvCBR uses statistical measures to identify similar events and performs path-based predictions, requiring no training step. To generalize our methods beyond the domain of event prediction, we frame our task as a 2-hop LP task, where the first hop is a causal relation connecting a cause event to a new effect event and the second hop is a property about the new event which we wish to predict. The effectiveness of our method is demonstrated using a novel dataset of newsworthy events with causal relations curated from Wikidata, where EvCBR outperforms baselines including translational-distance-based, GNN-based, and rule-based LP models. | Sola Shirai, Debarun Bhattacharjya, Oktie Hassanzadeh | IBM Research, USA; Rensselaer Polytechnic Institute, USA |
| 607 |  |  [Wikidata as a seed for Web Extraction](https://doi.org/10.1145/3543507.3583236) |  | 0 | Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph. | Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier | The QA Company SAS, France and Laboratoire Hubert Curien UMR 5516, Université Jean Monnet, France; The QA Company SAS, France and Laboratoire Hubert Curien, UMR CNRS 5516, Université Jean Monnet, France; Laboratoire Hubert Curien UMR 5516, Université Jean Monnet, France |
| 608 |  |  [Meta-Learning Based Knowledge Extrapolation for Temporal Knowledge Graph](https://doi.org/10.1145/3543507.3583279) |  | 0 | In the last few years, the solution to Knowledge Graph (KG) completion via learning embeddings of entities and relations has attracted a surge of interest. Temporal KGs(TKGs) extend traditional Knowledge Graphs (KGs) by associating static triples with timestamps forming quadruples. Different from KGs and TKGs in the transductive setting, constantly emerging entities and relations in incomplete TKGs create demand to predict missing facts with unseen components, which is the extrapolation setting. Traditional temporal knowledge graph embedding (TKGE) methods are limited in the extrapolation setting since they are trained within a fixed set of components. In this paper, we propose a Meta-Learning based Temporal Knowledge Graph Extrapolation (MTKGE) model, which is trained on link prediction tasks sampled from the existing TKGs and tested in the emerging TKGs with unseen entities and relations. Specifically, we meta-train a GNN framework that captures relative position patterns and temporal sequence patterns between relations. The learned embeddings of patterns can be transferred to embed unseen components. Experimental results on two different TKG extrapolation datasets show that MTKGE consistently outperforms both the existing state-of-the-art models for knowledge graph extrapolation and specifically adapted KGE and TKGE baselines. | Zhongwu Chen, Chengjin Xu, Fenglong Su, Zhen Huang, Yong Dou | National University of Defense Technology, China; International Digital Economy Academy, China |
| 609 |  |  [Can Persistent Homology provide an efficient alternative for Evaluation of Knowledge Graph Completion Methods?](https://doi.org/10.1145/3543507.3583308) |  | 0 | In this paper we present a novel method, $\textit{Knowledge Persistence}$ ($\mathcal{KP}$), for faster evaluation of Knowledge Graph (KG) completion approaches. Current ranking-based evaluation is quadratic in the size of the KG, leading to long evaluation times and consequently a high carbon footprint. $\mathcal{KP}$ addresses this by representing the topology of the KG completion methods through the lens of topological data analysis, concretely using persistent homology. The characteristics of persistent homology allow $\mathcal{KP}$ to evaluate the quality of the KG completion looking only at a fraction of the data. Experimental results on standard datasets show that the proposed metric is highly correlated with ranking metrics (Hits@N, MR, MRR). Performance evaluation shows that $\mathcal{KP}$ is computationally efficient: In some cases, the evaluation time (validation+test) of a KG completion method has been reduced from 18 hours (using Hits@10) to 27 seconds (using $\mathcal{KP}$), and on average (across methods & data) reduces the evaluation time (validation+test) by $\approx$ $\textbf{99.96}\%$. | Anson Bastos, Kuldeep Singh, Abhishek Nadgeri, Johannes Hoffart, Manish Singh, Toyotaro Suzumura | SAP, Germany; IIT, Hyderabad, India; Cerence GmbH and Zerotha Research, Germany; RWTH Aachen, Germany |
| 610 |  |  [Attribute-Consistent Knowledge Graph Representation Learning for Multi-Modal Entity Alignment](https://doi.org/10.1145/3543507.3583328) |  | 0 | The multi-modal entity alignment (MMEA) aims to find all equivalent entity pairs between multi-modal knowledge graphs (MMKGs). Rich attributes and neighboring entities are valuable for the alignment task, but existing works ignore contextual gap problems that the aligned entities have different numbers of attributes on specific modality when learning entity representations. In this paper, we propose a novel attribute-consistent knowledge graph representation learning framework for MMEA (ACK-MMEA) to compensate the contextual gaps through incorporating consistent alignment knowledge. Attribute-consistent KGs (ACKGs) are first constructed via multi-modal attribute uniformization with merge and generate operators so that each entity has one and only one uniform feature in each modality. The ACKGs are then fed into a relation-aware graph neural network with random dropouts, to obtain aggregated relation representations and robust entity representations. In order to evaluate the ACK-MMEA facilitated for entity alignment, we specially design a joint alignment loss for both entity and attribute evaluation. Extensive experiments conducted on two benchmark datasets show that our approach achieves excellent performance compared to its competitors. | Qian Li, Shu Guo, Yangyifei Luo, Cheng Ji, Lihong Wang, Jiawei Sheng, Jianxin Li | Institute of Information Engineering, Chinese Academy of Sciences, China; National Computer Network Emergency Response Technical Team/Coordination Center of China, China; Beihang University, China |
| 611 |  |  [Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs](https://doi.org/10.1145/3543507.3583376) |  | 0 | Knowledge graphs (KGs) have been widely used to enhance complex question answering (QA). To understand complex questions, existing studies employ language models (LMs) to encode contexts. Despite the simplicity, they neglect the latent relational information among question concepts and answers in KGs. While question concepts ubiquitously present hyponymy at the semantic level, e.g., mammals and animals, this feature is identically reflected in the hierarchical relations in KGs, e.g., a_type_of. Therefore, we are motivated to explore comprehensive reasoning by the hierarchical structures in KGs to help understand questions. However, it is non-trivial to reason over tree-like structures compared with chained paths. Moreover, identifying appropriate hierarchies relies on expertise. To this end, we propose HamQA, a novel Hierarchy-aware multi-hop Question Answering framework on knowledge graphs, to effectively align the mutual hierarchical information between question contexts and KGs. The entire learning is conducted in Hyperbolic space, inspired by its advantages of embedding hierarchical structures. Specifically, (i) we design a context-aware graph attentive network to capture context information. (ii) Hierarchical structures are continuously preserved in KGs by minimizing the Hyperbolic geodesic distances. The comprehensive reasoning is conducted to jointly train both components and provide a top-ranked candidate as an optimal answer. We achieve a higher ranking than the state-of-the-art multi-hop baselines on the official OpenBookQA leaderboard with an accuracy of 85%. | Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, Zhimeng Jiang | Texas A&M University, USA; The Hong Kong Polytechnic University, Hong Kong; National University of Singapore, Singapore |
| 612 |  |  [Unsupervised Entity Alignment for Temporal Knowledge Graphs](https://doi.org/10.1145/3543507.3583381) |  | 0 | Entity alignment (EA) is a fundamental data integration task that identifies equivalent entities between different knowledge graphs (KGs). Temporal Knowledge graphs (TKGs) extend traditional knowledge graphs by introducing timestamps, which have received increasing attention. State-of-the-art time-aware EA studies have suggested that the temporal information of TKGs facilitates the performance of EA. However, existing studies have not thoroughly exploited the advantages of temporal information in TKGs. Also, they perform EA by pre-aligning entity pairs, which can be labor-intensive and thus inefficient. In this paper, we present DualMatch which effectively fuses the relational and temporal information for EA. DualMatch transfers EA on TKGs into a weighted graph matching problem. More specifically, DualMatch is equipped with an unsupervised method, which achieves EA without necessitating seed alignment. DualMatch has two steps: (i) encoding temporal and relational information into embeddings separately using a novel label-free encoder, Dual-Encoder; and (ii) fusing both information and transforming it into alignment using a novel graph-matching-based decoder, GM-Decoder. DualMatch is able to perform EA on TKGs with or without supervision, due to its capability of effectively capturing temporal information. Extensive experiments on three real-world TKG datasets offer the insight that DualMatch outperforms the state-of-the-art methods in terms of H@1 by 2.4% - 10.7% and MRR by 1.7% - 7.6%, respectively. | Xiaoze Liu, Junyang Wu, Tianyi Li, Lu Chen, Yunjun Gao | Zhejiang University, China; Aalborg University, Denmark |
| 613 |  |  [KRACL: Contrastive Learning with Graph Context Modeling for Sparse Knowledge Graph Completion](https://doi.org/10.1145/3543507.3583412) |  | 0 | Knowledge Graph Embeddings (KGE) aim to map entities and relations to low dimensional spaces and have become the \textit{de-facto} standard for knowledge graph completion. Most existing KGE methods suffer from the sparsity challenge, where it is harder to predict entities that appear less frequently in knowledge graphs. In this work, we propose a novel framework KRACL to alleviate the widespread sparsity in KGs with graph context and contrastive learning. Firstly, we propose the Knowledge Relational Attention Network (KRAT) to leverage the graph context by simultaneously projecting neighboring triples to different latent spaces and jointly aggregating messages with the attention mechanism. KRAT is capable of capturing the subtle semantic information and importance of different context triples as well as leveraging multi-hop information in knowledge graphs. Secondly, we propose the knowledge contrastive loss by combining the contrastive loss with cross entropy loss, which introduces more negative samples and thus enriches the feedback to sparse entities. Our experiments demonstrate that KRACL achieves superior results across various standard knowledge graph benchmarks, especially on WN18RR and NELL-995 which have large numbers of low in-degree entities. Extensive experiments also bear out KRACL's effectiveness in handling sparse knowledge graphs and robustness against noisy triples. | Zhaoxuan Tan, Zilong Chen, Shangbin Feng, Qingyue Zhang, Qinghua Zheng, Jundong Li, Minnan Luo | University of Washington, USA; Xi'an Jiaotong University, China; University of Virginia, USA; Tsinghua University, China |
| 614 |  |  [IMF: Interactive Multimodal Fusion Model for Link Prediction](https://doi.org/10.1145/3543507.3583554) |  | 0 | Link prediction aims to identify potential missing triples in knowledge graphs. To get better results, some recent studies have introduced multimodal information to link prediction. However, these methods utilize multimodal information separately and neglect the complicated interaction between different modalities. In this paper, we aim at better modeling the inter-modality information and thus introduce a novel Interactive Multimodal Fusion (IMF) model to integrate knowledge from different modalities. To this end, we propose a two-stage multimodal fusion framework to preserve modality-specific knowledge as well as take advantage of the complementarity between different modalities. Instead of directly projecting different modalities into a unified space, our multimodal fusion module limits the representations of different modalities independent while leverages bilinear pooling for fusion and incorporates contrastive learning as additional constraints. Furthermore, the decision fusion module delivers the learned weighted average over the predictions of all modalities to better incorporate the complementarity of different modalities. Our approach has been demonstrated to be effective through empirical evaluations on several real-world datasets. The implementation code is available online at https://github.com/HestiaSky/IMF-Pytorch. | Xinhang Li, Xiangyu Zhao, Jiaxing Xu, Yong Zhang, Chunxiao Xing | School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Data Science, City University of Hong Kong, Hong Kong; Department of Computer Science and Technology, Tsinghua University, China |
| 615 |  |  [Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer](https://doi.org/10.1145/3543507.3583301) |  | 0 | Knowledge graphs (KG) are essential background knowledge providers in many tasks. When designing models for KG-related tasks, one of the key tasks is to devise the Knowledge Representation and Fusion (KRF) module that learns the representation of elements from KGs and fuses them with task representations. While due to the difference of KGs and perspectives to be considered during fusion across tasks, duplicate and ad hoc KRF modules design are conducted among tasks. In this paper, we propose a novel knowledge graph pretraining model KGTransformer that could serve as a uniform KRF module in diverse KG-related tasks. We pretrain KGTransformer with three self-supervised tasks with sampled sub-graphs as input. For utilization, we propose a general prompt-tuning mechanism regarding task data as a triple prompt to allow flexible interactions between task KGs and task data. We evaluate pretrained KGTransformer on three tasks, triple classification, zero-shot image classification, and question answering. KGTransformer consistently achieves better results than specifically designed task models. Through experiments, we justify that the pretrained KGTransformer could be used off the shelf as a general and effective KRF module across KG-related tasks. The code and datasets are available at https://github.com/zjukg/KGTransformer. | Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, Huajun Chen | Zhejiang University, China; Huawei Technologies Co., Ltd, China |
| 616 |  |  [TEA: Time-aware Entity Alignment in Knowledge Graphs](https://doi.org/10.1145/3543507.3583317) |  | 0 | Entity alignment (EA) aims to identify equivalent entities between knowledge graphs (KGs), which is a key technique to improve the coverage of existing KGs. Current EA models largely ignore the importance of time information contained in KGs and treat relational facts or attribute values of entities as time-invariant. However, real-world entities could evolve over time, making the knowledge of the aligned entities very different in multiple KGs. This may cause incorrect matching between KGs if such entity dynamics is ignored. In this paper, we propose a time-aware entity alignment (TEA) model that discovers the entity evolving behaviour by exploring the time contexts in KGs and aggregates various contextual information to make the alignment decision. In particular, we address two main challenges in the TEA model: 1) How to identify highly-correlated temporal facts; 2) How to capture entity dynamics and incorporate it to learn a more informative entity representation for the alignment task. Experiments on real-world datasets1 verify the superiority of our TEA model over state-of-the-art entity aligners. | Yu Liu, Wen Hua, Kexuan Xin, Saeid Hosseini, Xiaofang Zhou | School of Information Technology and Electrical Engineering, The University of Queensland, Australia; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, China; Department of Computing, The Hong Kong Polytechnic University, China; School of Information Science and Technology, University of International Relations, China; Sohar University, Oman |
| 617 |  |  [Knowledge Graph Completion with Counterfactual Augmentation](https://doi.org/10.1145/3543507.3583401) |  | 0 | Graph Neural Networks (GNNs) have demonstrated great success in Knowledge Graph Completion (KGC) by modeling how entities and relations interact in recent years. However, most of them are designed to learn from the observed graph structure, which appears to have imbalanced relation distribution during the training stage. Motivated by the causal relationship among the entities on a knowledge graph, we explore this defect through a counterfactual question: "would the relation still exist if the neighborhood of entities became different from observation?". With a carefully designed instantiation of a causal model on the knowledge graph, we generate the counterfactual relations to answer the question by regarding the representations of entity pair given relation as context, structural information of relation-aware neighborhood as treatment, and validity of the composed triplet as the outcome. Furthermore, we incorporate the created counterfactual relations with the GNN-based framework on KGs to augment their learning of entity pair representations from both the observed and counterfactual relations. Experiments on benchmarks show that our proposed method outperforms existing methods on the task of KGC, achieving new state-of-the-art results. Moreover, we demonstrate that the proposed counterfactual relations-based augmentation also enhances the interpretability of the GNN-based framework through the path interpretations of predictions. | Heng Chang, Jie Cai, Jia Li | Hong Kong University of Science and Technology (Guangzhou), China; Tsinghua University, China |
| 618 |  |  [Learning Social Meta-knowledge for Nowcasting Human Mobility in Disaster](https://doi.org/10.1145/3543507.3583991) |  | 0 | Human mobility nowcasting is a fundamental research problem for intelligent transportation planning, disaster responses and management, etc. In particular, human mobility under big disasters such as hurricanes and pandemics deviates from its daily routine to a large extent, which makes the task more challenging. Existing works mainly focus on traffic or crowd flow prediction in normal situations. To tackle this problem, in this study, disaster-related Twitter data is incorporated as a covariate to understand the public awareness and attention about the disaster events and thus perceive their impacts on the human mobility. Accordingly, we propose a Meta-knowledge-Memorizable Spatio-Temporal Network (MemeSTN), which leverages memory network and meta-learning to fuse social media and human mobility data. Extensive experiments over three real-world disasters including Japan 2019 typhoon season, Japan 2020 COVID-19 pandemic, and US 2019 hurricane season were conducted to illustrate the effectiveness of our proposed solution. Compared to the state-of-the-art spatio-temporal deep models and multivariate-time-series deep models, our model can achieve superior performance for nowcasting human mobility in disaster situations at both country level and state level. | Renhe Jiang, Zhaonan Wang, Yudong Tao, Chuang Yang, Xuan Song, Ryosuke Shibasaki, ShuChing Chen, MeiLing Shyu | University of Missouri-Kansas City, USA; University of Miami, USA; The University of Tokyo, Japan |
| 619 |  |  [Cashing in on Contacts: Characterizing the OnlyFans Ecosystem](https://doi.org/10.1145/3543507.3583210) |  | 0 | Adult video-sharing has undergone dramatic shifts. New platforms that directly interconnect (often amateur) producers and consumers now allow content creators to promote material across the web and directly monetize the content they produce. OnlyFans is the most prominent example of this new trend. OnlyFans is a content subscription service where creators earn money from users who subscribe to their material. In contrast to prior adult platforms, OnlyFans emphasizes creator-consumer interaction for audience accumulation and maintenance. This results in a wide cross-platform ecosystem geared towards bringing consumers to creators’ accounts. In this paper, we inspect this emerging ecosystem, focusing on content creators and the third-party platforms they connect to. | Pelayo Vallina, Ignacio Castro, Gareth Tyson | Hong Kong University of Science and Technology, China; IMDEA Networks, Spain and Universidad Carlos III de Madrid, Spain; Queen Mary University of London, United Kingdom |
| 620 |  |  [Automated Content Moderation Increases Adherence to Community Guidelines](https://doi.org/10.1145/3543507.3583275) |  | 0 | Online social media platforms use automated moderation systems to remove or reduce the visibility of rule-breaking content. While previous work has documented the importance of manual content moderation, the effects of automated content moderation remain largely unknown. Here, in a large study of Facebook comments (n=412M), we used a fuzzy regression discontinuity design to measure the impact of automated content moderation on subsequent rule-breaking behavior (number of comments hidden/deleted) and engagement (number of additional comments posted). We found that comment deletion decreased subsequent rule-breaking behavior in shorter threads (20 or fewer comments), even among other participants, suggesting that the intervention prevented conversations from derailing. Further, the effect of deletion on the affected user's subsequent rule-breaking behavior was longer-lived than its effect on reducing commenting in general, suggesting that users were deterred from rule-breaking but not from commenting. In contrast, hiding (rather than deleting) content had small and statistically insignificant effects. Our results suggest that automated content moderation increases adherence to community guidelines. | Manoel Horta Ribeiro, Justin Cheng, Robert West | Facebook, USA; EPFL, Switzerland |
| 621 |  |  [Mental Health Coping Stories on Social Media: A Causal-Inference Study of Papageno Effect](https://doi.org/10.1145/3543507.3583350) |  | 0 | The Papageno effect concerns how media can play a positive role in preventing and mitigating suicidal ideation and behaviors. With the increasing ubiquity and widespread use of social media, individuals often express and share lived experiences and struggles with mental health. However, there is a gap in our understanding about the existence and effectiveness of the Papageno effect in social media, which we study in this paper. In particular, we adopt a causal-inference framework to examine the impact of exposure to mental health coping stories on individuals on Twitter. We obtain a Twitter dataset with $\sim$2M posts by $\sim$10K individuals. We consider engaging with coping stories as the Treatment intervention, and adopt a stratified propensity score approach to find matched cohorts of Treatment and Control individuals. We measure the psychosocial shifts in affective, behavioral, and cognitive outcomes in longitudinal Twitter data before and after engaging with the coping stories. Our findings reveal that, engaging with coping stories leads to decreased stress and depression, and improved expressive writing, diversity, and interactivity. Our work discusses the practical and platform design implications in supporting mental wellbeing. | Yunhao Yuan, Koustuv Saha, Barbara Keller, Erkki Tapio Isometsä, Talayeh Aledavood | University of Helsinki, Finland; Microsoft Research, Canada; Department of Computer Science, Aalto University, Finland |
| 622 |  |  [A First Look at Public Service Websites from the Affordability Lens](https://doi.org/10.1145/3543507.3583415) |  | 0 | Public service websites act as official gateways to services provided by governments. Many of these websites are essential for citizens to receive reliable information and online government services. However, the lack of affordability of mobile broadband services in many developing countries and the rising complexity of websites create barriers for citizens in accessing these government websites. This paper presents the first large-scale analysis of the affordability of public service websites in developing countries. We do this by collecting a corpus of 1900 public service websites, including public websites from nine developing countries and for comparison websites from nine developed countries. Our investigation is driven by website complexity analysis as well as evaluation through a recently proposed affordability index. Our analysis reveals that, in general, public service websites in developing countries do not meet the affordability target set by the UN’s Broadband Commission. However, we show that several countries can be brought within or closer to the affordability target by implementing webpage optimizations to reduce page sizes. We also discuss policy interventions that can help make access to public service website more affordable. | Rumaisa Habib, Aimen Inam, Ayesha Ali, Ihsan Ayyub Qazi, Zafar Ayyub Qazi | LUMS, Pakistan |
| 623 |  |  [Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation](https://doi.org/10.1145/3543507.3583388) |  | 0 | The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinformation response pairs from in-the-wild social media and crowdsourcing from college-educated students. We annotate the collected data to distinguish poor from ideal responses that are factual, polite, and refute misinformation. We propose MisinfoCorrect, a reinforcement learning-based framework that learns to generate counter-misinformation responses for an input misinformation post. The model rewards the generator to increase the politeness, factuality, and refutation attitude while retaining text fluency and relevancy. Quantitative and qualitative evaluation shows that our model outperforms several baselines by generating high-quality counter-responses. This work illustrates the promise of generative text models for social good -- here, to help create a safe and reliable information ecosystem. The code and data is accessible on https://github.com/claws-lab/MisinfoCorrect. | Bing He, Mustaque Ahamad, Srijan Kumar | Georgia Institute of Technology, USA |
| 624 |  |  [Migration Reframed? A multilingual analysis on the stance shift in Europe during the Ukrainian crisis](https://doi.org/10.1145/3543507.3583442) |  | 0 | The war in Ukraine seems to have positively changed the attitude toward the critical societal topic of migration in Europe -- at least towards refugees from Ukraine. We investigate whether this impression is substantiated by how the topic is reflected in online news and social media, thus linking the representation of the issue on the Web to its perception in society. For this purpose, we combine and adapt leading-edge automatic text processing for a novel multilingual stance detection approach. Starting from 5.5M Twitter posts published by 565 European news outlets in one year, beginning September 2021, plus replies, we perform a multilingual analysis of migration-related media coverage and associated social media interaction for Europe and selected European countries. The results of our analysis show that there is actually a reframing of the discussion illustrated by the terminology change, e.g., from "migrant" to "refugee", often even accentuated with phrases such as "real refugees". However, concerning a stance shift in public perception, the picture is more diverse than expected. All analyzed cases show a noticeable temporal stance shift around the start of the war in Ukraine. Still, there are apparent national differences in the size and stability of this shift. | Sergej Wildemann, Claudia Niederée, Erick Elejalde | L3S Research Center, Leibniz Universität Hannover, Germany |
| 625 |  |  [Who Funds Misinformation? A Systematic Analysis of the Ad-related Profit Routines of Fake News Sites](https://doi.org/10.1145/3543507.3583443) |  | 0 | Fake news is an age-old phenomenon, widely assumed to be associated with political propaganda published to sway public opinion. Yet, with the growth of social media, it has become a lucrative business for Web publishers. Despite many studies performed and countermeasures proposed, unreliable news sites have increased in the last years their share of engagement among the top performing news sources. Stifling fake news impact depends on our efforts in limiting the (economic) incentives of fake news producers. In this paper, we aim at enhancing the transparency around these exact incentives, and explore: Who supports the existence of fake news websites via paid ads, either as an advertiser or an ad seller? Who owns these websites and what other Web business are they into? We are the first to systematize the auditing process of fake news revenue flows. We identify the companies that advertise in fake news websites and the intermediary companies responsible for facilitating those ad revenues. We study more than 2,400 popular news websites and show that well-known ad networks, such as Google and IndexExchange, have a direct advertising relation with more than 40% of fake news websites. Using a graph clustering approach on 114.5K sites, we show that entities who own fake news sites, also operate other types of websites pointing to the fact that owning a fake news website is part of a broader business operation. | Emmanouil Papadogiannakis, Panagiotis Papadopoulos, Evangelos P. Markatos, Nicolas Kourtellis |  |
| 626 |  |  [Evidence of Demographic rather than Ideological Segregation in News Discussion on Reddit](https://doi.org/10.1145/3543507.3583468) |  | 0 | We evaluate homophily and heterophily among ideological and demographic groups in a typical opinion formation context: online discussions of current news. We analyze user interactions across five years in the r/news community on Reddit, one of the most visited websites in the United States. Then, we estimate demographic and ideological attributes of these users. Thanks to a comparison with a carefully-crafted network null model, we establish which pairs of attributes foster interactions and which ones inhibit them. Individuals prefer to engage with the opposite ideological side, which contradicts the echo chamber narrative. Instead, demographic groups are homophilic, as individuals tend to interact within their own group - even in an online setting where such attributes are not directly observable. In particular, we observe age and income segregation consistently across years: users tend to avoid interactions when belonging to different groups. These results persist after controlling for the degree of interest by each demographic group in different news topics. Our findings align with the theory that affective polarization - the difficulty in socializing across political boundaries-is more connected with an increasingly divided society, rather than ideological echo chambers on social media. We publicly release our anonymized data set and all the code to reproduce our results: https://github.com/corradomonti/demographic-homophily | Corrado Monti, Jacopo D'Ignazi, Michele Starnini, Gianmarco De Francisci Morales | ISI Foundation, Italy; CENTAI, Italy |
| 627 |  |  [Longitudinal Assessment of Reference Quality on Wikipedia](https://doi.org/10.1145/3543507.3583218) |  | 0 | Wikipedia plays a crucial role in the integrity of the Web. This work analyzes the reliability of this global encyclopedia through the lens of its references. We operationalize the notion of reference quality by defining reference need (RN), i.e., the percentage of sentences missing a citation, and reference risk (RR), i.e., the proportion of non-authoritative references. We release Citation Detective, a tool for automatically calculating the RN score, and discover that the RN score has dropped by 20 percent point in the last decade, with more than half of verifiable statements now accompanying references. The RR score has remained below 1% over the years as a result of the efforts of the community to eliminate unreliable references. We propose pairing novice and experienced editors on the same Wikipedia article as a strategy to enhance reference quality. Our quasi-experiment indicates that such a co-editing experience can result in a lasting advantage in identifying unreliable sources in future edits. As Wikipedia is frequently used as the ground truth for numerous Web applications, our findings and suggestions on its reliability can have a far-reaching impact. We discuss the possibility of other Web services adopting Wiki-style user collaboration to eliminate unreliable content. | Aitolkyn Baigutanova, Jaehyeon Myung, Diego SáezTrumper, AiJou Chou, Miriam Redi, Changwook Jung, Meeyoung Cha | Wikimedia Foundation, Spain; IBS, Republic of Korea and KAIST, Republic of Korea; School of Computing, KAIST, Republic of Korea; Wikimedia Foundation, United Kingdom; KAIST, Republic of Korea |
| 628 |  |  [Gateway Entities in Problematic Trajectories](https://doi.org/10.1145/3543507.3583283) |  | 0 | Social media platforms like Facebook and YouTube connect people with communities that reflect their own values and experiences. People discover new communities either organically or through algorithmic recommendations based on their interests and preferences. We study online journeys users take through these communities, focusing particularly on ones that may lead to problematic outcomes. In particular, we propose and explore the concept of gateways, namely, entities associated with a higher likelihood of subsequent engagement with problematic content. We show, via a real-world application on Facebook groups, that a simple definition of gateway entities can be leveraged to reduce exposure to problematic content by 1% without any adverse impact on user engagement metrics. Motivated by this finding, we propose several formal definitions of gateways, via both frequentist and survival analysis methods, and evaluate their efficacy in predicting user behavior through offline experiments. Frequentist, duration-insensitive methods predict future harmful engagements with an 0.64–0.83 AUC, while survival analysis methods improve this to 0.72–0.90 AUC. | Xi Leslie Chen, Abhratanu Dutta, Sindhu Ernala, Stratis Ioannidis, Shankar Kalyanaraman, Israel Nir, Udi Weinsberg | Northeastern, USA; Northwestern, USA; Meta, USA; Columbia University, USA |
| 629 |  |  [Unsupervised Anomaly Detection on Microservice Traces through Graph VAE](https://doi.org/10.1145/3543507.3583215) |  | 0 | The microservice architecture is widely employed in large Internet systems. For each user request, a few of the microservices are called, and a trace is formed to record the tree-like call dependencies among microservices and the time consumption at each call node. Traces are useful in diagnosing system failures, but their complex structures make it difficult to model their patterns and detect their anomalies. In this paper, we propose a novel dual-variable graph variational autoencoder (VAE) for unsupervised anomaly detection on microservice traces. To reconstruct the time consumption of nodes, we propose a novel dispatching layer. We find that the inversion of negative log-likelihood (NLL) appears for some anomalous samples, which makes the anomaly score infeasible for anomaly detection. To address this, we point out that the NLL can be decomposed into KL-divergence and data entropy, whereas lower-dimensional anomalies can introduce an entropy gap with normal inputs. We propose three techniques to mitigate this entropy gap for trace anomaly detection: Bernoulli & Categorical Scaling, Node Count Normalization, and Gaussian Std-Limit. On five trace datasets from a top Internet company, our proposed TraceVAE achieves excellent F-scores. | Zhe Xie, Haowen Xu, Wenxiao Chen, Wanxue Li, Huai Jiang, Liangfei Su, Hanzhang Wang, Dan Pei | eBay, USA; eBay, China; Department of Computer Science and Technology, Tsinghua University, China |
| 630 |  |  [FedEdge: Accelerating Edge-Assisted Federated Learning](https://doi.org/10.1145/3543507.3583264) |  | 0 | Federated learning (FL) has been widely acknowledged as a promising solution to training machine learning (ML) model training with privacy preservation. To reduce the traffic overheads incurred by FL systems, edge servers have been included between clients and the parameter server to aggregate clients’ local models. Recent studies on this edge-assisted hierarchical FL scheme have focused on ensuring or accelerating model convergence by coping with various factors, e.g., uncertain network conditions, unreliable clients, heterogeneous compute resources, etc. This paper presents our three new discoveries of the edge-assisted hierarchical FL scheme: 1) it wastes significant time during its two-phase training rounds; 2) it does not recognize or utilize model diversity when producing a global model; and 3) it is vulnerable to model poisoning attacks. To overcome these drawbacks, we propose FedEdge, a novel edge-assisted hierarchical FL scheme that accelerates model training with asynchronous local federated training and adaptive model aggregation. Extensive experiments are conducted on two widely-used public datasets. The results demonstrate that, compared with state-of-the-art FL schemes, FedEdge accelerates model convergence by 1.14 × −3.20 ×, and improves model accuracy by 2.14% - 6.63%. | Kaibin Wang, Qiang He, Feifei Chen, Hai Jin, Yun Yang | Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; Deakin University, Australia |
| 631 |  |  [CausIL: Causal Graph for Instance Level Microservice Data](https://doi.org/10.1145/3543507.3583274) |  | 0 | AI-based monitoring has become crucial for cloud-based services due to its scale. A common approach to AI-based monitoring is to detect causal relationships among service components and build a causal graph. Availability of domain information makes cloud systems even better suited for such causal detection approaches. In modern cloud systems, however, auto-scalers dynamically change the number of microservice instances, and a load-balancer manages the load on each instance. This poses a challenge for off-the-shelf causal structure detection techniques as they neither incorporate the system architectural domain information nor provide a way to model distributed compute across varying numbers of service instances. To address this, we develop CausIL, which detects a causal structure among service metrics by considering compute distributed across dynamic instances and incorporating domain knowledge derived from system architecture. Towards the application in cloud systems, CausIL estimates a causal graph using instance-specific variations in performance metrics, modeling multiple instances of a service as independent, conditional on system assumptions. Simulation study shows the efficacy of CausIL over baselines by improving graph estimation accuracy by ~25% as measured by Structural Hamming Distance whereas the real-world dataset demonstrates CausIL's applicability in deployment settings. | Sarthak Chakraborty, Shaddy Garg, Shubham Agarwal, Ayush Chauhan, Shiv Kumar Saini | Adobe Research, India; The University of Texas at Austin, USA; Adobe, India |
| 632 |  |  [Learning Cooperative Oversubscription for Cloud by Chance-Constrained Multi-Agent Reinforcement Learning](https://doi.org/10.1145/3543507.3583298) |  | 0 | Oversubscription is a common practice for improving cloud resource utilization. It allows the cloud service provider to sell more resources than the physical limit, assuming not all users would fully utilize the resources simultaneously. However, how to design an oversubscription policy that improves utilization while satisfying the some safety constraints remains an open problem. Existing methods and industrial practices are over-conservative, ignoring the coordination of diverse resource usage patterns and probabilistic constraints. To address these two limitations, this paper formulates the oversubscription for cloud as a chance-constrained optimization problem and propose an effective Chance Constrained Multi-Agent Reinforcement Learning (C2MARL) method to solve this problem. Specifically, C2MARL reduces the number of constraints by considering their upper bounds and leverages a multi-agent reinforcement learning paradigm to learn a safe and optimal coordination policy. We evaluate our C2MARL on an internal cloud platform and public cloud datasets. Experiments show that our C2MARL outperforms existing methods in improving utilization ($20\%\sim 86\%$) under different levels of safety constraints. | Junjie Sheng, Lu Wang, Fangkai Yang, Bo Qiao, Hang Dong, Xiangfeng Wang, Bo Jin, Jun Wang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang | Microsoft Research, China; East China Normal University, China; Microsoft 365, USA |
| 633 |  |  [CMDiagnostor: An Ambiguity-Aware Root Cause Localization Approach Based on Call Metric Data](https://doi.org/10.1145/3543507.3583302) |  | 0 | The availability of online services is vital as its strong relevance to revenue and user experience. To ensure online services’ availability, quickly localizing the root causes of system failures is crucial. Given the high resource consumption of traces, call metric data are widely used by existing approaches to construct call graphs in practice. However, ambiguous correspondences between upstream and downstream calls may exist and result in exploring unexpected edges in the constructed call graph. Conducting root cause localization on this graph may lead to misjudgments of real root causes. To the best of our knowledge, we are the first to investigate such ambiguity, which is overlooked in the existing literature. Inspired by the law of large numbers and the Markov properties of network traffic, we propose a regression-based method (named AmSitor) to address this problem effectively. Based on AmSitor, we propose an ambiguity-aware root cause localization approach based on Call Metric Data named CMDiagnostor, containing metric anomaly detection, ambiguity-free call graph construction, root cause exploration, and candidate root cause ranking modules. The comprehensive experimental evaluations conducted on real-world datasets show that our CMDiagnostor can outperform the state-of-the-art approaches by 14% on the top-5 hit rate. Moreover, AmSitor can also be applied to existing baseline approaches separately to improve their performances one step further. The source code is released at https://github.com/NetManAIOps/CMDiagnostor. | Qingyang Yu, Changhua Pei, Bowen Hao, Mingjie Li, Zeyan Li, Shenglin Zhang, Xianglin Lu, Rui Wang, Jiaqi Li, Zhenyu Wu, Dan Pei | Computer Network Information Center, Chinese Academy of Sciences, China; Tencent, China; Nankai University, China; Tsinghua University, China |
| 634 |  |  [Visual-Aware Testing and Debugging for Web Performance Optimization](https://doi.org/10.1145/3543507.3583323) |  | 0 | Web performance optimization services, or web performance optimizers (WPOs), play a critical role in today’s web ecosystem by improving page load speed and saving network traffic. However, WPOs are known for introducing visual distortions that disrupt the users’ web experience. Unfortunately, visual distortions are hard to analyze, test, and debug, due to their subjective measure, dynamic content, and sophisticated WPO implementations. This paper presents Vetter, a novel and effective system that automatically tests and debugs visual distortions. Its key idea is to reason about the morphology of web pages, which describes the topological forms and scale-free geometrical structures of visual elements. Vetter efficiently calculates morphology and comparatively analyzes the morphologies of web pages before and after a WPO, which acts as a differential test oracle. Such morphology analysis enables Vetter to detect visual distortions accurately and reliably. Vetter further diagnoses the detected visual distortions to pinpoint the root causes in WPOs’ source code. This is achieved by morphological causal inference, which localizes the offending visual elements that trigger the distortion and maps them to the corresponding code. We applied Vetter to four representative WPOs. Vetter discovers 21 unknown defects responsible for 98% visual distortions; 12 of them have been confirmed and 5 have been fixed. | Xinlei Yang, Wei Liu, Hao Lin, Zhenhua Li, Feng Qian, Xianlong Wang, Yunhao Liu, Tianyin Xu |  |
| 635 |  |  [Demystifying Mobile Extended Reality in Web Browsers: How Far Can We Go?](https://doi.org/10.1145/3543507.3583329) |  | 0 | Mobile extended reality (XR) has developed rapidly in recent years. Compared with the app-based XR, XR in web browsers has the advantages of being lightweight and cross-platform, providing users with a pervasive experience. Therefore, many frameworks are emerging to support the development of XR in web browsers. However, little has been known about how well these frameworks perform and how complex XR apps modern web browsers can support on mobile devices. To fill the knowledge gap, in this paper, we conduct an empirical study of mobile XR in web browsers. We select seven most popular web-based XR frameworks and investigate their runtime performance, including 3D rendering, camera capturing, and real-world understanding. We find that current frameworks have the potential to further enhance their performance by increasing GPU utilization or improving computing parallelism. Besides, for 3D scenes with good rendering performance, developers can feel free to add camera capturing with little influence on performance to support augmented reality (AR) and mixed reality (MR) applications. Based on our findings, we draw several practical implications to provide better XR support in web browsers. | Weichen Bi, Yun Ma, Deyu Tian, Qi Yang, Mingtao Zhang, Xiang Jing | Institute for Artificial Intelligence, Peking University, China; School of Software & Microelectronics, Peking University, China; School of Computer Science and Engineering, University of New South Wales, Australia; School of Computer Science, Peking University, China |
| 636 |  |  [Look Deep into the Microservice System Anomaly through Very Sparse Logs](https://doi.org/10.1145/3543507.3583338) |  | 0 | Intensive monitoring and anomaly diagnosis have become a knotty problem for modern microservice architecture due to the dynamics of service dependency. While most previous studies rely heavily on ample monitoring metrics, we raise a fundamental but always neglected issue - the diagnostic metric integrity problem. This paper solves the problem by proposing MicroCU – a novel approach to diagnose microservice systems using very sparse API logs. We design a structure named dynamic causal curves to portray time-varying service dependencies and a temporal dynamics discovery algorithm based on Granger causal intervals. Our algorithm generates a smoother space of causal curves and designs the concept of causal unimodalization to calibrate the causality infidelities brought by missing metrics. Finally, a path search algorithm on dynamic causality graphs is proposed to pinpoint the root cause. Experiments on commercial system cases show that MicroCU outperforms many state-of-the-art approaches and reflects the superiorities of causal unimodalization to raw metric imputation. | Xinrui Jiang, Yicheng Pan, Meng Ma, Ping Wang | Peking University, China |
| 637 |  |  [Analyzing the Communication Clusters in Datacenters✱](https://doi.org/10.1145/3543507.3583410) |  | 0 | Datacenter networks have become a critical infrastructure of our digital society and over the last years, great efforts have been made to better understand the communication patterns inside datacenters. In particular, existing empirical studies showed that datacenter traffic typically features much temporal and spatial structure, and that at any given time, some communication pairs interact much more frequently than others. This paper generalizes this study to communication groups and analyzes how clustered the datacenter traffic is, and how stable these clusters are over time. To this end, we propose a methodology which revolves around a biclustering approach, allowing us to identify groups of racks and servers which communicate frequently over the network. In particular, we consider communication patterns occurring in three different Facebook datacenters: a Web cluster consisting of web servers serving web traffic, a Database cluster which mainly consists of MySQL servers, and a Hadoop cluster. Interestingly, we find that in all three clusters, small groups of racks and servers can produce a large fraction of the network traffic, and we can determine these groups even when considering short snapshots of network traffic. We also show empirically that these clusters are fairly stable across time. Our insights on the size and stability of communication clusters hence uncover an interesting potential for resource optimizations in datacenter infrastructures. | KlausTycho Foerster, Thibault Marette, Stefan Neumann, Claudia Plant, Ylli Sadikaj, Stefan Schmid, Yllka Velaj | KTH Royal Institute of Technology, Sweden; Faculty of Computer Science, University of Vienna, Austria; Faculty of Computer Science, University of Vienna, Austria and ds:Univie, University of Vienna, Austria; TU Berlin & University of Vienna, Germany; KTH Royal Institute of Technology, France; Faculty of Computer Science, University of Vienna, Austria and UniVie Doctoral School Computer Science, University of Vienna, Austria; TU Dortmund, Germany |
| 638 |  |  [DDPC: Automated Data-Driven Power-Performance Controller Design on-the-fly for Latency-sensitive Web Services](https://doi.org/10.1145/3543507.3583437) |  | 0 | Traditional power reduction techniques such as DVFS or RAPL are challenging to use with web services because they significantly affect the services’ latency and throughput. Previous work suggested the use of controllers based on control theory or machine learning to reduce performance degradation under constrained power. However, generating these controllers is challenging as every web service applications running in a data center requires a power-performance model and a fine-tuned controller. In this paper, we present DDPC, a system for autonomic data-driven controller generation for power-latency management. DDPC automates the process of designing and deploying controllers for dynamic power allocation to manage the power-performance trade-offs for latency-sensitive web applications such as a social network. For each application, DDPC uses system identification techniques to learn an adaptive power-performance model that captures the application’s power-latency trade-offs which is then used to generate and deploy a Proportional-Integral (PI) power controller with gain-scheduling to dynamically manage the power allocation to the server running application using RAPL. We evaluate DDPC with two realistic latency-sensitive web applications under varying load scenarios. Our results show that DDPC is capable of autonomically generating and deploying controllers within a few minutes reducing the active power allocation of a web-server by more than 50% compared to state-of-the-art techniques while maintaining the latency well below the target of the application. | Mehmet Savasci, Ahmed AliEldin, Johan Eker, Anders Robertsson, Prashant J. Shenoy | Lund University, Sweden; Chalmers University of Technology, Sweden; University of Massachusetts Amherst, USA; Ericsson Research, Sweden and Lund University, Sweden |
| 639 |  |  [Will Admins Cope? Decentralized Moderation in the Fediverse](https://doi.org/10.1145/3543507.3583487) |  | 0 | As an alternative to Twitter and other centralized social networks, the Fediverse is growing in popularity. The recent, and polemical, takeover of Twitter by Elon Musk has exacerbated this trend. The Fediverse includes a growing number of decentralized social networks, such as Pleroma or Mastodon, that share the same subscription protocol (ActivityPub). Each of these decentralized social networks is composed of independent instances that are run by different administrators. Users, however, can interact with other users across the Fediverse regardless of the instance they are signed up to. The growing user base of the Fediverse creates key challenges for the administrators, who may experience a growing burden. In this paper, we explore how large that overhead is, and whether there are solutions to alleviate the burden. We study the overhead of moderation on the administrators. We observe a diversity of administrator strategies, with evidence that administrators on larger instances struggle to find sufficient resources. We then propose a tool, WatchGen, to semi-automate the process. | Ishaku Hassan Anaobi, Aravindh Raman, Ignacio Castro, Haris Bin Zia, Damilola Ibosiola, Gareth Tyson | School of Electronic Engineering and Computer Science, Queen Mary University, United Kingdom; Hong Kong University of Science and Technology, China; Telefonica, Spain |
| 640 |  |  [Are Mobile Advertisements in Compliance with App's Age Group?](https://doi.org/10.1145/3543507.3583534) |  | 0 | As smartphones and mobile apps permeate every aspect of people’s lives, children are accessing mobile devices at an increasingly younger age. The inescapable exposure of advertisements in mobile apps to children has grown alarmingly. Mobile advertisements are placed by advertisers and subsequently distributed by ad SDKs, under the rare control of app developers and app markets’ content ratings. Indeed, content that is objectionable and harmful to children’s mental health has been reported to appear in advertising, such as pornography. However, few studies have yet concentrated on automatically and comprehensively identifying such kid-unsuitable mobile advertising. In this paper, we first characterize the regulations for mobile ads relating to children. We then propose our novel automated dynamic analysis framework, named AdRambler, that attempts to collect ad content throughout the lifespan of mobile ads and identify their inappropriateness for child app users. Using AdRambler, we conduct a large-scale (25,000 mobile apps) empirical investigation and reveal the non-incidental presence of inappropriate ads in apps with child-included target audiences. We collected 11,270 ad views and identified 1,289 ad violations (from 775 apps) of child user regulations, with roughly half of the app promotions not in compliance with host apps’ content ratings. Our finding indicates that even certified ad SDKs could still propagate inappropriate advertisements. We further delve into the question of accountability for the presence of inappropriate advertising and provide concrete suggestions for all stakeholders to take action for the benefit of children. | Yanjie Zhao, Tianming Liu, Haoyu Wang, Yepang Liu, John C. Grundy, Li Li | School of Software, Beihang University, China; Southern University of Science and Technology, China; School of Software, Beihang University, China and Monash University, Australia; Huazhong University of Science and Technology, China; Monash University, Australia |
| 641 |  |  [EdgeMove: Pipelining Device-Edge Model Training for Mobile Intelligence](https://doi.org/10.1145/3543507.3583540) |  | 0 | Training machine learning (ML) models on mobile and Web-of-Things (WoT) has been widely acknowledged and employed as a promising solution to privacy-preserving ML. However, these end-devices often suffer from constrained resources and fail to accommodate increasingly large ML models that crave great computation power. Offloading ML models partially to the cloud for training strikes a trade-off between privacy preservation and resource requirements. However, device-cloud training creates communication overheads that delay model training tremendously. This paper presents EdgeMove, the first device-edge training scheme that enables fast pipelined model training across edge devices and edge servers. It employs probing-based mechanisms to tackle the new challenges raised by device-edge training. Before training begins, it probes nearby edge servers’ training performance and bootstraps model training by constructing a training pipeline with an approximate model partitioning. During the training process, EdgeMove accommodates user mobility and system dynamics by probing nearby edge servers’ training performance adaptively and adapting the training pipeline proactively. Extensive experiments are conducted with two popular DNN models trained on four datasets for three ML tasks. The results demonstrate that EdgeMove achieves a 1.3 × -2.1 × speedup over the state-of-the-art scheme. | Zeqian Dong, Qiang He, Feifei Chen, Hai Jin, Tao Gu, Yun Yang | Macquarie University, Australia; Huazhong University of Science and Technology, China and Swinburne University of Technology, Australia; Swinburne University of Technology, Australia; Deakin University, Australia; Huazhong University of Science and Technology, China |
| 642 |  |  [HTTP Steady Connections for Robust Web Acceleration](https://doi.org/10.1145/3543507.3583550) |  | 0 | HTTP’s intrinsic request-and-response traffic pattern makes most web servers often idle, leaving a potential to accelerate page loads. We present the notion of HTTP steady connections, which fully utilizes the server’s available network bandwidth during a page load using the promising HTTP/3 server push, transforming the intermittent workload of loading a page into a more steady one. To construct a proper server push policy to achieve this, we separate the structure of a page, which is a relatively static factor, from the page load environments including client and network characteristics, which are generally dynamic and unknown to servers. We formulate a deadline-based sequencing problem using a page load model with dependency graphs and design a feedback-based reprioritization mechanism within HTTP server push to reactively match client progress robustly. Experiments with a prototype and a wide range of real-world pages show that HTTP steady connections significantly improve web page loads compared with state-of-the-art accelerators, even under packet losses and without any prior knowledge of network environments. | Sunjae Kim, Wonjun Lee | Korea University, Republic of Korea |
| 643 |  |  [Multitask Peer Prediction With Task-dependent Strategies](https://doi.org/10.1145/3543507.3583292) |  | 0 | Peer prediction aims to incentivize truthful reports from agents whose reports cannot be assessed with any objective ground truthful information. In the multi-task setting where each agent is asked multiple questions, a sequence of mechanisms have been proposed which are truthful — truth-telling is guaranteed to be an equilibrium, or even better, informed truthful — truth-telling is guaranteed to be one of the best-paid equilibria. However, these guarantees assume agents’ strategies are restricted to be task-independent: an agent’s report on a task is not affected by her information about other tasks. We provide the first discussion on how to design (informed) truthful mechanisms for task-dependent strategies, which allows the agents to report based on all her information on the assigned tasks. We call such stronger mechanisms (informed) omni-truthful. In particular, we propose the joint-disjoint task framework, a new paradigm which builds upon the previous penalty-bonus task framework. First, we show a natural reduction from mechanisms in the penalty-bonus task framework to mechanisms in the joint-disjoint task framework that maps every truthful mechanism to an omni-truthful mechanism. Such a reduction is non-trivial as we show that current penalty-bonus task mechanisms are not, in general, omni-truthful. Second, for a stronger truthful guarantee, we design the matching agreement (MA) mechanism which is informed omni-truthful. Finally, for the MA mechanism in the detail-free setting where no prior knowledge is assumed, we show how many tasks are required to (approximately) retain the truthful guarantees. | Yichi Zhang, Grant Schoenebeck | University of Michigan, USA |
| 644 |  |  [High-Effort Crowds: Limited Liability via Tournaments](https://doi.org/10.1145/3543507.3583334) |  | 0 | We consider the crowdsourcing setting where, in response to the assigned tasks, agents strategically decide both how much effort to exert (from a continuum) and whether to manipulate their reports. The goal is to design payment mechanisms that (1) satisfy limited liability (all payments are non-negative), (2) reduce the principal’s cost of budget, (3) incentivize effort and (4) incentivize truthful responses. In our framework, the payment mechanism composes a performance measurement, which noisily evaluates agents’ effort based on their reports, and a payment function, which converts the scores output by the performance measurement to payments. Previous literature suggests applying a peer prediction mechanism combined with a linear payment function. This method can achieve either (1), (3) and (4), or (2), (3) and (4) in the binary effort setting. In this paper, we suggest using a rank-order payment function (tournament). Assuming Gaussian noise, we analytically optimize the rank-order payment function, and identify a sufficient statistic, sensitivity, which serves as a metric for optimizing the performance measurements. This helps us obtain (1), (2) and (3) simultaneously. Additionally, we show that adding noise to agents’ scores can preserve the truthfulness of the performance measurements under the non-linear tournament, which gives us all four objectives. Our real-data estimated agent-based model experiments show that our method can greatly reduce the payment of effort elicitation while preserving the truthfulness of the performance measurement. In addition, we empirically evaluate several commonly used performance measurements in terms of their sensitivities and strategic robustness. | Yichi Zhang, Grant Schoenebeck | School of Information, University of Michigan, USA |
| 645 |  |  [Dynamic Interventions for Networked Contagions](https://doi.org/10.1145/3543507.3583470) |  | 0 | We study the problem of designing dynamic intervention policies for minimizing networked defaults in financial networks. Formally, we consider a dynamic version of the celebrated Eisenberg-Noe model of financial network liabilities and use this to study the design of external intervention policies. Our controller has a fixed resource budget in each round and can use this to minimize the effect of demand/supply shocks in the network. We formulate the optimal intervention problem as a Markov Decision Process and show how we can leverage the problem structure to efficiently compute optimal intervention policies with continuous interventions and provide approximation algorithms for discrete interventions. Going beyond financial networks, we argue that our model captures dynamic network intervention in a much broader class of dynamic demand/supply settings with networked inter-dependencies. To demonstrate this, we apply our intervention algorithms to various application domains, including ridesharing, online transaction platforms, and financial networks with agent mobility. In each case, we study the relationship between node centrality and intervention strength, as well as the fairness properties of the optimal interventions. | Marios Papachristou, Siddhartha Banerjee, Jon M. Kleinberg | Cornell University, USA |
| 646 |  |  [Randomized Pricing with Deferred Acceptance for Revenue Maximization with Submodular Objectives](https://doi.org/10.1145/3543507.3583477) |  | 0 | A lot of applications in web economics need to maximize the revenue under a budget for payments and also guarantee the truthfulness of users, so Budget-Feasible Mechanism (BFM) Design has aroused great interests during last decade. Most of the existing BFMs concentrate on maximizing a monotone submodular function subject to a knapsack constraint, which is insufficient for many applications with complex objectives or constraints. Observing this, the recent studies (e.g., [4, 5, 11]) have considered non-monotone submodular objectives or more complex constraints such as a k-system constraint. In this study, we follow this line of research and propose truthful BFMs with improved performance bounds for non-monotone submodular objectives with or without a k-system constraint. Our BFMs leverage the idea of providing random prices to users while deferring the decision on the final winning set, and are also based on a novel randomized algorithm for the canonical constrained submodular maximization problem achieving better performance bounds compared to the state-of-the-art. Finally, the effectiveness and efficiency of our approach are demonstrated by extensive experiments on several applications about social network marketing, crowdsourcing and personalized recommendation. | He Huang, Kai Han, Shuang Cui, Jing Tang | The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, China; School of Computer Science and Technology, Soochow University, China; School of Computer Science and Technology, University of Science and Technology of China, China |
| 647 |  |  [Fairness-aware Guaranteed Display Advertising Allocation under Traffic Cost Constraint](https://doi.org/10.1145/3543507.3583501) |  | 0 | Real-time Bidding (RTB) and Guaranteed Display (GD) advertising are two primary ways to sell impressions for publishers in online display advertising. Although GD contract serves less efficiently compared to RTB ads, it helps advertisers reach numerous target audiences at a lower cost and allows publishers to increase overall advertising revenue. However, with billion-scale requests online per day, it’s a challenging problem for publishers to decide whether and which GD ad to display for each impression. In this paper, we propose an optimal allocation model for GD contracts considering optimizing three objectives: maximizing guaranteed delivery and impressions’ quality and minimizing the extra traffic cost of GD contracts to increase overall revenue. The traffic cost of GD contracts is defined as the potential expected revenue if the impression is allocated to RTB ads. Our model dynamically adjusts the weights for each GD contract between impressions’ quality and traffic cost based on real-time performance, which produces fairness-aware allocation results. A parallel training framework based on Parameter-Server (PS) architecture is utilized to efficiently and periodically update the model. Deriving from the allocation model, we also propose a simple and adaptive online bidding strategy for GD contracts, which can be updated quickly by feedback-based algorithms to achieve optimal impression allocation even in complex and dynamic environments. We demonstrate the effectiveness of our proposed method by using both offline evaluation and online A/B testing. | Liang Dai, Zhonglin Zu, Hao Wu, Liang Wang, Bo Zheng | Alibaba Group, China |
| 648 |  |  [Is your digital neighbor a reliable investment advisor?](https://doi.org/10.1145/3543507.3583502) |  | 0 | The web and social media platforms have drastically changed how investors produce and consume financial advice. Historically, individual investors were often relying on newsletters and related prospectus backed by the reputation and track record of their issuers. Nowadays, financial advice is frequently offered online, by anonymous or pseudonymous parties with little at stake. As such, a natural question is to investigate whether these modern financial “influencers” operate in good faith, or whether they might be misleading their followers intentionally. To start answering this question, we obtained data from a very large cryptocurrency derivatives exchange, from which we derived individual trading positions. Some of the investors on that platform elect to link to their Twitter profiles. We were thus able to compare the positions publicly espoused on Twitter with those actually taken in the market. We discovered that 1) staunchly “bullish” investors on Twitter often took much more moderate, if not outright opposite, positions in their own trades when the market was down, 2) their followers tended to align their positions with bullish Twitter outlooks, and 3) moderate voices on Twitter (and their own followers) were on the other hand far more consistent with their actual investment strategies. In other words, while social media advice may attempt to foster a sense of camaraderie among people of like-minded beliefs, the reality is that this is merely an illusion, which may result in financial losses for people blindly following advice. | Daisuke Kawai, Alejandro Cuevas, Bryan R. Routledge, Kyle Soska, Ariel ZetlinJones, Nicolas Christin | Ramiel Capital, USA; Carnegie Mellon University, USA |
| 649 |  |  [Impartial Selection with Prior Information](https://doi.org/10.1145/3543507.3583553) |  | 0 | We study the problem of {\em impartial selection}, a topic that lies at the intersection of computational social choice and mechanism design. The goal is to select the most popular individual among a set of community members. The input can be modeled as a directed graph, where each node represents an individual, and a directed edge indicates nomination or approval of a community member to another. An {\em impartial mechanism} is robust to potential selfish behavior of the individuals and provides appropriate incentives to voters to report their true preferences by ensuring that the chance of a node to become a winner does not depend on its outgoing edges. The goal is to design impartial mechanisms that select a node with an in-degree that is as close as possible to the highest in-degree. We measure the efficiency of such a mechanism by the difference of these in-degrees, known as its {\em additive} approximation. In particular, we study the extent to which prior information on voters' preferences could be useful in the design of efficient deterministic impartial selection mechanisms with good additive approximation guarantees. We consider three models of prior information, which we call the {\em opinion poll}, the {\em a prior popularity}, and the {\em uniform} model. We analyze the performance of a natural selection mechanism that we call {\em approval voting with default} (AVD) and show that it achieves a $O(\sqrt{n\ln{n}})$ additive guarantee for opinion poll and a $O(\ln^2n)$ for a priori popularity inputs, where $n$ is the number of individuals. We consider this polylogarithmic bound as our main technical contribution. We complement this last result by showing that our analysis is close to tight, showing an $\Omega(\ln{n})$ lower bound. This holds in the uniform model, which is the simplest among the three models. | Ioannis Caragiannis, George Christodoulou, Nicos Protopapas | Aarhus University, Denmark; Aristotle University of Thessaloniki, Greece and Archimedes/RC Athena, Greece; University of Patras, Greece |
| 650 |  |  [Do Language Models Plagiarize?](https://doi.org/10.1145/3543507.3583199) |  | 0 | Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs "reuse" a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs' plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs' training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals' personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism. | Jooyoung Lee, Thai Le, Jinghui Chen, Dongwon Lee |  |
| 651 |  |  [Path-specific Causal Fair Prediction via Auxiliary Graph Structure Learning](https://doi.org/10.1145/3543507.3583280) |  | 0 | With ubiquitous adoption of machine learning algorithms in web technologies, such as recommendation system and social network, algorithm fairness has become a trending topic, and it has a great impact on social welfare. Among different fairness definitions, path-specific causal fairness is a widely adopted one with great potentials, as it distinguishes the fair and unfair effects that the sensitive attributes exert on algorithm predictions. Existing methods based on path-specific causal fairness either require graph structure as the prior knowledge or have high complexity in the calculation of path-specific effect. To tackle these challenges, we propose a novel casual graph based fair prediction framework which integrates graph structure learning into fair prediction to ensure that unfair pathways are excluded in the causal graph. Furthermore, we generalize the proposed framework to the scenarios where sensitive attributes can be non-root nodes and affected by other variables, which is commonly observed in real-world applications, such as recommendation system, but hardly addressed by existing works. We provide theoretical analysis on the generalization bound for the proposed fair prediction method, and conduct a series of experiments on real-world datasets to demonstrate that the proposed framework can provide better prediction performance and algorithm fairness trade-off. | Liuyi Yao, Yaliang Li, Bolin Ding, Jingren Zhou, Jinduo Liu, Mengdi Huai, Jing Gao | Alibaba, China; Alibaba Group, China; Beijing University of Technology, China; Alibaba Group, USA; Purdue University, USA; Iowa State University, USA |
| 652 |  |  [HateProof: Are Hateful Meme Detection Systems really Robust?](https://doi.org/10.1145/3543507.3583356) |  | 0 | Exploiting social media to spread hate has tremendously increased over the years. Lately, multi-modal hateful content such as memes has drawn relatively more traction than uni-modal content. Moreover, the availability of implicit content payloads makes them fairly challenging to be detected by existing hateful meme detection systems. In this paper, we present a use case study to analyze such systems' vulnerabilities against external adversarial attacks. We find that even very simple perturbations in uni-modal and multi-modal settings performed by humans with little knowledge about the model can make the existing detection models highly vulnerable. Empirically, we find a noticeable performance drop of as high as 10% in the macro-F1 score for certain attacks. As a remedy, we attempt to boost the model's robustness using contrastive learning as well as an adversarial training-based method - VILLA. Using an ensemble of the above two approaches, in two of our high resolution datasets, we are able to (re)gain back the performance to a large extent for certain attacks. We believe that ours is a first step toward addressing this crucial problem in an adversarial setting and would inspire more such investigations in the future. | Piush Aggarwal, Pranit Chawla, Mithun Das, Punyajoy Saha, Binny Mathew, Torsten Zesch, Animesh Mukherjee |  |
| 653 |  |  [DualFair: Fair Representation Learning at Both Group and Individual Levels via Contrastive Self-supervision](https://doi.org/10.1145/3543507.3583480) |  | 0 | Algorithmic fairness has become an important machine learning problem, especially for mission-critical Web applications. This work presents a self-supervised model, called DualFair, that can debias sensitive attributes like gender and race from learned representations. Unlike existing models that target a single type of fairness, our model jointly optimizes for two fairness criteria - group fairness and counterfactual fairness - and hence makes fairer predictions at both the group and individual levels. Our model uses contrastive loss to generate embeddings that are indistinguishable for each protected group, while forcing the embeddings of counterfactual pairs to be similar. It then uses a self-knowledge distillation method to maintain the quality of representation for the downstream tasks. Extensive analysis over multiple datasets confirms the model's validity and further shows the synergy of jointly addressing two fairness criteria, suggesting the model's potential value in fair intelligent Web applications. | Sungwon Han, SeungEon Lee, Fangzhao Wu, Sundong Kim, Chuhan Wu, Xiting Wang, Xing Xie, Meeyoung Cha |  |
| 654 |  |  [PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction](https://doi.org/10.1145/3543507.3583511) |  | 0 | Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation. | Shichang Zhang, Jiani Zhang, Xiang Song, Soji Adeshina, Da Zheng, Christos Faloutsos, Yizhou Sun |  |
| 655 |  |  [Fairness in model-sharing games](https://doi.org/10.1145/3543507.3583483) |  | 0 | In many real-world situations, data is distributed across multiple self-interested agents. These agents can collaborate to build a machine learning model based on data from multiple agents, potentially reducing the error each experiences. However, sharing models in this way raises questions of fairness: to what extent can the error experienced by one agent be significantly lower than the error experienced by another agent in the same coalition? In this work, we consider two notions of fairness that each may be appropriate in different circumstances: egalitarian fairness (which aims to bound how dissimilar error rates can be) and proportional fairness (which aims to reward players for contributing more data). We similarly consider two common methods of model aggregation, one where a single model is created for all agents (uniform), and one where an individualized model is created for each agent. For egalitarian fairness, we obtain a tight multiplicative bound on how widely error rates can diverge between agents collaborating (which holds for both aggregation methods). For proportional fairness, we show that the individualized aggregation method always gives a small player error that is upper bounded by proportionality. For uniform aggregation, we show that this upper bound is guaranteed for any individually rational coalition (where no player wishes to leave to do local learning). | Kate Donahue, Jon M. Kleinberg | Cornell University, USA |
| 656 |  |  [Combining Worker Factors for Heterogeneous Crowd Task Assignment](https://doi.org/10.1145/3543507.3583190) |  | 0 | Optimising the assignment of tasks to workers is an effective approach to ensure high quality in crowdsourced data - particularly in heterogeneous micro tasks. However, previous attempts at heterogeneous micro task assignment based on worker characteristics are limited to using cognitive skills, despite literature emphasising that worker performance varies based on other parameters. This study is an initial step towards understanding whether and how multiple parameters such as cognitive skills, mood, personality, alertness, comprehension skill, and social and physical context of workers can be leveraged in tandem to improve worker performance estimations in heterogeneous micro tasks. Our predictive models indicate that these parameters have varying effects on worker performance in the five task types considered – sentiment analysis, classification, transcription, named entity recognition and bounding box. Moreover, we note 0.003 - 0.018 reduction in mean absolute error of predicted worker accuracy across all tasks, when task assignment is based on models that consider all parameters vs. models that only consider workers’ cognitive skills. Our findings pave the way for the use of holistic approaches in micro task assignment that effectively quantify worker context. | Senuri Wijenayake, Danula Hettiachchi, Jorge Gonçalves | The University of Melbourne, Australia; The University of Sydney, Australia; RMIT University, Australia |
| 657 |  |  [Hidden Indicators of Collective Intelligence in Crowdfunding](https://doi.org/10.1145/3543507.3583414) |  | 0 | Extensive literature argues that crowds possess essential collective intelligence benefits that allow superior decision-making by untrained individuals working in low-information environments. Classic wisdom of crowds theory is based on evidence gathered from studying large groups of diverse and independent decision-makers. Yet, most human decisions are reached in online settings of interconnected like-minded people that challenge these criteria. This observation raises a key question: Are there surprising expressions of collective intelligence online? Here, we explore whether crowds furnish collective intelligence benefits in crowdfunding systems. Crowdfunding has grown and diversified quickly over the past decade, expanding from funding aspirant creative works and supplying pro-social donations to enabling large citizen-funded urban projects and providing commercial interest-based unsecured loans. Using nearly 10 million loan contributions from a market-dominant lending platform, we find evidence for collective intelligence indicators in crowdfunding. Our results, which are based on a two-stage Heckman selection model, indicate that opinion diversity and the speed at which funds are contributed predict who gets funded and who repays, even after accounting for traditional measures of creditworthiness. Moreover, crowds work consistently well in correctly assessing the outcome of high-risk projects. Finally, diversity and speed serve as early warning signals when inferring fundraising based solely on the initial part of the campaign. Our findings broaden the field of crowd-aware system design and inform discussions about the augmentation of traditional financing systems with tech innovations. | EmokeÁgnes Horvát, Henry Kudzanai Dambanemuya, Jayaram Uparna, Brian Uzzi | Indian Institute of Management Udaipur, India; Northwestern University, USA |
| 658 |  |  [Multiview Representation Learning from Crowdsourced Triplet Comparisons](https://doi.org/10.1145/3543507.3583431) |  | 0 | Crowdsourcing has been used to collect data at scale in numerous fields. Triplet similarity comparison is a type of crowdsourcing task, in which crowd workers are asked the question \`\`among three given objects, which two are more similar?'', which is relatively easy for humans to answer. However, the comparison can be sometimes based on multiple views, i.e., different independent attributes such as color and shape. Each view may lead to different results for the same three objects. Although an algorithm was proposed in prior work to produce multiview embeddings, it involves at least two problems: (1) the existing algorithm cannot independently predict multiview embeddings for a new sample, and (2) different people may prefer different views. In this study, we propose an end-to-end inductive deep learning framework to solve the multiview representation learning problem. The results show that our proposed method can obtain multiview embeddings of any object, in which each view corresponds to an independent attribute of the object. We collected two datasets from a crowdsourcing platform to experimentally investigate the performance of our proposed approach compared to conventional baseline methods. | Xiaotian Lu, Jiyi Li, Koh Takeuchi, Hisashi Kashima | University of Yamanashi, Japan; Kyoto University, Japan |
| 659 |  |  [Sedition Hunters: A Quantitative Study of the Crowdsourced Investigation into the 2021 U.S. Capitol Attack](https://doi.org/10.1145/3543507.3583514) |  | 0 | Social media platforms have enabled extremists to organize violent events, such as the 2021 U.S. Capitol Attack. Simultaneously, these platforms enable professional investigators and amateur sleuths to collaboratively collect and identify imagery of suspects with the goal of holding them accountable for their actions. Through a case study of Sedition Hunters, a Twitter community whose goal is to identify individuals who participated in the 2021 U.S. Capitol Attack, we explore what are the main topics or targets of the community, who participates in the community, and how. Using topic modeling, we find that information sharing is the main focus of the community. We also note an increase in awareness of privacy concerns. Furthermore, using social network analysis, we show how some participants played important roles in the community. Finally, we discuss implications for the content and structure of online crowdsourced investigations. | Tianjiao Yu, Sukrit Venkatagiri, Ismini Lourentzou, Kurt Luther | University of Washington, USA; Virginia Tech, USA |
| 660 |  |  [Human-in-the-loop Regular Expression Extraction for Single Column Format Inconsistency](https://doi.org/10.1145/3543507.3583515) |  | 0 | Format inconsistency is one of the most frequently appearing data quality issues encountered during data cleaning. Existing automated approaches commonly lack applicability and generalisability, while approaches with human inputs typically require specialized skills such as writing regular expressions. This paper proposes a novel hybrid human-machine system, namely “Data-Scanner-4C”, which leverages crowdsourcing to address syntactic format inconsistencies in a single column effectively. We first ask crowd workers to create examples from single-column data through “data selection” and “result validation” tasks. Then, we propose and use a novel rule-based learning algorithm to infer the regular expressions that propagate formats from created examples to the entire column. Our system integrates crowdsourcing and algorithmic format extraction techniques in a single workflow. Having human experts write regular expressions is no longer required, thereby reducing both the time as well as the opportunity for error. We conducted experiments through both synthetic and real-world datasets, and our results show how the proposed approach is applicable and effective across data types and formats. | Shaochen Yu, Lei Han, Marta Indulska, Shazia W. Sadiq, Gianluca Demartini | The University of Queensland, Australia |
| 661 |  |  [Identifying Creative Harmful Memes via Prompt based Approach](https://doi.org/10.1145/3543507.3587427) |  | 0 | The creative nature of memes has made it possible for harmful content to spread quickly and widely on the internet. Harmful memes can range from spreading hate speech promoting violence, and causing emotional distress to individuals or communities. These memes are often designed to be misleading, manipulative, and controversial, making it challenging to detect and remove them from online platforms. Previous studies focused on how to fuse visual and language modalities to capture contextual information. However, meme analysis still severely suffers from data deficiency, resulting in insufficient learning of fusion modules. Further, using conventional pretrained encoders for text and images exhibits a greater semantic gap in feature spaces and leads to low performance. To address these gaps, this paper reformulates a harmful meme analysis as an auto-filling and presents a prompt-based approach to identify harmful memes. Specifically, we first transform multimodal data to a single (i.e., textual) modality by generating the captions and attributes of the visual data and then prepend the textual data in the prompt-based pre-trained language model. Experimental results on two benchmark harmful memes datasets demonstrate that our method outperformed state-of-the-art methods. We conclude with the transferability and robustness of our approach to identify creative harmful memes. | Junhui Ji, Wei Ren, Usman Naseem | School of Computer Science, University of Sydney, Australia |
| 662 |  |  [SA-Fusion: Multimodal Fusion Approach for Web-based Human-Computer Interaction in the Wild](https://doi.org/10.1145/3543507.3587429) |  | 0 | Web-based AR technology has broadened human-computer interaction scenes from traditional mechanical devices and flat screens to the real world, resulting in unconstrained environmental challenges such as complex backgrounds, extreme illumination, depth range differences, and hand-object interaction. The previous hand detection and 3D hand pose estimation methods are usually based on single modality such as RGB or depth data, which are not available in some scenarios in unconstrained environments due to the differences between the two modalities. To address this problem, we propose a multimodal fusion approach, named Scene-Adapt Fusion (SA-Fusion), which can fully utilize the complementarity of RGB and depth modalities in web-based HCI tasks. SA-Fusion can be applied in existing hand detection and 3D hand pose estimation frameworks to boost their performance, and can be further integrated into the prototyping AR system to construct a web-based interactive AR application for unconstrained environments. To evaluate the proposed multimodal fusion method, we conduct two user studies on CUG Hand and DexYCB dataset, to demonstrate its effectiveness in terms of accurately detecting hand and estimating 3D hand pose in unconstrained environments and hand-object interaction. | Xingyu Liu, Pengfei Ren, Yuchen Chen, Cong Liu, Jing Wang, Haifeng Sun, Qi Qi, Jingyu Wang | State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China; China Mobile Research Institute, China |
| 663 |  |  [The Harmonic Memory: a Knowledge Graph of harmonic patterns as a trustworthy framework for computational creativity](https://doi.org/10.1145/3543507.3587428) |  | 0 | Computationally creative systems for music have recently achieved impressive results, fuelled by progress in generative machine learning. However, black-box approaches have raised fundamental concerns for ethics, accountability, explainability, and musical plausibility. To enable trustworthy machine creativity, we introduce the Harmonic Memory, a Knowledge Graph (KG) of harmonic patterns extracted from a large and heterogeneous musical corpus. By leveraging a cognitive model of tonal harmony, chord progressions are segmented into meaningful structures, and patterns emerge from their comparison via harmonic similarity. Akin to a music memory, the KG holds temporal connections between consecutive patterns, as well as salient similarity relationships. After demonstrating the validity of our choices, we provide examples of how this design enables novel pathways for combinational creativity. The memory provides a fully accountable and explainable framework to inspire and support creative professionals – allowing for the discovery of progressions consistent with given criteria, the recomposition of harmonic sections, but also the co-creation of new progressions. | Jacopo de Berardinis, Albert MeroñoPeñuela, Andrea Poltronieri, Valentina Presutti | Department of Informatics, King's College London, United Kingdom; Department of Modern Languages, Literatures, and Cultures, University of Bologna, Italy; Deapartment of Computer Science and Engineering, University of Bologna, Italy |
| 664 |  |  [A Prompt Log Analysis of Text-to-Image Generation Systems](https://doi.org/10.1145/3543507.3587430) |  | 0 | Recent developments in large language models (LLM) and generative AI have unleashed the astonishing capabilities of text-to-image generation systems to synthesize high-quality images that are faithful to a given reference text, known as a "prompt". These systems have immediately received lots of attention from researchers, creators, and common users. Despite the plenty of efforts to improve the generative models, there is limited work on understanding the information needs of the users of these systems at scale. We conduct the first comprehensive analysis of large-scale prompt logs collected from multiple text-to-image generation systems. Our work is analogous to analyzing the query logs of Web search engines, a line of work that has made critical contributions to the glory of the Web search industry and research. Compared with Web search queries, text-to-image prompts are significantly longer, often organized into special structures that consist of the subject, form, and intent of the generation tasks and present unique categories of information needs. Users make more edits within creation sessions, which present remarkable exploratory patterns. There is also a considerable gap between the user-input prompts and the captions of the images included in the open training data of the generative models. Our findings provide concrete implications on how to improve text-to-image generation systems for creation purposes. | Yutong Xie, Zhaoying Pan, Jinge Ma, Luo Jie, Qiaozhu Mei | Electrical Engineering and Computer Science Department, University of Michigan, USA; Niantic Inc., USA; School of Information, University of Michigan, USA |
| 665 |  |  [CAM: A Large Language Model-based Creative Analogy Mining Framework](https://doi.org/10.1145/3543507.3587431) |  | 0 | Analogies inspire creative solutions to problems, and facilitate the creative expression of ideas and the explanation of complex concepts. They have widespread applications in scientific innovation, creative writing, and education. The ability to discover creative analogies that are not explicitly mentioned but can be inferred from the web is highly desirable to power all such applications dynamically and augment human creativity. Recently, Large Pre-trained Language Models (PLMs), trained on massive Web data, have shown great promise in generating mostly known analogies that are explicitly mentioned on the Web. However, it is unclear how they could be leveraged for mining creative analogies not explicitly mentioned on the Web. We address this challenge and propose Creative Analogy Mining (CAM), a novel framework for mining creative analogies, which consists of the following three main steps: 1) Generate analogies using PLMs with effectively designed prompts, 2) Evaluate their quality using scoring functions, and 3) Refine the low-quality analogies by another round of prompt-based generation. We propose both unsupervised and supervised instantiations of the framework so that it can be used even without any annotated data. Based on human evaluation using Amazon Mechanical Turk, we find that our unsupervised framework can mine 13.7% highly-creative and 56.37% somewhat-creative analogies. Moreover, our supervised scores are generally better than the unsupervised ones and correlate moderately with human evaluators, indicating that they would be even more effective at mining creative analogies. These findings also shed light on the creativity of PLMs 1. | Bhavya, Jinjun Xiong, Chengxiang Zhai | University of Illinois at Urbana-Champaign, USA; University at Buffalo, USA |
| 666 |  |  [Tangible Web: An Interactive Immersion Virtual Reality Creativity System that Travels Across Reality](https://doi.org/10.1145/3543507.3587432) |  | 0 | We are all connected." These days, one hears constant reference to the notion that all human beings are in some fashion related and that what one person does inevitably affects other people around them. The idea has permeated pop culture in recent years, with novelists even making human ties tangible or visible in some way. In Stranger in a Strange Land, a Martian teaches humans to grok, which at first means only to understand one another, but which takes on richer dimensions as the book progresses. Carlos Castaneda made human ties visible to spiritual questers in his Journey to Ixtlan series. Orson Scott Card (Ender's Game, The Lost Boys) more recently gave human ties a basis in physics and used them for interstellar transport in one of his science fiction series. There is another sense in which "we are all connected" has come true in recent years. Since the invention of the telegraph in the 1840s, instantaneous communication has been possible and wires connecting people to one another have spread like kudzu across the face of the planet. The telephone network and, more recently, the Internet (the convergence of telecom and computing) are further tangible, visible expressions of the ties between people. It is an increasingly networked world. The observation is now commonplace that global computer networking has enabled the state-of-the-art in many fields to advance more quickly. Formerly, researchers were isolated and had to wait years for the work of others in their field to be available in published form. Now, with the Internet, researchers can communicate their results instantly to others whose minds are similarly engaged. Collaborative effort has to some extent displaced solitary pursuit. In this connection, the Internet has been called the World Brain. But humans are connected not only through their intellects but also through their emotional ties. Completely overlooked has been the amazing potential of the Internet to facilitate emotional ties among people no matter where they happen to be physically located and, thus, to draw the human family closer together. Internet technologies can be deployed to favorably condition empathic response in those who have offended against community norms out of a lack of appreciation for the effects that their criminal behavior has on other people. Technology is widely perceived as soulless and even as anti-human by some. But technology is only a tool and can be placed in the service of humane values. There is no reason the Internet cannot become the World Heart as well as the World Brain. Thinking in the field of criminology has cycled around broad themes of punishment, deterrence and rehabilitation for centuries. The recent trend toward victims' rights is really just the latest variation on a very old theme. The primacy of the victim hearkens back to Saxon, England, where victims and offenders handled restitution privately among themselves. Called "restorative justice" today, the concept has spawned, among other things, Victim-Offender Rehabilitation Programs which bring offenders face-to-face with their victims, the end product being written restitution agreements. Many offenders find emotional release in being understood ("I wanted to let them know I am not a bad person. I just made a mistake."). Forgiveness, understanding, redemption, empathy, atonement, the opportunity to "make things right" - the emotional intensity of these programs for offenders demonstrates that emotions are in play in offender psychology. This raises the possibility that emotions can be used to restructure thinking and reduce criminal propensities. Some years back, a TV newsmagazine featured a woman who went around to prisons talking about what a crime did to her and her family. She had the prisoners in tears as they came to realize the extent of the harm they themselves had caused their own victims. Empathy is now taught in some prisons and grade schools. One school found that students eventually internalize empathy and learn to rein in their own behavior. In California prisons, victim affinity groups explore the effects of a participant's crime on the victim, the victim's family and the offender's own family. In this way, empathy is learned and self-control is reinforced. One must make only modest claims for the benefits of teaching empathy to offenders. No single technique will reach every offender. Some will be deterred by punishment, and others will achieve self-control through moral instruction or the removal of cognitive distortions like exaggerated needs for immediate gratification. Nevertheless, some offenders will arrive at correct behavior through their emotional intelligence and, therefore, there is value in exploring new ways to encourage greater empathic response among them. Teaching offenders empathy might not change the world, but it will help. The Internet is a powerful medium. Its power lies in the ways it can allow people to interact. The Internet is an interactive medium, unlike today's TV or newspapers. It is the interactive nature of the Internet that opens new possibilities for teaching empathy to offenders. Reading a book is usually a solitary experience. Writing interactive fiction in real-time with other online users or participating in other online forums shifts the paradigm from solitary activity to collaboration. Successful collaboration requires consideration of others and their feelings, and thus may greater empathic response be born. Recreational materials like science fiction paperbacks and music cassettes are the most popular items in prison collections. However, many offenders are also interested in improving their lives. Many seek transformational experiences, creative writing outlets, ways to remain connected to the larger world around them and new skills like computer literacy to ease their transition back into community life. Many have trouble reading. Information professionals have a golden opportunity to address all these needs with current and future networking technologies. Prisoners receive visitors and get mail from the outside. Networking technologies can provide additional ties to the outside world and create a sense of community for those prisoners who would not dream of speaking up at a lecture or being brought face-to-face with their victims. Networking can also let prison librarians leverage their resources. Whatever is created in one location can be networked to other facilities via the Internet. The types of forums and multi-user domains available today and the virtual worlds of tomorrow can be disseminated instantly to other prisons around the world. The Present - Jurisdictions vary in the degree to which they currently use networking technologies in prison libraries. In Maryland, for example, the state is working to provide e-mail and the state information system ('Sailor') to its prison librarians. Because security is viewed as a manageable problem, some degree of Internet access for prisoners is envisioned for the future. In stark contrast, authorities in the federal system believe that sophisticated inmates will use the Internet to run criminal enterprises from prison and therefore have declared, once and for all, "no modems for prisoners." However, the availability of secure servers and firewall technology puts the federal authorities on the wrong side of history. The federal system may hold out for a time but will eventually follow Maryland's lead. The networking paradigm is unstoppable. A number of networking technologies and applications that could be used in prison libraries to teach empathy or serve other valid purposes already exist: • Interactive Forums - Moderated forums and online conferences can be used to bring victim presentations and victim affinity groups to broader prison audiences. Victims could tell their stories and offenders could discuss how becoming aware of the damage done to another changes their perceptions. • Interactive Fiction - Several multi-user domains (MUDs) have arisen in which hundreds of people participate in writing a story. Many prisoners have a strong need to write, and this would afford them the opportunity to do so in a collaborative instead of solitary fashion. There are classics of prison literature and someday there will be new classics of interactive prison literature. • Web Radio - Inmates could work together to produce shows on any number of topics - substance abuse, for example. Entire talk shows (complete with listeners calling in) could be offered. • Information Needs - Some argue that inmates have a "right to read" and should have access to the same materials available to other people. If so, then Internet access is the next logical step. A wealth of good, solid information is available on the Internet and can be brought to the desktop with the click of a mouse. Not only are books available in electronic form over the Internet, some are even available as audio files, perfect for a population with low education levels and literacy needs. Many people now research their own medical problems on the Internet. • Education - Many inmates take college-level courses and pursue degrees while in prison. Entire distance learning courses are delivered over the Internet and could be made available to inmates. • Recreation and Entertainment - There are any number of Web-based games now available from chess to action games like Quake! These have become enormously popular and provide interactivity with others who are online at the same time. The Future - Behold the future: networked virtual reality. Networked virtual reality is destined to play a prominent role in the future of mass computing because it is an extension of what may be called the "graphical revolution." Until recent times, computers were difficult to operate, required specialized knowledge of arcane keyboard commands and were the province of experts. It was not until the early 1980s that computers began to win a place in large numbers on the desktops of ordinary users. The creation of a mass market had to await the "graphical user interface," i.e., the point-&-click operating systems that made computers much easier to use. Ease of use is the cardinal principle that drives mass market adoption of new technologies. The graphical revolution continued in the mid-1990s when the Web, with its colorful graphics and point-&-click browser software, virtually eclipsed all of gopherspace and turned the Internet into a mass phenomenon. The next step in the graphical revolution is networked virtual reality precisely because the same ingredients are at work - visualization and ease of use. Networked virtual reality will have numerous applications - some useful and some entertaining - and will require no specialized knowledge of computing to operate. Virtual Reality is a term loosely applied to a set of developments ranging from enhanced computer graphics to the creation of entire imaginary worlds in which the computer user feels totally immersed. The ability to select any and all viewing positions places the user inside a computer-generated world that can be explored much like someone would move around a zoo or shopping mall in real life. Although the line is somewhat blurred between enhanced computer graphics and virtual reality, it is said that the sense of total immersion and the ability to interact with features within the simulated environment are the distinguishing characteristics of virtual reality. Everyone has seen the clumsy head gear commonly associated with virtual reality, but avatars (on-screen user representations having human form), voice commands and head trackers can effectively allow users to feel as if they are inhabiting the computer-generated space without bulky helmets, body suits or data gloves, simply by viewing an ordinary screen. The three-dimensional visual effects can be enhanced by using stereoscopic glasses of various types already available. Just a few years ago, the idea of a virtual reality Net was the stuff of science fiction. However, networked virtual environments will soon be upon us. Virtual space on the Internet is already in its infancy, characterized by static scenes and cartoon worlds. One can already visit sites on the Web featuring enhanced 3-D graphics generated with Virtual Reality Markup Language (VRML). The important point to grasp is that the Internet is not standing still; it is continuously undergoing development and improvement. Next Generation Internet initiatives will bring increased bandwidth, distributed supercomputing and virtual reality to the desktop within the next 5 to10 years. Because virtual reality is graphical, easy to use and requires no formal computer training, it is destined to hit a home run in the mass market. Accessing virtual worlds on the Net will be as routine tomorrow as online text searching is today. Virtual reality is already being used in training and education. VR flight simulators give new airline pilots a good sense of what it is like to fly a real plane. Virtual reality is also being used to recondition emotional responses. VR experiments have been conducted and have been proven effective in desensitizing people's fears of, among other things, bugs and airline travel. Decreased fears have been documented and the effects have been shown to be long-lasting. Moreover, subjects report that they recall their VR experiences when they encounter the real thing and that this calms them down. Thus, VR is shaping up to be a very powerful teaching tool. Using computer-generated environments to simulate real-world experiences allows people to learn at their own pace. Additionally, people are more highly motivated to learn when they are working through their own choices. Moreover, VR learning is multimodal and studies show that higher retention is achieved when learners "see, hear and do" than when the signal comes through just a single channel. VR lets people learn from their experiences, not just memorize a bunch of rules. For all of these reasons, training in sophisticated settings from business to the military is shifting to virtual reality. If VR has been shown effective in desensitizing emotional response, can the use of VR for consciousness-raising and heightening sensitivity be far behind? Psychology software is already in use in prisons addressing problem areas like addiction, stress, communication skills and relationships. Inmates are amazed at how accurately the programs describe their deficiencies. Virtual reality is the obvious next step. Virtual reality, because it can engender a sense of participating in an experience in which personal human choices determine the outcome, can be used to teach empathy to criminal offenders. Here are some ideas for using VR to teach empathy and to impart other humane values to prisoners: • Sage - An inmate takes a stroll in the garden with Aristotle, Martin Luther King or other wise personage who would be programmed to answer questions in words close to what the figure actually said in history. Physical activities could include sitting on benches and tossing coins from a bridge. • Ouch! - An inmate gives a virtual haircut to a person seated in a barber's chair. The challenge would be to find out what kind of haircut the person wants and to accomplish the task without inflicting pain. It would take time for the inmate to achieve the dexterity required to do a good job for someone without hurting them. Success in the game would be rewarded in real life with additional days of "good time" (time off for good behavior built into every sentence). • Commute - Here the inmate takes a ride on a crowded subway and tries to maneuver without bumping into anyone or knocking anyone with a briefcase. A pregnant woman with small child passes by a seated offender until the offender gets the point that the seat should be given up to a person who needs it more. Points would accrue if the offender apologizes for mistakes. This could be a multi-user environment offering interaction with other prisoners who are online at the same time. • Red Cross - A multi-user game where offenders win good time by successfully helping the Red Cross give relief to victims at an earthquake or other virtual disaster scene. Offenders must master carrying blankets, putting up tents, affixing bandages, resuscitating victims, etc. Some of these tasks would require cooperation so offenders would not win points unless they learned how to interact successfully with others. The casualty count and thus the reward of "good-time" would be determined by how effectively inmates cooperate in administering aid. • Everest - A single- or multi-user game in which the offender joins an expedition to climb Mt. Everest. Being tied by a rope to other climbers forces the inmate into an interdependent situation in which helping and being helped are critical to survival when missteps occur. • Jumbo Jet - An interactive modeling game where inmates cooperate in designing a jumbo jetliner and getting it to fly successfully. • Sighted Guide - Last, but not least, a game in which inmates score points by successfully leading a blind person through a building. This scenario is a natural for virtual reality because the guide has to navigate around corners, assist the blind person on a set of stairs, etc., all the while using proper techniques and avoiding potential hazards. For example, inmates must learn not to put items down where blind people can trip over them. Inmates must also learn not to carry on conversations in doorways with their backs to the traffic flow because blind people will run into them. Not only must the inmate faithfully discharge responsibility to another person in order to score points, the inmate acquires the perspective that comes with the realization that, no matter how bad the inmate's life has been, others are worse off. Networked virtual reality games in prison libraries? Maybe not this year or the next, but global computer networking is reaching critical mass. It is incumbent on information professionals to keep up with networking technologies and, just as importantly, to devise ways to deploy them in the service of humane values. All of the pieces for networked virtual reality applications will soon be ready for assembly. If information professionals don't do it, someone else will. | Simin Yang, Ze Gao, Reza Hadi Mogavi, Pan Hui, Tristan Braud |  |
| 667 |  |  [Coherent Topic Modeling for Creative Multimodal Data on Social Media](https://doi.org/10.1145/3543507.3587433) |  | 0 | The creative web is all about combining different types of media to create a unique and engaging online experience. Multimodal data, such as text and images, is a key component in the creative web. Social media posts that incorporate both text descriptions and images offer a wealth of information and context. Text in social media posts typically relates to one topic, while images often convey information about multiple topics due to the richness of visual content. Despite this potential, many existing multimodal topic models do not take these criteria into account, resulting in poor quality topics being generated. Therefore, we proposed a Coherent Topic modeling for Multimodal Data (CTM-MM), which takes into account that text in social media posts typically relates to one topic, while images can contain information about multiple topics. Our experimental results show that CTM-MM outperforms traditional multimodal topic models in terms of classification and topic coherence. | Junaid Rashid, Jungeun Kim, Usman Naseem | Department of Software, Kongju National University, Cheonan, Republic of Korea, Republic of Korea; Department of Data Science, Sejong University, Seoul, Republic of Korea, Republic of Korea; School of Computer Science, The University of Sydney, Sydney, Australia, Australia |
| 668 |  |  [Improving Health Mention Classification Through Emphasising Literal Meanings: A Study Towards Diversity and Generalisation for Public Health Surveillance](https://doi.org/10.1145/3543507.3583877) |  | 0 | People often use disease or symptom terms on social media and online forums in ways other than to describe their health. Thus the NLP health mention classification (HMC) task aims to identify posts where users are discussing health conditions literally, not figuratively. Existing computational research typically only studies health mentions within well-represented groups in developed nations. Developing countries with limited health surveillance abilities fail to benefit from such data to manage public health crises. To advance the HMC research and benefit more diverse populations, we present the Nairaland health mention dataset (NHMD), a new dataset collected from a dedicated web forum for Nigerians. NHMD consists of 7,763 manually labelled posts extracted based on four prevalent diseases (HIV/AIDS, Malaria, Stroke and Tuberculosis) in Nigeria. With NHMD, we conduct extensive experiments using current state-of-the-art models for HMC and identify that, compared to existing public datasets, NHMD contains out-of-distribution examples. Hence, it is well suited for domain adaptation studies. The introduction of the NHMD dataset imposes better diversity coverage of vulnerable populations and generalisation for HMC tasks in a global public health surveillance setting. Additionally, we present a novel multi-task learning approach for HMC tasks by combining literal word meaning prediction as an auxiliary task. Experimental results demonstrate that the proposed approach outperforms state-of-the-art methods statistically significantly (p < 0.01, Wilcoxon test) in terms of F1 score over the state-of-the-art and shows that our new dataset poses a strong challenge to the existing HMC methods. | Olanrewaju Tahir Aduragba, Jialin Yu, Alexandra I. Cristea, Yang Long | Department of Computer Science, Durham University, United Kingdom; Department of Computer Science, Durham University, United Kingdom and University College London, United Kingdom; Department of Computer Science, Durham University, United Kingdom and Kwara State University, Nigeria |
| 669 |  |  [Learning Faithful Attention for Interpretable Classification of Crisis-Related Microblogs under Constrained Human Budget](https://doi.org/10.1145/3543507.3583861) |  | 0 | The recent widespread use of social media platforms has created convenient ways to obtain and spread up-to-date information during crisis events such as disasters. Time-critical analysis of crisis data can help human organizations gain actionable information and plan for aid responses. Many existing studies have proposed methods to identify informative messages and categorize them into different humanitarian classes. Advanced neural network architectures tend to achieve state-of-the-art performance, but the model decisions are opaque. While attention heatmaps show insights into the model’s prediction, some studies found that standard attention does not provide meaningful explanations. Alternatively, recent works proposed interpretable approaches for the classification of crisis events that rely on human rationales to train and extract short snippets as explanations. However, the rationale annotations are not always available, especially in real-time situations for new tasks and events. In this paper, we propose a two-stage approach to learn the rationales under minimal human supervision and derive faithful machine attention. Extensive experiments over four crisis events show that our model is able to obtain better or comparable classification performance (∼ 86% Macro-F1) to baselines and faithful attention heatmaps using only 40-50% human-level supervision. Further, we employ a zero-shot learning setup to detect actionable tweets along with actionable word snippets as rationales. | Thi Huyen Nguyen, Koustav Rudra | Indian Institute of Technology (Indian School of Mines) Dhanbad, India; L3S Research Center, Germany |
| 670 |  |  [Attacking Fake News Detectors via Manipulating News Social Engagement](https://doi.org/10.1145/3543507.3583868) |  | 0 | Social media is one of the main sources for news consumption, especially among the younger generation. With the increasing popularity of news consumption on various social media platforms, there has been a surge of misinformation which includes false information or unfounded claims. As various text- and social context-based fake news detectors are proposed to detect misinformation on social media, recent works start to focus on the vulnerabilities of fake news detectors. In this paper, we present the first adversarial attack framework against Graph Neural Network (GNN)-based fake news detectors to probe their robustness. Specifically, we leverage a multi-agent reinforcement learning (MARL) framework to simulate the adversarial behavior of fraudsters on social media. Research has shown that in real-world settings, fraudsters coordinate with each other to share different news in order to evade the detection of fake news detectors. Therefore, we modeled our MARL framework as a Markov Game with bot, cyborg, and crowd worker agents, which have their own distinctive cost, budget, and influence. We then use deep Q-learning to search for the optimal policy that maximizes the rewards. Extensive experimental results on two real-world fake news propagation datasets demonstrate that our proposed framework can effectively sabotage the GNN-based fake news detector performance. We hope this paper can provide insights for future research on fake news detection. | Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S. Yu, Kai Shu | Department of Computer Science, Illinois Institute of Technology, USA; Department of Computer Science, University of Illinois Chicago, USA and Visa Research, USA; Department of Computer Science and Engineering, Lehigh University, USA; Department of Computer Science, University of Illinois at Chicago, USA |
| 671 |  |  [ContrastFaux: Sparse Semi-supervised Fauxtography Detection on the Web using Multi-view Contrastive Learning](https://doi.org/10.1145/3543507.3583869) |  | 0 | The widespread misinformation on the Web has raised many concerns with serious societal consequences. In this paper, we study a critical type of online misinformation, namely fauxtography, where the image and associated text of a social media post jointly convey a questionable or false sense. In particular, we focus on a sparse semi-supervised fauxtography detection problem, which aims to accurately identify fauxtography by only using the sparsely annotated ground truth labels of social media posts. Our problem is motivated by the key limitation of current fauxtography detection approaches that often require a large amount of expensive and inefficient manual annotations to train an effective fauxtography detection model. We identify two key technical challenges in solving the problem: 1) it is non-trivial to train an accurate detection model given the sparse fauxtography annotations, and 2) it is difficult to extract the heterogeneous and complicated fauxtography features from the multi-modal social media posts for accurate fauxtography detection. To address the above challenges, we propose ContrastFaux, a multi-view contrastive learning framework that jointly explores the sparse fauxtography annotations and the cross-modal fauxtography feature similarity between the image and text in multi-modal posts to accurately detect fauxtography on social media. Evaluation results on two social media datasets demonstrate that ContrastFaux consistently outperforms state-of-the-art deep learning and semi-supervised learning fauxtography detection baselines by achieving the highest fauxtography detection accuracy. | Ruohan Zong, Yang Zhang, Lanyu Shang, Dong Wang | School of Information Sciences, University of Illinois Urbana-Champaign, USA |
| 672 |  |  [Interpreting wealth distribution via poverty map inference using multimodal data](https://doi.org/10.1145/3543507.3583862) |  | 0 | Poverty maps are essential tools for governments and NGOs to track socioeconomic changes and adequately allocate infrastructure and services in places in need. Sensor and online crowd-sourced data combined with machine learning methods have provided a recent breakthrough in poverty map inference. However, these methods do not capture local wealth fluctuations, and are not optimized to produce accountable results that guarantee accurate predictions to all sub-populations. Here, we propose a pipeline of machine learning models to infer the mean and standard deviation of wealth across multiple geographically clustered populated places, and illustrate their performance in Sierra Leone and Uganda. These models leverage seven independent and freely available feature sources based on satellite images, and metadata collected via online crowd-sourcing and social media. Our models show that combined metadata features are the best predictors of wealth in rural areas, outperforming image-based models, which are the best for predicting the highest wealth quintiles. Our results recover the local mean and variation of wealth, and correctly capture the positive yet non-monotonous correlation between them. We further demonstrate the capabilities and limitations of model transfer across countries and the effects of data recency and other biases. Our methodology provides open tools to build towards more transparent and interpretable models to help governments and NGOs to make informed decisions based on data availability, urbanization level, and poverty thresholds. | Lisette EspínNoboa, János Kertész, Márton Karsai | Central European University, Austria and Complexity Science Hub Vienna, Austria; Central European University, Austria and Rènyi Institute of Mathematics, Hungary |
| 673 |  |  [MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering](https://doi.org/10.1145/3543507.3583878) |  | 0 | Question answering (QA) is a task in the field of natural language processing (NLP) and information retrieval, which has pivotal applications in areas such as online reading comprehension and web search engines. Currently, Bidirectional Encoder Representations from Transformers (BERT) and its biomedical variation (BioBERT) achieve impressive results on the reading comprehension QA datasets and medical-related QA datasets, and so they are widely used for a variety of passage-based QA tasks. However, their performances rapidly deteriorate when encountering passage and context ambiguities. This issue is prevalent and unavoidable in many fields, notably the web-based medical field. In this paper, we introduced a novel approach called the Multiple Synonymous Questions BioBERT (MSQ-BioBERT), which integrates question augmentation, rather than the typical single question used by traditional BioBERT, to elevate BioBERT’s performance on medical QA tasks. In addition, we constructed an ambiguous medical dataset based on the information from Wikipedia web. Experiments with both this web-based constructed medical dataset and open biomedical datasets demonstrate the significant performance gains of the MSQ-BioBERT approach, showcasing a new method for addressing ambiguity in medical QA tasks. | Muzhe Guo, Muhao Guo, Edward T. Dougherty, Fang Jin | George Washington University, USA; Roger Williams University, USA; Arizona State University, USA |
| 674 |  |  [Graph-based Village Level Poverty Identification](https://doi.org/10.1145/3543507.3583864) |  | 0 | Poverty status identification is the first obstacle to eradicating poverty. Village-level poverty identification is very challenging due to the arduous field investigation and insufficient information. The development of the Web infrastructure and its modeling tools provides fresh approaches to identifying poor villages. Upon those techniques, we build a village graph for village poverty status identification. By modeling the village connections as a graph through the geographic distance, we show the correlation between village poverty status and its graph topological position and identify two key factors (Centrality, Homophily Decaying effect) for identifying villages. We further propose the first graph-based method to identify poor villages. It includes a global Centrality2Vec module to embed village centrality into the dense vector and a local graph distance convolution module that captures the decaying effect. In this paper, we make the first attempt to interpret and identify village-level poverty from a graph perspective. | Jing Ma, Liangwei Yang, Qiong Feng, Weizhi Zhang, Philip S. Yu | University of Illinois Chicago, USA; University of Electronic Science and Technology of China, China; Southwest Minzu University, China |
| 675 |  |  [Mapping Flood Exposure, Damage, and Population Needs Using Remote and Social Sensing: A Case Study of 2022 Pakistan Floods](https://doi.org/10.1145/3543507.3583881) |  | 0 | The devastating 2022 floods in Pakistan resulted in a catastrophe impacting millions of people and destroying thousands of homes. While disaster management efforts were taken, crisis responders struggled to understand the country-wide flood extent, population exposure, urgent needs of affected people, and various types of damage. To tackle this challenge, we leverage remote and social sensing with geospatial data using state-of-the-art machine learning techniques for text and image processing. Our satellite-based analysis over a one-month period (25 Aug–25 Sep) revealed that 11.48% of Pakistan was inundated. When combined with geospatial data, this meant 18.9 million people were at risk across 160 districts in Pakistan, with adults constituting 50% of the exposed population. Our social sensing data analysis surfaced 106.7k reports pertaining to deaths, injuries, and concerns of the affected people. To understand the urgent needs of the affected population, we analyzed tweet texts and found that South Karachi, Chitral and North Waziristan required the most basic necessities like food and shelter. Further analysis of tweet images revealed that Lasbela, Rajanpur, and Jhal Magsi had the highest damage reports normalized by their population. These damage reports were found to correlate strongly with affected people reports and need reports, achieving an R-Square of 0.96 and 0.94, respectively. Our extensive study shows that combining remote sensing, social sensing, and geospatial data can provide accurate and timely information during a disaster event, which is crucial in prioritizing areas for immediate and gradual response. | Zainab Akhtar, Umair Qazi, Rizwan Sadiq, Aya ElSakka, Muhammad Sajjad, Ferda Ofli, Muhammad Imran | Qatar Computing Research Institute, Qatar; Department of Geography and Centre for Geocomputation Studies, Hong Kong Baptist University, Hong Kong |
| 676 |  |  [Web Information Extraction for Social Good: Food Pantry Answering As an Example](https://doi.org/10.1145/3543507.3583880) |  | 0 | Social determinants of health (SDH) are the conditions in which we are born, live, work, and age. Food insecurity (FI) is an important domain of SDH. FI is associated with poor health outcomes. Food bank/pantry (food pantry) directly addresses FI. Improving the availability and quality of food from food pantries could reduce FI, leading to improved health outcomes. However, it is difficult for a client to access food pantry information. In this study, we built a food pantry answering framework by combining location-aware information retrieval, web information extraction and domain-specific answering. Our proposed framework first retrieves pantry candidates based on geolocation of the client, and utilizes structural information from markup language to extract semantic chunks related to six common client requests. We use BERT and RoBERTa as information extraction models and compare three different web page segmentation methods in the experiments. | HuanYuan Chen, Hong Yu | University of Massachusetts Amherst, USA; University of Massachusetts Lowell, USA |
| 677 |  |  [Gender Pay Gap in Sports on a Fan-Request Celebrity Video Site](https://doi.org/10.1145/3543507.3583884) |  | 0 | The internet is often thought of as a democratizer, enabling equality in aspects such as pay, as well as a tool introducing novel communication and monetization opportunities. In this study we examine athletes on Cameo, a website that enables bi-directional fan-celebrity interactions, questioning whether the well-documented gender pay gaps in sports persist in this digital setting. Traditional studies into gender pay gaps in sports are mostly in a centralized setting where an organization decides the pay for the players, while Cameo facilitates grass-roots fan engagement where fans pay for video messages from their preferred athletes. The results showed that even on such a platform gender pay gaps persist, both in terms of cost-per-message, and in the number of requests, proxied by number of ratings. For instance, we find that female athletes have a median pay of 30$ per-video, while the same statistic is 40$ for men. The results also contribute to the study of parasocial relationships and personalized fan engagements over a distance. Something that has become more relevant during the ongoing COVID-19 pandemic, where in-person fan engagement has often been limited. | Nazanin Sabri, Stephen Reysen, Ingmar Weber | Saarland University, Germany; University of California, San Diego, USA; Texas A&M University-Commerce, USA |
| 678 |  |  [Knowledge-infused Contrastive Learning for Urban Imagery-based Socioeconomic Prediction](https://doi.org/10.1145/3543507.3583876) |  | 0 | Monitoring sustainable development goals requires accurate and timely socioeconomic statistics, while ubiquitous and frequently-updated urban imagery in web like satellite/street view images has emerged as an important source for socioeconomic prediction. Especially, recent studies turn to self-supervised contrastive learning with manually designed similarity metrics for urban imagery representation learning and further socioeconomic prediction, which however suffers from effectiveness and robustness issues. To address such issues, in this paper, we propose a Knowledge-infused Contrastive Learning (KnowCL) model for urban imagery-based socioeconomic prediction. Specifically, we firstly introduce knowledge graph (KG) to effectively model the urban knowledge in spatiality, mobility, etc., and then build neural network based encoders to learn representations of an urban image in associated semantic and visual spaces, respectively. Finally, we design a cross-modality based contrastive learning framework with a novel image-KG contrastive loss, which maximizes the mutual information between semantic and visual representations for knowledge infusion. Extensive experiments of applying the learnt visual representations for socioeconomic prediction on three datasets demonstrate the superior performance of KnowCL with over 30\% improvements on $R^2$ compared with baselines. Especially, our proposed KnowCL model can apply to both satellite and street imagery with both effectiveness and transferability achieved, which provides insights into urban imagery-based socioeconomic prediction. | Yu Liu, Xin Zhang, Jingtao Ding, Yanxin Xi, Yong Li | University of Helsinki, Finland; Tsinghua University, China |
| 679 |  |  [Leveraging Existing Literature on the Web and Deep Neural Models to Build a Knowledge Graph Focused on Water Quality and Health Risks](https://doi.org/10.1145/3543507.3584185) |  | 0 | A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models. | Nikita Gautam, David Shumway, Megan Kowalcyk, Sarthak Khanal, Doina Caragea, Cornelia Caragea, Hande Mcginty, Samuel Dorevitch | Computer Science, University of Illinois Chicago, USA; Computer Science, Kansas State University, USA; Public Health, University of Illinois Chicago, USA |
| 680 |  |  [Believability and Harmfulness Shape the Virality of Misleading Social Media Posts](https://doi.org/10.1145/3543507.3583857) |  | 0 | Misinformation on social media presents a major threat to modern societies. While previous research has analyzed the virality across true and false social media posts, not every misleading post is necessarily equally viral. Rather, misinformation has different characteristics and varies in terms of its believability and harmfulness - which might influence its spread. In this work, we study how the perceived believability and harmfulness of misleading posts are associated with their virality on social media. Specifically, we analyze (and validate) a large sample of crowd-annotated social media posts from Twitter's Birdwatch platform, on which users can rate the believability and harmfulness of misleading tweets. To address our research questions, we implement an explanatory regression model and link the crowd ratings for believability and harmfulness to the virality of misleading posts on Twitter. Our findings imply that misinformation that is (i) easily believable and (ii) not particularly harmful is associated with more viral resharing cascades. These results offer insights into how different kinds of crowd fact-checked misinformation spreads and suggest that the most viral misleading posts are often not the ones that are particularly concerning from the perspective of public safety. From a practical view, our findings may help platforms to develop more effective strategies to curb the proliferation of misleading posts on social media. | Chiara Patricia Drolsbach, Nicolas Pröllochs |  |
| 681 |  |  [Enhancing Deep Knowledge Tracing with Auxiliary Tasks](https://doi.org/10.1145/3543507.3583866) |  | 0 | Knowledge tracing (KT) is the problem of predicting students' future performance based on their historical interactions with intelligent tutoring systems. Recent studies have applied multiple types of deep neural networks to solve the KT problem. However, there are two important factors in real-world educational data that are not well represented. First, most existing works augment input representations with the co-occurrence matrix of questions and knowledge components\footnote{\label{ft:kc}A KC is a generalization of everyday terms like concept, principle, fact, or skill.} (KCs) but fail to explicitly integrate such intrinsic relations into the final response prediction task. Second, the individualized historical performance of students has not been well captured. In this paper, we proposed \emph{AT-DKT} to improve the prediction performance of the original deep knowledge tracing model with two auxiliary learning tasks, i.e., \emph{question tagging (QT) prediction task} and \emph{individualized prior knowledge (IK) prediction task}. Specifically, the QT task helps learn better question representations by predicting whether questions contain specific KCs. The IK task captures students' global historical performance by progressively predicting student-level prior knowledge that is hidden in students' historical learning interactions. We conduct comprehensive experiments on three real-world educational datasets and compare the proposed approach to both deep sequential KT models and non-sequential models. Experimental results show that \emph{AT-DKT} outperforms all sequential models with more than 0.9\% improvements of AUC for all datasets, and is almost the second best compared to non-sequential models. Furthermore, we conduct both ablation studies and quantitative analysis to show the effectiveness of auxiliary tasks and the superior prediction outcomes of \emph{AT-DKT}. | Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Boyu Gao, Weiqi Luo, Jian Weng | TAL Education Group, China; Guangdong Institute of Smart Education, Jinan University, China; College of Information Science and Technology, Jinan University, China |
| 682 |  |  [Learning to Simulate Crowd Trajectories with Graph Networks](https://doi.org/10.1145/3543507.3583858) |  | 0 | Crowd stampede disasters often occur, such as recent ones in Indonesia and South Korea, and crowd simulation is particularly important to prevent and avoid such disasters. Most traditional models for crowd simulation, such as the social force model, are hand-designed formulas, which use Newtonian forces to model the interactions between pedestrians. However, such formula-based methods may not be flexible enough to capture the complex interaction patterns in diverse crowd scenarios. Recently, due to the development of the Internet, a large amount of pedestrian movement data has been collected, allowing us to study crowd simulation in a data-driven way. Inspired by the recent success of graph network-based simulation (GNS), we propose a novel method under the framework of GNS, which simulates the crowd in a data-driven way. Specifically, we propose to model the interactions among people and the environment using a heterogeneous graph. Then, we design a heterogeneous gated message-passing network to learn the interaction pattern that depends on the visual field. Finally, the randomness is introduced by modeling the context’s different influences on pedestrians with a probabilistic emission function. Extensive experiments on synthetic data, controlled-environment data and real-world data are performed. Extensive results show that our model can generally capture the three main factors which contribute to crowd trajectories while adapting to the data characteristics beyond the strong assumption of formulas-based methods. As a result, the proposed method outperforms existing methods by a large margin. | Hongzhi Shi, Quanming Yao, Yong Li | Tsinghua University, China |
