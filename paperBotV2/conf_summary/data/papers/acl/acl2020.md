# ACL2020

## 会议论文列表

本会议共有 872 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Adaptive Transformers for Learning Multimodal Representations](https://doi.org/10.18653/v1/2020.acl-srw.1) |  | 0 | The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these... | Prajjwal Bhargava |  |
| 2 |  |  [Story-level Text Style Transfer: A Proposal](https://doi.org/10.18653/v1/2020.acl-srw.2) |  | 0 | Text style transfer aims to change the style of the input text to the target style while preserving the content to some extent. Previous works on this task are on the sentence level. We aim to work on story-level text style transfer to generate stories that preserve the plot of the input story while exhibiting a strong target style. The challenge in this task compared to previous work is that the structure of the input story, consisting of named entities and their relations with each other, needs to be preserved, and that... | Yusu Qian |  |
| 3 |  |  [Unsupervised Paraphasia Classification in Aphasic Speech](https://doi.org/10.18653/v1/2020.acl-srw.3) |  | 0 | Aphasia is a speech and language disorder which results from brain damage, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia). Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods cant be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel unsupervised method which can be implemented without the need for labeled paraphasia data. Our evaluations show... | Nikhil Sachdeva, Prince Sachdeva, Rajiv Ratn Shah, Sharan Pai |  |
| 4 |  |  [HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing](https://doi.org/10.18653/v1/2020.acl-srw.4) |  | 0 | Recently deep learning has been used in Medical subject headings (MeSH) indexing to reduce the time and monetary cost by manual annotation, including DeepMeSH, TextCNN, etc. However, these models still suffer from failing to capture the complex correlations between MeSH terms. To this end, we introduce Graph Convolution Network (GCN) to learn the relationship between these terms, and present a novel Hybrid Graph Convolution Net for MeSH index (HGCN4MeSH). Basically, we utilize two BiGRUs to learn the embedding representation... | Chenhui Li, Miaomiao Yu, Yujiu Yang |  |
| 5 |  |  [Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner's Error Tendency](https://doi.org/10.18653/v1/2020.acl-srw.5) |  | 0 | Recently, several studies have focused on improving the performance of grammatical error correction (GEC) tasks using pseudo data. However, a large amount of pseudo data are required to train an accurate GEC model. To address the limitations of language and computational resources, we assume that introducing pseudo errors into sentences similar to those written by the language learners is more efficient, rather than incorporating random pseudo errors into monolingual data. In this regard, we study the effect of pseudo data... | Mamoru Komachi, Satoru Katsumata, Yujin Takahashi |  |
| 6 |  |  [Research on Task Discovery for Transfer Learning in Deep Neural Networks](https://doi.org/10.18653/v1/2020.acl-srw.6) |  | 0 | Deep neural network based machine learning models are shown to perform poorly on unseen or out-of-domain examples by numerous recent studies. Transfer learning aims to avoid overfitting and to improve generalizability by leveraging the information obtained from multiple tasks. Yet, the benefits of transfer learning depend largely on task selection and finding the right method of sharing. In this thesis, we hypothesize that current deep neural network based transfer learning models do not achieve their fullest potential for... | Arda Akdemir |  |
| 7 |  |  [RPD: A Distance Function Between Word Embeddings](https://doi.org/10.18653/v1/2020.acl-srw.7) |  | 0 | It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of em-beddings deviate from each other. In this paper, we propose a novel metric called Relative Pairwise Inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This unitary-invariant metric has a unified scale for comparing different sets of word embeddings. Based... | Shujian Huang, Xuhui Zhou, Zaixiang Zheng |  |
| 8 |  |  [Reflection-based Word Attribute Transfer](https://doi.org/10.18653/v1/2020.acl-srw.8) |  | 0 | Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word’s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental... | Katsuhito Sudoh, Koichiro Yoshino, Satoshi Nakamura, Yoichi Ishibashi |  |
| 9 |  |  [Topic Balancing with Additive Regularization of Topic Models](https://doi.org/10.18653/v1/2020.acl-srw.9) |  | 0 | This article proposes a new approach for building topic models on unbalanced collections in topic modelling, based on the existing methods and our experiments with such methods. Real-world data collections contain topics in various proportions, and often documents of the relatively small theme become distributed all over the larger topics instead of being grouped into one topic. To address this issue, we design a new regularizer for Theta and Phi matrices in probabilistic Latent Semantic Analysis (pLSA) model. We make sure... | Eugenia Veselova, Konstantin V. Vorontsov |  |
| 10 |  |  [Combining Subword Representations into Word-level Representations in the Transformer Architecture](https://doi.org/10.18653/v1/2020.acl-srw.10) |  | 0 | In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies. We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and... | José A. R. Fonollosa, Marta R. Costajussà, Noe Casas |  |
| 11 |  |  [Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition](https://doi.org/10.18653/v1/2020.acl-srw.11) |  | 0 | The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes. We demonstrate that our method can effectively learn North Korean to English translation and... | Hwichan Kim, Mamoru Komachi, Tosho Hirasawa |  |
| 12 |  |  [Media Bias, the Social Sciences, and NLP: Automating Frame Analyses to Identify Bias by Word Choice and Labeling](https://doi.org/10.18653/v1/2020.acl-srw.12) |  | 0 | Media bias can strongly impact the public perception of topics reported in the news. A difficult to detect, yet powerful form of slanted news coverage is called bias by word choice and labeling (WCL). WCL bias can occur, for example, when journalists refer to the same semantic concept by using different terms that frame the concept differently and consequently may lead to different assessments by readers, such as the terms “freedom fighters” and “terrorists,” or “gun rights” and “gun control.” In this research project, I aim... | Felix Hamborg |  |
| 13 |  |  [SCAR: Sentence Compression using Autoencoders for Reconstruction](https://doi.org/10.18653/v1/2020.acl-srw.13) |  | 0 | Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is primarily composed of two encoder-decoder pairs: a compressor and a reconstructor. The compressor masks the input, and the reconstructor... | Chanakya Malireddy, Manish Shrivastava, Tirth Maniar |  |
| 14 |  |  [Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information](https://doi.org/10.18653/v1/2020.acl-srw.14) |  | 0 | Medical image captioning can reduce the workload of physicians and save time and expense by automatically generating reports. However, current datasets are small and limited, creating additional challenges for researchers. In this study, we propose a feature difference and tag information combined long short-term memory (LSTM) model for chest x-ray report generation. A feature vector extracted from the image conveys visual information, but its ability to describe the image is limited. Other image captioning studies exhibited... | Hyeryun Park, Jinwook Choi, Jooyoung Yoon, Kyungmo Kim, Seongkeun Park |  |
| 15 |  |  [Multi-Task Neural Model for Agglutinative Language Translation](https://doi.org/10.18653/v1/2020.acl-srw.15) |  | 0 | Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single... | Rui Dong, Xiao Li, Yating Yang, Yirong Pan |  |
| 16 |  |  [Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling](https://doi.org/10.18653/v1/2020.acl-srw.16) |  | 0 | Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model’s decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models.... | Christoph Alt, David Harbecke |  |
| 17 |  |  [Non-Topical Coherence in Social Talk: A Call for Dialogue Model Enrichment](https://doi.org/10.18653/v1/2020.acl-srw.17) |  | 0 | Current models of dialogue mainly focus on utterances within a topically coherent discourse segment, rather than new-topic utterances (NTUs), which begin a new topic not correlating with the content of prior discourse. As a result, these models may sufficiently account for discourse context of task-oriented but not social conversations. We conduct a pilot annotation study of NTUs as a first step towards a model capable of rationalizing conversational coherence in social talk. We start with the naturally occurring social... | Alex Luu, Sophia A. Malamud |  |
| 18 |  |  [Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models](https://doi.org/10.18653/v1/2020.acl-srw.18) |  | 0 | What do powerful models of word mean- ing created from distributional data (e.g. Word2vec (Mikolov et al., 2013) BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018)) represent? What causes words to be similar in the semantic space? What type of information is lacking? This thesis proposal presents a framework for investigating the information encoded in distributional semantic models. Several analysis methods have been suggested, but they have been shown to be limited and are not well understood. This approach pairs... | Pia Sommerauer |  |
| 19 |  |  [A Simple and Effective Dependency Parser for Telugu](https://doi.org/10.18653/v1/2020.acl-srw.19) |  | 0 | We present a simple and effective dependency parser for Telugu, a morphologically rich, free word order language. We propose to replace the rich linguistic feature templates used in the past approaches with a minimal feature function using contextual vector representations. We train a BERT model on the Telugu Wikipedia data and use vector representations from this model to train the parser. Each sentence token is associated with a vector representing the token in the context of that sentence and the feature vectors are... | Dipti Misra Sharma, Manish Shrivastava, Sneha Nallani |  |
| 20 |  |  [Pointwise Paraphrase Appraisal is Potentially Problematic](https://doi.org/10.18653/v1/2020.acl-srw.20) |  | 0 | The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for... | David Evans, Hannah Chen, Yangfeng Ji |  |
| 21 |  |  [Let's be Humorous: Knowledge Enhanced Humor Generation](https://aclanthology.org/2020.acl-srw.21/) |  | 0 |  | Cheng Luo, Dayiheng Liu, Hang Zhang, Jiancheng Lv |  |
| 22 |  |  [Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages](https://doi.org/10.18653/v1/2020.acl-srw.22) |  | 0 | A large percentage of the world’s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a... | Dipti Misra Sharma, Sourav Kumar, Vikrant Goyal |  |
| 23 |  |  [Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder](https://doi.org/10.18653/v1/2020.acl-srw.23) |  | 0 | We propose an interpretable approach for event extraction that mitigates the tension between generalization and interpretability by jointly training for the two goals. Our approach uses an encoder-decoder architecture, which jointly trains a classifier for event extraction, and a rule decoder that generates syntactico-semantic rules that explain the decisions of the event classifier. We evaluate the proposed approach on three biomedical events and show that the decoder generates interpretable rules that serve as accurate... | Gus HahnPowell, Mihai Surdeanu, Zheng Tang |  |
| 24 |  |  [Crossing the Line: Where do Demographic Variables Fit into Humor Detection?](https://doi.org/10.18653/v1/2020.acl-srw.24) |  | 0 | Recent humor classification shared tasks have struggled with two issues: either the data comprises a highly constrained genre of humor which does not broadly represent humor, or the data is so indiscriminate that the inter-annotator agreement on its humor content is drastically low. These tasks typically average over all annotators’ judgments, in spite of the fact that humor is a highly subjective phenomenon. We argue that demographic factors influence whether a text is perceived as humorous or not. We propose the addition... | J. A. Meaney |  |
| 25 |  |  [Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions](https://doi.org/10.18653/v1/2020.acl-srw.25) |  | 0 | Parallel corpora are key to developing good machine translation systems. However, abundant parallel data are hard to come by, especially for languages with a low number of speakers. When rich morphology exacerbates the data sparsity problem, it is imperative to have accurate alignment and filtering methods that can help make the most of what is available by maximising the number of correctly translated segments in a corpus and minimising noise by removing incorrect translations and segments containing extraneous data. This... | Andy Way, Hrafn Loftsson, Steinþór Steingrímsson |  |
| 26 |  |  [Understanding Points of Correspondence between Sentences for Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-srw.26) |  | 0 | Fusing sentences containing disparate content is a remarkable human ability that helps create informative and succinct summaries. Such a simple task for humans has remained challenging for modern abstractive summarizers, substantially restricting their applicability in real-world scenarios. In this paper, we present an investigation into fusing sentences drawn from a document by introducing the notion of points of correspondence, which are cohesive devices that tie any two sentences together into a coherent text. The types... | Doo Soon Kim, Fei Liu, Franck Dernoncourt, John Muchovej, Lidan Wang, Logan Lebanoff, Walter Chang |  |
| 27 |  |  [uBLEU: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-srw.27) |  | 0 | Because open-domain dialogues allow diverse responses, basic reference-based metrics such as BLEU do not work well unless we prepare a massive reference set of high-quality responses for input utterances. To reduce this burden, a human-aided, uncertainty-aware metric, ΔBLEU, has been proposed; it embeds human judgment on the quality of reference outputs into the computation of multiple-reference BLEU. In this study, we instead propose a fully automatic, uncertainty-aware evaluation method for open-domain dialogue systems,... | Masashi Toyoda, Naoki Yoshinaga, Tsuta Yuma |  |
| 28 |  |  [To compress or not to compress? A Finite-State approach to Nen verbal morphology](https://doi.org/10.18653/v1/2020.acl-srw.28) |  | 0 | This paper describes the development of a verbal morphological parser for an under-resourced Papuan language, Nen. Nen verbal morphology is particularly complex, with a transitive verb taking up to 1,740 unique features. The structural properties exhibited by Nen verbs raises interesting choices for analysis. Here we compare two possible methods of analysis: ‘Chunking’ and decomposition. ‘Chunking’ refers to the concept of collating morphological segments into one, whereas the decomposition model follows a more classical... | Hanna Suominen, Nicholas Evans, Saliha Muradoglu |  |
| 29 |  |  [AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss](https://doi.org/10.18653/v1/2020.acl-srw.29) |  | 0 | Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis, word segmentation, and hand-crafted feature engineering. These could be eliminated by using character-level features. We propose a novel end-to-end Arabic document classification framework, Arabic document image-based classifier (AraDIC), inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier. They are trained in an end-to-end... | Hitoshi Iyatomi, Mahmoud Daif, Shunsuke Kitada |  |
| 30 |  |  [Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-srw.30) |  | 0 | In general, the labels used in sequence labeling consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as Person, can be beneficial for label prediction. In this work, we propose to integrate label component information as embeddings into models. Through experiments on English... | Hiroki Ouchi, Jun Suzuki, Kaori Abe, Kentaro Inui, Shumpei Miyawaki, Takuma Kato |  |
| 31 |  |  [Building a Japanese Typo Dataset from Wikipedia's Revision History](https://doi.org/10.18653/v1/2020.acl-srw.31) |  | 0 | User generated texts contain many typos for which correction is necessary for NLP systems to work. Although a large number of typo–correction pairs are needed to develop a data-driven typo correction system, no such dataset is available for Japanese. In this paper, we extract over half a million Japanese typo–correction pairs from Wikipedia’s revision history. Unlike other languages, Japanese poses unique challenges: (1) Japanese texts are unsegmented so that we cannot simply apply a spelling checker, and (2) the way people... | Daisuke Kawahara, Sadao Kurohashi, Yu Tanaka, Yugo Murawaki |  |
| 32 |  |  [Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation](https://doi.org/10.18653/v1/2020.acl-srw.32) |  | 0 | Many recent Short Answer Scoring (SAS) systems have employed Quadratic Weighted Kappa (QWK) as the evaluation measure of their systems. However, we hypothesize that QWK is unsatisfactory for the evaluation of the SAS systems when we consider measuring their effectiveness in actual usage. We introduce a new task formulation of SAS that matches the actual usage. In our formulation, the SAS systems should extract as many scoring predictions that are not critical scoring errors (CSEs). We conduct the experiments in our new task... | Hiroaki Funayama, Jun Suzuki, Kentaro Inui, Masato Mita, Shota Sasaki, Tomoya Mizumoto, Yuichiroh Matsubayashi |  |
| 33 |  |  [How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?](https://doi.org/10.18653/v1/2020.acl-srw.33) |  | 0 | Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between... | Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal |  |
| 34 |  |  [Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](https://doi.org/10.18653/v1/2020.acl-srw.34) |  | 0 | Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two... | Eneko Agirre, Gorka Labaka, Ivana Kvapilíková, Mikel Artetxe, Ondrej Bojar |  |
| 35 |  |  [Logical Inferences with Comparatives and Generalized Quantifiers](https://doi.org/10.18653/v1/2020.acl-srw.35) |  | 0 | Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In... | Daisuke Bekki, Izumi Haruta, Koji Mineshima |  |
| 36 |  |  [Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources](https://doi.org/10.18653/v1/2020.acl-srw.36) |  | 0 | In this work, we present an effective method for semantic specialization of word vector representations. To this end, we use traditional word embeddings and apply specialization methods to better capture semantic relations between words. In our approach, we leverage external knowledge from rich lexical resources such as BabelNet. We also show that our proposed post-specialization method based on an adversarial neural network with the Wasserstein distance allows to gain improvements over state-of-the-art methods on two tasks:... | Bardia Rafieian, Magdalena Biesialska, Marta R. Costajussà |  |
| 37 |  |  [Pre-training via Leveraging Assisting Languages for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-srw.37) |  | 0 | Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical... | Eiichiro Sumita, Fei Cheng, Haiyue Song, Raj Dabre, Sadao Kurohashi, Zhuoyuan Mao |  |
| 38 |  |  [Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems](https://doi.org/10.18653/v1/2020.acl-srw.38) |  | 0 | In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the N-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder’s... | Dipti Misra Sharma, Vinay Pandramish |  |
| 39 |  |  [Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup](https://doi.org/10.18653/v1/2020.acl-srw.39) |  | 0 | Distinguishing informative and actionable messages from a social media platform like Twitter is critical for facilitating disaster management. For this purpose, we compile a multilingual dataset of over 130K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labelled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main model is based on Multilingual BERT, which we further improve with Manifold Mixup. We show that our... | Cornelia Caragea, Doina Caragea, Jishnu Ray Chowdhury |  |
| 40 |  |  [Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition](https://doi.org/10.18653/v1/2020.acl-srw.40) |  | 0 | The principle of compositionality has deep roots in linguistics: the meaning of an expression is determined by its structure and the meanings of its constituents. However, modern neural network models such as long short-term memory network process expressions in a linear fashion and do not seem to incorporate more complex compositional patterns. In this work, we show that we can explicitly induce grammar by tracing the computational process of a long short-term memory network. We show: (i) the multiplicative nature of long... | Allen Nie, Yuhui Zhang |  |
| 41 |  |  [Exploring the Role of Context to Distinguish Rhetorical and Information-Seeking Questions](https://doi.org/10.18653/v1/2020.acl-srw.41) |  | 0 | Social media posts often contain questions, but many of the questions are rhetorical and do not seek information. Our work studies the problem of distinguishing rhetorical and information-seeking questions on Twitter. Most work has focused on features of the question itself, but we hypothesize that the prior context plays a role too. This paper introduces a new dataset containing questions in tweets paired with their prior tweets to provide context. We create classification models to assess the difficulty of distinguishing... | Ellen Riloff, Yuan Zhuang |  |
| 42 |  |  [Compositional Generalization by Factorizing Alignment and Translation](https://doi.org/10.18653/v1/2020.acl-srw.42) |  | 0 | Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in cognitive science suggesting a functional distinction between systems for syntactic and semantic processing, we implement a modification to an existing approach in neural machine... | Jacob L. Russin, Jason Jo, Randall C. O'Reilly, Yoshua Bengio |  |
| 43 |  |  [#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media](https://doi.org/10.18653/v1/2020.acl-srw.43) |  | 0 | The recent surge in online forums and movements supporting sexual assault survivors has led to the emergence of a ‘virtual bubble’ where survivors can recount their stories. However, this also makes the survivors vulnerable to bullying, trolling and victim blaming. Specifically, victim blaming has been shown to have acute psychological effects on the survivors and further discourage formal reporting of such crimes. Therefore, it is important to devise computationally relevant methods to identify and prevent victim blaming to... | Ashima Suvarna, Grusha Bhalla |  |
| 44 |  |  [Xiaomingbot: A Multilingual Robot News Reporter](https://doi.org/10.18653/v1/2020.acl-demos.1) |  | 0 | This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the... | Hao Zhou, Jiaze Chen, Jun Cao, Lei Li, Li Chen, Mingxuan Wang, Runxin Xu, Songcheng Jiang, Xiang Yin, Xijin Zhang, Ying Zeng, Yuping Wang, Yuxuan Wang |  |
| 45 |  |  [TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-demos.2) |  | 0 | In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom... | Guoping Hu, Shijin Wang, Ting Liu, Wanxiang Che, Yiming Cui, Zhipeng Chen, Ziqing Yang |  |
| 46 |  |  [Syntactic Search by Example](https://doi.org/10.18653/v1/2020.acl-demos.3) |  | 0 | We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs. In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine.... | Hillel TaubTabib, Micah Shlain, Shoval Sadde, Yoav Goldberg |  |
| 47 |  |  [Tabouid: a Wikipedia-based word guessing game](https://doi.org/10.18653/v1/2020.acl-demos.4) |  | 0 | We present Tabouid, a word-guessing game automatically generated from Wikipedia. Tabouid contains 10,000 (virtual) cards in English, and as many in French, covering not only words and linguistic expressions but also a variety of topics including artists, historical events or scientific concepts. Each card corresponds to a Wikipedia article, and conversely, any article could be turned into a card. A range of relatively simple NLP and machine-learning techniques are effectively integrated into a two-stage process. First, a... | Timothée Bernard |  |
| 48 |  |  [Talk to Papers: Bringing Neural Question Answering to Academic Search](https://doi.org/10.18653/v1/2020.acl-demos.5) |  | 0 | We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It’s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research... | Kyusong Lee, Tiancheng Zhao |  |
| 49 |  |  [Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.acl-demos.6) |  | 0 | Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web interface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a... | Fabrizio Brignone, Federico Scozzafava, Giovanni Torrisi, Marco Maru, Roberto Navigli |  |
| 50 |  |  [pyBART: Evidence-based Syntactic Transformations for IE](https://doi.org/10.18653/v1/2020.acl-demos.7) |  | 0 | Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications. Proposals like English Enhanced UD improve the situation by extending... | Aryeh Tiktinsky, Reut Tsarfaty, Yoav Goldberg |  |
| 51 |  |  [EVIDENCEMINER: Textual Evidence Discovery for Life Sciences](https://doi.org/10.18653/v1/2020.acl-demos.8) |  | 0 | Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated... | Aabhas Chauhan, David Liem, Dibakar Sigdel, Enyi Jiang, Jiawei Han, John Caufield, Peipei Ping, Qi Li, Weili Liu, Xuan Wang, Yingjun Guan |  |
| 52 |  |  [Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time](https://doi.org/10.18653/v1/2020.acl-demos.9) |  | 0 | We introduce Trialstreamer, a living database of clinical trial reports. Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these. Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured. The system then attempts to infer which interventions were reported to work best... | Ani Nenkova, Benjamin E. Nye, Byron C. Wallace, Iain James Marshall |  |
| 53 |  |  [SyntaxGym: An Online Platform for Targeted Evaluation of Language Models](https://doi.org/10.18653/v1/2020.acl-demos.10) |  | 0 | Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing... | Ethan Wilcox, Jennifer Hu, Jon Gauthier, Peng Qian, Roger Levy |  |
| 54 |  |  [GAIA: A Fine-grained Multimedia Knowledge Extraction System](https://doi.org/10.18653/v1/2020.acl-demos.11) |  | 0 | We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC... | Alireza Zareian, Bo Wu, Brian Chen, Clare R. Voss, Daniel Napierski, Heng Ji, Manling Li, Marjorie Freedman, ShihFu Chang, Spencer Whitehead, Xiaoman Pan, Ying Lin |  |
| 55 |  |  [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://doi.org/10.18653/v1/2020.acl-demos.12) |  | 0 | We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub. The models embed text from 16 languages into a shared semantic space using a multi-task trained dual-encoder that learns tied cross-lingual representations via translation bridge tasks (Chidambaram et al., 2018). The models achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR). Competitive performance is obtained on the related tasks of translation pair bitext... | Amin Ahmad, Brian Strope, Chris Tar, Daniel Cer, Gustavo Hernández Ábrego, Jax Law, Mandy Guo, Noah Constant, Ray Kurzweil, Steve Yuan, Yinfei Yang, YunHsuan Sung |  |
| 56 |  |  [BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab](https://doi.org/10.18653/v1/2020.acl-demos.13) |  | 0 | CodaLab is an open-source web-based platform for collaborative computational research. Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines. In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc. Since these steps require different tools which are usually scattered in different... | Fei Li, Hong Yu, Yonghao Jin |  |
| 57 |  |  [Stanza: A Python Natural Language Processing Toolkit for Many Human Languages](https://doi.org/10.18653/v1/2020.acl-demos.14) |  | 0 | We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and... | Christopher D. Manning, Jason Bolton, Peng Qi, Yuhao Zhang, Yuhui Zhang |  |
| 58 |  |  [jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models](https://doi.org/10.18653/v1/2020.acl-demos.15) |  | 0 | We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT. | Alex Wang, Haokun Liu, Ian Tenney, Jason Phang, Philip Yeres, Phu Mon Htut, Samuel R. Bowman, Yada Pruksachatkun |  |
| 59 |  |  [The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-demos.16) |  | 0 | We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the... | Emmanuel Awa, Guihong Cao, Hao Cheng, Hoifung Poon, Jianfeng Gao, Jianshu Ji, Pengcheng He, Weizhu Chen, Xiaodong Liu, Xueyun Zhu, Yu Wang |  |
| 60 |  |  [LinggleWrite: a Coaching System for Essay Writing](https://doi.org/10.18653/v1/2020.acl-demos.17) |  | 0 | This paper presents LinggleWrite, a writing coach that provides writing suggestions, assesses writing proficiency levels, detects grammatical errors, and offers corrective feedback in response to user’s essay. The method involves extracting grammar patterns, training models for automated essay scoring (AES) and grammatical error detection (GED), and finally retrieving plausible corrections from a n-gram search engine. Experiments on public test sets indicate that both AES and GED models achieve state-of-the-art performance.... | Chingyu Yang, ChungTing Tsai, Jason S. Chang, JhihJie Chen |  |
| 61 |  |  [CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task](https://doi.org/10.18653/v1/2020.acl-demos.18) |  | 0 | We present CLIReval, an easy-to-use toolkit for evaluating machine translation (MT) with the proxy task of cross-lingual information retrieval (CLIR). Contrary to what the project name might suggest, CLIReval does not actually require any annotated CLIR dataset. Instead, it automatically transforms translations and references used in MT evaluations into a synthetic CLIR dataset; it then sets up a standard search engine (Elasticsearch) and computes various information retrieval metrics (e.g., mean average precision) by... | Kevin Duh, Shuo Sun, Suzanna Sia |  |
| 62 |  |  [ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-demos.19) |  | 0 | We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and... | Baolin Peng, Jianfeng Gao, Jinchao Li, Minlie Huang, Qi Zhu, Ryuichi Takanobu, Xiang Li, Xiaoyan Zhu, Yan Fang, Zheng Zhang |  |
| 63 |  |  [OpusFilter: A Configurable Parallel Corpus Filtering Toolbox](https://doi.org/10.18653/v1/2020.acl-demos.20) |  | 0 | This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of... | Jörg Tiedemann, Mikko Aulamo, Sami Virpioja |  |
| 64 |  |  [Label Noise in Context](https://doi.org/10.18653/v1/2020.acl-demos.21) |  | 0 | Label noise—incorrectly or ambiguously labeled training examples—can negatively impact model performance. Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process. Examples incorrectly flagged as noise waste reviewers’ time, and correcting label noise without guidance can be difficult. We propose LNIC, a noise-detection method that uses an example’s neighborhood within the training set to (a) reduce false positives and (b) provide an... | Catherine FineganDollak, Jeffrey Boston, Matthew Arnold, Michael Desmond |  |
| 65 |  |  [exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models](https://doi.org/10.18653/v1/2020.acl-demos.22) |  | 0 | Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation... | Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann |  |
| 66 |  |  [Nakdan: Professional Hebrew Diacritizer](https://doi.org/10.18653/v1/2020.acl-demos.23) |  | 0 | We present a system for automatic diacritization of Hebrew Text. The system combines modern neural models with carefully curated declarative linguistic knowledge and comprehensive manually constructed tables and dictionaries. Besides providing state of the art diacritization accuracy, the system also supports an interface for manual editing and correction of the automatic output, and has several features which make it particularly useful for preparation of scientific editions of historical Hebrew texts. The system supports... | Avi Shmidman, Moshe Koppel, Shaltiel Shmidman, Yoav Goldberg |  |
| 67 |  |  [Photon: A Robust Cross-Domain Text-to-SQL System](https://doi.org/10.18653/v1/2020.acl-demos.24) |  | 0 | Natural language interfaces to databases(NLIDB) democratize end user access to relational data. Due to fundamental differences between natural language communication and programming, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying query language. We present PHOTON, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping cannot be immediately determined. PHOTON consists of a strong neural semantic... | Caiming Xiong, Irwin King, Jichuan Zeng, Michael R. Lyu, Richard Socher, Steven C. H. Hoi, Xi Victoria Lin |  |
| 68 |  |  [Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations](https://doi.org/10.18653/v1/2020.acl-demos.25) |  | 0 | We show SUGILITE, an intelligent task automation agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations, using the graphical user interfaces (GUIs) of third-party mobile apps. This system provides several interesting features: (1) it allows users to teach new task procedures and concepts through verbal instructions together with demonstration of the steps of a script using GUIs; (2) it supports users in clarifying their intents for... | Brad A. Myers, Toby JiaJun Li, Tom M. Mitchell |  |
| 69 |  |  [MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform](https://doi.org/10.18653/v1/2020.acl-demos.26) |  | 0 | We present MixingBoard, a platform for quickly building demos with a focus on knowledge grounded stylized text generation. We unify existing text generation algorithms in a shared codebase and further adapt earlier algorithms for constrained generation. To borrow advantages from different models, we implement strategies for cross-model integration, from the token probability level to the latent space level. An interface to external knowledge is provided via a module that retrieves, on-the-fly, relevant knowledge from... | Bill Dolan, Michel Galley, Xiang Gao |  |
| 70 |  |  [NLP Scholar: An Interactive Visual Explorer for Natural Language Processing Literature](https://doi.org/10.18653/v1/2020.acl-demos.27) |  | 0 | As part of the NLP Scholar project, we created a single unified dataset of NLP papers and their meta-information (including citation numbers), by extracting and aligning information from the ACL Anthology and Google Scholar. In this paper, we describe several interconnected interactive visualizations (dashboards) that present various aspects of the data. Clicking on an item within a visualization or entering query terms in the search boxes filters the data in all visualizations in the dashboard. This allows users to search... | Saif M. Mohammad |  |
| 71 |  |  [Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines](https://doi.org/10.18653/v1/2020.acl-demos.28) |  | 0 | Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting... | Henry A. Kautz, John Krumm, Nabil Hossain, Tanvir Sajed |  |
| 72 |  |  [Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-demos.29) |  | 0 | The reader of a choose your own adventure novel and the user of a modern virtual assistant have a subtle similarity; both may, through the right lens, be viewed as engaging with a work of Interactive Fiction. This literary form emerged in the 1970s and has grown like a vine along the branch of modern technology, one guided by the advances of the other. In this work we weave together threads from the Interactive Fiction community and neural semantic parsing for dialog systems, defining the data model and necessary algorithms... | Ben Swanson, Boris Smus |  |
| 73 |  |  [DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation](https://doi.org/10.18653/v1/2020.acl-demos.30) |  | 0 | We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful... | Bill Dolan, Chris Brockett, Jianfeng Gao, Jingjing Liu, Michel Galley, Siqi Sun, Xiang Gao, YenChun Chen, Yizhe Zhang |  |
| 74 |  |  [ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents](https://doi.org/10.18653/v1/2020.acl-demos.31) |  | 0 | We present ADVISER - an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), socially-engaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or... | ChiaYu Li, Daniel Ortega, Dirk Väth, Florian Lux, Lindsey Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz Völkel, Ngoc Thang Vu, Pavel Denisov, Sabrina Jenne, Zorica Kacarevic |  |
| 75 |  |  [Prta: A System to Support the Analysis of Propaganda Techniques in the News](https://doi.org/10.18653/v1/2020.acl-demos.32) |  | 0 | Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 “infodemic”, have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on fact-checking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of... | Alberto BarrónCedeño, Giovanni Da San Martino, Preslav Nakov, Seunghak Yu, Shaden Shaar, Yifan Zhang |  |
| 76 |  |  [Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes](https://doi.org/10.18653/v1/2020.acl-demos.33) |  | 0 | In this paper, we introduce Clinical-Coder, an online system aiming to assign ICD codes to Chinese clinical notes. ICD coding has been a research hotspot of clinical medicine, but the interpretability of prediction hinders its practical application. We exploit a Dilated Convolutional Attention network with N-gram Matching mechanism (DCANM) to capture semantic features for non-continuous words and continuous n-gram words, concentrating on explaining the reason why each ICD code to be predicted. The experiments demonstrate... | Chenwei Yan, Jun Zhao, Kang Liu, Pengfei Cao, Shengping Liu, Weifeng Chong, Xiangling Fu, Yubo Chen |  |
| 77 |  |  [ESPnet-ST: All-in-One Speech Translation Toolkit](https://doi.org/10.18653/v1/2020.acl-demos.34) |  | 0 | We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results... | Hirofumi Inaguma, Kevin Duh, Nelson Yalta, Shigeki Karita, Shinji Watanabe, Shun Kiyono, Tomoki Hayashi |  |
| 78 |  |  [Penman: An Open-Source Library and Tool for AMR Graphs](https://doi.org/10.18653/v1/2020.acl-demos.35) |  | 0 | Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a framework for semantic dependencies that encodes its rooted and directed acyclic graphs in a format called PENMAN notation. The format is simple enough that users of AMR data often write small scripts or libraries for parsing it into an internal graph representation, but there is enough complexity that these users could benefit from a more sophisticated and well-tested solution. The open-source Python library Penman provides a robust parser, functions for... | Michael Wayne Goodman |  |
| 79 |  |  [Embedding-based Scientific Literature Discovery in a Text Editor Application](https://doi.org/10.18653/v1/2020.acl-demos.36) |  | 0 | Each claim in a research paper requires all relevant prior knowledge to be discovered, assimilated, and appropriately cited. However, despite the availability of powerful search engines and sophisticated text editing software, discovering relevant papers and integrating the knowledge into a manuscript remain complex tasks associated with high cognitive load. To define comprehensive search queries requires strong motivation from authors, irrespective of their familiarity with the research field. Moreover, switching between... | Jonathan Prada, Nianlong Gu, Nikola I. Nikolov, Onur Gökçe, Richard H. R. Hahnloser |  |
| 80 |  |  [MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation](https://doi.org/10.18653/v1/2020.acl-demos.37) |  | 0 | The shift from traditional translation to post-editing (PE) of machine-translated (MT) text can save time and reduce errors, but it also affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. Users can... | Antonio Krüger, Josef van Genabith, Kalliopi Meladaki, Mahsa Monshizadeh, Nico Herbig, Santanu Pal, Tim Düwel, Vladislav Hnatovskiy |  |
| 81 |  |  [Torch-Struct: Deep Structured Prediction Library](https://doi.org/10.18653/v1/2020.acl-demos.38) |  | 0 | The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. Torch-Struct includes a broad collection of probabilistic structures accessed through a simple and flexible... | Alexander M. Rush |  |
| 82 |  |  [Conversation Learner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2020.acl-demos.39) |  | 0 | Traditionally, industry solutions for building a task-oriented dialog system have relied on helping dialog authors define rule-based dialog managers, represented as dialog flows. While dialog flows are intuitively interpretable and good for simple scenarios, they fall short of performance in terms of the flexibility needed to handle complex dialogs. On the other hand, purely machine-learned models can handle complex dialogs, but they are considered to be black boxes and require large amounts of training data. In this... | Baolin Peng, Eslam Kamal, Jianfeng Gao, Jinchao Li, Lars Liden, Matt Mazzola, Shahin Shayandeh, Swadheen Shukla, Thomas Park |  |
| 83 |  |  [NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg](https://doi.org/10.18653/v1/2020.acl-demos.40) |  | 0 | Millions of news articles from hundreds of thousands of sources around the globe appear in news aggregators every day. Consuming such a volume of news presents an almost insurmountable challenge. For example, a reader searching on Bloomberg’s system for news about the U.K. would find 10,000 articles on a typical day. Apple Inc., the world’s most journalistically covered company, garners around 1,800 news articles a day. We realized that a new kind of summarization engine was needed, one that would condense large volumes of... | Andy Almonte, Guim Perarnau, Iat Chong Chan, Igor Malioutov, Joshua Bambrick, Minjie Xu, Vittorio Selo |  |
| 84 |  |  [SUPP.AI: finding evidence for supplement-drug interactions](https://doi.org/10.18653/v1/2020.acl-demos.41) |  | 0 | Dietary supplements are used by a large portion of the population, but information on their pharmacologic interactions is incomplete. To address this challenge, we present SUPP.AI, an application for browsing evidence of supplement-drug interactions (SDIs) extracted from the biomedical literature. We train a model to automatically extract supplement information and identify such interactions from the scientific literature. To address the lack of labeled data for SDI identification, we use labels of the closely related task... | Arman Cohan, Carissa Schoenick, Lucy Lu Wang, Nick Botner, Oyvind Tafjord, Sam Skjonsberg, Sarthak Jain, Waleed Ammar |  |
| 85 |  |  [LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from Explanation](https://doi.org/10.18653/v1/2020.acl-demos.42) |  | 0 | Successfully training a deep neural network demands a huge corpus of labeled data. However, each label only provides limited information to learn from, and collecting the requisite number of labels involves massive human effort. In this work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN framework for sequence labeling and classification tasks, with an easy-to-use UI that not only allows an annotator to provide the needed labels for a task but also enables LearnIng From Explanations for each labeling... | Bill Yuchen Lin, DongHo Lee, Elizabeth Boschee, Leonardo Neves, Qinyuan Ye, Rahul Khanna, Seyeon Lee, Xiang Ren |  |
| 86 |  |  [What's The Latest? A Question-driven News Chatbot](https://doi.org/10.18653/v1/2020.acl-demos.43) |  | 0 | This work describes an automatic news chatbot that draws content from a diverse set of news articles and creates conversations with a user about the news. Key components of the system include the automatic organization of news articles into topical chatrooms, integration of automatically generated questions into the conversation, and a novel method for choosing which questions to present which avoids repetitive suggestions. We describe the algorithmic framework and present the results of a usability study that shows that... | John F. Canny, Marti A. Hearst, Philippe Laban |  |
| 87 |  |  [Interpretability and Analysis in Neural NLP](https://doi.org/10.18653/v1/2020.acl-tutorials.1) |  | 0 | While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and... | Ellie Pavlick, Sebastian Gehrmann, Yonatan Belinkov |  |
| 88 |  |  [Integrating Ethics into the NLP Curriculum](https://doi.org/10.18653/v1/2020.acl-tutorials.2) |  | 0 | To raise awareness among future NLP practitioners and prevent inertia in the field, we need to place ethics in the curriculum for all NLP students—not as an elective, but as a core part of their education. Our goal in this tutorial is to empower NLP researchers and practitioners with tools and resources to teach others about how to ethically apply NLP techniques. We will present both high-level strategies for developing an ethics-oriented curriculum, based on experience and best practices, as well as specific sample... | Alexandra Schofield, Dirk Hovy, Emily M. Bender |  |
| 89 |  |  [Achieving Common Ground in Multi-modal Dialogue](https://doi.org/10.18653/v1/2020.acl-tutorials.3) |  | 0 | All communication aims at achieving common ground (grounding): interlocutors can work together effectively only with mutual beliefs about what the state of the world is, about what their goals are, and about how they plan to make their goals a reality. Computational dialogue research offers some classic results on grouding, which unfortunately offer scant guidance to the design of grounding modules and behaviors in cutting-edge systems. In this tutorial, we focus on three main topic areas: 1) grounding in human-human... | Malihe Alikhani, Matthew Stone |  |
| 90 |  |  [Reviewing Natural Language Processing Research](https://doi.org/10.18653/v1/2020.acl-tutorials.4) |  | 0 | This tutorial will cover the theory and practice of reviewing research in natural language processing. Heavy reviewing burdens on natural language processing researchers have made it clear that our community needs to increase the size of our pool of potential reviewers. Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences—have raised awareness of the fact that our reviewing practices leave something to be... | Aurélie Névéol, K. Bretonnel Cohen, Karën Fort, Margot Mieskes |  |
| 91 |  |  [Stylized Text Generation: Approaches and Applications](https://doi.org/10.18653/v1/2020.acl-tutorials.5) |  | 0 | Text generation has played an important role in various applications of natural language processing (NLP), and kn recent studies, researchers are paying increasing attention to modeling and manipulating the style of the generation text, which we call stylized text generation. In this tutorial, we will provide a comprehensive literature review in this direction. We start from the definition of style and different settings of stylized text generation, illustrated with various applications. Then, we present different settings... | Lili Mou, Olga Vechtomova |  |
| 92 |  |  [Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web](https://doi.org/10.18653/v1/2020.acl-tutorials.6) |  | 0 | The World Wide Web contains vast quantities of textual information in several forms: unstructured text, template-based semi-structured webpages (which present data in key-value pairs and lists), and tables. Methods for extracting information from these sources and converting it to a structured form have been a target of research from the natural language processing (NLP), data mining, and database communities. While these researchers have largely separated extraction from web data into different problems based on the... | Colin Lockard, Hannaneh Hajishirzi, Prashant Shiralkar, Xin Luna Dong |  |
| 93 |  |  [Commonsense Reasoning for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-tutorials.7) |  | 0 | Commonsense knowledge, such as knowing that “bumping into people annoys them” or “rain makes the road slippery”, helps humans navigate everyday situations seamlessly. Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades. In recent years, commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community, yielding exploratory studies in automated commonsense... | Antoine Bosselut, Dan Roth, Maarten Sap, Vered Shwartz, Yejin Choi |  |
| 94 |  |  [Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.acl-tutorials.8) |  | 0 | This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain QA,... | Danqi Chen, Wentau Yih |  |
| 95 |  |  [Learning to Understand Child-directed and Adult-directed Speech](https://doi.org/10.18653/v1/2020.acl-main.1) |  | 0 | Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech... | Afra Alishahi, Grzegorz Chrupala, Lieke Gelderloos |  |
| 96 |  |  [Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts](https://doi.org/10.18653/v1/2020.acl-main.2) |  | 0 | Accurately diagnosing depression is difficult– requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to define high-level conversational contexts that... | Alex Rinaldi, Jean E. Fox Tree, Snigdha Chaturvedi |  |
| 97 |  |  [Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling](https://doi.org/10.18653/v1/2020.acl-main.3) |  | 0 | As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities.... | Genta Indra Winata, Pascale Fung, Peng Xu, Zihan Liu |  |
| 98 |  |  [Designing Precise and Robust Dialogue Response Evaluators](https://doi.org/10.18653/v1/2020.acl-main.4) |  | 0 | Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes... | Divesh Lala, Tatsuya Kawahara, Tianyu Zhao |  |
| 99 |  |  [Dialogue State Tracking with Explicit Slot Connection Modeling](https://doi.org/10.18653/v1/2020.acl-main.5) |  | 0 | Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source... | Jiajun Chen, Moxin Chen, Shujian Huang, Xinyu Dai, Yawen Ouyang, Yinggong Zhao |  |
| 100 |  |  [Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy](https://doi.org/10.18653/v1/2020.acl-main.6) |  | 0 | Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. Furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external... | Jianshan He, Taifeng Wang, Wei Chu, Weiyu Jian, Xiexiong Lin |  |
| 101 |  |  [Guiding Variational Response Generator to Exploit Persona](https://doi.org/10.18653/v1/2020.acl-main.7) |  | 0 | Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the... | Baoxun Wang, Bowen Wu, Derek F. Wong, Junhong Huang, Mengyuan Li, Qihang Feng, Yifu Chen, Zongsheng Wang |  |
| 102 |  |  [Large Scale Multi-Actor Generative Dialog Modeling](https://doi.org/10.18653/v1/2020.acl-main.8) |  | 0 | Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent’s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person’s speech and better emulate them in generated responses. This work... | Alex Boyd, Bryan Catanzaro, Mohammad Shoeybi, Mostofa Patwary, Raul Puri |  |
| 103 |  |  [PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable](https://doi.org/10.18653/v1/2020.acl-main.9) |  | 0 | Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to... | Fan Wang, Haifeng Wang, Hua Wu, Huang He, Siqi Bao |  |
| 104 |  |  [Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network](https://doi.org/10.18653/v1/2020.acl-main.10) |  | 0 | Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving... | Kaisheng Yao, Libo Qin, Ting Liu, Wanxiang Che, Xiaolong Li, Yangming Li |  |
| 105 |  |  [Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations](https://doi.org/10.18653/v1/2020.acl-main.11) |  | 0 | We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2)... | Daniela Gerz, Ivan Vulic, Matthew Henderson, Sam Coope, Tyler Farghly |  |
| 106 |  |  [Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2020.acl-main.12) |  | 0 | Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on... | Agata Foryciarz, Giovanni Campagna, Mehrad Moradshahi, Monica S. Lam |  |
| 107 |  |  [A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle](https://doi.org/10.18653/v1/2020.acl-main.13) |  | 0 | This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves... | HenHsen Huang, HsinHsi Chen, ShyhShiun Hung |  |
| 108 |  |  [TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2020.acl-main.14) |  | 0 | Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called... | Fengyu Guo, Jian Wang, Ruifang He, Yugui Han |  |
| 109 |  |  [A Study of Non-autoregressive Model for Sequence Generation](https://doi.org/10.18653/v1/2020.acl-main.15) |  | 0 | Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up... | Jinglin Liu, Sheng Zhao, TieYan Liu, Xu Tan, Yi Ren, Zhou Zhao |  |
| 110 |  |  [Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage](https://doi.org/10.18653/v1/2020.acl-main.16) |  | 0 | Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages... | Ashish V. Thapliyal, Radu Soricut |  |
| 111 |  |  [Fact-based Text Editing](https://doi.org/10.18653/v1/2020.acl-main.17) |  | 0 | We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the... | Chao Qiao, Hang Li, Hayate Iso |  |
| 112 |  |  [Few-Shot NLG with Pre-Trained Language Model](https://doi.org/10.18653/v1/2020.acl-main.18) |  | 0 | Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture... | Harini Eavani, Wenhu Chen, William Yang Wang, Yinyin Liu, Zhiyu Chen |  |
| 113 |  |  [Fluent Response Generation for Conversational Question Answering](https://doi.org/10.18653/v1/2020.acl-main.19) |  | 0 | Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while... | Alan Ritter, Ashutosh Baheti, Kevin Small |  |
| 114 |  |  [Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs](https://doi.org/10.18653/v1/2020.acl-main.20) |  | 0 | One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts,... | Dong Bok Lee, Donghwan Kim, Seanie Lee, Sung Ju Hwang, Woo Tae Jeong |  |
| 115 |  |  [Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction](https://doi.org/10.18653/v1/2020.acl-main.21) |  | 0 | Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only... | Xiaojun Wan, Zi Chai |  |
| 116 |  |  [Neural Syntactic Preordering for Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2020.acl-main.22) |  | 0 | Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model. First, given an... | Greg Durrett, Tanya Goyal |  |
| 117 |  |  [Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders](https://doi.org/10.18653/v1/2020.acl-main.23) |  | 0 | Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation.... | Canwen Xu, Chenliang Li, Jialong Han, Jiaxin Pei, Yu Duan |  |
| 118 |  |  [Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order](https://doi.org/10.18653/v1/2020.acl-main.24) |  | 0 | Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on... | Qun Liu, Xin Jiang, Yi Liao |  |
| 119 |  |  [Reverse Engineering Configurations of Neural Text Generation Models](https://doi.org/10.18653/v1/2020.acl-main.25) |  | 0 | Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding... | Andrew Tomkins, Che Zheng, Clifford Brunk, Dara Bahri, Donald Metzler, Yi Tay |  |
| 120 |  |  [Review-based Question Generation with Adaptive Instance Transfer and Augmentation](https://doi.org/10.18653/v1/2020.acl-main.26) |  | 0 | While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review... | Lidong Bing, Luo Si, Qian Yu, Qiong Zhang, Wai Lam |  |
| 121 |  |  [TAG : Type Auxiliary Guiding for Code Comment Generation](https://doi.org/10.18653/v1/2020.acl-main.27) |  | 0 | Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type... | Boyan Xu, Ruichu Cai, Yao Chen, Yuexing Hao, Zhihao Liang, Zijian Li |  |
| 122 |  |  [Unsupervised Paraphrasing by Simulated Annealing](https://doi.org/10.18653/v1/2020.acl-main.28) |  | 0 | We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA... | Fandong Meng, Hao Zhou, Jie Zhou, Lili Mou, Sen Song, Xianggen Liu |  |
| 123 |  |  [A Joint Model for Document Segmentation and Segment Labeling](https://doi.org/10.18653/v1/2020.acl-main.29) |  | 0 | Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors... | Douglas W. Oard, Joe Barrow, Philip Resnik, Rajiv Jain, Varun Manjunatha, Vlad I. Morariu |  |
| 124 |  |  [Contextualized Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2020.acl-main.30) |  | 0 | Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed... | Dheeraj Mekala, Jingbo Shang |  |
| 125 |  |  [Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks](https://doi.org/10.18653/v1/2020.acl-main.31) |  | 0 | Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations... | Liang Wang, Shu Wu, Xueli Yu, Yufeng Zhang, Zeyu Cui, Zhongzhen Wen |  |
| 126 |  |  [Neural Topic Modeling with Bidirectional Adversarial Training](https://doi.org/10.18653/v1/2020.acl-main.32) |  | 0 | Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach,... | Chenchen Ye, Deyu Zhou, Haiyang Xu, Rui Wang, Xuemeng Hu, Yulan He, Yuxuan Xiong |  |
| 127 |  |  [Text Classification with Negative Supervision](https://doi.org/10.18653/v1/2020.acl-main.33) |  | 0 | Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different... | Chenhui Chu, Junya Takayama, Sora Ohashi, Tomoyuki Kajiwara, Yuki Arase |  |
| 128 |  |  [Content Word Aware Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.34) |  | 0 | Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation... | Eiichiro Sumita, Kehai Chen, Masao Utiyama, Rui Wang |  |
| 129 |  |  [Evaluating Explanation Methods for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.35) |  | 0 | Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity... | Guanlin Li, Guoping Huang, Huayang Li, Jierui Li, Lemao Liu, Shuming Shi |  |
| 130 |  |  [Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.36) |  | 0 | The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role... | Enhong Chen, Junliang Guo, Linli Xu |  |
| 131 |  |  [Learning Source Phrase Representations for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.37) |  | 0 | The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has... | Deyi Xiong, Hongfei Xu, Jingyi Zhang, Josef van Genabith, Qiuhui Liu |  |
| 132 |  |  [Lipschitz Constrained Parameter Initialization for Deep Transformers](https://doi.org/10.18653/v1/2020.acl-main.38) |  | 0 | The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which... | Deyi Xiong, Hongfei Xu, Jingyi Zhang, Josef van Genabith, Qiuhui Liu |  |
| 133 |  |  [Location Attention for Extrapolation to Longer Sequences](https://doi.org/10.18653/v1/2020.acl-main.39) |  | 0 | Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization... | Dieuwke Hupkes, Elia Bruni, Gautier Dagan, Yann Dubois |  |
| 134 |  |  [Multiscale Collaborative Deep Models for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.40) |  | 0 | Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a... | Heng Yu, Rongxiang Weng, Weihua Luo, Xiangpeng Wei, Yue Hu, Yue Zhang |  |
| 135 |  |  [Norm-Based Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.41) |  | 0 | A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty... | Derek F. Wong, Houtim Lai, Lidia S. Chao, Xuebo Liu |  |
| 136 |  |  [Opportunistic Decoding with Timely Correction for Simultaneous Translation](https://doi.org/10.18653/v1/2020.acl-main.42) |  | 0 | Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest... | Baigong Zheng, Kaibo Liu, Liang Huang, Mingbo Ma, Renjie Zheng |  |
| 137 |  |  [A Formal Hierarchy of RNN Architectures](https://doi.org/10.18653/v1/2020.acl-main.43) |  | 0 | We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models’ expressive capacity is... | Eran Yahav, Gail Weiss, Noah A. Smith, Roy Schwartz, William Merrill, Yoav Goldberg |  |
| 138 |  |  [A Three-Parameter Rank-Frequency Relation in Natural Languages](https://doi.org/10.18653/v1/2020.acl-main.44) |  | 0 | We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters. The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f). The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0. We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an... | Chenchen Ding, Eiichiro Sumita, Masao Utiyama |  |
| 139 |  |  [Dice Loss for Data-imbalanced NLP Tasks](https://doi.org/10.18653/v1/2020.acl-main.45) |  | 0 | Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more... | Fei Wu, Jiwei Li, Junjun Liang, Xiaofei Sun, Xiaoya Li, Yuxian Meng |  |
| 140 |  |  [Emergence of Syntax Needs Minimal Supervision](https://doi.org/10.18653/v1/2020.acl-main.46) |  | 0 | This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on... | Kata Gábor, Raphaël Bailly |  |
| 141 |  |  [Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese](https://doi.org/10.18653/v1/2020.acl-main.47) |  | 0 | We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word... | Jun Suzuki, Kentaro Inui, Takumi Ito, Tatsuki Kuribayashi |  |
| 142 |  |  [GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media](https://doi.org/10.18653/v1/2020.acl-main.48) |  | 0 | This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real... | ChengTe Li, YiJu Lu |  |
| 143 |  |  [Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection](https://doi.org/10.18653/v1/2020.acl-main.49) |  | 0 | Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional... | Juan Cao, Junbo Guo, Lei Zhong, Qiang Sheng, Ziang Wang |  |
| 144 |  |  [Predicting the Topical Stance and Political Leaning of Media using Tweets](https://doi.org/10.18653/v1/2020.acl-main.50) |  | 0 | Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the... | Atanas Atanasov, Kareem Darwish, Peter Stefanov, Preslav Nakov |  |
| 145 |  |  [Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora](https://doi.org/10.18653/v1/2020.acl-main.51) |  | 0 | The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results.... | Djamé Seddah, Ganesh Jawahar, Hila Gonen, Yoav Goldberg |  |
| 146 |  |  [CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation](https://doi.org/10.18653/v1/2020.acl-main.52) |  | 0 | Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between... | Lei Shen, Yang Feng |  |
| 147 |  |  [Efficient Dialogue State Tracking by Selectively Overwriting Memory](https://doi.org/10.18653/v1/2020.acl-main.53) |  | 0 | Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2)... | Gyuwan Kim, SangWoo Lee, Sohee Yang, Sungdong Kim |  |
| 148 |  |  [End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2](https://doi.org/10.18653/v1/2020.acl-main.54) |  | 0 | The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture... | DongHoon Ham, JeongGwan Lee, KeeEung Kim, Youngsoo Jang |  |
| 149 |  |  [Evaluating Dialogue Generation Systems via Response Selection](https://doi.org/10.18653/v1/2020.acl-main.55) |  | 0 | Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as... | Hiroki Ouchi, Jun Suzuki, Kentaro Inui, Reina Akama, Shiki Sato |  |
| 150 |  |  [Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection](https://doi.org/10.18653/v1/2020.acl-main.56) |  | 0 | Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen... | Hui Lin, Ruobing Li, Yefei Zha |  |
| 151 |  |  [Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment](https://doi.org/10.18653/v1/2020.acl-main.57) |  | 0 | Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for... | Chengguang Tang, Hangyu Li, Jian Sun, Xiaodan Zhu, Yinpei Dai, Yongbin Li |  |
| 152 |  |  [Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge](https://doi.org/10.18653/v1/2020.acl-main.58) |  | 0 | Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our... | Keqing He, Weiran Xu, Yuanmeng Yan |  |
| 153 |  |  [Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition](https://doi.org/10.18653/v1/2020.acl-main.59) |  | 0 | Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a... | Minlie Huang, Runze Liang, Ryuichi Takanobu |  |
| 154 |  |  [Paraphrase Augmented Task-Oriented Dialog Generation](https://doi.org/10.18653/v1/2020.acl-main.60) |  | 0 | Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set... | Silin Gao, Yichi Zhang, Zhijian Ou, Zhou Yu |  |
| 155 |  |  [Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation](https://doi.org/10.18653/v1/2020.acl-main.61) |  | 0 | Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document. In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. In this paper, we... | Dongkyu Lee, Lanqing Xue, Nevin L. Zhang, Wei Bi, Xiaojiang Liu, Yiping Song, Zhiliang Tian |  |
| 156 |  |  [Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation](https://doi.org/10.18653/v1/2020.acl-main.62) |  | 0 | Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor... | Jianzhong Qi, Rui Zhang, Xinting Huang, Yu Sun |  |
| 157 |  |  [Towards Unsupervised Language Understanding and Generation by Joint Dual Learning](https://doi.org/10.18653/v1/2020.acl-main.63) |  | 0 | In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning... | ChaoWei Huang, ShangYu Su, YunNung Chen |  |
| 158 |  |  [USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation](https://doi.org/10.18653/v1/2020.acl-main.64) |  | 0 | The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level:... | Maxine Eskénazi, Shikib Mehri |  |
| 159 |  |  [Explicit Semantic Decomposition for Definition Generation](https://doi.org/10.18653/v1/2020.acl-main.65) |  | 0 | Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models... | Jiahuan Li, Jiajun Chen, Shujian Huang, Xinyu Dai, Yu Bao |  |
| 160 |  |  [Improved Natural Language Generation via Loss Truncation](https://doi.org/10.18653/v1/2020.acl-main.66) |  | 0 | Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and... | Daniel Kang, Tatsunori Hashimoto |  |
| 161 |  |  [Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks](https://doi.org/10.18653/v1/2020.acl-main.67) |  | 0 | Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships... | Kai Yu, Lu Chen, Ruisheng Cao, Su Zhu, Yanbin Zhao, Zhi Chen |  |
| 162 |  |  [Rigid Formats Controlled Text Generation](https://doi.org/10.18653/v1/2020.acl-main.68) |  | 0 | Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes.... | Haisong Zhang, Piji Li, Shuming Shi, Xiaojiang Liu |  |
| 163 |  |  [Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation](https://doi.org/10.18653/v1/2020.acl-main.69) |  | 0 | Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps... | Christopher D. Manning, Kaustubh D. Dhole |  |
| 164 |  |  [An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering](https://doi.org/10.18653/v1/2020.acl-main.70) |  | 0 | Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering. Therefore, in this paper, we propose an... | Jay Kumar, Junming Shao, Salah Uddin, Wazir Ali |  |
| 165 |  |  [Generative Semantic Hashing Enhanced via Boltzmann Machines](https://doi.org/10.18653/v1/2020.acl-main.71) |  | 0 | Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash... | Changyou Chen, Dinghan Shen, Lin Zheng, Qinliang Su |  |
| 166 |  |  [Interactive Construction of User-Centric Dictionary for Text Analytics](https://doi.org/10.18653/v1/2020.acl-main.72) |  | 0 | We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms. Along with the... | Hiroshi Kanayama, Issei Yoshida, Ryosuke Kohita, Tetsuya Nasukawa |  |
| 167 |  |  [Tree-Structured Neural Topic Model](https://doi.org/10.18653/v1/2020.acl-main.73) |  | 0 | This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the... | Danushka Bollegala, Ichiro Sakata, Junichiro Mori, Masaru Isonuma |  |
| 168 |  |  [Unsupervised FAQ Retrieval with Question Generation and BERT](https://doi.org/10.18653/v1/2020.acl-main.74) |  | 0 | We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. The two models match user queries to FAQ answers and questions, respectively. We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. We show that our model is on par and even outperforms supervised models on existing... | Boaz Carmeli, David Konopnicki, Haggai Roitman, Yosi Mass |  |
| 169 |  |  ["The Boating Store Had Its Best Sail Ever": Pronunciation-attentive Contextualized Pun Recognition](https://doi.org/10.18653/v1/2020.acl-main.75) |  | 0 | Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains... | Jieyu Zhao, JyunYu Jiang, KaiWei Chang, Wei Wang, Yichao Zhou |  |
| 170 |  |  [Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning](https://doi.org/10.18653/v1/2020.acl-main.76) |  | 0 | Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In... | Joongbo Shin, Kyomin Jung, Seunghyun Yoon, Yoonhyung Lee |  |
| 171 |  |  [Fine-grained Interest Matching for Neural News Recommendation](https://doi.org/10.18653/v1/2020.acl-main.77) |  | 0 | Personalized news recommendation is a critical technology to improve users’ online news reading experience. The core of news recommendation is accurate matching between user’s interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose... | Fangzhao Wu, Heyuan Wang, Xing Xie, Zheng Liu |  |
| 172 |  |  [Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder](https://doi.org/10.18653/v1/2020.acl-main.78) |  | 0 | Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We... | Fan Zhou, Shengming Zhang, Yi Yang |  |
| 173 |  |  [Interpreting Twitter User Geolocation](https://doi.org/10.18653/v1/2020.acl-main.79) |  | 0 | Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users. This methodology helps... | Fan Zhou, Goce Trajcevski, Kunpeng Zhang, Tianliang Wang, Ting Zhong, Yi Yang |  |
| 174 |  |  [Modeling Code-Switch Languages Using Bilingual Parallel Corpus](https://doi.org/10.18653/v1/2020.acl-main.80) |  | 0 | Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual... | Grandee Lee, Haizhou Li |  |
| 175 |  |  [SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://doi.org/10.18653/v1/2020.acl-main.81) |  | 0 | Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters,... | Feng Wang, Kunlong Chen, Shaohua Jiang, Taifeng Wang, Wei Chu, Weidi Xu, Xingyi Cheng, Yuan Qi |  |
| 176 |  |  [Spelling Error Correction with Soft-Masked BERT](https://doi.org/10.18653/v1/2020.acl-main.82) |  | 0 | Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this paper. A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model. The accuracy of the method can be sub-optimal, however,... | Hang Li, Haoran Huang, Jicong Liu, Shaohua Zhang |  |
| 177 |  |  [A Frame-based Sentence Representation for Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.83) |  | 0 | Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models,... | Hongyan Zhao, Hongye Tan, Ru Li, Shaoru Guo, Xiaoli Li, Yong Guan, Yueping Zhang |  |
| 178 |  |  [A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation](https://doi.org/10.18653/v1/2020.acl-main.84) |  | 0 | In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context... | Dirk Von Gruenigen, Eneko Agirre, Jan Deriu, Katsiaryna Mlynchyk, Kurt Stockinger, Mark Cieliebak, Nicolas Kaiser, Philippe Schläpfer, Álvaro Rodrigo |  |
| 179 |  |  [Contextualized Sparse Representations for Real-Time Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.acl-main.85) |  | 0 | Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified... | Hannaneh Hajishirzi, Jaewoo Kang, Jinhyuk Lee, Min Joon Seo |  |
| 180 |  |  [Dynamic Sampling Strategies for Multi-Task Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.86) |  | 0 | Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community. Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up. We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model’s current performance on a dataset relative to its single task performance, gives... | Ananth Gottumukkala, Dheeru Dua, Matt Gardner, Sameer Singh |  |
| 181 |  |  [Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.87) |  | 0 | Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages. However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary. In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which... | Daxin Jiang, Fei Yuan, Linjun Shou, Ming Gong, Nan Duan, Xuanyu Bai, Yan Fu, Yaobo Liang |  |
| 182 |  |  [Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading](https://doi.org/10.18653/v1/2020.acl-main.88) |  | 0 | The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions. Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them. In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision. Moreover, our... | Caiming Xiong, ChienSheng Wu, Irwin King, Michael R. Lyu, Richard Socher, Shafiq R. Joty, Steven C. H. Hoi, Yifan Gao |  |
| 183 |  |  [Injecting Numerical Reasoning Skills into Language Models](https://doi.org/10.18653/v1/2020.acl-main.89) |  | 0 | Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of... | Ankit Gupta, Jonathan Berant, Mor Geva |  |
| 184 |  |  [Learning to Identify Follow-Up Questions in Conversational Question Answering](https://doi.org/10.18653/v1/2020.acl-main.90) |  | 0 | Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions. Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently. In this paper, we introduce a new follow-up question identification task. We propose a three-way attentive pooling network... | Hwee Tou Ng, Qian Lin, Souvik Kundu |  |
| 185 |  |  [Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases](https://doi.org/10.18653/v1/2020.acl-main.91) |  | 0 | Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations. In this paper, we handle both types of complexity at the same time. Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs. Our experiments clearly... | Jing Jiang, Yunshi Lan |  |
| 186 |  |  [A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers](https://doi.org/10.18653/v1/2020.acl-main.92) |  | 0 | We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level... | ChaoChun Liang, KehYih Su, ShenYun Miao |  |
| 187 |  |  [Improving Image Captioning Evaluation by Considering Inter References Variance](https://doi.org/10.18653/v1/2020.acl-main.93) |  | 0 | Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image. Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions. It usually leads to over-penalization and thus a bad correlation to human judgment. Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some... | Hangyu Deng, Jinglu Hu, Yanzhi Yi |  |
| 188 |  |  [Revisiting the Context Window for Cross-lingual Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.94) |  | 0 | Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar. The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines. Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work. In this work, we provide a thorough evaluation, in various... | Ryokan Ri, Yoshimasa Tsuruoka |  |
| 189 |  |  [Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders](https://doi.org/10.18653/v1/2020.acl-main.95) |  | 0 | A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training. We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense. The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense... | Luke Zettlemoyer, Terra Blevins |  |
| 190 |  |  [Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection](https://doi.org/10.18653/v1/2020.acl-main.96) |  | 0 | In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications. In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks. We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications. | Animesh Mukherjee, Ayush Suhane, Jasabanta Patro, Srijan Bansal, Vishal Garimella |  |
| 191 |  |  [DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification](https://doi.org/10.18653/v1/2020.acl-main.97) |  | 0 | Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for... | Ambreen Nazir, Hao Liang, Lianwei Wu, Yongqiang Zhao, Yuan Rao |  |
| 192 |  |  [Towards Conversational Recommendation over Multi-Type Dialogs](https://doi.org/10.18653/v1/2020.acl-main.98) |  | 0 | We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user’s interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot).... | Haifeng Wang, Hua Wu, Ting Liu, Wanxiang Che, Zeming Liu, ZhengYu Niu |  |
| 193 |  |  [Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification](https://doi.org/10.18653/v1/2020.acl-main.99) |  | 0 | User intent classification plays a vital role in dialogue systems. Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun. This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection. In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more... | Albert Y. S. Lam, Guangfeng Yan, Han Liu, Lu Fan, Qimai Li, XiaoMing Wu, Xiaotong Zhang |  |
| 194 |  |  [Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen](https://doi.org/10.18653/v1/2020.acl-main.100) |  | 0 | The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep... | Liangming Pan, MinYen Kan, Ruihao Shui, TatSeng Chua, Yixin Cao, Zhiyuan Liu |  |
| 195 |  |  [Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints](https://doi.org/10.18653/v1/2020.acl-main.101) |  | 0 | Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a... | Bang An, Changyou Chen, Dong Yu, Xiaoyang Wang, Zhenyi Wang |  |
| 196 |  |  [Dynamic Memory Induction Networks for Few-Shot Text Classification](https://doi.org/10.18653/v1/2020.acl-main.102) |  | 0 | This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification. The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification. The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning. The proposed model brings forward the state-of-the-art performance significantly by 2 4% improvement on the... | Binhua Li, Jian Sun, Ruiying Geng, Xiaodan Zhu, Yongbin Li |  |
| 197 |  |  [Exclusive Hierarchical Decoding for Deep Keyphrase Generation](https://doi.org/10.18653/v1/2020.acl-main.103) |  | 0 | Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases. A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce. Previous work in this setting employs a sequential decoding process to generate keyphrases. However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document.... | Hou Pong Chan, Irwin King, Piji Li, Wang Chen |  |
| 198 |  |  [Hierarchy-Aware Global Model for Hierarchical Text Classification](https://doi.org/10.18653/v1/2020.acl-main.104) |  | 0 | Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy. Existing methods have difficulties in modeling the hierarchical label structure in a global view. Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space. In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies. Based on the hierarchy encoder, we... | Chunping Ma, Dingkun Long, Gongshen Liu, Guangwei Xu, Haoyu Zhang, Jie Zhou, Ning Ding, Pengjun Xie |  |
| 199 |  |  [Keyphrase Generation for Scientific Document Retrieval](https://doi.org/10.18653/v1/2020.acl-main.105) |  | 0 | Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval. This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models. Using this framework, we point out and discuss the difficulties encountered with supplementing... | Akiko Aizawa, Florian Boudin, Ygor Gallina |  |
| 200 |  |  [A Graph Auto-encoder Model of Derivational Morphology](https://doi.org/10.18653/v1/2020.acl-main.106) |  | 0 | There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics. We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation. The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon. | Hinrich Schütze, Janet B. Pierrehumbert, Valentin Hofmann |  |
| 201 |  |  [Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell](https://doi.org/10.18653/v1/2020.acl-main.107) |  | 0 | We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments... | Abhishek Srivastava, Amal Fethi, Benjamin Muller, Benoît Sagot, Djamé Seddah, Farah Essaidi, Matthieu Futeral, Pedro Javier Ortiz Suárez |  |
| 202 |  |  [Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis](https://doi.org/10.18653/v1/2020.acl-main.108) |  | 0 | This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully preprocessed email corpus to date. We crawled more than 153 million emails from 14,699 mailing lists and segmented them into semantically consistent components using a new neural segmentation model. With 96% accuracy on 15 classes of email segments, our model achieves state-of-the-art performance while being more efficient to train than previous ones. All data, code, and trained models are made freely available alongside the... | Benno Stein, Janek Bevendorff, Khalid Al Khatib, Martin Potthast |  |
| 203 |  |  [Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences](https://doi.org/10.18653/v1/2020.acl-main.109) |  | 0 | The patterns in which the syntax of different languages converges and diverges are often used to inform work on cross-lingual transfer. Nevertheless, little empirical work has been done on quantifying the prevalence of different syntactic divergences across language pairs. We propose a framework for extracting divergence patterns for any language pair from a parallel corpus, building on Universal Dependencies. We show that our framework provides a detailed picture of cross-language divergences, generalizes previous... | Dmitry Nikolaev, Lilja Maria Saeboe, Neta Kenneth, Ofir Arviv, Omri Abend, Taelin Karidi, Veronika Mitnik |  |
| 204 |  |  [Generating Counter Narratives against Online Hate Speech: Data and Strategies](https://doi.org/10.18653/v1/2020.acl-main.110) |  | 0 | Recently research has started focusing on avoiding undesired effects that come with content moderation, such as censorship and overblocking, when dealing with hatred online. The core idea is to directly intervene in the discussion with textual responses that are meant to counter the hate content and prevent it from further spreading. Accordingly, automation strategies, such as natural language generation, are beginning to be investigated. Still, they suffer from the lack of sufficient amount of quality data and tend to... | Marco Guerini, Serra Sinem Tekiroglu, YiLing Chung |  |
| 205 |  |  [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.111) |  | 0 | In recent years, a series of Transformer-based models unlocked major improvements in general natural language understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language understanding, accompanied by an online leaderboard. It consists of a diverse... | Ireneusz Gawlik, Janusz Tracz, Piotr Rybak, Robert Mroczkowski |  |
| 206 |  |  [Learning and Evaluating Emotion Lexicons for 91 Languages](https://doi.org/10.18653/v1/2020.acl-main.112) |  | 0 | Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating... | Susanna Rücker, Sven Buechel, Udo Hahn |  |
| 207 |  |  [Multi-Hypothesis Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.acl-main.113) |  | 0 | Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem. One of the main challenges is the fact that multiple outputs can be equally valid. Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references. The latter has been shown to significantly improve the performance of evaluation metrics. However, collecting multiple references is expensive and in practice a single reference is generally used. In this... | Francisco Guzmán, Lucia Specia, Marina Fomicheva |  |
| 208 |  |  [Multimodal Quality Estimation for Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.114) |  | 0 | We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE. We compare various multimodality integration and fusion strategies. For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality. | Frédéric Blain, Lucia Specia, Shu Okabe |  |
| 209 |  |  [PuzzLing Machines: A Challenge on Learning From Small Data](https://doi.org/10.18653/v1/2020.acl-main.115) |  | 0 | Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal... | Gözde Gül Sahin, Iryna Gurevych, Phillip Rust, Yova Kementchedjhieva |  |
| 210 |  |  [The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain](https://doi.org/10.18653/v1/2020.acl-main.116) |  | 0 | This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of... | Anika Marusczyk, Annemarie Friedrich, Federico Tomazic, Heike Adel, Johannes Hingerl, Lukas Lange, Renou Benteau |  |
| 211 |  |  [The TechQA Dataset](https://doi.org/10.18653/v1/2020.acl-main.117) |  | 0 | We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain. The TECHQA corpus highlights two real-world issues from the automated customer support domain. First, it contains actual questions posed by users on a technical forum, rather than questions generated specifically for a competition or a task. Second, it has a real-world size – 600 training, 310 dev, and 490 evaluation question/answer pairs – thus reflecting the cost of creating large labeled datasets with actual data. Hence,... | Andrzej Sakrajda, Anthony Ferritto, Avirup Sil, Cezar Pendus, Dinesh Garg, Dinesh Khandelwal, J. Scott McCarley, John F. Pitrelli, Lin Pan, Martin Franz, Mike McCawley, Mohamed Nasr, Radu Florian, Rishav Chakravarti, Rong Zhang, Rosario UcedaSosa, Salim Roukos, Saswati Dana, Saurabh Pujar, Todd Ward, Vittorio Castelli |  |
| 212 |  |  [iSarcasm: A Dataset of Intended Sarcasm](https://doi.org/10.18653/v1/2020.acl-main.118) |  | 0 | We consider the distinction between intended and perceived sarcasm in the context of textual sarcasm detection. The former occurs when an utterance is sarcastic from the perspective of its author, while the latter occurs when the utterance is interpreted as sarcastic by the audience. We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the iSarcasm dataset of tweets labeled for sarcasm directly by their authors. Examining the state-of-the-art sarcasm detection models on our... | Silviu Oprea, Walid Magdy |  |
| 213 |  |  [AMR Parsing via Graph-Sequence Iterative Inference](https://doi.org/10.18653/v1/2020.acl-main.119) |  | 0 | We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that... | Deng Cai, Wai Lam |  |
| 214 |  |  [A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal](https://doi.org/10.18653/v1/2020.acl-main.120) |  | 0 | Multi-document summarization (MDS) aims to compress the content in large document collections into short summaries and has important applications in story clustering for newsfeeds, presentation of search results, and timeline generation. However, there is a lack of datasets that realistically address such use cases at a scale large enough for training supervised models for this task. This work presents a new dataset for MDS that is large both in the total number of document clusters and in the size of individual clusters. We... | Chris Hokamp, Demian Gholipour Ghalandari, Georgiana Ifrim, John Glover, Nghia The Pham |  |
| 215 |  |  [Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2020.acl-main.121) |  | 0 | Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three... | Chengqing Zong, Jiajun Zhang, Junnan Zhu, Yu Zhou |  |
| 216 |  |  [Examining the State-of-the-Art in News Timeline Summarization](https://doi.org/10.18653/v1/2020.acl-main.122) |  | 0 | Previous work on automatic news timeline summarization (TLS) leaves an unclear picture about how this task can generally be approached and how well it is currently solved. This is mostly due to the focus on individual subtasks, such as date selection and date summarization, and to the previous lack of appropriate evaluation metrics for the full TLS task. In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a simple and effective combination of methods that improves over the... | Demian Gholipour Ghalandari, Georgiana Ifrim |  |
| 217 |  |  [Improving Truthfulness of Headline Generation](https://doi.org/10.18653/v1/2020.acl-main.123) |  | 0 | Most studies on abstractive summarization report ROUGE scores between system and reference summaries. However, we have a concern about the truthfulness of generated summaries: whether all facts of a generated summary are mentioned in the source text. This paper explores improving the truthfulness in headline generation on two popular datasets. Analyzing headlines generated by the state-of-the-art encoder-decoder model, we show that the model sometimes generates untruthful headlines. We conjecture that one of the reasons lies... | Kazuki Matsumaru, Naoaki Okazaki, Sho Takase |  |
| 218 |  |  [SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.124) |  | 0 | We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better... | Steffen Eger, Wei Zhao, Yang Gao |  |
| 219 |  |  [Self-Attention Guided Copy Mechanism for Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.125) |  | 0 | Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary. Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge. In this work, we propose a Transformer-based model to enhance the copy mechanism. Specifically, we identify the importance of each source word based on the degree centrality with a directed graph... | Bowen Zhou, Haoran Li, Peng Yuan, Song Xu, Xiaodong He, Youzheng Wu |  |
| 220 |  |  [Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation](https://doi.org/10.18653/v1/2020.acl-main.126) |  | 0 | Open Domain dialog system evaluation is one of the most important challenges in dialog research. Existing automatic evaluation metrics, such as BLEU are mostly reference-based. They calculate the difference between the generated response and a limited number of available references. Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots. However, self-reported user rating suffers from bias and variance among different users. To alleviate this... | James Zou, Weixin Liang, Zhou Yu |  |
| 221 |  |  [Conversational Word Embedding for Retrieval-Based Dialog System](https://doi.org/10.18653/v1/2020.acl-main.127) |  | 0 | Human conversations contain many types of information, e.g., knowledge, common sense, and language habits. In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding. Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply. To catch the information among the pair, we first introduce the word alignment model from statistical machine translation... | Dong Wang, Guoping Hu, Shijin Wang, Ting Liu, Wentao Ma, Yiming Cui |  |
| 222 |  |  [Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network](https://doi.org/10.18653/v1/2020.acl-main.128) |  | 0 | In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label... | Han Liu, Ting Liu, Wanxiang Che, Yijia Liu, Yongkui Lai, Yutai Hou, Zhihan Zhou |  |
| 223 |  |  [Learning Dialog Policies from Weak Demonstrations](https://doi.org/10.18653/v1/2020.acl-main.129) |  | 0 | Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems. Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests. We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert... | Gabriel GordonHall, Philip John Gorinski, Shay B. Cohen |  |
| 224 |  |  [MuTual: A Dataset for Multi-Turn Dialogue Reasoning](https://doi.org/10.18653/v1/2020.acl-main.130) |  | 0 | Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques. Given a context, current systems are able to yield a relevant and fluent response, but sometimes make logical mistakes because of weak reasoning capabilities. To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student... | Leyang Cui, Ming Zhou, Shujie Liu, Yu Wu, Yue Zhang |  |
| 225 |  |  [You Impress Me: Dialogue Generation via Mutual Persona Perception](https://doi.org/10.18653/v1/2020.acl-main.131) |  | 0 | Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling... | Bei Chen, Bin Zhou, Dongmei Zhang, JianGuang Lou, Qian Liu, Yihong Chen, Zixuan Chen |  |
| 226 |  |  [Bridging Anaphora Resolution as Question Answering](https://doi.org/10.18653/v1/2020.acl-main.132) |  | 0 | Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given. In this paper, we cast bridging anaphora resolution as question answering based on context. This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself). We present a question answering framework (BARQA) for this task, which leverages the power of... | Yufang Hou |  |
| 227 |  |  [Dialogue Coherence Assessment Without Explicit Dialogue Act Labels](https://doi.org/10.18653/v1/2020.acl-main.133) |  | 0 | Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use... | Iryna Gurevych, Mohsen Mesgar, Sebastian Bücker |  |
| 228 |  |  [Fast and Accurate Non-Projective Dependency Tree Linearization](https://doi.org/10.18653/v1/2020.acl-main.134) |  | 0 | We propose a graph-based method to tackle the dependency tree linearization task. We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs. We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree. We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences. Our proposed method outperforms the state-of-the-art linearizer... | Jonas Kuhn, Ngoc Thang Vu, Simon Tannert, Xiang Yu |  |
| 229 |  |  [Semantic Graphs for Generating Deep Questions](https://doi.org/10.18653/v1/2020.acl-main.135) |  | 0 | This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations... | Liangming Pan, MinYen Kan, TatSeng Chua, Yansong Feng, Yuxi Xie |  |
| 230 |  |  [A Novel Cascade Binary Tagging Framework for Relational Triple Extraction](https://doi.org/10.18653/v1/2020.acl-main.136) |  | 0 | Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in... | Jianlin Su, Yi Chang, Yuan Tian, Yue Wang, Zhepei Wei |  |
| 231 |  |  [In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts](https://doi.org/10.18653/v1/2020.acl-main.137) |  | 0 | Information Extraction (IE) from scientific texts can be used to guide readers to the central information in scientific documents. But narrow IE systems extract only a fraction of the information captured, and Open IE systems do not perform well on the long and complex sentences encountered in scientific texts. In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new task that we explore in the Biology domain. First, we present the Focused Open Biological Information... | Ioannis Konstas, Jessica ChenBurger, Julian F. V. Vincent, Marc P. Y. Desmulliez, Ruben Kruiper |  |
| 232 |  |  [NAT: Noise-Aware Training for Robust Neural Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.138) |  | 0 | Sequence labeling systems should perform reliably not only under ideal conditions but also with corrupted inputs—as these systems often process user-generated text or follow an error-prone upstream component. To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on perturbed input: Our data augmentation method trains a neural model using a mixture of... | Joachim Köhler, Marcin Namysl, Sven Behnke |  |
| 233 |  |  [Named Entity Recognition without Labelled Data: A Weak Supervision Approach](https://doi.org/10.18653/v1/2020.acl-main.139) |  | 0 | Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad... | Aliaksandr Hubin, Jeremy Barnes, Pierre Lison, Samia Touileb |  |
| 234 |  |  [Probing Linguistic Features of Sentence-Level Representations in Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.140) |  | 0 | Despite the recent progress, little is known about the features captured by state-of-the-art neural relation extraction (RE) models. Common methods encode the source sentence, conditioned on the entity mentions, before classifying the relation. However, the complexity of the task makes it difficult to understand how encoder architecture and supporting linguistic knowledge affect the features learned by the encoder. We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study... | Aleksandra Gabryszak, Christoph Alt, Leonhard Hennig |  |
| 235 |  |  [Reasoning with Latent Structure Refinement for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.141) |  | 0 | Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich... | Guoshun Nan, Ivan Sekulic, Wei Lu, Zhijiang Guo |  |
| 236 |  |  [TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task](https://doi.org/10.18653/v1/2020.acl-main.142) |  | 0 | TACRED is one of the largest, most widely used crowdsourced datasets in Relation Extraction (RE). But, even with recent advances in unsupervised pre-training and knowledge enhanced neural RE, models still show a high error rate. In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for improvement? And how do crowd annotations, dataset, and models contribute to this error rate? To answer these questions, we first validate the most challenging 5K examples in the development... | Aleksandra Gabryszak, Christoph Alt, Leonhard Hennig |  |
| 237 |  |  [Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences](https://doi.org/10.18653/v1/2020.acl-main.143) |  | 0 | In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary. Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences. We propose anchored training (AT) to tackle the task. AT uses the bilingual... | Baijun Ji, Boxing Chen, Hao Jia, Min Tan, Min Zhang, Weihua Luo, Xiangyu Duan, Yue Zhang |  |
| 238 |  |  [Boosting Neural Machine Translation with Similar Translations](https://doi.org/10.18653/v1/2020.acl-main.144) |  | 0 | This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with “copy”... | Jean Senellart, Jitao Xu, Josep Maria Crego |  |
| 239 |  |  [Character-Level Translation with Self-attention](https://doi.org/10.18653/v1/2020.acl-main.145) |  | 0 | We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the... | Nikola I. Nikolov, Richard H. R. Hahnloser, Yingqiang Gao, Yuhuang Hu |  |
| 240 |  |  [End-to-End Neural Word Alignment Outperforms GIZA++](https://doi.org/10.18653/v1/2020.acl-main.146) |  | 0 | Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key... | Joern Wuebker, John DeNero, Thomas Zenkel |  |
| 241 |  |  [Enhancing Machine Translation with Dependency-Aware Self-Attention](https://doi.org/10.18653/v1/2020.acl-main.147) |  | 0 | Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and... | Emanuele Bugliarello, Naoaki Okazaki |  |
| 242 |  |  [Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation](https://doi.org/10.18653/v1/2020.acl-main.148) |  | 0 | Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a... | Biao Zhang, Ivan Titov, Philip Williams, Rico Sennrich |  |
| 243 |  |  [It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](https://doi.org/10.18653/v1/2020.acl-main.149) |  | 0 | The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the... | Antonios Anastasopoulos, Emanuele Bugliarello, Naoaki Okazaki, Ryan Cotterell, Sabrina J. Mielke |  |
| 244 |  |  [Language-aware Interlingua for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.150) |  | 0 | Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to... | Changfeng Zhu, Heng Yu, Shanbo Cheng, Weihua Luo |  |
| 245 |  |  [On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.acl-main.151) |  | 0 | Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We... | Goran Glavas, Maxime Peyrard, Robert West, Steffen Eger, Wei Zhao, Yang Gao |  |
| 246 |  |  [Parallel Sentence Mining by Constrained Decoding](https://doi.org/10.18653/v1/2020.acl-main.152) |  | 0 | We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions. | Faheem Kirefu, Kenneth Heafield, Nikolay Bogoychev, Pinzhen Chen |  |
| 247 |  |  [Self-Attention with Cross-Lingual Position Representation](https://doi.org/10.18653/v1/2020.acl-main.153) |  | 0 | Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with... | Dacheng Tao, Liang Ding, Longyue Wang |  |
| 248 |  |  ["You Sound Just Like Your Father" Commercial Machine Translation Systems Include Stylistic Biases](https://doi.org/10.18653/v1/2020.acl-main.154) |  | 0 | The main goal of machine translation has been to convey the correct content. Stylistic considerations have been at best secondary. We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL, Google) make demographically diverse samples from five languages “sound” older and more male than the original. Our findings suggest that translation models reflect demographic bias in the training data. This opens up interesting new research avenues in machine translation to take stylistic... | Dirk Hovy, Federico Bianchi, Tommaso Fornaciari |  |
| 249 |  |  [MMPE: A Multi-Modal Interface for Post-Editing Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.155) |  | 0 | Current advances in machine translation (MT) increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and reduces errors. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional... | Antonio Krüger, Josef van Genabith, Kalliopi Meladaki, Mahsa Monshizadeh, Nico Herbig, Santanu Pal, Tim Düwel |  |
| 250 |  |  [A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages](https://doi.org/10.18653/v1/2020.acl-main.156) |  | 0 | We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia.... | Benoît Sagot, Laurent Romary, Pedro Javier Ortiz Suárez |  |
| 251 |  |  [Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter](https://doi.org/10.18653/v1/2020.acl-main.157) |  | 0 | We present a new challenging stance detection dataset, called Will-They-Won’t-They (WT–WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain. | Chryssi Giannitsarou, Costanza Conforti, Flavio Toxvaerd, Jakob Berndt, Mohammad Taher Pilehvar, Nigel Collier |  |
| 252 |  |  [A Systematic Assessment of Syntactic Generalization in Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.158) |  | 0 | While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on... | Ethan Wilcox, Jennifer Hu, Jon Gauthier, Peng Qian, Roger Levy |  |
| 253 |  |  [Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals](https://doi.org/10.18653/v1/2020.acl-main.159) |  | 0 | Can artificial neural networks learn to represent inflectional morphology and generalize to new words as human speakers do? Kirov and Cotterell (2018) argue that the answer is yes: modern Encoder-Decoder (ED) architectures learn human-like behavior when inflecting English verbs, such as extending the regular past tense form /-(e)d/ to novel words. However, their work does not address the criticism raised by Marcus et al. (1995): that neural models may learn to extend not the regular, but the most frequent class — and thus... | Adam Lopez, Kate McCurdy, Sharon Goldwater |  |
| 254 |  |  [Overestimation of Syntactic Representation in Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.160) |  | 0 | With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. Several testing methodologies have been developed to probe models’ syntactic representations. One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with... | Jordan Kodner, Nitish Gupta |  |
| 255 |  |  [Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation](https://doi.org/10.18653/v1/2020.acl-main.161) |  | 0 | Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling. While there is a vast theoretical literature on suspense, it is computationally not well understood. We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is. Both can be computed either directly over story representations or over their... | David Wilmot, Frank Keller |  |
| 256 |  |  [You Don't Have Time to Read This: An Exploration of Document Reading Time Prediction](https://doi.org/10.18653/v1/2020.acl-main.162) |  | 0 | Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time. However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word). We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics. We perform a novel... | Christopher Challis, E. Shannon Tass, Ilya Reznik, Jordan Hildebrandt, Kevin D. Seppi, Orion Weller, Quinn Snell |  |
| 257 |  |  [A Generative Model for Joint Natural Language Understanding and Generation](https://doi.org/10.18653/v1/2020.acl-main.163) |  | 0 | Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to... | BoHsiang Tseng, David Vandyke, Jianpeng Cheng, Yimai Fang |  |
| 258 |  |  [Automatic Detection of Generated Text is Easiest when Humans are Fooled](https://doi.org/10.18653/v1/2020.acl-main.164) |  | 0 | Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-_k_, nucleus sampling, and untruncated random sampling—and show that improvements in... | Chris CallisonBurch, Daniel Duckworth, Daphne Ippolito, Douglas Eck |  |
| 259 |  |  [Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing](https://doi.org/10.18653/v1/2020.acl-main.165) |  | 0 | Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion,... | Chen Liang, Chong Wang, Haoming Jiang, Tuo Zhao |  |
| 260 |  |  [Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation](https://doi.org/10.18653/v1/2020.acl-main.166) |  | 0 | To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent “what to say” and “how to say”, and edges to represent natural transition between a message (the last utterance in a dialog context) and its... | Haifeng Wang, Hua Wu, Jun Xu, Ting Liu, Wanxiang Che, ZhengYu Niu |  |
| 261 |  |  [GPT-too: A Language-Model-First Approach for AMR-to-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.167) |  | 0 | Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10... | Manuel Mager, Md. Arafat Sultan, Radu Florian, Ramón Fernandez Astudillo, Salim Roukos, Tahira Naseem, YoungSuk Lee |  |
| 262 |  |  [Learning to Update Natural Language Comments Based on Code Changes](https://doi.org/10.18653/v1/2020.acl-main.168) |  | 0 | We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies. We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications. We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent... | Junyi Jessy Li, Milos Gligoric, Pengyu Nie, Raymond J. Mooney, Sheena Panthaplackel |  |
| 263 |  |  [Politeness Transfer: A Tag and Generate Approach](https://doi.org/10.18653/v1/2020.acl-main.169) |  | 0 | This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks,... | Alan W. Black, Aman Madaan, Amrith Setlur, Barnabás Póczos, Graham Neubig, Ruslan Salakhutdinov, Shrimai Prabhumoye, Tanmay Parekh, Yiming Yang |  |
| 264 |  |  [BPE-Dropout: Simple and Effective Subword Regularization](https://doi.org/10.18653/v1/2020.acl-main.170) |  | 0 | Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to... | Dmitrii Emelianenko, Elena Voita, Ivan Provilkov |  |
| 265 |  |  [Improving Non-autoregressive Neural Machine Translation with Monolingual Data](https://doi.org/10.18653/v1/2020.acl-main.171) |  | 0 | Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model. Under this framework, we leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting. On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of... | Jiawei Zhou, Phillip Keung |  |
| 266 |  |  [Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.172) |  | 0 | Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task. It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source. In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer. Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically... | Nazli Goharian, Ross W. Filice, Sajad Sotudeh Gharebagh |  |
| 267 |  |  [On Faithfulness and Factuality in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.173) |  | 0 | It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems... | Bernd Bohnet, Joshua Maynez, Ryan T. McDonald, Shashi Narayan |  |
| 268 |  |  [Screenplay Summarization Using Latent Narrative Structure](https://doi.org/10.18653/v1/2020.acl-main.174) |  | 0 | Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront. As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document. When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient. In this paper, we propose to explicitly incorporate the underlying structure of narratives into general... | Frank Keller, Lea Frermann, Mirella Lapata, Pinelopi Papalampidi |  |
| 269 |  |  [Unsupervised Opinion Summarization with Noising and Denoising](https://doi.org/10.18653/v1/2020.acl-main.175) |  | 0 | The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries. We create a... | Mirella Lapata, Reinald Kim Amplayo |  |
| 270 |  |  [A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer's Type](https://doi.org/10.18653/v1/2020.acl-main.176) |  | 0 | In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls. The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.... | Serguei Pakhomov, Trevor Cohen |  |
| 271 |  |  [Probing Linguistic Systematicity](https://doi.org/10.18653/v1/2020.acl-main.177) |  | 0 | Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which... | Emily Goodwin, Koustuv Sinha, Timothy J. O'Donnell |  |
| 272 |  |  [Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.178) |  | 0 | We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined... | Eric Horvitz, James W. Pennebaker, Maarten Sap, Noah A. Smith, Yejin Choi |  |
| 273 |  |  [Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment](https://doi.org/10.18653/v1/2020.acl-main.179) |  | 0 | A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with... | Forrest Davis, Marten van Schijndel |  |
| 274 |  |  [Speakers enhance contextually confusable words](https://doi.org/10.18653/v1/2020.acl-main.180) |  | 0 | Recent work has found evidence that natural languages are shaped by pressures for efficient communication — e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (Piantadosi et al. 2011). Research on the degree to which speech and language are shaped by pressures for effective communication — robustness in the face of noise and uncertainty — has been more equivocal. We develop a measure of contextual confusability during word recognition based on psychoacoustic data. Applying this... | Eric Bakovic, Eric Meinhardt, Leon Bergen |  |
| 275 |  |  [What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks](https://doi.org/10.18653/v1/2020.acl-main.181) |  | 0 | We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun. We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension. The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce. We... | Gregory Scontras, Richard Futrell, William Dyer |  |
| 276 |  |  ["None of the Above": Measure Uncertainty in Dialog Response Retrieval](https://doi.org/10.18653/v1/2020.acl-main.182) |  | 0 | This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus. We show that instead of retraining models for this specific purpose, we can capture the original retrieval model’s underlying confidence concerning the best prediction using trivial additional computation. | Maxine Eskénazi, Shikib Mehri, Tiancheng Zhao, Yulan Feng |  |
| 277 |  |  [Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills](https://doi.org/10.18653/v1/2020.acl-main.183) |  | 0 | Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent. Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them. But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow. In this work, we investigate several ways to combine models trained towards isolated... | Eric Michael Smith, Jason Weston, Kurt Shuster, Mary Williamson, YLan Boureau |  |
| 278 |  |  [Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs](https://doi.org/10.18653/v1/2020.acl-main.184) |  | 0 | Human conversations naturally evolve around related concepts and hop to distant concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the... | Chenyan Xiong, Houyu Zhang, Zhenghao Liu, Zhiyuan Liu |  |
| 279 |  |  [Negative Training for Neural Dialogue Response Generation](https://doi.org/10.18653/v1/2020.acl-main.185) |  | 0 | Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses. In this work, we propose a framework named “Negative Training” to minimize such behaviors. Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training... | James R. Glass, Tianxing He |  |
| 280 |  |  [Recursive Template-based Frame Generation for Task Oriented Dialog](https://doi.org/10.18653/v1/2020.acl-main.186) |  | 0 | The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST). This information is typically represented as a semantic frame that captures the intent and slot-labels provided by the user. We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many... | Balakrishnan Narayanaswamy, Rashmi Gangadharaiah |  |
| 281 |  |  [Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback](https://doi.org/10.18653/v1/2020.acl-main.187) |  | 0 | We study the task of semantic parse correction with natural language feedback. Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form. In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance. We focus on natural language... | Ahmed Elgohary, Ahmed Hassan Awadallah, Saghar Hosseini |  |
| 282 |  |  [Calibrating Structured Output Predictors for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-main.188) |  | 0 | We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration... | Abhyuday Jagannatha, Hong Yu |  |
| 283 |  |  [Active Imitation Learning with Noisy Guidance](https://doi.org/10.18653/v1/2020.acl-main.189) |  | 0 | Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies. Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical. To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic... | Amr Sharaf, Hal Daumé III, Kianté Brantley |  |
| 284 |  |  [ExpBERT: Representation Engineering with Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.190) |  | 0 | Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x... | Pang Wei Koh, Percy Liang, Shikhar Murty |  |
| 285 |  |  [GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples](https://doi.org/10.18653/v1/2020.acl-main.191) |  | 0 | Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi-... | Danilo Croce, Giuseppe Castellucci, Roberto Basili |  |
| 286 |  |  [Generalizing Natural Language Analysis through Span-relation Representations](https://doi.org/10.18653/v1/2020.acl-main.192) |  | 0 | Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks... | Graham Neubig, Jun Araki, Wei Xu, Zhengbao Jiang |  |
| 287 |  |  [Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.193) |  | 0 | Sequence labeling is a fundamental task for a range of natural language processing problems. When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly. In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible. In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd... | Bill Yuchen Lin, He Jiang, Liyuan Liu, Ouyu Lan, Xiang Ren, Xiao Huang |  |
| 288 |  |  [MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2020.acl-main.194) |  | 0 | This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned... | Diyi Yang, Jiaao Chen, Zichao Yang |  |
| 289 |  |  [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://doi.org/10.18653/v1/2020.acl-main.195) |  | 0 | Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.... | Denny Zhou, Hongkun Yu, Renjie Liu, Xiaodan Song, Yiming Yang, Zhiqing Sun |  |
| 290 |  |  [On Importance Sampling-Based Evaluation of Latent Language Models](https://doi.org/10.18653/v1/2020.acl-main.196) |  | 0 | Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution... | Matt Gardner, Robert L. Logan IV, Sameer Singh |  |
| 291 |  |  [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization](https://doi.org/10.18653/v1/2020.acl-main.197) |  | 0 | Transfer learning has fundamentally changed the landscape of natural language processing (NLP). Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data. To address such an issue in a principled manner, we... | Haoming Jiang, Jianfeng Gao, Pengcheng He, Tuo Zhao, Weizhu Chen, Xiaodong Liu |  |
| 292 |  |  [Stolen Probability: A Structural Weakness of Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.198) |  | 0 | Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space. The dot-product distance metric forms part of the inductive bias of NNLMs. Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning... | David Demeter, Doug Downey, Gregory Kimmel |  |
| 293 |  |  [Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer](https://doi.org/10.18653/v1/2020.acl-main.199) |  | 0 | Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications. Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks. However, there has been no attempt to exploit GNN to create taxonomies. In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task. Our main contribution is to learn the latent features of taxonomy construction... | Alfio Gliozzo, Chao Shang, Md. Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Sarthak Dash |  |
| 294 |  |  [To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks](https://doi.org/10.18653/v1/2020.acl-main.200) |  | 0 | Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks. This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%. Our findings indicate that... | Hao Ma, Madian Khabsa, Sinong Wang |  |
| 295 |  |  [Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](https://doi.org/10.18653/v1/2020.acl-main.201) |  | 0 | Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple... | Jordan L. BoydGraber, Michael J. Paul, Mozhi Zhang, Yoshinari Fujinuma |  |
| 296 |  |  [XtremeDistil: Multi-stage Distillation for Massive Multilingual Models](https://doi.org/10.18653/v1/2020.acl-main.202) |  | 0 | Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks. However, the huge size of these models could be a deterrent to using them in practice. Some recent works use knowledge distillation to compress these huge models into shallow ones. In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER). In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal... | Ahmed Hassan Awadallah, Subhabrata Mukherjee |  |
| 297 |  |  [A Girl Has A Name: Detecting Authorship Obfuscation](https://doi.org/10.18653/v1/2020.acl-main.203) |  | 0 | Authorship attribution aims to identify the author of a text based on the stylometric analysis. Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style. In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model. An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the... | Asad Mahmood, Padmini Srinivasan, Zubair Shafiq |  |
| 298 |  |  [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://doi.org/10.18653/v1/2020.acl-main.204) |  | 0 | Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further... | Jaejun Lee, Ji Xin, Jimmy Lin, Raphael Tang, Yaoliang Yu |  |
| 299 |  |  [Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks](https://doi.org/10.18653/v1/2020.acl-main.205) |  | 0 | In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy. Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model. We first define the task as a sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic task of bottom-up-classification. Then, from external... | Arturo Oncevay, Gina Bustamante, Kervy Rivas Rojas, Marco Antonio Sobrevilla Cabezudo |  |
| 300 |  |  [Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions](https://doi.org/10.18653/v1/2020.acl-main.206) |  | 0 | We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts. Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language... | Andrew Caines, Hannah Craighead, Helen Yannakoudakis, Paula Buttery |  |
| 301 |  |  [SPECTER: Document-level Representation Learning using Citation-informed Transformers](https://doi.org/10.18653/v1/2020.acl-main.207) |  | 0 | Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity. We propose... | Arman Cohan, Daniel S. Weld, Doug Downey, Iz Beltagy, Sergey Feldman |  |
| 302 |  |  [Semantic Scaffolds for Pseudocode-to-Code Generation](https://doi.org/10.18653/v1/2020.acl-main.208) |  | 0 | We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program. By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques. We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode... | Dan Klein, Mitchell Stern, Ruiqi Zhong |  |
| 303 |  |  [Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction](https://doi.org/10.18653/v1/2020.acl-main.209) |  | 0 | Open Information Extraction systems extract (“subject text”, “relation text”, “object text”) triples from raw text. Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations. In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge. For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (“subject text”, “relation... | Kiril Gashteovski, Rainer Gemulla, Samuel Broscheit, Yanjie Wang |  |
| 304 |  |  [INFOTABS: Inference on Tables as Semi-structured Data](https://doi.org/10.18653/v1/2020.acl-main.210) |  | 0 | In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes. Our analysis shows that the... | Maitrey Mehta, Pegah Nokhiz, Vivek Gupta, Vivek Srikumar |  |
| 305 |  |  [Interactive Machine Comprehension with Information Seeking Agents](https://doi.org/10.18653/v1/2020.acl-main.211) |  | 0 | Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA). We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed. In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments. Specifically, we “occlude” the majority of a... | Adam Trischler, Chris Pal, Jie Fu, MarcAlexandre Côté, Xingdi Yuan, Yi Tay |  |
| 306 |  |  [Syntactic Data Augmentation Increases Robustness to Inference Heuristics](https://doi.org/10.18653/v1/2020.acl-main.212) |  | 0 | Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets... | Dipanjan Das, Emily Pitler, Junghyun Min, R. Thomas McCoy, Tal Linzen |  |
| 307 |  |  [Improved Speech Representations with Multi-Target Autoregressive Predictive Coding](https://doi.org/10.18653/v1/2020.acl-main.213) |  | 0 | Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech. One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames. The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks. In this paper we extend this... | James R. Glass, YuAn Chung |  |
| 308 |  |  [Integrating Multimodal Information in Large Pretrained Transformers](https://doi.org/10.18653/v1/2020.acl-main.214) |  | 0 | Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face... | AmirAli Bagher Zadeh, Chengfeng Mao, LouisPhilippe Morency, Md. Kamrul Hasan, Mohammed E. Hoque, Sangwu Lee, Wasifur Rahman |  |
| 309 |  |  [MultiQT: Multimodal learning for real-time question tracking in speech](https://doi.org/10.18653/v1/2020.acl-main.215) |  | 0 | We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers. We propose a novel multimodal approach to real-time sequence labeling in speech. Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition. Our... | Jakob D. Havtorn, Jan Latko, Joakim Edin, Lars Maaløe, Lasse Borgholt, Lorenzo Belgrano, Nicolai Jacobsen, Regitze Sdun, Zeljko Agic |  |
| 310 |  |  [Multimodal and Multiresolution Speech Recognition with Transformers](https://doi.org/10.18653/v1/2020.acl-main.216) |  | 0 | This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and... | Aparna Khare, Georgios Paraskevopoulos, Shiva Sundaram, Srinivas Parthasarathy |  |
| 311 |  |  [Phone Features Improve Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.217) |  | 0 | End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation. Their performance is often assumed to be superior, though in many conditions this is not yet the case. We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines. Further, we introduce two... | Alan W. Black, Elizabeth Salesky |  |
| 312 |  |  [Grounding Conversations with Improvised Dialogues](https://doi.org/10.18653/v1/2020.acl-main.218) |  | 0 | Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We... | Hyundong Cho, Jonathan May |  |
| 313 |  |  [Image-Chat: Engaging Grounded Conversations](https://doi.org/10.18653/v1/2020.acl-main.219) |  | 0 | To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners. Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014). In this work we study large-scale architectures and datasets for this goal. We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components. To... | Antoine Bordes, Jason Weston, Kurt Shuster, Samuel Humeau |  |
| 314 |  |  [Learning an Unreferenced Metric for Online Dialogue Evaluation](https://doi.org/10.18653/v1/2020.acl-main.220) |  | 0 | Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue. There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation. Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of... | Jasmine Wang, Joelle Pineau, Koustuv Sinha, Prasanna Parthasarathi, Ryan Lowe, William L. Hamilton |  |
| 315 |  |  [Neural Generation of Dialogue Response Timings](https://doi.org/10.18653/v1/2020.acl-main.221) |  | 0 | The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue. We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn. The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS). We evaluate our models using offline experiments as well as human listening tests. We show that human listeners consider certain response... | Matthew Roddy, Naomi Harte |  |
| 316 |  |  [The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents](https://doi.org/10.18653/v1/2020.acl-main.222) |  | 0 | We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images. By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting. We show that such... | Da Ju, Emily Dinan, Jason Weston, Kurt Shuster, Stephen Roller, YLan Boureau |  |
| 317 |  |  [Automatic Poetry Generation from Prosaic Text](https://doi.org/10.18653/v1/2020.acl-main.223) |  | 0 | In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is... | Tim Van de Cruys |  |
| 318 |  |  [Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.224) |  | 0 | Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose... | Chao Zhao, Marilyn A. Walker, Snigdha Chaturvedi |  |
| 319 |  |  [Enabling Language Models to Fill in the Blanks](https://doi.org/10.18653/v1/2020.acl-main.225) |  | 0 | We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document. While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling—a special case of infilling where text is predicted at the end of a document. In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling. To this end, we train (or fine tune) off-the-shelf LMs on sequences... | Chris Donahue, Mina Lee, Percy Liang |  |
| 320 |  |  [INSET: Sentence Infilling with INter-SEntential Transformer](https://doi.org/10.18653/v1/2020.acl-main.226) |  | 0 | Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion. This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context. Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation. In this paper, we propose a framework to decouple... | Oussama Elachqar, Yichen Huang, Yizhe Zhang, Yu Cheng |  |
| 321 |  |  [Improving Adversarial Text Generation by Modeling the Distant Future](https://doi.org/10.18653/v1/2020.acl-main.227) |  | 0 | Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation. Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply. We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues. Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist... | Changyou Chen, Dinghan Shen, Guoyin Wang, Lawrence Carin, Ruiyi Zhang, Wenlin Wang, Zhe Gan, Zheng Wen |  |
| 322 |  |  [Simple and Effective Retrieve-Edit-Rerank Text Generation](https://doi.org/10.18653/v1/2020.acl-main.228) |  | 0 | Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output. We propose to extend this framework with a simple and effective post-generation ranking approach. Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output. We use a standard editing model with simple task-specific re-ranking approaches, and we show... | Luke Zettlemoyer, Marjan Ghazvininejad, Nabil Hossain |  |
| 323 |  |  [BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps](https://doi.org/10.18653/v1/2020.acl-main.229) |  | 0 | Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN). In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones. We show that existing state-of-the-art agents do not generalize well. To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially. A special design memory buffer is used by... | Eugene Ie, Fei Sha, Hexiang Hu, Jiacheng Chen, Vihan Jain, Wang Zhu, Zhiwei Deng |  |
| 324 |  |  [Cross-media Structured Common Space for Multimedia Event Extraction](https://doi.org/10.18653/v1/2020.acl-main.230) |  | 0 | We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly... | Alireza Zareian, Di Lu, Heng Ji, Manling Li, Qi Zeng, ShihFu Chang, Spencer Whitehead |  |
| 325 |  |  [Learning to Segment Actions from Observation and Narration](https://doi.org/10.18653/v1/2020.acl-main.231) |  | 0 | We apply a generative segmental model of task structure, guided by narration, to action segmentation in video. We focus on unsupervised and weakly-supervised settings where no action labels are known during training. Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos. Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality. | Aida Nematzadeh, Chris Dyer, Daniel Fried, JeanBaptiste Alayrac, Phil Blunsom, Stephen Clark |  |
| 326 |  |  [Learning to execute instructions in a Minecraft dialogue](https://doi.org/10.18653/v1/2020.acl-main.232) |  | 0 | The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment. We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B’s past actions as well as B’s perspective leads to a significant improvement in performance on this challenging language understanding problem. | Anjali NarayanChen, Julia Hockenmaier, Prashant Jayannavar |  |
| 327 |  |  [MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning](https://doi.org/10.18653/v1/2020.acl-main.233) |  | 0 | Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph. Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture. The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better... | Dong Yu, Jie Lei, Liwei Wang, Mohit Bansal, Tamara L. Berg, Yelong Shen |  |
| 328 |  |  [What is Learned in Visually Grounded Neural Syntax Acquisition](https://doi.org/10.18653/v1/2020.acl-main.234) |  | 0 | Visual features are a promising signal for learning bootstrap textual models. However, blackbox learning models make it difficult to isolate the specific contribution of visual components. In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal. By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance. Contrary to what the model might be... | Alexander M. Rush, Hadar AverbuchElor, Noriyuki Kojima, Yoav Artzi |  |
| 329 |  |  [A Batch Normalized Inference Network Keeps the KL Vanishing Away](https://doi.org/10.18653/v1/2020.acl-main.235) |  | 0 | Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”. Previous approaches consider the Kullback–Leibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is... | Dapeng Wu, Qile Zhu, Wei Bi, Xiaojiang Liu, Xiaolin Li, Xiyao Ma |  |
| 330 |  |  [Contextual Embeddings: When Are They Worth It?](https://doi.org/10.18653/v1/2020.acl-main.236) |  | 0 | We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline—random word embeddings—focusing on the impact of the training set size and the linguistic properties of the task. Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks. Furthermore, we identify... | Avner May, Christopher Ré, Jian Zhang, Simran Arora |  |
| 331 |  |  [Interactive Classification by Asking Informative Questions](https://doi.org/10.18653/v1/2020.acl-main.237) |  | 0 | We study the potential for interaction in natural language classification. We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions. At each turn, our system decides between asking the most informative question or making the final classification pre-diction. The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on... | Howard Chen, Lili Yu, Sida I. Wang, Tao Lei, Yoav Artzi |  |
| 332 |  |  [Knowledge Graph Embedding Compression](https://doi.org/10.18653/v1/2020.acl-main.238) |  | 0 | Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these... | Mrinmaya Sachan |  |
| 333 |  |  [Low Resource Sequence Tagging using Sentence Reconstruction](https://doi.org/10.18653/v1/2020.acl-main.239) |  | 0 | This work revisits the task of training sequence tagging models with limited resources using transfer learning. We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings. Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines. We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method... | Raja Giryes, Sriram Chaudhury, Tal Perl |  |
| 334 |  |  [Masked Language Model Scoring](https://doi.org/10.18653/v1/2020.acl-main.240) |  | 0 | Pretrained masked language models (MLMs) require finetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with... | Davis Liang, Julian Salazar, Katrin Kirchhoff, Toan Q. Nguyen |  |
| 335 |  |  [Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2020.acl-main.241) |  | 0 | Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for... | Bowen Zhou, Guangtao Wang, Jing Huang, Xiaodong He, Yun Tang |  |
| 336 |  |  [Posterior Calibrated Training on Sentence Classification Tasks](https://doi.org/10.18653/v1/2020.acl-main.242) |  | 0 | Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability. In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone. When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little... | Dongyeop Kang, Hua Cheng, Lucas Mentch, Taehee Jung, Thomas Schaaf |  |
| 337 |  |  [Posterior Control of Blackbox Generation](https://doi.org/10.18653/v1/2020.acl-main.243) |  | 0 | Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model... | Alexander M. Rush, Xiang Lisa Li |  |
| 338 |  |  [Pretrained Transformers Improve Out-of-Distribution Robustness](https://doi.org/10.18653/v1/2020.acl-main.244) |  | 0 | Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also... | Adam Dziedzic, Dan Hendrycks, Dawn Song, Eric Wallace, Rishabh Krishnan, Xiaoyuan Liu |  |
| 339 |  |  [Robust Encodings: A Framework for Combating Adversarial Typos](https://doi.org/10.18653/v1/2020.acl-main.245) |  | 0 | Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs. Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT. In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.... | Aditi Raghunathan, Erik Jones, Percy Liang, Robin Jia |  |
| 340 |  |  [Showing Your Work Doesn't Always Work](https://doi.org/10.18653/v1/2020.acl-main.246) |  | 0 | In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks. One exemplar publication, titled “Show Your Work: Improved Reporting of Experimental Results” (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats... | Jaejun Lee, Ji Xin, Jimmy Lin, Raphael Tang, Xinyu Liu, Yaoliang Yu |  |
| 341 |  |  [Span Selection Pre-training for Question Answering](https://doi.org/10.18653/v1/2020.acl-main.247) |  | 0 | BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA). BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction. In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. Span Selection PreTraining (SSPT) poses cloze-like training instances,... | Alfio Gliozzo, Anthony Ferritto, Avirup Sil, Dinesh Garg, G. P. Shrivatsa Bhargav, Lin Pan, Michael R. Glass, Rishav Chakravarti |  |
| 342 |  |  [Topological Sort for Sentence Ordering](https://doi.org/10.18653/v1/2020.acl-main.248) |  | 0 | Sentence ordering is the task of arranging the sentences of a given text in the correct order. Recent work using deep neural networks for this task has framed it as a sequence prediction problem. In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it. Additionally, we propose a human evaluation for this task. The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in... | Alan W. Black, Ruslan Salakhutdinov, Shrimai Prabhumoye |  |
| 343 |  |  [Weight Poisoning Attacks on Pretrained Models](https://doi.org/10.18653/v1/2020.acl-main.249) |  | 0 | Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model... | Graham Neubig, Keita Kurita, Paul Michel |  |
| 344 |  |  [schuBERT: Optimizing Elements of BERT](https://doi.org/10.18653/v1/2020.acl-main.250) |  | 0 | Transformers have gradually become a key component for many state-of-the-art natural language representation models. A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0. This model however is computationally prohibitive and has a huge number of parameters. In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model. We focus on reducing the number of parameters yet our methods can be... | Ashish Khetan, Zohar S. Karnin |  |
| 345 |  |  [ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.251) |  | 0 | We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model. In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy. This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model. Our approach, which we call ENGINE (ENerGy-based Inference... | Kevin Gimpel, Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman |  |
| 346 |  |  [Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.252) |  | 0 | Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision... | Aditya Siddhant, Ankur Bapna, Mia Xu Chen, Naveen Arivazhagan, Orhan Firat, Sneha Reddy Kudugunta, Yonghui Wu, Yuan Cao |  |
| 347 |  |  [On The Evaluation of Machine Translation SystemsTrained With Back-Translation](https://doi.org/10.18653/v1/2020.acl-main.253) |  | 0 | Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality... | Marc'Aurelio Ranzato, Michael Auli, Myle Ott, Sergey Edunov |  |
| 348 |  |  [Simultaneous Translation Policies: From Fixed to Adaptive](https://doi.org/10.18653/v1/2020.acl-main.254) |  | 0 | Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -> English and German -> English show that our... | Baigong Zheng, Hairong Liu, Kaibo Liu, Liang Huang, Mingbo Ma, Renjie Zheng |  |
| 349 |  |  [Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information](https://doi.org/10.18653/v1/2020.acl-main.255) |  | 0 | Neural architectures are the current state of the art in Word Sense Disambiguation (WSD). However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB). We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets... | Michele Bevilacqua, Roberto Navigli |  |
| 350 |  |  [Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs](https://doi.org/10.18653/v1/2020.acl-main.256) |  | 0 | Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus. We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics. We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem. Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese... | HongYou Chen, Shoude Lin, SzHan Yu |  |
| 351 |  |  [Multidirectional Associative Optimization of Function-Specific Word Representations](https://doi.org/10.18653/v1/2020.acl-main.257) |  | 0 | We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures. Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together. The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure. We show the robustness and versatility of the proposed framework... | Anna Korhonen, Daniela Gerz, Ivan Vulic, Marek Rei, Roi Reichart |  |
| 352 |  |  [Predicting Degrees of Technicality in Automatic Terminology Extraction](https://doi.org/10.18653/v1/2020.acl-main.258) |  | 0 | While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied. We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction. When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.... | Anna Hätty, Dominik Schlechtweg, Michael Dorna, Sabine Schulte im Walde |  |
| 353 |  |  [Verbal Multiword Expressions for Identification of Metaphor](https://doi.org/10.18653/v1/2020.acl-main.259) |  | 0 | Metaphor is a linguistic device in which a concept is expressed by mentioning another. Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics. Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models. This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby... | Le An Ha, Marek Rei, Omid Rohanian, Shiva Taslimipoor |  |
| 354 |  |  [Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer](https://doi.org/10.18653/v1/2020.acl-main.260) |  | 0 | Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias... | Ahmed Hassan Awadallah, Jieyu Zhao, KaiWei Chang, Saghar Hosseini, Subhabrata Mukherjee |  |
| 355 |  |  [Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?](https://doi.org/10.18653/v1/2020.acl-main.261) |  | 0 | As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research. We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis. We focus in particular on the role... | Jey Han Lau, Kobi Leins, Timothy Baldwin |  |
| 356 |  |  [Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds](https://doi.org/10.18653/v1/2020.acl-main.262) |  | 0 | Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity). However, manually annotating a large dataset with a protected attribute is slow and expensive. Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias? While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the... | Kawin Ethayarajh |  |
| 357 |  |  [It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations](https://doi.org/10.18653/v1/2020.acl-main.263) |  | 0 | Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves... | MinYen Kan, Richard Socher, Samson Tan, Shafiq R. Joty |  |
| 358 |  |  [Mitigating Gender Bias Amplification in Distribution by Posterior Regularization](https://doi.org/10.18653/v1/2020.acl-main.264) |  | 0 | Advanced machine learning techniques have boosted the performance of natural language processing. Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it. However, their analysis is conducted only on models’ top predictions. In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over... | Jieyu Zhao, KaiWei Chang, Shengyu Jia, Tao Meng |  |
| 359 |  |  [Towards Understanding Gender Bias in Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.265) |  | 0 | Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction. While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems. In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems. We... | Andrew Gaut, Diba Mirza, Elizabeth M. Belding, Jieyu Zhao, Jing Qian, KaiWei Chang, Mai ElSherief, Shirlyn Tang, Tony Sun, William Yang Wang, Yuxin Huang |  |
| 360 |  |  [A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing](https://doi.org/10.18653/v1/2020.acl-main.266) |  | 0 | We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents. We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance. Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector... | Chris Dyer, Christopher N. Warren, Kartik Goyal, Max G'Sell, Taylor BergKirkpatrick |  |
| 361 |  |  [Attentive Pooling with Learnable Norms for Text Representation](https://doi.org/10.18653/v1/2020.acl-main.267) |  | 0 | Pooling is an important technique for learning text representations in many neural NLP models. In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features. However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones... | Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, Yongfeng Huang |  |
| 362 |  |  [Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks](https://doi.org/10.18653/v1/2020.acl-main.268) |  | 0 | Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to... | Chris Biemann, Fynn Schröder |  |
| 363 |  |  [How Does Selective Mechanism Improve Self-Attention Networks?](https://doi.org/10.18653/v1/2020.acl-main.269) |  | 0 | Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and... | Bing Qin, Longyue Wang, Ting Liu, Xing Wang, Xinwei Geng, Zhaopeng Tu |  |
| 364 |  |  [Improving Transformer Models by Reordering their Sublayers](https://doi.org/10.18653/v1/2020.acl-main.270) |  | 0 | Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new... | Noah A. Smith, Ofir Press, Omer Levy |  |
| 365 |  |  [Single Model Ensemble using Pseudo-Tags and Distinct Vectors](https://doi.org/10.18653/v1/2020.acl-main.271) |  | 0 | Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional... | Hideki Nakayama, Jun Suzuki, Ryosuke Kuwabara |  |
| 366 |  |  [Zero-shot Text Classification via Reinforced Self-training](https://doi.org/10.18653/v1/2020.acl-main.272) |  | 0 | Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity. In this situation, transferring from seen classes to unseen classes is extremely hard. To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data. Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets. We propose a... | Feng Wang, Huajun Chen, Jiaoyan Chen, Jingmin Chen, Jun Zhang, Suhang Zheng, Xiaoxiao Xu, Yuxia Geng, Zhiquan Ye |  |
| 367 |  |  [A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.273) |  | 0 | Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images. However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning. To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT. Specifically, we first represent the input sentence and image using a unified... | Chulun Zhou, Fandong Meng, Jie Zhou, Jiebo Luo, Jinsong Su, Yongjing Yin, Zhengyuan Yang |  |
| 368 |  |  [A Relaxed Matching Procedure for Unsupervised BLI](https://doi.org/10.18653/v1/2020.acl-main.274) |  | 0 | Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest. One of the crucial parts in methods for the BLI task is the matching procedure. Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings. Thus We propose a relaxed matching procedure to find a more precise matching between two languages. We also find that aligning source and target language embedding space bidirectionally will bring significant... | Hao Wu, Xu Zhao, Yong Zhang, Zihao Wang |  |
| 369 |  |  [Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.275) |  | 0 | This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means... | Gholamreza Haffari, Mohammad Norouzi, Xuanli He |  |
| 370 |  |  [Geometry-aware domain adaptation for unsupervised alignment of word embeddings](https://doi.org/10.18653/v1/2020.acl-main.276) |  | 0 | We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages. Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices. This viewpoint arises from the aim to align the second order information of the two language spaces. The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed... | Bamdev Mishra, Mayank Meghwanshi, Pratik Jawanpuria |  |
| 371 |  |  [Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.277) |  | 0 | Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a... | Jie Zhou, Peng Li, Qiu Ran, Yankai Lin |  |
| 372 |  |  [On the Inference Calibration of Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.278) |  | 0 | Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output. While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference. By carefully... | Shuming Shi, Shuo Wang, Yang Liu, Zhaopeng Tu |  |
| 373 |  |  [Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning](https://doi.org/10.18653/v1/2020.acl-main.279) |  | 0 | We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task. A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation. A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation. The preliminary experiment demonstrates the proposed SIGNAL model is... | Changlong Sun, Qiong Zhang, Xiaozhong Liu, Yangyang Kang, Yu Duan, Zhe Gao, Zhuoren Jiang |  |
| 374 |  |  [Distinguish Confusing Law Articles for Legal Judgment Prediction](https://doi.org/10.18653/v1/2020.acl-main.280) |  | 0 | Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public. In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged. To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems. In this paper, we present an end-to-end... | Junzhou Zhao, Li Pan, Long Chen, Nuo Xu, Pinghui Wang, Xiaoyan Wang |  |
| 375 |  |  [Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation](https://doi.org/10.18653/v1/2020.acl-main.281) |  | 0 | Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think. It is challenging to specify the level of education, experience, relevant skills per the company information and job description. To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions. To deal with this task, we devise a data-driven global Skill-Aware... | Jie Liu, Liting Liu, Wenxuan Shi, Wenzheng Zhang, Yalou Huang, Ziming Chi |  |
| 376 |  |  [HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding](https://doi.org/10.18653/v1/2020.acl-main.282) |  | 0 | The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code. ICD coding aims to assign proper ICD codes to a medical record. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence. In this paper, we propose a Hyperbolic and... | Jun Zhao, Kang Liu, Pengfei Cao, Shengping Liu, Weifeng Chong, Yubo Chen |  |
| 377 |  |  [Hyperbolic Capsule Networks for Multi-Label Classification](https://doi.org/10.18653/v1/2020.acl-main.283) |  | 0 | Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling). Such operations limit the performance. For instance, a multi-label document may contain several concepts. In this case, one vector can not sufficiently capture its salient and discriminative content. Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.... | Boli Chen, Lin Xiao, Liping Jing, Xin Huang |  |
| 378 |  |  [Improving Segmentation for Technical Support Problems](https://doi.org/10.18653/v1/2020.acl-main.284) |  | 0 | Technical support problems are often long and complex. They typically contain user descriptions of the problem, the setup, and steps for attempted resolution. Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces. These elements contain potentially crucial information for problem resolution. However, they cannot be correctly parsed by tools designed for natural language. In this paper, we address the problem of segmentation for technical... | Abhirut Gupta, Kushal Chauhan |  |
| 379 |  |  [MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs](https://doi.org/10.18653/v1/2020.acl-main.285) |  | 0 | The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we... | Chenyu Wang, Gan Luo, Jie Tang, Jifan Yu, Juanzi Li, Junyi Luo, Lei Hou, Qingyang Zhong, Tong Xiao, Wenzheng Feng, Yuquan Wang, Zhiyuan Liu |  |
| 380 |  |  [Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs](https://doi.org/10.18653/v1/2020.acl-main.286) |  | 0 | The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the... | Chao Lu, Haifeng Huang, Jun Chen, Quan Yuan, Xiaoya Dai |  |
| 381 |  |  [Analyzing the Persuasive Effect of Style in News Editorial Argumentation](https://doi.org/10.18653/v1/2020.acl-main.287) |  | 0 | News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies. Previous research has investigated such persuasive effects for argumentative content. In contrast, this paper studies how important the style of news editorials is to achieve persuasion. To this end, we first compare content- and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations. We find that conservative readers are resistant to NYTimes... | Benno Stein, Henning Wachsmuth, Khalid Al Khatib, Roxanne El Baff |  |
| 382 |  |  [ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction](https://doi.org/10.18653/v1/2020.acl-main.288) |  | 0 | In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis. It aims at extracting the potential pairs of emotions and their corresponding causes in a document. To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes. However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at... | Jianfei Yu, Rui Xia, Zixiang Ding |  |
| 383 |  |  [Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.289) |  | 0 | Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document. Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs. However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well. In this paper, we tackle emotion-cause... | Jiahao Zhao, Penghui Wei, Wenji Mao |  |
| 384 |  |  [Embarrassingly Simple Unsupervised Aspect Extraction](https://doi.org/10.18653/v1/2020.acl-main.290) |  | 0 | We present a simple but effective method for aspect identification in sentiment analysis. Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages. We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable. Previous work relied on syntactic features and complex neural models. We show that given the simplicity of... | Andreas van Cranenburgh, Stéphan Tulkens |  |
| 385 |  |  [Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge](https://doi.org/10.18653/v1/2020.acl-main.291) |  | 0 | Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets. In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge... | Bowen Zhang, Kuai Dai, Min Yang, Xiaofei Xu, Xutao Li, Yunming Ye |  |
| 386 |  |  [KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.292) |  | 0 | Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis. In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge. We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts. These concepts are... | Abhinaba Roy, Deepanway Ghosal, Devamanyu Hazarika, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 387 |  |  [Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.293) |  | 0 | The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification. Rather than considering the tasks separately, we build an end-to-end ABSA solution. Previous works in ABSA tasks did not fully leverage the importance of syntactical information. Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms. On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation... | MinhHieu Phan, Philip O. Ogunbona |  |
| 388 |  |  [Parallel Data Augmentation for Formality Style Transfer](https://doi.org/10.18653/v1/2020.acl-main.294) |  | 0 | The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data. In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems. Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset. | Tao Ge, Xu Sun, Yi Zhang |  |
| 389 |  |  [Relational Graph Attention Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.295) |  | 0 | Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews. Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words. However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections. In this paper, we address this problem by means of effective encoding of syntax information. Firstly, we define a unified aspect-oriented dependency... | Kai Wang, Rui Wang, Weizhou Shen, Xiaojun Quan, Yunyi Yang |  |
| 390 |  |  [SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction](https://doi.org/10.18653/v1/2020.acl-main.296) |  | 0 | Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA). The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. To this end, this paper... | He Zhao, Hui Xue, Longtao Huang, Quan Lu, Rong Zhang |  |
| 391 |  |  [Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks](https://doi.org/10.18653/v1/2020.acl-main.297) |  | 0 | Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”. Due to the scarcity of labeled data, ORL remains challenging for data-driven methods. In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations. We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels. In order to compensate for parser inaccuracy... | Bo Zhang, Min Zhang, Rui Wang, Yue Zhang, Zhenghua Li |  |
| 392 |  |  [Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization](https://doi.org/10.18653/v1/2020.acl-main.298) |  | 0 | State-of-the-art argument mining studies have advanced the techniques for predicting argument structures. However, the technology for capturing non-tree-structured arguments is still in its infancy. In this paper, we focus on non-tree argument mining with a neural model. We jointly predict proposition types and edges between propositions. Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can... | Gaku Morio, Hiroaki Ozaki, Kohsuke Yanai, Terufumi Morishita, Yuta Koreeda |  |
| 393 |  |  [A Span-based Linearization for Constituent Trees](https://doi.org/10.18653/v1/2020.acl-main.299) |  | 0 | We propose a novel linearization of a constituent tree, together with a new locally normalized model. For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them. Compared with global models, our model is fast and parallelizable. Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.... | Man Lan, Yang Wei, Yuanbin Wu |  |
| 394 |  |  [An Empirical Comparison of Unsupervised Constituency Parsing Methods](https://doi.org/10.18653/v1/2020.acl-main.300) |  | 0 | Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations. While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics. In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability... | Jiong Cai, Jun Li, Kewei Tu, Yifan Cao, Yong Jiang |  |
| 395 |  |  [Efficient Constituency Parsing by Pointing](https://doi.org/10.18653/v1/2020.acl-main.301) |  | 0 | We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks. Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span. Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference. The experiments on the standard English Penn Treebank parsing task show that... | Shafiq R. Joty, ThanhTung Nguyen, Xiaoli Li, XuanPhi Nguyen |  |
| 396 |  |  [Efficient Second-Order TreeCRF for Neural Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.302) |  | 0 | In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the... | Min Zhang, Yu Zhang, Zhenghua Li |  |
| 397 |  |  [Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs](https://doi.org/10.18653/v1/2020.acl-main.303) |  | 0 | Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks. Such tree-based networks can be provided with a constituency parse, a dependency parse, or both. We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task. We find that a constituency-based network generalizes more robustly than a... | Michael A. Lepori, R. Thomas McCoy, Tal Linzen |  |
| 398 |  |  [Structure-Level Knowledge Distillation For Multilingual Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.304) |  | 0 | Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages. Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages. However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations. In this paper, we propose to reduce the gap between monolingual models and... | Fei Huang, Kewei Tu, Nguyen Bach, Tao Wang, Xinyu Wang, Yong Jiang |  |
| 399 |  |  [Dynamic Online Conversation Recommendation](https://doi.org/10.18653/v1/2020.acl-main.305) |  | 0 | Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner. Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests. While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time. Concretely, we... | Jing Li, KamFai Wong, Lu Wang, Xingshan Zeng, Zhiming Mao |  |
| 400 |  |  [Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer](https://doi.org/10.18653/v1/2020.acl-main.306) |  | 0 | In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts. Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context. To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations. To alleviate the visual bias,... | Jianfei Yu, Jing Jiang, Li Yang, Rui Xia |  |
| 401 |  |  [Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization](https://doi.org/10.18653/v1/2020.acl-main.307) |  | 0 | Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements. The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction. In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding. The stock... | Kumiko TanakaIshii, Xin Du |  |
| 402 |  |  [What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context](https://doi.org/10.18653/v1/2020.acl-main.308) |  | 0 | Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction. The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically. Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content. This... | Ahmed Ali, Georgi Karadzhov, Haewoon Kwak, James R. Glass, Jisun An, Preslav Nakov, Ramy Baly, Yoan Dinkov |  |
| 403 |  |  [An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.309) |  | 0 | We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in \*The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using... | Hiroshi Noji, Hiroya Takamura |  |
| 404 |  |  [On the Robustness of Language Encoders against Grammatical Errors](https://doi.org/10.18653/v1/2020.acl-main.310) |  | 0 | We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors. Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data. We use this approach to facilitate debugging models on downstream applications. Results confirm that the performance of all tested models is affected but the degree of impact varies. To interpret model behaviors, we... | Fan Yin, KaiWei Chang, Quanyu Long, Tao Meng |  |
| 405 |  |  [Roles and Utilization of Attention Heads in Transformer-based Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.311) |  | 0 | Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of... | Jaeyoung Jo, SungHyon Myaeng |  |
| 406 |  |  [Understanding Attention for Text Classification](https://doi.org/10.18653/v1/2020.acl-main.312) |  | 0 | Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We... | Wei Lu, Xiaobing Sun |  |
| 407 |  |  [A Relational Memory-based Embedding Model for Triple Classification and Search Personalization](https://doi.org/10.18653/v1/2020.acl-main.313) |  | 0 | Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems. To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples. R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism. Thus R-MeN encodes new information from... | Dai Quoc Nguyen, Dinh Phung, Tuan Nguyen |  |
| 408 |  |  [Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods](https://doi.org/10.18653/v1/2020.acl-main.314) |  | 0 | It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data. In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem. In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones. Experiments on a variety of text generation datasets show that MC-Tailor... | Hao Zhou, Lei Li, Ning Miao, Yuxuan Song |  |
| 409 |  |  [Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention](https://doi.org/10.18653/v1/2020.acl-main.315) |  | 0 | Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese. Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models. Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to... | Bowen Yu, Mengge Xue, Tingwen Liu, Yanzeng Li |  |
| 410 |  |  [On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond](https://doi.org/10.18653/v1/2020.acl-main.316) |  | 0 | Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map... | Chen Wu, Prince Zizhuang Wang, William Yang Wang |  |
| 411 |  |  [SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions](https://doi.org/10.18653/v1/2020.acl-main.317) |  | 0 | State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences,... | Chengyue Gong, Mao Ye, Qiang Liu |  |
| 412 |  |  [A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2020.acl-main.318) |  | 0 | Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages. Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages. However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some... | Ming Zhou, Shuai Ma, Shujie Liu, Shuo Ren |  |
| 413 |  |  [A Reinforced Generation of Adversarial Examples for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.319) |  | 0 | Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. Our paradigm could expose pitfalls for a given... | Jiajun Chen, Jun Xie, Shujian Huang, Wei Zou, Xinyu Dai |  |
| 414 |  |  [A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.320) |  | 0 | The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance. The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance. In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.... | Ming Zhou, Shuai Ma, Shujie Liu, Shuo Ren, Yu Wu |  |
| 415 |  |  [A Simple and Effective Unified Encoder for Document-Level Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.321) |  | 0 | Most of the existing models for document-level machine translation adopt dual-encoder structures. The representation of the source sentences and the document-level contexts are modeled with two separate encoders. Although these models can make use of the document-level contexts, they do not fully model the interaction between the contexts and the source sentences, and can not directly adapt to the recent pre-training models (e.g., BERT) which encodes multiple sentences with a single encoder. In this work, we propose a simple... | Dongdong Zhang, Ming Zhou, Shuming Ma |  |
| 416 |  |  [Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.322) |  | 0 | In encoder-decoder neural models, multiple encoders are in general used to represent the contextual information in addition to the individual sentence. In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT). Surprisingly, we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. This makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust... | Bei Li, Changliang Li, Hui Liu, Jingbo Zhu, Tong Xiao, Tongran Liu, Yufan Jiang, Ziyang Wang |  |
| 417 |  |  [Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change](https://doi.org/10.18653/v1/2020.acl-main.323) |  | 0 | The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient... | Deyi Xiong, Hongfei Xu, Josef van Genabith, Qiuhui Liu |  |
| 418 |  |  [Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.324) |  | 0 | Unsupervised neural machine translation (UNMT) has recently achieved remarkable results for several language pairs. However, it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. That is, research on multilingual UNMT has been limited. In this paper, we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder, making use of multilingual data to improve UNMT for all language... | Eiichiro Sumita, Haipeng Sun, Kehai Chen, Masao Utiyama, Rui Wang, Tiejun Zhao |  |
| 419 |  |  [Lexically Constrained Neural Machine Translation with Levenshtein Transformer](https://doi.org/10.18653/v1/2020.acl-main.325) |  | 0 | This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation. Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads. Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed. Our method... | Liling Tan, Raymond Hendy Susanto, Shamil Chollampatt |  |
| 420 |  |  [On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.326) |  | 0 | The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum... | Chaojun Wang, Rico Sennrich |  |
| 421 |  |  [Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model](https://doi.org/10.18653/v1/2020.acl-main.327) |  | 0 | We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method... | Katsuhito Sudoh, Kosuke Takahashi, Satoshi Nakamura |  |
| 422 |  |  [ChartDialogs: Plotting from Natural Language Instructions](https://doi.org/10.18653/v1/2020.acl-main.328) |  | 0 | This paper presents the problem of conversational plotting agents that carry out plotting actions from natural language instructions. To facilitate the development of such agents, we introduce ChartDialogs, a new multi-turn dialog dataset, covering a popular plotting library, matplotlib. The dataset contains over 15,000 dialog turns from 3,200 dialogs covering the majority of matplotlib plot types. Extensive experiments show the best-performing method achieving 61% plotting accuracy, demonstrating that the dataset presents a... | Ndapa Nakashole, Yutong Shao |  |
| 423 |  |  [GLUECoS: An Evaluation Benchmark for Code-Switched NLP](https://doi.org/10.18653/v1/2020.acl-main.329) |  | 0 | Code-switching is the use of more than one language in the same conversation or utterance. Recently, multilingual contextual embedding models, trained on multiple monolingual corpora, have shown promising results on cross-lingual and multilingual tasks. We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in English-Hindi and English-Spanish. Specifically, our evaluation benchmark includes Language Identification from text, POS tagging, Named Entity Recognition, Sentiment... | Anirudh Srinivasan, Monojit Choudhury, Sandipan Dandapat, Simran Khanuja, Sunayana Sitaram |  |
| 424 |  |  [MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization](https://doi.org/10.18653/v1/2020.acl-main.330) |  | 0 | Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. Based on such rich information, MATINF is... | Canwen Xu, Chenliang Li, Hongtao Wu, Jiaxin Pei, Yiyu Liu |  |
| 425 |  |  [MIND: A Large-scale Dataset for News Recommendation](https://doi.org/10.18653/v1/2020.acl-main.331) |  | 0 | News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named MIND for news recommendation. Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual... | Chuhan Wu, Danyang Liu, Fangzhao Wu, Jianfeng Gao, Jianxun Lian, JiunHung Chen, Ming Zhou, Tao Qi, Winnie Wu, Xing Xie, Ying Qiao |  |
| 426 |  |  [That is a Known Lie: Detecting Previously Fact-Checked Claims](https://doi.org/10.18653/v1/2020.acl-main.332) |  | 0 | The recent proliferation of ”fake news” has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite... | Giovanni Da San Martino, Nikolay Babulkov, Preslav Nakov, Shaden Shaar |  |
| 427 |  |  [Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation](https://doi.org/10.18653/v1/2020.acl-main.333) |  | 0 | Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram... | Bo Pang, Erik Nijkamp, Kewei Tu, Linqi Zhou, Wenjuan Han, Yixian Liu |  |
| 428 |  |  [BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection](https://doi.org/10.18653/v1/2020.acl-main.334) |  | 0 | The hypernymy detection task has been addressed under various frameworks. Previously, the design of unsupervised hypernymy scores has been extensively studied. In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”. In this work, we revisit supervised distributional models for hypernymy detection. Rather than taking embeddings of two terms as classification inputs, we introduce a representation... | Chengyu Wang, Xiaofeng He |  |
| 429 |  |  [Biomedical Entity Representations with Synonym Marginalization](https://doi.org/10.18653/v1/2020.acl-main.335) |  | 0 | Biomedical named entities often play important roles in many biomedical text mining tools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.... | Hwisang Jeon, Jaewoo Kang, Jinhyuk Lee, Mujeen Sung |  |
| 430 |  |  [Hypernymy Detection for Low-Resource Languages via Meta Learning](https://doi.org/10.18653/v1/2020.acl-main.336) |  | 0 | Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks. Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios. This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages. We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.... | Changlong Yu, Haisong Zhang, Jialong Han, Wilfred Ng |  |
| 431 |  |  [Investigating Word-Class Distributions in Word Vector Spaces](https://doi.org/10.18653/v1/2020.acl-main.337) |  | 0 | This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space. To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model. Specifically, we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that... | Anna Korhonen, Ryohei Sasano |  |
| 432 |  |  [Aspect Sentiment Classification with Document-level Sentiment Preference Modeling](https://doi.org/10.18653/v1/2020.acl-main.338) |  | 0 | In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC. In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment... | Changlong Sun, Guodong Zhou, Jingjing Wang, Luo Si, Min Zhang, Shoushan Li, Xiao Chen |  |
| 433 |  |  [Don't Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction](https://doi.org/10.18653/v1/2020.acl-main.339) |  | 0 | The current aspect extraction methods suffer from boundary errors. In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth. However, they hurt the performance severely. In this paper, we propose to utilize a pointer network for repositioning the boundaries. Recycling mechanism is used, which enables the training data to be collected without manual intervention. We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant. Experimental... | Bowei Zou, Jianmin Yao, Meng Cheng, Yu Hong, Zhenkai Wei |  |
| 434 |  |  [Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.340) |  | 0 | Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification. Most existing studies focused on one of these subtasks only. Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework. However, the interactive relations among three subtasks are still under-exploited. We argue that such relations encode collaborative signals between different subtasks. For example, when the... | Tieyun Qian, Zhuang Chen |  |
| 435 |  |  [SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics](https://doi.org/10.18653/v1/2020.acl-main.341) |  | 0 | We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related... | Da Yin, KaiWei Chang, Tao Meng |  |
| 436 |  |  [Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.342) |  | 0 | Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text. Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation. Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction. The proposed model incrementally generates the directed graph with labeled edges based on a sequence of... | Chaofa Yuan, Chuang Fan, Jiachen Du, Lin Gui, Min Yang, Ruifeng Xu |  |
| 437 |  |  [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality](https://doi.org/10.18653/v1/2020.acl-main.343) |  | 0 | Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows... | Fanyang Meng, Hua Xu, Jiele Wu, Jiyun Zou, Kaicheng Yang, Wenmeng Yu, Yilin Zhu, Yixiao Ma |  |
| 438 |  |  [Curriculum Pre-training for End-to-End Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.344) |  | 0 | End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously. To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features. However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered. Inspired by this, we propose a curriculum pre-training method that includes an elementary course for... | Chengyi Wang, Ming Zhou, Shujie Liu, Yu Wu, Zhenglu Yang |  |
| 439 |  |  [How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems](https://doi.org/10.18653/v1/2020.acl-main.345) |  | 0 | In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system. We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents. We examine the effects of accent on the internal representation using three main probing techniques: a)... | Archiki Prasad, Preethi Jyothi |  |
| 440 |  |  [Improving Disfluency Detection by Self-Training a Self-Attentive Model](https://doi.org/10.18653/v1/2020.acl-main.346) |  | 0 | Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts. Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant. However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive... | Mark Johnson, Paria Jamshid Lou |  |
| 441 |  |  [Learning Spoken Language Representations with Neural Lattice Language Modeling](https://doi.org/10.18653/v1/2020.acl-main.347) |  | 0 | Pre-trained language models have achieved huge improvement on many NLP tasks. However, these methods are usually designed for written text, so they do not consider the properties of spoken language. Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems. We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks. The proposed two-stage pre-training approach reduces... | ChaoWei Huang, YunNung Chen |  |
| 442 |  |  [Meta-Transfer Learning for Code-Switched Speech Recognition](https://doi.org/10.18653/v1/2020.acl-main.348) |  | 0 | An increasing number of people in the world today speak a mixed-language as a result of being multilingual. However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data. We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource... | Genta Indra Winata, Pascale Fung, Peng Xu, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu |  |
| 443 |  |  [Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association](https://doi.org/10.18653/v1/2020.acl-main.349) |  | 0 | Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means. With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms. In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions. Thus traditional text-based methods are insufficient to detect multimodal sarcasm. To reason with multimodal sarcastic... | Nan Xu, Wenji Mao, Zhixiong Zeng |  |
| 444 |  |  [SimulSpeech: End-to-End Simultaneous Speech to Text Translation](https://doi.org/10.18653/v1/2020.acl-main.350) |  | 0 | In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation. SimulSpeech is... | Chen Zhang, Jinglin Liu, Tao Qin, TieYan Liu, Xu Tan, Yi Ren, Zhou Zhao |  |
| 445 |  |  [Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations](https://doi.org/10.18653/v1/2020.acl-main.351) |  | 0 | Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR). We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding. Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features. This... | David C. Atkins, Karan Singla, Shrikanth Narayanan, Zhuohao Chen |  |
| 446 |  |  [Neural Temporal Opinion Modelling for Opinion Prediction on Twitter](https://doi.org/10.18653/v1/2020.acl-main.352) |  | 0 | Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context. In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours. We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context. Experimental results show that the proposed model predicts both... | Deyu Zhou, Lixing Zhu, Yulan He |  |
| 447 |  |  [It Takes Two to Lie: One to Lie, and One to Listen](https://doi.org/10.18653/v1/2020.acl-main.353) |  | 0 | Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support. But trust can be betrayed through deception. We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other. Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.... | Ahmed Elgohary, Benny Cheng, Cristian DanescuNiculescuMizil, Denis Peskov, Joe Barrow, Jordan L. BoydGraber |  |
| 448 |  |  [Learning Implicit Text Generation via Feature Matching](https://doi.org/10.18653/v1/2020.acl-main.354) |  | 0 | Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks. In this paper, we present new GFMN formulations that are effective for sequential data. Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer. SeqGFMN... | Cícero Nogueira dos Santos, Inkit Padhi, Ke Bai, Payel Das, Pierre L. Dognin, Vijil Chenthamarakshan, Youssef Mroueh |  |
| 449 |  |  [Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data](https://doi.org/10.18653/v1/2020.acl-main.355) |  | 0 | A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.... | Hamidreza Shahidi, Jimmy Lin, Ming Li |  |
| 450 |  |  [Bayesian Hierarchical Words Representation Learning](https://doi.org/10.18653/v1/2020.acl-main.356) |  | 0 | This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm. BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors. By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations. Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.... | Avi Caciularu, Idan Rejwan, Noam Koenigstein, Oren Barkan |  |
| 451 |  |  [Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning](https://doi.org/10.18653/v1/2020.acl-main.357) |  | 0 | Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text... | Alexandre Tamborrino, Baptiste Pannier, Louise Naudin, Nicola Pellicanò, Pascal Voitot |  |
| 452 |  |  [SEEK: Segmented Embedding of Knowledge Graphs](https://doi.org/10.18653/v1/2020.acl-main.358) |  | 0 | In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive... | Bin Shao, Jian Yin, Liang He, Shun Zheng, TieYan Liu, Wentao Xu |  |
| 453 |  |  [Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.359) |  | 0 | Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation. Combining backtranslated data from different sources has led to better results than when using such data in isolation. In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems. We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a... | Alberto Poncelas, Andy Way, Dimitar Sht. Shterionov, Xabier Soto |  |
| 454 |  |  [Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture](https://doi.org/10.18653/v1/2020.acl-main.360) |  | 0 | Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs. This is relevant both for time-critical and on-device computations using neural networks. The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model. On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning... | Christopher Brix, Hermann Ney, Parnia Bahar |  |
| 455 |  |  [A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction](https://doi.org/10.18653/v1/2020.acl-main.361) |  | 0 | Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor. The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence. Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and... | Fangkai Jiao, Jingfang Xu, Mantong Zhou, Minlie Huang, Ting Yao, Yilin Niu |  |
| 456 |  |  [Graph-to-Tree Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2020.acl-main.362) |  | 0 | While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in... | EePeng Lim, Jie Shao, Jipeng Zhang, Lei Wang, Roy KaWei Lee, Yan Wang, Yi Bin |  |
| 457 |  |  [An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results](https://doi.org/10.18653/v1/2020.acl-main.363) |  | 0 | In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis. Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes). In this paper we propose a new metric for... | Enrique Amigó, Jorge CarrillodeAlbornoz, Julio Gonzalo, Stefano Mizzaro |  |
| 458 |  |  [Adaptive Compression of Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.364) |  | 0 | Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks. However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices). In this paper, we propose a novel method to adaptively compress word embeddings. We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4). However, unlike prior works that assign the same... | KangMin Kim, SangKeun Lee, Yeachan Kim |  |
| 459 |  |  [Analysing Lexical Semantic Change with Contextualised Word Representations](https://doi.org/10.18653/v1/2020.acl-main.365) |  | 0 | This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive... | Marco Del Tredici, Mario Giulianelli, Raquel Fernández |  |
| 460 |  |  [Autoencoding Keyword Correlation Graph for Document Clustering](https://doi.org/10.18653/v1/2020.acl-main.366) |  | 0 | Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global). Existing representation learning models do not fully capture these features. To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph. The graph is constructed with topical keywords as nodes and multiple local and global features as edges. A GAE is employed... | Billy Chiu, Derek Thomas, Mohammady Mahdy, Neha Sengupta, Sunil Kumar Sahu |  |
| 461 |  |  [Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics](https://doi.org/10.18653/v1/2020.acl-main.367) |  | 0 | Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector. However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge. In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to... | Guy Emerson |  |
| 462 |  |  [BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance](https://doi.org/10.18653/v1/2020.acl-main.368) |  | 0 | Pretraining deep language models has led to large performance gains in NLP. Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words. For static word embeddings, this problem has been addressed by separately learning representations for rare words. In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input... | Hinrich Schütze, Timo Schick |  |
| 463 |  |  [CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages](https://doi.org/10.18653/v1/2020.acl-main.369) |  | 0 | Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that CluBERT learns distributions over English... | Bianca Scarlini, Federico Scozzafava, Tommaso Pasini |  |
| 464 |  |  [Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.370) |  | 0 | Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation. Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring... | Chunning Du, Haifeng Sun, Jianxin Liao, Jingyu Wang, Qi Qi |  |
| 465 |  |  [From Arguments to Key Points: Towards Automatic Argument Summarization](https://doi.org/10.18653/v1/2020.acl-main.371) |  | 0 | Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem. We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience. We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments. Furthermore, we found that a domain expert can often predict these key points in... | Dan Lahav, Lilach Eden, Noam Slonim, Roni Friedman, Roy BarHaim, Yoav Kantor |  |
| 466 |  |  [GoEmotions: A Dataset of Fine-Grained Emotions](https://doi.org/10.18653/v1/2020.acl-main.372) |  | 0 | Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We... | Alan S. Cowen, Dana MovshovitzAttias, Dorottya Demszky, Gaurav Nemade, Jeongwoo Ko, Sujith Ravi |  |
| 467 |  |  [He said "who's gonna take care of your children when you are at ACL?": Reported Sexist Acts are Not Sexist](https://doi.org/10.18653/v1/2020.acl-main.373) |  | 0 | In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman. We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of... | Alda Mari, Farah Benamara, Gloria Origgi, Marlène CoulombGully, Patricia Chiril, Véronique Moriceau |  |
| 468 |  |  [SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.374) |  | 0 | Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches. However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. With the help of automatically-mined knowledge,... | Bolei He, Can Gao, Feng Wu, Haifeng Wang, Hao Liu, Hao Tian, Hua Wu, Xinyan Xiao |  |
| 469 |  |  [Do Neural Language Models Show Preferences for Syntactic Formalisms?](https://doi.org/10.18653/v1/2020.acl-main.375) |  | 0 | Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent... | Artur Kulmizev, Joakim Nivre, Mostafa Abdou, Vinit Ravishankar |  |
| 470 |  |  [Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing](https://doi.org/10.18653/v1/2020.acl-main.376) |  | 0 | Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among... | Carlos GómezRodríguez, Daniel FernándezGonzález |  |
| 471 |  |  [Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis](https://doi.org/10.18653/v1/2020.acl-main.377) |  | 0 | A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar. We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs). The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules. In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a... | Weiwei Sun, Yajie Ye |  |
| 472 |  |  [Max-Margin Incremental CCG Parsing](https://doi.org/10.18653/v1/2020.acl-main.378) |  | 0 | Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT. Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like. A very incremental transition mechanism of a recently proposed CCG parser when trained in... | Mark Steedman, Milos Stanojevic |  |
| 473 |  |  [Neural Reranking for Dependency Parsing: An Evaluation](https://doi.org/10.18653/v1/2020.acl-main.379) |  | 0 | Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser. However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology. In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech. In addition, we introduce a new variation of a discriminative... | BichNgoc Do, Ines Rehbein |  |
| 474 |  |  [Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting](https://doi.org/10.18653/v1/2020.acl-main.380) |  | 0 | With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., “gay”, “black”) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like “She makes me happy to be gay” as abusive simply because of the word “gay.” In this paper, we formalize the unintended biases... | Bing Bai, Conghui Zhu, Guanhua Zhang, Junqi Zhang, Kun Bai, Tiejun Zhao |  |
| 475 |  |  [Analyzing analytical methods: The case of phonology in neural models of spoken language](https://doi.org/10.18653/v1/2020.acl-main.381) |  | 0 | Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method. As a step in this direction we study the case of representations of phonology in neural network models of spoken language. We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences. We... | Afra Alishahi, Bertrand Higy, Grzegorz Chrupala |  |
| 476 |  |  [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.382) |  | 0 | To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a... | Brendan Shillingford, OanaMaria Camburu, Pasquale Minervini, Phil Blunsom, Thomas Lukasiewicz |  |
| 477 |  |  [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://doi.org/10.18653/v1/2020.acl-main.383) |  | 0 | By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a... | Ben Kao, Qun Liu, Yun Chen, Zhiyong Wu |  |
| 478 |  |  [Probing for Referential Information in Language Models](https://doi.org/10.18653/v1/2020.acl-main.384) |  | 0 | Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical... | Gemma Boleda, IonutTeodor Sorodoc, Kristina Gulordava |  |
| 479 |  |  [Quantifying Attention Flow in Transformers](https://doi.org/10.18653/v1/2020.acl-main.385) |  | 0 | In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights,... | Samira Abnar, Willem H. Zuidema |  |
| 480 |  |  [Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?](https://doi.org/10.18653/v1/2020.acl-main.386) |  | 0 | With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches... | Alon Jacovi, Yoav Goldberg |  |
| 481 |  |  [Towards Transparent and Explainable Attention Models](https://doi.org/10.18653/v1/2020.acl-main.387) |  | 0 | Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide... | Akash Kumar Mohankumar, Balaji Vasan Srinivasan, Balaraman Ravindran, Mitesh M. Khapra, Preksha Nema, Sharan Narasimhan |  |
| 482 |  |  [Tchebycheff Procedure for Multi-task Text Classification](https://doi.org/10.18653/v1/2020.acl-main.388) |  | 0 | Multi-task Learning methods have achieved great progress in text classification. However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications. To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption. The extensive experiments back up our theoretical analysis and validate the superiority of our proposals. | Bo Du, Shuang Yun, Weiwei Liu, Yuren Mao |  |
| 483 |  |  [Modeling Word Formation in English-German Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.389) |  | 0 | This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology. Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation. The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system. | Alexander M. Fraser, Marion WellerDi Marco |  |
| 484 |  |  [Empowering Active Learning to Jointly Optimize System and User Demands](https://doi.org/10.18653/v1/2020.acl-main.390) |  | 0 | Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training. However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading. In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system... | Christian M. Meyer, Iryna Gurevych, JiUng Lee |  |
| 485 |  |  [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](https://doi.org/10.18653/v1/2020.acl-main.391) |  | 0 | This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC). The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC. For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the... | Jun Suzuki, Kentaro Inui, Masahiro Kaneko, Masato Mita, Shun Kiyono |  |
| 486 |  |  [Graph Neural News Recommendation with Unsupervised Preference Disentanglement](https://doi.org/10.18653/v1/2020.acl-main.392) |  | 0 | With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents. Most existing methods usually learn the representations of users and news from news contents for recommendation. However, they seldom consider high-order connectivity underlying the user-news interactions. Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news. In this paper, we model the user-news... | Chen Li, Cheng Yang, Chuan Shi, Linmei Hu, Ming Zhou, Nan Duan, Siyong Xu, Xing Xie |  |
| 487 |  |  [Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description](https://doi.org/10.18653/v1/2020.acl-main.393) |  | 0 | In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case. We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story. We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework. Experimental results on a real-world dataset demonstrate the behavior analysis can... | Wenhan Chao, Yakun Hu, Zhunchen Luo |  |
| 488 |  |  [Joint Modelling of Emotion and Abusive Language Detection](https://doi.org/10.18653/v1/2020.acl-main.394) |  | 0 | The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online. Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection. While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how... | Ekaterina Shutova, Helen Yannakoudakis, Pushkar Mishra, Santhosh Rajamanickam |  |
| 489 |  |  [Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.395) |  | 0 | The key to effortless end-user programming is natural language. We examine how to teach intelligent systems new functions, expressed in natural language. As a first step, we collected 3168 samples of teaching efforts in plain English. Then we built fuSE, a novel system that translates English function descriptions into code. Our approach is three-tiered and each task is evaluated separately. We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT). Then we... | Sebastian Weigelt, Tobias Hey, Vanessa Steurer, Walter F. Tichy |  |
| 490 |  |  [Toxicity Detection: Does Context Really Matter?](https://doi.org/10.18653/v1/2020.acl-main.396) |  | 0 | Moderation is crucial to promoting healthy online discussions. Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently. We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the... | Ion Androutsopoulos, Jeffrey Sorensen, John Pavlopoulos, Lucas Dixon, Nithum Thain |  |
| 491 |  |  [AMR Parsing with Latent Structural Information](https://doi.org/10.18653/v1/2020.acl-main.397) |  | 0 | Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences. We investigate parsing AMR with explicit dependency structures and interpretable latent structures. We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks. The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on... | Donghong Ji, Hao Tang, Qiji Zhou, Yue Zhang |  |
| 492 |  |  [TaPas: Weakly Supervised Table Parsing via Pre-training](https://doi.org/10.18653/v1/2020.acl-main.398) |  | 0 | Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present TaPas, an approach to question answering over tables... | Francesco Piccinno, Jonathan Herzig, Julian Martin Eisenschlos, Pawel Krzysztof Nowak, Thomas Müller |  |
| 493 |  |  [Target Inference in Argument Conclusion Generation](https://doi.org/10.18653/v1/2020.acl-main.399) |  | 0 | In argumentation, people state premises to reason towards a conclusion. The conclusion conveys a stance towards some target, such as a concept or statement. Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons. However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation. We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises. In particular, we... | Henning Wachsmuth, Martin Potthast, Milad Alshomary, Shahbaz Syed |  |
| 494 |  |  [Multimodal Transformer for Multimodal Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.400) |  | 0 | Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the... | Shaowei Yao, Xiaojun Wan |  |
| 495 |  |  [Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis](https://doi.org/10.18653/v1/2020.acl-main.401) |  | 0 | In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario. We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit. For multi-tasking, we propose two attention mechanisms, viz. Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment... | Asif Ekbal, Dhanush S. R, Dushyant Singh Chauhan, Pushpak Bhattacharyya |  |
| 496 |  |  [Towards Emotion-aided Multi-modal Dialogue Act Classification](https://doi.org/10.18653/v1/2020.acl-main.402) |  | 0 | The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively. But these studies limit themselves to text. Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task. Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions. Hence, the effect of emotion too on... | Aditya Prakash Patra, Pushpak Bhattacharyya, Sriparna Saha, Tulika Saha |  |
| 497 |  |  [Analyzing Political Parody in Social Media](https://doi.org/10.18653/v1/2020.acl-main.403) |  | 0 | Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from... | Antonis Maronikolakis, Danae Sanchez Villegas, Daniel PreotiucPietro, Nikolaos Aletras |  |
| 498 |  |  [Masking Actor Information Leads to Fairer Political Claims Detection](https://doi.org/10.18653/v1/2020.acl-main.404) |  | 0 | A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties. We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better. We propose two... | Erenay Dayanik, Sebastian Padó |  |
| 499 |  |  [When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?](https://doi.org/10.18653/v1/2020.acl-main.405) |  | 0 | Social biases are encoded in word embeddings. This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications. Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods. We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning. However, we also... | Jonathan H. Morgan, Kenneth Joseph |  |
| 500 |  |  ["Who said it, and Why?" Provenance for Natural Language Claims](https://doi.org/10.18653/v1/2020.acl-main.406) |  | 0 | In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale. This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims. To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has... | Dan Roth, Yi Zhang, Zachary G. Ives |  |
| 501 |  |  [Compositionality and Generalization In Emergent Languages](https://doi.org/10.18653/v1/2020.acl-main.407) |  | 0 | Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement... | Diane Bouchacourt, Emmanuel Dupoux, Eugene Kharitonov, Marco Baroni, Rahma Chaabouni |  |
| 502 |  |  [ERASER: A Benchmark to Evaluate Rationalized NLP Models](https://doi.org/10.18653/v1/2020.acl-main.408) |  | 0 | State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER a... | Byron C. Wallace, Caiming Xiong, Eric Lehman, Jay DeYoung, Nazneen Fatema Rajani, Richard Socher, Sarthak Jain |  |
| 503 |  |  [Learning to Faithfully Rationalize by Construction](https://doi.org/10.18653/v1/2020.acl-main.409) |  | 0 | In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text ‘responsible for’ corresponding model output; when such a snippet comprises tokens that indeed informed the model’s prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet... | Byron C. Wallace, Sarah Wiegreffe, Sarthak Jain, Yuval Pinter |  |
| 504 |  |  [Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset](https://doi.org/10.18653/v1/2020.acl-main.410) |  | 0 | Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation. Recently, Pampari et al. (EMNLP’18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes. In this paper, we provide an in-depth analysis of this... | Bernal Jimenez Gutierrez, Huan Sun, Xiang Yue |  |
| 505 |  |  [DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering](https://doi.org/10.18653/v1/2020.acl-main.411) |  | 0 | Transformer-based QA models use input-wide self-attention – i.e. across both the question and the input passage – at all layers, causing them to be slow and memory-intensive. It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers. We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. This allows for question-independent processing of the input text... | Aruna Balasubramanian, Harsh Trivedi, Niranjan Balasubramanian, Qingqing Cao |  |
| 506 |  |  [Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings](https://doi.org/10.18653/v1/2020.acl-main.412) |  | 0 | Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant... | Aditay Tripathi, Apoorv Saxena, Partha P. Talukdar |  |
| 507 |  |  [Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering](https://doi.org/10.18653/v1/2020.acl-main.413) |  | 0 | Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows. A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset. This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data. We propose an unsupervised approach to training QA models with generated pseudo-training data. We show that generating questions for QA training by applying a simple template... | Alexander R. Fabbri, Bing Xiang, Patrick Ng, Ramesh Nallapati, Zhiguo Wang |  |
| 508 |  |  [Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering](https://doi.org/10.18653/v1/2020.acl-main.414) |  | 0 | Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method. We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which... | Mihai Surdeanu, Steven Bethard, Vikas Yadav |  |
| 509 |  |  [A Corpus for Large-Scale Phonetic Typology](https://doi.org/10.18653/v1/2020.acl-main.415) |  | 0 | A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and... | Alan W. Black, Eleanor Chodroff, Elizabeth Salesky, Jason Eisner, Matthew Wiesner, Ryan Cotterell, Tiago Pimentel |  |
| 510 |  |  [Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing](https://doi.org/10.18653/v1/2020.acl-main.416) |  | 0 | Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length. Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance. DRSs are typically visualized as boxes which are not straightforward to process automatically. Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs. However, this metric is computationally costly (with respect to memory and CPU time)... | Jiangming Liu, Mirella Lapata, Shay B. Cohen |  |
| 511 |  |  [ParaCrawl: Web-Scale Acquisition of Parallel Corpora](https://doi.org/10.18653/v1/2020.acl-main.417) |  | 0 | We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems. | Amir Kamran, Barry Haddow, Brian Thompson, Dion Wiggins, Elsa Sarrías, Faheem Kirefu, Gema RamírezSánchez, Hieu Hoang, Jaume Zaragoza, Kenneth Heafield, Leopoldo Pla Sempere, Marek Strelec, Marta Bañón, Mikel L. Forcada, Miquel EsplàGomis, Philipp Koehn, Pinzhen Chen, Sergio OrtizRojas, William Waites |  |
| 512 |  |  [Toward Gender-Inclusive Coreference Resolution](https://doi.org/10.18653/v1/2020.acl-main.418) |  | 0 | Correctly resolving textual mentions of people fundamentally entails making inferences about those people. Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders. To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems. Through these... | Hal Daumé III, Yang Trista Cao |  |
| 513 |  |  [Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?](https://doi.org/10.18653/v1/2020.acl-main.419) |  | 0 | Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention. In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. To achieve this, we design and conduct a large-scale crowd-sourcing... | Biao Yin, Cansu Sen, Elke A. Rundensteiner, Thomas Hartvigsen, Xiangnan Kong |  |
| 514 |  |  [Information-Theoretic Probing for Linguistic Structure](https://doi.org/10.18653/v1/2020.acl-main.420) |  | 0 | The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using... | Adina Williams, Josef Valvoda, Ran Zmigrod, Rowan Hall Maudslay, Ryan Cotterell, Tiago Pimentel |  |
| 515 |  |  [On the Cross-lingual Transferability of Monolingual Representations](https://doi.org/10.18653/v1/2020.acl-main.421) |  | 0 | State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked... | Dani Yogatama, Mikel Artetxe, Sebastian Ruder |  |
| 516 |  |  [Similarity Analysis of Contextual Word Representation Models](https://doi.org/10.18653/v1/2020.acl-main.422) |  | 0 | This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic... | Fahim Dalvi, Hassan Sajjad, James R. Glass, John M. Wu, Nadir Durrani, Yonatan Belinkov |  |
| 517 |  |  [SenseBERT: Driving Some Sense into BERT](https://doi.org/10.18653/v1/2020.acl-main.423) |  | 0 | The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a... | Amnon Shashua, Barak Lenz, Dan Padnos, Or Dagan, Or Sharir, Ori Ram, Shai ShalevShwartz, Yoav Levine, Yoav Shoham |  |
| 518 |  |  [ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations](https://doi.org/10.18653/v1/2020.acl-main.424) |  | 0 | In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it... | Antoine Bordes, Benoît Sagot, Carolina Scarton, Fernando AlvaManchego, Louis Martin, Lucia Specia |  |
| 519 |  |  [Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts](https://doi.org/10.18653/v1/2020.acl-main.425) |  | 0 | Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom. However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets. Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges. We fill this gap by presenting BabelPic, a hand-labeled dataset... | Agostina Calabrese, Michele Bevilacqua, Roberto Navigli |  |
| 520 |  |  [Modeling Label Semantics for Predicting Emotional Reactions](https://doi.org/10.18653/v1/2020.acl-main.426) |  | 0 | Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict. They ignore information that may be conveyed by the emotion labels themselves. We propose that the semantics of emotion labels can guide a model’s attention when representing the input story. Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness. In this... | Heeyoung Kwon, Mohaddeseh Bastan, Nathanael Chambers, Niranjan Balasubramanian, Radhika Gaonkar |  |
| 521 |  |  [CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant](https://doi.org/10.18653/v1/2020.acl-main.427) |  | 0 | We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft. The dataset consists of 7K human utterances and their corresponding parses. Given proper world state, the parses can be interpreted and executed in game. We report the performance of baseline models, and analyze their successes and failures. | Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite |  |
| 522 |  |  [Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training](https://doi.org/10.18653/v1/2020.acl-main.428) |  | 0 | Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws. In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases. We show that appropriate loss... | Ilia Kulikov, Jason Weston, Kyunghyun Cho, Margaret Li, Sean Welleck, Stephen Roller, YLan Boureau |  |
| 523 |  |  [How does BERT's attention change when you fine-tune? An analysis methodology and a case study in negation scope](https://doi.org/10.18653/v1/2020.acl-main.429) |  | 0 | Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and... | Steven Bethard, Yiyun Zhao |  |
| 524 |  |  [Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models](https://doi.org/10.18653/v1/2020.acl-main.430) |  | 0 | LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks. Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English. Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain. Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other... | Anupam Datta, Kaiji Lu, Klas Leino, Matt Fredrikson, Piotr Mardziel |  |
| 525 |  |  [Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings](https://doi.org/10.18653/v1/2020.acl-main.431) |  | 0 | Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and... | Claire Cardie, Kelly Davis, Rishi Bommasani |  |
| 526 |  |  [Learning to Deceive with Attention-Based Explanations](https://doi.org/10.18653/v1/2020.acl-main.432) |  | 0 | Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method... | Bhuwan Dhingra, Danish Pruthi, Graham Neubig, Mansi Gupta, Zachary C. Lipton |  |
| 527 |  |  [On the Spontaneous Emergence of Discrete and Compositional Signals](https://doi.org/10.18653/v1/2020.acl-main.433) |  | 0 | We propose a general framework to study language emergence through signaling games with neural agents. Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge. We explore whether categorical perception effects follow and show that the messages are not compositional. | Emmanuel Chemla, Nur Geffen Lan, Shane SteinertThrelkeld |  |
| 528 |  |  [Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words](https://doi.org/10.18653/v1/2020.acl-main.434) |  | 0 | Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our... | Allyson Ettinger, Josef Klafka |  |
| 529 |  |  [Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA](https://doi.org/10.18653/v1/2020.acl-main.435) |  | 0 | Videos convey rich information. Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip. Hence, it is important to develop automated models that can accurately extract such information from videos. Answering questions on videos is one of the tasks which can evaluate such AI abilities. In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions.... | Hyounghun Kim, Mohit Bansal, Zineng Tang |  |
| 530 |  |  [Shaping Visual Representations with Language for Few-Shot Classification](https://doi.org/10.18653/v1/2020.acl-main.436) |  | 0 | By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning... | Jesse Mu, Noah D. Goodman, Percy Liang |  |
| 531 |  |  [Discrete Latent Variable Representations for Low-Resource Text Classification](https://doi.org/10.18653/v1/2020.acl-main.437) |  | 0 | While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient. We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable. We compare the performance of the learned representations as features for low-resource document and sentence classification. Our best models outperform the previous... | Karen Livescu, Karl Stratos, Sam Wiseman, Shuning Jin |  |
| 532 |  |  [Learning Constraints for Structured Prediction Using Rectifier Networks](https://doi.org/10.18653/v1/2020.acl-main.438) |  | 0 | Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and... | Maitrey Mehta, Vivek Srikumar, Xingyuan Pan |  |
| 533 |  |  [Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models](https://doi.org/10.18653/v1/2020.acl-main.439) |  | 0 | Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and... | Dan Iter, Dan Jurafsky, Kelvin Guu, Larry Lansing |  |
| 534 |  |  [A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks](https://doi.org/10.18653/v1/2020.acl-main.440) |  | 0 | Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e. procedures) that describe how to make the same dish (i.e. high-level task). Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world... | Angela S. Lin, Asli Celikyilmaz, Bill Dolan, Chris Brockett, Debadeepta Dey, Elnaz Nouri, Sudha Rao |  |
| 535 |  |  [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.441) |  | 0 | We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in... | Adina Williams, Douwe Kiela, Emily Dinan, Jason Weston, Mohit Bansal, Yixin Nie |  |
| 536 |  |  [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://doi.org/10.18653/v1/2020.acl-main.442) |  | 0 | Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as... | Carlos Guestrin, Marco Túlio Ribeiro, Sameer Singh, Tongshuang Wu |  |
| 537 |  |  [Code and Named Entity Recognition in StackOverflow](https://doi.org/10.18653/v1/2020.acl-main.443) |  | 0 | There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet. For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users. Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences. In this paper, we introduce a new named entity recognition (NER) corpus... | Alan Ritter, Jeniya Tabassum, Mounica Maddela, Wei Xu |  |
| 538 |  |  [Dialogue-Based Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.444) |  | 0 | We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue. We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences. We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks. Considering the timeliness of... | Claire Cardie, Dian Yu, Dong Yu, Kai Sun |  |
| 539 |  |  [Facet-Aware Evaluation for Extractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.445) |  | 0 | Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level. In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries. Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and... | Jiawei Han, Liyuan Liu, Qi Zhu, Xiang Ren, Yuning Mao |  |
| 540 |  |  [More Diverse Dialogue Datasets via Diversity-Informed Data Collection](https://doi.org/10.18653/v1/2020.acl-main.446) |  | 0 | Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational... | Grace Hui Yang, Katherine Stasaski, Marti A. Hearst |  |
| 541 |  |  [S2ORC: The Semantic Scholar Open Research Corpus](https://doi.org/10.18653/v1/2020.acl-main.447) |  | 0 | We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and... | Daniel S. Weld, Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney |  |
| 542 |  |  [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://doi.org/10.18653/v1/2020.acl-main.448) |  | 0 | Automatic metrics are fundamental for the development and evaluation of machine translation systems. Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy. Finally, we turn to pairwise system ranking, developing a... | Nitika Mathur, Timothy Baldwin, Trevor Cohn |  |
| 543 |  |  [A Transformer-based Approach for Source Code Summarization](https://doi.org/10.18653/v1/2020.acl-main.449) |  | 0 | Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms... | Baishakhi Ray, KaiWei Chang, Saikat Chakraborty, Wasi Uddin Ahmad |  |
| 544 |  |  [Asking and Answering Questions to Evaluate the Factual Consistency of Summaries](https://doi.org/10.18653/v1/2020.acl-main.450) |  | 0 | Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually... | Alex Wang, Kyunghyun Cho, Mike Lewis |  |
| 545 |  |  [Discourse-Aware Neural Extractive Text Summarization](https://doi.org/10.18653/v1/2020.acl-main.451) |  | 0 | Recently BERT has been adopted for document encoding in state-of-the-art text summarization models. However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents. To address these issues, we present a discourse-aware neural summarization model - DiscoBert. DiscoBert extracts sub-sentential discourse units (instead of... | Jiacheng Xu, Jingjing Liu, Yu Cheng, Zhe Gan |  |
| 546 |  |  [Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction](https://doi.org/10.18653/v1/2020.acl-main.452) |  | 0 | Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information. A good summary is characterized by language fluency and high information overlap with the source sentence. We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics. We search for a high-scoring summary by discrete optimization. Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to... | Katja Markert, Lili Mou, Olga Vechtomova, Raphael Schumann, Yao Lu |  |
| 547 |  |  [Exploring Content Selection in Summarization of Novel Chapters](https://doi.org/10.18653/v1/2020.acl-main.453) |  | 0 | We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides. This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries. We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries. We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also... | Bryan Li, Faisal Ladhak, Kathleen R. McKeown, Yaser AlOnaizan |  |
| 548 |  |  [FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.454) |  | 0 | Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the... | Esin Durmus, He He, Mona T. Diab |  |
| 549 |  |  [Fact-based Content Weighting for Evaluating Abstractive Summarisation](https://doi.org/10.18653/v1/2020.acl-main.455) |  | 0 | Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient. We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of the document to the facts of the summary. We fol- low the assumption that a good summary will reflect all relevant facts, i.e. the ones present in the ground truth (human-generated refer- ence summary). We confirm this hypothe- sis by showing that our weightings are highly correlated to human... | Ioannis Konstas, Jingyi Li, Ondrej Dusek, Verena Rieser, Xinnuo Xu |  |
| 550 |  |  [Hooks in the Headline: Learning to Generate Headlines with Controlled Styles](https://doi.org/10.18653/v1/2020.acl-main.456) |  | 0 | Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and... | Di Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits, Zhijing Jin |  |
| 551 |  |  [Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward](https://doi.org/10.18653/v1/2020.acl-main.457) |  | 0 | Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven... | Lingfei Wu, Lu Wang, Luyang Huang |  |
| 552 |  |  [Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports](https://doi.org/10.18653/v1/2020.acl-main.458) |  | 0 | Neural abstractive summarization models are able to generate summaries which have high overlap with human references. However, existing models are not optimized for factual correctness, a critical metric in real-world applications. In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module. We further propose a training strategy which optimizes a neural summarization model with a... | Christopher D. Manning, Curtis P. Langlotz, Derek Merck, Emily Bao Tsai, Yuhao Zhang |  |
| 553 |  |  [Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset](https://doi.org/10.18653/v1/2020.acl-main.459) |  | 0 | This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses. Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game. The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns. It also includes corresponding abstractive summaries collected from the Fandom wiki. The dataset is linguistically unique in that the narratives are generated entirely through... | Peter Bailey, Revanth Rameshkumar |  |
| 554 |  |  [The Summary Loop: Learning to Write Abstractive Summaries Without Examples](https://doi.org/10.18653/v1/2020.acl-main.460) |  | 0 | This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score... | Andrew Hsi, John F. Canny, Marti A. Hearst, Philippe Laban |  |
| 555 |  |  [Unsupervised Opinion Summarization as Copycat-Review Generation](https://doi.org/10.18653/v1/2020.acl-main.461) |  | 0 | Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews. While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries. Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs.... | Arthur Brazinskas, Ivan Titov, Mirella Lapata |  |
| 556 |  |  [(Re)construing Meaning in NLP](https://doi.org/10.18653/v1/2020.acl-main.462) |  | 0 | Human speakers have an extensive toolkit of ways to express themselves. In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding—namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed. We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics. We then survey some dimensions of construed meaning and show how... | Nancy Chang, Nathan Schneider, Sean Trott, Tiago Timponi Torrent |  |
| 557 |  |  [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://doi.org/10.18653/v1/2020.acl-main.463) |  | 0 | The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help... | Alexander Koller, Emily M. Bender |  |
| 558 |  |  [Examining Citations of Natural Language Processing Literature](https://doi.org/10.18653/v1/2020.acl-main.464) |  | 0 | We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers. We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc. Notably, we show that only about 56% of the papers in AA are cited ten or more times. CL Journal has the most cited papers, but its citation dominance has lessened in recent years. On average, long papers... | Saif M. Mohammad |  |
| 559 |  |  [How Can We Accelerate Progress Towards Human-like Linguistic Generalization?](https://doi.org/10.18653/v1/2020.acl-main.465) |  | 0 | This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple,... | Tal Linzen |  |
| 560 |  |  [How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence](https://doi.org/10.18653/v1/2020.acl-main.466) |  | 0 | Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers... | Chaojun Xiao, Cunchao Tu, Haoxi Zhong, Maosong Sun, Tianyang Zhang, Zhiyuan Liu |  |
| 561 |  |  [Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://doi.org/10.18653/v1/2020.acl-main.467) |  | 0 | While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained... | Clara Vania, Haokun Liu, Jason Phang, Katharina Kann, Phu Mon Htut, Richard Yuanzhe Pang, Samuel R. Bowman, Xiaoyi Zhang, Yada Pruksachatkun |  |
| 562 |  |  [Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview](https://doi.org/10.18653/v1/2020.acl-main.468) |  | 0 | An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we... | Deven Shah, Dirk Hovy, H. Andrew Schwartz |  |
| 563 |  |  [What Does BERT with Vision Look At?](https://doi.org/10.18653/v1/2020.acl-main.469) |  | 0 | Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. Specifically, some heads can map entities to image regions, performing the task known as entity grounding. Some heads can even detect the syntactic relations... | ChoJui Hsieh, Da Yin, KaiWei Chang, Liunian Harold Li, Mark Yatskar |  |
| 564 |  |  [Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards](https://doi.org/10.18653/v1/2020.acl-main.470) |  | 0 | Throughout a conversation, participants make choices that can orient the flow of the interaction. Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation. In this work, we develop an unsupervised methodology to quantify how counselors manage this balance. Our main intuition is that if an utterance can only receive a... | Cristian DanescuNiculescuMizil, Justine Zhang |  |
| 565 |  |  [Detecting Perceived Emotions in Hurricane Disasters](https://doi.org/10.18653/v1/2020.acl-main.471) |  | 0 | Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake. People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community. Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale. In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria. We present a comprehensive study... | Cornelia Caragea, Junyi Jessy Li, Shrey Desai |  |
| 566 |  |  [Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention](https://doi.org/10.18653/v1/2020.acl-main.472) |  | 0 | Not all documents are equally important. Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative. In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits. We demonstrate that models with message-level attention... | H. Andrew Schwartz, Niranjan Balasubramanian, Veronica E. Lynn |  |
| 567 |  |  [Measuring Forecasting Skill from Text](https://doi.org/10.18653/v1/2020.acl-main.473) |  | 0 | People vary in their ability to make accurate predictions about the future. Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy. This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill. Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an... | Alan Ritter, Eduard H. Hovy, Shi Zong |  |
| 568 |  |  [Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates](https://doi.org/10.18653/v1/2020.acl-main.474) |  | 0 | Many applications of computational social science aim to infer causal conclusions from non-experimental data. Such observational data often contains confounders, variables that influence both potential causes and potential effects. Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text. For example, an individual’s entire history of social media posts or the content of a news article could provide a rich measurement of multiple... | Brendan O'Connor, David D. Jensen, Katherine A. Keith |  |
| 569 |  |  [Text-Based Ideal Points](https://doi.org/10.18653/v1/2020.acl-main.475) |  | 0 | Ideal point models analyze lawmakers’ votes to quantify their political positions, or ideal points. But votes are not the only way to express a political position. Lawmakers also give speeches, release press statements, and post tweets. In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors. We demonstrate the TBIP with two types of politicized text data: U.S. Senate speeches and senator tweets.... | David M. Blei, Keyon Vafa, Suresh Naidu |  |
| 570 |  |  [Understanding the Language of Political Agreement and Disagreement in Legislative Texts](https://doi.org/10.18653/v1/2020.acl-main.476) |  | 0 | While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level. Despite this fact, there is little awareness of the dynamics that lead to adopting these policies. In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods. We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information,... | Dan Goldwasser, Eric Waltenburg, Maryam Davoodi |  |
| 571 |  |  [Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences](https://doi.org/10.18653/v1/2020.acl-main.477) |  | 0 | Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. Concretely, we present a new task and corpus for learning alignments between machine and human preferences. Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained... | Alvin Chan, Anh Tuan Luu, Chris Pal, Donovan Ong, Jie Fu, Nancy F. Chen, Yi Tay |  |
| 572 |  |  [Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event](https://doi.org/10.18653/v1/2020.acl-main.478) |  | 0 | Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event. To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources. Next, we propose several document-level neural-network models to automatically construct news content structures. Finally, we... | Aaron Lee, Lu Wang, Prafulla Kumar Choubey, Ruihong Huang |  |
| 573 |  |  [Harnessing the linguistic signal to predict scalar inferences](https://doi.org/10.18653/v1/2020.acl-main.479) |  | 0 | Pragmatic inferences often subtly depend on the presence or absence of linguistic features. For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing “Chris ate some of the cookies” than after hearing the same utterance without a partitive, “Chris ate some cookies”. In this work, we explore to what extent neural network sentence encoders can learn to predict the... | Judith Degen, Sebastian Schuster, Yuxing Chen |  |
| 574 |  |  [Implicit Discourse Relation Classification: We Need to Talk about Evaluation](https://doi.org/10.18653/v1/2020.acl-main.480) |  | 0 | Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations. However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature. In this work, we highlight these inconsistencies and propose an improved evaluation protocol. Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0.... | Luis A. Lastras, Najoung Kim, R. Chulaka Gunasekara, Song Feng |  |
| 575 |  |  [PeTra: A Sparsely Supervised Memory Model for People Tracking](https://doi.org/10.18653/v1/2020.acl-main.481) |  | 0 | We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots. PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture. We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance. To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation... | Allyson Ettinger, Karen Livescu, Kevin Gimpel, Shubham Toshniwal |  |
| 576 |  |  [ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT](https://doi.org/10.18653/v1/2020.acl-main.482) |  | 0 | Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively. We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately. For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees. Experiments on two benchmarks show that joint modeling significantly outperforms our... | Dong Yu, Jianshu Chen, Kun Xu, Linfeng Song, Yue Zhang |  |
| 577 |  |  [Contextualizing Hate Speech Classifiers with Post-hoc Explanation](https://doi.org/10.18653/v1/2020.acl-main.483) |  | 0 | Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “gay” or “black” are used in offensive or prejudiced ways. Such biases manifest in false positives when these identifiers are present, due to models’ inability to learn the contexts which constitute a hateful usage of identifiers. We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then, we propose a novel regularization technique based on these explanations that... | Aida Mostafazadeh Davani, Brendan Kennedy, Morteza Dehghani, Xiang Ren, Xisen Jin |  |
| 578 |  |  [Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation](https://doi.org/10.18653/v1/2020.acl-main.484) |  | 0 | Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose... | Bryan McCann, Caiming Xiong, Nazneen Fatema Rajani, Tianlu Wang, Vicente Ordonez, Xi Victoria Lin |  |
| 579 |  |  [Language (Technology) is Power: A Critical Survey of "Bias" in NLP](https://doi.org/10.18653/v1/2020.acl-main.485) |  | 0 | We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three... | Hal Daumé III, Hanna M. Wallach, Solon Barocas, Su Lin Blodgett |  |
| 580 |  |  [Social Bias Frames: Reasoning about Social and Power Implications of Language](https://doi.org/10.18653/v1/2020.acl-main.486) |  | 0 | Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people’s judgments about others. For example, given a statement that “we shouldn’t lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates) are less qualified.” Most... | Dan Jurafsky, Lianhui Qin, Maarten Sap, Noah A. Smith, Saadia Gabriel, Yejin Choi |  |
| 581 |  |  [Social Biases in NLP Models as Barriers for Persons with Disabilities](https://doi.org/10.18653/v1/2020.acl-main.487) |  | 0 | Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the... | Ben Hutchinson, Emily Denton, Kellie Webster, Stephen Denuyl, Vinodkumar Prabhakaran, Yu Zhong |  |
| 582 |  |  [Towards Debiasing Sentence Representations](https://doi.org/10.18653/v1/2020.acl-main.488) |  | 0 | As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the... | Emily Zheng, Irene Mengze Li, LouisPhilippe Morency, Paul Pu Liang, Ruslan Salakhutdinov, Yao Chong Lim |  |
| 583 |  |  [A Re-evaluation of Knowledge Graph Completion Methods](https://doi.org/10.18653/v1/2020.acl-main.489) |  | 0 | Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation... | Partha P. Talukdar, Shikhar Vashishth, Soumya Sanyal, Yiming Yang, Zhiqing Sun |  |
| 584 |  |  [Cross-Linguistic Syntactic Evaluation of Word Prediction Models](https://doi.org/10.18653/v1/2020.acl-main.490) |  | 0 | A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models’ ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German,... | Aaron Mueller, Garrett Nicolai, Natalia Talmina, Panayiota PetrouZeniou, Tal Linzen |  |
| 585 |  |  [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://doi.org/10.18653/v1/2020.acl-main.491) |  | 0 | Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME,... | Mohit Bansal, Peter Hase |  |
| 586 |  |  [Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions](https://doi.org/10.18653/v1/2020.acl-main.492) |  | 0 | Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for... | Byron C. Wallace, Xiaochuang Han, Yulia Tsvetkov |  |
| 587 |  |  [Finding Universal Grammatical Relations in Multilingual BERT](https://doi.org/10.18653/v1/2020.acl-main.493) |  | 0 | Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that... | Christopher D. Manning, Ethan A. Chi, John Hewitt |  |
| 588 |  |  [Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection](https://doi.org/10.18653/v1/2020.acl-main.494) |  | 0 | Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions.... | Guangtao Zheng, Hanjie Chen, Yangfeng Ji |  |
| 589 |  |  [Obtaining Faithful Interpretations from Compositional Neural Networks](https://doi.org/10.18653/v1/2020.acl-main.495) |  | 0 | Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a... | Ben Bogin, Jonathan Berant, Matt Gardner, Nitish Gupta, Sameer Singh, Sanjay Subramanian, Tomer Wolfson |  |
| 590 |  |  [Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport](https://doi.org/10.18653/v1/2020.acl-main.496) |  | 0 | Selecting input features of top relevance has become a popular method for building self-explaining models. In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction. Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs. However, directly applying OT often produces dense and therefore uninterpretable alignments. To overcome... | Kyle Swanson, Lili Yu, Tao Lei |  |
| 591 |  |  [Benefits of Intermediate Annotations in Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.497) |  | 0 | Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer. A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task. In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection. We find that these... | Dheeru Dua, Matt Gardner, Sameer Singh |  |
| 592 |  |  [Crossing Variational Autoencoders for Answer Retrieval](https://doi.org/10.18653/v1/2020.acl-main.498) |  | 0 | Answer retrieval is to find the most aligned answer from a large set of candidates given a question. Learning vector representations of questions/answers is the key factor. Question-answer alignment and question/answer semantics are two important signals for learning the representations. Existing methods learned semantic representations with dual encoders or dual variational auto-encoders. The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes. However, the... | Lingfei Wu, Meng Jiang, Qingkai Zeng, Shu Tao, Wenhao Yu, Yu Deng |  |
| 593 |  |  [Logic-Guided Data Augmentation and Regularization for Consistent Question Answering](https://doi.org/10.18653/v1/2020.acl-main.499) |  | 0 | Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over... | Akari Asai, Hannaneh Hajishirzi |  |
| 594 |  |  [On the Importance of Diversity in Question Generation for QA](https://doi.org/10.18653/v1/2020.acl-main.500) |  | 0 | Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA). In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search. We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated... | Md. Arafat Sultan, Ramón Fernandez Astudillo, Shubham Chandel, Vittorio Castelli |  |
| 595 |  |  [Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering](https://doi.org/10.18653/v1/2020.acl-main.501) |  | 0 | We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings. We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans). We show that these assumptions interact, and that different configurations provide complementary benefits. We demonstrate that a multi-objective model can efficiently combine the... | Hao Cheng, Kenton Lee, Kristina Toutanova, MingWei Chang |  |
| 596 |  |  [SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations](https://doi.org/10.18653/v1/2020.acl-main.502) |  | 0 | We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction. SCDE is a human created sentence cloze dataset, collected from public school English examinations. Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers. Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood. The blanks require joint solving... | Eduard H. Hovy, Varun Gangal, Xiang Kong |  |
| 597 |  |  [Selective Question Answering under Domain Shift](https://doi.org/10.18653/v1/2020.acl-main.503) |  | 0 | To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering. Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical. In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy. Abstention... | Amita Kamath, Percy Liang, Robin Jia |  |
| 598 |  |  [The Cascade Transformer: an Application for Efficient Answer Sentence Selection](https://doi.org/10.18653/v1/2020.acl-main.504) |  | 0 | Large transformer-based language models have been shown to be very effective in many classification tasks. However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates. While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference. In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based... | Alessandro Moschitti, Luca Soldaini |  |
| 599 |  |  [Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering](https://doi.org/10.18653/v1/2020.acl-main.505) |  | 0 | We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts. Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA). Our approach is... | Changmao Li, Jinho D. Choi |  |
| 600 |  |  [Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses](https://doi.org/10.18653/v1/2020.acl-main.506) |  | 0 | Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues. While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community. We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference). Since... | Ashish Sabharwal, Dan Roth, Daniel Khashabi, Erfan Sadeqi Azer |  |
| 601 |  |  [STARC: Structured Annotations for Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.507) |  | 0 | We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions. Our framework introduces a principled structure for the answer choices and ties them to textual span annotations. The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English. We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of... | Jonathan Malmaud, Roger Levy, Yevgeni Berzak |  |
| 602 |  |  [WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge](https://doi.org/10.18653/v1/2020.acl-main.508) |  | 0 | In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC). For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories. By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense... | Hongming Zhang, Xinran Zhao, Yangqiu Song |  |
| 603 |  |  [Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity](https://doi.org/10.18653/v1/2020.acl-main.509) |  | 0 | In online debates, users express different levels of agreement/disagreement with one another’s arguments and ideas. Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions. Existing stance detection methods predict the polarity of a post’s stance toward a topic or post, but don’t consider the stance’s degree of intensity. We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts. This problem is... | Douglas Adams, Joseph Sirrianni, Xiaoqing Liu |  |
| 604 |  |  [Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning](https://doi.org/10.18653/v1/2020.acl-main.510) |  | 0 | Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages. Their success heavily depends on the availability of a large amount of labeled data or parallel corpus. In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus. We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that... | Hongliang Fei, Ping Li |  |
| 605 |  |  [Efficient Pairwise Annotation of Argument Quality](https://doi.org/10.18653/v1/2020.acl-main.511) |  | 0 | We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work. A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments. The model’s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments... | Benno Stein, Lukas Gienapp, Martin Potthast, Matthias Hagen |  |
| 606 |  |  [Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification](https://doi.org/10.18653/v1/2020.acl-main.512) |  | 0 | This paper studies the task of comparative preference classification (CPC). Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities. Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities. Some also use... | Bing Liu, Hao Wang, Nianzu Ma, Sahisnu Mazumder |  |
| 607 |  |  [OpinionDigest: A Simple Framework for Opinion Summarization](https://doi.org/10.18653/v1/2020.acl-main.513) |  | 0 | We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Transformer model, which verbalizes them into an... | Stefanos Angelidis, WangChiew Tan, Xiaolan Wang, Yoshihiko Suhara |  |
| 608 |  |  [A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks](https://doi.org/10.18653/v1/2020.acl-main.514) |  | 0 | Affective tasks such as sentiment analysis, emotion classification, and sarcasm detection have been popular in recent years due to an abundance of user-generated data, accurate computational linguistic models, and a broad range of relevant applications in various domains. At the same time, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task. While preprocessing in affective systems is well-studied, preprocessing in... | Aijun An, Ameeta Agrawal, Manos Papagelis, Nastaran Babanejad |  |
| 609 |  |  [Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness](https://doi.org/10.18653/v1/2020.acl-main.515) |  | 0 | Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper... | Dawei Zhang, Sixing Wu, Yang Zhou, Ying Li, Zhonghai Wu |  |
| 610 |  |  [Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation](https://doi.org/10.18653/v1/2020.acl-main.516) |  | 0 | Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines. The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models. Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words. In this work, we introduce... | Haoyu Song, Ting Liu, Weinan Zhang, Xiaojiang Liu, Yan Wang |  |
| 611 |  |  [Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks](https://doi.org/10.18653/v1/2020.acl-main.517) |  | 0 | Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems. Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task. However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks. In this paper, we propose an algorithm that can customize a unique dialogue... | Ming Zhang, Rui Yan, Wei Bi, Yiping Song, Zequn Liu |  |
| 612 |  |  [Video-Grounded Dialogues with Pretrained Generation Language Models](https://doi.org/10.18653/v1/2020.acl-main.518) |  | 0 | Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses. In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over... | Hung Le, Steven C. H. Hoi |  |
| 613 |  |  [A Unified MRC Framework for Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.519) |  | 0 | The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not. Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels. In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks.... | Fei Wu, Jingrong Feng, Jiwei Li, Qinghong Han, Xiaoya Li, Yuxian Meng |  |
| 614 |  |  [An Effective Transition-based Model for Discontinuous NER](https://doi.org/10.18653/v1/2020.acl-main.520) |  | 0 | Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans. Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions. We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER. Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions... | Ben Hachey, Cécile Paris, Sarvnaz Karimi, Xiang Dai |  |
| 615 |  |  [IMoJIE: Iterative Memory-Based Joint Open Information Extraction](https://doi.org/10.18653/v1/2020.acl-main.521) |  | 0 | While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples.... | Keshav Kolluru, Mausam, Samarth Aggarwal, Soumen Chakrabarti, Vipul Rathore |  |
| 616 |  |  [Improving Event Detection via Open-domain Trigger Knowledge](https://doi.org/10.18653/v1/2020.acl-main.522) |  | 0 | Event Detection (ED) is a fundamental task in automatically structuring texts. Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words. To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations. Experiments on benchmark ACE2005 show that our model outperforms... | Bin Xu, Juanzi Li, Jun Xie, Lei Hou, Meihan Tong, Shuai Wang, Yixin Cao |  |
| 617 |  |  [Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling](https://doi.org/10.18653/v1/2020.acl-main.523) |  | 0 | Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate. Current models for jointly learning sentence and token labeling are limited to binary classification. We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors. Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the... | Canasai Kruengkrai, Lidong Bing, Sharifah Aljunied Mahani, Thien Hai Nguyen |  |
| 618 |  |  [Multi-Cell Compositional LSTM for NER Domain Adaptation](https://doi.org/10.18653/v1/2020.acl-main.524) |  | 0 | Cross-domain NER is a challenging yet practical problem. Entity mentions can be highly different across domains. However, the correlations between entity types can be relatively more stable across domains. We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state. With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level. Theoretically, the resulting distinct feature distributions for each entity type... | Chen Jia, Yue Zhang |  |
| 619 |  |  [Pyramid: A Layered Model for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.525) |  | 0 | This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER). In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape. Each time an embedding passes through a layer of the pyramid, its length is reduced by one. Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention. We also design an inverse... | Gang Chen, Jue Wang, Ke Chen, Lidan Shou |  |
| 620 |  |  [ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2020.acl-main.526) |  | 0 | The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still... | Guangyou Zhou, Jimmy Xiangji Huang, Jin Liu, Zhiwen Xie |  |
| 621 |  |  [Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents](https://doi.org/10.18653/v1/2020.acl-main.527) |  | 0 | Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts. In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation. We propose a joint extraction approach... | Daoyuan Chen, Kai Lei, Yaliang Li, Ying Shen |  |
| 622 |  |  [Simplify the Usage of Lexicon in Chinese NER](https://doi.org/10.18653/v1/2020.acl-main.528) |  | 0 | Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons. As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets. However, Lattice-LSTM has a complex model architecture. This limits its application in many industrial areas where real-time NER responses are needed. In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations. This method avoids... | Minlong Peng, Qi Zhang, Ruotian Ma, Xuanjing Huang, Zhongyu Wei |  |
| 623 |  |  [AdvAug: Robust Adversarial Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.529) |  | 0 | In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence... | Jacob Eisenstein, Lu Jiang, Wolfgang Macherey, Yong Cheng |  |
| 624 |  |  [Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns](https://doi.org/10.18653/v1/2020.acl-main.530) |  | 0 | The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation,... | Gholamreza Haffari, KayYen Wong, Sameen Maruf |  |
| 625 |  |  [Improving Neural Machine Translation with Soft Template Prediction](https://doi.org/10.18653/v1/2020.acl-main.531) |  | 0 | Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation. Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure. In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates. We... | Dongdong Zhang, Jian Yang, Ming Zhou, Shuming Ma, Zhoujun Li |  |
| 626 |  |  [Tagged Back-translation Revisited: Why Does It Really Work?](https://doi.org/10.18653/v1/2020.acl-main.532) |  | 0 | In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish... | Atsushi Fujita, Benjamin Marie, Raphael Rubino |  |
| 627 |  |  [Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.533) |  | 0 | Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the... | Alexander H. Liu, Hungyi Lee, ShunPo Chuang, TzuWei Sung |  |
| 628 |  |  [Neural-DINF: A Neural Network based Framework for Measuring Document Influence](https://doi.org/10.18653/v1/2020.acl-main.534) |  | 0 | Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models, which only consider the word frequency change. In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework. Our model has three steps. Firstly, we train the word embeddings for different time periods. Subsequently, we propose an unsupervised method to... | Changlin Yang, Chen Huang, Jie Tan, Siliang Tang, Ying Li, Yueting Zhuang |  |
| 629 |  |  [Paraphrase Generation by Learning How to Edit from Samples](https://doi.org/10.18653/v1/2020.acl-main.535) |  | 0 | Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted... | Amirhossein Kazemnejad, Mahdieh Soleymani Baghshah, Mohammadreza Salehi |  |
| 630 |  |  [Emerging Cross-lingual Structure in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.536) |  | 0 | We study the problem of multilingual masked language modeling, i.e. the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer. We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains. The only requirement is that there are some shared... | Alexis Conneau, Haoran Li, Luke Zettlemoyer, Shijie Wu, Veselin Stoyanov |  |
| 631 |  |  [FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://doi.org/10.18653/v1/2020.acl-main.537) |  | 0 | Pre-trained language models like BERT have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique... | Haotang Deng, Peng Zhou, Qi Ju, Weijie Liu, Zhe Zhao, Zhiruo Wang |  |
| 632 |  |  [Incorporating External Knowledge through Pre-training for Natural Language to Code Generation](https://doi.org/10.18653/v1/2020.acl-main.538) |  | 0 | Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two... | Bogdan Vasilescu, Frank F. Xu, Graham Neubig, Pengcheng Yin, Zhengbao Jiang |  |
| 633 |  |  [LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network](https://doi.org/10.18653/v1/2020.acl-main.539) |  | 0 | Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc. In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking. It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables. This is achieved by a graph... | Daxin Jiang, Duyu Tang, Jiahai Wang, Jian Yin, Linjun Shou, Ming Gong, Ming Zhou, Nan Duan, Wanjun Zhong, Zhangyin Feng |  |
| 634 |  |  [Word-level Textual Adversarial Attacking as Combinatorial Optimization](https://doi.org/10.18653/v1/2020.acl-main.540) |  | 0 | Adversarial attacks are carried out to reveal the vulnerability of deep neural networks. Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input. Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods. However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization... | Chenghao Yang, Fanchao Qi, Maosong Sun, Meng Zhang, Qun Liu, Yuan Zang, Zhiyuan Liu |  |
| 635 |  |  [Benchmarking Multimodal Regex Synthesis with Complex Structures](https://doi.org/10.18653/v1/2020.acl-main.541) |  | 0 | Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse. We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects. First, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from... | Greg Durrett, Isil Dillig, Qiaochu Chen, Xi Ye |  |
| 636 |  |  [Curriculum Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.542) |  | 0 | With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks. At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally. However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach. By... | Benfeng Xu, Hongtao Xie, Licheng Zhang, Quan Wang, Yongdong Zhang, Zhendong Mao |  |
| 637 |  |  [Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?](https://doi.org/10.18653/v1/2020.acl-main.543) |  | 0 | Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and... | Daisuke Bekki, Hitomi Yanaka, Kentaro Inui, Koji Mineshima |  |
| 638 |  |  [Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder](https://doi.org/10.18653/v1/2020.acl-main.544) |  | 0 | Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs. Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation. To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts. Our approach works in an... | Daxin Jiang, Daya Guo, Duyu Tang, Jian Yin, Ming Zhou, Nan Duan |  |
| 639 |  |  [How to Ask Good Questions? Try to Leverage Paraphrases](https://doi.org/10.18653/v1/2020.acl-main.545) |  | 0 | Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications. Inspired by human’s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions. Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method. On the one hand, we... | Wenjie Zhou, Xin Jia, Xu Sun, Yunfang Wu |  |
| 640 |  |  [NeuInfer: Knowledge Inference on N-ary Facts](https://doi.org/10.18653/v1/2020.acl-main.546) |  | 0 | Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications. However, researchers have mainly poured attention to knowledge inference on binary facts. The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world. Therefore, this paper addresses knowledge inference on n-ary facts. We represent each n-ary fact as a primary... | Jiafeng Guo, Saiping Guan, Xiaolong Jin, Xueqi Cheng, Yuanzhuo Wang |  |
| 641 |  |  [Neural Graph Matching Networks for Chinese Short Text Matching](https://doi.org/10.18653/v1/2020.acl-main.547) |  | 0 | Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word... | Boer Lyu, Kai Yu, Lesheng Jin, Lu Chen, Su Zhu, Yanbin Zhao, Zhi Chen |  |
| 642 |  |  [Neural Mixed Counting Models for Dispersed Topic Discovery](https://doi.org/10.18653/v1/2020.acl-main.548) |  | 0 | Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for... | Fu Lee Wang, Haoran Xie, Jiemin Wu, Qing Li, Yanghui Rao, Ziye Chen, Zusheng Zhang |  |
| 643 |  |  [Reasoning Over Semantic-Level Graph for Fact Checking](https://doi.org/10.18653/v1/2020.acl-main.549) |  | 0 | Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to... | Duyu Tang, Jiahai Wang, Jian Yin, Jingjing Xu, Ming Zhou, Nan Duan, Wanjun Zhong, Zenan Xu |  |
| 644 |  |  [Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study](https://doi.org/10.18653/v1/2020.acl-main.550) |  | 0 | In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers. Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A. One big challenge for addressing this task is the lack of training data. Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers. We thus first train an implicit citation extraction model based on BERT and... | Xiaojun Wan, Xiaosheng Fan, Xinyu Xing |  |
| 645 |  |  [Composing Elementary Discourse Units in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.551) |  | 0 | In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion... | Sujian Li, Wenhao Wu, Zhenwen Li |  |
| 646 |  |  [Extractive Summarization as Text Matching](https://doi.org/10.18653/v1/2020.acl-main.552) |  | 0 | This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to semantic matching framework is well-grounded in... | Danqing Wang, Ming Zhong, Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Yiran Chen |  |
| 647 |  |  [Heterogeneous Graph Neural Networks for Extractive Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.553) |  | 0 | As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary... | Danqing Wang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Yining Zheng |  |
| 648 |  |  [Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2020.acl-main.554) |  | 0 | Cross-lingual summarization is the task of generating a summary in one language given a text in a different language. Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data. However, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time. In this paper, we propose to ease the cross-lingual... | Hui Liu, Xiaojun Wan, Yue Cao |  |
| 649 |  |  [Leveraging Graph to Improve Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.555) |  | 0 | Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture... | Haifeng Wang, Hua Wu, Jiachen Liu, Junping Du, Wei Li, Xinyan Xiao |  |
| 650 |  |  [Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.556) |  | 0 | In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves... | Hanqi Jin, Tianming Wang, Xiaojun Wan |  |
| 651 |  |  [Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference](https://doi.org/10.18653/v1/2020.acl-main.557) |  | 0 | We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence. In order to maximally leverage current neural architectures, the model scores each word’s tags in parallel, with minimal task-specific structure. After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time. Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies. | Dan Klein, Nikita Kitaev |  |
| 652 |  |  [Are we Estimating or Guesstimating Translation Quality?](https://doi.org/10.18653/v1/2020.acl-main.558) |  | 0 | Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can... | Francisco Guzmán, Lucia Specia, Shuo Sun |  |
| 653 |  |  [Language (Re)modelling: Towards Embodied Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.559) |  | 0 | While natural language understanding (NLU) is advancing rapidly, today’s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of... | Chen Shani, Dafna Shahaf, Miriam R. L. Petruck, Omri Abend, Ronen Tamari, Tom Hope |  |
| 654 |  |  [The State and Fate of Linguistic Diversity and Inclusion in the NLP World](https://doi.org/10.18653/v1/2020.acl-main.560) |  | 0 | Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between... | Amar Budhiraja, Kalika Bali, Monojit Choudhury, Pratik Joshi, Sebastin Santy |  |
| 655 |  |  [The Unstoppable Rise of Computational Linguistics in Deep Learning](https://doi.org/10.18653/v1/2020.acl-main.561) |  | 0 | In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language... | James Henderson |  |
| 656 |  |  [To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying](https://doi.org/10.18653/v1/2020.acl-main.562) |  | 0 | Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora. In this role they play an important part in making those resources usable for a wider audience. Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details. This paper offers a broad overview of the history of corpora and corpus query tools. It focusses strongly on the query side and hints at exciting directions... | Kerstin Jung, Markus Gärtner |  |
| 657 |  |  [A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://doi.org/10.18653/v1/2020.acl-main.563) |  | 0 | Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs. However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history. Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance.... | Cheng Niu, Fandong Meng, Jie Zhou, Jinchao Zhang, Yang Feng, Yong Shan, Zekang Li |  |
| 658 |  |  [Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight](https://doi.org/10.18653/v1/2020.acl-main.564) |  | 0 | Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm. As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model. However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear. This impedes the learning of those data-driven neural dialogue models. Therefore, effective... | Cheng Zhang, Dawei Yin, Hengyi Cai, Hongshen Chen, Xiaofang Zhao, Yonghao Song |  |
| 659 |  |  [Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog](https://doi.org/10.18653/v1/2020.acl-main.565) |  | 0 | Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that... | Libo Qin, Ting Liu, Wanxiang Che, Xiao Xu, Yue Zhang |  |
| 660 |  |  [Learning Efficient Dialogue Policy from Demonstrations through Shaping](https://doi.org/10.18653/v1/2020.acl-main.566) |  | 0 | Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. Human demonstrations can be used to accelerate learning progress. However, how to effectively leverage demonstrations to learn dialogue policy remains less explored. In this paper, we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. We use an imitation model to distill knowledge from demonstrations,... | Baolin Peng, Huimin Wang, KamFai Wong |  |
| 661 |  |  [SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing](https://doi.org/10.18653/v1/2020.acl-main.567) |  | 0 | Dialogue state tracker is responsible for inferring user intentions through dialogue history. Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information. We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking. Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate... | Chencai Chen, Jiaying Hu, Liang He, Yan Yang, Zhou Yu |  |
| 662 |  |  [Speaker Sensitive Response Evaluation Model](https://doi.org/10.18653/v1/2020.acl-main.568) |  | 0 | Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context. Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth. One approach to resolve this problem is to consider the similarity of the generated response with the conversational context. In this paper, we propose an automatic evaluation model... | Alice Oh, JinYeong Bak |  |
| 663 |  |  [A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure](https://doi.org/10.18653/v1/2020.acl-main.569) |  | 0 | Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years. However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse. In this paper, we justify from both computational and perceptive points-of-view... | Fang Kong, Guodong Zhou, Longyin Zhang, Peifeng Li, Yuqing Xing |  |
| 664 |  |  [Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification](https://doi.org/10.18653/v1/2020.acl-main.570) |  | 0 | An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more. Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI. As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two... | Pratik Dutta, Sriparna Saha |  |
| 665 |  |  [Bipartite Flat-Graph Network for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.571) |  | 0 | In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies. Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones... | Hai Zhao, Ying Luo |  |
| 666 |  |  [Connecting Embeddings for Knowledge Graph Entity Typing](https://doi.org/10.18653/v1/2020.acl-main.572) |  | 0 | Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs. Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. Accordingly, we build two novel... | Anxiang Zhang, Kang Liu, Ruobing Xie, Xiaojie Wang, Yu Zhao |  |
| 667 |  |  [Continual Relation Learning via Episodic Memory Activation and Reconsolidation](https://doi.org/10.18653/v1/2020.acl-main.573) |  | 0 | Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause... | Jie Zhou, Maosong Sun, Peng Li, Tianyu Gao, Xu Han, Yankai Lin, Yi Dai, Zhiyuan Liu |  |
| 668 |  |  [Handling Rare Entities for Neural Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.574) |  | 0 | One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases. Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation. In this work, we propose approaches to address this problem. For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations. For LF entities, we present... | Han Li, Kaisheng Yao, Xiaolong Li, Yangming Li |  |
| 669 |  |  [Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.575) |  | 0 | Interpretable rationales for model predictions play a critical role in practical applications. In this study, we develop models possessing interpretable inference process for structured prediction. Specifically, we present a method of instance-based learning that learns similarities between spans. At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions. Through empirical analysis on... | Hiroki Ouchi, Jun Suzuki, Kentaro Inui, Ryuto Konno, Sho Yokoi, Sosuke Kobayashi, Tatsuki Kuribayashi |  |
| 670 |  |  [MIE: A Medical Information Extractor towards Medical Dialogues](https://doi.org/10.18653/v1/2020.acl-main.576) |  | 0 | Electronic Medical Records (EMRs) have become key components of modern medical care systems. Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation.... | Jiarun Cao, Jun Zhao, Kang Liu, Shengping Liu, Shiwan Liu, Tao Zhang, Yuanzhe Zhang, Zhongtao Jiang |  |
| 671 |  |  [Named Entity Recognition as Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.577) |  | 0 | Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of... | Bernd Bohnet, Juntao Yu, Massimo Poesio |  |
| 672 |  |  [Neighborhood Matching Network for Entity Alignment](https://doi.org/10.18653/v1/2020.acl-main.578) |  | 0 | Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a... | Dongyan Zhao, Xiao Liu, Yansong Feng, Yuting Wu, Zheng Wang |  |
| 673 |  |  [Relation Extraction with Explanation](https://doi.org/10.18653/v1/2020.acl-main.579) |  | 0 | Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their... | Hamed Shahbazi, Prasad Tadepalli, Reza Ghaeini, Xiaoli Z. Fern |  |
| 674 |  |  [Representation Learning for Information Extraction from Form-like Documents](https://doi.org/10.18653/v1/2020.acl-main.580) |  | 0 | We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two... | Bodhisattwa Prasad Majumder, James Bradley Wendt, Marc Najork, Navneet Potti, Qi Zhao, Sandeep Tata |  |
| 675 |  |  [Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language](https://doi.org/10.18653/v1/2020.acl-main.581) |  | 0 | To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data. Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer. However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language. In this... | Biqing Huang, Börje Karlsson, Jianguang Lou, Qianhui Wu, Zijia Lin |  |
| 676 |  |  [Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.582) |  | 0 | Opinion entity extraction is a fundamental task in fine-grained opinion mining. Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them. However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc. In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs. To deal with this task, we propose Synchronous Double-channel Recurrent... | Jie Liu, Shaowei Chen, Wenzheng Zhang, Yu Wang, Ziming Chi |  |
| 677 |  |  [Cross-modal Coherence Modeling for Caption Generation](https://doi.org/10.18653/v1/2020.acl-main.583) |  | 0 | We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning. Using an annotation protocol specifically devised for capturing image–caption coherence relations, we annotate 10,000 instances from publicly-available image–caption pairs. We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also... | Malihe Alikhani, Matthew Stone, Piyush Sharma, Radu Soricut, Shengjie Li |  |
| 678 |  |  [Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms](https://doi.org/10.18653/v1/2020.acl-main.584) |  | 0 | In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts. We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects. Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion. We find that both models consistently outperform a bottom-up baseline that predicts... | Simeon Schüz, Sina Zarrieß |  |
| 679 |  |  [Span-based Localizing Network for Natural Language Video Localization](https://doi.org/10.18653/v1/2020.acl-main.585) |  | 0 | Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query. Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span. In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage. We propose a video span localizing network (VSLNet), on top of the standard... | Aixin Sun, Hao Zhang, Joey Tianyi Zhou, Wei Jing |  |
| 680 |  |  [Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions](https://doi.org/10.18653/v1/2020.acl-main.586) |  | 0 | Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image. We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter. To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic... | Arjun R. Akula, Siva Reddy, SongChun Zhu, Spandana Gella, Yaser AlOnaizan |  |
| 681 |  |  [A Mixture of h - 1 Heads is Better than h Heads](https://doi.org/10.18653/v1/2020.acl-main.587) |  | 0 | Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate... | Dianqi Li, Hao Peng, Noah A. Smith, Roy Schwartz |  |
| 682 |  |  [Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2020.acl-main.588) |  | 0 | Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect. One sentence may contain various sentiments for different aspects. Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge. Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words. But... | Chenliang Li, Donghong Ji, Hao Tang, Qiji Zhou |  |
| 683 |  |  [Differentiable Window for Dynamic Local Attention](https://doi.org/10.18653/v1/2020.acl-main.589) |  | 0 | We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection. While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions. We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways. We evaluate our proposed approach on a myriad of NLP tasks, including machine translation,... | Shafiq R. Joty, ThanhTung Nguyen, Xiaoli Li, XuanPhi Nguyen |  |
| 684 |  |  [Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples](https://doi.org/10.18653/v1/2020.acl-main.590) |  | 0 | Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and... | ChoJui Hsieh, Jiehang Zeng, Minhao Cheng, Xiaoqing Zheng, Xuanjing Huang, Yi Zhou |  |
| 685 |  |  [Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach](https://doi.org/10.18653/v1/2020.acl-main.591) |  | 0 | It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation. Experimental... | Timothy J. O'Donnell, Wenyu Du, Yikang Shen, Yoshua Bengio, Yue Zhang, Zhouhan Lin |  |
| 686 |  |  [Learning Architectures from an Extended Search Space for Language Modeling](https://doi.org/10.18653/v1/2020.acl-main.592) |  | 0 | Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS. In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS). For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously. We implement our model in a differentiable architecture search system.... | Changliang Li, Chi Hu, Jingbo Zhu, Nuo Xu, Tong Xiao, Tongran Liu, Yinqiao Li, Yufan Jiang, Yuhao Zhang |  |
| 687 |  |  [The Right Tool for the Job: Matching Model and Instance Complexities](https://doi.org/10.18653/v1/2020.acl-main.593) |  | 0 | As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores... | Gabriel Stanovsky, Jesse Dodge, Noah A. Smith, Roy Schwartz, Swabha Swayamdipta |  |
| 688 |  |  [Bootstrapping Techniques for Polysynthetic Morphological Analysis](https://doi.org/10.18653/v1/2020.acl-main.594) |  | 0 | Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word. This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies. To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language. We generate data from a finite state... | Steven Bird, William Lane |  |
| 689 |  |  [Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation](https://doi.org/10.18653/v1/2020.acl-main.595) |  | 0 | Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS). Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem. In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS. 1) We rethink the essence of “Chinese words” and design an automatic distant annotation... | Dingkun Long, Guangwei Xu, Haitao Zheng, Muhua Zhu, Ning Ding, Pengjun Xie, Xiaobin Wang |  |
| 690 |  |  [Modeling Morphological Typology for Unsupervised Learning of Language Morphology](https://doi.org/10.18653/v1/2020.acl-main.596) |  | 0 | This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology. By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology. The system is tested on nine typologically and genetically diverse languages, and shows... | Charles Yang, Hongzhi Xu, Jordan Kodner, Mitchell Marcus |  |
| 691 |  |  [Predicting Declension Class from Form and Meaning](https://doi.org/10.18653/v1/2020.acl-main.597) |  | 0 | The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also... | Adina Williams, Arya D. McCarthy, Eleanor Chodroff, Hagen Blix, Ryan Cotterell, Tiago Pimentel |  |
| 692 |  |  [Unsupervised Morphological Paradigm Completion](https://doi.org/10.18653/v1/2020.acl-main.598) |  | 0 | We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological... | Arya McCarthy, Chen Xia, Huiming Jin, Katharina Kann, Liwei Cai, Yihui Peng |  |
| 693 |  |  [Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.599) |  | 0 | Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their... | Bo Zheng, Daxin Jiang, Haoyang Wen, Ming Zhou, Nan Duan, Ting Liu, Wanxiang Che, Yaobo Liang |  |
| 694 |  |  [Harvesting and Refining Question-Answer Pairs for Unsupervised QA](https://doi.org/10.18653/v1/2020.acl-main.600) |  | 0 | Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models. Recent research works have attempted to extend these successes to the settings with few or no labeled data available. In this work, we introduce two approaches to improve unsupervised QA. First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA). Second, we take advantage of the QA model... | Furu Wei, Ke Xu, Li Dong, Wenhui Wang, Zhongli Li |  |
| 695 |  |  [Low-Resource Generation of Multi-hop Reasoning Questions](https://doi.org/10.18653/v1/2020.acl-main.601) |  | 0 | This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance. Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text. Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text. Since the labeled data is limited and insufficient for training, we propose to learn the model... | Jian Yin, Jianxing Yu, Kai Wang, Qinliang Su, Shuang Qiu, Wei Liu, Xiaojun Quan |  |
| 696 |  |  [R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason](https://doi.org/10.18653/v1/2020.acl-main.602) |  | 0 | Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets. This prevents the community from reliably measuring the progress of RC systems. To address this issue, we introduce R4C, a new task for evaluating RC systems’ internal reasoning. R4C requires giving not only answers but also derivations: explanations that justify predicted answers. We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations. We... | Kentaro Inui, Naoya Inoue, Pontus Stenetorp |  |
| 697 |  |  [Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.603) |  | 0 | In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. However, these transformer models can only take as input a fixed-length (e.g., 512) text. To deal with even longer text... | Dian Yu, Dong Yu, Hongyu Gong, Jianshu Chen, Yelong Shen |  |
| 698 |  |  [RikiNet: Reading Wikipedia Pages for Natural Question Answering](https://doi.org/10.18653/v1/2020.acl-main.604) |  | 0 | Reading long documents to answer open-domain questions remains challenging in natural language understanding. In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering. RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor. The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms. The representations are then fed into the predictor to obtain the span of the... | Daxin Jiang, Dayiheng Liu, Jiancheng Lv, Jie Fu, Jiusheng Chen, Nan Duan, Yeyun Gong, Yu Yan |  |
| 699 |  |  [Parsing into Variable-in-situ Logico-Semantic Graphs](https://doi.org/10.18653/v1/2020.acl-main.605) |  | 0 | We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing. The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach. Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available. We demonstrate the effectiveness of this new perspective by... | Weiwei Sun, Yufei Chen |  |
| 700 |  |  [Semantic Parsing for English as a Second Language](https://doi.org/10.18653/v1/2020.acl-main.606) |  | 0 | This paper is concerned with semantic parsing for English as a second language (ESL). Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings. We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model.... | Junjie Cao, Weiwei Sun, Xiaojun Wan, Yuanyuan Zhao |  |
| 701 |  |  [Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders](https://doi.org/10.18653/v1/2020.acl-main.607) |  | 0 | Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations. We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework. Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence. Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent... | Jiong Cai, Kewei Tu, Youmi Ma, Zixia Jia |  |
| 702 |  |  [Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.608) |  | 0 | One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored... | Chen Liu, Chenyu Yang, Kai Yu, Lu Chen, Rao Ma, Ruisheng Cao, Su Zhu, Yanbin Zhao |  |
| 703 |  |  [DRTS Parsing with Structure-Aware Encoding and Decoding](https://doi.org/10.18653/v1/2020.acl-main.609) |  | 0 | Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently. State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem. Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing. In this work, we propose a structural-aware model at... | Jiangming Liu, Meishan Zhang, Qiankun Fu, Yue Zhang |  |
| 704 |  |  [A Two-Stage Masked LM Method for Term Set Expansion](https://doi.org/10.18653/v1/2020.acl-main.610) |  | 0 | We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class. The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples. Previous approaches to the TSE task can be characterized as either distributional or pattern-based. We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches. Due... | Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg |  |
| 705 |  |  [FLAT: Chinese NER Using Flat-Lattice Transformer](https://doi.org/10.18653/v1/2020.acl-main.611) |  | 0 | Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed. In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans. Each span corresponds to a... | Hang Yan, Xiaonan Li, Xipeng Qiu, Xuanjing Huang |  |
| 706 |  |  [Improving Entity Linking through Semantic Reinforced Entity Embeddings](https://doi.org/10.18653/v1/2020.acl-main.612) |  | 0 | Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models. Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities. Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality. We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the... | Feng Hou, Jun He, Ruili Wang, Yi Zhou |  |
| 707 |  |  [Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain](https://doi.org/10.18653/v1/2020.acl-main.613) |  | 0 | We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine... | Pavel Pecina, Shadi Saleh |  |
| 708 |  |  [Learning Robust Models for e-Commerce Product Search](https://doi.org/10.18653/v1/2020.acl-main.614) |  | 0 | Showing items that do not match search query intent degrades customer experience in e-commerce. These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs. Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain. In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier. We... | Karthik Subbian, Nikhil Rao, Thanh V. Nguyen |  |
| 709 |  |  [Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing](https://doi.org/10.18653/v1/2020.acl-main.615) |  | 0 | Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2)... | Clara Meister, Elizabeth Salesky, Ryan Cotterell |  |
| 710 |  |  [Highway Transformer: Self-Gating Enhanced Self-Attentive Networks](https://doi.org/10.18653/v1/2020.acl-main.616) |  | 0 | Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow... | Jin Shuo, Xinwen Hou, Yekun Chai |  |
| 711 |  |  [Low-Dimensional Hyperbolic Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2020.acl-main.617) |  | 0 | Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that... | Adva Wolf, Christopher Ré, DaCheng Juan, Frederic Sala, Ines Chami, Sujith Ravi |  |
| 712 |  |  [Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2020.acl-main.618) |  | 0 | Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure. It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings. In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs. Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process. We show the... | Anna Korhonen, Goran Glavas, Ivan Vulic, Mladen Karan |  |
| 713 |  |  [Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus](https://doi.org/10.18653/v1/2020.acl-main.619) |  | 0 | Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines. This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included. Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities. But... | Beatrice Savoldi, Luisa Bentivogli, Marco Turchi, Matteo Negri, Mattia Antonino Di Gangi, Roldano Cattoni |  |
| 714 |  |  [Uncertainty-Aware Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.620) |  | 0 | Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current... | Baosong Yang, Derek F. Wong, Lidia S. Chao, Yikai Zhou, Yu Wan |  |
| 715 |  |  [Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain](https://doi.org/10.18653/v1/2020.acl-main.621) |  | 0 | Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts. However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks. In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction. In... | Heike Adel, Jannik Strötgen, Lukas Lange |  |
| 716 |  |  [CorefQA: Coreference Resolution as Query-based Span Prediction](https://doi.org/10.18653/v1/2020.acl-main.622) |  | 0 | In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task. We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query. This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving... | Arianna Yuan, Fei Wang, Fei Wu, Jiwei Li, Wei Wu |  |
| 717 |  |  [Estimating predictive uncertainty for rumour verification models](https://doi.org/10.18653/v1/2020.acl-main.623) |  | 0 | The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also... | Elena Kochkina, Maria Liakata |  |
| 718 |  |  [From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains](https://doi.org/10.18653/v1/2020.acl-main.624) |  | 0 | Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data. However, in the above scenario, there exists hardly... | Iryna Gurevych, JanChristoph Klie, Richard Eckart de Castilho |  |
| 719 |  |  [Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions](https://doi.org/10.18653/v1/2020.acl-main.625) |  | 0 | Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks. However, fine-tuning still requires task-specific training data. In this paper, we propose N3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model. N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural... | Alexandre E. Eichenberger, LouisPhilippe Morency, Shengjia Yan, Tian Jin, Zhun Liu |  |
| 720 |  |  [Controlled Crowdsourcing for High-Quality QA-SRL Annotation](https://doi.org/10.18653/v1/2020.acl-main.626) |  | 0 | Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen. Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released. Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation. In this paper, we present an improved crowdsourcing protocol for complex... | Ayal Klein, Daniela Stepanov, Gabriel Stanovsky, Ido Dagan, Jonathan Mamou, Julian Michael, Luke Zettlemoyer, Paul Roit |  |
| 721 |  |  [Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus](https://doi.org/10.18653/v1/2020.acl-main.627) |  | 0 | Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and... | Donghong Ji, Hao Fei, Meishan Zhang |  |
| 722 |  |  [Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity](https://doi.org/10.18653/v1/2020.acl-main.628) |  | 0 | We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings. We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schütze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018). Our sentence meta-embeddings set a new unsupervised State of The Art... | Hinrich Schütze, Nina Pörner, Ulli Waltinger |  |
| 723 |  |  [Transition-based Semantic Dependency Parsing with Pointer Networks](https://doi.org/10.18653/v1/2020.acl-main.629) |  | 0 | Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task. In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing. In addition, we enhance our approach with... | Carlos GómezRodríguez, Daniel FernándezGonzález |  |
| 724 |  |  [tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection](https://doi.org/10.18653/v1/2020.acl-main.630) |  | 0 | Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as BERT. We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a... | Dong Nguyen, Maria Liakata, Nicole Peinelt |  |
| 725 |  |  [Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation](https://doi.org/10.18653/v1/2020.acl-main.631) |  | 0 | Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis. One of the big challenges with this task is the lack of sufficient annotated data. While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly. In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and... | Chengbo Chen, Kun Li, Qing Ling, Xiaojun Quan, Yan Song |  |
| 726 |  |  [Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness](https://doi.org/10.18653/v1/2020.acl-main.632) |  | 0 | Predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising. While clearly relevant to the task, the personal characteristics of an argument’s source and audience have not yet been fully exploited toward automated persuasiveness prediction. In this paper, we model debaters’ prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires. Using a dataset of over 60,000... | Benno Stein, Khalid Al Khatib, Michael Völske, Nikolay Kolyada, Shahbaz Syed |  |
| 727 |  |  [Out of the Echo Chamber: Detecting Countering Debate Speeches](https://doi.org/10.18653/v1/2020.acl-main.633) |  | 0 | An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in “echo chambers” and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns – that of detecting articles that most effectively counter the arguments – and not just the stance – made in a given... | Assaf Toledo, Dan Lahav, Matan Orbach, Michal Jacovi, Noam Slonim, Ranit Aharonov, Yonatan Bilu |  |
| 728 |  |  [Diversifying Dialogue Generation with Non-Conversational Text](https://doi.org/10.18653/v1/2020.acl-main.634) |  | 0 | Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation. As bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective. In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text. Compared with... | Cheng Niu, Hui Su, Jie Zhou, Pengwei Hu, Randy Zhong, Sanqiang Zhao, Xiao Zhou, Xiaoyu Shen |  |
| 729 |  |  [KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation](https://doi.org/10.18653/v1/2020.acl-main.635) |  | 0 | The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations... | Chujie Zheng, Hao Zhou, Kaili Huang, Minlie Huang, Xiaoyan Zhu |  |
| 730 |  |  [Meta-Reinforced Multi-Domain State Generator for Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-main.636) |  | 0 | A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system. Tremendous progress has been made in recent years. However, the major challenges remain. The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task. A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch. In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET). Our first contribution is to improve the DST accuracy. We enhance a... | Junlan Feng, Min Hu, Shuo Ma, Xiaoting Wu, Xiaoyu Du, Yi Huang |  |
| 731 |  |  [Modeling Long Context for Task-Oriented Dialogue State Generation](https://doi.org/10.18653/v1/2020.acl-main.637) |  | 0 | Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly... | Deyi Xiong, Jun Quan |  |
| 732 |  |  [Multi-Domain Dialogue Acts and Response Co-Generation](https://doi.org/10.18653/v1/2020.acl-main.638) |  | 0 | Generating fluent and informative responses is of critical importance for task-oriented dialogue systems. Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation. There are at least two shortcomings with such approaches. First, the inherent structures of multi-domain dialogue acts are neglected. Second, the semantic associations between acts and responses are not taken into account for response generation. To address these issues, we propose a neural... | Jianxing Yu, Junfeng Tian, Kai Wang, Rui Wang, Xiaojun Quan |  |
| 733 |  |  [Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer](https://doi.org/10.18653/v1/2020.acl-main.639) |  | 0 | Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data. In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences. In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer. Specifically, we... | Chulun Zhou, Hua Wu, Jiachen Liu, Jinsong Su, Liangyu Chen, Sheng Guo, Xinyan Xiao |  |
| 734 |  |  [Heterogeneous Graph Transformer for Graph-to-Sequence Learning](https://doi.org/10.18653/v1/2020.acl-main.640) |  | 0 | The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation. Recent studies propose various models to encode graph structure. However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way. In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct... | Shaowei Yao, Tianming Wang, Xiaojun Wan |  |
| 735 |  |  [Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence](https://doi.org/10.18653/v1/2020.acl-main.641) |  | 0 | The neural attention model has achieved great success in data-to-text generation tasks. Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and “hallucination”. Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial. To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences. The segmentation and correspondence are... | Cheng Niu, Dietrich Klakow, Ernie Chang, Hui Su, Xiaoyu Shen |  |
| 736 |  |  [Aligned Dual Channel Graph Convolutional Network for Visual Question Answering](https://doi.org/10.18653/v1/2020.acl-main.642) |  | 0 | Visual question answering aims to answer the natural language question about a given image. Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question. To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages.... | Changmeng Zheng, Hofung Leung, Jielong Wei, Junying Chen, Qing Li, Qingbao Huang, Yi Cai |  |
| 737 |  |  [Multimodal Neural Graph Memory Networks for Visual Question Answering](https://doi.org/10.18653/v1/2020.acl-main.643) |  | 0 | We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image. The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image. The RGCs capture object attributes... | Mahmoud Khademi |  |
| 738 |  |  [Refer360$^\circ$: A Referring Expression Recognition Dataset in 360$^\circ$ Images](https://doi.org/10.18653/v1/2020.acl-main.644) |  | 0 | We propose a novel large-scale referring expression recognition dataset, Refer360°, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes. Refer360° differs from existing related datasets in three ways. First, we propose a more realistic scenario where instructors and the followers have partial, yet dynamic, views of the scene – followers continuously modify their field-of-view (FoV) while interpreting instructions that specify a final target location. Second,... | LouisPhilippe Morency, Taylor BergKirkpatrick, Volkan Cirik |  |
| 739 |  |  [CamemBERT: a Tasty French Language Model](https://doi.org/10.18653/v1/2020.acl-main.645) |  | 0 | Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging,... | Benjamin Muller, Benoît Sagot, Djamé Seddah, Laurent Romary, Louis Martin, Pedro Javier Ortiz Suárez, Yoann Dupont, Éric de la Clergerie |  |
| 740 |  |  [Effective Estimation of Deep Generative Language Models](https://doi.org/10.18653/v1/2020.acl-main.646) |  | 0 | Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively. We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models... | Tom Pelsmaeker, Wilker Aziz |  |
| 741 |  |  [Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection](https://doi.org/10.18653/v1/2020.acl-main.647) |  | 0 | The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to... | Hila Gonen, Michael Twiton, Shauli Ravfogel, Yanai Elazar, Yoav Goldberg |  |
| 742 |  |  [2kenize: Tying Subword Sequences for Chinese Script Conversion](https://doi.org/10.18653/v1/2020.acl-main.648) |  | 0 | Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP. Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters. Here, we propose a model that can disambiguate between mappings and convert between the two scripts. The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences. We... | Isabelle Augenstein, Pranav A |  |
| 743 |  |  [Predicting the Growth of Morphological Families from Social and Linguistic Factors](https://doi.org/10.18653/v1/2020.acl-main.649) |  | 0 | We present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as “trump”, “antitrumpism”, and “detrumpify”, in social media. We introduce the novel task of Morphological Family Expansion Prediction (MFEP) as predicting the increase in the size of a morphological family. We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark. Our experiments demonstrate very good performance on MFEP. | Hinrich Schütze, Janet B. Pierrehumbert, Valentin Hofmann |  |
| 744 |  |  [Semi-supervised Contextual Historical Text Normalization](https://doi.org/10.18653/v1/2020.acl-main.650) |  | 0 | Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017). Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation. By utilizing a... | Peter Makarov, Simon Clematide |  |
| 745 |  |  [ClarQ: A large-scale and diverse dataset for Clarification Question Generation](https://doi.org/10.18653/v1/2020.acl-main.651) |  | 0 | Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework... | Alan W. Black, Vaibhav Kumar |  |
| 746 |  |  [DoQA - Accessing Domain-Specific FAQs via Conversational QA](https://doi.org/10.18653/v1/2020.acl-main.652) |  | 0 | The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we... | Aitor Soroa, Arantxa Otegi, Eneko Agirre, Jan Deriu, Jon Ander Campos, Mark Cieliebak |  |
| 747 |  |  [MLQA: Evaluating Cross-lingual Extractive Question Answering](https://doi.org/10.18653/v1/2020.acl-main.653) |  | 0 | Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to... | Barlas Oguz, Holger Schwenk, Patrick Lewis, Ruty Rinott, Sebastian Riedel |  |
| 748 |  |  [Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering](https://doi.org/10.18653/v1/2020.acl-main.654) |  | 0 | Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations. Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization. To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA. In this framework, we first extend meta learning by... | Di Jin, Hao Zhang, Joey Tianyi Zhou, Ming Yan |  |
| 749 |  |  [Fine-grained Fact Verification with Kernel Graph Attention Network](https://doi.org/10.18653/v1/2020.acl-main.655) |  | 0 | Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct... | Chenyan Xiong, Maosong Sun, Zhenghao Liu, Zhiyuan Liu |  |
| 750 |  |  [Generating Fact Checking Explanations](https://doi.org/10.18653/v1/2020.acl-main.656) |  | 0 | Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and... | Christina Lioma, Isabelle Augenstein, Jakob Grue Simonsen, Pepa Atanasova |  |
| 751 |  |  [Premise Selection in Natural Language Mathematical Texts](https://doi.org/10.18653/v1/2020.acl-main.657) |  | 0 | The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text. The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement. We propose an approach to solve this task as a link prediction problem, using Deep Convolutional... | André Freitas, Deborah Ferreira |  |
| 752 |  |  [A Call for More Rigor in Unsupervised Cross-lingual Learning](https://doi.org/10.18653/v1/2020.acl-main.658) |  | 0 | We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world’s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then... | Dani Yogatama, Eneko Agirre, Gorka Labaka, Mikel Artetxe, Sebastian Ruder |  |
| 753 |  |  [A Tale of a Probe and a Parser](https://doi.org/10.18653/v1/2020.acl-main.659) |  | 0 | Measuring what linguistic information is encoded in neural models of language has become popular in NLP. Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model’s output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit... | Adina Williams, Josef Valvoda, Rowan Hall Maudslay, Ryan Cotterell, Tiago Pimentel |  |
| 754 |  |  [From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?](https://doi.org/10.18653/v1/2020.acl-main.660) |  | 0 | It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs). Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs. We then aim to offer a climax, suggesting that... | Amit Seker, Dan Bareket, Reut Tsarfaty, Stav Klein |  |
| 755 |  |  [Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](https://doi.org/10.18653/v1/2020.acl-main.661) |  | 0 | Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention. This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech... | Matthias Paulik, Matthias Sperber |  |
| 756 |  |  [What Question Answering can Learn from Trivia Nerds](https://doi.org/10.18653/v1/2020.acl-main.662) |  | 0 | In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems. We argue that creating a QA dataset—and the ubiquitous leaderboard that goes with it—closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner. However, the research community has ignored the hard-learned lessons from... | Benjamin Börschinger, Jordan L. BoydGraber |  |
| 757 |  |  [What are the Goals of Distributional Semantics?](https://doi.org/10.18653/v1/2020.acl-main.663) |  | 0 | Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks. However, assessing long-term progress requires explicit long-term goals. In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges. Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them. I conclude that, while linguistic insights can guide the design of model... | Guy Emerson |  |
| 758 |  |  [Improving Image Captioning with Better Use of Caption](https://doi.org/10.18653/v1/2020.acl-main.664) |  | 0 | Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then... | Xiaodan Zhu, Xipeng Qiu, Xu Zhou, Zhan Shi |  |
| 759 |  |  [Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization](https://doi.org/10.18653/v1/2020.acl-main.665) |  | 0 | The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages. In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task. Contrary to the findings of the... | Alexander O'Connor, Henry Elder, Jennifer Foster, Robert Burke |  |
| 760 |  |  [Toward Better Storylines with Sentence-Level Language Models](https://doi.org/10.18653/v1/2020.acl-main.666) |  | 0 | We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably... | Chris CallisonBurch, Daphne Ippolito, David Grangier, Douglas Eck |  |
| 761 |  |  [A Two-Step Approach for Implicit Event Argument Detection](https://doi.org/10.18653/v1/2020.acl-main.667) |  | 0 | In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling... | Eduard H. Hovy, Xiang Kong, Xuezhe Ma, Zhengzhong Liu, Zhisong Zhang |  |
| 762 |  |  [Machine Reading of Historical Events](https://doi.org/10.18653/v1/2020.acl-main.668) |  | 0 | Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities. Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it. Given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from Wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification.... | Lucas Torroba Hennigen, Omri Abend, Or Honovich, Shay B. Cohen |  |
| 763 |  |  [Revisiting Unsupervised Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.669) |  | 0 | Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to... | Phong Le, Sophia Ananiadou, Thy Thy Tran |  |
| 764 |  |  [SciREX: A Challenge Dataset for Document-Level Information Extraction](https://doi.org/10.18653/v1/2020.acl-main.670) |  | 0 | Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses... | Hannaneh Hajishirzi, Iz Beltagy, Madeleine van Zuylen, Sarthak Jain |  |
| 765 |  |  [Contrastive Self-Supervised Learning for Commonsense Reasoning](https://doi.org/10.18653/v1/2020.acl-main.671) |  | 0 | We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called “trigger” words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced... | Moin Nabi, Tassilo Klein |  |
| 766 |  |  [Do Transformers Need Deep Long-Range Memory?](https://doi.org/10.18653/v1/2020.acl-main.672) |  | 0 | Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set... | Ali Razavi, Jack W. Rae |  |
| 767 |  |  [Improving Disentangled Text Representation Learning with Information-Theoretic Guidance](https://doi.org/10.18653/v1/2020.acl-main.673) |  | 0 | Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that... | Christopher Malon, Dinghan Shen, Lawrence Carin, Martin Renqiang Min, Pengyu Cheng, Yitong Li, Yizhe Zhang |  |
| 768 |  |  [Understanding Advertisements with BERT](https://doi.org/10.18653/v1/2020.acl-main.674) |  | 0 | We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task involves detecting the viewer’s interpretation of an Ad image captured as text. Recent results have shown that the embedded scene-text in the image holds a vital cue for this task. Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task. Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data.... | Bhargav Kurma, Kanika Kalra, Manasi Patwardhan, Shirish Karande, Silpa Vadakkeeveetil Sreelatha |  |
| 769 |  |  [Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces](https://doi.org/10.18653/v1/2020.acl-main.675) |  | 0 | We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings. Unlike prior work, it deviates from learning a single global linear projection. InstaMap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point’s nearest neighbours in... | Goran Glavas, Ivan Vulic |  |
| 770 |  |  [Good-Enough Compositional Data Augmentation](https://doi.org/10.18653/v1/2020.acl-main.676) |  | 0 | We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic... | Jacob Andreas |  |
| 771 |  |  [RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers](https://doi.org/10.18653/v1/2020.acl-main.677) |  | 0 | When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and... | Bailin Wang, Matthew Richardson, Oleksandr Polozov, Richard Shin, Xiaodong Liu |  |
| 772 |  |  [Temporal Common Sense Acquisition with Minimal Supervision](https://doi.org/10.18653/v1/2020.acl-main.678) |  | 0 | Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality... | Ben Zhou, Dan Roth, Daniel Khashabi, Qiang Ning |  |
| 773 |  |  [The Sensitivity of Language Models and Humans to Winograd Schema Perturbations](https://doi.org/10.18653/v1/2020.acl-main.679) |  | 0 | Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and... | Anders Søgaard, Desmond Elliott, Maria Barrett, Mostafa Abdou, Vinit Ravishankar, Yonatan Belinkov |  |
| 774 |  |  [Temporally-Informed Analysis of Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.680) |  | 0 | Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text. However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account. We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition. To support these experiments, we introduce a novel data set of English tweets... | Daniel PreotiucPietro, Shruti Rijhwani |  |
| 775 |  |  [Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation](https://doi.org/10.18653/v1/2020.acl-main.681) |  | 0 | We tackle the task of building supervised event trigger identification models which can generalize better across domains. Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance. ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example’s domain. It requires no labeled data from the target domain, making it completely unsupervised. Experiments with two domains (English literature and news) show that... | Aakanksha Naik, Carolyn P. Rosé |  |
| 776 |  |  [CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning](https://doi.org/10.18653/v1/2020.acl-main.682) |  | 0 | Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations. To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose... | Alessandro Suglia, Andrea Vanzo, Desmond Elliott, Emanuele Bastianelli, Ioannis Konstas, Oliver Lemon, Stella Frank |  |
| 777 |  |  [Cross-Modality Relevance for Reasoning on Language and Vision](https://doi.org/10.18653/v1/2020.acl-main.683) |  | 0 | This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR). We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original... | Chen Zheng, Parisa Kordjamshidi, Quan Guo |  |
| 778 |  |  [Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context](https://doi.org/10.18653/v1/2020.acl-main.684) |  | 0 | We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration. Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations. By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the... | Christopher Meek, Nebojsa Jojic, Oleksandr Polozov, Shashank Srivastava |  |
| 779 |  |  [Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning](https://doi.org/10.18653/v1/2020.acl-main.685) |  | 0 | We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way... | Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman |  |
| 780 |  |  [HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-main.686) |  | 0 | Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently... | Chuang Gan, Han Cai, Hanrui Wang, Ligeng Zhu, Song Han, Zhanghao Wu, Zhijian Liu |  |
| 781 |  |  [Hard-Coded Gaussian Attention for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.687) |  | 0 | Recent work has questioned the importance of the Transformer’s multi-headed attention for achieving high translation quality. We push further in this direction by developing a “hard-coded” attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the... | Mohit Iyyer, Simeng Sun, Weiqiu You |  |
| 782 |  |  [In Neural Machine Translation, What Does Transfer Learning Transfer?](https://doi.org/10.18653/v1/2020.acl-main.688) |  | 0 | Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the... | Alham Fikri Aji, Kenneth Heafield, Nikolay Bogoychev, Rico Sennrich |  |
| 783 |  |  [Learning a Multi-Domain Curriculum for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.689) |  | 0 | Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the... | Isaac Caswell, Jiquan Ngiam, Wei Wang, Ye Tian, Yinfei Yang, Zarana Parekh |  |
| 784 |  |  [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://doi.org/10.18653/v1/2020.acl-main.690) |  | 0 | Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a... | Bill Byrne, Danielle Saunders |  |
| 785 |  |  [Translationese as a Language in "Multilingual" NMT](https://doi.org/10.18653/v1/2020.acl-main.691) |  | 0 | Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters. Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish... | David Grangier, Isaac Caswell, Markus Freitag, Parker Riley |  |
| 786 |  |  [Unsupervised Domain Clusters in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.692) |  | 0 | The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data. We harness this property and... | Roee Aharoni, Yoav Goldberg |  |
| 787 |  |  [Using Context in Neural Machine Translation Training Objectives](https://doi.org/10.18653/v1/2020.acl-main.693) |  | 0 | We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents. Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU. Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure. We find that each of these lines of research has a clear space in it for the other, and propose... | Bill Byrne, Danielle Saunders, Felix Stahlberg |  |
| 788 |  |  [Variational Neural Machine Translation with Normalizing Flows](https://doi.org/10.18653/v1/2020.acl-main.694) |  | 0 | Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables. The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy. Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training... | Hendra Setiawan, Matthias Paulik, Matthias Sperber, Udhyakumar Nallasamy |  |
| 789 |  |  [The Paradigm Discovery Problem](https://doi.org/10.18653/v1/2020.acl-main.695) |  | 0 | This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap... | Alexander Erdmann, Micha Elsner, Nizar Habash, Ryan Cotterell, Shijie Wu |  |
| 790 |  |  [Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi](https://doi.org/10.18653/v1/2020.acl-main.696) |  | 0 | Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted). Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis. We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches. We trained our model on a newly-compiled pronunciation lexicon extracted from various... | Aryaman Arora, Luke Gessler, Nathan Schneider |  |
| 791 |  |  [Automated Evaluation of Writing - 50 Years and Counting](https://doi.org/10.18653/v1/2020.acl-main.697) |  | 0 | In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page’s seminal 1966 paper to frame the presentation. We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology. | Beata Beigman Klebanov, Nitin Madnani |  |
| 792 |  |  [Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly](https://doi.org/10.18653/v1/2020.acl-main.698) |  | 0 | Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (‘‘Birds cannot [MASK]”) and non-negated (‘‘Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (‘‘Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to... | Hinrich Schütze, Nora Kassner |  |
| 793 |  |  [On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology](https://doi.org/10.18653/v1/2020.acl-main.699) |  | 0 | The field of natural language processing is experiencing a period of unprecedented growth, and with it a surge of published papers. This represents an opportunity for us to take stock of how we cite the work of other researchers, and whether this growth comes at the expense of “forgetting” about older literature. In this paper, we address this question through bibliographic analysis. By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, we find that there is indeed a... | Desmond Elliott, Marcel Bollmann |  |
| 794 |  |  [Returning the N to NLP: Towards Contextually Personalized Classification Models](https://doi.org/10.18653/v1/2020.acl-main.700) |  | 0 | Most NLP models today treat language as universal, even though socio- and psycholingustic research shows that the communicated message is influenced by the characteristics of the speaker as well as the target audience. This paper surveys the landscape of personalization in natural language processing and related fields, and offers a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings, allowing to flexibly process the “natural” language of each user rather than enforcing a uniform... | Lucie Flek |  |
| 795 |  |  [To Test Machine Comprehension, Start by Defining Comprehension](https://doi.org/10.18653/v1/2020.acl-main.701) |  | 0 | Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult. Rarely, however, do task designers start by considering what systems should in fact comprehend. In this paper we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a widely useful class of texts, namely... | Akash Bharadwaj, David A. Ferrucci, Gregory Burnham, Jennifer ChuCarroll, Jesse Dunietz, Owen Rambow |  |
| 796 |  |  [Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations](https://doi.org/10.18653/v1/2020.acl-main.702) |  | 0 | Disparities in authorship and citations across gender can have substantial adverse consequences not just on the disadvantaged genders, but also on the field of study as a whole. Measuring gender gaps is a crucial step towards addressing them. In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing (1965 to 2019). We determine aggregate-level statistics using an existing manually curated author–gender list as well as first names strongly associated with a... | Saif M. Mohammad |  |
| 797 |  |  [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://doi.org/10.18653/v1/2020.acl-main.703) |  | 0 | We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the... | Abdelrahman Mohamed, Luke Zettlemoyer, Marjan Ghazvininejad, Mike Lewis, Naman Goyal, Omer Levy, Veselin Stoyanov, Yinhan Liu |  |
| 798 |  |  [BLEURT: Learning Robust Metrics for Text Generation](https://doi.org/10.18653/v1/2020.acl-main.704) |  | 0 | Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art... | Ankur P. Parikh, Dipanjan Das, Thibault Sellam |  |
| 799 |  |  [Distilling Knowledge Learned in BERT for Text Generation](https://doi.org/10.18653/v1/2020.acl-main.705) |  | 0 | Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks. However, it remains an open question how to utilize BERT for language generation. In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks. The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance. By leveraging BERT’s... | Jingjing Liu, Jingzhou Liu, YenChun Chen, Yu Cheng, Zhe Gan |  |
| 800 |  |  [ESPRIT: Explaining Solutions to Physical Reasoning Tasks](https://doi.org/10.18653/v1/2020.acl-main.706) |  | 0 | Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training. We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events. We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach. Our framework learns to generate... | Aadit Vyas, Abhijit Gupta, Caiming Xiong, Dragomir R. Radev, Jeremy Weiss, Nazneen Fatema Rajani, Richard Socher, Rui Zhang, Stephan Zheng, Yi Chern Tan |  |
| 801 |  |  [Iterative Edit-Based Unsupervised Sentence Simplification](https://doi.org/10.18653/v1/2020.acl-main.707) |  | 0 | We present a novel iterative, edit-based approach to unsupervised sentence simplification. Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation. Then, we iteratively perform word and phrase-level edits on the complex sentence. Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable. Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised... | Dhruv Kumar, Lili Mou, Lukasz Golab, Olga Vechtomova |  |
| 802 |  |  [Logical Natural Language Generation from Open-Domain Tables](https://doi.org/10.18653/v1/2020.acl-main.708) |  | 0 | Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence. However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table. To facilitate the study of the... | Jianshu Chen, Wenhu Chen, William Yang Wang, Yu Su, Zhiyu Chen |  |
| 803 |  |  [Neural CRF Model for Sentence Alignment in Text Simplification](https://doi.org/10.18653/v1/2020.acl-main.709) |  | 0 | The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles. To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia. We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents... | Chao Jiang, Mounica Maddela, Wei Xu, Wuwei Lan, Yang Zhong |  |
| 804 |  |  [One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases](https://doi.org/10.18653/v1/2020.acl-main.710) |  | 0 | Different texts shall by nature correspond to different number of keyphrases. This desideratum is largely missing from existing neural keyphrase generation models. In this study, we address this problem from both modeling and evaluation perspectives. We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences. Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states. In contrast to previous approaches, our model is capable... | Adam Trischler, Daqing He, Khushboo Thaker, Peter Brusilovsky, Rui Meng, Tong Wang, Xingdi Yuan |  |
| 805 |  |  [R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge](https://doi.org/10.18653/v1/2020.acl-main.711) |  | 0 | We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence. Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener. While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the... | Debanjan Ghosh, Nanyun Peng, Smaranda Muresan, Tuhin Chakrabarty |  |
| 806 |  |  [Structural Information Preserving for Graph-to-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.712) |  | 0 | The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs. As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs. We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information. In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a. views) of input... | Ante Wang, Dong Yu, Jinsong Su, Kun Xu, Linfeng Song, Yubin Ge, Yue Zhang |  |
| 807 |  |  [A Joint Neural Model for Information Extraction with Global Features](https://doi.org/10.18653/v1/2020.acl-main.713) |  | 0 | Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE... | Fei Huang, Heng Ji, Lingfei Wu, Ying Lin |  |
| 808 |  |  [Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding](https://doi.org/10.18653/v1/2020.acl-main.714) |  | 0 | Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on... | Claire Cardie, Xinya Du |  |
| 809 |  |  [Exploiting the Syntax-Model Consistency for Neural Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.715) |  | 0 | This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text. In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. The... | Amir Pouran Ben Veyseh, Dejing Dou, Franck Dernoncourt, Thien Huu Nguyen |  |
| 810 |  |  [From English to Code-Switching: Transfer Learning with Strong Morphological Clues](https://doi.org/10.18653/v1/2020.acl-main.716) |  | 0 | Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing. The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular. This is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms. In this paper, we aim at adapting monolingual models to code-switched text in various tasks. Specifically, we transfer English knowledge from a pre-trained ELMo... | Gustavo Aguilar, Thamar Solorio |  |
| 811 |  |  [Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts](https://doi.org/10.18653/v1/2020.acl-main.717) |  | 0 | Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs. The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and... | Jingyuan Zhang, Mingming Sun, Ping Li, Yue Feng |  |
| 812 |  |  [Multi-Sentence Argument Linking](https://doi.org/10.18653/v1/2020.acl-main.718) |  | 0 | We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets. | Benjamin Van Durme, Kyle Rawlins, Patrick Xia, Ryan Culkin, Seth Ebner |  |
| 813 |  |  [Rationalizing Medical Relation Prediction from Corpus-level Statistics](https://doi.org/10.18653/v1/2020.acl-main.719) |  | 0 | Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain. Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition. Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated... | Huan Sun, Jennifer Lee, Simon M. Lin, Zhen Wang |  |
| 814 |  |  [Sources of Transfer in Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.720) |  | 0 | Named-entities are inherently multilingual, and annotations in any given language may be limited. This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language. However, a straightforward implementation of this simple idea does not always work in practice: naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more... | David Mueller, Mark Dredze, Nicholas Andrews |  |
| 815 |  |  [ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages](https://doi.org/10.18653/v1/2020.acl-main.721) |  | 0 | In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color. Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template. In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen... | Colin Lockard, Hannaneh Hajishirzi, Prashant Shiralkar, Xin Luna Dong |  |
| 816 |  |  [Soft Gazetteers for Low-Resource Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.722) |  | 0 | Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of “soft gazetteers” that... | Graham Neubig, Jaime G. Carbonell, Shruti Rijhwani, Shuyan Zhou |  |
| 817 |  |  [A Prioritization Model for Suicidality Risk Assessment](https://doi.org/10.18653/v1/2020.acl-main.723) |  | 0 | We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available. Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts. | Douglas W. Oard, HanChin Shing, Philip Resnik |  |
| 818 |  |  [CluHTM - Semantic Hierarchical Topic Modeling based on CluWords](https://doi.org/10.18653/v1/2020.acl-main.724) |  | 0 | Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the “ideal” number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic... | Antônio Pereira De Souza Júnior, Christian Gomes, Felipe Viegas, Leonardo Rocha, Marcos André Gonçalves, Washington Cunha |  |
| 819 |  |  [Empower Entity Set Expansion via Language Model Probing](https://doi.org/10.18653/v1/2020.acl-main.725) |  | 0 | Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics... | Jiaming Shen, Jiawei Han, Jingbo Shang, Yunyi Zhang |  |
| 820 |  |  [Feature Projection for Improved Text Classification](https://doi.org/10.18653/v1/2020.acl-main.726) |  | 0 | In classification, there are usually some good features that are indicative of class labels. For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment. However, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification). Although deep learning has made... | Bing Liu, Qi Qin, Wenpeng Hu |  |
| 821 |  |  [A negative case analysis of visual grounding methods for VQA](https://doi.org/10.18653/v1/2020.acl-main.727) |  | 0 | Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons. To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains. However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to... | Christopher Kanan, Kushal Kafle, Robik Shrestha |  |
| 822 |  |  [History for Visual Dialog: Do we really need it?](https://doi.org/10.18653/v1/2020.acl-main.728) |  | 0 | Visual Dialogue involves “understanding” the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response. In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don’t, achieving state-of-the-art performance (72 % NDCG on val set). However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue... | Ioannis Konstas, JoonYoung Lee, Shubham Agarwal, Trung Bui, Verena Rieser |  |
| 823 |  |  [Mapping Natural Language Instructions to Mobile UI Action Sequences](https://doi.org/10.18653/v1/2020.acl-main.729) |  | 0 | We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it. For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator. To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces. We use a Transformer to extract action... | Jason Baldridge, Jiacong He, Xin Zhou, Yang Li, Yuan Zhang |  |
| 824 |  |  [TVQA+: Spatio-Temporal Grounding for Video Question Answering](https://doi.org/10.18653/v1/2020.acl-main.730) |  | 0 | We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that... | Jie Lei, Licheng Yu, Mohit Bansal, Tamara L. Berg |  |
| 825 |  |  [Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](https://doi.org/10.18653/v1/2020.acl-main.731) |  | 0 | Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space... | Alexander G. Hauptmann, Junjie Hu, PoYao Huang, Xiaojun Chang |  |
| 826 |  |  [A Multitask Learning Approach for Diacritic Restoration](https://doi.org/10.18653/v1/2020.acl-main.732) |  | 0 | In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings. Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word. This results in a more ambiguous text making computational processing on such text more difficult. Diacritic restoration is the task of restoring missing diacritics in the written text. Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the... | Ajay Mishra, Mona T. Diab, Sawsan Alqahtani |  |
| 827 |  |  [Frugal Paradigm Completion](https://doi.org/10.18653/v1/2020.acl-main.733) |  | 0 | Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive. We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible. It induces typological information during training which it uses to determine the best sources at test time. We evaluate our language-agnostic approach on 7 diverse languages. Compared to popular alternative approaches, ours... | Alexander Erdmann, Christian Schallhart, Markus Becker, Tom Kenter |  |
| 828 |  |  [Improving Chinese Word Segmentation with Wordhood Memory Networks](https://doi.org/10.18653/v1/2020.acl-main.734) |  | 0 | Contextual features always play an important role in Chinese word segmentation (CWS). Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters. However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks. In this paper, we therefore propose a neural framework, WMSeg, which uses memory... | Fei Xia, Tong Zhang, Yan Song, Yonggang Wang, Yuanhe Tian |  |
| 829 |  |  [Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge](https://doi.org/10.18653/v1/2020.acl-main.735) |  | 0 | Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks. Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models. However, for many cases, the joint tagging needs not only modeling from context features but also... | Fei Xia, Tong Zhang, Xiang Ao, Xiaojun Quan, Yan Song, Yonggang Wang, Yuanhe Tian |  |
| 830 |  |  [Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging](https://doi.org/10.18653/v1/2020.acl-main.736) |  | 0 | The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma. This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography. The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others. Joint modeling of the lexicalized and non-lexicalized... | Nasser Zalmout, Nizar Habash |  |
| 831 |  |  [Phonetic and Visual Priors for Decipherment of Informal Romanization](https://doi.org/10.18653/v1/2020.acl-main.737) |  | 0 | Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages—namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from... | Maria Ryskina, Matthew R. Gormley, Taylor BergKirkpatrick |  |
| 832 |  |  [Active Learning for Coreference Resolution using Discrete Annotation](https://doi.org/10.18653/v1/2020.acl-main.738) |  | 0 | We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent. This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget. In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant... | Belinda Z. Li, Gabriel Stanovsky, Luke Zettlemoyer |  |
| 833 |  |  [Beyond Possession Existence: Duration and Co-Possession](https://doi.org/10.18653/v1/2020.acl-main.739) |  | 0 | This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time. We present new annotations on top of corpora annotating possession existence and experimental results. Regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds. Regarding co-possessions, we use a binary label. Cohen’s kappa coefficients indicate substantial agreement, and... | Dhivya Chinnappa, Eduardo Blanco, Srikala Murugan |  |
| 834 |  |  [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://doi.org/10.18653/v1/2020.acl-main.740) |  | 0 | Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and... | Ana Marasovic, Doug Downey, Iz Beltagy, Kyle Lo, Noah A. Smith, Suchin Gururangan, Swabha Swayamdipta |  |
| 835 |  |  [Estimating Mutual Information Between Dense Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.741) |  | 0 | Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks. Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity). Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression.... | Aleksandar Savkov, Nils Hammerla, Vitalii Zhelezniak |  |
| 836 |  |  [Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.742) |  | 0 | We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training. Recently, several datasets, including Spider, were proposed to support development of XSP systems. We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use. We re-purpose eight semantic parsing datasets that have been well-studied in the setting... | Alane Suhr, Kenton Lee, MingWei Chang, Peter Shaw |  |
| 837 |  |  [Predicting the Focus of Negation: Model and Error Analysis](https://doi.org/10.18653/v1/2020.acl-main.743) |  | 0 | The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances. In this paper, we experiment with neural networks to predict the focus of negation. Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network. Experimental results show that doing so obtains the best results to date. Additionally, we perform a detailed error analysis providing insights into the main error... | Alexis Palmer, Eduardo Blanco, Kathleen E. Hamilton, Md Mosharaf Hossain |  |
| 838 |  |  [Structured Tuning for Semantic Role Labeling](https://doi.org/10.18653/v1/2020.acl-main.744) |  | 0 | Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores. These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models. Introducing the benefits of structure to inform neural models presents a methodological challenge. In this paper, we present a structured tuning framework to improve models using softened constraints only at training time.... | Martha Palmer, Parth Anand Jawale, Tao Li, Vivek Srikumar |  |
| 839 |  |  [TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data](https://doi.org/10.18653/v1/2020.acl-main.745) |  | 0 | Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is... | Graham Neubig, Pengcheng Yin, Sebastian Riedel, Wentau Yih |  |
| 840 |  |  [Universal Decompositional Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.746) |  | 0 | We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores. We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction. By analyzing the attribute prediction errors, we find the model captures... | Aaron Steven White, Benjamin Van Durme, Elias StengelEskin, Sheng Zhang |  |
| 841 |  |  [Unsupervised Cross-lingual Representation Learning at Scale](https://doi.org/10.18653/v1/2020.acl-main.747) |  | 0 | This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R... | Alexis Conneau, Edouard Grave, Francisco Guzmán, Guillaume Wenzek, Kartikay Khandelwal, Luke Zettlemoyer, Myle Ott, Naman Goyal, Veselin Stoyanov, Vishrav Chaudhary |  |
| 842 |  |  [A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization](https://doi.org/10.18653/v1/2020.acl-main.748) |  | 0 | Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large. In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology. In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT. The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any... | Dongfang Xu, Steven Bethard, Zeyu Zhang |  |
| 843 |  |  [Hierarchical Entity Typing via Multi-level Learning to Rank](https://doi.org/10.18653/v1/2020.acl-main.749) |  | 0 | We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction. At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree. During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s). Our approach significantly outperform prior work on strict accuracy, demonstrating the... | Benjamin Van Durme, Tongfei Chen, Yunmo Chen |  |
| 844 |  |  [Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference](https://doi.org/10.18653/v1/2020.acl-main.750) |  | 0 | Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input. However, domain transfer of NER models with data from multiple genres has not been widely studied. To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training. We introduce a new architecture tailored to this task... | Daniel PreotiucPietro, Jing Wang, Mayank Kulkarni |  |
| 845 |  |  [TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories](https://doi.org/10.18653/v1/2020.acl-main.751) |  | 0 | Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and... | Giannis Karamanolakis, Jun Ma, Xin Luna Dong |  |
| 846 |  |  [TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.752) |  | 0 | Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect. Thus, a crucial research question is how to obtain supervision in a cost-effective way. In this paper, we introduce “entity triggers,” an effective proxy of human explanations for facilitating label-efficient learning of NER models. An entity trigger is defined as a group of words in a sentence that helps... | Bill Yuchen Lin, DongHo Lee, Ming Shen, Prashant Shiralkar, Ryan Moreno, Xiang Ren, Xiao Huang |  |
| 847 |  |  [Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.753) |  | 0 | This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the... | Arya D. McCarthy, Jiatao Gu, Ning Dong, Xian Li |  |
| 848 |  |  [Balancing Training for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.754) |  | 0 | When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize... | Graham Neubig, Xinyi Wang, Yulia Tsvetkov |  |
| 849 |  |  [Evaluating Robustness to Input Perturbations for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.755) |  | 0 | Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness... | Georgiana Dinu, Prashant Mathur, Xing Niu, Yaser AlOnaizan |  |
| 850 |  |  [Parallel Corpus Filtering via Pre-trained Language Models](https://doi.org/10.18653/v1/2020.acl-main.756) |  | 0 | Web-crawled data provides a good source of parallel corpora for training machine translation models. It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. We measure sentence parallelism by leveraging the multilingual capability of BERT and use the... | Ajay Nagesh, Boliang Zhang, Kevin Knight |  |
| 851 |  |  [Regularized Context Gates on Transformer for Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.757) |  | 0 | Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate... | Guoping Huang, Lemao Liu, Max Meng, Rui Wang, Xintong Li |  |
| 852 |  |  [A Multi-Perspective Architecture for Semantic Code Search](https://doi.org/10.18653/v1/2020.acl-main.758) |  | 0 | The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories. In this paper, we propose a novel multi-perspective cross-lingual neural framework for code–text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities. Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual... | Jinjun Xiong, Julia Hockenmaier, Lingfei Wu, Rajarshi Haldar |  |
| 853 |  |  [Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring](https://doi.org/10.18653/v1/2020.acl-main.759) |  | 0 | While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision. However, a neural AES typically does not provide useful feature representations for supporting AWE. This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers. We evaluate performance using a feature-based AES requiring TCs.... | Diane J. Litman, Haoran Zhang |  |
| 854 |  |  [Clinical Concept Linking with Contextualized Neural Representations](https://doi.org/10.18653/v1/2020.acl-main.760) |  | 0 | In traditional approaches to entity linking, linking decisions are based on three sources of information – the similarity of the mention string to an entity’s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB). In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity. We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical... | Andriy Mulyar, Elliot Schumacher, Mark Dredze |  |
| 855 |  |  [DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking](https://doi.org/10.18653/v1/2020.acl-main.761) |  | 0 | The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence. The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction. We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions, temporal reasoning,... | Christopher Hidey, Kriste Krstovski, Mona T. Diab, Siddharth Varia, Smaranda Muresan, Tariq Alhindi, Tuhin Chakrabarty |  |
| 856 |  |  [Let Me Choose: From Verbal Context to Font Selection](https://doi.org/10.18653/v1/2020.acl-main.762) |  | 0 | In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to. Compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document. We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing. Due to the subjective nature of the task,... | Amirreza Shirani, Franck Dernoncourt, Jose Echevarria, Nedim Lipka, Paul Asente, Thamar Solorio |  |
| 857 |  |  [Multi-Label and Multilingual News Framing Analysis](https://doi.org/10.18653/v1/2020.acl-main.763) |  | 0 | News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation. In NLP, although recent works have studied framing in English news, few have studied how the analysis can be extended to other languages and in a multi-label setting. In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language. We propose a... | Afra Feyza Akyürek, Derry Tanti Wijaya, Lei Guo, Margrit Betke, Prakash Ishwar, Randa I. Elanwar |  |
| 858 |  |  [Predicting Performance for Natural Language Processing Tasks](https://doi.org/10.18653/v1/2020.acl-main.764) |  | 0 | Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model. To do so, we build regression models to predict the evaluation score of an NLP experiment given... | Antonios Anastasopoulos, Graham Neubig, Mengzhou Xia, Ruochen Xu, Yiming Yang |  |
| 859 |  |  [ScriptWriter: Narrative-Guided Script Generation](https://doi.org/10.18653/v1/2020.acl-main.765) |  | 0 | It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach. In dialogue systems, it would also be useful to drive dialogues by a dialogue plan. In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative. The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative. It keeps track of what in the narrative has been said and what... | JianYun Nie, Jin Zhou, Ruihua Song, Yutao Zhu, Zhicheng Dou |  |
| 860 |  |  [Should All Cross-Lingual Embeddings Speak English?](https://doi.org/10.18653/v1/2020.acl-main.766) |  | 0 | Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard... | Antonios Anastasopoulos, Graham Neubig |  |
| 861 |  |  [Smart To-Do: Automatic Generation of To-Do Items from Emails](https://doi.org/10.18653/v1/2020.acl-main.767) |  | 0 | Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks. In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails. We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action. We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence... | Ahmed Hassan Awadallah, Marcello Hasegawa, Ryen White, Subhabrata Mukherjee, Sudipto Mukherjee |  |
| 862 |  |  [Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition](https://doi.org/10.18653/v1/2020.acl-main.768) |  | 0 | Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on... | Adina Williams, Alex Warstadt, Paloma Jeretic, Suvrat Bhooshan |  |
| 863 |  |  [End-to-End Bias Mitigation by Modelling Biases in Corpora](https://doi.org/10.18653/v1/2020.acl-main.769) |  | 0 | Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios. We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets. The biases are specified in terms of one or more bias-only models, which learn to... | James Henderson, Rabeeh Karimi Mahabadi, Yonatan Belinkov |  |
| 864 |  |  [Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance](https://doi.org/10.18653/v1/2020.acl-main.770) |  | 0 | Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us... | Iryna Gurevych, Nafise Sadat Moosavi, Prasetya Ajie Utama |  |
| 865 |  |  [NILE : Natural Language Inference with Faithful Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.771) |  | 0 | The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model’s internal decision making. In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high... | Partha P. Talukdar, Sawan Kumar |  |
| 866 |  |  [QuASE: Question-Answer Driven Sentence Encoding](https://doi.org/10.18653/v1/2020.acl-main.772) |  | 0 | Question-answering (QA) data often encodes essential information in many facets. This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the question-answer driven sentence encoding (QuASE) framework. QuASE learns representations from QA data, using BERT or other state-of-the-art... | Dan Roth, Hangfeng He, Qiang Ning |  |
| 867 |  |  [Towards Robustifying NLI Models Against Lexical Dataset Biases](https://doi.org/10.18653/v1/2020.acl-main.773) |  | 0 | While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics. Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases. First, we debias the dataset through data augmentation and... | Mohit Bansal, Xiang Zhou |  |
| 868 |  |  [Uncertain Natural Language Inference](https://doi.org/10.18653/v1/2020.acl-main.774) |  | 0 | We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments. We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise. We describe a direct scalar regression modeling... | Adam Poliak, Benjamin Van Durme, Keisuke Sakaguchi, Tongfei Chen, Zhengping Jiang |  |
| 869 |  |  [Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches](https://doi.org/10.18653/v1/2020.acl-main.775) |  | 0 | An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (“Wells Fargo”) and dates (“July 5, 2020”) as well as certain productive constructions (“blow for blow”, “day after day”). Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same... | Lillian Lee, Tianze Shi |  |
| 870 |  |  [Revisiting Higher-Order Dependency Parsers](https://doi.org/10.18653/v1/2020.acl-main.776) |  | 0 | Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers. This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree. We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT. While the gains of... | André F. T. Martins, Erick Rocha Fonseca |  |
| 871 |  |  [SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.777) |  | 0 | Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings. It is effective and can be easily adopted on lots of image classification and text classification tasks. However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF). CRF can significantly boost accuracy for sequence models by... | Jianhua Lu, Luoxin Chen, Weitong Ruan, Xinyue Liu |  |
| 872 |  |  [Treebank Embedding Vectors for Out-of-domain Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.778) |  | 0 | A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time. We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined... | James Barry, Jennifer Foster, Joachim Wagner |  |
