# ACL2020

## 会议论文列表

本会议共有 872 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Adaptive Transformers for Learning Multimodal Representations](https://doi.org/10.18653/v1/2020.acl-srw.1) |  | 0 | The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena. | Prajjwal Bhargava |  |
| 2 |  |  [Story-level Text Style Transfer: A Proposal](https://doi.org/10.18653/v1/2020.acl-srw.2) |  | 0 | Text style transfer aims to change the style of the input text to the target style while preserving the content to some extent. Previous works on this task are on the sentence level. We aim to work on story-level text style transfer to generate stories that preserve the plot of the input story while exhibiting a strong target style. The challenge in this task compared to previous work is that the structure of the input story, consisting of named entities and their relations with each other, needs to be preserved, and that the generated story needs to be consistent after adding flavors. We plan to explore three methods including the BERT-based method, the Story Realization method, and the Graph-based method. | Yusu Qian |  |
| 3 |  |  [Unsupervised Paraphasia Classification in Aphasic Speech](https://doi.org/10.18653/v1/2020.acl-srw.3) |  | 0 | Aphasia is a speech and language disorder which results from brain damage, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia). Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods cant be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel unsupervised method which can be implemented without the need for labeled paraphasia data. Our evaluations show that our method outperforms previous work based on supervised learning and transfer learning approaches for English. We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from aphasia in any language. | Sharan Pai, Nikhil Sachdeva, Prince Sachdeva, Rajiv Ratn Shah |  |
| 4 |  |  [HGCN4MeSH: Hybrid Graph Convolution Network for MeSH Indexing](https://doi.org/10.18653/v1/2020.acl-srw.4) |  | 0 | Recently deep learning has been used in Medical subject headings (MeSH) indexing to reduce the time and monetary cost by manual annotation, including DeepMeSH, TextCNN, etc. However, these models still suffer from failing to capture the complex correlations between MeSH terms. To this end, we introduce Graph Convolution Network (GCN) to learn the relationship between these terms, and present a novel Hybrid Graph Convolution Net for MeSH index (HGCN4MeSH). Basically, we utilize two BiGRUs to learn the embedding representation of the abstract and the title of the MeSH index text respectively. At the same time, we establish the adjacency matrix of MeSH terms based on the co-occurrence relationships in Corpus, which is easy to apply for GCN representation learning. On the basis of learning the mixed representation, the prediction problem of the MeSH index keywords is transformed into an extreme multi-label classification problem after the attention layer operation. Experimental results on two datasets show that HGCN4MeSH is competitive compared with the state-of-the-art methods. | Miaomiao Yu, Yujiu Yang, Chenhui Li |  |
| 5 |  |  [Grammatical Error Correction Using Pseudo Learner Corpus Considering Learner's Error Tendency](https://doi.org/10.18653/v1/2020.acl-srw.5) |  | 0 | Recently, several studies have focused on improving the performance of grammatical error correction (GEC) tasks using pseudo data. However, a large amount of pseudo data are required to train an accurate GEC model. To address the limitations of language and computational resources, we assume that introducing pseudo errors into sentences similar to those written by the language learners is more efficient, rather than incorporating random pseudo errors into monolingual data. In this regard, we study the effect of pseudo data on GEC task performance using two approaches. First, we extract sentences that are similar to the learners’ sentences from monolingual data. Second, we generate realistic pseudo errors by considering error types that learners often make. Based on our comparative results, we observe that F0.5 scores for the Russian GEC task are significantly improved. | Yujin Takahashi, Satoru Katsumata, Mamoru Komachi |  |
| 6 |  |  [Research on Task Discovery for Transfer Learning in Deep Neural Networks](https://doi.org/10.18653/v1/2020.acl-srw.6) |  | 0 | Deep neural network based machine learning models are shown to perform poorly on unseen or out-of-domain examples by numerous recent studies. Transfer learning aims to avoid overfitting and to improve generalizability by leveraging the information obtained from multiple tasks. Yet, the benefits of transfer learning depend largely on task selection and finding the right method of sharing. In this thesis, we hypothesize that current deep neural network based transfer learning models do not achieve their fullest potential for various tasks and there are still many task combinations that will benefit from transfer learning that are not considered by the current models. To this end, we started our research by implementing a novel multi-task learner with relaxed annotated data requirements and obtained a performance improvement on two NLP tasks. We will further devise models to tackle tasks from multiple areas of machine learning, such as Bioinformatics and Computer Vision, in addition to NLP. | Arda Akdemir |  |
| 7 |  |  [RPD: A Distance Function Between Word Embeddings](https://doi.org/10.18653/v1/2020.acl-srw.7) |  | 0 | It is well-understood that different algorithms, training processes, and corpora produce different word embeddings. However, less is known about the relation between different embedding spaces, i.e. how far different sets of em-beddings deviate from each other. In this paper, we propose a novel metric called Relative Pairwise Inner Product Distance (RPD) to quantify the distance between different sets of word embeddings. This unitary-invariant metric has a unified scale for comparing different sets of word embeddings. Based on the properties of RPD, we study the relations of word embeddings of different algorithms systematically and investigate the influence of different training processes and corpora. The results shed light on the poorly understood word embeddings and justify RPD as a measure of the distance of embedding space. | Xuhui Zhou, Shujian Huang, Zaixiang Zheng |  |
| 8 |  |  [Reflection-based Word Attribute Transfer](https://doi.org/10.18653/v1/2020.acl-srw.8) |  | 0 | Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word’s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes. | Yoichi Ishibashi, Katsuhito Sudoh, Koichiro Yoshino, Satoshi Nakamura |  |
| 9 |  |  [Topic Balancing with Additive Regularization of Topic Models](https://doi.org/10.18653/v1/2020.acl-srw.9) |  | 0 | This article proposes a new approach for building topic models on unbalanced collections in topic modelling, based on the existing methods and our experiments with such methods. Real-world data collections contain topics in various proportions, and often documents of the relatively small theme become distributed all over the larger topics instead of being grouped into one topic. To address this issue, we design a new regularizer for Theta and Phi matrices in probabilistic Latent Semantic Analysis (pLSA) model. We make sure this regularizer increases the quality of topic models, trained on unbalanced collections. Besides, we conceptually support this regularizer by our experiments. | Eugenia Veselova, Konstantin V. Vorontsov |  |
| 10 |  |  [Combining Subword Representations into Word-level Representations in the Transformer Architecture](https://doi.org/10.18653/v1/2020.acl-srw.10) |  | 0 | In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies. We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information. Our experiments show that this approach maintains the translation quality with respect to the normal Transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies. | Noe Casas, Marta R. Costajussà, José A. R. Fonollosa |  |
| 11 |  |  [Zero-shot North Korean to English Neural Machine Translation by Character Tokenization and Phoneme Decomposition](https://doi.org/10.18653/v1/2020.acl-srw.11) |  | 0 | The primary limitation of North Korean to English translation is the lack of a parallel corpus; therefore, high translation accuracy cannot be achieved. To address this problem, we propose a zero-shot approach using South Korean data, which are remarkably similar to North Korean data. We train a neural machine translation model after tokenizing a South Korean text at the character level and decomposing characters into phonemes. We demonstrate that our method can effectively learn North Korean to English translation and improve the BLEU scores by +1.01 points in comparison with the baseline. | Hwichan Kim, Tosho Hirasawa, Mamoru Komachi |  |
| 12 |  |  [Media Bias, the Social Sciences, and NLP: Automating Frame Analyses to Identify Bias by Word Choice and Labeling](https://doi.org/10.18653/v1/2020.acl-srw.12) |  | 0 | Media bias can strongly impact the public perception of topics reported in the news. A difficult to detect, yet powerful form of slanted news coverage is called bias by word choice and labeling (WCL). WCL bias can occur, for example, when journalists refer to the same semantic concept by using different terms that frame the concept differently and consequently may lead to different assessments by readers, such as the terms “freedom fighters” and “terrorists,” or “gun rights” and “gun control.” In this research project, I aim to devise methods that identify instances of WCL bias and estimate the frames they induce, e.g., not only is “terrorists” of negative polarity but also ascribes to aggression and fear. To achieve this, I plan to research methods using natural language processing and deep learning while employing models and using analysis concepts from the social sciences, where researchers have studied media bias for decades. The first results indicate the effectiveness of this interdisciplinary research approach. My vision is to devise a system that helps news readers to become aware of the differences in media coverage caused by bias. | Felix Hamborg |  |
| 13 |  |  [SCAR: Sentence Compression using Autoencoders for Reconstruction](https://doi.org/10.18653/v1/2020.acl-srw.13) |  | 0 | Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is primarily composed of two encoder-decoder pairs: a compressor and a reconstructor. The compressor masks the input, and the reconstructor tries to regenerate it. The model is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. SCAR’s merit lies in the novel Linkage Loss function, which correlates the compressor and its effect on reconstruction, guiding it to drop inferable tokens. SCAR achieves higher ROUGE scores on benchmark datasets than the existing state-of-the-art methods and baselines. We also conduct a user study to demonstrate the application of our model as a text highlighting system. Using our model to underscore salient information facilitates speed-reading and reduces the time required to skim a document. | Chanakya Malireddy, Tirth Maniar, Manish Shrivastava |  |
| 14 |  |  [Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information](https://doi.org/10.18653/v1/2020.acl-srw.14) |  | 0 | Medical image captioning can reduce the workload of physicians and save time and expense by automatically generating reports. However, current datasets are small and limited, creating additional challenges for researchers. In this study, we propose a feature difference and tag information combined long short-term memory (LSTM) model for chest x-ray report generation. A feature vector extracted from the image conveys visual information, but its ability to describe the image is limited. Other image captioning studies exhibited improved performance by exploiting feature differences, so the proposed model also utilizes them. First, we propose a difference and tag (DiTag) model containing the difference between the patient and normal images. Then, we propose a multi-difference and tag (mDiTag) model that also contains information about low-level differences, such as contrast, texture, and localized area. Evaluation of the proposed models demonstrates that the mDiTag model provides more information to generate captions and outperforms all other models. | Hyeryun Park, Kyungmo Kim, Jooyoung Yoon, Seongkeun Park, Jinwook Choi |  |
| 15 |  |  [Multi-Task Neural Model for Agglutinative Language Translation](https://doi.org/10.18653/v1/2020.acl-srw.15) |  | 0 | Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data. | Yirong Pan, Xiao Li, Yating Yang, Rui Dong |  |
| 16 |  |  [Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling](https://doi.org/10.18653/v1/2020.acl-srw.16) |  | 0 | Recently, state-of-the-art NLP models gained an increasing syntactic and semantic understanding of language, and explanation methods are crucial to understand their decisions. Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model’s decision. We argue that current occlusion-based methods often produce invalid or syntactically incorrect language data, neglecting the improved abilities of recent NLP models. Furthermore, gradient-based explanation methods disregard the discrete distribution of data in NLP. Thus, we propose OLM: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood, given the context of the original input. We lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in NLP and provide results that underline the importance of considering data likelihood in occlusion-based explanation. | David Harbecke, Christoph Alt |  |
| 17 |  |  [Non-Topical Coherence in Social Talk: A Call for Dialogue Model Enrichment](https://doi.org/10.18653/v1/2020.acl-srw.17) |  | 0 | Current models of dialogue mainly focus on utterances within a topically coherent discourse segment, rather than new-topic utterances (NTUs), which begin a new topic not correlating with the content of prior discourse. As a result, these models may sufficiently account for discourse context of task-oriented but not social conversations. We conduct a pilot annotation study of NTUs as a first step towards a model capable of rationalizing conversational coherence in social talk. We start with the naturally occurring social dialogues in the Disco-SPICE corpus, annotated with discourse relations in the Penn Discourse Treebank and Cognitive approach to Coherence Relations frameworks. We first annotate content-based coherence relations that are not available in Disco-SPICE, and then heuristically identify NTUs, which lack a coherence relation to prior discourse. Based on the interaction between NTUs and their discourse context, we construct a classification for NTUs that actually convey certain non-topical coherence in social talk. This classification introduces new sequence-based social intents that traditional taxonomies of speech acts do not capture. The new findings advocates the development of a Bayesian game-theoretic model for social talk. | Alex Luu, Sophia A. Malamud |  |
| 18 |  |  [Why is penguin more similar to polar bear than to sea gull? Analyzing conceptual knowledge in distributional models](https://doi.org/10.18653/v1/2020.acl-srw.18) |  | 0 | What do powerful models of word mean- ing created from distributional data (e.g. Word2vec (Mikolov et al., 2013) BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018)) represent? What causes words to be similar in the semantic space? What type of information is lacking? This thesis proposal presents a framework for investigating the information encoded in distributional semantic models. Several analysis methods have been suggested, but they have been shown to be limited and are not well understood. This approach pairs observations made on actual corpora with insights obtained from data manipulation experiments. The expected outcome is a better understanding of (1) the semantic information we can infer purely based on linguistic co-occurrence patterns and (2) the potential of distributional semantic models to pick up linguistic evidence. | Pia Sommerauer |  |
| 19 |  |  [A Simple and Effective Dependency Parser for Telugu](https://doi.org/10.18653/v1/2020.acl-srw.19) |  | 0 | We present a simple and effective dependency parser for Telugu, a morphologically rich, free word order language. We propose to replace the rich linguistic feature templates used in the past approaches with a minimal feature function using contextual vector representations. We train a BERT model on the Telugu Wikipedia data and use vector representations from this model to train the parser. Each sentence token is associated with a vector representing the token in the context of that sentence and the feature vectors are constructed by concatenating two token representations from the stack and one from the buffer. We put the feature representations through a feedforward network and train with a greedy transition based approach. The resulting parser has a very simple architecture with minimal feature engineering and achieves state-of-the-art results for Telugu. | Sneha Nallani, Manish Shrivastava, Dipti Misra Sharma |  |
| 20 |  |  [Pointwise Paraphrase Appraisal is Potentially Problematic](https://doi.org/10.18653/v1/2020.acl-srw.20) |  | 0 | The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models. As a first step towards that goal, we show that although the standard way of fine-tuning BERT for paraphrase identification by pairing two sentences as one sequence results in a model with state-of-the-art performance, that model may perform poorly on simple tasks like identifying pairs with two identical sentences. Moreover, we show that these models may even predict a pair of randomly-selected sentences with higher paraphrase score than a pair of identical ones. | Hannah Chen, Yangfeng Ji, David Evans |  |
| 21 |  |  [Let's be Humorous: Knowledge Enhanced Humor Generation](https://aclanthology.org/2020.acl-srw.21/) |  | 0 |  | Hang Zhang, Dayiheng Liu, Jiancheng Lv, Cheng Luo |  |
| 22 |  |  [Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages](https://doi.org/10.18653/v1/2020.acl-srw.22) |  | 0 | A large percentage of the world’s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines. | Vikrant Goyal, Sourav Kumar, Dipti Misra Sharma |  |
| 23 |  |  [Exploring Interpretability in Event Extraction: Multitask Learning of a Neural Event Classifier and an Explanation Decoder](https://doi.org/10.18653/v1/2020.acl-srw.23) |  | 0 | We propose an interpretable approach for event extraction that mitigates the tension between generalization and interpretability by jointly training for the two goals. Our approach uses an encoder-decoder architecture, which jointly trains a classifier for event extraction, and a rule decoder that generates syntactico-semantic rules that explain the decisions of the event classifier. We evaluate the proposed approach on three biomedical events and show that the decoder generates interpretable rules that serve as accurate explanations for the event classifier’s decisions, and, importantly, that the joint training generally improves the performance of the event classifier. Lastly, we show that our approach can be used for semi-supervised learning, and that its performance improves when trained on automatically-labeled data generated by a rule-based system. | Zheng Tang, Gus HahnPowell, Mihai Surdeanu |  |
| 24 |  |  [Crossing the Line: Where do Demographic Variables Fit into Humor Detection?](https://doi.org/10.18653/v1/2020.acl-srw.24) |  | 0 | Recent humor classification shared tasks have struggled with two issues: either the data comprises a highly constrained genre of humor which does not broadly represent humor, or the data is so indiscriminate that the inter-annotator agreement on its humor content is drastically low. These tasks typically average over all annotators’ judgments, in spite of the fact that humor is a highly subjective phenomenon. We argue that demographic factors influence whether a text is perceived as humorous or not. We propose the addition of demographic information about the humor annotators in order to bin ratings more sensibly. We also suggest the addition of an ‘offensive’ label to distinguish between different generations, in terms of humor. This would allow for more nuanced shared tasks and could lead to better performance on downstream tasks, such as content moderation. | J. A. Meaney |  |
| 25 |  |  [Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions](https://doi.org/10.18653/v1/2020.acl-srw.25) |  | 0 | Parallel corpora are key to developing good machine translation systems. However, abundant parallel data are hard to come by, especially for languages with a low number of speakers. When rich morphology exacerbates the data sparsity problem, it is imperative to have accurate alignment and filtering methods that can help make the most of what is available by maximising the number of correctly translated segments in a corpus and minimising noise by removing incorrect translations and segments containing extraneous data. This paper sets out a research plan for improving alignment and filtering methods for parallel texts in low-resource settings. We propose an effective unsupervised alignment method to tackle the alignment problem. Moreover, we propose a strategy to supplement state-of-the-art models with automatically extracted information using basic NLP tools to effectively handle rich morphology. | Steinþór Steingrímsson, Hrafn Loftsson, Andy Way |  |
| 26 |  |  [Understanding Points of Correspondence between Sentences for Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-srw.26) |  | 0 | Fusing sentences containing disparate content is a remarkable human ability that helps create informative and succinct summaries. Such a simple task for humans has remained challenging for modern abstractive summarizers, substantially restricting their applicability in real-world scenarios. In this paper, we present an investigation into fusing sentences drawn from a document by introducing the notion of points of correspondence, which are cohesive devices that tie any two sentences together into a coherent text. The types of points of correspondence are delineated by text cohesion theory, covering pronominal and nominal referencing, repetition and beyond. We create a dataset containing the documents, source and fusion sentences, and human annotations of points of correspondence between sentences. Our dataset bridges the gap between coreference resolution and summarization. It is publicly shared to serve as a basis for future work to measure the success of sentence fusion systems. | Logan Lebanoff, John Muchovej, Franck Dernoncourt, Doo Soon Kim, Lidan Wang, Walter Chang, Fei Liu |  |
| 27 |  |  [uBLEU: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-srw.27) |  | 0 | Because open-domain dialogues allow diverse responses, basic reference-based metrics such as BLEU do not work well unless we prepare a massive reference set of high-quality responses for input utterances. To reduce this burden, a human-aided, uncertainty-aware metric, ΔBLEU, has been proposed; it embeds human judgment on the quality of reference outputs into the computation of multiple-reference BLEU. In this study, we instead propose a fully automatic, uncertainty-aware evaluation method for open-domain dialogue systems, υBLEU. This method first collects diverse reference responses from massive dialogue data and then annotates their quality judgments by using a neural network trained on automatically collected training data. Experimental results on massive Twitter data confirmed that υBLEU is comparable to ΔBLEU in terms of its correlation with human judgment and that the state of the art automatic evaluation method, RUBER, is improved by integrating υBLEU. | Tsuta Yuma, Naoki Yoshinaga, Masashi Toyoda |  |
| 28 |  |  [To compress or not to compress? A Finite-State approach to Nen verbal morphology](https://doi.org/10.18653/v1/2020.acl-srw.28) |  | 0 | This paper describes the development of a verbal morphological parser for an under-resourced Papuan language, Nen. Nen verbal morphology is particularly complex, with a transitive verb taking up to 1,740 unique features. The structural properties exhibited by Nen verbs raises interesting choices for analysis. Here we compare two possible methods of analysis: ‘Chunking’ and decomposition. ‘Chunking’ refers to the concept of collating morphological segments into one, whereas the decomposition model follows a more classical linguistic approach. Both models are built using the Finite-State Transducer toolkit foma. The resultant architecture shows differences in size and structural clarity. While the ‘Chunking’ model is under half the size of the full de-composed counterpart, the decomposition displays higher structural order. In this paper, we describe the challenges encountered when modelling a language exhibiting distributed exponence and present the first morphological analyser for Nen, with an overall accuracy of 80.3%. | Saliha Muradoglu, Nicholas Evans, Hanna Suominen |  |
| 29 |  |  [AraDIC: Arabic Document Classification Using Image-Based Character Embeddings and Class-Balanced Loss](https://doi.org/10.18653/v1/2020.acl-srw.29) |  | 0 | Classical and some deep learning techniques for Arabic text classification often depend on complex morphological analysis, word segmentation, and hand-crafted feature engineering. These could be eliminated by using character-level features. We propose a novel end-to-end Arabic document classification framework, Arabic document image-based classifier (AraDIC), inspired by the work on image-based character embeddings. AraDIC consists of an image-based character encoder and a classifier. They are trained in an end-to-end fashion using the class balanced loss to deal with the long-tailed data distribution problem. To evaluate the effectiveness of AraDIC, we created and published two datasets, the Arabic Wikipedia title (AWT) dataset and the Arabic poetry (AraP) dataset. To the best of our knowledge, this is the first image-based character embedding framework addressing the problem of Arabic text classification. We also present the first deep learning-based text classifier widely evaluated on modern standard Arabic, colloquial Arabic, and Classical Arabic. AraDIC shows performance improvement over classical and deep learning baselines by 12.29% and 23.05% for the micro and macro F-score, respectively. | Mahmoud Daif, Shunsuke Kitada, Hitoshi Iyatomi |  |
| 30 |  |  [Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-srw.30) |  | 0 | In general, the labels used in sequence labeling consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as Person, can be beneficial for label prediction. In this work, we propose to integrate label component information as embeddings into models. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels. | Takuma Kato, Kaori Abe, Hiroki Ouchi, Shumpei Miyawaki, Jun Suzuki, Kentaro Inui |  |
| 31 |  |  [Building a Japanese Typo Dataset from Wikipedia's Revision History](https://doi.org/10.18653/v1/2020.acl-srw.31) |  | 0 | User generated texts contain many typos for which correction is necessary for NLP systems to work. Although a large number of typo–correction pairs are needed to develop a data-driven typo correction system, no such dataset is available for Japanese. In this paper, we extract over half a million Japanese typo–correction pairs from Wikipedia’s revision history. Unlike other languages, Japanese poses unique challenges: (1) Japanese texts are unsegmented so that we cannot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction. | Yu Tanaka, Yugo Murawaki, Daisuke Kawahara, Sadao Kurohashi |  |
| 32 |  |  [Preventing Critical Scoring Errors in Short Answer Scoring with Confidence Estimation](https://doi.org/10.18653/v1/2020.acl-srw.32) |  | 0 | Many recent Short Answer Scoring (SAS) systems have employed Quadratic Weighted Kappa (QWK) as the evaluation measure of their systems. However, we hypothesize that QWK is unsatisfactory for the evaluation of the SAS systems when we consider measuring their effectiveness in actual usage. We introduce a new task formulation of SAS that matches the actual usage. In our formulation, the SAS systems should extract as many scoring predictions that are not critical scoring errors (CSEs). We conduct the experiments in our new task formulation and demonstrate that a typical SAS system can predict scores with zero CSE for approximately 50% of test data at maximum by filtering out low-reliablility predictions on the basis of a certain confidence estimation. This result directly indicates the possibility of reducing half the scoring cost of human raters, which is more preferable for the evaluation of SAS systems. | Hiroaki Funayama, Shota Sasaki, Yuichiroh Matsubayashi, Tomoya Mizumoto, Jun Suzuki, Masato Mita, Kentaro Inui |  |
| 33 |  |  [How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?](https://doi.org/10.18653/v1/2020.acl-srw.33) |  | 0 | Long short-term memory (LSTM) networks and their variants are capable of encapsulating long-range dependencies, which is evident from their performance on a variety of linguistic tasks. On the other hand, simple recurrent networks (SRNs), which appear more biologically grounded in terms of synaptic connections, have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. In this paper, we seek to develop models that bridge the gap between biological plausibility and linguistic competence. We propose a new architecture, the Decay RNN, which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. Besides its biological inspiration, our model also shows competitive performance relative to LSTMs on subject-verb agreement, sentence grammaticality, and language modeling tasks. These results provide some pointers towards probing the nature of the inductive biases required for RNN architectures to model linguistic phenomena successfully. | Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal |  |
| 34 |  |  [Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining](https://doi.org/10.18653/v1/2020.acl-srw.34) |  | 0 | Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs. | Ivana Kvapilíková, Mikel Artetxe, Gorka Labaka, Eneko Agirre, Ondrej Bojar |  |
| 35 |  |  [Logical Inferences with Comparatives and Generalized Quantifiers](https://doi.org/10.18653/v1/2020.acl-srw.35) |  | 0 | Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models. | Izumi Haruta, Koji Mineshima, Daisuke Bekki |  |
| 36 |  |  [Enhancing Word Embeddings with Knowledge Extracted from Lexical Resources](https://doi.org/10.18653/v1/2020.acl-srw.36) |  | 0 | In this work, we present an effective method for semantic specialization of word vector representations. To this end, we use traditional word embeddings and apply specialization methods to better capture semantic relations between words. In our approach, we leverage external knowledge from rich lexical resources such as BabelNet. We also show that our proposed post-specialization method based on an adversarial neural network with the Wasserstein distance allows to gain improvements over state-of-the-art methods on two tasks: word similarity and dialog state tracking. | Magdalena Biesialska, Bardia Rafieian, Marta R. Costajussà |  |
| 37 |  |  [Pre-training via Leveraging Assisting Languages for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-srw.37) |  | 0 | Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios. | Haiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng, Sadao Kurohashi, Eiichiro Sumita |  |
| 38 |  |  [Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems](https://doi.org/10.18653/v1/2020.acl-srw.38) |  | 0 | In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the N-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder’s ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well. | Vinay Pandramish, Dipti Misra Sharma |  |
| 39 |  |  [Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup](https://doi.org/10.18653/v1/2020.acl-srw.39) |  | 0 | Distinguishing informative and actionable messages from a social media platform like Twitter is critical for facilitating disaster management. For this purpose, we compile a multilingual dataset of over 130K samples for multi-label classification of disaster-related tweets. We present a masking-based loss function for partially labelled samples and demonstrate the effectiveness of Manifold Mixup in the text domain. Our main model is based on Multilingual BERT, which we further improve with Manifold Mixup. We show that our model generalizes to unseen disasters in the test set. Furthermore, we analyze the capability of our model for zero-shot generalization to new languages. Our code, dataset, and other resources are available on Github. | Jishnu Ray Chowdhury, Cornelia Caragea, Doina Caragea |  |
| 40 |  |  [Inducing Grammar from Long Short-Term Memory Networks by Shapley Decomposition](https://doi.org/10.18653/v1/2020.acl-srw.40) |  | 0 | The principle of compositionality has deep roots in linguistics: the meaning of an expression is determined by its structure and the meanings of its constituents. However, modern neural network models such as long short-term memory network process expressions in a linear fashion and do not seem to incorporate more complex compositional patterns. In this work, we show that we can explicitly induce grammar by tracing the computational process of a long short-term memory network. We show: (i) the multiplicative nature of long short-term memory network allows complex interaction beyond sequential linear combination; (ii) we can generate compositional trees from the network without external linguistic knowledge; (iii) we evaluate the syntactic difference between the generated trees, randomly generated trees and gold reference trees produced by constituency parsers; (iv) we evaluate whether the generated trees contain the rich semantic information. | Yuhui Zhang, Allen Nie |  |
| 41 |  |  [Exploring the Role of Context to Distinguish Rhetorical and Information-Seeking Questions](https://doi.org/10.18653/v1/2020.acl-srw.41) |  | 0 | Social media posts often contain questions, but many of the questions are rhetorical and do not seek information. Our work studies the problem of distinguishing rhetorical and information-seeking questions on Twitter. Most work has focused on features of the question itself, but we hypothesize that the prior context plays a role too. This paper introduces a new dataset containing questions in tweets paired with their prior tweets to provide context. We create classification models to assess the difficulty of distinguishing rhetorical and information-seeking questions, and experiment with different properties of the prior context. Our results show that the prior tweet and topic features can improve performance on this task. | Yuan Zhuang, Ellen Riloff |  |
| 42 |  |  [Compositional Generalization by Factorizing Alignment and Translation](https://doi.org/10.18653/v1/2020.acl-srw.42) |  | 0 | Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in cognitive science suggesting a functional distinction between systems for syntactic and semantic processing, we implement a modification to an existing approach in neural machine translation, imposing an analogous separation between alignment and translation. The resulting architecture substantially outperforms standard recurrent networks on the SCAN dataset, a compositional generalization task, without any additional supervision. Our work suggests that learning to align and to translate in separate modules may be a useful heuristic for capturing compositional structure. | Jacob L. Russin, Jason Jo, Randall C. O'Reilly, Yoshua Bengio |  |
| 43 |  |  [#NotAWhore! A Computational Linguistic Perspective of Rape Culture and Victimization on Social Media](https://doi.org/10.18653/v1/2020.acl-srw.43) |  | 0 | The recent surge in online forums and movements supporting sexual assault survivors has led to the emergence of a ‘virtual bubble’ where survivors can recount their stories. However, this also makes the survivors vulnerable to bullying, trolling and victim blaming. Specifically, victim blaming has been shown to have acute psychological effects on the survivors and further discourage formal reporting of such crimes. Therefore, it is important to devise computationally relevant methods to identify and prevent victim blaming to protect the victims. In our work, we discuss the drastic effects of victim blaming through a short case study and then propose a single step transfer-learning based classification method to identify victim blaming language on Twitter. Finally, we compare the performance of our proposed model against various deep learning and machine learning models on a manually annotated domain-specific dataset. | Ashima Suvarna, Grusha Bhalla |  |
| 44 |  |  [Xiaomingbot: A Multilingual Robot News Reporter](https://doi.org/10.18653/v1/2020.acl-demos.1) |  | 0 | This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities: news generation, news translation, news reading and avatar animation. Its system summarizes Chinese news that it automatically generates from data tables. Next, it translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through synthesized speech. Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person’s voice data in one input language. The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news. Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms. | Runxin Xu, Jun Cao, Mingxuan Wang, Jiaze Chen, Hao Zhou, Ying Zeng, Yuping Wang, Li Chen, Xiang Yin, Xijin Zhang, Songcheng Jiang, Yuxuan Wang, Lei Li |  |
| 45 |  |  [TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-demos.2) |  | 0 | In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters. | Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin Wang, Guoping Hu |  |
| 46 |  |  [Syntactic Search by Example](https://doi.org/10.18653/v1/2020.acl-demos.3) |  | 0 | We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs. In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine. This allows for rapid exploration, development and refinement of syntax-based queries. We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts. A demo of the wikipedia system is available at https://allenai.github.io/spike/ . | Micah Shlain, Hillel TaubTabib, Shoval Sadde, Yoav Goldberg |  |
| 47 |  |  [Tabouid: a Wikipedia-based word guessing game](https://doi.org/10.18653/v1/2020.acl-demos.4) |  | 0 | We present Tabouid, a word-guessing game automatically generated from Wikipedia. Tabouid contains 10,000 (virtual) cards in English, and as many in French, covering not only words and linguistic expressions but also a variety of topics including artists, historical events or scientific concepts. Each card corresponds to a Wikipedia article, and conversely, any article could be turned into a card. A range of relatively simple NLP and machine-learning techniques are effectively integrated into a two-stage process. First, a large subset of Wikipedia articles are scored - this score estimates the difficulty, or alternatively, the playability of the page. Then, the best articles are turned into cards by selecting, for each of them, a list of banned words based on its content. We believe that the game we present is more than mere entertainment and that, furthermore, this paper has pedagogical potential. | Timothée Bernard |  |
| 48 |  |  [Talk to Papers: Bringing Neural Question Answering to Academic Search](https://doi.org/10.18653/v1/2020.acl-demos.5) |  | 0 | We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It’s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort. | Tiancheng Zhao, Kyusong Lee |  |
| 49 |  |  [Personalized PageRank with Syntagmatic Information for Multilingual Word Sense Disambiguation](https://doi.org/10.18653/v1/2020.acl-demos.6) |  | 0 | Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance. We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web interface and a RESTful API. SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting. Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/). | Federico Scozzafava, Marco Maru, Fabrizio Brignone, Giovanni Torrisi, Roberto Navigli |  |
| 50 |  |  [pyBART: Evidence-based Syntactic Transformations for IE](https://doi.org/10.18653/v1/2020.acl-demos.7) |  | 0 | Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications. Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs. However, they are not available to Python users, and are also limited in coverage. We introduce a broad-coverage, data-driven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit. We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation. The library can work as a standalone package or be integrated within a spaCy NLP pipeline. When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns. | Aryeh Tiktinsky, Yoav Goldberg, Reut Tsarfaty |  |
| 51 |  |  [EVIDENCEMINER: Textual Evidence Discovery for Life Sciences](https://doi.org/10.18653/v1/2020.acl-demos.8) |  | 0 | Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements. Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation. We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences. EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation. It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction. The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval. The annotation results are also highlighted in the original document for better visualization. EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization. EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis. The system of EVIDENCEMINER is available at https://evidenceminer.firebaseapp.com/. | Xuan Wang, Yingjun Guan, Weili Liu, Aabhas Chauhan, Enyi Jiang, Qi Li, David Liem, Dibakar Sigdel, John Caufield, Peipei Ping, Jiawei Han |  |
| 52 |  |  [Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time](https://doi.org/10.18653/v1/2020.acl-demos.9) |  | 0 | We introduce Trialstreamer, a living database of clinical trial reports. Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these. Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured. The system then attempts to infer which interventions were reported to work best by determining their relationship with identified trial outcome measures. In addition to summarizing individual trials, these extracted data elements allow automatic synthesis of results across many trials on the same topic. We apply the system at scale to all reports of randomized controlled trials indexed in MEDLINE, powering the automatic generation of evidence maps, which provide a global view of the efficacy of different interventions combining data from all relevant clinical trials on a topic. We make all code and models freely available alongside a demonstration of the web interface. | Benjamin E. Nye, Ani Nenkova, Iain James Marshall, Byron C. Wallace |  |
| 53 |  |  [SyntaxGym: An Online Platform for Targeted Evaluation of Language Models](https://doi.org/10.18653/v1/2020.acl-demos.10) |  | 0 | Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, ‘syntaxgym‘ and ‘lm-zoo‘, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine. | Jon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian, Roger Levy |  |
| 54 |  |  [GAIA: A Fine-grained Multimedia Knowledge Extraction System](https://doi.org/10.18653/v1/2020.acl-demos.11) |  | 0 | We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system. | Manling Li, Alireza Zareian, Ying Lin, Xiaoman Pan, Spencer Whitehead, Brian Chen, Bo Wu, Heng Ji, ShihFu Chang, Clare R. Voss, Daniel Napierski, Marjorie Freedman |  |
| 55 |  |  [Multilingual Universal Sentence Encoder for Semantic Retrieval](https://doi.org/10.18653/v1/2020.acl-demos.12) |  | 0 | We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub. The models embed text from 16 languages into a shared semantic space using a multi-task trained dual-encoder that learns tied cross-lingual representations via translation bridge tasks (Chidambaram et al., 2018). The models achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR). Competitive performance is obtained on the related tasks of translation pair bitext retrieval (BR) and retrieval question answering (ReQA). On transfer learning tasks, our multilingual embeddings approach, and in some cases exceed, the performance of English only sentence embeddings. | Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernández Ábrego, Steve Yuan, Chris Tar, YunHsuan Sung, Brian Strope, Ray Kurzweil |  |
| 56 |  |  [BENTO: A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab](https://doi.org/10.18653/v1/2020.acl-demos.13) |  | 0 | CodaLab is an open-source web-based platform for collaborative computational research. Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines. In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc. Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets. In this paper, we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines. BENTO comes with a number of clinical NLP tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical NLP tasks. It also allows researchers and developers to create their custom tools (e.g., pre-trained NLP models) and use them in a controlled and reproducible way. In addition, the GUI interface enables researchers with limited computer background to compose tools into NLP pipelines and then apply the pipelines on their own datasets in a “what you see is what you get” (WYSIWYG) way. Although BENTO is designed for clinical NLP applications, the underlying architecture is flexible to be tailored to any other domains. | Yonghao Jin, Fei Li, Hong Yu |  |
| 57 |  |  [Stanza: A Python Natural Language Processing Toolkit for Many Human Languages](https://doi.org/10.18653/v1/2020.acl-demos.14) |  | 0 | We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/. | Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D. Manning |  |
| 58 |  |  [jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models](https://doi.org/10.18653/v1/2020.acl-demos.15) |  | 0 | We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT. | Yada Pruksachatkun, Philip Yeres, Haokun Liu, Jason Phang, Phu Mon Htut, Alex Wang, Ian Tenney, Samuel R. Bowman |  |
| 59 |  |  [The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-demos.16) |  | 0 | We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models. Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM). A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm. To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn. | Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He, Weizhu Chen, Hoifung Poon, Guihong Cao, Jianfeng Gao |  |
| 60 |  |  [LinggleWrite: a Coaching System for Essay Writing](https://doi.org/10.18653/v1/2020.acl-demos.17) |  | 0 | This paper presents LinggleWrite, a writing coach that provides writing suggestions, assesses writing proficiency levels, detects grammatical errors, and offers corrective feedback in response to user’s essay. The method involves extracting grammar patterns, training models for automated essay scoring (AES) and grammatical error detection (GED), and finally retrieving plausible corrections from a n-gram search engine. Experiments on public test sets indicate that both AES and GED models achieve state-of-the-art performance. These results show that LinggleWrite is potentially useful in helping learners improve their writing skills. | ChungTing Tsai, JhihJie Chen, Chingyu Yang, Jason S. Chang |  |
| 61 |  |  [CLIReval: Evaluating Machine Translation as a Cross-Lingual Information Retrieval Task](https://doi.org/10.18653/v1/2020.acl-demos.18) |  | 0 | We present CLIReval, an easy-to-use toolkit for evaluating machine translation (MT) with the proxy task of cross-lingual information retrieval (CLIR). Contrary to what the project name might suggest, CLIReval does not actually require any annotated CLIR dataset. Instead, it automatically transforms translations and references used in MT evaluations into a synthetic CLIR dataset; it then sets up a standard search engine (Elasticsearch) and computes various information retrieval metrics (e.g., mean average precision) by treating the translations as documents to be retrieved. The idea is to gauge the quality of MT by its impact on the document translation approach to CLIR. As a case study, we run CLIReval on the “metrics shared task” of WMT2019; while this extrinsic metric is not intended to replace popular intrinsic metrics such as BLEU, results suggest CLIReval is competitive in many language pairs in terms of correlation to human judgments of quality. CLIReval is publicly available at https://github.com/ssun32/CLIReval. | Shuo Sun, Suzanna Sia, Kevin Duh |  |
| 62 |  |  [ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-demos.19) |  | 0 | We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides an user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. | Qi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi Takanobu, Jinchao Li, Baolin Peng, Jianfeng Gao, Xiaoyan Zhu, Minlie Huang |  |
| 63 |  |  [OpusFilter: A Configurable Parallel Corpus Filtering Toolbox](https://doi.org/10.18653/v1/2020.acl-demos.20) |  | 0 | This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation. | Mikko Aulamo, Sami Virpioja, Jörg Tiedemann |  |
| 64 |  |  [Label Noise in Context](https://doi.org/10.18653/v1/2020.acl-demos.21) |  | 0 | Label noise—incorrectly or ambiguously labeled training examples—can negatively impact model performance. Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process. Examples incorrectly flagged as noise waste reviewers’ time, and correcting label noise without guidance can be difficult. We propose LNIC, a noise-detection method that uses an example’s neighborhood within the training set to (a) reduce false positives and (b) provide an explanation as to why the ex- ample was flagged as noise. We demonstrate on several short-text classification datasets that LNIC outperforms the state of the art on measures of precision and F0.5-score. We also show how LNIC’s training set context helps a reviewer to understand and correct label noise in a dataset. The LNIC tool lowers the barriers to label noise remediation, increasing its utility for NLP practitioners. | Michael Desmond, Catherine FineganDollak, Jeffrey Boston, Matthew Arnold |  |
| 65 |  |  [exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models](https://doi.org/10.18653/v1/2020.acl-demos.22) |  | 0 | Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques. To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process. exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets. By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models. | Benjamin Hoover, Hendrik Strobelt, Sebastian Gehrmann |  |
| 66 |  |  [Nakdan: Professional Hebrew Diacritizer](https://doi.org/10.18653/v1/2020.acl-demos.23) |  | 0 | We present a system for automatic diacritization of Hebrew Text. The system combines modern neural models with carefully curated declarative linguistic knowledge and comprehensive manually constructed tables and dictionaries. Besides providing state of the art diacritization accuracy, the system also supports an interface for manual editing and correction of the automatic output, and has several features which make it particularly useful for preparation of scientific editions of historical Hebrew texts. The system supports Modern Hebrew, Rabbinic Hebrew and Poetic Hebrew. The system is freely accessible for all use at http://nakdanpro.dicta.org.il | Avi Shmidman, Shaltiel Shmidman, Moshe Koppel, Yoav Goldberg |  |
| 67 |  |  [Photon: A Robust Cross-Domain Text-to-SQL System](https://doi.org/10.18653/v1/2020.acl-demos.24) |  | 0 | Natural language interfaces to databases(NLIDB) democratize end user access to relational data. Due to fundamental differences between natural language communication and programming, it is common for end users to issue questions that are ambiguous to the system or fall outside the semantic scope of its underlying query language. We present PHOTON, a robust, modular, cross-domain NLIDB that can flag natural language input to which a SQL mapping cannot be immediately determined. PHOTON consists of a strong neural semantic parser (63.2% structure accuracy on the Spider dev benchmark), a human-in-the-loop question corrector, a SQL executor and a response generator. The question corrector isa discriminative neural sequence editor which detects confusion span(s) in the input question and suggests rephrasing until a translatable input is given by the user or a maximum number of iterations are conducted. Experiments on simulated data show that the proposed method effectively improves the robustness of text-to-SQL system against untranslatable user input. The live demo of our system is available at http://www.naturalsql.com | Jichuan Zeng, Xi Victoria Lin, Steven C. H. Hoi, Richard Socher, Caiming Xiong, Michael R. Lyu, Irwin King |  |
| 68 |  |  [Interactive Task Learning from GUI-Grounded Natural Language Instructions and Demonstrations](https://doi.org/10.18653/v1/2020.acl-demos.25) |  | 0 | We show SUGILITE, an intelligent task automation agent that can learn new tasks and relevant associated concepts interactively from the user’s natural language instructions and demonstrations, using the graphical user interfaces (GUIs) of third-party mobile apps. This system provides several interesting features: (1) it allows users to teach new task procedures and concepts through verbal instructions together with demonstration of the steps of a script using GUIs; (2) it supports users in clarifying their intents for demonstrated actions using GUI-grounded verbal instructions; (3) it infers parameters of tasks and their possible values in utterances using the hierarchical structures of the underlying app GUIs; and (4) it generalizes taught concepts to different contexts and task domains. We describe the architecture of the SUGILITE system, explain the design and implementation of its key features, and show a prototype in the form of a conversational assistant on Android. | Toby JiaJun Li, Tom M. Mitchell, Brad A. Myers |  |
| 69 |  |  [MixingBoard: a Knowledgeable Stylized Integrated Text Generation Platform](https://doi.org/10.18653/v1/2020.acl-demos.26) |  | 0 | We present MixingBoard, a platform for quickly building demos with a focus on knowledge grounded stylized text generation. We unify existing text generation algorithms in a shared codebase and further adapt earlier algorithms for constrained generation. To borrow advantages from different models, we implement strategies for cross-model integration, from the token probability level to the latent space level. An interface to external knowledge is provided via a module that retrieves, on-the-fly, relevant knowledge from passages on the web or a document collection. A user interface for local development, remote webpage access, and a RESTful API are provided to make it simple for users to build their own demos. | Xiang Gao, Michel Galley, Bill Dolan |  |
| 70 |  |  [NLP Scholar: An Interactive Visual Explorer for Natural Language Processing Literature](https://doi.org/10.18653/v1/2020.acl-demos.27) |  | 0 | As part of the NLP Scholar project, we created a single unified dataset of NLP papers and their meta-information (including citation numbers), by extracting and aligning information from the ACL Anthology and Google Scholar. In this paper, we describe several interconnected interactive visualizations (dashboards) that present various aspects of the data. Clicking on an item within a visualization or entering query terms in the search boxes filters the data in all visualizations in the dashboard. This allows users to search for papers in the area of their interest, published within specific time periods, published by specified authors, etc. The interactive visualizations presented here, and the associated dataset of papers mapped to citations, have additional uses as well including understanding how the field is growing (both overall and across sub-areas), as well as quantifying the impact of different types of papers on subsequent publications. | Saif M. Mohammad |  |
| 71 |  |  [Stimulating Creativity with FunLines: A Case Study of Humor Generation in Headlines](https://doi.org/10.18653/v1/2020.acl-demos.28) |  | 0 | Building datasets of creative text, such as humor, is quite challenging. We introduce FunLines, a competitive game where players edit news headlines to make them funny, and where they rate the funniness of headlines edited by others. FunLines makes the humor generation process fun, interactive, collaborative, rewarding and educational, keeping players engaged and providing humor data at a very low cost compared to traditional crowdsourcing approaches. FunLines offers useful performance feedback, assisting players in getting better over time at generating and assessing humor, as our analysis shows. This helps to further increase the quality of the generated dataset. We show the effectiveness of this data by training humor classification models that outperform a previous benchmark, and we release this dataset to the public. | Nabil Hossain, John Krumm, Tanvir Sajed, Henry A. Kautz |  |
| 72 |  |  [Usnea: An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-demos.29) |  | 0 | The reader of a choose your own adventure novel and the user of a modern virtual assistant have a subtle similarity; both may, through the right lens, be viewed as engaging with a work of Interactive Fiction. This literary form emerged in the 1970s and has grown like a vine along the branch of modern technology, one guided by the advances of the other. In this work we weave together threads from the Interactive Fiction community and neural semantic parsing for dialog systems, defining the data model and necessary algorithms for a novel type of Interactive Fiction and open sourcing its accompanying authoring tool. Specifically, our work integrates retrieval based semantic parsing predicates into the branching story structures well known to the Interactive Fiction community, relaxing the relatively strict lexical options of preexisting systems. | Ben Swanson, Boris Smus |  |
| 73 |  |  [DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation](https://doi.org/10.18653/v1/2020.acl-demos.30) |  | 0 | We present a large, tunable neural conversational response generation model, DIALOGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems. | Yizhe Zhang, Siqi Sun, Michel Galley, YenChun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan |  |
| 74 |  |  [ADVISER: A Toolkit for Developing Multi-modal, Multi-domain and Socially-engaged Conversational Agents](https://doi.org/10.18653/v1/2020.acl-demos.31) |  | 0 | We present ADVISER - an open-source, multi-domain dialog system toolkit that enables the development of multi-modal (incorporating speech, text and vision), socially-engaged (e.g. emotion recognition, engagement level prediction and backchanneling) conversational agents. The final Python-based implementation of our toolkit is flexible, easy to use, and easy to extend not only for technically experienced users, such as machine learning researchers, but also for less technically experienced users, such as linguists or cognitive scientists, thereby providing a flexible platform for collaborative research. | ChiaYu Li, Daniel Ortega, Dirk Väth, Florian Lux, Lindsey Vanderlyn, Maximilian Schmidt, Michael Neumann, Moritz Völkel, Pavel Denisov, Sabrina Jenne, Zorica Kacarevic, Ngoc Thang Vu |  |
| 75 |  |  [Prta: A System to Support the Analysis of Propaganda Techniques in the News](https://doi.org/10.18653/v1/2020.acl-demos.32) |  | 0 | Recent events, such as the 2016 US Presidential Campaign, Brexit and the COVID-19 “infodemic”, have brought into the spotlight the dangers of online disinformation. There has been a lot of research focusing on fact-checking and disinformation detection. However, little attention has been paid to the specific rhetorical and psychological techniques used to convey propaganda messages. Revealing the use of such techniques can help promote media literacy and critical thinking, and eventually contribute to limiting the impact of “fake news” and disinformation campaigns. Prta (Propaganda Persuasion Techniques Analyzer) allows users to explore the articles crawled on a regular basis by highlighting the spans in which propaganda techniques occur and to compare them on the basis of their use of propaganda techniques. The system further reports statistics about the use of such techniques, overall and over time, or according to filtering criteria specified by the user based on time interval, keywords, and/or political orientation of the media. Moreover, it allows users to analyze any text or URL through a dedicated interface or via an API. The system is available online: https://www.tanbih.org/prta. | Giovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu, Alberto BarrónCedeño, Preslav Nakov |  |
| 76 |  |  [Clinical-Coder: Assigning Interpretable ICD-10 Codes to Chinese Clinical Notes](https://doi.org/10.18653/v1/2020.acl-demos.33) |  | 0 | In this paper, we introduce Clinical-Coder, an online system aiming to assign ICD codes to Chinese clinical notes. ICD coding has been a research hotspot of clinical medicine, but the interpretability of prediction hinders its practical application. We exploit a Dilated Convolutional Attention network with N-gram Matching mechanism (DCANM) to capture semantic features for non-continuous words and continuous n-gram words, concentrating on explaining the reason why each ICD code to be predicted. The experiments demonstrate that our approach is effective and that our system is able to provide supporting information in clinical decision making. | Pengfei Cao, Chenwei Yan, Xiangling Fu, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu, Weifeng Chong |  |
| 77 |  |  [ESPnet-ST: All-in-One Speech Translation Toolkit](https://doi.org/10.18653/v1/2020.acl-demos.34) |  | 0 | We present ESPnet-ST, which is designed for the quick development of speech-to-speech translation systems in a single framework. ESPnet-ST is a new project inside end-to-end speech processing toolkit, ESPnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at https://github.com/espnet/espnet. | Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Yalta, Tomoki Hayashi, Shinji Watanabe |  |
| 78 |  |  [Penman: An Open-Source Library and Tool for AMR Graphs](https://doi.org/10.18653/v1/2020.acl-demos.35) |  | 0 | Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a framework for semantic dependencies that encodes its rooted and directed acyclic graphs in a format called PENMAN notation. The format is simple enough that users of AMR data often write small scripts or libraries for parsing it into an internal graph representation, but there is enough complexity that these users could benefit from a more sophisticated and well-tested solution. The open-source Python library Penman provides a robust parser, functions for graph inspection and manipulation, and functions for formatting graphs into PENMAN notation. Many functions are also available in a command-line tool, thus extending its utility to non-Python setups. | Michael Wayne Goodman |  |
| 79 |  |  [Embedding-based Scientific Literature Discovery in a Text Editor Application](https://doi.org/10.18653/v1/2020.acl-demos.36) |  | 0 | Each claim in a research paper requires all relevant prior knowledge to be discovered, assimilated, and appropriately cited. However, despite the availability of powerful search engines and sophisticated text editing software, discovering relevant papers and integrating the knowledge into a manuscript remain complex tasks associated with high cognitive load. To define comprehensive search queries requires strong motivation from authors, irrespective of their familiarity with the research field. Moreover, switching between independent applications for literature discovery, bibliography management, reading papers, and writing text burdens authors further and interrupts their creative process. Here, we present a web application that combines text editing and literature discovery in an interactive user interface. The application is equipped with a search engine that couples Boolean keyword filtering with nearest neighbor search over text embeddings, providing a discovery experience tuned to an author’s manuscript and his interests. Our application aims to take a step towards more enjoyable and effortless academic writing. The demo of the application (https://SciEditorDemo2020.herokuapp.com) and a short video tutorial (https://youtu.be/pkdVU60IcRc) are available online. | Onur Gökçe, Jonathan Prada, Nikola I. Nikolov, Nianlong Gu, Richard H. R. Hahnloser |  |
| 80 |  |  [MMPE: A Multi-Modal Interface using Handwriting, Touch Reordering, and Speech Commands for Post-Editing Machine Translation](https://doi.org/10.18653/v1/2020.acl-demos.37) |  | 0 | The shift from traditional translation to post-editing (PE) of machine-translated (MT) text can save time and reduce errors, but it also affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT. Users can directly cross out or hand-write new text, drag and drop words for reordering, or use spoken commands to update the text in place. All text manipulations are logged in an easily interpretable format to simplify subsequent translation process research. The results of an evaluation with professional translators suggest that pen and touch interaction are suitable for deletion and reordering tasks, while speech and multi-modal combinations of select & speech are considered suitable for replacements and insertions. Overall, experiment participants were enthusiastic about the new modalities and saw them as useful extensions to mouse & keyboard, but not as a complete substitute. | Nico Herbig, Santanu Pal, Tim Düwel, Kalliopi Meladaki, Mahsa Monshizadeh, Vladislav Hnatovskiy, Antonio Krüger, Josef van Genabith |  |
| 81 |  |  [Torch-Struct: Deep Structured Prediction Library](https://doi.org/10.18653/v1/2020.acl-demos.38) |  | 0 | The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. Torch-Struct includes a broad collection of probabilistic structures accessed through a simple and flexible distribution-based API that connects to any deep learning model. The library utilizes batched, vectorized operations and exploits auto-differentiation to produce readable, fast, and testable code. Internally, we also include a number of general-purpose optimizations to provide cross-algorithm efficiency. Experiments show significant performance gains over fast baselines and case-studies demonstrate the benefits of the library. Torch-Struct is available at https://github.com/harvardnlp/pytorch-struct. | Alexander M. Rush |  |
| 82 |  |  [Conversation Learner - A Machine Teaching Tool for Building Dialog Managers for Task-Oriented Dialog Systems](https://doi.org/10.18653/v1/2020.acl-demos.39) |  | 0 | Traditionally, industry solutions for building a task-oriented dialog system have relied on helping dialog authors define rule-based dialog managers, represented as dialog flows. While dialog flows are intuitively interpretable and good for simple scenarios, they fall short of performance in terms of the flexibility needed to handle complex dialogs. On the other hand, purely machine-learned models can handle complex dialogs, but they are considered to be black boxes and require large amounts of training data. In this demonstration, we showcase Conversation Learner, a machine teaching tool for building dialog managers. It combines the best of both approaches by enabling dialog authors to create a dialog flow using familiar tools, converting the dialog flow into a parametric model (e.g., neural networks), and allowing dialog authors to improve the dialog manager (i.e., the parametric model) over time by leveraging user-system dialog logs as training data through a machine teaching interface. | Swadheen Shukla, Lars Liden, Shahin Shayandeh, Eslam Kamal, Jinchao Li, Matt Mazzola, Thomas Park, Baolin Peng, Jianfeng Gao |  |
| 83 |  |  [NSTM: Real-Time Query-Driven News Overview Composition at Bloomberg](https://doi.org/10.18653/v1/2020.acl-demos.40) |  | 0 | Millions of news articles from hundreds of thousands of sources around the globe appear in news aggregators every day. Consuming such a volume of news presents an almost insurmountable challenge. For example, a reader searching on Bloomberg’s system for news about the U.K. would find 10,000 articles on a typical day. Apple Inc., the world’s most journalistically covered company, garners around 1,800 news articles a day. We realized that a new kind of summarization engine was needed, one that would condense large volumes of news into short, easy to absorb points. The system would filter out noise and duplicates to identify and summarize key news about companies, countries or markets. When given a user query, Bloomberg’s solution, Key News Themes (or NSTM), leverages state-of-the-art semantic clustering techniques and novel summarization methods to produce comprehensive, yet concise, digests to dramatically simplify the news consumption process. NSTM is available to hundreds of thousands of readers around the world and serves thousands of requests daily with sub-second latency. At ACL 2020, we will present a demo of NSTM. | Joshua Bambrick, Minjie Xu, Andy Almonte, Igor Malioutov, Guim Perarnau, Vittorio Selo, Iat Chong Chan |  |
| 84 |  |  [SUPP.AI: finding evidence for supplement-drug interactions](https://doi.org/10.18653/v1/2020.acl-demos.41) |  | 0 | Dietary supplements are used by a large portion of the population, but information on their pharmacologic interactions is incomplete. To address this challenge, we present SUPP.AI, an application for browsing evidence of supplement-drug interactions (SDIs) extracted from the biomedical literature. We train a model to automatically extract supplement information and identify such interactions from the scientific literature. To address the lack of labeled data for SDI identification, we use labels of the closely related task of identifying drug-drug interactions (DDIs) for supervision. We fine-tune the contextualized word representations of the RoBERTa language model using labeled DDI data, and apply the fine-tuned model to identify supplement interactions. We extract 195k evidence sentences from 22M articles (P=0.82, R=0.58, F1=0.68) for 60k interactions. We create the SUPP.AI application for users to search evidence sentences extracted by our model. SUPP.AI is an attempt to close the information gap on dietary supplements by making up-to-date evidence on SDIs more discoverable for researchers, clinicians, and consumers. An informational video on how to use SUPP.AI is available at: https://youtu.be/dR0ucKdORwc | Lucy Lu Wang, Oyvind Tafjord, Arman Cohan, Sarthak Jain, Sam Skjonsberg, Carissa Schoenick, Nick Botner, Waleed Ammar |  |
| 85 |  |  [LEAN-LIFE: A Label-Efficient Annotation Framework Towards Learning from Explanation](https://doi.org/10.18653/v1/2020.acl-demos.42) |  | 0 | Successfully training a deep neural network demands a huge corpus of labeled data. However, each label only provides limited information to learn from, and collecting the requisite number of labels involves massive human effort. In this work, we introduce LEAN-LIFE, a web-based, Label-Efficient AnnotatioN framework for sequence labeling and classification tasks, with an easy-to-use UI that not only allows an annotator to provide the needed labels for a task but also enables LearnIng From Explanations for each labeling decision. Such explanations enable us to generate useful additional labeled data from unlabeled instances, bolstering the pool of available training data. On three popular NLP tasks (named entity recognition, relation extraction, sentiment analysis), we find that using this enhanced supervision allows our models to surpass competitive baseline F1 scores by more than 5-10 percentage points, while using 2X times fewer labeled instances. Our framework is the first to utilize this enhanced supervision technique and does so for three important tasks – thus providing improved annotation recommendations to users and an ability to build datasets of (data, label, explanation) triples instead of the regular (data, label) pair. | DongHo Lee, Rahul Khanna, Bill Yuchen Lin, Seyeon Lee, Qinyuan Ye, Elizabeth Boschee, Leonardo Neves, Xiang Ren |  |
| 86 |  |  [What's The Latest? A Question-driven News Chatbot](https://doi.org/10.18653/v1/2020.acl-demos.43) |  | 0 | This work describes an automatic news chatbot that draws content from a diverse set of news articles and creates conversations with a user about the news. Key components of the system include the automatic organization of news articles into topical chatrooms, integration of automatically generated questions into the conversation, and a novel method for choosing which questions to present which avoids repetitive suggestions. We describe the algorithmic framework and present the results of a usability study that shows that news readers using the system successfully engage in multi-turn conversations about specific news stories. | Philippe Laban, John F. Canny, Marti A. Hearst |  |
| 87 |  |  [Interpretability and Analysis in Neural NLP](https://doi.org/10.18653/v1/2020.acl-tutorials.1) |  | 0 | While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts. | Yonatan Belinkov, Sebastian Gehrmann, Ellie Pavlick |  |
| 88 |  |  [Integrating Ethics into the NLP Curriculum](https://doi.org/10.18653/v1/2020.acl-tutorials.2) |  | 0 | To raise awareness among future NLP practitioners and prevent inertia in the field, we need to place ethics in the curriculum for all NLP students—not as an elective, but as a core part of their education. Our goal in this tutorial is to empower NLP researchers and practitioners with tools and resources to teach others about how to ethically apply NLP techniques. We will present both high-level strategies for developing an ethics-oriented curriculum, based on experience and best practices, as well as specific sample exercises that can be brought to a classroom. This highly interactive work session will culminate in a shared online resource page that pools lesson plans, assignments, exercise ideas, reading suggestions, and ideas from the attendees. Though the tutorial will focus particularly on examples for university classrooms, we believe these ideas can extend to company-internal workshops or tutorials in a variety of organizations. In this setting, a key lesson is that there is no single approach to ethical NLP: each project requires thoughtful consideration about what steps can be taken to best support people affected by that project. However, we can learn (and teach) what issues to be aware of, what questions to ask, and what strategies are available to mitigate harm. | Emily M. Bender, Dirk Hovy, Alexandra Schofield |  |
| 89 |  |  [Achieving Common Ground in Multi-modal Dialogue](https://doi.org/10.18653/v1/2020.acl-tutorials.3) |  | 0 | All communication aims at achieving common ground (grounding): interlocutors can work together effectively only with mutual beliefs about what the state of the world is, about what their goals are, and about how they plan to make their goals a reality. Computational dialogue research offers some classic results on grouding, which unfortunately offer scant guidance to the design of grounding modules and behaviors in cutting-edge systems. In this tutorial, we focus on three main topic areas: 1) grounding in human-human communication; 2) grounding in dialogue systems; and 3) grounding in multi-modal interactive systems, including image-oriented conversations and human-robot interactions. We highlight a number of achievements of recent computational research in coordinating complex content, show how these results lead to rich and challenging opportunities for doing grounding in more flexible and powerful ways, and canvass relevant insights from the literature on human–human conversation. We expect that the tutorial will be of interest to researchers in dialogue systems, computational semantics and cognitive modeling, and hope that it will catalyze research and system building that more directly explores the creative, strategic ways conversational agents might be able to seek and offer evidence about their understanding of their interlocutors. | Malihe Alikhani, Matthew Stone |  |
| 90 |  |  [Reviewing Natural Language Processing Research](https://doi.org/10.18653/v1/2020.acl-tutorials.4) |  | 0 | This tutorial will cover the theory and practice of reviewing research in natural language processing. Heavy reviewing burdens on natural language processing researchers have made it clear that our community needs to increase the size of our pool of potential reviewers. Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences—have raised awareness of the fact that our reviewing practices leave something to be desired. We do not often talk about “false positives” with respect to conference papers, but leaders in the field have noted that we seem to have a publication bias towards papers that report high performance, with perhaps not much else of interest in them. It need not be this way. Reviewing is a learnable skill, and you will learn it here via lectures and a considerable amount of hands-on practice. | K. Bretonnel Cohen, Karën Fort, Margot Mieskes, Aurélie Névéol |  |
| 91 |  |  [Stylized Text Generation: Approaches and Applications](https://doi.org/10.18653/v1/2020.acl-tutorials.5) |  | 0 | Text generation has played an important role in various applications of natural language processing (NLP), and kn recent studies, researchers are paying increasing attention to modeling and manipulating the style of the generation text, which we call stylized text generation. In this tutorial, we will provide a comprehensive literature review in this direction. We start from the definition of style and different settings of stylized text generation, illustrated with various applications. Then, we present different settings of stylized generation, such as style-conditioned generation, style-transfer generation, and style-adversarial generation. In each setting, we delve deep into machine learning methods, including embedding learning techniques to represent style, adversarial learning, and reinforcement learning with cycle consistency to match content but to distinguish different styles. We also introduce current approaches to evaluating stylized text generation systems. We conclude our tutorial by presenting the challenges of stylized text generation and discussing future directions, such as small-data training, non-categorical style modeling, and a generalized scope of style transfer (e.g., controlling the syntax as a style). | Lili Mou, Olga Vechtomova |  |
| 92 |  |  [Multi-modal Information Extraction from Text, Semi-structured, and Tabular Data on the Web](https://doi.org/10.18653/v1/2020.acl-tutorials.6) |  | 0 | The World Wide Web contains vast quantities of textual information in several forms: unstructured text, template-based semi-structured webpages (which present data in key-value pairs and lists), and tables. Methods for extracting information from these sources and converting it to a structured form have been a target of research from the natural language processing (NLP), data mining, and database communities. While these researchers have largely separated extraction from web data into different problems based on the modality of the data, they have faced similar problems such as learning with limited labeled data, defining (or avoiding defining) ontologies, making use of prior knowledge, and scaling solutions to deal with the size of the Web. In this tutorial we take a holistic view toward information extraction, exploring the commonalities in the challenges and solutions developed to address these different forms of text. We will explore the approaches targeted at unstructured text that largely rely on learning syntactic or semantic textual patterns, approaches targeted at semi-structured documents that learn to identify structural patterns in the template, and approaches targeting web tables which rely heavily on entity linking and type information. While these different data modalities have largely been considered separately in the past, recent research has started taking a more inclusive approach toward textual extraction, in which the multiple signals offered by textual, layout, and visual clues are combined into a single extraction model made possible by new deep learning approaches. At the same time, trends within purely textual extraction have shifted toward full-document understanding rather than considering sentences as independent units. With this in mind, it is worth considering the information extraction problem as a whole to motivate solutions that harness textual semantics along with visual and semi-structured layout information. We will discuss these approaches and suggest avenues for future work. | Xin Luna Dong, Hannaneh Hajishirzi, Colin Lockard, Prashant Shiralkar |  |
| 93 |  |  [Commonsense Reasoning for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-tutorials.7) |  | 0 | Commonsense knowledge, such as knowing that “bumping into people annoys them” or “rain makes the road slippery”, helps humans navigate everyday situations seamlessly. Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades. In recent years, commonsense knowledge and reasoning have received renewed attention from the natural language processing (NLP) community, yielding exploratory studies in automated commonsense understanding. We organize this tutorial to provide researchers with the critical foundations and recent advances in commonsense representation and reasoning, in the hopes of casting a brighter light on this promising area of future research. In our tutorial, we will (1) outline the various types of commonsense (e.g., physical, social), and (2) discuss techniques to gather and represent commonsense knowledge, while highlighting the challenges specific to this type of knowledge (e.g., reporting bias). We will then (3) discuss the types of commonsense knowledge captured by modern NLP systems (e.g., large pretrained language models), and (4) present ways to measure systems’ commonsense reasoning abilities. We will finish with (5) a discussion of various ways in which commonsense reasoning can be used to improve performance on NLP tasks, exemplified by an (6) interactive session on integrating commonsense into a downstream task. | Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, Dan Roth |  |
| 94 |  |  [Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.acl-tutorials.8) |  | 0 | This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain QA, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain QA research in the next phase. | Danqi Chen, Wentau Yih |  |
| 95 |  |  [Learning to Understand Child-directed and Adult-directed Speech](https://doi.org/10.18653/v1/2020.acl-main.1) |  | 0 | Speech directed to children differs from adult-directed speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech. | Lieke Gelderloos, Grzegorz Chrupala, Afra Alishahi |  |
| 96 |  |  [Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts](https://doi.org/10.18653/v1/2020.acl-main.2) |  | 0 | Accurately diagnosing depression is difficult– requiring time-intensive interviews, assessments, and analysis. Hence, automated methods that can assess linguistic patterns in these interviews could help psychiatric professionals make faster, more informed decisions about diagnosis. We propose JLPC, a model that analyzes interview transcripts to identify depression while jointly categorizing interview prompts into latent categories. This latent categorization allows the model to define high-level conversational contexts that influence patterns of language in depressed individuals. We show that the proposed model not only outperforms competitive baselines, but that its latent prompt categories provide psycholinguistic insights about depression. | Alex Rinaldi, Jean E. Fox Tree, Snigdha Chaturvedi |  |
| 97 |  |  [Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling](https://doi.org/10.18653/v1/2020.acl-main.3) |  | 0 | As an essential task in task-oriented dialog systems, slot filling requires extensive training data in a certain domain. However, such data are not always available. Hence, cross-domain slot filling has naturally arisen to cope with this data scarcity problem. In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling. Our model first learns the general pattern of slot entities by detecting whether the tokens are slot entities or not. It then predicts the specific types for the slot entities. In addition, we propose a template regularization approach to improve the adaptation robustness by regularizing the representation of utterances based on utterance templates. Experimental results show that our model significantly outperforms state-of-the-art approaches in slot filling. Furthermore, our model can also be applied to the cross-domain named entity recognition task, and it achieves better adaptation performance than other existing baselines. The code is available at https://github.com/zliucr/coach. | Zihan Liu, Genta Indra Winata, Peng Xu, Pascale Fung |  |
| 98 |  |  [Designing Precise and Robust Dialogue Response Evaluators](https://doi.org/10.18653/v1/2020.acl-main.4) |  | 0 | Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation (> 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing. | Tianyu Zhao, Divesh Lala, Tatsuya Kawahara |  |
| 99 |  |  [Dialogue State Tracking with Explicit Slot Connection Modeling](https://doi.org/10.18653/v1/2020.acl-main.5) |  | 0 | Recent proposed approaches have made promising progress in dialogue state tracking (DST). However, in multi-domain scenarios, ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains. To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to explicitly consider slot correlations across different domains. Given a target slot, the slot connecting mechanism in DST-SC can infer its source slot and copy the source slot value directly, thus significantly reducing the difficulty of learning and reasoning. Experimental results verify the benefits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. | Yawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang, Jiajun Chen |  |
| 100 |  |  [Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy](https://doi.org/10.18653/v1/2020.acl-main.6) |  | 0 | Knowledge-driven conversation approaches have achieved remarkable research attention recently. However, generating an informative response with multiple relevant knowledge without losing fluency and coherence is still one of the main challenges. To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response decoding steps to incorporate appropriate knowledge. Furthermore, we introduce a knowledge copy mechanism using a knowledge-aware pointer network to copy words from external knowledge according to knowledge attention distribution. Our joint neural conversation model which integrates recurrent Knowledge-Interaction and knowledge Copy (KIC) performs well on generating informative responses. Experiments demonstrate that our model with fewer parameters yields significant improvements over competitive baselines on two datasets Wizard-of-Wikipedia(average Bleu +87%; abs.: 0.034) and DuConv(average Bleu +20%; abs.: 0.047)) with different knowledge formats (textual & structured) and different languages (English & Chinese). | Xiexiong Lin, Weiyu Jian, Jianshan He, Taifeng Wang, Wei Chu |  |
| 101 |  |  [Guiding Variational Response Generator to Exploit Persona](https://doi.org/10.18653/v1/2020.acl-main.7) |  | 0 | Leveraging persona information of users in Neural Response Generators (NRG) to perform personalized conversations has been considered as an attractive and important topic in the research of conversational agents over the past few years. Despite of the promising progress achieved by recent studies in this field, persona information tends to be incorporated into neural networks in the form of user embeddings, with the expectation that the persona can be involved via End-to-End learning. This paper proposes to adopt the personality-related characteristics of human conversations into variational response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant responses. Besides, to reasonably evaluate the performances of various persona modeling approaches, this paper further presents three direct persona-oriented metrics from different perspectives. The experimental results have shown that our proposed methodology can notably improve the performance of persona-aware response generation, and the metrics are reasonable to evaluate the results. | Bowen Wu, Mengyuan Li, Zongsheng Wang, Yifu Chen, Derek F. Wong, Qihang Feng, Junhong Huang, Baoxun Wang |  |
| 102 |  |  [Large Scale Multi-Actor Generative Dialog Modeling](https://doi.org/10.18653/v1/2020.acl-main.8) |  | 0 | Non-goal oriented dialog agents (i.e. chatbots) aim to produce varying and engaging conversations with a user; however, they typically exhibit either inconsistent personality across conversations or the average personality of all users. This paper addresses these issues by controlling an agent’s persona upon generation via conditioning on prior conversations of a target actor. In doing so, we are able to utilize more abstract patterns within a person’s speech and better emulate them in generated responses. This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor’s persona. We introduce an accompanying data collection procedure to obtain 10.3M conversations from 6 months worth of Reddit comments. We demonstrate that scaling model sizes from 117M to 8.3B parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7M held out Reddit conversations. Increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism (31% increased to 37% preference), style matching (37% to 42%), grammar and content quality (29% to 42%), and conversation coherency (32% to 40%). We find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. Through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control. | Alex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa Patwary, Bryan Catanzaro |  |
| 103 |  |  [PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable](https://doi.org/10.18653/v1/2020.acl-main.9) |  | 0 | Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework. | Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang |  |
| 104 |  |  [Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network](https://doi.org/10.18653/v1/2020.acl-main.10) |  | 0 | Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness. | Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, Ting Liu |  |
| 105 |  |  [Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations](https://doi.org/10.18653/v1/2020.acl-main.11) |  | 0 | We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task. This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019). We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor. In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain. | Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vulic, Matthew Henderson |  |
| 106 |  |  [Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2020.acl-main.12) |  | 0 | Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21%. | Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, Monica S. Lam |  |
| 107 |  |  [A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle](https://doi.org/10.18653/v1/2020.acl-main.13) |  | 0 | This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance. | ShyhShiun Hung, HenHsen Huang, HsinHsi Chen |  |
| 108 |  |  [TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2020.acl-main.14) |  | 0 | Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues. Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal. Therefore, we propose a novel TransS-driven joint learning architecture to address the issues. Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task. Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems. | Ruifang He, Jian Wang, Fengyu Guo, Yugui Han |  |
| 109 |  |  [A Study of Non-autoregressive Model for Sequence Generation](https://doi.org/10.18653/v1/2020.acl-main.15) |  | 0 | Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy. Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others. In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer: (1) Why NAR models can catch up with AR models in some tasks but not all? (2) Why techniques like knowledge distillation and source-target alignment can help NAR models. Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens. To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks. We have several interesting findings: 1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least. 2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models. 3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models. | Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, TieYan Liu |  |
| 110 |  |  [Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage](https://doi.org/10.18653/v1/2020.acl-main.16) |  | 0 | Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations. We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage. We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption. We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset. Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model. | Ashish V. Thapliyal, Radu Soricut |  |
| 111 |  |  [Fact-based Text Editing](https://doi.org/10.18653/v1/2020.acl-main.17) |  | 0 | We propose a novel text editing task, referred to as fact-based text editing, in which the goal is to revise a given document to better describe the facts in a knowledge base (e.g., several triples). The task is important in practice because reflecting the truth is a common requirement in text editing. First, we propose a method for automatically generating a dataset for research on fact-based text editing, where each instance consists of a draft text, a revised text, and several facts represented in triples. We apply the method into two public table-to-text datasets, obtaining two new datasets consisting of 233k and 37k instances, respectively. Next, we propose a new neural network architecture for fact-based text editing, called FactEditor, which edits a draft text by referring to given facts using a buffer, a stream, and a memory. A straightforward approach to address the problem would be to employ an encoder-decoder model. Our experimental results on the two datasets show that FactEditor outperforms the encoder-decoder approach in terms of fidelity and fluency. The results also show that FactEditor conducts inference faster than the encoder-decoder approach. | Hayate Iso, Chao Qiao, Hang Li |  |
| 112 |  |  [Few-Shot NLG with Pre-Trained Language Model](https://doi.org/10.18653/v1/2020.acl-main.18) |  | 0 | Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of few-shot natural language generation. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG | Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang |  |
| 113 |  |  [Fluent Response Generation for Conversational Question Answering](https://doi.org/10.18653/v1/2020.acl-main.19) |  | 0 | Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask. One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents. In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness. From a technical perspective, we use data augmentation to generate training data for an end-to-end system. Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019). Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses. We further show our model’s scalability by conducting tests on the CoQA dataset. The code and data are available at https://github.com/abaheti95/QADialogSystem. | Ashutosh Baheti, Alan Ritter, Kevin Small |  |
| 114 |  |  [Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs](https://doi.org/10.18653/v1/2020.acl-main.20) |  | 0 | One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training. | Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, Sung Ju Hwang |  |
| 115 |  |  [Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction](https://doi.org/10.18653/v1/2020.acl-main.21) |  | 0 | Traditional Question Generation (TQG) aims to generate a question given an input passage and an answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Since the frequently occurred information omission and coreference between questions, SQG is rather challenging. Prior works regarded SQG as a dialog generation task and recurrently produced each question. However, they suffered from problems caused by error cascades and could only capture limited context dependencies. To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works. | Zi Chai, Xiaojun Wan |  |
| 116 |  |  [Neural Syntactic Preordering for Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2020.acl-main.22) |  | 0 | Paraphrasing natural language sentences is a multifaceted process: it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization. Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner. Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model. First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model. This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks. Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order. Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases. | Tanya Goyal, Greg Durrett |  |
| 117 |  |  [Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders](https://doi.org/10.18653/v1/2020.acl-main.23) |  | 0 | Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents. Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion. When a new condition added, these techniques require full retraining. In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation. PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation. When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications. Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort. | Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li |  |
| 118 |  |  [Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order](https://doi.org/10.18653/v1/2020.acl-main.24) |  | 0 | Masked language model and autoregressive language model are two types of language models. While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG). In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM). We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM. We prove that u-PMLM is equivalent to an autoregressive permutated language model. One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation. Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks. | Yi Liao, Xin Jiang, Qun Liu |  |
| 119 |  |  [Reverse Engineering Configurations of Neural Text Generation Models](https://doi.org/10.18653/v1/2020.acl-main.25) |  | 0 | Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways. It is therefore desirable to develop a deeper understanding of the fundamental properties of such models. The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area. To this end, the extent and degree to which these artifacts surface in generated text is still unclear. In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text. Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate. Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone. This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought. | Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew Tomkins |  |
| 120 |  |  [Review-based Question Generation with Adaptive Instance Transfer and Augmentation](https://doi.org/10.18653/v1/2020.acl-main.26) |  | 0 | While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need. We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences. One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences. To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation. To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation. Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task. | Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam, Luo Si |  |
| 121 |  |  [TAG : Type Auxiliary Guiding for Code Comment Generation](https://doi.org/10.18653/v1/2020.acl-main.27) |  | 0 | Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies. | Ruichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuexing Hao, Yao Chen |  |
| 122 |  |  [Unsupervised Paraphrasing by Simulated Annealing](https://doi.org/10.18653/v1/2020.acl-main.28) |  | 0 | We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing. We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases. UPSA searches the sentence space towards this objective by performing a sequence of local editing. We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter. Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations. Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA. | Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song |  |
| 123 |  |  [A Joint Model for Document Segmentation and Segment Labeling](https://doi.org/10.18653/v1/2020.acl-main.29) |  | 0 | Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections. Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly. We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments. In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments. We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling. | Joe Barrow, Rajiv Jain, Vlad I. Morariu, Varun Manjunatha, Douglas W. Oard, Philip Resnik |  |
| 124 |  |  [Contextualized Weak Supervision for Text Classification](https://doi.org/10.18653/v1/2020.acl-main.30) |  | 0 | Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained. | Dheeraj Mekala, Jingbo Shang |  |
| 125 |  |  [Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks](https://doi.org/10.18653/v1/2020.acl-main.31) |  | 0 | Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task. However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words. Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN. We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document. Finally, the word nodes are aggregated as the document embedding. Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods. | Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, Liang Wang |  |
| 126 |  |  [Neural Topic Modeling with Bidirectional Adversarial Training](https://doi.org/10.18653/v1/2020.acl-main.32) |  | 0 | Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA). However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document. To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution. It uses a generator to capture the semantic patterns from texts and an encoder for topic inference. Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments. The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines. Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy. | Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye, Haiyang Xu |  |
| 127 |  |  [Text Classification with Negative Supervision](https://doi.org/10.18653/v1/2020.acl-main.33) |  | 0 | Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. To address this problem, we propose a simple multitask learning model that uses negative supervision. Specifically, our model encourages texts with different labels to have distinct representations. Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages. | Sora Ohashi, Junya Takayama, Tomoyuki Kajiwara, Chenhui Chu, Yuki Arase |  |
| 128 |  |  [Content Word Aware Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.34) |  | 0 | Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word. However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words). To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance. Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT. | Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita |  |
| 129 |  |  [Evaluating Explanation Methods for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.35) |  | 0 | Recently many efforts have been devoted to interpreting the black-box NMT models, but little progress has been made on metrics to evaluate explanation methods. Word Alignment Error Rate can be used as such a metric that matches human understanding, however, it can not measure explanation methods on those target words that are not aligned to any source word. This paper thereby makes an initial attempt to evaluate explanation methods from an alternative viewpoint. To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT model. As the exact computation for this metric is intractable, we employ an efficient approach as its approximation. On six standard translation tasks, we quantitatively evaluate several explanation methods in terms of the proposed metric and we reveal some valuable findings for these explanation methods in our experiments. | Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi |  |
| 130 |  |  [Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.36) |  | 0 | The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model. | Junliang Guo, Linli Xu, Enhong Chen |  |
| 131 |  |  [Learning Source Phrase Representations for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.37) |  | 0 | The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT). Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018). Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships. In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships. In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach. Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps. The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations. | Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu, Jingyi Zhang |  |
| 132 |  |  [Lipschitz Constrained Parameter Initialization for Deep Transformers](https://doi.org/10.18653/v1/2020.acl-main.38) |  | 0 | The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders. | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Jingyi Zhang |  |
| 133 |  |  [Location Attention for Extrapolation to Longer Sequences](https://doi.org/10.18653/v1/2020.acl-main.39) |  | 0 | Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues. | Yann Dubois, Gautier Dagan, Dieuwke Hupkes, Elia Bruni |  |
| 134 |  |  [Multiscale Collaborative Deep Models for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.40) |  | 0 | Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2 +3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials. | Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo |  |
| 135 |  |  [Norm-Based Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.41) |  | 0 | A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x). | Xuebo Liu, Houtim Lai, Derek F. Wong, Lidia S. Chao |  |
| 136 |  |  [Opportunistic Decoding with Timely Correction for Simultaneous Translation](https://doi.org/10.18653/v1/2020.acl-main.42) |  | 0 | Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation. | Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Liang Huang |  |
| 137 |  |  [A Formal Hierarchy of RNN Architectures](https://doi.org/10.18653/v1/2020.acl-main.43) |  | 0 | We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of “saturated” RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. We provide empirical results to support this conjecture. Experimental findings from training unsaturated networks on formal languages support this conjecture. | William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, Eran Yahav |  |
| 138 |  |  [A Three-Parameter Rank-Frequency Relation in Natural Languages](https://doi.org/10.18653/v1/2020.acl-main.44) |  | 0 | We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters. The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f). The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0. We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora. | Chenchen Ding, Masao Utiyama, Eiichiro Sumita |  |
| 139 |  |  [Dice Loss for Data-imbalanced NLP Tasks](https://doi.org/10.18653/v1/2020.acl-main.45) |  | 0 | Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training. The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples. In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the Sørensen–Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification. | Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li |  |
| 140 |  |  [Emergence of Syntax Needs Minimal Supervision](https://doi.org/10.18653/v1/2020.acl-main.46) |  | 0 | This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model. | Raphaël Bailly, Kata Gábor |  |
| 141 |  |  [Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese](https://doi.org/10.18653/v1/2020.acl-main.47) |  | 0 | We examine a methodology using neural language models (LMs) for analyzing the word order of language. This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods. In this study, we explore whether the LM-based method is valid for analyzing the word order. As a case study, this study focuses on Japanese due to its complex and flexible word order. To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies. Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool. Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments. | Tatsuki Kuribayashi, Takumi Ito, Jun Suzuki, Kentaro Inui |  |
| 142 |  |  [GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media](https://doi.org/10.18653/v1/2020.acl-main.48) |  | 0 | This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations. | YiJu Lu, ChengTe Li |  |
| 143 |  |  [Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection](https://doi.org/10.18653/v1/2020.acl-main.49) |  | 0 | Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability. | Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo, Ziang Wang |  |
| 144 |  |  [Predicting the Topical Stance and Political Leaning of Media using Tweets](https://doi.org/10.18653/v1/2020.acl-main.50) |  | 0 | Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers. Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly. In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic. We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy. | Peter Stefanov, Kareem Darwish, Atanas Atanasov, Preslav Nakov |  |
| 145 |  |  [Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora](https://doi.org/10.18653/v1/2020.acl-main.51) |  | 0 | The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew). | Hila Gonen, Ganesh Jawahar, Djamé Seddah, Yoav Goldberg |  |
| 146 |  |  [CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation](https://doi.org/10.18653/v1/2020.acl-main.52) |  | 0 | Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. CDL utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors. | Lei Shen, Yang Feng |  |
| 147 |  |  [Efficient Dialogue State Tracking by Selectively Overwriting Memory](https://doi.org/10.18653/v1/2020.acl-main.53) |  | 0 | Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance. | Sungdong Kim, Sohee Yang, Gyuwan Kim, SangWoo Lee |  |
| 148 |  |  [End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2](https://doi.org/10.18653/v1/2020.acl-main.54) |  | 0 | The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal. The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually. However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system. On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus. This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response. In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above. In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8). | DongHoon Ham, JeongGwan Lee, Youngsoo Jang, KeeEung Kim |  |
| 149 |  |  [Evaluating Dialogue Generation Systems via Response Selection](https://doi.org/10.18653/v1/2020.acl-main.55) |  | 0 | Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU. | Shiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki, Kentaro Inui |  |
| 150 |  |  [Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection](https://doi.org/10.18653/v1/2020.acl-main.56) |  | 0 | Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system. In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training. In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts. We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts. Moreover, a new negative sampling method is proposed to augment training data. Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts. | Yefei Zha, Ruobing Li, Hui Lin |  |
| 151 |  |  [Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment](https://doi.org/10.18653/v1/2020.acl-main.57) |  | 0 | Existing end-to-end dialog systems perform less effectively when data is scarce. To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems. In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration. We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning. Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset. | Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin Li, Jian Sun, Xiaodan Zhu |  |
| 152 |  |  [Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge](https://doi.org/10.18653/v1/2020.acl-main.58) |  | 0 | Neural-based context-aware models for slot tagging have achieved state-of-the-art performance. However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly model lexical relations. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets. | Keqing He, Yuanmeng Yan, Weiran Xu |  |
| 153 |  |  [Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition](https://doi.org/10.18653/v1/2020.acl-main.59) |  | 0 | Many studies have applied reinforcement learning to train a dialog policy and show great promise these years. One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. However, modeling a realistic user simulator is challenging. A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents. Two agents interact with each other and are jointly learned simultaneously. The method uses the actor-critic framework to facilitate pretraining and improve scalability. We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction. | Ryuichi Takanobu, Runze Liang, Minlie Huang |  |
| 154 |  |  [Paraphrase Augmented Task-Oriented Dialog Generation](https://doi.org/10.18653/v1/2020.acl-main.60) |  | 0 | Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set. However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings. We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance. We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels. PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019). Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ. PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings. | Silin Gao, Yichi Zhang, Zhijian Ou, Zhou Yu |  |
| 155 |  |  [Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation](https://doi.org/10.18653/v1/2020.acl-main.61) |  | 0 | Neural conversation models are known to generate appropriate but non-informative responses in general. A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document. In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory. In this paper, we propose to create the document memory with some anticipated responses in mind. This is achieved using a teacher-student framework. The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information. The student learns to construct a response-anticipated document memory from the first two sources, and teacher’s insight on memory creation. Empirical results show that our model outperforms the previous state-of-the-art for the CbR task. | Zhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing Xue, Yiping Song, Xiaojiang Liu, Nevin L. Zhang |  |
| 156 |  |  [Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation](https://doi.org/10.18653/v1/2020.acl-main.62) |  | 0 | Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues. To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive. To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning. The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations. The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. We further propose to learn action embeddings for a better generalization of the reward function. The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset. | Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang |  |
| 157 |  |  [Towards Unsupervised Language Understanding and Generation by Joint Dual Learning](https://doi.org/10.18653/v1/2020.acl-main.63) |  | 0 | In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. The source code is available at: https://github.com/MiuLab/DuaLUG. | ShangYu Su, ChaoWei Huang, YunNung Chen |  |
| 158 |  |  [USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation](https://doi.org/10.18653/v1/2020.acl-main.64) |  | 0 | The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog. | Shikib Mehri, Maxine Eskénazi |  |
| 159 |  |  [Explicit Semantic Decomposition for Definition Generation](https://doi.org/10.18653/v1/2020.acl-main.65) |  | 0 | Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts. However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines. | Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai, Jiajun Chen |  |
| 160 |  |  [Improved Natural Language Generation via Loss Truncation](https://doi.org/10.18653/v1/2020.acl-main.66) |  | 0 | Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss. While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts). Even a small fraction of noisy data can degrade the performance of log loss. As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references. However, distinguishability has not been used in practice due to challenges in optimization and estimation. We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability. Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task. Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references. | Daniel Kang, Tatsunori Hashimoto |  |
| 161 |  |  [Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks](https://doi.org/10.18653/v1/2020.acl-main.67) |  | 0 | Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling. | Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, Kai Yu |  |
| 162 |  |  [Rigid Formats Controlled Text Generation](https://doi.org/10.18653/v1/2020.acl-main.68) |  | 0 | Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation. | Piji Li, Haisong Zhang, Xiaojiang Liu, Shuming Shi |  |
| 163 |  |  [Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation](https://doi.org/10.18653/v1/2020.acl-main.69) |  | 0 | Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form. We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs. We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems. In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules. A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions. | Kaustubh D. Dhole, Christopher D. Manning |  |
| 164 |  |  [An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering](https://doi.org/10.18653/v1/2020.acl-main.70) |  | 0 | Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution. Existing approaches often exploit short text streams in a batch way. However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve. In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering. Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way. Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets. | Jay Kumar, Junming Shao, Salah Uddin, Wazir Ali |  |
| 165 |  |  [Generative Semantic Hashing Enhanced via Boltzmann Machines](https://doi.org/10.18653/v1/2020.acl-main.71) |  | 0 | Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint. For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes. From the perspectives of both model representation and code space size, independence is always not the best assumption. In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior. To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution. Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO). With these novel techniques, the entire model can be optimized efficiently. Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains. | Lin Zheng, Qinliang Su, Dinghan Shen, Changyou Chen |  |
| 166 |  |  [Interactive Construction of User-Centric Dictionary for Text Analytics](https://doi.org/10.18653/v1/2020.acl-main.72) |  | 0 | We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis. This paper introduces the first formulation of interactive dictionary construction to address this issue. To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms. Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task. Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions. | Ryosuke Kohita, Issei Yoshida, Hiroshi Kanayama, Tetsuya Nasukawa |  |
| 167 |  |  [Tree-Structured Neural Topic Model](https://doi.org/10.18653/v1/2020.acl-main.73) |  | 0 | This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks. | Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata |  |
| 168 |  |  [Unsupervised FAQ Retrieval with Question Generation and BERT](https://doi.org/10.18653/v1/2020.acl-main.74) |  | 0 | We focus on the task of Frequently Asked Questions (FAQ) retrieval. A given user query can be matched against the questions and/or the answers in the FAQ. We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models. The two models match user queries to FAQ answers and questions, respectively. We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases. We show that our model is on par and even outperforms supervised models on existing datasets. | Yosi Mass, Boaz Carmeli, Haggai Roitman, David Konopnicki |  |
| 169 |  |  ["The Boating Store Had Its Best Sail Ever": Pronunciation-attentive Contextualized Pun Recognition](https://doi.org/10.18653/v1/2020.acl-main.75) |  | 0 | Humor plays an important role in human languages and it is essential to model humor when building intelligence systems. Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity. However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks. In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence. PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols. Extensive experiments are conducted on two benchmark datasets. Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks. In-depth analyses verify the effectiveness and robustness of PCPR. | Yichao Zhou, JyunYu Jiang, Jieyu Zhao, KaiWei Chang, Wei Wang |  |
| 170 |  |  [Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning](https://doi.org/10.18653/v1/2020.acl-main.76) |  | 0 | Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA). The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT. In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task. Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks. Code is available at https://github.com/joongbo/tta. | Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung |  |
| 171 |  |  [Fine-grained Interest Matching for Neural News Recommendation](https://doi.org/10.18653/v1/2020.acl-main.77) |  | 0 | Personalized news recommendation is a critical technology to improve users’ online news reading experience. The core of news recommendation is accurate matching between user’s interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation. Instead of aggregating user’s all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation. | Heyuan Wang, Fangzhao Wu, Zheng Liu, Xing Xie |  |
| 172 |  |  [Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder](https://doi.org/10.18653/v1/2020.acl-main.78) |  | 0 | Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We empirically evaluate the framework on a real-world dataset. The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. SemiORC also outperforms other baseline methods on operational risk classification. | Fan Zhou, Shengming Zhang, Yi Yang |  |
| 173 |  |  [Interpreting Twitter User Geolocation](https://doi.org/10.18653/v1/2020.acl-main.79) |  | 0 | Identifying user geolocation in online social networks is an essential task in many location-based applications. Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior. In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users. This methodology helps with providing meaningful explanations on prediction results. Furthermore, it also initiates an attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes. | Ting Zhong, Tianliang Wang, Fan Zhou, Goce Trajcevski, Kunpeng Zhang, Yi Yang |  |
| 174 |  |  [Modeling Code-Switch Languages Using Bilingual Parallel Corpus](https://doi.org/10.18653/v1/2020.acl-main.80) |  | 0 | Language modeling is the technique to estimate the probability of a sequence of words. A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. The attention mechanism learns the bilingual context from a parallel corpus. BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result. We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea. | Grandee Lee, Haizhou Li |  |
| 175 |  |  [SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check](https://doi.org/10.18653/v1/2020.acl-main.81) |  | 0 | Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN). The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin. | Xingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua Jiang, Feng Wang, Taifeng Wang, Wei Chu, Yuan Qi |  |
| 176 |  |  [Spelling Error Correction with Soft-Masked BERT](https://doi.org/10.18653/v1/2020.acl-main.82) |  | 0 | Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability. Without loss of generality we consider Chinese spelling error correction (CSC) in this paper. A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model. The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling. In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique. Our method of using ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems. Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. | Shaohua Zhang, Haoran Huang, Jicong Liu, Hang Li |  |
| 177 |  |  [A Frame-based Sentence Representation for Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.83) |  | 0 | Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC). MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge. To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling. Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema. Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations. Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task. | Shaoru Guo, Ru Li, Hongye Tan, Xiaoli Li, Yong Guan, Hongyan Zhao, Yueping Zhang |  |
| 178 |  |  [A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation](https://doi.org/10.18653/v1/2020.acl-main.84) |  | 0 | In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT). This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of the tokens to the operations. Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens. We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance. | Jan Deriu, Katsiaryna Mlynchyk, Philippe Schläpfer, Álvaro Rodrigo, Dirk Von Gruenigen, Nicolas Kaiser, Kurt Stockinger, Eneko Agirre, Mark Cieliebak |  |
| 179 |  |  [Contextualized Sparse Representations for Real-Time Open-Domain Question Answering](https://doi.org/10.18653/v1/2020.acl-main.85) |  | 0 |  | Jinhyuk Lee, Min Joon Seo, Hannaneh Hajishirzi, Jaewoo Kang |  |
| 180 |  |  [Dynamic Sampling Strategies for Multi-Task Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.86) |  | 0 |  | Ananth Gottumukkala, Dheeru Dua, Sameer Singh, Matt Gardner |  |
| 181 |  |  [Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.87) |  | 0 |  | Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu, Daxin Jiang |  |
| 182 |  |  [Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading](https://doi.org/10.18653/v1/2020.acl-main.88) |  | 0 |  | Yifan Gao, ChienSheng Wu, Shafiq R. Joty, Caiming Xiong, Richard Socher, Irwin King, Michael R. Lyu, Steven C. H. Hoi |  |
| 183 |  |  [Injecting Numerical Reasoning Skills into Language Models](https://doi.org/10.18653/v1/2020.acl-main.89) |  | 0 |  | Mor Geva, Ankit Gupta, Jonathan Berant |  |
| 184 |  |  [Learning to Identify Follow-Up Questions in Conversational Question Answering](https://doi.org/10.18653/v1/2020.acl-main.90) |  | 0 |  | Souvik Kundu, Qian Lin, Hwee Tou Ng |  |
| 185 |  |  [Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases](https://doi.org/10.18653/v1/2020.acl-main.91) |  | 0 |  | Yunshi Lan, Jing Jiang |  |
| 186 |  |  [A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers](https://doi.org/10.18653/v1/2020.acl-main.92) |  | 0 |  | ShenYun Miao, ChaoChun Liang, KehYih Su |  |
| 187 |  |  [Improving Image Captioning Evaluation by Considering Inter References Variance](https://doi.org/10.18653/v1/2020.acl-main.93) |  | 0 |  | Yanzhi Yi, Hangyu Deng, Jinglu Hu |  |
| 188 |  |  [Revisiting the Context Window for Cross-lingual Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.94) |  | 0 |  | Ryokan Ri, Yoshimasa Tsuruoka |  |
| 189 |  |  [Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders](https://doi.org/10.18653/v1/2020.acl-main.95) |  | 0 |  | Terra Blevins, Luke Zettlemoyer |  |
| 190 |  |  [Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection](https://doi.org/10.18653/v1/2020.acl-main.96) |  | 0 |  | Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro, Animesh Mukherjee |  |
| 191 |  |  [DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification](https://doi.org/10.18653/v1/2020.acl-main.97) |  | 0 |  | Lianwei Wu, Yuan Rao, Yongqiang Zhao, Hao Liang, Ambreen Nazir |  |
| 192 |  |  [Towards Conversational Recommendation over Multi-Type Dialogs](https://doi.org/10.18653/v1/2020.acl-main.98) |  | 0 |  | Zeming Liu, Haifeng Wang, ZhengYu Niu, Hua Wu, Wanxiang Che, Ting Liu |  |
| 193 |  |  [Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent Classification](https://doi.org/10.18653/v1/2020.acl-main.99) |  | 0 |  | Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, XiaoMing Wu, Albert Y. S. Lam |  |
| 194 |  |  [Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen](https://doi.org/10.18653/v1/2020.acl-main.100) |  | 0 |  | Yixin Cao, Ruihao Shui, Liangming Pan, MinYen Kan, Zhiyuan Liu, TatSeng Chua |  |
| 195 |  |  [Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints](https://doi.org/10.18653/v1/2020.acl-main.101) |  | 0 |  | Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen |  |
| 196 |  |  [Dynamic Memory Induction Networks for Few-Shot Text Classification](https://doi.org/10.18653/v1/2020.acl-main.102) |  | 0 |  | Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu |  |
| 197 |  |  [Exclusive Hierarchical Decoding for Deep Keyphrase Generation](https://doi.org/10.18653/v1/2020.acl-main.103) |  | 0 |  | Wang Chen, Hou Pong Chan, Piji Li, Irwin King |  |
| 198 |  |  [Hierarchy-Aware Global Model for Hierarchical Text Classification](https://doi.org/10.18653/v1/2020.acl-main.104) |  | 0 |  | Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie, Gongshen Liu |  |
| 199 |  |  [Keyphrase Generation for Scientific Document Retrieval](https://doi.org/10.18653/v1/2020.acl-main.105) |  | 0 |  | Florian Boudin, Ygor Gallina, Akiko Aizawa |  |
| 200 |  |  [A Graph Auto-encoder Model of Derivational Morphology](https://doi.org/10.18653/v1/2020.acl-main.106) |  | 0 |  | Valentin Hofmann, Hinrich Schütze, Janet B. Pierrehumbert |  |
| 201 |  |  [Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell](https://doi.org/10.18653/v1/2020.acl-main.107) |  | 0 |  | Djamé Seddah, Farah Essaidi, Amal Fethi, Matthieu Futeral, Benjamin Muller, Pedro Javier Ortiz Suárez, Benoît Sagot, Abhishek Srivastava |  |
| 202 |  |  [Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis](https://doi.org/10.18653/v1/2020.acl-main.108) |  | 0 |  | Janek Bevendorff, Khalid Al Khatib, Martin Potthast, Benno Stein |  |
| 203 |  |  [Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences](https://doi.org/10.18653/v1/2020.acl-main.109) |  | 0 |  | Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika Mitnik, Lilja Maria Saeboe, Omri Abend |  |
| 204 |  |  [Generating Counter Narratives against Online Hate Speech: Data and Strategies](https://doi.org/10.18653/v1/2020.acl-main.110) |  | 0 |  | Serra Sinem Tekiroglu, YiLing Chung, Marco Guerini |  |
| 205 |  |  [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.111) |  | 0 |  | Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik |  |
| 206 |  |  [Learning and Evaluating Emotion Lexicons for 91 Languages](https://doi.org/10.18653/v1/2020.acl-main.112) |  | 0 |  | Sven Buechel, Susanna Rücker, Udo Hahn |  |
| 207 |  |  [Multi-Hypothesis Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.acl-main.113) |  | 0 |  | Marina Fomicheva, Lucia Specia, Francisco Guzmán |  |
| 208 |  |  [Multimodal Quality Estimation for Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.114) |  | 0 |  | Shu Okabe, Frédéric Blain, Lucia Specia |  |
| 209 |  |  [PuzzLing Machines: A Challenge on Learning From Small Data](https://doi.org/10.18653/v1/2020.acl-main.115) |  | 0 |  | Gözde Gül Sahin, Yova Kementchedjhieva, Phillip Rust, Iryna Gurevych |  |
| 210 |  |  [The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain](https://doi.org/10.18653/v1/2020.acl-main.116) |  | 0 |  | Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Marusczyk, Lukas Lange |  |
| 211 |  |  [The TechQA Dataset](https://doi.org/10.18653/v1/2020.acl-main.117) |  | 0 |  | Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, J. Scott McCarley, Mike McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John F. Pitrelli, Saurabh Pujar, Salim Roukos, Andrzej Sakrajda, Avirup Sil, Rosario UcedaSosa, Todd Ward, Rong Zhang |  |
| 212 |  |  [iSarcasm: A Dataset of Intended Sarcasm](https://doi.org/10.18653/v1/2020.acl-main.118) |  | 0 |  | Silviu Oprea, Walid Magdy |  |
| 213 |  |  [AMR Parsing via Graph-Sequence Iterative Inference](https://doi.org/10.18653/v1/2020.acl-main.119) |  | 0 |  | Deng Cai, Wai Lam |  |
| 214 |  |  [A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal](https://doi.org/10.18653/v1/2020.acl-main.120) |  | 0 |  | Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, Georgiana Ifrim |  |
| 215 |  |  [Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2020.acl-main.121) |  | 0 |  | Junnan Zhu, Yu Zhou, Jiajun Zhang, Chengqing Zong |  |
| 216 |  |  [Examining the State-of-the-Art in News Timeline Summarization](https://doi.org/10.18653/v1/2020.acl-main.122) |  | 0 |  | Demian Gholipour Ghalandari, Georgiana Ifrim |  |
| 217 |  |  [Improving Truthfulness of Headline Generation](https://doi.org/10.18653/v1/2020.acl-main.123) |  | 0 |  | Kazuki Matsumaru, Sho Takase, Naoaki Okazaki |  |
| 218 |  |  [SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.124) |  | 0 |  | Yang Gao, Wei Zhao, Steffen Eger |  |
| 219 |  |  [Self-Attention Guided Copy Mechanism for Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.125) |  | 0 |  | Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He, Bowen Zhou |  |
| 220 |  |  [Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation](https://doi.org/10.18653/v1/2020.acl-main.126) |  | 0 |  | Weixin Liang, James Zou, Zhou Yu |  |
| 221 |  |  [Conversational Word Embedding for Retrieval-Based Dialog System](https://doi.org/10.18653/v1/2020.acl-main.127) |  | 0 |  | Wentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang, Guoping Hu |  |
| 222 |  |  [Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network](https://doi.org/10.18653/v1/2020.acl-main.128) |  | 0 |  | Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu, Ting Liu |  |
| 223 |  |  [Learning Dialog Policies from Weak Demonstrations](https://doi.org/10.18653/v1/2020.acl-main.129) |  | 0 |  | Gabriel GordonHall, Philip John Gorinski, Shay B. Cohen |  |
| 224 |  |  [MuTual: A Dataset for Multi-Turn Dialogue Reasoning](https://doi.org/10.18653/v1/2020.acl-main.130) |  | 0 |  | Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, Ming Zhou |  |
| 225 |  |  [You Impress Me: Dialogue Generation via Mutual Persona Perception](https://doi.org/10.18653/v1/2020.acl-main.131) |  | 0 |  | Qian Liu, Yihong Chen, Bei Chen, JianGuang Lou, Zixuan Chen, Bin Zhou, Dongmei Zhang |  |
| 226 |  |  [Bridging Anaphora Resolution as Question Answering](https://doi.org/10.18653/v1/2020.acl-main.132) |  | 0 |  | Yufang Hou |  |
| 227 |  |  [Dialogue Coherence Assessment Without Explicit Dialogue Act Labels](https://doi.org/10.18653/v1/2020.acl-main.133) |  | 0 |  | Mohsen Mesgar, Sebastian Bücker, Iryna Gurevych |  |
| 228 |  |  [Fast and Accurate Non-Projective Dependency Tree Linearization](https://doi.org/10.18653/v1/2020.acl-main.134) |  | 0 |  | Xiang Yu, Simon Tannert, Ngoc Thang Vu, Jonas Kuhn |  |
| 229 |  |  [Semantic Graphs for Generating Deep Questions](https://doi.org/10.18653/v1/2020.acl-main.135) |  | 0 |  | Liangming Pan, Yuxi Xie, Yansong Feng, TatSeng Chua, MinYen Kan |  |
| 230 |  |  [A Novel Cascade Binary Tagging Framework for Relational Triple Extraction](https://doi.org/10.18653/v1/2020.acl-main.136) |  | 0 |  | Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, Yi Chang |  |
| 231 |  |  [In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts](https://doi.org/10.18653/v1/2020.acl-main.137) |  | 0 |  | Ruben Kruiper, Julian F. V. Vincent, Jessica ChenBurger, Marc P. Y. Desmulliez, Ioannis Konstas |  |
| 232 |  |  [NAT: Noise-Aware Training for Robust Neural Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.138) |  | 0 |  | Marcin Namysl, Sven Behnke, Joachim Köhler |  |
| 233 |  |  [Named Entity Recognition without Labelled Data: A Weak Supervision Approach](https://doi.org/10.18653/v1/2020.acl-main.139) |  | 0 |  | Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, Samia Touileb |  |
| 234 |  |  [Probing Linguistic Features of Sentence-Level Representations in Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.140) |  | 0 |  | Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig |  |
| 235 |  |  [Reasoning with Latent Structure Refinement for Document-Level Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.141) |  | 0 |  | Guoshun Nan, Zhijiang Guo, Ivan Sekulic, Wei Lu |  |
| 236 |  |  [TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task](https://doi.org/10.18653/v1/2020.acl-main.142) |  | 0 |  | Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig |  |
| 237 |  |  [Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences](https://doi.org/10.18653/v1/2020.acl-main.143) |  | 0 |  | Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo, Yue Zhang |  |
| 238 |  |  [Boosting Neural Machine Translation with Similar Translations](https://doi.org/10.18653/v1/2020.acl-main.144) |  | 0 |  | Jitao Xu, Josep Maria Crego, Jean Senellart |  |
| 239 |  |  [Character-Level Translation with Self-attention](https://doi.org/10.18653/v1/2020.acl-main.145) |  | 0 |  | Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, Richard H. R. Hahnloser |  |
| 240 |  |  [End-to-End Neural Word Alignment Outperforms GIZA++](https://doi.org/10.18653/v1/2020.acl-main.146) |  | 0 |  | Thomas Zenkel, Joern Wuebker, John DeNero |  |
| 241 |  |  [Enhancing Machine Translation with Dependency-Aware Self-Attention](https://doi.org/10.18653/v1/2020.acl-main.147) |  | 0 |  | Emanuele Bugliarello, Naoaki Okazaki |  |
| 242 |  |  [Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation](https://doi.org/10.18653/v1/2020.acl-main.148) |  | 0 |  | Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich |  |
| 243 |  |  [It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information](https://doi.org/10.18653/v1/2020.acl-main.149) |  | 0 |  | Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, Naoaki Okazaki |  |
| 244 |  |  [Language-aware Interlingua for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.150) |  | 0 |  | Changfeng Zhu, Heng Yu, Shanbo Cheng, Weihua Luo |  |
| 245 |  |  [On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation](https://doi.org/10.18653/v1/2020.acl-main.151) |  | 0 |  | Wei Zhao, Goran Glavas, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger |  |
| 246 |  |  [Parallel Sentence Mining by Constrained Decoding](https://doi.org/10.18653/v1/2020.acl-main.152) |  | 0 |  | Pinzhen Chen, Nikolay Bogoychev, Kenneth Heafield, Faheem Kirefu |  |
| 247 |  |  [Self-Attention with Cross-Lingual Position Representation](https://doi.org/10.18653/v1/2020.acl-main.153) |  | 0 |  | Liang Ding, Longyue Wang, Dacheng Tao |  |
| 248 |  |  ["You Sound Just Like Your Father" Commercial Machine Translation Systems Include Stylistic Biases](https://doi.org/10.18653/v1/2020.acl-main.154) |  | 0 |  | Dirk Hovy, Federico Bianchi, Tommaso Fornaciari |  |
| 249 |  |  [MMPE: A Multi-Modal Interface for Post-Editing Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.155) |  | 0 |  | Nico Herbig, Tim Düwel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Krüger, Josef van Genabith |  |
| 250 |  |  [A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages](https://doi.org/10.18653/v1/2020.acl-main.156) |  | 0 |  | Pedro Javier Ortiz Suárez, Laurent Romary, Benoît Sagot |  |
| 251 |  |  [Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter](https://doi.org/10.18653/v1/2020.acl-main.157) |  | 0 |  | Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel Collier |  |
| 252 |  |  [A Systematic Assessment of Syntactic Generalization in Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.158) |  | 0 |  | Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, Roger Levy |  |
| 253 |  |  [Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for German Plurals](https://doi.org/10.18653/v1/2020.acl-main.159) |  | 0 |  | Kate McCurdy, Sharon Goldwater, Adam Lopez |  |
| 254 |  |  [Overestimation of Syntactic Representation in Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.160) |  | 0 |  | Jordan Kodner, Nitish Gupta |  |
| 255 |  |  [Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation](https://doi.org/10.18653/v1/2020.acl-main.161) |  | 0 |  | David Wilmot, Frank Keller |  |
| 256 |  |  [You Don't Have Time to Read This: An Exploration of Document Reading Time Prediction](https://doi.org/10.18653/v1/2020.acl-main.162) |  | 0 |  | Orion Weller, Jordan Hildebrandt, Ilya Reznik, Christopher Challis, E. Shannon Tass, Quinn Snell, Kevin D. Seppi |  |
| 257 |  |  [A Generative Model for Joint Natural Language Understanding and Generation](https://doi.org/10.18653/v1/2020.acl-main.163) |  | 0 |  | BoHsiang Tseng, Jianpeng Cheng, Yimai Fang, David Vandyke |  |
| 258 |  |  [Automatic Detection of Generated Text is Easiest when Humans are Fooled](https://doi.org/10.18653/v1/2020.acl-main.164) |  | 0 |  | Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, Douglas Eck |  |
| 259 |  |  [Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing](https://doi.org/10.18653/v1/2020.acl-main.165) |  | 0 |  | Haoming Jiang, Chen Liang, Chong Wang, Tuo Zhao |  |
| 260 |  |  [Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation](https://doi.org/10.18653/v1/2020.acl-main.166) |  | 0 |  | Jun Xu, Haifeng Wang, ZhengYu Niu, Hua Wu, Wanxiang Che, Ting Liu |  |
| 261 |  |  [GPT-too: A Language-Model-First Approach for AMR-to-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.167) |  | 0 |  | Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md. Arafat Sultan, YoungSuk Lee, Radu Florian, Salim Roukos |  |
| 262 |  |  [Learning to Update Natural Language Comments Based on Code Changes](https://doi.org/10.18653/v1/2020.acl-main.168) |  | 0 |  | Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, Raymond J. Mooney |  |
| 263 |  |  [Politeness Transfer: A Tag and Generate Approach](https://doi.org/10.18653/v1/2020.acl-main.169) |  | 0 |  | Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabás Póczos, Graham Neubig, Yiming Yang, Ruslan Salakhutdinov, Alan W. Black, Shrimai Prabhumoye |  |
| 264 |  |  [BPE-Dropout: Simple and Effective Subword Regularization](https://doi.org/10.18653/v1/2020.acl-main.170) |  | 0 |  | Ivan Provilkov, Dmitrii Emelianenko, Elena Voita |  |
| 265 |  |  [Improving Non-autoregressive Neural Machine Translation with Monolingual Data](https://doi.org/10.18653/v1/2020.acl-main.171) |  | 0 |  | Jiawei Zhou, Phillip Keung |  |
| 266 |  |  [Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.172) |  | 0 |  | Sajad Sotudeh Gharebagh, Nazli Goharian, Ross W. Filice |  |
| 267 |  |  [On Faithfulness and Factuality in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.173) |  | 0 |  | Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan T. McDonald |  |
| 268 |  |  [Screenplay Summarization Using Latent Narrative Structure](https://doi.org/10.18653/v1/2020.acl-main.174) |  | 0 |  | Pinelopi Papalampidi, Frank Keller, Lea Frermann, Mirella Lapata |  |
| 269 |  |  [Unsupervised Opinion Summarization with Noising and Denoising](https://doi.org/10.18653/v1/2020.acl-main.175) |  | 0 |  | Reinald Kim Amplayo, Mirella Lapata |  |
| 270 |  |  [A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimer's Type](https://doi.org/10.18653/v1/2020.acl-main.176) |  | 0 |  | Trevor Cohen, Serguei Pakhomov |  |
| 271 |  |  [Probing Linguistic Systematicity](https://doi.org/10.18653/v1/2020.acl-main.177) |  | 0 |  | Emily Goodwin, Koustuv Sinha, Timothy J. O'Donnell |  |
| 272 |  |  [Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.178) |  | 0 |  | Maarten Sap, Eric Horvitz, Yejin Choi, Noah A. Smith, James W. Pennebaker |  |
| 273 |  |  [Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment](https://doi.org/10.18653/v1/2020.acl-main.179) |  | 0 |  | Forrest Davis, Marten van Schijndel |  |
| 274 |  |  [Speakers enhance contextually confusable words](https://doi.org/10.18653/v1/2020.acl-main.180) |  | 0 |  | Eric Meinhardt, Eric Bakovic, Leon Bergen |  |
| 275 |  |  [What determines the order of adjectives in English? Comparing efficiency-based theories using dependency treebanks](https://doi.org/10.18653/v1/2020.acl-main.181) |  | 0 |  | Richard Futrell, William Dyer, Gregory Scontras |  |
| 276 |  |  ["None of the Above": Measure Uncertainty in Dialog Response Retrieval](https://doi.org/10.18653/v1/2020.acl-main.182) |  | 0 |  | Yulan Feng, Shikib Mehri, Maxine Eskénazi, Tiancheng Zhao |  |
| 277 |  |  [Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills](https://doi.org/10.18653/v1/2020.acl-main.183) |  | 0 |  | Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, YLan Boureau |  |
| 278 |  |  [Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs](https://doi.org/10.18653/v1/2020.acl-main.184) |  | 0 |  | Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu |  |
| 279 |  |  [Negative Training for Neural Dialogue Response Generation](https://doi.org/10.18653/v1/2020.acl-main.185) |  | 0 |  | Tianxing He, James R. Glass |  |
| 280 |  |  [Recursive Template-based Frame Generation for Task Oriented Dialog](https://doi.org/10.18653/v1/2020.acl-main.186) |  | 0 |  | Rashmi Gangadharaiah, Balakrishnan Narayanaswamy |  |
| 281 |  |  [Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback](https://doi.org/10.18653/v1/2020.acl-main.187) |  | 0 |  | Ahmed Elgohary, Saghar Hosseini, Ahmed Hassan Awadallah |  |
| 282 |  |  [Calibrating Structured Output Predictors for Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-main.188) |  | 0 |  | Abhyuday Jagannatha, Hong Yu |  |
| 283 |  |  [Active Imitation Learning with Noisy Guidance](https://doi.org/10.18653/v1/2020.acl-main.189) |  | 0 |  | Kianté Brantley, Hal Daumé III, Amr Sharaf |  |
| 284 |  |  [ExpBERT: Representation Engineering with Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.190) |  | 0 |  | Shikhar Murty, Pang Wei Koh, Percy Liang |  |
| 285 |  |  [GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples](https://doi.org/10.18653/v1/2020.acl-main.191) |  | 0 |  | Danilo Croce, Giuseppe Castellucci, Roberto Basili |  |
| 286 |  |  [Generalizing Natural Language Analysis through Span-relation Representations](https://doi.org/10.18653/v1/2020.acl-main.192) |  | 0 |  | Zhengbao Jiang, Wei Xu, Jun Araki, Graham Neubig |  |
| 287 |  |  [Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.193) |  | 0 |  | Ouyu Lan, Xiao Huang, Bill Yuchen Lin, He Jiang, Liyuan Liu, Xiang Ren |  |
| 288 |  |  [MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2020.acl-main.194) |  | 0 |  | Jiaao Chen, Zichao Yang, Diyi Yang |  |
| 289 |  |  [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://doi.org/10.18653/v1/2020.acl-main.195) |  | 0 |  | Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou |  |
| 290 |  |  [On Importance Sampling-Based Evaluation of Latent Language Models](https://doi.org/10.18653/v1/2020.acl-main.196) |  | 0 |  | Robert L. Logan IV, Matt Gardner, Sameer Singh |  |
| 291 |  |  [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization](https://doi.org/10.18653/v1/2020.acl-main.197) |  | 0 |  | Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Tuo Zhao |  |
| 292 |  |  [Stolen Probability: A Structural Weakness of Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.198) |  | 0 |  | David Demeter, Gregory Kimmel, Doug Downey |  |
| 293 |  |  [Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer](https://doi.org/10.18653/v1/2020.acl-main.199) |  | 0 |  | Chao Shang, Sarthak Dash, Md. Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Alfio Gliozzo |  |
| 294 |  |  [To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks](https://doi.org/10.18653/v1/2020.acl-main.200) |  | 0 |  | Sinong Wang, Madian Khabsa, Hao Ma |  |
| 295 |  |  [Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries](https://doi.org/10.18653/v1/2020.acl-main.201) |  | 0 |  | Mozhi Zhang, Yoshinari Fujinuma, Michael J. Paul, Jordan L. BoydGraber |  |
| 296 |  |  [XtremeDistil: Multi-stage Distillation for Massive Multilingual Models](https://doi.org/10.18653/v1/2020.acl-main.202) |  | 0 |  | Subhabrata Mukherjee, Ahmed Hassan Awadallah |  |
| 297 |  |  [A Girl Has A Name: Detecting Authorship Obfuscation](https://doi.org/10.18653/v1/2020.acl-main.203) |  | 0 |  | Asad Mahmood, Zubair Shafiq, Padmini Srinivasan |  |
| 298 |  |  [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://doi.org/10.18653/v1/2020.acl-main.204) |  | 0 |  | Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin |  |
| 299 |  |  [Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks](https://doi.org/10.18653/v1/2020.acl-main.205) |  | 0 |  | Kervy Rivas Rojas, Gina Bustamante, Arturo Oncevay, Marco Antonio Sobrevilla Cabezudo |  |
| 300 |  |  [Investigating the effect of auxiliary objectives for the automated grading of learner English speech transcriptions](https://doi.org/10.18653/v1/2020.acl-main.206) |  | 0 |  | Hannah Craighead, Andrew Caines, Paula Buttery, Helen Yannakoudakis |  |
| 301 |  |  [SPECTER: Document-level Representation Learning using Citation-informed Transformers](https://doi.org/10.18653/v1/2020.acl-main.207) |  | 0 |  | Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S. Weld |  |
| 302 |  |  [Semantic Scaffolds for Pseudocode-to-Code Generation](https://doi.org/10.18653/v1/2020.acl-main.208) |  | 0 |  | Ruiqi Zhong, Mitchell Stern, Dan Klein |  |
| 303 |  |  [Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction](https://doi.org/10.18653/v1/2020.acl-main.209) |  | 0 |  | Samuel Broscheit, Kiril Gashteovski, Yanjie Wang, Rainer Gemulla |  |
| 304 |  |  [INFOTABS: Inference on Tables as Semi-structured Data](https://doi.org/10.18653/v1/2020.acl-main.210) |  | 0 |  | Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar |  |
| 305 |  |  [Interactive Machine Comprehension with Information Seeking Agents](https://doi.org/10.18653/v1/2020.acl-main.211) |  | 0 |  | Xingdi Yuan, Jie Fu, MarcAlexandre Côté, Yi Tay, Chris Pal, Adam Trischler |  |
| 306 |  |  [Syntactic Data Augmentation Increases Robustness to Inference Heuristics](https://doi.org/10.18653/v1/2020.acl-main.212) |  | 0 |  | Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, Tal Linzen |  |
| 307 |  |  [Improved Speech Representations with Multi-Target Autoregressive Predictive Coding](https://doi.org/10.18653/v1/2020.acl-main.213) |  | 0 |  | YuAn Chung, James R. Glass |  |
| 308 |  |  [Integrating Multimodal Information in Large Pretrained Transformers](https://doi.org/10.18653/v1/2020.acl-main.214) |  | 0 |  | Wasifur Rahman, Md. Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, LouisPhilippe Morency, Mohammed E. Hoque |  |
| 309 |  |  [MultiQT: Multimodal learning for real-time question tracking in speech](https://doi.org/10.18653/v1/2020.acl-main.215) |  | 0 |  | Jakob D. Havtorn, Jan Latko, Joakim Edin, Lars Maaløe, Lasse Borgholt, Lorenzo Belgrano, Nicolai Jacobsen, Regitze Sdun, Zeljko Agic |  |
| 310 |  |  [Multimodal and Multiresolution Speech Recognition with Transformers](https://doi.org/10.18653/v1/2020.acl-main.216) |  | 0 |  | Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, Shiva Sundaram |  |
| 311 |  |  [Phone Features Improve Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.217) |  | 0 |  | Elizabeth Salesky, Alan W. Black |  |
| 312 |  |  [Grounding Conversations with Improvised Dialogues](https://doi.org/10.18653/v1/2020.acl-main.218) |  | 0 |  | Hyundong Cho, Jonathan May |  |
| 313 |  |  [Image-Chat: Engaging Grounded Conversations](https://doi.org/10.18653/v1/2020.acl-main.219) |  | 0 |  | Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston |  |
| 314 |  |  [Learning an Unreferenced Metric for Online Dialogue Evaluation](https://doi.org/10.18653/v1/2020.acl-main.220) |  | 0 |  | Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L. Hamilton, Joelle Pineau |  |
| 315 |  |  [Neural Generation of Dialogue Response Timings](https://doi.org/10.18653/v1/2020.acl-main.221) |  | 0 |  | Matthew Roddy, Naomi Harte |  |
| 316 |  |  [The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents](https://doi.org/10.18653/v1/2020.acl-main.222) |  | 0 |  | Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, YLan Boureau, Jason Weston |  |
| 317 |  |  [Automatic Poetry Generation from Prosaic Text](https://doi.org/10.18653/v1/2020.acl-main.223) |  | 0 |  | Tim Van de Cruys |  |
| 318 |  |  [Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.224) |  | 0 |  | Chao Zhao, Marilyn A. Walker, Snigdha Chaturvedi |  |
| 319 |  |  [Enabling Language Models to Fill in the Blanks](https://doi.org/10.18653/v1/2020.acl-main.225) |  | 0 |  | Chris Donahue, Mina Lee, Percy Liang |  |
| 320 |  |  [INSET: Sentence Infilling with INter-SEntential Transformer](https://doi.org/10.18653/v1/2020.acl-main.226) |  | 0 |  | Yichen Huang, Yizhe Zhang, Oussama Elachqar, Yu Cheng |  |
| 321 |  |  [Improving Adversarial Text Generation by Modeling the Distant Future](https://doi.org/10.18653/v1/2020.acl-main.227) |  | 0 |  | Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence Carin |  |
| 322 |  |  [Simple and Effective Retrieve-Edit-Rerank Text Generation](https://doi.org/10.18653/v1/2020.acl-main.228) |  | 0 |  | Nabil Hossain, Marjan Ghazvininejad, Luke Zettlemoyer |  |
| 323 |  |  [BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps](https://doi.org/10.18653/v1/2020.acl-main.229) |  | 0 |  | Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha |  |
| 324 |  |  [Cross-media Structured Common Space for Multimedia Event Extraction](https://doi.org/10.18653/v1/2020.acl-main.230) |  | 0 |  | Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, ShihFu Chang |  |
| 325 |  |  [Learning to Segment Actions from Observation and Narration](https://doi.org/10.18653/v1/2020.acl-main.231) |  | 0 |  | Daniel Fried, JeanBaptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, Aida Nematzadeh |  |
| 326 |  |  [Learning to execute instructions in a Minecraft dialogue](https://doi.org/10.18653/v1/2020.acl-main.232) |  | 0 |  | Prashant Jayannavar, Anjali NarayanChen, Julia Hockenmaier |  |
| 327 |  |  [MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning](https://doi.org/10.18653/v1/2020.acl-main.233) |  | 0 |  | Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, Mohit Bansal |  |
| 328 |  |  [What is Learned in Visually Grounded Neural Syntax Acquisition](https://doi.org/10.18653/v1/2020.acl-main.234) |  | 0 |  | Noriyuki Kojima, Hadar AverbuchElor, Alexander M. Rush, Yoav Artzi |  |
| 329 |  |  [A Batch Normalized Inference Network Keeps the KL Vanishing Away](https://doi.org/10.18653/v1/2020.acl-main.235) |  | 0 |  | Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li, Dapeng Wu |  |
| 330 |  |  [Contextual Embeddings: When Are They Worth It?](https://doi.org/10.18653/v1/2020.acl-main.236) |  | 0 |  | Simran Arora, Avner May, Jian Zhang, Christopher Ré |  |
| 331 |  |  [Interactive Classification by Asking Informative Questions](https://doi.org/10.18653/v1/2020.acl-main.237) |  | 0 |  | Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, Yoav Artzi |  |
| 332 |  |  [Knowledge Graph Embedding Compression](https://doi.org/10.18653/v1/2020.acl-main.238) |  | 0 |  | Mrinmaya Sachan |  |
| 333 |  |  [Low Resource Sequence Tagging using Sentence Reconstruction](https://doi.org/10.18653/v1/2020.acl-main.239) |  | 0 |  | Tal Perl, Sriram Chaudhury, Raja Giryes |  |
| 334 |  |  [Masked Language Model Scoring](https://doi.org/10.18653/v1/2020.acl-main.240) |  | 0 |  | Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff |  |
| 335 |  |  [Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2020.acl-main.241) |  | 0 |  | Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, Bowen Zhou |  |
| 336 |  |  [Posterior Calibrated Training on Sentence Classification Tasks](https://doi.org/10.18653/v1/2020.acl-main.242) |  | 0 |  | Taehee Jung, Dongyeop Kang, Hua Cheng, Lucas Mentch, Thomas Schaaf |  |
| 337 |  |  [Posterior Control of Blackbox Generation](https://doi.org/10.18653/v1/2020.acl-main.243) |  | 0 |  | Xiang Lisa Li, Alexander M. Rush |  |
| 338 |  |  [Pretrained Transformers Improve Out-of-Distribution Robustness](https://doi.org/10.18653/v1/2020.acl-main.244) |  | 0 |  | Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song |  |
| 339 |  |  [Robust Encodings: A Framework for Combating Adversarial Typos](https://doi.org/10.18653/v1/2020.acl-main.245) |  | 0 |  | Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang |  |
| 340 |  |  [Showing Your Work Doesn't Always Work](https://doi.org/10.18653/v1/2020.acl-main.246) |  | 0 |  | Raphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yaoliang Yu, Jimmy Lin |  |
| 341 |  |  [Span Selection Pre-training for Question Answering](https://doi.org/10.18653/v1/2020.acl-main.247) |  | 0 |  | Michael R. Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G. P. Shrivatsa Bhargav, Dinesh Garg, Avirup Sil |  |
| 342 |  |  [Topological Sort for Sentence Ordering](https://doi.org/10.18653/v1/2020.acl-main.248) |  | 0 |  | Shrimai Prabhumoye, Ruslan Salakhutdinov, Alan W. Black |  |
| 343 |  |  [Weight Poisoning Attacks on Pretrained Models](https://doi.org/10.18653/v1/2020.acl-main.249) |  | 0 |  | Keita Kurita, Paul Michel, Graham Neubig |  |
| 344 |  |  [schuBERT: Optimizing Elements of BERT](https://doi.org/10.18653/v1/2020.acl-main.250) |  | 0 |  | Ashish Khetan, Zohar S. Karnin |  |
| 345 |  |  [ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.251) |  | 0 |  | Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel |  |
| 346 |  |  [Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.252) |  | 0 |  | Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Xu Chen, Sneha Reddy Kudugunta, Naveen Arivazhagan, Yonghui Wu |  |
| 347 |  |  [On The Evaluation of Machine Translation SystemsTrained With Back-Translation](https://doi.org/10.18653/v1/2020.acl-main.253) |  | 0 |  | Sergey Edunov, Myle Ott, Marc'Aurelio Ranzato, Michael Auli |  |
| 348 |  |  [Simultaneous Translation Policies: From Fixed to Adaptive](https://doi.org/10.18653/v1/2020.acl-main.254) |  | 0 |  | Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, Liang Huang |  |
| 349 |  |  [Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information](https://doi.org/10.18653/v1/2020.acl-main.255) |  | 0 |  | Michele Bevilacqua, Roberto Navigli |  |
| 350 |  |  [Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs](https://doi.org/10.18653/v1/2020.acl-main.256) |  | 0 |  | HongYou Chen, SzHan Yu, Shoude Lin |  |
| 351 |  |  [Multidirectional Associative Optimization of Function-Specific Word Representations](https://doi.org/10.18653/v1/2020.acl-main.257) |  | 0 |  | Daniela Gerz, Ivan Vulic, Marek Rei, Roi Reichart, Anna Korhonen |  |
| 352 |  |  [Predicting Degrees of Technicality in Automatic Terminology Extraction](https://doi.org/10.18653/v1/2020.acl-main.258) |  | 0 |  | Anna Hätty, Dominik Schlechtweg, Michael Dorna, Sabine Schulte im Walde |  |
| 353 |  |  [Verbal Multiword Expressions for Identification of Metaphor](https://doi.org/10.18653/v1/2020.acl-main.259) |  | 0 |  | Omid Rohanian, Marek Rei, Shiva Taslimipoor, Le An Ha |  |
| 354 |  |  [Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer](https://doi.org/10.18653/v1/2020.acl-main.260) |  | 0 |  | Jieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini, KaiWei Chang, Ahmed Hassan Awadallah |  |
| 355 |  |  [Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on What Basis?](https://doi.org/10.18653/v1/2020.acl-main.261) |  | 0 |  | Kobi Leins, Jey Han Lau, Timothy Baldwin |  |
| 356 |  |  [Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds](https://doi.org/10.18653/v1/2020.acl-main.262) |  | 0 |  | Kawin Ethayarajh |  |
| 357 |  |  [It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations](https://doi.org/10.18653/v1/2020.acl-main.263) |  | 0 |  | Samson Tan, Shafiq R. Joty, MinYen Kan, Richard Socher |  |
| 358 |  |  [Mitigating Gender Bias Amplification in Distribution by Posterior Regularization](https://doi.org/10.18653/v1/2020.acl-main.264) |  | 0 |  | Shengyu Jia, Tao Meng, Jieyu Zhao, KaiWei Chang |  |
| 359 |  |  [Towards Understanding Gender Bias in Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.265) |  | 0 |  | Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth M. Belding, KaiWei Chang, William Yang Wang |  |
| 360 |  |  [A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing](https://doi.org/10.18653/v1/2020.acl-main.266) |  | 0 |  | Kartik Goyal, Chris Dyer, Christopher N. Warren, Max G'Sell, Taylor BergKirkpatrick |  |
| 361 |  |  [Attentive Pooling with Learnable Norms for Text Representation](https://doi.org/10.18653/v1/2020.acl-main.267) |  | 0 |  | Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, Yongfeng Huang |  |
| 362 |  |  [Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks](https://doi.org/10.18653/v1/2020.acl-main.268) |  | 0 |  | Fynn Schröder, Chris Biemann |  |
| 363 |  |  [How Does Selective Mechanism Improve Self-Attention Networks?](https://doi.org/10.18653/v1/2020.acl-main.269) |  | 0 |  | Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu |  |
| 364 |  |  [Improving Transformer Models by Reordering their Sublayers](https://doi.org/10.18653/v1/2020.acl-main.270) |  | 0 |  | Ofir Press, Noah A. Smith, Omer Levy |  |
| 365 |  |  [Single Model Ensemble using Pseudo-Tags and Distinct Vectors](https://doi.org/10.18653/v1/2020.acl-main.271) |  | 0 |  | Ryosuke Kuwabara, Jun Suzuki, Hideki Nakayama |  |
| 366 |  |  [Zero-shot Text Classification via Reinforced Self-training](https://doi.org/10.18653/v1/2020.acl-main.272) |  | 0 |  | Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, Suhang Zheng, Feng Wang, Jun Zhang, Huajun Chen |  |
| 367 |  |  [A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.273) |  | 0 |  | Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo |  |
| 368 |  |  [A Relaxed Matching Procedure for Unsupervised BLI](https://doi.org/10.18653/v1/2020.acl-main.274) |  | 0 |  | Xu Zhao, Zihao Wang, Yong Zhang, Hao Wu |  |
| 369 |  |  [Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.275) |  | 0 |  | Xuanli He, Gholamreza Haffari, Mohammad Norouzi |  |
| 370 |  |  [Geometry-aware domain adaptation for unsupervised alignment of word embeddings](https://doi.org/10.18653/v1/2020.acl-main.276) |  | 0 |  | Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra |  |
| 371 |  |  [Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.277) |  | 0 |  | Qiu Ran, Yankai Lin, Peng Li, Jie Zhou |  |
| 372 |  |  [On the Inference Calibration of Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.278) |  | 0 |  | Shuo Wang, Zhaopeng Tu, Shuming Shi, Yang Liu |  |
| 373 |  |  [Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning](https://doi.org/10.18653/v1/2020.acl-main.279) |  | 0 |  | Zhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang, Changlong Sun, Qiong Zhang, Xiaozhong Liu |  |
| 374 |  |  [Distinguish Confusing Law Articles for Legal Judgment Prediction](https://doi.org/10.18653/v1/2020.acl-main.280) |  | 0 |  | Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, Junzhou Zhao |  |
| 375 |  |  [Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation](https://doi.org/10.18653/v1/2020.acl-main.281) |  | 0 |  | Liting Liu, Jie Liu, Wenzheng Zhang, Ziming Chi, Wenxuan Shi, Yalou Huang |  |
| 376 |  |  [HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding](https://doi.org/10.18653/v1/2020.acl-main.282) |  | 0 |  | Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu, Weifeng Chong |  |
| 377 |  |  [Hyperbolic Capsule Networks for Multi-Label Classification](https://doi.org/10.18653/v1/2020.acl-main.283) |  | 0 |  | Boli Chen, Xin Huang, Lin Xiao, Liping Jing |  |
| 378 |  |  [Improving Segmentation for Technical Support Problems](https://doi.org/10.18653/v1/2020.acl-main.284) |  | 0 |  | Kushal Chauhan, Abhirut Gupta |  |
| 379 |  |  [MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs](https://doi.org/10.18653/v1/2020.acl-main.285) |  | 0 |  | Jifan Yu, Gan Luo, Tong Xiao, Qingyang Zhong, Yuquan Wang, Wenzheng Feng, Junyi Luo, Chenyu Wang, Lei Hou, Juanzi Li, Zhiyuan Liu, Jie Tang |  |
| 380 |  |  [Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs](https://doi.org/10.18653/v1/2020.acl-main.286) |  | 0 |  | Jun Chen, Xiaoya Dai, Quan Yuan, Chao Lu, Haifeng Huang |  |
| 381 |  |  [Analyzing the Persuasive Effect of Style in News Editorial Argumentation](https://doi.org/10.18653/v1/2020.acl-main.287) |  | 0 |  | Roxanne El Baff, Henning Wachsmuth, Khalid Al Khatib, Benno Stein |  |
| 382 |  |  [ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and Prediction](https://doi.org/10.18653/v1/2020.acl-main.288) |  | 0 |  | Zixiang Ding, Rui Xia, Jianfei Yu |  |
| 383 |  |  [Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.289) |  | 0 |  | Penghui Wei, Jiahao Zhao, Wenji Mao |  |
| 384 |  |  [Embarrassingly Simple Unsupervised Aspect Extraction](https://doi.org/10.18653/v1/2020.acl-main.290) |  | 0 |  | Stéphan Tulkens, Andreas van Cranenburgh |  |
| 385 |  |  [Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge](https://doi.org/10.18653/v1/2020.acl-main.291) |  | 0 |  | Bowen Zhang, Min Yang, Xutao Li, Yunming Ye, Xiaofei Xu, Kuai Dai |  |
| 386 |  |  [KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.292) |  | 0 |  | Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 387 |  |  [Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.293) |  | 0 |  | MinhHieu Phan, Philip O. Ogunbona |  |
| 388 |  |  [Parallel Data Augmentation for Formality Style Transfer](https://doi.org/10.18653/v1/2020.acl-main.294) |  | 0 |  | Yi Zhang, Tao Ge, Xu Sun |  |
| 389 |  |  [Relational Graph Attention Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.295) |  | 0 |  | Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang |  |
| 390 |  |  [SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction](https://doi.org/10.18653/v1/2020.acl-main.296) |  | 0 |  | He Zhao, Longtao Huang, Rong Zhang, Quan Lu, Hui Xue |  |
| 391 |  |  [Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks](https://doi.org/10.18653/v1/2020.acl-main.297) |  | 0 |  | Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, Min Zhang |  |
| 392 |  |  [Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific Parameterization](https://doi.org/10.18653/v1/2020.acl-main.298) |  | 0 |  | Gaku Morio, Hiroaki Ozaki, Terufumi Morishita, Yuta Koreeda, Kohsuke Yanai |  |
| 393 |  |  [A Span-based Linearization for Constituent Trees](https://doi.org/10.18653/v1/2020.acl-main.299) |  | 0 |  | Yang Wei, Yuanbin Wu, Man Lan |  |
| 394 |  |  [An Empirical Comparison of Unsupervised Constituency Parsing Methods](https://doi.org/10.18653/v1/2020.acl-main.300) |  | 0 |  | Jun Li, Yifan Cao, Jiong Cai, Yong Jiang, Kewei Tu |  |
| 395 |  |  [Efficient Constituency Parsing by Pointing](https://doi.org/10.18653/v1/2020.acl-main.301) |  | 0 |  | ThanhTung Nguyen, XuanPhi Nguyen, Shafiq R. Joty, Xiaoli Li |  |
| 396 |  |  [Efficient Second-Order TreeCRF for Neural Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.302) |  | 0 |  | Yu Zhang, Zhenghua Li, Min Zhang |  |
| 397 |  |  [Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive LSTMs](https://doi.org/10.18653/v1/2020.acl-main.303) |  | 0 |  | Michael A. Lepori, Tal Linzen, R. Thomas McCoy |  |
| 398 |  |  [Structure-Level Knowledge Distillation For Multilingual Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.304) |  | 0 |  | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, Kewei Tu |  |
| 399 |  |  [Dynamic Online Conversation Recommendation](https://doi.org/10.18653/v1/2020.acl-main.305) |  | 0 |  | Xingshan Zeng, Jing Li, Lu Wang, Zhiming Mao, KamFai Wong |  |
| 400 |  |  [Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer](https://doi.org/10.18653/v1/2020.acl-main.306) |  | 0 |  | Jianfei Yu, Jing Jiang, Li Yang, Rui Xia |  |
| 401 |  |  [Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio Optimization](https://doi.org/10.18653/v1/2020.acl-main.307) |  | 0 |  | Xin Du, Kumiko TanakaIshii |  |
| 402 |  |  [What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context](https://doi.org/10.18653/v1/2020.acl-main.308) |  | 0 |  | Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James R. Glass, Preslav Nakov |  |
| 403 |  |  [An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.309) |  | 0 |  | Hiroshi Noji, Hiroya Takamura |  |
| 404 |  |  [On the Robustness of Language Encoders against Grammatical Errors](https://doi.org/10.18653/v1/2020.acl-main.310) |  | 0 |  | Fan Yin, Quanyu Long, Tao Meng, KaiWei Chang |  |
| 405 |  |  [Roles and Utilization of Attention Heads in Transformer-based Neural Language Models](https://doi.org/10.18653/v1/2020.acl-main.311) |  | 0 |  | Jaeyoung Jo, SungHyon Myaeng |  |
| 406 |  |  [Understanding Attention for Text Classification](https://doi.org/10.18653/v1/2020.acl-main.312) |  | 0 |  | Xiaobing Sun, Wei Lu |  |
| 407 |  |  [A Relational Memory-based Embedding Model for Triple Classification and Search Personalization](https://doi.org/10.18653/v1/2020.acl-main.313) |  | 0 |  | Dai Quoc Nguyen, Tuan Nguyen, Dinh Phung |  |
| 408 |  |  [Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods](https://doi.org/10.18653/v1/2020.acl-main.314) |  | 0 |  | Ning Miao, Yuxuan Song, Hao Zhou, Lei Li |  |
| 409 |  |  [Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention](https://doi.org/10.18653/v1/2020.acl-main.315) |  | 0 |  | Yanzeng Li, Bowen Yu, Mengge Xue, Tingwen Liu |  |
| 410 |  |  [On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond](https://doi.org/10.18653/v1/2020.acl-main.316) |  | 0 |  | Chen Wu, Prince Zizhuang Wang, William Yang Wang |  |
| 411 |  |  [SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions](https://doi.org/10.18653/v1/2020.acl-main.317) |  | 0 |  | Mao Ye, Chengyue Gong, Qiang Liu |  |
| 412 |  |  [A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2020.acl-main.318) |  | 0 |  | Shuo Ren, Shujie Liu, Ming Zhou, Shuai Ma |  |
| 413 |  |  [A Reinforced Generation of Adversarial Examples for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.319) |  | 0 |  | Wei Zou, Shujian Huang, Jun Xie, Xinyu Dai, Jiajun Chen |  |
| 414 |  |  [A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.320) |  | 0 |  | Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma |  |
| 415 |  |  [A Simple and Effective Unified Encoder for Document-Level Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.321) |  | 0 |  | Shuming Ma, Dongdong Zhang, Ming Zhou |  |
| 416 |  |  [Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.322) |  | 0 |  | Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li |  |
| 417 |  |  [Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change](https://doi.org/10.18653/v1/2020.acl-main.323) |  | 0 |  | Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu |  |
| 418 |  |  [Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.324) |  | 0 |  | Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao |  |
| 419 |  |  [Lexically Constrained Neural Machine Translation with Levenshtein Transformer](https://doi.org/10.18653/v1/2020.acl-main.325) |  | 0 |  | Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan |  |
| 420 |  |  [On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.326) |  | 0 |  | Chaojun Wang, Rico Sennrich |  |
| 421 |  |  [Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model](https://doi.org/10.18653/v1/2020.acl-main.327) |  | 0 |  | Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura |  |
| 422 |  |  [ChartDialogs: Plotting from Natural Language Instructions](https://doi.org/10.18653/v1/2020.acl-main.328) |  | 0 |  | Yutong Shao, Ndapa Nakashole |  |
| 423 |  |  [GLUECoS: An Evaluation Benchmark for Code-Switched NLP](https://doi.org/10.18653/v1/2020.acl-main.329) |  | 0 |  | Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, Monojit Choudhury |  |
| 424 |  |  [MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization](https://doi.org/10.18653/v1/2020.acl-main.330) |  | 0 |  | Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, Chenliang Li |  |
| 425 |  |  [MIND: A Large-scale Dataset for News Recommendation](https://doi.org/10.18653/v1/2020.acl-main.331) |  | 0 |  | Fangzhao Wu, Ying Qiao, JiunHung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, Ming Zhou |  |
| 426 |  |  [That is a Known Lie: Detecting Previously Fact-Checked Claims](https://doi.org/10.18653/v1/2020.acl-main.332) |  | 0 |  | Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino, Preslav Nakov |  |
| 427 |  |  [Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation](https://doi.org/10.18653/v1/2020.acl-main.333) |  | 0 |  | Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, Kewei Tu |  |
| 428 |  |  [BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection](https://doi.org/10.18653/v1/2020.acl-main.334) |  | 0 |  | Chengyu Wang, Xiaofeng He |  |
| 429 |  |  [Biomedical Entity Representations with Synonym Marginalization](https://doi.org/10.18653/v1/2020.acl-main.335) |  | 0 |  | Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, Jaewoo Kang |  |
| 430 |  |  [Hypernymy Detection for Low-Resource Languages via Meta Learning](https://doi.org/10.18653/v1/2020.acl-main.336) |  | 0 |  | Changlong Yu, Jialong Han, Haisong Zhang, Wilfred Ng |  |
| 431 |  |  [Investigating Word-Class Distributions in Word Vector Spaces](https://doi.org/10.18653/v1/2020.acl-main.337) |  | 0 |  | Ryohei Sasano, Anna Korhonen |  |
| 432 |  |  [Aspect Sentiment Classification with Document-level Sentiment Preference Modeling](https://doi.org/10.18653/v1/2020.acl-main.338) |  | 0 |  | Xiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang, Guodong Zhou |  |
| 433 |  |  [Don't Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for Aspect Extraction](https://doi.org/10.18653/v1/2020.acl-main.339) |  | 0 |  | Zhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng, Jianmin Yao |  |
| 434 |  |  [Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.340) |  | 0 |  | Zhuang Chen, Tieyun Qian |  |
| 435 |  |  [SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics](https://doi.org/10.18653/v1/2020.acl-main.341) |  | 0 |  | Da Yin, Tao Meng, KaiWei Chang |  |
| 436 |  |  [Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.342) |  | 0 |  | Chuang Fan, Chaofa Yuan, Jiachen Du, Lin Gui, Min Yang, Ruifeng Xu |  |
| 437 |  |  [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality](https://doi.org/10.18653/v1/2020.acl-main.343) |  | 0 |  | Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, Kaicheng Yang |  |
| 438 |  |  [Curriculum Pre-training for End-to-End Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.344) |  | 0 |  | Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang |  |
| 439 |  |  [How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems](https://doi.org/10.18653/v1/2020.acl-main.345) |  | 0 |  | Archiki Prasad, Preethi Jyothi |  |
| 440 |  |  [Improving Disfluency Detection by Self-Training a Self-Attentive Model](https://doi.org/10.18653/v1/2020.acl-main.346) |  | 0 |  | Paria Jamshid Lou, Mark Johnson |  |
| 441 |  |  [Learning Spoken Language Representations with Neural Lattice Language Modeling](https://doi.org/10.18653/v1/2020.acl-main.347) |  | 0 |  | ChaoWei Huang, YunNung Chen |  |
| 442 |  |  [Meta-Transfer Learning for Code-Switched Speech Recognition](https://doi.org/10.18653/v1/2020.acl-main.348) |  | 0 |  | Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, Pascale Fung |  |
| 443 |  |  [Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association](https://doi.org/10.18653/v1/2020.acl-main.349) |  | 0 |  | Nan Xu, Zhixiong Zeng, Wenji Mao |  |
| 444 |  |  [SimulSpeech: End-to-End Simultaneous Speech to Text Translation](https://doi.org/10.18653/v1/2020.acl-main.350) |  | 0 |  | Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, TieYan Liu |  |
| 445 |  |  [Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy conversations](https://doi.org/10.18653/v1/2020.acl-main.351) |  | 0 |  | Karan Singla, Zhuohao Chen, David C. Atkins, Shrikanth Narayanan |  |
| 446 |  |  [Neural Temporal Opinion Modelling for Opinion Prediction on Twitter](https://doi.org/10.18653/v1/2020.acl-main.352) |  | 0 |  | Lixing Zhu, Yulan He, Deyu Zhou |  |
| 447 |  |  [It Takes Two to Lie: One to Lie, and One to Listen](https://doi.org/10.18653/v1/2020.acl-main.353) |  | 0 |  | Denis Peskov, Benny Cheng, Ahmed Elgohary, Joe Barrow, Cristian DanescuNiculescuMizil, Jordan L. BoydGraber |  |
| 448 |  |  [Learning Implicit Text Generation via Feature Matching](https://doi.org/10.18653/v1/2020.acl-main.354) |  | 0 |  | Inkit Padhi, Pierre L. Dognin, Ke Bai, Cícero Nogueira dos Santos, Vijil Chenthamarakshan, Youssef Mroueh, Payel Das |  |
| 449 |  |  [Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data](https://doi.org/10.18653/v1/2020.acl-main.355) |  | 0 |  | Hamidreza Shahidi, Ming Li, Jimmy Lin |  |
| 450 |  |  [Bayesian Hierarchical Words Representation Learning](https://doi.org/10.18653/v1/2020.acl-main.356) |  | 0 |  | Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein |  |
| 451 |  |  [Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning](https://doi.org/10.18653/v1/2020.acl-main.357) |  | 0 |  | Alexandre Tamborrino, Nicola Pellicanò, Baptiste Pannier, Pascal Voitot, Louise Naudin |  |
| 452 |  |  [SEEK: Segmented Embedding of Knowledge Graphs](https://doi.org/10.18653/v1/2020.acl-main.358) |  | 0 |  | Wentao Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin, TieYan Liu |  |
| 453 |  |  [Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.359) |  | 0 |  | Xabier Soto, Dimitar Sht. Shterionov, Alberto Poncelas, Andy Way |  |
| 454 |  |  [Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture](https://doi.org/10.18653/v1/2020.acl-main.360) |  | 0 |  | Christopher Brix, Parnia Bahar, Hermann Ney |  |
| 455 |  |  [A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction](https://doi.org/10.18653/v1/2020.acl-main.361) |  | 0 |  | Yilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, Jingfang Xu, Minlie Huang |  |
| 456 |  |  [Graph-to-Tree Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2020.acl-main.362) |  | 0 |  | Jipeng Zhang, Lei Wang, Roy KaWei Lee, Yi Bin, Yan Wang, Jie Shao, EePeng Lim |  |
| 457 |  |  [An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results](https://doi.org/10.18653/v1/2020.acl-main.363) |  | 0 |  | Enrique Amigó, Julio Gonzalo, Stefano Mizzaro, Jorge CarrillodeAlbornoz |  |
| 458 |  |  [Adaptive Compression of Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.364) |  | 0 |  | Yeachan Kim, KangMin Kim, SangKeun Lee |  |
| 459 |  |  [Analysing Lexical Semantic Change with Contextualised Word Representations](https://doi.org/10.18653/v1/2020.acl-main.365) |  | 0 |  | Mario Giulianelli, Marco Del Tredici, Raquel Fernández |  |
| 460 |  |  [Autoencoding Keyword Correlation Graph for Document Clustering](https://doi.org/10.18653/v1/2020.acl-main.366) |  | 0 |  | Billy Chiu, Sunil Kumar Sahu, Derek Thomas, Neha Sengupta, Mohammady Mahdy |  |
| 461 |  |  [Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional Semantics](https://doi.org/10.18653/v1/2020.acl-main.367) |  | 0 |  | Guy Emerson |  |
| 462 |  |  [BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance](https://doi.org/10.18653/v1/2020.acl-main.368) |  | 0 |  | Timo Schick, Hinrich Schütze |  |
| 463 |  |  [CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages](https://doi.org/10.18653/v1/2020.acl-main.369) |  | 0 |  | Tommaso Pasini, Federico Scozzafava, Bianca Scarlini |  |
| 464 |  |  [Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.370) |  | 0 |  | Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao |  |
| 465 |  |  [From Arguments to Key Points: Towards Automatic Argument Summarization](https://doi.org/10.18653/v1/2020.acl-main.371) |  | 0 |  | Roy BarHaim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav, Noam Slonim |  |
| 466 |  |  [GoEmotions: A Dataset of Fine-Grained Emotions](https://doi.org/10.18653/v1/2020.acl-main.372) |  | 0 |  | Dorottya Demszky, Dana MovshovitzAttias, Jeongwoo Ko, Alan S. Cowen, Gaurav Nemade, Sujith Ravi |  |
| 467 |  |  [He said "who's gonna take care of your children when you are at ACL?": Reported Sexist Acts are Not Sexist](https://doi.org/10.18653/v1/2020.acl-main.373) |  | 0 |  | Patricia Chiril, Véronique Moriceau, Farah Benamara, Alda Mari, Gloria Origgi, Marlène CoulombGully |  |
| 468 |  |  [SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis](https://doi.org/10.18653/v1/2020.acl-main.374) |  | 0 |  | Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, Feng Wu |  |
| 469 |  |  [Do Neural Language Models Show Preferences for Syntactic Formalisms?](https://doi.org/10.18653/v1/2020.acl-main.375) |  | 0 |  | Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou, Joakim Nivre |  |
| 470 |  |  [Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing](https://doi.org/10.18653/v1/2020.acl-main.376) |  | 0 |  | Daniel FernándezGonzález, Carlos GómezRodríguez |  |
| 471 |  |  [Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis](https://doi.org/10.18653/v1/2020.acl-main.377) |  | 0 |  | Yajie Ye, Weiwei Sun |  |
| 472 |  |  [Max-Margin Incremental CCG Parsing](https://doi.org/10.18653/v1/2020.acl-main.378) |  | 0 |  | Milos Stanojevic, Mark Steedman |  |
| 473 |  |  [Neural Reranking for Dependency Parsing: An Evaluation](https://doi.org/10.18653/v1/2020.acl-main.379) |  | 0 |  | BichNgoc Do, Ines Rehbein |  |
| 474 |  |  [Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting](https://doi.org/10.18653/v1/2020.acl-main.380) |  | 0 |  | Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu, Tiejun Zhao |  |
| 475 |  |  [Analyzing analytical methods: The case of phonology in neural models of spoken language](https://doi.org/10.18653/v1/2020.acl-main.381) |  | 0 |  | Grzegorz Chrupala, Bertrand Higy, Afra Alishahi |  |
| 476 |  |  [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.382) |  | 0 |  | OanaMaria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom |  |
| 477 |  |  [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://doi.org/10.18653/v1/2020.acl-main.383) |  | 0 |  | Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu |  |
| 478 |  |  [Probing for Referential Information in Language Models](https://doi.org/10.18653/v1/2020.acl-main.384) |  | 0 |  | IonutTeodor Sorodoc, Kristina Gulordava, Gemma Boleda |  |
| 479 |  |  [Quantifying Attention Flow in Transformers](https://doi.org/10.18653/v1/2020.acl-main.385) |  | 0 |  | Samira Abnar, Willem H. Zuidema |  |
| 480 |  |  [Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?](https://doi.org/10.18653/v1/2020.acl-main.386) |  | 0 |  | Alon Jacovi, Yoav Goldberg |  |
| 481 |  |  [Towards Transparent and Explainable Attention Models](https://doi.org/10.18653/v1/2020.acl-main.387) |  | 0 |  | Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran |  |
| 482 |  |  [Tchebycheff Procedure for Multi-task Text Classification](https://doi.org/10.18653/v1/2020.acl-main.388) |  | 0 |  | Yuren Mao, Shuang Yun, Weiwei Liu, Bo Du |  |
| 483 |  |  [Modeling Word Formation in English-German Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.389) |  | 0 |  | Marion WellerDi Marco, Alexander M. Fraser |  |
| 484 |  |  [Empowering Active Learning to Jointly Optimize System and User Demands](https://doi.org/10.18653/v1/2020.acl-main.390) |  | 0 |  | JiUng Lee, Christian M. Meyer, Iryna Gurevych |  |
| 485 |  |  [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](https://doi.org/10.18653/v1/2020.acl-main.391) |  | 0 |  | Masahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki, Kentaro Inui |  |
| 486 |  |  [Graph Neural News Recommendation with Unsupervised Preference Disentanglement](https://doi.org/10.18653/v1/2020.acl-main.392) |  | 0 |  | Linmei Hu, Siyong Xu, Chen Li, Cheng Yang, Chuan Shi, Nan Duan, Xing Xie, Ming Zhou |  |
| 487 |  |  [Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description](https://doi.org/10.18653/v1/2020.acl-main.393) |  | 0 |  | Yakun Hu, Zhunchen Luo, Wenhan Chao |  |
| 488 |  |  [Joint Modelling of Emotion and Abusive Language Detection](https://doi.org/10.18653/v1/2020.acl-main.394) |  | 0 |  | Santhosh Rajamanickam, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova |  |
| 489 |  |  [Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.395) |  | 0 |  | Sebastian Weigelt, Vanessa Steurer, Tobias Hey, Walter F. Tichy |  |
| 490 |  |  [Toxicity Detection: Does Context Really Matter?](https://doi.org/10.18653/v1/2020.acl-main.396) |  | 0 |  | John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, Ion Androutsopoulos |  |
| 491 |  |  [AMR Parsing with Latent Structural Information](https://doi.org/10.18653/v1/2020.acl-main.397) |  | 0 |  | Qiji Zhou, Yue Zhang, Donghong Ji, Hao Tang |  |
| 492 |  |  [TaPas: Weakly Supervised Table Parsing via Pre-training](https://doi.org/10.18653/v1/2020.acl-main.398) |  | 0 |  | Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, Julian Martin Eisenschlos |  |
| 493 |  |  [Target Inference in Argument Conclusion Generation](https://doi.org/10.18653/v1/2020.acl-main.399) |  | 0 |  | Milad Alshomary, Shahbaz Syed, Martin Potthast, Henning Wachsmuth |  |
| 494 |  |  [Multimodal Transformer for Multimodal Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.400) |  | 0 |  | Shaowei Yao, Xiaojun Wan |  |
| 495 |  |  [Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis](https://doi.org/10.18653/v1/2020.acl-main.401) |  | 0 |  | Dushyant Singh Chauhan, Dhanush S. R, Asif Ekbal, Pushpak Bhattacharyya |  |
| 496 |  |  [Towards Emotion-aided Multi-modal Dialogue Act Classification](https://doi.org/10.18653/v1/2020.acl-main.402) |  | 0 |  | Tulika Saha, Aditya Prakash Patra, Sriparna Saha, Pushpak Bhattacharyya |  |
| 497 |  |  [Analyzing Political Parody in Social Media](https://doi.org/10.18653/v1/2020.acl-main.403) |  | 0 |  | Antonis Maronikolakis, Danae Sanchez Villegas, Daniel PreotiucPietro, Nikolaos Aletras |  |
| 498 |  |  [Masking Actor Information Leads to Fairer Political Claims Detection](https://doi.org/10.18653/v1/2020.acl-main.404) |  | 0 |  | Erenay Dayanik, Sebastian Padó |  |
| 499 |  |  [When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?](https://doi.org/10.18653/v1/2020.acl-main.405) |  | 0 |  | Kenneth Joseph, Jonathan H. Morgan |  |
| 500 |  |  ["Who said it, and Why?" Provenance for Natural Language Claims](https://doi.org/10.18653/v1/2020.acl-main.406) |  | 0 |  | Yi Zhang, Zachary G. Ives, Dan Roth |  |
| 501 |  |  [Compositionality and Generalization In Emergent Languages](https://doi.org/10.18653/v1/2020.acl-main.407) |  | 0 |  | Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, Marco Baroni |  |
| 502 |  |  [ERASER: A Benchmark to Evaluate Rationalized NLP Models](https://doi.org/10.18653/v1/2020.acl-main.408) |  | 0 |  | Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C. Wallace |  |
| 503 |  |  [Learning to Faithfully Rationalize by Construction](https://doi.org/10.18653/v1/2020.acl-main.409) |  | 0 |  | Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace |  |
| 504 |  |  [Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset](https://doi.org/10.18653/v1/2020.acl-main.410) |  | 0 |  | Xiang Yue, Bernal Jimenez Gutierrez, Huan Sun |  |
| 505 |  |  [DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering](https://doi.org/10.18653/v1/2020.acl-main.411) |  | 0 |  | Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian |  |
| 506 |  |  [Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings](https://doi.org/10.18653/v1/2020.acl-main.412) |  | 0 |  | Apoorv Saxena, Aditay Tripathi, Partha P. Talukdar |  |
| 507 |  |  [Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering](https://doi.org/10.18653/v1/2020.acl-main.413) |  | 0 |  | Alexander R. Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang |  |
| 508 |  |  [Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering](https://doi.org/10.18653/v1/2020.acl-main.414) |  | 0 |  | Vikas Yadav, Steven Bethard, Mihai Surdeanu |  |
| 509 |  |  [A Corpus for Large-Scale Phonetic Typology](https://doi.org/10.18653/v1/2020.acl-main.415) |  | 0 |  | Elizabeth Salesky, Eleanor Chodroff, Tiago Pimentel, Matthew Wiesner, Ryan Cotterell, Alan W. Black, Jason Eisner |  |
| 510 |  |  [Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing](https://doi.org/10.18653/v1/2020.acl-main.416) |  | 0 |  | Jiangming Liu, Shay B. Cohen, Mirella Lapata |  |
| 511 |  |  [ParaCrawl: Web-Scale Acquisition of Parallel Corpora](https://doi.org/10.18653/v1/2020.acl-main.417) |  | 0 |  | Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel EsplàGomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio OrtizRojas, Leopoldo Pla Sempere, Gema RamírezSánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, Jaume Zaragoza |  |
| 512 |  |  [Toward Gender-Inclusive Coreference Resolution](https://doi.org/10.18653/v1/2020.acl-main.418) |  | 0 |  | Yang Trista Cao, Hal Daumé III |  |
| 513 |  |  [Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?](https://doi.org/10.18653/v1/2020.acl-main.419) |  | 0 |  | Cansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan Kong, Elke A. Rundensteiner |  |
| 514 |  |  [Information-Theoretic Probing for Linguistic Structure](https://doi.org/10.18653/v1/2020.acl-main.420) |  | 0 |  | Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell |  |
| 515 |  |  [On the Cross-lingual Transferability of Monolingual Representations](https://doi.org/10.18653/v1/2020.acl-main.421) |  | 0 |  | Mikel Artetxe, Sebastian Ruder, Dani Yogatama |  |
| 516 |  |  [Similarity Analysis of Contextual Word Representation Models](https://doi.org/10.18653/v1/2020.acl-main.422) |  | 0 |  | John M. Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James R. Glass |  |
| 517 |  |  [SenseBERT: Driving Some Sense into BERT](https://doi.org/10.18653/v1/2020.acl-main.423) |  | 0 |  | Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai ShalevShwartz, Amnon Shashua, Yoav Shoham |  |
| 518 |  |  [ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations](https://doi.org/10.18653/v1/2020.acl-main.424) |  | 0 |  | Fernando AlvaManchego, Louis Martin, Antoine Bordes, Carolina Scarton, Benoît Sagot, Lucia Specia |  |
| 519 |  |  [Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts](https://doi.org/10.18653/v1/2020.acl-main.425) |  | 0 |  | Agostina Calabrese, Michele Bevilacqua, Roberto Navigli |  |
| 520 |  |  [Modeling Label Semantics for Predicting Emotional Reactions](https://doi.org/10.18653/v1/2020.acl-main.426) |  | 0 |  | Radhika Gaonkar, Heeyoung Kwon, Mohaddeseh Bastan, Niranjan Balasubramanian, Nathanael Chambers |  |
| 521 |  |  [CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant](https://doi.org/10.18653/v1/2020.acl-main.427) |  | 0 |  | Kavya Srinet, Yacine Jernite, Jonathan Gray, Arthur Szlam |  |
| 522 |  |  [Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training](https://doi.org/10.18653/v1/2020.acl-main.428) |  | 0 |  | Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, YLan Boureau, Kyunghyun Cho, Jason Weston |  |
| 523 |  |  [How does BERT's attention change when you fine-tune? An analysis methodology and a case study in negation scope](https://doi.org/10.18653/v1/2020.acl-main.429) |  | 0 |  | Yiyun Zhao, Steven Bethard |  |
| 524 |  |  [Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models](https://doi.org/10.18653/v1/2020.acl-main.430) |  | 0 |  | Kaiji Lu, Piotr Mardziel, Klas Leino, Matt Fredrikson, Anupam Datta |  |
| 525 |  |  [Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings](https://doi.org/10.18653/v1/2020.acl-main.431) |  | 0 |  | Rishi Bommasani, Kelly Davis, Claire Cardie |  |
| 526 |  |  [Learning to Deceive with Attention-Based Explanations](https://doi.org/10.18653/v1/2020.acl-main.432) |  | 0 |  | Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton |  |
| 527 |  |  [On the Spontaneous Emergence of Discrete and Compositional Signals](https://doi.org/10.18653/v1/2020.acl-main.433) |  | 0 |  | Nur Geffen Lan, Emmanuel Chemla, Shane SteinertThrelkeld |  |
| 528 |  |  [Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words](https://doi.org/10.18653/v1/2020.acl-main.434) |  | 0 |  | Josef Klafka, Allyson Ettinger |  |
| 529 |  |  [Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA](https://doi.org/10.18653/v1/2020.acl-main.435) |  | 0 |  | Hyounghun Kim, Zineng Tang, Mohit Bansal |  |
| 530 |  |  [Shaping Visual Representations with Language for Few-Shot Classification](https://doi.org/10.18653/v1/2020.acl-main.436) |  | 0 |  | Jesse Mu, Percy Liang, Noah D. Goodman |  |
| 531 |  |  [Discrete Latent Variable Representations for Low-Resource Text Classification](https://doi.org/10.18653/v1/2020.acl-main.437) |  | 0 |  | Shuning Jin, Sam Wiseman, Karl Stratos, Karen Livescu |  |
| 532 |  |  [Learning Constraints for Structured Prediction Using Rectifier Networks](https://doi.org/10.18653/v1/2020.acl-main.438) |  | 0 |  | Xingyuan Pan, Maitrey Mehta, Vivek Srikumar |  |
| 533 |  |  [Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models](https://doi.org/10.18653/v1/2020.acl-main.439) |  | 0 |  | Dan Iter, Kelvin Guu, Larry Lansing, Dan Jurafsky |  |
| 534 |  |  [A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks](https://doi.org/10.18653/v1/2020.acl-main.440) |  | 0 |  | Angela S. Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris Brockett, Debadeepta Dey, Bill Dolan |  |
| 535 |  |  [Adversarial NLI: A New Benchmark for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.441) |  | 0 |  | Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela |  |
| 536 |  |  [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://doi.org/10.18653/v1/2020.acl-main.442) |  | 0 |  | Marco Túlio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh |  |
| 537 |  |  [Code and Named Entity Recognition in StackOverflow](https://doi.org/10.18653/v1/2020.acl-main.443) |  | 0 |  | Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter |  |
| 538 |  |  [Dialogue-Based Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.444) |  | 0 |  | Dian Yu, Kai Sun, Claire Cardie, Dong Yu |  |
| 539 |  |  [Facet-Aware Evaluation for Extractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.445) |  | 0 |  | Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, Jiawei Han |  |
| 540 |  |  [More Diverse Dialogue Datasets via Diversity-Informed Data Collection](https://doi.org/10.18653/v1/2020.acl-main.446) |  | 0 |  | Katherine Stasaski, Grace Hui Yang, Marti A. Hearst |  |
| 541 |  |  [S2ORC: The Semantic Scholar Open Research Corpus](https://doi.org/10.18653/v1/2020.acl-main.447) |  | 0 |  | Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel S. Weld |  |
| 542 |  |  [Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics](https://doi.org/10.18653/v1/2020.acl-main.448) |  | 0 |  | Nitika Mathur, Timothy Baldwin, Trevor Cohn |  |
| 543 |  |  [A Transformer-based Approach for Source Code Summarization](https://doi.org/10.18653/v1/2020.acl-main.449) |  | 0 |  | Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, KaiWei Chang |  |
| 544 |  |  [Asking and Answering Questions to Evaluate the Factual Consistency of Summaries](https://doi.org/10.18653/v1/2020.acl-main.450) |  | 0 |  | Alex Wang, Kyunghyun Cho, Mike Lewis |  |
| 545 |  |  [Discourse-Aware Neural Extractive Text Summarization](https://doi.org/10.18653/v1/2020.acl-main.451) |  | 0 |  | Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu |  |
| 546 |  |  [Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction](https://doi.org/10.18653/v1/2020.acl-main.452) |  | 0 |  | Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert |  |
| 547 |  |  [Exploring Content Selection in Summarization of Novel Chapters](https://doi.org/10.18653/v1/2020.acl-main.453) |  | 0 |  | Faisal Ladhak, Bryan Li, Yaser AlOnaizan, Kathleen R. McKeown |  |
| 548 |  |  [FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.454) |  | 0 |  | Esin Durmus, He He, Mona T. Diab |  |
| 549 |  |  [Fact-based Content Weighting for Evaluating Abstractive Summarisation](https://doi.org/10.18653/v1/2020.acl-main.455) |  | 0 |  | Xinnuo Xu, Ondrej Dusek, Jingyi Li, Verena Rieser, Ioannis Konstas |  |
| 550 |  |  [Hooks in the Headline: Learning to Generate Headlines with Controlled Styles](https://doi.org/10.18653/v1/2020.acl-main.456) |  | 0 |  | Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits |  |
| 551 |  |  [Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward](https://doi.org/10.18653/v1/2020.acl-main.457) |  | 0 |  | Luyang Huang, Lingfei Wu, Lu Wang |  |
| 552 |  |  [Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports](https://doi.org/10.18653/v1/2020.acl-main.458) |  | 0 |  | Yuhao Zhang, Derek Merck, Emily Bao Tsai, Christopher D. Manning, Curtis P. Langlotz |  |
| 553 |  |  [Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset](https://doi.org/10.18653/v1/2020.acl-main.459) |  | 0 |  | Revanth Rameshkumar, Peter Bailey |  |
| 554 |  |  [The Summary Loop: Learning to Write Abstractive Summaries Without Examples](https://doi.org/10.18653/v1/2020.acl-main.460) |  | 0 |  | Philippe Laban, Andrew Hsi, John F. Canny, Marti A. Hearst |  |
| 555 |  |  [Unsupervised Opinion Summarization as Copycat-Review Generation](https://doi.org/10.18653/v1/2020.acl-main.461) |  | 0 |  | Arthur Brazinskas, Mirella Lapata, Ivan Titov |  |
| 556 |  |  [(Re)construing Meaning in NLP](https://doi.org/10.18653/v1/2020.acl-main.462) |  | 0 |  | Sean Trott, Tiago Timponi Torrent, Nancy Chang, Nathan Schneider |  |
| 557 |  |  [Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data](https://doi.org/10.18653/v1/2020.acl-main.463) |  | 0 |  | Emily M. Bender, Alexander Koller |  |
| 558 |  |  [Examining Citations of Natural Language Processing Literature](https://doi.org/10.18653/v1/2020.acl-main.464) |  | 0 |  | Saif M. Mohammad |  |
| 559 |  |  [How Can We Accelerate Progress Towards Human-like Linguistic Generalization?](https://doi.org/10.18653/v1/2020.acl-main.465) |  | 0 |  | Tal Linzen |  |
| 560 |  |  [How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence](https://doi.org/10.18653/v1/2020.acl-main.466) |  | 0 |  | Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, Maosong Sun |  |
| 561 |  |  [Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://doi.org/10.18653/v1/2020.acl-main.467) |  | 0 |  | Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, Samuel R. Bowman |  |
| 562 |  |  [Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview](https://doi.org/10.18653/v1/2020.acl-main.468) |  | 0 |  | Deven Shah, H. Andrew Schwartz, Dirk Hovy |  |
| 563 |  |  [What Does BERT with Vision Look At?](https://doi.org/10.18653/v1/2020.acl-main.469) |  | 0 |  | Liunian Harold Li, Mark Yatskar, Da Yin, ChoJui Hsieh, KaiWei Chang |  |
| 564 |  |  [Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards](https://doi.org/10.18653/v1/2020.acl-main.470) |  | 0 |  | Justine Zhang, Cristian DanescuNiculescuMizil |  |
| 565 |  |  [Detecting Perceived Emotions in Hurricane Disasters](https://doi.org/10.18653/v1/2020.acl-main.471) |  | 0 |  | Shrey Desai, Cornelia Caragea, Junyi Jessy Li |  |
| 566 |  |  [Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention](https://doi.org/10.18653/v1/2020.acl-main.472) |  | 0 |  | Veronica E. Lynn, Niranjan Balasubramanian, H. Andrew Schwartz |  |
| 567 |  |  [Measuring Forecasting Skill from Text](https://doi.org/10.18653/v1/2020.acl-main.473) |  | 0 |  | Shi Zong, Alan Ritter, Eduard H. Hovy |  |
| 568 |  |  [Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates](https://doi.org/10.18653/v1/2020.acl-main.474) |  | 0 |  | Katherine A. Keith, David D. Jensen, Brendan O'Connor |  |
| 569 |  |  [Text-Based Ideal Points](https://doi.org/10.18653/v1/2020.acl-main.475) |  | 0 |  | Keyon Vafa, Suresh Naidu, David M. Blei |  |
| 570 |  |  [Understanding the Language of Political Agreement and Disagreement in Legislative Texts](https://doi.org/10.18653/v1/2020.acl-main.476) |  | 0 |  | Maryam Davoodi, Eric Waltenburg, Dan Goldwasser |  |
| 571 |  |  [Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences](https://doi.org/10.18653/v1/2020.acl-main.477) |  | 0 |  | Yi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy F. Chen, Anh Tuan Luu, Chris Pal |  |
| 572 |  |  [Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event](https://doi.org/10.18653/v1/2020.acl-main.478) |  | 0 |  | Prafulla Kumar Choubey, Aaron Lee, Ruihong Huang, Lu Wang |  |
| 573 |  |  [Harnessing the linguistic signal to predict scalar inferences](https://doi.org/10.18653/v1/2020.acl-main.479) |  | 0 |  | Sebastian Schuster, Yuxing Chen, Judith Degen |  |
| 574 |  |  [Implicit Discourse Relation Classification: We Need to Talk about Evaluation](https://doi.org/10.18653/v1/2020.acl-main.480) |  | 0 |  | Najoung Kim, Song Feng, R. Chulaka Gunasekara, Luis A. Lastras |  |
| 575 |  |  [PeTra: A Sparsely Supervised Memory Model for People Tracking](https://doi.org/10.18653/v1/2020.acl-main.481) |  | 0 |  | Shubham Toshniwal, Allyson Ettinger, Kevin Gimpel, Karen Livescu |  |
| 576 |  |  [ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT](https://doi.org/10.18653/v1/2020.acl-main.482) |  | 0 |  | Linfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, Dong Yu |  |
| 577 |  |  [Contextualizing Hate Speech Classifiers with Post-hoc Explanation](https://doi.org/10.18653/v1/2020.acl-main.483) |  | 0 |  | Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, Xiang Ren |  |
| 578 |  |  [Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation](https://doi.org/10.18653/v1/2020.acl-main.484) |  | 0 |  | Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez, Caiming Xiong |  |
| 579 |  |  [Language (Technology) is Power: A Critical Survey of "Bias" in NLP](https://doi.org/10.18653/v1/2020.acl-main.485) |  | 0 |  | Su Lin Blodgett, Solon Barocas, Hal Daumé III, Hanna M. Wallach |  |
| 580 |  |  [Social Bias Frames: Reasoning about Social and Power Implications of Language](https://doi.org/10.18653/v1/2020.acl-main.486) |  | 0 |  | Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, Yejin Choi |  |
| 581 |  |  [Social Biases in NLP Models as Barriers for Persons with Disabilities](https://doi.org/10.18653/v1/2020.acl-main.487) |  | 0 |  | Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, Stephen Denuyl |  |
| 582 |  |  [Towards Debiasing Sentence Representations](https://doi.org/10.18653/v1/2020.acl-main.488) |  | 0 |  | Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 583 |  |  [A Re-evaluation of Knowledge Graph Completion Methods](https://doi.org/10.18653/v1/2020.acl-main.489) |  | 0 |  | Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha P. Talukdar, Yiming Yang |  |
| 584 |  |  [Cross-Linguistic Syntactic Evaluation of Word Prediction Models](https://doi.org/10.18653/v1/2020.acl-main.490) |  | 0 |  | Aaron Mueller, Garrett Nicolai, Panayiota PetrouZeniou, Natalia Talmina, Tal Linzen |  |
| 585 |  |  [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://doi.org/10.18653/v1/2020.acl-main.491) |  | 0 |  | Peter Hase, Mohit Bansal |  |
| 586 |  |  [Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions](https://doi.org/10.18653/v1/2020.acl-main.492) |  | 0 |  | Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov |  |
| 587 |  |  [Finding Universal Grammatical Relations in Multilingual BERT](https://doi.org/10.18653/v1/2020.acl-main.493) |  | 0 |  | Ethan A. Chi, John Hewitt, Christopher D. Manning |  |
| 588 |  |  [Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection](https://doi.org/10.18653/v1/2020.acl-main.494) |  | 0 |  | Hanjie Chen, Guangtao Zheng, Yangfeng Ji |  |
| 589 |  |  [Obtaining Faithful Interpretations from Compositional Neural Networks](https://doi.org/10.18653/v1/2020.acl-main.495) |  | 0 |  | Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner |  |
| 590 |  |  [Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport](https://doi.org/10.18653/v1/2020.acl-main.496) |  | 0 |  | Kyle Swanson, Lili Yu, Tao Lei |  |
| 591 |  |  [Benefits of Intermediate Annotations in Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.497) |  | 0 |  | Dheeru Dua, Sameer Singh, Matt Gardner |  |
| 592 |  |  [Crossing Variational Autoencoders for Answer Retrieval](https://doi.org/10.18653/v1/2020.acl-main.498) |  | 0 |  | Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang |  |
| 593 |  |  [Logic-Guided Data Augmentation and Regularization for Consistent Question Answering](https://doi.org/10.18653/v1/2020.acl-main.499) |  | 0 |  | Akari Asai, Hannaneh Hajishirzi |  |
| 594 |  |  [On the Importance of Diversity in Question Generation for QA](https://doi.org/10.18653/v1/2020.acl-main.500) |  | 0 |  | Md. Arafat Sultan, Shubham Chandel, Ramón Fernandez Astudillo, Vittorio Castelli |  |
| 595 |  |  [Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question Answering](https://doi.org/10.18653/v1/2020.acl-main.501) |  | 0 |  | Hao Cheng, MingWei Chang, Kenton Lee, Kristina Toutanova |  |
| 596 |  |  [SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations](https://doi.org/10.18653/v1/2020.acl-main.502) |  | 0 |  | Xiang Kong, Varun Gangal, Eduard H. Hovy |  |
| 597 |  |  [Selective Question Answering under Domain Shift](https://doi.org/10.18653/v1/2020.acl-main.503) |  | 0 |  | Amita Kamath, Robin Jia, Percy Liang |  |
| 598 |  |  [The Cascade Transformer: an Application for Efficient Answer Sentence Selection](https://doi.org/10.18653/v1/2020.acl-main.504) |  | 0 |  | Luca Soldaini, Alessandro Moschitti |  |
| 599 |  |  [Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering](https://doi.org/10.18653/v1/2020.acl-main.505) |  | 0 |  | Changmao Li, Jinho D. Choi |  |
| 600 |  |  [Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses](https://doi.org/10.18653/v1/2020.acl-main.506) |  | 0 |  | Erfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal, Dan Roth |  |
| 601 |  |  [STARC: Structured Annotations for Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.507) |  | 0 |  | Yevgeni Berzak, Jonathan Malmaud, Roger Levy |  |
| 602 |  |  [WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge](https://doi.org/10.18653/v1/2020.acl-main.508) |  | 0 |  | Hongming Zhang, Xinran Zhao, Yangqiu Song |  |
| 603 |  |  [Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity](https://doi.org/10.18653/v1/2020.acl-main.509) |  | 0 |  | Joseph Sirrianni, Xiaoqing Liu, Douglas Adams |  |
| 604 |  |  [Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning](https://doi.org/10.18653/v1/2020.acl-main.510) |  | 0 |  | Hongliang Fei, Ping Li |  |
| 605 |  |  [Efficient Pairwise Annotation of Argument Quality](https://doi.org/10.18653/v1/2020.acl-main.511) |  | 0 |  | Lukas Gienapp, Benno Stein, Matthias Hagen, Martin Potthast |  |
| 606 |  |  [Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification](https://doi.org/10.18653/v1/2020.acl-main.512) |  | 0 |  | Nianzu Ma, Sahisnu Mazumder, Hao Wang, Bing Liu |  |
| 607 |  |  [OpinionDigest: A Simple Framework for Opinion Summarization](https://doi.org/10.18653/v1/2020.acl-main.513) |  | 0 |  | Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, WangChiew Tan |  |
| 608 |  |  [A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks](https://doi.org/10.18653/v1/2020.acl-main.514) |  | 0 |  | Nastaran Babanejad, Ameeta Agrawal, Aijun An, Manos Papagelis |  |
| 609 |  |  [Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness](https://doi.org/10.18653/v1/2020.acl-main.515) |  | 0 |  | Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, Zhonghai Wu |  |
| 610 |  |  [Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation](https://doi.org/10.18653/v1/2020.acl-main.516) |  | 0 |  | Haoyu Song, Yan Wang, Weinan Zhang, Xiaojiang Liu, Ting Liu |  |
| 611 |  |  [Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks](https://doi.org/10.18653/v1/2020.acl-main.517) |  | 0 |  | Yiping Song, Zequn Liu, Wei Bi, Rui Yan, Ming Zhang |  |
| 612 |  |  [Video-Grounded Dialogues with Pretrained Generation Language Models](https://doi.org/10.18653/v1/2020.acl-main.518) |  | 0 |  | Hung Le, Steven C. H. Hoi |  |
| 613 |  |  [A Unified MRC Framework for Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.519) |  | 0 |  | Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li |  |
| 614 |  |  [An Effective Transition-based Model for Discontinuous NER](https://doi.org/10.18653/v1/2020.acl-main.520) |  | 0 |  | Xiang Dai, Sarvnaz Karimi, Ben Hachey, Cécile Paris |  |
| 615 |  |  [IMoJIE: Iterative Memory-Based Joint Open Information Extraction](https://doi.org/10.18653/v1/2020.acl-main.521) |  | 0 |  | Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti |  |
| 616 |  |  [Improving Event Detection via Open-domain Trigger Knowledge](https://doi.org/10.18653/v1/2020.acl-main.522) |  | 0 |  | Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li, Jun Xie |  |
| 617 |  |  [Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling](https://doi.org/10.18653/v1/2020.acl-main.523) |  | 0 |  | Canasai Kruengkrai, Thien Hai Nguyen, Sharifah Aljunied Mahani, Lidong Bing |  |
| 618 |  |  [Multi-Cell Compositional LSTM for NER Domain Adaptation](https://doi.org/10.18653/v1/2020.acl-main.524) |  | 0 |  | Chen Jia, Yue Zhang |  |
| 619 |  |  [Pyramid: A Layered Model for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.525) |  | 0 |  | Jue Wang, Lidan Shou, Ke Chen, Gang Chen |  |
| 620 |  |  [ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2020.acl-main.526) |  | 0 |  | Zhiwen Xie, Guangyou Zhou, Jin Liu, Jimmy Xiangji Huang |  |
| 621 |  |  [Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents](https://doi.org/10.18653/v1/2020.acl-main.527) |  | 0 |  | Daoyuan Chen, Yaliang Li, Kai Lei, Ying Shen |  |
| 622 |  |  [Simplify the Usage of Lexicon in Chinese NER](https://doi.org/10.18653/v1/2020.acl-main.528) |  | 0 |  | Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei, Xuanjing Huang |  |
| 623 |  |  [AdvAug: Robust Adversarial Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.529) |  | 0 |  | Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein |  |
| 624 |  |  [Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns](https://doi.org/10.18653/v1/2020.acl-main.530) |  | 0 |  | KayYen Wong, Sameen Maruf, Gholamreza Haffari |  |
| 625 |  |  [Improving Neural Machine Translation with Soft Template Prediction](https://doi.org/10.18653/v1/2020.acl-main.531) |  | 0 |  | Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, Ming Zhou |  |
| 626 |  |  [Tagged Back-translation Revisited: Why Does It Really Work?](https://doi.org/10.18653/v1/2020.acl-main.532) |  | 0 |  | Benjamin Marie, Raphael Rubino, Atsushi Fujita |  |
| 627 |  |  [Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation](https://doi.org/10.18653/v1/2020.acl-main.533) |  | 0 |  | ShunPo Chuang, TzuWei Sung, Alexander H. Liu, Hungyi Lee |  |
| 628 |  |  [Neural-DINF: A Neural Network based Framework for Measuring Document Influence](https://doi.org/10.18653/v1/2020.acl-main.534) |  | 0 |  | Jie Tan, Changlin Yang, Ying Li, Siliang Tang, Chen Huang, Yueting Zhuang |  |
| 629 |  |  [Paraphrase Generation by Learning How to Edit from Samples](https://doi.org/10.18653/v1/2020.acl-main.535) |  | 0 |  | Amirhossein Kazemnejad, Mohammadreza Salehi, Mahdieh Soleymani Baghshah |  |
| 630 |  |  [Emerging Cross-lingual Structure in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.536) |  | 0 |  | Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov |  |
| 631 |  |  [FastBERT: a Self-distilling BERT with Adaptive Inference Time](https://doi.org/10.18653/v1/2020.acl-main.537) |  | 0 |  | Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju |  |
| 632 |  |  [Incorporating External Knowledge through Pre-training for Natural Language to Code Generation](https://doi.org/10.18653/v1/2020.acl-main.538) |  | 0 |  | Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig |  |
| 633 |  |  [LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network](https://doi.org/10.18653/v1/2020.acl-main.539) |  | 0 |  | Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang, Jiahai Wang, Jian Yin |  |
| 634 |  |  [Word-level Textual Adversarial Attacking as Combinatorial Optimization](https://doi.org/10.18653/v1/2020.acl-main.540) |  | 0 |  | Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun |  |
| 635 |  |  [Benchmarking Multimodal Regex Synthesis with Complex Structures](https://doi.org/10.18653/v1/2020.acl-main.541) |  | 0 |  | Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett |  |
| 636 |  |  [Curriculum Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.542) |  | 0 |  | Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang |  |
| 637 |  |  [Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?](https://doi.org/10.18653/v1/2020.acl-main.543) |  | 0 |  | Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui |  |
| 638 |  |  [Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder](https://doi.org/10.18653/v1/2020.acl-main.544) |  | 0 |  | Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang, Ming Zhou |  |
| 639 |  |  [How to Ask Good Questions? Try to Leverage Paraphrases](https://doi.org/10.18653/v1/2020.acl-main.545) |  | 0 |  | Xin Jia, Wenjie Zhou, Xu Sun, Yunfang Wu |  |
| 640 |  |  [NeuInfer: Knowledge Inference on N-ary Facts](https://doi.org/10.18653/v1/2020.acl-main.546) |  | 0 |  | Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng |  |
| 641 |  |  [Neural Graph Matching Networks for Chinese Short Text Matching](https://doi.org/10.18653/v1/2020.acl-main.547) |  | 0 |  | Lu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi Chen, Su Zhu, Kai Yu |  |
| 642 |  |  [Neural Mixed Counting Models for Dispersed Topic Discovery](https://doi.org/10.18653/v1/2020.acl-main.548) |  | 0 |  | Jiemin Wu, Yanghui Rao, Zusheng Zhang, Haoran Xie, Qing Li, Fu Lee Wang, Ziye Chen |  |
| 643 |  |  [Reasoning Over Semantic-Level Graph for Fact Checking](https://doi.org/10.18653/v1/2020.acl-main.549) |  | 0 |  | Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin |  |
| 644 |  |  [Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study](https://doi.org/10.18653/v1/2020.acl-main.550) |  | 0 |  | Xinyu Xing, Xiaosheng Fan, Xiaojun Wan |  |
| 645 |  |  [Composing Elementary Discourse Units in Abstractive Summarization](https://doi.org/10.18653/v1/2020.acl-main.551) |  | 0 |  | Zhenwen Li, Wenhao Wu, Sujian Li |  |
| 646 |  |  [Extractive Summarization as Text Matching](https://doi.org/10.18653/v1/2020.acl-main.552) |  | 0 |  | Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang |  |
| 647 |  |  [Heterogeneous Graph Neural Networks for Extractive Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.553) |  | 0 |  | Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang |  |
| 648 |  |  [Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2020.acl-main.554) |  | 0 |  | Yue Cao, Hui Liu, Xiaojun Wan |  |
| 649 |  |  [Leveraging Graph to Improve Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.555) |  | 0 |  | Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du |  |
| 650 |  |  [Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization](https://doi.org/10.18653/v1/2020.acl-main.556) |  | 0 |  | Hanqi Jin, Tianming Wang, Xiaojun Wan |  |
| 651 |  |  [Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference](https://doi.org/10.18653/v1/2020.acl-main.557) |  | 0 |  | Nikita Kitaev, Dan Klein |  |
| 652 |  |  [Are we Estimating or Guesstimating Translation Quality?](https://doi.org/10.18653/v1/2020.acl-main.558) |  | 0 |  | Shuo Sun, Francisco Guzmán, Lucia Specia |  |
| 653 |  |  [Language (Re)modelling: Towards Embodied Language Understanding](https://doi.org/10.18653/v1/2020.acl-main.559) |  | 0 |  | Ronen Tamari, Chen Shani, Tom Hope, Miriam R. L. Petruck, Omri Abend, Dafna Shahaf |  |
| 654 |  |  [The State and Fate of Linguistic Diversity and Inclusion in the NLP World](https://doi.org/10.18653/v1/2020.acl-main.560) |  | 0 |  | Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury |  |
| 655 |  |  [The Unstoppable Rise of Computational Linguistics in Deep Learning](https://doi.org/10.18653/v1/2020.acl-main.561) |  | 0 |  | James Henderson |  |
| 656 |  |  [To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying](https://doi.org/10.18653/v1/2020.acl-main.562) |  | 0 |  | Markus Gärtner, Kerstin Jung |  |
| 657 |  |  [A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://doi.org/10.18653/v1/2020.acl-main.563) |  | 0 |  | Yong Shan, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Cheng Niu, Jie Zhou |  |
| 658 |  |  [Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to Augment and Reweight](https://doi.org/10.18653/v1/2020.acl-main.564) |  | 0 |  | Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, Dawei Yin |  |
| 659 |  |  [Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog](https://doi.org/10.18653/v1/2020.acl-main.565) |  | 0 |  | Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, Ting Liu |  |
| 660 |  |  [Learning Efficient Dialogue Policy from Demonstrations through Shaping](https://doi.org/10.18653/v1/2020.acl-main.566) |  | 0 |  | Huimin Wang, Baolin Peng, KamFai Wong |  |
| 661 |  |  [SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing](https://doi.org/10.18653/v1/2020.acl-main.567) |  | 0 |  | Jiaying Hu, Yan Yang, Chencai Chen, Liang He, Zhou Yu |  |
| 662 |  |  [Speaker Sensitive Response Evaluation Model](https://doi.org/10.18653/v1/2020.acl-main.568) |  | 0 |  | JinYeong Bak, Alice Oh |  |
| 663 |  |  [A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure](https://doi.org/10.18653/v1/2020.acl-main.569) |  | 0 |  | Longyin Zhang, Yuqing Xing, Fang Kong, Peifeng Li, Guodong Zhou |  |
| 664 |  |  [Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction identification](https://doi.org/10.18653/v1/2020.acl-main.570) |  | 0 |  | Pratik Dutta, Sriparna Saha |  |
| 665 |  |  [Bipartite Flat-Graph Network for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.571) |  | 0 |  | Ying Luo, Hai Zhao |  |
| 666 |  |  [Connecting Embeddings for Knowledge Graph Entity Typing](https://doi.org/10.18653/v1/2020.acl-main.572) |  | 0 |  | Yu Zhao, Anxiang Zhang, Ruobing Xie, Kang Liu, Xiaojie Wang |  |
| 667 |  |  [Continual Relation Learning via Episodic Memory Activation and Reconsolidation](https://doi.org/10.18653/v1/2020.acl-main.573) |  | 0 |  | Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 668 |  |  [Handling Rare Entities for Neural Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.574) |  | 0 |  | Yangming Li, Han Li, Kaisheng Yao, Xiaolong Li |  |
| 669 |  |  [Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.575) |  | 0 |  | Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, Tatsuki Kuribayashi, Ryuto Konno, Kentaro Inui |  |
| 670 |  |  [MIE: A Medical Information Extractor towards Medical Dialogues](https://doi.org/10.18653/v1/2020.acl-main.576) |  | 0 |  | Yuanzhe Zhang, Zhongtao Jiang, Tao Zhang, Shiwan Liu, Jiarun Cao, Kang Liu, Shengping Liu, Jun Zhao |  |
| 671 |  |  [Named Entity Recognition as Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.577) |  | 0 |  | Juntao Yu, Bernd Bohnet, Massimo Poesio |  |
| 672 |  |  [Neighborhood Matching Network for Entity Alignment](https://doi.org/10.18653/v1/2020.acl-main.578) |  | 0 |  | Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Dongyan Zhao |  |
| 673 |  |  [Relation Extraction with Explanation](https://doi.org/10.18653/v1/2020.acl-main.579) |  | 0 |  | Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Prasad Tadepalli |  |
| 674 |  |  [Representation Learning for Information Extraction from Form-like Documents](https://doi.org/10.18653/v1/2020.acl-main.580) |  | 0 |  | Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork |  |
| 675 |  |  [Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language](https://doi.org/10.18653/v1/2020.acl-main.581) |  | 0 |  | Qianhui Wu, Zijia Lin, Börje Karlsson, Jianguang Lou, Biqing Huang |  |
| 676 |  |  [Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction](https://doi.org/10.18653/v1/2020.acl-main.582) |  | 0 |  | Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang, Ziming Chi |  |
| 677 |  |  [Cross-modal Coherence Modeling for Caption Generation](https://doi.org/10.18653/v1/2020.acl-main.583) |  | 0 |  | Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, Matthew Stone |  |
| 678 |  |  [Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms](https://doi.org/10.18653/v1/2020.acl-main.584) |  | 0 |  | Simeon Schüz, Sina Zarrieß |  |
| 679 |  |  [Span-based Localizing Network for Natural Language Video Localization](https://doi.org/10.18653/v1/2020.acl-main.585) |  | 0 |  | Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou |  |
| 680 |  |  [Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions](https://doi.org/10.18653/v1/2020.acl-main.586) |  | 0 |  | Arjun R. Akula, Spandana Gella, Yaser AlOnaizan, SongChun Zhu, Siva Reddy |  |
| 681 |  |  [A Mixture of h - 1 Heads is Better than h Heads](https://doi.org/10.18653/v1/2020.acl-main.587) |  | 0 |  | Hao Peng, Roy Schwartz, Dianqi Li, Noah A. Smith |  |
| 682 |  |  [Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification](https://doi.org/10.18653/v1/2020.acl-main.588) |  | 0 |  | Hao Tang, Donghong Ji, Chenliang Li, Qiji Zhou |  |
| 683 |  |  [Differentiable Window for Dynamic Local Attention](https://doi.org/10.18653/v1/2020.acl-main.589) |  | 0 |  | ThanhTung Nguyen, XuanPhi Nguyen, Shafiq R. Joty, Xiaoli Li |  |
| 684 |  |  [Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples](https://doi.org/10.18653/v1/2020.acl-main.590) |  | 0 |  | Xiaoqing Zheng, Jiehang Zeng, Yi Zhou, ChoJui Hsieh, Minhao Cheng, Xuanjing Huang |  |
| 685 |  |  [Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach](https://doi.org/10.18653/v1/2020.acl-main.591) |  | 0 |  | Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang |  |
| 686 |  |  [Learning Architectures from an Extended Search Space for Language Modeling](https://doi.org/10.18653/v1/2020.acl-main.592) |  | 0 |  | Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li |  |
| 687 |  |  [The Right Tool for the Job: Matching Model and Instance Complexities](https://doi.org/10.18653/v1/2020.acl-main.593) |  | 0 |  | Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah A. Smith |  |
| 688 |  |  [Bootstrapping Techniques for Polysynthetic Morphological Analysis](https://doi.org/10.18653/v1/2020.acl-main.594) |  | 0 |  | William Lane, Steven Bird |  |
| 689 |  |  [Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation](https://doi.org/10.18653/v1/2020.acl-main.595) |  | 0 |  | Ning Ding, Dingkun Long, Guangwei Xu, Muhua Zhu, Pengjun Xie, Xiaobin Wang, Haitao Zheng |  |
| 690 |  |  [Modeling Morphological Typology for Unsupervised Learning of Language Morphology](https://doi.org/10.18653/v1/2020.acl-main.596) |  | 0 |  | Hongzhi Xu, Jordan Kodner, Mitchell Marcus, Charles Yang |  |
| 691 |  |  [Predicting Declension Class from Form and Meaning](https://doi.org/10.18653/v1/2020.acl-main.597) |  | 0 |  | Adina Williams, Tiago Pimentel, Hagen Blix, Arya D. McCarthy, Eleanor Chodroff, Ryan Cotterell |  |
| 692 |  |  [Unsupervised Morphological Paradigm Completion](https://doi.org/10.18653/v1/2020.acl-main.598) |  | 0 |  | Huiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya McCarthy, Katharina Kann |  |
| 693 |  |  [Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.599) |  | 0 |  | Bo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan, Wanxiang Che, Daxin Jiang, Ming Zhou, Ting Liu |  |
| 694 |  |  [Harvesting and Refining Question-Answer Pairs for Unsupervised QA](https://doi.org/10.18653/v1/2020.acl-main.600) |  | 0 |  | Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, Ke Xu |  |
| 695 |  |  [Low-Resource Generation of Multi-hop Reasoning Questions](https://doi.org/10.18653/v1/2020.acl-main.601) |  | 0 |  | Jianxing Yu, Wei Liu, Shuang Qiu, Qinliang Su, Kai Wang, Xiaojun Quan, Jian Yin |  |
| 696 |  |  [R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason](https://doi.org/10.18653/v1/2020.acl-main.602) |  | 0 |  | Naoya Inoue, Pontus Stenetorp, Kentaro Inui |  |
| 697 |  |  [Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension](https://doi.org/10.18653/v1/2020.acl-main.603) |  | 0 |  | Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, Dong Yu |  |
| 698 |  |  [RikiNet: Reading Wikipedia Pages for Natural Question Answering](https://doi.org/10.18653/v1/2020.acl-main.604) |  | 0 |  | Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Nan Duan |  |
| 699 |  |  [Parsing into Variable-in-situ Logico-Semantic Graphs](https://doi.org/10.18653/v1/2020.acl-main.605) |  | 0 |  | Yufei Chen, Weiwei Sun |  |
| 700 |  |  [Semantic Parsing for English as a Second Language](https://doi.org/10.18653/v1/2020.acl-main.606) |  | 0 |  | Yuanyuan Zhao, Weiwei Sun, Junjie Cao, Xiaojun Wan |  |
| 701 |  |  [Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders](https://doi.org/10.18653/v1/2020.acl-main.607) |  | 0 |  | Zixia Jia, Youmi Ma, Jiong Cai, Kewei Tu |  |
| 702 |  |  [Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.608) |  | 0 |  | Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen, Kai Yu |  |
| 703 |  |  [DRTS Parsing with Structure-Aware Encoding and Decoding](https://doi.org/10.18653/v1/2020.acl-main.609) |  | 0 |  | Qiankun Fu, Yue Zhang, Jiangming Liu, Meishan Zhang |  |
| 704 |  |  [A Two-Stage Masked LM Method for Term Set Expansion](https://doi.org/10.18653/v1/2020.acl-main.610) |  | 0 |  | Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg |  |
| 705 |  |  [FLAT: Chinese NER Using Flat-Lattice Transformer](https://doi.org/10.18653/v1/2020.acl-main.611) |  | 0 |  | Xiaonan Li, Hang Yan, Xipeng Qiu, Xuanjing Huang |  |
| 706 |  |  [Improving Entity Linking through Semantic Reinforced Entity Embeddings](https://doi.org/10.18653/v1/2020.acl-main.612) |  | 0 |  | Feng Hou, Ruili Wang, Jun He, Yi Zhou |  |
| 707 |  |  [Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain](https://doi.org/10.18653/v1/2020.acl-main.613) |  | 0 |  | Shadi Saleh, Pavel Pecina |  |
| 708 |  |  [Learning Robust Models for e-Commerce Product Search](https://doi.org/10.18653/v1/2020.acl-main.614) |  | 0 |  | Thanh V. Nguyen, Nikhil Rao, Karthik Subbian |  |
| 709 |  |  [Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing](https://doi.org/10.18653/v1/2020.acl-main.615) |  | 0 |  | Clara Meister, Elizabeth Salesky, Ryan Cotterell |  |
| 710 |  |  [Highway Transformer: Self-Gating Enhanced Self-Attentive Networks](https://doi.org/10.18653/v1/2020.acl-main.616) |  | 0 |  | Yekun Chai, Jin Shuo, Xinwen Hou |  |
| 711 |  |  [Low-Dimensional Hyperbolic Knowledge Graph Embeddings](https://doi.org/10.18653/v1/2020.acl-main.617) |  | 0 |  | Ines Chami, Adva Wolf, DaCheng Juan, Frederic Sala, Sujith Ravi, Christopher Ré |  |
| 712 |  |  [Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction](https://doi.org/10.18653/v1/2020.acl-main.618) |  | 0 |  | Mladen Karan, Ivan Vulic, Anna Korhonen, Goran Glavas |  |
| 713 |  |  [Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus](https://doi.org/10.18653/v1/2020.acl-main.619) |  | 0 |  | Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mattia Antonino Di Gangi, Roldano Cattoni, Marco Turchi |  |
| 714 |  |  [Uncertainty-Aware Curriculum Learning for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.620) |  | 0 |  | Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, Lidia S. Chao |  |
| 715 |  |  [Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain](https://doi.org/10.18653/v1/2020.acl-main.621) |  | 0 |  | Lukas Lange, Heike Adel, Jannik Strötgen |  |
| 716 |  |  [CorefQA: Coreference Resolution as Query-based Span Prediction](https://doi.org/10.18653/v1/2020.acl-main.622) |  | 0 |  | Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, Jiwei Li |  |
| 717 |  |  [Estimating predictive uncertainty for rumour verification models](https://doi.org/10.18653/v1/2020.acl-main.623) |  | 0 |  | Elena Kochkina, Maria Liakata |  |
| 718 |  |  [From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains](https://doi.org/10.18653/v1/2020.acl-main.624) |  | 0 |  | JanChristoph Klie, Richard Eckart de Castilho, Iryna Gurevych |  |
| 719 |  |  [Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions](https://doi.org/10.18653/v1/2020.acl-main.625) |  | 0 |  | Tian Jin, Zhun Liu, Shengjia Yan, Alexandre E. Eichenberger, LouisPhilippe Morency |  |
| 720 |  |  [Controlled Crowdsourcing for High-Quality QA-SRL Annotation](https://doi.org/10.18653/v1/2020.acl-main.626) |  | 0 |  | Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke Zettlemoyer, Ido Dagan |  |
| 721 |  |  [Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus](https://doi.org/10.18653/v1/2020.acl-main.627) |  | 0 |  | Hao Fei, Meishan Zhang, Donghong Ji |  |
| 722 |  |  [Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity](https://doi.org/10.18653/v1/2020.acl-main.628) |  | 0 |  | Nina Pörner, Ulli Waltinger, Hinrich Schütze |  |
| 723 |  |  [Transition-based Semantic Dependency Parsing with Pointer Networks](https://doi.org/10.18653/v1/2020.acl-main.629) |  | 0 |  | Daniel FernándezGonzález, Carlos GómezRodríguez |  |
| 724 |  |  [tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection](https://doi.org/10.18653/v1/2020.acl-main.630) |  | 0 |  | Nicole Peinelt, Dong Nguyen, Maria Liakata |  |
| 725 |  |  [Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation](https://doi.org/10.18653/v1/2020.acl-main.631) |  | 0 |  | Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling, Yan Song |  |
| 726 |  |  [Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness](https://doi.org/10.18653/v1/2020.acl-main.632) |  | 0 |  | Khalid Al Khatib, Michael Völske, Shahbaz Syed, Nikolay Kolyada, Benno Stein |  |
| 727 |  |  [Out of the Echo Chamber: Detecting Countering Debate Speeches](https://doi.org/10.18653/v1/2020.acl-main.633) |  | 0 |  | Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim |  |
| 728 |  |  [Diversifying Dialogue Generation with Non-Conversational Text](https://doi.org/10.18653/v1/2020.acl-main.634) |  | 0 |  | Hui Su, Xiaoyu Shen, Sanqiang Zhao, Xiao Zhou, Pengwei Hu, Randy Zhong, Cheng Niu, Jie Zhou |  |
| 729 |  |  [KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation](https://doi.org/10.18653/v1/2020.acl-main.635) |  | 0 |  | Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, Xiaoyan Zhu |  |
| 730 |  |  [Meta-Reinforced Multi-Domain State Generator for Dialogue Systems](https://doi.org/10.18653/v1/2020.acl-main.636) |  | 0 |  | Yi Huang, Junlan Feng, Min Hu, Xiaoting Wu, Xiaoyu Du, Shuo Ma |  |
| 731 |  |  [Modeling Long Context for Task-Oriented Dialogue State Generation](https://doi.org/10.18653/v1/2020.acl-main.637) |  | 0 |  | Jun Quan, Deyi Xiong |  |
| 732 |  |  [Multi-Domain Dialogue Acts and Response Co-Generation](https://doi.org/10.18653/v1/2020.acl-main.638) |  | 0 |  | Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu |  |
| 733 |  |  [Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer](https://doi.org/10.18653/v1/2020.acl-main.639) |  | 0 |  | Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, Hua Wu |  |
| 734 |  |  [Heterogeneous Graph Transformer for Graph-to-Sequence Learning](https://doi.org/10.18653/v1/2020.acl-main.640) |  | 0 |  | Shaowei Yao, Tianming Wang, Xiaojun Wan |  |
| 735 |  |  [Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence](https://doi.org/10.18653/v1/2020.acl-main.641) |  | 0 |  | Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow |  |
| 736 |  |  [Aligned Dual Channel Graph Convolutional Network for Visual Question Answering](https://doi.org/10.18653/v1/2020.acl-main.642) |  | 0 |  | Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Hofung Leung, Qing Li |  |
| 737 |  |  [Multimodal Neural Graph Memory Networks for Visual Question Answering](https://doi.org/10.18653/v1/2020.acl-main.643) |  | 0 |  | Mahmoud Khademi |  |
| 738 |  |  [Refer360$^\circ$: A Referring Expression Recognition Dataset in 360$^\circ$ Images](https://doi.org/10.18653/v1/2020.acl-main.644) |  | 0 |  | Volkan Cirik, Taylor BergKirkpatrick, LouisPhilippe Morency |  |
| 739 |  |  [CamemBERT: a Tasty French Language Model](https://doi.org/10.18653/v1/2020.acl-main.645) |  | 0 |  | Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah, Benoît Sagot |  |
| 740 |  |  [Effective Estimation of Deep Generative Language Models](https://doi.org/10.18653/v1/2020.acl-main.646) |  | 0 |  | Tom Pelsmaeker, Wilker Aziz |  |
| 741 |  |  [Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection](https://doi.org/10.18653/v1/2020.acl-main.647) |  | 0 |  | Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg |  |
| 742 |  |  [2kenize: Tying Subword Sequences for Chinese Script Conversion](https://doi.org/10.18653/v1/2020.acl-main.648) |  | 0 |  | Pranav A, Isabelle Augenstein |  |
| 743 |  |  [Predicting the Growth of Morphological Families from Social and Linguistic Factors](https://doi.org/10.18653/v1/2020.acl-main.649) |  | 0 |  | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 744 |  |  [Semi-supervised Contextual Historical Text Normalization](https://doi.org/10.18653/v1/2020.acl-main.650) |  | 0 |  | Peter Makarov, Simon Clematide |  |
| 745 |  |  [ClarQ: A large-scale and diverse dataset for Clarification Question Generation](https://doi.org/10.18653/v1/2020.acl-main.651) |  | 0 |  | Vaibhav Kumar, Alan W. Black |  |
| 746 |  |  [DoQA - Accessing Domain-Specific FAQs via Conversational QA](https://doi.org/10.18653/v1/2020.acl-main.652) |  | 0 |  | Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, Eneko Agirre |  |
| 747 |  |  [MLQA: Evaluating Cross-lingual Extractive Question Answering](https://doi.org/10.18653/v1/2020.acl-main.653) |  | 0 |  | Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk |  |
| 748 |  |  [Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering](https://doi.org/10.18653/v1/2020.acl-main.654) |  | 0 |  | Ming Yan, Hao Zhang, Di Jin, Joey Tianyi Zhou |  |
| 749 |  |  [Fine-grained Fact Verification with Kernel Graph Attention Network](https://doi.org/10.18653/v1/2020.acl-main.655) |  | 0 |  | Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu |  |
| 750 |  |  [Generating Fact Checking Explanations](https://doi.org/10.18653/v1/2020.acl-main.656) |  | 0 |  | Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein |  |
| 751 |  |  [Premise Selection in Natural Language Mathematical Texts](https://doi.org/10.18653/v1/2020.acl-main.657) |  | 0 |  | Deborah Ferreira, André Freitas |  |
| 752 |  |  [A Call for More Rigor in Unsupervised Cross-lingual Learning](https://doi.org/10.18653/v1/2020.acl-main.658) |  | 0 |  | Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko Agirre |  |
| 753 |  |  [A Tale of a Probe and a Parser](https://doi.org/10.18653/v1/2020.acl-main.659) |  | 0 |  | Rowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams, Ryan Cotterell |  |
| 754 |  |  [From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich Languages (MRLs)?](https://doi.org/10.18653/v1/2020.acl-main.660) |  | 0 |  | Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker |  |
| 755 |  |  [Speech Translation and the End-to-End Promise: Taking Stock of Where We Are](https://doi.org/10.18653/v1/2020.acl-main.661) |  | 0 |  | Matthias Sperber, Matthias Paulik |  |
| 756 |  |  [What Question Answering can Learn from Trivia Nerds](https://doi.org/10.18653/v1/2020.acl-main.662) |  | 0 |  | Jordan L. BoydGraber, Benjamin Börschinger |  |
| 757 |  |  [What are the Goals of Distributional Semantics?](https://doi.org/10.18653/v1/2020.acl-main.663) |  | 0 |  | Guy Emerson |  |
| 758 |  |  [Improving Image Captioning with Better Use of Caption](https://doi.org/10.18653/v1/2020.acl-main.664) |  | 0 |  | Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu |  |
| 759 |  |  [Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization](https://doi.org/10.18653/v1/2020.acl-main.665) |  | 0 |  | Henry Elder, Robert Burke, Alexander O'Connor, Jennifer Foster |  |
| 760 |  |  [Toward Better Storylines with Sentence-Level Language Models](https://doi.org/10.18653/v1/2020.acl-main.666) |  | 0 |  | Daphne Ippolito, David Grangier, Douglas Eck, Chris CallisonBurch |  |
| 761 |  |  [A Two-Step Approach for Implicit Event Argument Detection](https://doi.org/10.18653/v1/2020.acl-main.667) |  | 0 |  | Zhisong Zhang, Xiang Kong, Zhengzhong Liu, Xuezhe Ma, Eduard H. Hovy |  |
| 762 |  |  [Machine Reading of Historical Events](https://doi.org/10.18653/v1/2020.acl-main.668) |  | 0 |  | Or Honovich, Lucas Torroba Hennigen, Omri Abend, Shay B. Cohen |  |
| 763 |  |  [Revisiting Unsupervised Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.669) |  | 0 |  | Thy Thy Tran, Phong Le, Sophia Ananiadou |  |
| 764 |  |  [SciREX: A Challenge Dataset for Document-Level Information Extraction](https://doi.org/10.18653/v1/2020.acl-main.670) |  | 0 |  | Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy |  |
| 765 |  |  [Contrastive Self-Supervised Learning for Commonsense Reasoning](https://doi.org/10.18653/v1/2020.acl-main.671) |  | 0 |  | Tassilo Klein, Moin Nabi |  |
| 766 |  |  [Do Transformers Need Deep Long-Range Memory?](https://doi.org/10.18653/v1/2020.acl-main.672) |  | 0 |  | Jack W. Rae, Ali Razavi |  |
| 767 |  |  [Improving Disentangled Text Representation Learning with Information-Theoretic Guidance](https://doi.org/10.18653/v1/2020.acl-main.673) |  | 0 |  | Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin |  |
| 768 |  |  [Understanding Advertisements with BERT](https://doi.org/10.18653/v1/2020.acl-main.674) |  | 0 |  | Kanika Kalra, Bhargav Kurma, Silpa Vadakkeeveetil Sreelatha, Manasi Patwardhan, Shirish Karande |  |
| 769 |  |  [Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces](https://doi.org/10.18653/v1/2020.acl-main.675) |  | 0 |  | Goran Glavas, Ivan Vulic |  |
| 770 |  |  [Good-Enough Compositional Data Augmentation](https://doi.org/10.18653/v1/2020.acl-main.676) |  | 0 |  | Jacob Andreas |  |
| 771 |  |  [RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers](https://doi.org/10.18653/v1/2020.acl-main.677) |  | 0 |  | Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson |  |
| 772 |  |  [Temporal Common Sense Acquisition with Minimal Supervision](https://doi.org/10.18653/v1/2020.acl-main.678) |  | 0 |  | Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth |  |
| 773 |  |  [The Sensitivity of Language Models and Humans to Winograd Schema Perturbations](https://doi.org/10.18653/v1/2020.acl-main.679) |  | 0 |  | Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov, Desmond Elliott, Anders Søgaard |  |
| 774 |  |  [Temporally-Informed Analysis of Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.680) |  | 0 |  | Shruti Rijhwani, Daniel PreotiucPietro |  |
| 775 |  |  [Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation](https://doi.org/10.18653/v1/2020.acl-main.681) |  | 0 |  | Aakanksha Naik, Carolyn P. Rosé |  |
| 776 |  |  [CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning](https://doi.org/10.18653/v1/2020.acl-main.682) |  | 0 |  | Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon |  |
| 777 |  |  [Cross-Modality Relevance for Reasoning on Language and Vision](https://doi.org/10.18653/v1/2020.acl-main.683) |  | 0 |  | Chen Zheng, Quan Guo, Parisa Kordjamshidi |  |
| 778 |  |  [Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context](https://doi.org/10.18653/v1/2020.acl-main.684) |  | 0 |  | Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic, Christopher Meek |  |
| 779 |  |  [Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning](https://doi.org/10.18653/v1/2020.acl-main.685) |  | 0 |  | Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman |  |
| 780 |  |  [HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://doi.org/10.18653/v1/2020.acl-main.686) |  | 0 |  | Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han |  |
| 781 |  |  [Hard-Coded Gaussian Attention for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.687) |  | 0 |  | Weiqiu You, Simeng Sun, Mohit Iyyer |  |
| 782 |  |  [In Neural Machine Translation, What Does Transfer Learning Transfer?](https://doi.org/10.18653/v1/2020.acl-main.688) |  | 0 |  | Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, Rico Sennrich |  |
| 783 |  |  [Learning a Multi-Domain Curriculum for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.689) |  | 0 |  | Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell, Zarana Parekh |  |
| 784 |  |  [Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem](https://doi.org/10.18653/v1/2020.acl-main.690) |  | 0 |  | Danielle Saunders, Bill Byrne |  |
| 785 |  |  [Translationese as a Language in "Multilingual" NMT](https://doi.org/10.18653/v1/2020.acl-main.691) |  | 0 |  | Parker Riley, Isaac Caswell, Markus Freitag, David Grangier |  |
| 786 |  |  [Unsupervised Domain Clusters in Pretrained Language Models](https://doi.org/10.18653/v1/2020.acl-main.692) |  | 0 |  | Roee Aharoni, Yoav Goldberg |  |
| 787 |  |  [Using Context in Neural Machine Translation Training Objectives](https://doi.org/10.18653/v1/2020.acl-main.693) |  | 0 |  | Danielle Saunders, Felix Stahlberg, Bill Byrne |  |
| 788 |  |  [Variational Neural Machine Translation with Normalizing Flows](https://doi.org/10.18653/v1/2020.acl-main.694) |  | 0 |  | Hendra Setiawan, Matthias Sperber, Udhyakumar Nallasamy, Matthias Paulik |  |
| 789 |  |  [The Paradigm Discovery Problem](https://doi.org/10.18653/v1/2020.acl-main.695) |  | 0 |  | Alexander Erdmann, Micha Elsner, Shijie Wu, Ryan Cotterell, Nizar Habash |  |
| 790 |  |  [Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi](https://doi.org/10.18653/v1/2020.acl-main.696) |  | 0 |  | Aryaman Arora, Luke Gessler, Nathan Schneider |  |
| 791 |  |  [Automated Evaluation of Writing - 50 Years and Counting](https://doi.org/10.18653/v1/2020.acl-main.697) |  | 0 |  | Beata Beigman Klebanov, Nitin Madnani |  |
| 792 |  |  [Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly](https://doi.org/10.18653/v1/2020.acl-main.698) |  | 0 |  | Nora Kassner, Hinrich Schütze |  |
| 793 |  |  [On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology](https://doi.org/10.18653/v1/2020.acl-main.699) |  | 0 |  | Marcel Bollmann, Desmond Elliott |  |
| 794 |  |  [Returning the N to NLP: Towards Contextually Personalized Classification Models](https://doi.org/10.18653/v1/2020.acl-main.700) |  | 0 |  | Lucie Flek |  |
| 795 |  |  [To Test Machine Comprehension, Start by Defining Comprehension](https://doi.org/10.18653/v1/2020.acl-main.701) |  | 0 |  | Jesse Dunietz, Gregory Burnham, Akash Bharadwaj, Owen Rambow, Jennifer ChuCarroll, David A. Ferrucci |  |
| 796 |  |  [Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations](https://doi.org/10.18653/v1/2020.acl-main.702) |  | 0 |  | Saif M. Mohammad |  |
| 797 |  |  [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://doi.org/10.18653/v1/2020.acl-main.703) |  | 0 |  | Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer |  |
| 798 |  |  [BLEURT: Learning Robust Metrics for Text Generation](https://doi.org/10.18653/v1/2020.acl-main.704) |  | 0 |  | Thibault Sellam, Dipanjan Das, Ankur P. Parikh |  |
| 799 |  |  [Distilling Knowledge Learned in BERT for Text Generation](https://doi.org/10.18653/v1/2020.acl-main.705) |  | 0 |  | YenChun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu |  |
| 800 |  |  [ESPRIT: Explaining Solutions to Physical Reasoning Tasks](https://doi.org/10.18653/v1/2020.acl-main.706) |  | 0 |  | Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta, Caiming Xiong, Richard Socher, Dragomir R. Radev |  |
| 801 |  |  [Iterative Edit-Based Unsupervised Sentence Simplification](https://doi.org/10.18653/v1/2020.acl-main.707) |  | 0 |  | Dhruv Kumar, Lili Mou, Lukasz Golab, Olga Vechtomova |  |
| 802 |  |  [Logical Natural Language Generation from Open-Domain Tables](https://doi.org/10.18653/v1/2020.acl-main.708) |  | 0 |  | Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang |  |
| 803 |  |  [Neural CRF Model for Sentence Alignment in Text Simplification](https://doi.org/10.18653/v1/2020.acl-main.709) |  | 0 |  | Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu |  |
| 804 |  |  [One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases](https://doi.org/10.18653/v1/2020.acl-main.710) |  | 0 |  | Xingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He, Adam Trischler |  |
| 805 |  |  [R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge](https://doi.org/10.18653/v1/2020.acl-main.711) |  | 0 |  | Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng |  |
| 806 |  |  [Structural Information Preserving for Graph-to-Text Generation](https://doi.org/10.18653/v1/2020.acl-main.712) |  | 0 |  | Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu |  |
| 807 |  |  [A Joint Neural Model for Information Extraction with Global Features](https://doi.org/10.18653/v1/2020.acl-main.713) |  | 0 |  | Ying Lin, Heng Ji, Fei Huang, Lingfei Wu |  |
| 808 |  |  [Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding](https://doi.org/10.18653/v1/2020.acl-main.714) |  | 0 |  | Xinya Du, Claire Cardie |  |
| 809 |  |  [Exploiting the Syntax-Model Consistency for Neural Relation Extraction](https://doi.org/10.18653/v1/2020.acl-main.715) |  | 0 |  | Amir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou, Thien Huu Nguyen |  |
| 810 |  |  [From English to Code-Switching: Transfer Learning with Strong Morphological Clues](https://doi.org/10.18653/v1/2020.acl-main.716) |  | 0 |  | Gustavo Aguilar, Thamar Solorio |  |
| 811 |  |  [Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts](https://doi.org/10.18653/v1/2020.acl-main.717) |  | 0 |  | Jingyuan Zhang, Mingming Sun, Yue Feng, Ping Li |  |
| 812 |  |  [Multi-Sentence Argument Linking](https://doi.org/10.18653/v1/2020.acl-main.718) |  | 0 |  | Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, Benjamin Van Durme |  |
| 813 |  |  [Rationalizing Medical Relation Prediction from Corpus-level Statistics](https://doi.org/10.18653/v1/2020.acl-main.719) |  | 0 |  | Zhen Wang, Jennifer Lee, Simon M. Lin, Huan Sun |  |
| 814 |  |  [Sources of Transfer in Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.720) |  | 0 |  | David Mueller, Nicholas Andrews, Mark Dredze |  |
| 815 |  |  [ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages](https://doi.org/10.18653/v1/2020.acl-main.721) |  | 0 |  | Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi |  |
| 816 |  |  [Soft Gazetteers for Low-Resource Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.722) |  | 0 |  | Shruti Rijhwani, Shuyan Zhou, Graham Neubig, Jaime G. Carbonell |  |
| 817 |  |  [A Prioritization Model for Suicidality Risk Assessment](https://doi.org/10.18653/v1/2020.acl-main.723) |  | 0 |  | HanChin Shing, Philip Resnik, Douglas W. Oard |  |
| 818 |  |  [CluHTM - Semantic Hierarchical Topic Modeling based on CluWords](https://doi.org/10.18653/v1/2020.acl-main.724) |  | 0 |  | Felipe Viegas, Washington Cunha, Christian Gomes, Antônio Pereira De Souza Júnior, Leonardo Rocha, Marcos André Gonçalves |  |
| 819 |  |  [Empower Entity Set Expansion via Language Model Probing](https://doi.org/10.18653/v1/2020.acl-main.725) |  | 0 |  | Yunyi Zhang, Jiaming Shen, Jingbo Shang, Jiawei Han |  |
| 820 |  |  [Feature Projection for Improved Text Classification](https://doi.org/10.18653/v1/2020.acl-main.726) |  | 0 |  | Qi Qin, Wenpeng Hu, Bing Liu |  |
| 821 |  |  [A negative case analysis of visual grounding methods for VQA](https://doi.org/10.18653/v1/2020.acl-main.727) |  | 0 |  | Robik Shrestha, Kushal Kafle, Christopher Kanan |  |
| 822 |  |  [History for Visual Dialog: Do we really need it?](https://doi.org/10.18653/v1/2020.acl-main.728) |  | 0 |  | Shubham Agarwal, Trung Bui, JoonYoung Lee, Ioannis Konstas, Verena Rieser |  |
| 823 |  |  [Mapping Natural Language Instructions to Mobile UI Action Sequences](https://doi.org/10.18653/v1/2020.acl-main.729) |  | 0 |  | Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge |  |
| 824 |  |  [TVQA+: Spatio-Temporal Grounding for Video Question Answering](https://doi.org/10.18653/v1/2020.acl-main.730) |  | 0 |  | Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal |  |
| 825 |  |  [Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting](https://doi.org/10.18653/v1/2020.acl-main.731) |  | 0 |  | PoYao Huang, Junjie Hu, Xiaojun Chang, Alexander G. Hauptmann |  |
| 826 |  |  [A Multitask Learning Approach for Diacritic Restoration](https://doi.org/10.18653/v1/2020.acl-main.732) |  | 0 |  | Sawsan Alqahtani, Ajay Mishra, Mona T. Diab |  |
| 827 |  |  [Frugal Paradigm Completion](https://doi.org/10.18653/v1/2020.acl-main.733) |  | 0 |  | Alexander Erdmann, Tom Kenter, Markus Becker, Christian Schallhart |  |
| 828 |  |  [Improving Chinese Word Segmentation with Wordhood Memory Networks](https://doi.org/10.18653/v1/2020.acl-main.734) |  | 0 |  | Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, Yonggang Wang |  |
| 829 |  |  [Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed Knowledge](https://doi.org/10.18653/v1/2020.acl-main.735) |  | 0 |  | Yuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiaojun Quan, Tong Zhang, Yonggang Wang |  |
| 830 |  |  [Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging](https://doi.org/10.18653/v1/2020.acl-main.736) |  | 0 |  | Nasser Zalmout, Nizar Habash |  |
| 831 |  |  [Phonetic and Visual Priors for Decipherment of Informal Romanization](https://doi.org/10.18653/v1/2020.acl-main.737) |  | 0 |  | Maria Ryskina, Matthew R. Gormley, Taylor BergKirkpatrick |  |
| 832 |  |  [Active Learning for Coreference Resolution using Discrete Annotation](https://doi.org/10.18653/v1/2020.acl-main.738) |  | 0 |  | Belinda Z. Li, Gabriel Stanovsky, Luke Zettlemoyer |  |
| 833 |  |  [Beyond Possession Existence: Duration and Co-Possession](https://doi.org/10.18653/v1/2020.acl-main.739) |  | 0 |  | Dhivya Chinnappa, Srikala Murugan, Eduardo Blanco |  |
| 834 |  |  [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://doi.org/10.18653/v1/2020.acl-main.740) |  | 0 |  | Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A. Smith |  |
| 835 |  |  [Estimating Mutual Information Between Dense Word Embeddings](https://doi.org/10.18653/v1/2020.acl-main.741) |  | 0 |  | Vitalii Zhelezniak, Aleksandar Savkov, Nils Hammerla |  |
| 836 |  |  [Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.742) |  | 0 |  | Alane Suhr, MingWei Chang, Peter Shaw, Kenton Lee |  |
| 837 |  |  [Predicting the Focus of Negation: Model and Error Analysis](https://doi.org/10.18653/v1/2020.acl-main.743) |  | 0 |  | Md Mosharaf Hossain, Kathleen E. Hamilton, Alexis Palmer, Eduardo Blanco |  |
| 838 |  |  [Structured Tuning for Semantic Role Labeling](https://doi.org/10.18653/v1/2020.acl-main.744) |  | 0 |  | Tao Li, Parth Anand Jawale, Martha Palmer, Vivek Srikumar |  |
| 839 |  |  [TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data](https://doi.org/10.18653/v1/2020.acl-main.745) |  | 0 |  | Pengcheng Yin, Graham Neubig, Wentau Yih, Sebastian Riedel |  |
| 840 |  |  [Universal Decompositional Semantic Parsing](https://doi.org/10.18653/v1/2020.acl-main.746) |  | 0 |  | Elias StengelEskin, Aaron Steven White, Sheng Zhang, Benjamin Van Durme |  |
| 841 |  |  [Unsupervised Cross-lingual Representation Learning at Scale](https://doi.org/10.18653/v1/2020.acl-main.747) |  | 0 |  | Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov |  |
| 842 |  |  [A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization](https://doi.org/10.18653/v1/2020.acl-main.748) |  | 0 |  | Dongfang Xu, Zeyu Zhang, Steven Bethard |  |
| 843 |  |  [Hierarchical Entity Typing via Multi-level Learning to Rank](https://doi.org/10.18653/v1/2020.acl-main.749) |  | 0 |  | Tongfei Chen, Yunmo Chen, Benjamin Van Durme |  |
| 844 |  |  [Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference](https://doi.org/10.18653/v1/2020.acl-main.750) |  | 0 |  | Jing Wang, Mayank Kulkarni, Daniel PreotiucPietro |  |
| 845 |  |  [TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories](https://doi.org/10.18653/v1/2020.acl-main.751) |  | 0 |  | Giannis Karamanolakis, Jun Ma, Xin Luna Dong |  |
| 846 |  |  [TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition](https://doi.org/10.18653/v1/2020.acl-main.752) |  | 0 |  | Bill Yuchen Lin, DongHo Lee, Ming Shen, Ryan Moreno, Xiao Huang, Prashant Shiralkar, Xiang Ren |  |
| 847 |  |  [Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.753) |  | 0 |  | Arya D. McCarthy, Xian Li, Jiatao Gu, Ning Dong |  |
| 848 |  |  [Balancing Training for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.754) |  | 0 |  | Xinyi Wang, Yulia Tsvetkov, Graham Neubig |  |
| 849 |  |  [Evaluating Robustness to Input Perturbations for Neural Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.755) |  | 0 |  | Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser AlOnaizan |  |
| 850 |  |  [Parallel Corpus Filtering via Pre-trained Language Models](https://doi.org/10.18653/v1/2020.acl-main.756) |  | 0 |  | Boliang Zhang, Ajay Nagesh, Kevin Knight |  |
| 851 |  |  [Regularized Context Gates on Transformer for Machine Translation](https://doi.org/10.18653/v1/2020.acl-main.757) |  | 0 |  | Xintong Li, Lemao Liu, Rui Wang, Guoping Huang, Max Meng |  |
| 852 |  |  [A Multi-Perspective Architecture for Semantic Code Search](https://doi.org/10.18653/v1/2020.acl-main.758) |  | 0 |  | Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, Julia Hockenmaier |  |
| 853 |  |  [Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay Scoring](https://doi.org/10.18653/v1/2020.acl-main.759) |  | 0 |  | Haoran Zhang, Diane J. Litman |  |
| 854 |  |  [Clinical Concept Linking with Contextualized Neural Representations](https://doi.org/10.18653/v1/2020.acl-main.760) |  | 0 |  | Elliot Schumacher, Andriy Mulyar, Mark Dredze |  |
| 855 |  |  [DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking](https://doi.org/10.18653/v1/2020.acl-main.761) |  | 0 |  | Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona T. Diab, Smaranda Muresan |  |
| 856 |  |  [Let Me Choose: From Verbal Context to Font Selection](https://doi.org/10.18653/v1/2020.acl-main.762) |  | 0 |  | Amirreza Shirani, Franck Dernoncourt, Jose Echevarria, Paul Asente, Nedim Lipka, Thamar Solorio |  |
| 857 |  |  [Multi-Label and Multilingual News Framing Analysis](https://doi.org/10.18653/v1/2020.acl-main.763) |  | 0 |  | Afra Feyza Akyürek, Lei Guo, Randa I. Elanwar, Prakash Ishwar, Margrit Betke, Derry Tanti Wijaya |  |
| 858 |  |  [Predicting Performance for Natural Language Processing Tasks](https://doi.org/10.18653/v1/2020.acl-main.764) |  | 0 |  | Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham Neubig |  |
| 859 |  |  [ScriptWriter: Narrative-Guided Script Generation](https://doi.org/10.18653/v1/2020.acl-main.765) |  | 0 |  | Yutao Zhu, Ruihua Song, Zhicheng Dou, JianYun Nie, Jin Zhou |  |
| 860 |  |  [Should All Cross-Lingual Embeddings Speak English?](https://doi.org/10.18653/v1/2020.acl-main.766) |  | 0 |  | Antonios Anastasopoulos, Graham Neubig |  |
| 861 |  |  [Smart To-Do: Automatic Generation of To-Do Items from Emails](https://doi.org/10.18653/v1/2020.acl-main.767) |  | 0 |  | Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White |  |
| 862 |  |  [Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition](https://doi.org/10.18653/v1/2020.acl-main.768) |  | 0 |  | Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams |  |
| 863 |  |  [End-to-End Bias Mitigation by Modelling Biases in Corpora](https://doi.org/10.18653/v1/2020.acl-main.769) |  | 0 |  | Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson |  |
| 864 |  |  [Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance](https://doi.org/10.18653/v1/2020.acl-main.770) |  | 0 |  | Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych |  |
| 865 |  |  [NILE : Natural Language Inference with Faithful Natural Language Explanations](https://doi.org/10.18653/v1/2020.acl-main.771) |  | 0 |  | Sawan Kumar, Partha P. Talukdar |  |
| 866 |  |  [QuASE: Question-Answer Driven Sentence Encoding](https://doi.org/10.18653/v1/2020.acl-main.772) |  | 0 |  | Hangfeng He, Qiang Ning, Dan Roth |  |
| 867 |  |  [Towards Robustifying NLI Models Against Lexical Dataset Biases](https://doi.org/10.18653/v1/2020.acl-main.773) |  | 0 |  | Xiang Zhou, Mohit Bansal |  |
| 868 |  |  [Uncertain Natural Language Inference](https://doi.org/10.18653/v1/2020.acl-main.774) |  | 0 |  | Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, Benjamin Van Durme |  |
| 869 |  |  [Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches](https://doi.org/10.18653/v1/2020.acl-main.775) |  | 0 |  | Tianze Shi, Lillian Lee |  |
| 870 |  |  [Revisiting Higher-Order Dependency Parsers](https://doi.org/10.18653/v1/2020.acl-main.776) |  | 0 |  | Erick Rocha Fonseca, André F. T. Martins |  |
| 871 |  |  [SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling](https://doi.org/10.18653/v1/2020.acl-main.777) |  | 0 |  | Luoxin Chen, Weitong Ruan, Xinyue Liu, Jianhua Lu |  |
| 872 |  |  [Treebank Embedding Vectors for Out-of-domain Dependency Parsing](https://doi.org/10.18653/v1/2020.acl-main.778) |  | 0 |  | Joachim Wagner, James Barry, Jennifer Foster |  |
