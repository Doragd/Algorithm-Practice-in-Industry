# ACL2021

## 会议论文列表

本会议共有 748 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Investigation on Data Adaptation Techniques for Neural Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-srw.1) |  | 0 |  | Evgeniia Tokarchuk, David Thulke, Weiyue Wang, Christian Dugast, Hermann Ney |  |
| 2 |  |  [Stage-wise Fine-tuning for Graph-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-srw.2) |  | 0 |  | Qingyun Wang, Semih Yavuz, Xi Victoria Lin, Heng Ji, Nazneen Fatema Rajani |  |
| 3 |  |  [Transformer-Based Direct Hidden Markov Model for Machine Translation](https://doi.org/10.18653/v1/2021.acl-srw.3) |  | 0 | The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding. | Weiyue Wang, Zijian Yang, Yingbo Gao, Hermann Ney |  |
| 4 |  |  [AutoRC: Improving BERT Based Relation Classification Models via Architecture Search](https://doi.org/10.18653/v1/2021.acl-srw.4) |  | 0 |  | Wei Zhu |  |
| 5 |  |  [How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages](https://doi.org/10.18653/v1/2021.acl-srw.5) |  | 0 |  | Rachit Bansal, Himanshu Choudhary, Ravneet Punia, Niko Schenk, Émilie PagéPerron, Jacob L. Dahl |  |
| 6 |  |  [On the Relationship between Zipf's Law of Abbreviation and Interfering Noise in Emergent Languages](https://doi.org/10.18653/v1/2021.acl-srw.6) |  | 0 | This paper studies whether emergent languages in a signaling game follow Zipf’s law of abbreviation (ZLA), especially when the communication ability of agents is limited because of interfering noises. ZLA is a well-known tendency in human languages where the more frequently a word is used, the shorter it will be. Surprisingly, previous work demonstrated that emergent languages do not obey ZLA at all when neural agents play a signaling game. It also reported that a ZLA-like tendency appeared by adding an explicit penalty on word lengths, which can be considered some external factors in reality such as articulatory effort. We hypothesize, on the other hand, that there might be not only such external factors but also some internal factors related to cognitive abilities. We assume that it could be simulated by modeling the effect of noises on the agents’ environment. In our experimental setup, the hidden states of the LSTM-based speaker and listener were added with Gaussian noise, while the channel was subject to discrete random replacement. Our results suggest that noise on a speaker is one of the factors for ZLA or at least causes emergent languages to approach ZLA, while noise on a listener and a channel is not. | Ryo Ueda, Koki Washio |  |
| 7 |  |  [Long Document Summarization in a Low Resource Setting using Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-srw.7) |  | 0 |  | Ahsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Pradhiksha Ashok Kumar, Rheeya Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer, Rajarshi Das, Andrew McCallum |  |
| 8 |  |  [Attending Self-Attention: A Case Study of Visually Grounded Supervision in Vision-and-Language Transformers](https://doi.org/10.18653/v1/2021.acl-srw.8) |  | 0 | The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the behavior of such models. We compare three different methods on attention supervision and their impact on the performances of a state-of-the-art visually grounded language model on two popular vision-and-language tasks. | Jules Samaran, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima |  |
| 9 |  |  [Video-guided Machine Translation with Spatial Hierarchical Attention Network](https://doi.org/10.18653/v1/2021.acl-srw.9) |  | 0 | Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial and motion representations in videos. For spatial features, we propose a hierarchical attention network to model the spatial information from object-level to video-level. Experiments on the VATEX dataset show that our system achieves 35.86 BLEU-4 score, which is 0.51 score higher than the single model of the SOTA method. | Weiqi Gu, Haiyue Song, Chenhui Chu, Sadao Kurohashi |  |
| 10 |  |  [Stylistic approaches to predicting Reddit popularity in diglossia](https://doi.org/10.18653/v1/2021.acl-srw.10) |  | 0 | Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit’s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (‘Singlish’) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting Reddit post scores in a diglossia. Using data from the Singaporean and British subreddits, I show that while the acrolect’s prestige attracts more upvotes, the most popular posts also draw on Singlish vocabulary to appeal to the community identity. | Huikai Chua |  |
| 11 |  |  ["I've Seen Things You People Wouldn't Believe": Hallucinating Entities in GuessWhat?!](https://doi.org/10.18653/v1/2021.acl-srw.11) |  | 0 | Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image captioning; hence, we adapt to the GuessWhat?! task the best visual processing models at disposal, and propose two new models to play the Questioner agent. We show that the new models generate few hallucinations compared to other renowned models available in the literature. Moreover, their hallucinations are less severe (affect task-accuracy less) and are more human-like. We also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns, cause a cascade hallucination effect, and are often preceded by negative answers, which have been shown to be harder to ground. | Alberto Testoni, Raffaella Bernardi |  |
| 12 |  |  [How do different factors Impact the Inter-language Similarity? A Case Study on Indian languages](https://doi.org/10.18653/v1/2021.acl-srw.12) |  | 0 |  | Sourav Kumar, Salil Aggarwal, Dipti Misra Sharma, Radhika Mamidi |  |
| 13 |  |  [COVID-19 and Misinformation: A Large-Scale Lexical Analysis on Twitter](https://doi.org/10.18653/v1/2021.acl-srw.13) |  | 0 | Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative in tweets related to misinformation than in randomly extracted tweets. In addition to our qualitative analysis, our experimental results show that a simple linear model based only on lexical features is effective in identifying misinformation-related tweets (with accuracy over 80%), providing evidence to the fact that the vocabulary used in misinformation largely differs from generic tweets. | Dimosthenis Antypas, José CamachoCollados, Alun D. Preece, David Rogers |  |
| 14 |  |  [Situation-Based Multiparticipant Chat Summarization: a Concept, an Exploration-Annotation Tool and an Example Collection](https://doi.org/10.18653/v1/2021.acl-srw.14) |  | 0 | Currently, text chatting is one of the primary means of communication. However, modern text chat still in general does not offer any navigation or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing training and gold-standard data. A situation is a subset of messages revolving around a single event in both temporal and contextual senses: e.g, a group of friends arranging a meeting in chat, agreeing on date, time, and place. Situations can be extracted via information retrieval, natural language processing, and machine learning techniques. Since the task is novel, neither training nor gold-standard datasets for it have been created yet. In this paper, we present the formulation of the situation-based summarization problem. Next, we describe Chat Corpora Annotator (CCA): the first annotation system designed specifically for exploring and annotating chat log data. We also introduce a custom query language for semi-automatic situation extraction. Finally, we present the first gold-standard dataset for situation-based summarization. The software source code and the dataset are publicly available. | Anna N. Smirnova, Evgeniy Slobodkin, George A. Chernishev |  |
| 15 |  |  [Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings](https://doi.org/10.18653/v1/2021.acl-srw.15) |  | 0 | In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. We qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model. | Seiichi Inoue, Taichi Aida, Mamoru Komachi, Manabu Asai |  |
| 16 |  |  [Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2](https://doi.org/10.18653/v1/2021.acl-srw.16) |  | 0 | The semantics of a text is manifested not only by what is read but also by what is not read. In this article, we will study how those implicit “not read” information such as end-of-paragraph () and end-of-sequence () affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the in the fine-tuning stage. Experimental results on English story generation show that can lead to higher BLEU scores and lower perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without and during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with . | He Bai, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu, Ming Li |  |
| 17 |  |  [Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings](https://doi.org/10.18653/v1/2021.acl-srw.17) |  | 0 |  | Sosuke Nishikawa, Ryokan Ri, Yoshimasa Tsuruoka |  |
| 18 |  |  [Joint Detection and Coreference Resolution of Entities and Events with Document-level Context Aggregation](https://doi.org/10.18653/v1/2021.acl-srw.18) |  | 0 | Constructing knowledge graphs from unstructured text is an important task that is relevant to many domains. Most previous work focuses on extracting information from sentences or paragraphs, due to the difficulty of analyzing longer contexts. In this paper we propose a new jointly trained model that can be used for various information extraction tasks at the document level. The tasks performed by this system are entity and event identification, typing, and coreference resolution. In order to improve entity and event typing, we utilize context-aware representations aggregated from the detected mentions of the corresponding entities and events across the entire document. By extending our system to document-level, we can improve our results by incorporating cross-sentence dependencies and additional contextual information that might not be available at the sentence level, which allows for more globally optimized predictions. We evaluate our system on documents from the ACE05-E+ dataset and find significant improvement over the sentence-level SOTA on entity and event trigger identification and classification. | Samuel Kriman, Heng Ji |  |
| 19 |  |  ["Hold on honey, men at work": A semi-supervised approach to detecting sexism in sitcoms](https://doi.org/10.18653/v1/2021.acl-srw.19) |  | 0 | Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model to our dataset to analyze the evolution of sexist content over the years. We propose a domain-specific semi-supervised architecture for the aforementioned detection of sexism. Through extensive experiments, we show that our model often yields better classification performance over generic deep learn-ing based sentence classification that does not employ domain-specific training. We find that while sexism decreases over time on average,the proportion of sexist dialogue for the most sexist sitcom actually increases. A quantitative analysis along with a detailed error analysis presents the case for our proposed methodology | Smriti Singh, Tanvi Anand, Arijit Ghosh Chowdhury, Zeerak Waseem |  |
| 20 |  |  [Observing the Learning Curve of NMT Systems With Regard to Linguistic Phenomena](https://doi.org/10.18653/v1/2021.acl-srw.20) |  | 0 |  | Patrick Stadler, Vivien Macketanz, Eleftherios Avramidis |  |
| 21 |  |  [Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation](https://doi.org/10.18653/v1/2021.acl-srw.21) |  | 0 | Question answering (QA) models for reading comprehension have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack robustness to challenge sets, whose distribution is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the distribution of a challenge set is known a priori, making them less applicable to unseen challenge sets. In this study, we focus on question-answer pair generation (QAG) to mitigate this problem. While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy. | Kazutoshi Shinoda, Saku Sugawara, Akiko Aizawa |  |
| 22 |  |  [Tools Impact on the Quality of Annotations for Chat Untangling](https://doi.org/10.18653/v1/2021.acl-srw.22) |  | 0 |  | Jhonny Cerezo, Felipe BravoMarquez, Alexandre Bergel |  |
| 23 |  |  [How Many Layers and Why? An Analysis of the Model Depth in Transformers](https://doi.org/10.18653/v1/2021.acl-srw.23) |  | 0 | In this study, we investigate the role of the multiple layers in deep transformer models. We design a variant of Albert that dynamically adapts the number of layers for each token of the input. The key specificity of Albert is that weights are tied across layers. Therefore, the stack of encoder layers iteratively repeats the application of the same transformation function on the input. We interpret the repetition of this application as an iterative process where the token contextualized representations are progressively refined. We analyze this process at the token level during pre-training, fine-tuning, and inference. We show that tokens do not require the same amount of iterations and that difficult or crucial tokens for the task are subject to more iterations. | Antoine Simoulin, Benoît Crabbé |  |
| 24 |  |  [Edit Distance Based Curriculum Learning for Paraphrase Generation](https://doi.org/10.18653/v1/2021.acl-srw.24) |  | 0 | Curriculum learning has improved the quality of neural machine translation, where only source-side features are considered in the metrics to determine the difficulty of translation. In this study, we apply curriculum learning to paraphrase generation for the first time. Different from machine translation, paraphrase generation allows a certain level of discrepancy in semantics between source and target, which results in diverse transformations from lexical substitution to reordering of clauses. Hence, the difficulty of transformations requires considering both source and target contexts. Experiments on formality transfer using GYAFC showed that our curriculum learning with edit distance improves the quality of paraphrase generation. Additionally, the proposed method improves the quality of difficult samples, which was not possible for previous methods. | Sora Kadotani, Tomoyuki Kajiwara, Yuki Arase, Makoto Onizuka |  |
| 25 |  |  [Changing the Basis of Contextual Representations with Explicit Semantics](https://doi.org/10.18653/v1/2021.acl-srw.25) |  | 0 | The application of transformer-based contextual representations has became a de facto solution for solving complex NLP tasks. Despite their successes, such representations are arguably opaque as their latent dimensions are not directly interpretable. To alleviate this limitation of contextual representations, we devise such an algorithm where the output representation expresses human-interpretable information of each dimension. We achieve this by constructing a transformation matrix based on the semantic content of the embedding space and predefined semantic categories using Hellinger distance. We evaluate our inferred representations on supersense prediction task. Our experiments reveal that the interpretable nature of transformed contextual representations makes it possible to accurately predict the supersense category of a word by simply looking for its transformed coordinate with the largest coefficient. We quantify the effects of our proposed transformation when applied over traditional dense contextual embeddings. We additionally investigate and report consistent improvements for the integration of sparse contextual word representations into our proposed algorithm. | Tamás Ficsor, Gábor Berend |  |
| 26 |  |  [Personal Bias in Prediction of Emotions Elicited by Textual Opinions](https://doi.org/10.18653/v1/2021.acl-srw.26) |  | 0 | Analysis of emotions elicited by opinions, comments, or articles commonly exploits annotated corpora, in which the labels assigned to documents average the views of all annotators, or represent a majority decision. The models trained on such data are effective at identifying the general views of the population. However, their usefulness for predicting the emotions evoked by the textual content in a particular individual is limited. In this paper, we present a study performed on a dataset containing 7,000 opinions, each annotated by about 50 people with two dimensions: valence, arousal, and with intensity of eight emotions from Plutchik’s model. Our study showed that individual responses often significantly differed from the mean. Therefore, we proposed a novel measure to estimate this effect – Personal Emotional Bias (PEB). We also developed a new BERT-based transformer architecture to predict emotions from an individual human perspective. We found PEB a major factor for improving the quality of personalized reasoning. Both the method and measure may boost the quality of content recommendation systems and personalized solutions that protect users from hate speech or unwanted content, which are highly subjective in nature. | Piotr Milkowski, Marcin Gruza, Kamil Kanclerz, Przemyslaw Kazienko, Damian Grimling, Jan Kocon |  |
| 27 |  |  [MVP-BERT: Multi-Vocab Pre-training for Chinese BERT](https://doi.org/10.18653/v1/2021.acl-srw.27) |  | 0 |  | Wei Zhu |  |
| 28 |  |  [CMTA: COVID-19 Misinformation Multilingual Analysis on Twitter](https://doi.org/10.18653/v1/2021.acl-srw.28) |  | 0 | The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and opinion about the pandemic. Evaluation of tweets for recognizing misinformation can create beneficial understanding to review the top quality and also the readability of online information concerning the COVID-19. This paper presents a multilingual COVID-19 related tweet analysis method, CMTA, that uses BERT, a deep learning model for multilingual tweet misinformation detection and classification. CMTA extracts features from multilingual textual data, which is then categorized into specific information classes. Classification is done by a Dense-CNN model trained on tweets manually annotated into information classes (i.e., ‘false’, ‘partly false’, ‘misleading’). The paper presents an analysis of multilingual tweets from February to June, showing the distribution type of information spread across different languages. To access the performance of the CMTA multilingual model, we performed a comparative analysis of 8 monolingual model and CMTA for the misinformation detection task. The results show that our proposed CMTA model has surpassed various monolingual models which consolidated the fact that through transfer learning a multilingual framework could be developed. | Raj Ratn Pranesh, Mehrdad Farokhenajd, Ambesh Shekhar, Genoveva VargasSolar |  |
| 29 |  |  [Predicting pragmatic discourse features in the language of adults with autism spectrum disorder](https://doi.org/10.18653/v1/2021.acl-srw.29) |  | 0 | Individuals with autism spectrum disorder (ASD) experience difficulties in social aspects of communication, but the linguistic characteristics associated with deficits in discourse and pragmatic expression are often difficult to precisely identify and quantify. We are currently collecting a corpus of transcribed natural conversations produced in an experimental setting in which participants with and without ASD complete a number of collaborative tasks with their neurotypical peers. Using this dyadic conversational data, we investigate three pragmatic features – politeness, uncertainty, and informativeness – and present a dataset of utterances annotated for each of these features on a three-point scale. We then introduce ongoing work in developing and training neural models to automatically predict these features, with the goal of identifying the same between-groups differences that are observed using manual annotations. We find the best performing model for all three features is a feed-forward neural network trained with BERT embeddings. Our models yield higher accuracy than ones used in previous approaches for deriving these features, with F1 exceeding 0.82 for all three pragmatic features. | Christine Yang, Duanchen Liu, Qingyun Yang, Zoey Liu, Emily Prud'hommeaux |  |
| 30 |  |  [SumPubMed: Summarization Dataset of PubMed Scientific Articles](https://doi.org/10.18653/v1/2021.acl-srw.30) |  | 0 | Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and informativeness on SumPubMed . SumPubMed is challenging because (a) the summary is distributed throughout the text (not-localized on top), and (b) it contains rare domain-specific scientific terms. We observe that seq2seq models that adequately summarize news articles struggle to summarize SumPubMed . Thus, SumPubMed opens new avenues for the future improvement of models as well as the development of new evaluation metrics. | Vivek Gupta, Prerna Bharti, Pegah Nokhiz, Harish Karnick |  |
| 31 |  |  [A Case Study of Analysis of Construals in Language on Social Media Surrounding a Crisis Event](https://doi.org/10.18653/v1/2021.acl-srw.31) |  | 0 |  | Lolo Aboufoul, Khyati Mahajan, Tiffany Gallicano, Sara Levens, Samira Shaikh |  |
| 32 |  |  [Cross-lingual Evidence Improves Monolingual Fake News Detection](https://doi.org/10.18653/v1/2021.acl-srw.32) |  | 0 |  | Daryna Dementieva, Alexander Panchenko |  |
| 33 |  |  [Neural Machine Translation with Synchronous Latent Phrase Structure](https://doi.org/10.18653/v1/2021.acl-srw.33) |  | 0 |  | Shintaro Harada, Taro Watanabe |  |
| 34 |  |  [Zero Pronouns Identification based on Span prediction](https://doi.org/10.18653/v1/2021.acl-srw.34) |  | 0 | The presence of zero-pronoun (ZP) greatly affects the downstream tasks of NLP in pro-drop languages such as Japanese and Chinese. To tackle the problem, the previous works identified ZPs as sequence labeling on the word sequence or the linearlized tree nodes of the input. We propose a novel approach to ZP identification by casting it as a query-based argument span prediction task. Given a predicate as a query, our model predicts the omission with ZP. In the experiments, our model surpassed the sequence labeling baseline. | Sei Iwata, Taro Watanabe, Masaaki Nagata |  |
| 35 |  |  [On the differences between BERT and MT encoder spaces and how to address them in translation tasks](https://doi.org/10.18653/v1/2021.acl-srw.35) |  | 0 |  | Raúl Vázquez, Hande Çelikkanat, Mathias Creutz, Jörg Tiedemann |  |
| 36 |  |  [Synchronous Syntactic Attention for Transformer Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-srw.36) |  | 0 | This paper proposes a novel attention mechanism for Transformer Neural Machine Translation, “Synchronous Syntactic Attention,” inspired by synchronous dependency grammars. The mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side self-attentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. The experiments show that the proposed method improves the translation performance on WMT14 En-De, WMT16 En-Ro, and ASPEC Ja-En (up to +0.38 points in BLEU). | Hiroyuki Deguchi, Akihiro Tamura, Takashi Ninomiya |  |
| 37 |  |  [Frontmatter](https://aclanthology.org/2021.acl-short.0) |  | 0 |  |  |  |
| 38 |  |  [Catchphrase: Automatic Detection of Cultural References](https://doi.org/10.18653/v1/2021.acl-short.1) |  | 0 |  | Nir Sweed, Dafna Shahaf |  |
| 39 |  |  [On Training Instance Selection for Few-Shot Neural Text Generation](https://doi.org/10.18653/v1/2021.acl-short.2) |  | 0 | Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot neural text generation. The selection decision is made based only on the unlabeled data so as to identify the most worthwhile data points that should be annotated under some budget of labeling cost. Based on the intuition that the few-shot training instances should be diverse and representative of the entire data distribution, we propose a simple selection strategy with K-means clustering. We show that even with the naive clustering-based approach, the generation models consistently outperform random sampling on three text generation tasks: data-to-text generation, document summarization and question generation. The code and training data are made available. We hope that this work will call for more attention on this largely unexplored area. | Ernie Chang, Xiaoyu Shen, HuiSyuan Yeh, Vera Demberg |  |
| 40 |  |  [Coreference Resolution without Span Representations](https://doi.org/10.18653/v1/2021.acl-short.3) |  | 0 |  | Yuval Kirstain, Ori Ram, Omer Levy |  |
| 41 |  |  [Enhancing Entity Boundary Detection for Better Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-short.4) |  | 0 |  | Chun Chen, Fang Kong |  |
| 42 |  |  [Difficulty-Aware Machine Translation Evaluation](https://doi.org/10.18653/v1/2021.acl-short.5) |  | 0 |  | Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao |  |
| 43 |  |  [Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition](https://doi.org/10.18653/v1/2021.acl-short.6) |  | 0 | Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience expectations. With increasingly powerful language models, we were able to feed the set-up along with the punchline into the GPT-2 language model, and calculate the uncertainty and surprisal values of the jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found that these two features have better capabilities of telling jokes from non-jokes, compared with existing baselines. | Yubo Xie, Junze Li, Pearl Pu |  |
| 44 |  |  [Counterfactuals to Control Latent Disentangled Text Representations for Style Transfer](https://doi.org/10.18653/v1/2021.acl-short.7) |  | 0 |  | Sharmila Reddy Nangi, Niyati Chhaya, Sopan Khosla, Nikhil Kaushik, Harshit Nyati |  |
| 45 |  |  [Attention Flows are Shapley Value Explanations](https://doi.org/10.18653/v1/2021.acl-short.8) |  | 0 |  | Kawin Ethayarajh, Dan Jurafsky |  |
| 46 |  |  [Video Paragraph Captioning as a Text Summarization Task](https://doi.org/10.18653/v1/2021.acl-short.9) |  | 0 | Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. Most previous methods simplify this task by using ground-truth event segments. In this work, we propose a novel framework by taking this task as a text summarization task. We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. Our method does not depend on ground-truth event segments. Experiments on two popular datasets ActivityNet Captions and YouCookII demonstrate the advantages of our new framework. On the ActivityNet dataset, our method even outperforms some previous methods using ground-truth event segment labels. | Hui Liu, Xiaojun Wan |  |
| 47 |  |  [Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions](https://doi.org/10.18653/v1/2021.acl-short.10) |  | 0 |  | Daniel Rosenberg, Itai Gat, Amir Feder, Roi Reichart |  |
| 48 |  |  [How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?](https://doi.org/10.18653/v1/2021.acl-short.11) |  | 0 | Existing approaches for the Table-to-Text task suffer from issues such as missing information, hallucination and repetition. Many approaches to this problem use Reinforcement Learning (RL), which maximizes a single manually defined reward, such as BLEU. In this work, we instead pose the Table-to-Text task as Inverse Reinforcement Learning (IRL) problem. We explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function. The composite reward function and the description generator are learned jointly. We find that IRL outperforms strong RL baselines marginally. We further study the generalization of learned IRL rewards in scenarios involving domain adaptation. Our experiments reveal significant challenges in using IRL for this task. | Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, Shashank Srivastava |  |
| 49 |  |  [Automatic Fake News Detection: Are Models Learning to Reason?](https://doi.org/10.18653/v1/2021.acl-short.12) |  | 0 |  | Casper Hansen, Christian Hansen, Lucas Chaves Lima |  |
| 50 |  |  [Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries](https://doi.org/10.18653/v1/2021.acl-short.13) |  | 0 | Despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems, most dialogue systems rely on hybrid approaches which use a combination of rule-based, retrieval and generative approaches for generating a set of ranked responses. Such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system. While, dialogue systems today rely on static and unnatural responses like “I don’t know the answer to that question” or “I’m not sure about that”, we design a neural approach which generates responses which are contextually aware with the user query as well as say no to the user. Such customized responses provide paraphrasing ability and contextualization as well as improve the interaction with the user and reduce dialogue monotonicity. Our simple approach makes use of rules over dependency parses and a text-to-text transformer fine-tuned on synthetic data of question-response pairs generating highly relevant, grammatical as well as diverse questions. We perform automatic and manual evaluations to demonstrate the efficacy of the system. | Ashish Shrivastava, Kaustubh D. Dhole, Abhinav Bhatt, Sharvani Raghunath |  |
| 51 |  |  [N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses](https://doi.org/10.18653/v1/2021.acl-short.14) |  | 0 | Spoken Language Understanding (SLU) systems parse speech into semantic structures like dialog acts and slots. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best ASR alternatives, where each alternative is separated by a special delimiter [SEP]. In our work, we test our hypothesis by using the concatenated N-best ASR alternatives as the input to the transformer encoder models, namely BERT and XLM-RoBERTa, and achieve equivalent performance to the prior state-of-the-art model on DSTC2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this methodology is accessible to users of third-party ASR APIs which do not provide word-lattice information. | Karthik Ganesan, Pakhi Bamdev, Jaivarsan B, Amresh Venugopal, Abhinav Tushar |  |
| 52 |  |  [Gender bias amplification during Speed-Quality optimization in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.15) |  | 0 | Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer. While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate. | Adithya Renduchintala, Denise Díaz, Kenneth Heafield, Xian Li, Mona T. Diab |  |
| 53 |  |  [Machine Translation into Low-resource Language Varieties](https://doi.org/10.18653/v1/2021.acl-short.16) |  | 0 | State-of-the-art machine translation (MT) systems are typically trained to generate “standard” target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but different from, the standard target language, using no parallel (source–variety) data. This also includes adaptation of MT systems to low-resource typologically-related target languages. We experiment with adapting an English–Russian MT system to generate Ukrainian and Belarusian, an English–Norwegian Bokmål system to generate Nynorsk, and an English–Arabic system to generate four Arabic dialects, obtaining significant improvements over competitive baselines. | Sachin Kumar, Antonios Anastasopoulos, Shuly Wintner, Yulia Tsvetkov |  |
| 54 |  |  [Is Sparse Attention more Interpretable?](https://doi.org/10.18653/v1/2021.acl-short.17) |  | 0 | Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists—under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior. | Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell |  |
| 55 |  |  [The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](https://doi.org/10.18653/v1/2021.acl-short.18) |  | 0 | Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters. | Ulme Wennberg, Gustav Eje Henter |  |
| 56 |  |  [Relative Importance in Sentence Processing](https://doi.org/10.18653/v1/2021.acl-short.19) |  | 0 | Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. For human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. In neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. In this work, we compare patterns of relative importance in English language processing by humans and models and analyze the underlying linguistic patterns. We find that human processing patterns in English correlate strongly with saliency-based importance in language models and not with attention-based importance. Our results indicate that saliency could be a cognitively more plausible metric for interpreting neural language models. The code is available on github: https://github.com/beinborn/relative_importance. | Nora Hollenstein, Lisa Beinborn |  |
| 57 |  |  [Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models](https://doi.org/10.18653/v1/2021.acl-short.20) |  | 0 | Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model’s generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect. | Mingyue Han, Yinglin Wang |  |
| 58 |  |  [AND does not mean OR: Using Formal Languages to Study Language Models' Representations](https://doi.org/10.18653/v1/2021.acl-short.21) |  | 0 | A current open question in natural language processing is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between meaning and form, but rather meaning constrains form in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion: Under what conditions should we expect that meaning and form covary sufficiently, such that a language model with access only to form might nonetheless succeed in emulating meaning? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model’s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative: none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit. | Aaron Traylor, Roman Feiman, Ellie Pavlick |  |
| 59 |  |  [Enforcing Consistency in Weakly Supervised Semantic Parsing](https://doi.org/10.18653/v1/2021.acl-short.22) |  | 0 | The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model’s training signal) towards programs that map the same phrase in related inputs to the same sub-parts in their respective programs. Additionally, we study the importance of designing logical formalisms that facilitate this kind of consistency-based training. We find that a more consistent formalism leads to improved model performance even without consistency-based training. When combined together, these two insights lead to a 10% absolute improvement over the best prior result on the Natural Language Visual Reasoning dataset. | Nitish Gupta, Sameer Singh, Matt Gardner |  |
| 60 |  |  [An Improved Model for Voicing Silent Speech](https://doi.org/10.18653/v1/2021.acl-short.23) |  | 0 | In this paper, we present an improved model for voicing silent speech, where audio is synthesized from facial electromyography (EMG) signals. To give our model greater flexibility to learn its own input features, we directly use EMG signals as input in the place of hand-designed features used by prior work. Our model uses convolutional layers to extract features from the signals and Transformer layers to propagate information across longer distances. To provide better signal for learning, we also introduce an auxiliary task of predicting phoneme labels in addition to predicting speech audio features. On an open vocabulary intelligibility evaluation, our model improves the state of the art for this task by an absolute 25.8%. | David Gaddy, Dan Klein |  |
| 61 |  |  [What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus](https://doi.org/10.18653/v1/2021.acl-short.24) |  | 0 | Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis. | Alexandra Sasha Luccioni, Joseph D. Viviano |  |
| 62 |  |  [Continual Quality Estimation with Online Bayesian Meta-Learning](https://doi.org/10.18653/v1/2021.acl-short.25) |  | 0 | Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach. | Abiola Obamuyide, Marina Fomicheva, Lucia Specia |  |
| 63 |  |  [A Span-based Dynamic Local Attention Model for Sequential Sentence Classification](https://doi.org/10.18653/v1/2021.acl-short.26) |  | 0 | Sequential sentence classification aims to classify each sentence in the document based on the context in which sentences appear. Most existing work addresses this problem using a hierarchical sequence labeling network. However, they ignore considering the latent segment structure of the document, in which contiguous sentences often have coherent semantics. In this paper, we proposed a span-based dynamic local attention model that could explicitly capture the structural information by the proposed supervised dynamic local attention. We further introduce an auxiliary task called span-based classification to explore the span-level representations. Extensive experiments show that our model achieves better or competitive performance against state-of-the-art baselines on two benchmark datasets. | Xichen Shang, Qianli Ma, Zhenxi Lin, Jiangyue Yan, Zipeng Chen |  |
| 64 |  |  [How effective is BERT without word ordering? Implications for language understanding and data privacy](https://doi.org/10.18653/v1/2021.acl-short.27) |  | 0 |  | Jack Hessel, Alexandra Schofield |  |
| 65 |  |  [WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation](https://doi.org/10.18653/v1/2021.acl-short.28) |  | 0 | Recent works made significant advances on summarization tasks, facilitated by summarization datasets. Several existing datasets have the form of coherent-paragraph summaries. However, these datasets were curated from academic documents that were written for experts, thus making the essential step of assessing the summarization output through human-evaluation very demanding. To overcome these limitations, we present a dataset based on article summaries appearing on the WikiHow website, composed of how-to articles and coherent-paragraph summaries written in plain language. We compare our dataset attributes to existing ones, including readability and world-knowledge, showing our dataset makes human evaluation significantly easier and thus, more effective. A human evaluation conducted on PubMed and the proposed dataset reinforces our findings. | Nachshon Cohen, Oren Kalinsky, Yftah Ziser, Alessandro Moschitti |  |
| 66 |  |  [UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-short.29) |  | 0 |  | Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin Jung |  |
| 67 |  |  [Anchor-based Bilingual Word Embeddings for Low-Resource Languages](https://doi.org/10.18653/v1/2021.acl-short.30) |  | 0 |  | Tobias Eder, Viktor Hangya, Alexander M. Fraser |  |
| 68 |  |  [Multilingual Agreement for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.31) |  | 0 | Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. | Jian Yang, Yuwei Yin, Shuming Ma, Haoyang Huang, Dongdong Zhang, Zhoujun Li, Furu Wei |  |
| 69 |  |  [Higher-order Derivatives of Weighted Finite-state Machines](https://doi.org/10.18653/v1/2021.acl-short.32) |  | 0 | Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(Aˆ2 Nˆ4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations. | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 70 |  |  [Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards](https://doi.org/10.18653/v1/2021.acl-short.33) |  | 0 | The growth of online consumer health questions has led to the necessity for reliable and accurate question answering systems. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a reinforcement learning-based framework for abstractive question summarization. We propose two novel rewards obtained from the downstream tasks of (i) question-type identification and (ii) question-focus recognition to regularize the question generation model. These rewards ensure the generation of semantically valid questions and encourage the inclusion of key medical entities/foci in the question summary. We evaluated our proposed method on two benchmark datasets and achieved higher performance over state-of-the-art models. The manual evaluation of the summaries reveals that the generated questions are more diverse and have fewer factual inconsistencies than the baseline summaries. The source code is available here: https://github.com/shwetanlp/CHQ-Summ. | Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina DemnerFushman |  |
| 71 |  |  [A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.acl-short.34) |  | 0 | Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing systems use a wide variety of heuristics, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets. These are based on either DBpedia or Wikidata, demonstrating that our approach is effective across KGs. | Tahira Naseem, Srinivas Ravishankar, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, YoungSuk Lee, Pavan Kapanipathi, Salim Roukos, Alfio Gliozzo, Alexander G. Gray |  |
| 72 |  |  [Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation](https://doi.org/10.18653/v1/2021.acl-short.35) |  | 0 | Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR). | Yinfei Yang, Ning Jin, Kuo Lin, Mandy Guo, Daniel Cer |  |
| 73 |  |  [Enhancing Descriptive Image Captioning with Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-short.36) |  | 0 | Generating descriptive sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference captions based on natural language inference. A PageRank algorithm is then employed to estimate the descriptiveness score of each node. Built on that, we use reference sampling and weighted designated rewards to guide captioning to generate descriptive captions. The results on MSCOCO show that the proposed method outperforms the baselines significantly on a wide range of conventional and descriptiveness-related evaluation metrics. | Zhan Shi, Hui Liu, Xiaodan Zhu |  |
| 74 |  |  [MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network](https://doi.org/10.18653/v1/2021.acl-short.37) |  | 0 | We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as “class prototypes” as inference involves retrieving from the full set of labeled entity mentions in the training set and applying the nearest mention neighbor’s entity label. Our model is trained on a large multilingual corpus of mention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor inference on an index of 700 million mentions. It is simpler to train, gives more interpretable predictions, and outperforms all other systems on two multilingual entity linking benchmarks. | Nicholas FitzGerald, Daniel M. Bikel, Jan A. Botha, Daniel Gillick, Tom Kwiatkowski, Andrew McCallum |  |
| 75 |  |  [eMLM: A New Pre-training Objective for Emotion Related Tasks](https://doi.org/10.18653/v1/2021.acl-short.38) |  | 0 | BERT has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model. In this paper, we present Emotion Masked Language Modelling, a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks. Using the same pre-training corpora as the original model, Wikipedia and BookCorpus, our BERT variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2% F-1. Moreover, our approach shows an increased performance in our task-specific robustness tests. | Tiberiu Sosea, Cornelia Caragea |  |
| 76 |  |  [On Positivity Bias in Negative Reviews](https://doi.org/10.18653/v1/2021.acl-short.39) |  | 0 | Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of negation, and show that negations are commonly associated with positive words in negative reviews. Furthermore, in negative reviews, the majority of sentences with positive words express negative opinions based on sentiment classifiers, indicating some form of negation. | Madhusudhan Aithal, Chenhao Tan |  |
| 77 |  |  [PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation](https://doi.org/10.18653/v1/2021.acl-short.40) |  | 0 | Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt PRALon three downstream tasks. The results show that PRAL outperforms or is on par with state-of-the-art models. | Jing Gu, Qingyang Wu, Chongruo Wu, Weiyan Shi, Zhou Yu |  |
| 78 |  |  [ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction](https://doi.org/10.18653/v1/2021.acl-short.41) |  | 0 | Natural reading orders of words are crucial for information extraction from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE generates unique reading order codes for neighboring words relative to the target word given a word-level graph connectivity. We study two fundamental document entity extraction tasks including word labeling and word grouping on the public FUNSD dataset and a large-scale payment dataset. We show that ROPE consistently improves existing GCNs with a margin up to 8.4% F1-score. | ChenYu Lee, ChunLiang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, Siyang Qin, Ashok C. Popat, Tomas Pfister |  |
| 79 |  |  [Zero-shot Event Extraction via Transfer Learning: Challenges and Insights](https://doi.org/10.18653/v1/2021.acl-short.42) |  | 0 | Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. “A city was attacked” entails “There is an attack”), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current QA and TE technologies fail in transferring to a different domain. To investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions. | Qing Lyu, Hongming Zhang, Elior Sulem, Dan Roth |  |
| 80 |  |  [Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models](https://doi.org/10.18653/v1/2021.acl-short.43) |  | 0 | Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found that several pre-trained language models, including BERT, ALBERT, and RoBERTa, show consistent preference to some options, even when these options are irrelevant to the question. When interfered by these irrelevant options, the performance of MRC models can be reduced from human-level performance to the chance-level performance. Human readers, however, are not clearly affected by these irrelevant options. Finally, we propose an augmented training method that can greatly reduce models’ statistical biases. | Jieyu Lin, Jiajie Zou, Nai Ding |  |
| 81 |  |  [Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing](https://doi.org/10.18653/v1/2021.acl-short.44) |  | 0 | Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair wage. Through analysis of worker discussions and guidance for researchers, we estimate that workers spend approximately 2.25 months of full time effort on poorly paid tasks in order to get the qualifications needed for better paid tasks. We discuss alternatives to this qualification and conduct a study of the correlation between qualifications and work quality on two NLP tasks. We find that it is possible to reduce the burden on workers while still collecting high quality data. | Jonathan K. Kummerfeld |  |
| 82 |  |  [Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia](https://doi.org/10.18653/v1/2021.acl-short.45) |  | 0 | Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic information consisting of 7,854 fragments from 10,412 celebrities. Then we detect events with a state-of-the-art event detection model, calibrate the results using strategically generated templates, and extract events that have asymmetric associations with genders. Our study discovers that the Wikipedia pages tend to intermingle personal life events with professional events for females but not for males, which calls for the awareness of the Wikipedia community to formalize guidelines and train the editors to mind the implicit biases that contributors carry. Our work also lays the foundation for future works on quantifying and discovering event biases at the corpus level. | Jiao Sun, Nanyun Peng |  |
| 83 |  |  [Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.46) |  | 0 | Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity. In this paper, we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality. Unlike previous work which feeds the same input to several transformations and merges their outputs into one, we present a Multi-Input-Multi-Output (MIMO) architecture that allows each transformation of the block to have its own input. We also present a task-aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions. Our model surpasses previous work and establishes a new state-of-the-art on the large scale OPUS-100 corpus while being 1.31 times as fast. | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong |  |
| 84 |  |  [Adaptive Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.47) |  | 0 | kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the number of k for each target token. We achieve this by introducing a light-weight Meta-k Network, which can be efficiently trained with only a few training samples. On four benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively filter out the noises in retrieval results and significantly outperforms the vanilla kNN-MT model. Even more noteworthy is that the Meta-k Network learned on one domain could be directly applied to other domains and obtain consistent improvements, illustrating the generality of our method. Our implementation is open-sourced at https://github.com/zhengxxn/adaptive-knn-mt. | Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, Jiajun Chen |  |
| 85 |  |  [On Orthogonality Constraints for Transformers](https://doi.org/10.18653/v1/2021.acl-short.48) |  | 0 | Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. For example, on the large-scale WMT’16 En→De benchmark, simply plugging-and-playing orthogonality constraints on the original transformer model (Vaswani et al., 2017) increases the BLEU from 28.4 to 29.6, coming close to the 29.7 BLEU achieved by the very competitive dynamic convolution (Wu et al., 2019). | Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao Yao, Roy KaWei Lee |  |
| 86 |  |  [Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning](https://doi.org/10.18653/v1/2021.acl-short.49) |  | 0 | Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models. | Piotr Piekos, Mateusz Malinowski, Henryk Michalewski |  |
| 87 |  |  [Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter](https://doi.org/10.18653/v1/2021.acl-short.50) |  | 0 | Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for three new tasks, including induced sentiment prediction and multilabel classification of induced emotions. Our method and dataset open new research opportunities in emotion detection and affective computing. | Boaz Shmueli, Soumya Ray, LunWei Ku |  |
| 88 |  |  [Exploring Listwise Evidence Reasoning with T5 for Fact Verification](https://doi.org/10.18653/v1/2021.acl-short.51) |  | 0 | This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. Experimental results on the FEVER task show that our system attains a FEVER score of 75.87% on the blind test set. This puts our approach atop the competitive FEVER leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score. | Kelvin Jiang, Ronak Pradeep, Jimmy Lin |  |
| 89 |  |  [DefSent: Sentence Embeddings using Definition Sentences](https://doi.org/10.18653/v1/2021.acl-short.52) |  | 0 | Sentence embedding methods using natural language inference (NLI) datasets have been successfully applied to various tasks. However, these methods are only available for limited languages due to relying heavily on the large NLI datasets. In this paper, we propose DefSent, a sentence embedding method that uses definition sentences from a word dictionary, which performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks than conventional methods. Since dictionaries are available for many languages, DefSent is more broadly applicable than methods using NLI datasets without constructing additional datasets. We demonstrate that DefSent performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks to the methods using large NLI datasets. Our code is publicly available at https://github.com/hpprc/defsent. | Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda |  |
| 90 |  |  [Discrete Cosine Transform as Universal Sentence Encoder](https://doi.org/10.18653/v1/2021.acl-short.53) |  | 0 | Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While averaging is the most commonly used efficient sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an alternative that captures the underlying syntactic characteristics of a given text without compromising practical efficiency compared to averaging. However, as with most other sentence encoders, the DCT sentence encoder was only evaluated in English. To this end, we utilize DCT encoder to generate universal sentence representation for different languages such as German, French, Spanish and Russian. The experimental results clearly show the superior effectiveness of DCT encoding in which consistent performance improvements are achieved over strong baselines on multiple standardized datasets | Nada AlMarwani, Mona T. Diab |  |
| 91 |  |  [AligNarr: Aligning Narratives on Movies](https://doi.org/10.18653/v1/2021.acl-short.54) |  | 0 | High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. Experimental results on ten movies show the viability of our method with 76% F1-score and its superiority over a previous baseline. We publish alignments for 914 movies to foster research in this new topic. | Paramita Mirza, Mostafa Abouhamra, Gerhard Weikum |  |
| 92 |  |  [An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](https://doi.org/10.18653/v1/2021.acl-short.55) |  | 0 | Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios. | Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov |  |
| 93 |  |  [Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](https://doi.org/10.18653/v1/2021.acl-short.56) |  | 0 | A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a task-specific pre-training strategy to enhance the model. The generated adversarial examples are gradually added to the training set. Experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple CSC models across three different datasets, achieving state-of-the-art performance for CSC task. | Chong Li, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang |  |
| 94 |  |  [Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints](https://doi.org/10.18653/v1/2021.acl-short.57) |  | 0 | Adaptive Computation (AC) has been shown to be effective in improving the efficiency of Open-Domain Question Answering (ODQA) systems. However, the current AC approaches require tuning of all model parameters, and training state-of-the-art ODQA models requires significant computational resources that may not be available for most researchers. We propose Adaptive Passage Encoder, an AC method that can be applied to an existing ODQA model and can be trained efficiently on a single GPU. It keeps the parameters of the base ODQA model fixed, but it overrides the default layer-by-layer computation of the encoder with an AC policy that is trained to optimise the computational efficiency of the model. Our experimental results show that our method improves upon a state-of-the-art model on two datasets, and is also more accurate than previous AC methods due to the stronger base ODQA model. All source code and datasets are available at https://github.com/uclnlp/APE. | Yuxiang Wu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel |  |
| 95 |  |  [An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter](https://doi.org/10.18653/v1/2021.acl-short.58) |  | 0 | In this paper, we empirically investigate adversarial attack on NMT from two aspects: languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences is more effective than attacking other positions. We further exploit the attention distribution of the victim model to attack source sentences at positions that have a strong association with front target words. Experiment results demonstrate that our attention-based adversarial attack is more effective than adversarial attacks by sampling positions randomly or according to gradients. | Zhiyuan Zeng, Deyi Xiong |  |
| 96 |  |  [OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres](https://doi.org/10.18653/v1/2021.acl-short.59) |  | 0 | SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM, an English corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in GUM, we are able to create the largest human-annotated coreference corpus following the OntoNotes guidelines, and the first to be evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models. | Yilun Zhu, Sameer Pradhan, Amir Zeldes |  |
| 97 |  |  [In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering](https://doi.org/10.18653/v1/2021.acl-short.60) |  | 0 | Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study. | Peter Vickers, Nikolaos Aletras, Emilio Monti, Loïc Barrault |  |
| 98 |  |  [Zero-shot Fact Verification by Claim Generation](https://doi.org/10.18653/v1/2021.acl-short.61) |  | 0 | Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model’s F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available. | Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, William Yang Wang |  |
| 99 |  |  [Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer](https://doi.org/10.18653/v1/2021.acl-short.62) |  | 0 | Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content –the two core aspects of the task– we achieve a new state-of-the-art. | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 100 |  |  [Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-short.63) |  | 0 |  | Shinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam Park, Gaeun Seo, EungGyun Kim, Harksoo Kim |  |
| 101 |  |  [Towards Generative Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-short.64) |  | 0 |  | Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam |  |
| 102 |  |  [Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.65) |  | 0 |  | Yangyifan Xu, Yijin Liu, Fandong Meng, Jiajun Zhang, Jinan Xu, Jie Zhou |  |
| 103 |  |  [Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking](https://doi.org/10.18653/v1/2021.acl-short.66) |  | 0 |  | Binzong Geng, Fajie Yuan, Qiancheng Xu, Ying Shen, Ruifeng Xu, Min Yang |  |
| 104 |  |  [TIMERS: Document-level Temporal Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.67) |  | 0 |  | Puneet Mathur, Rajiv Jain, Franck Dernoncourt, Vlad I. Morariu, Quan Hung Tran, Dinesh Manocha |  |
| 105 |  |  [Improving Arabic Diacritization with Regularized Decoding and Adversarial Training](https://doi.org/10.18653/v1/2021.acl-short.68) |  | 0 |  | Han Qin, Guimin Chen, Yuanhe Tian, Yan Song |  |
| 106 |  |  [When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.69) |  | 0 |  | Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, Jiajun Chen |  |
| 107 |  |  [More than Text: Multi-modal Chinese Word Segmentation](https://doi.org/10.18653/v1/2021.acl-short.70) |  | 0 |  | Dong Zhang, Zheng Hu, Shoushan Li, Hanqian Wu, Qiaoming Zhu, Guodong Zhou |  |
| 108 |  |  [A Mixture-of-Experts Model for Antonym-Synonym Discrimination](https://doi.org/10.18653/v1/2021.acl-short.71) |  | 0 |  | Zhipeng Xie, Nan Zeng |  |
| 109 |  |  [Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking](https://doi.org/10.18653/v1/2021.acl-short.72) |  | 0 |  | Fangyu Liu, Ivan Vulic, Anna Korhonen, Nigel Collier |  |
| 110 |  |  [A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space](https://doi.org/10.18653/v1/2021.acl-short.73) |  | 0 |  | Sara Rajaee, Mohammad Taher Pilehvar |  |
| 111 |  |  [Unsupervised Enrichment of Persona-grounded Dialog with Background Stories](https://doi.org/10.18653/v1/2021.acl-short.74) |  | 0 |  | Bodhisattwa Prasad Majumder, Taylor BergKirkpatrick, Julian J. McAuley, Harsh Jhamtani |  |
| 112 |  |  [Beyond Laurel/Yanny: An Autoencoder-Enabled Search for Polyperceivable Audio](https://doi.org/10.18653/v1/2021.acl-short.75) |  | 0 |  | Kartik Chandra, Chuma Kabaghe, Gregory Valiant |  |
| 113 |  |  [Don't Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models](https://doi.org/10.18653/v1/2021.acl-short.76) |  | 0 |  | Mahnaz Koupaee, Greg Durrett, Nathanael Chambers, Niranjan Balasubramanian |  |
| 114 |  |  [The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes](https://doi.org/10.18653/v1/2021.acl-short.77) |  | 0 |  | Nils Reimers, Iryna Gurevych |  |
| 115 |  |  [Cross-lingual Text Classification with Heterogeneous Graph Neural Network](https://doi.org/10.18653/v1/2021.acl-short.78) |  | 0 |  | Ziyun Wang, Xuan Liu, Peiji Yang, Shixing Liu, Zhisheng Wang |  |
| 116 |  |  [Towards more equitable question answering systems: How much more data do you need?](https://doi.org/10.18653/v1/2021.acl-short.79) |  | 0 |  | Arnab Debnath, Navid Rajabi, Fardina Fathmiul Alam, Antonios Anastasopoulos |  |
| 117 |  |  [Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event](https://doi.org/10.18653/v1/2021.acl-short.80) |  | 0 |  | Nazanin Dehghani, Hassan Hajipoor, Hadi Amiri |  |
| 118 |  |  [Improving Compositional Generalization in Classification Tasks via Structure Annotations](https://doi.org/10.18653/v1/2021.acl-short.81) |  | 0 |  | Juyong Kim, Pradeep Ravikumar, Joshua Ainslie, Santiago Ontañón |  |
| 119 |  |  [Learning to Generate Task-Specific Adapters from Task Description](https://doi.org/10.18653/v1/2021.acl-short.82) |  | 0 |  | Qinyuan Ye, Xiang Ren |  |
| 120 |  |  [QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining](https://doi.org/10.18653/v1/2021.acl-short.83) |  | 0 |  | Xinya Du, Luheng He, Qi Li, Dian Yu, Panupong Pasupat, Yuan Zhang |  |
| 121 |  |  [Domain-Adaptive Pretraining Methods for Dialogue Understanding](https://doi.org/10.18653/v1/2021.acl-short.84) |  | 0 |  | Han Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong Zhang, Linqi Song |  |
| 122 |  |  [Targeting the Benchmark: On Methodology in Current Natural Language Processing Research](https://doi.org/10.18653/v1/2021.acl-short.85) |  | 0 |  | David Schlangen |  |
| 123 |  |  [X-Fact: A New Benchmark Dataset for Multilingual Fact Checking](https://doi.org/10.18653/v1/2021.acl-short.86) |  | 0 |  | Ashim Gupta, Vivek Srikumar |  |
| 124 |  |  [nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?](https://doi.org/10.18653/v1/2021.acl-short.87) |  | 0 |  | Mihir Kale, Aditya Siddhant, Rami AlRfou, Linting Xue, Noah Constant, Melvin Johnson |  |
| 125 |  |  [Question Generation for Adaptive Education](https://doi.org/10.18653/v1/2021.acl-short.88) |  | 0 |  | Megha Srivastava, Noah D. Goodman |  |
| 126 |  |  [A Simple Recipe for Multilingual Grammatical Error Correction](https://doi.org/10.18653/v1/2021.acl-short.89) |  | 0 |  | Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn |  |
| 127 |  |  [Towards Visual Question Answering on Pathology Images](https://doi.org/10.18653/v1/2021.acl-short.90) |  | 0 |  | Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric P. Xing, Pengtao Xie |  |
| 128 |  |  [Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations](https://doi.org/10.18653/v1/2021.acl-short.91) |  | 0 |  | Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell |  |
| 129 |  |  [mTVR: Multilingual Moment Retrieval in Videos](https://doi.org/10.18653/v1/2021.acl-short.92) |  | 0 |  | Jie Lei, Tamara L. Berg, Mohit Bansal |  |
| 130 |  |  [Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-short.93) |  | 0 |  | Pei Chen, Haibo Ding, Jun Araki, Ruihong Huang |  |
| 131 |  |  [Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](https://doi.org/10.18653/v1/2021.acl-short.94) |  | 0 |  | Gyubok Lee, Seongjun Yang, Edward Choi |  |
| 132 |  |  [Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations](https://doi.org/10.18653/v1/2021.acl-short.95) |  | 0 |  | Lingzhi Wang, Xingshan Zeng, KamFai Wong |  |
| 133 |  |  [Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence](https://doi.org/10.18653/v1/2021.acl-short.96) |  | 0 |  | Federico Bianchi, Silvia Terragni, Dirk Hovy |  |
| 134 |  |  [Input Representations for Parsing Discourse Representation Structures: Comparing English with Chinese](https://doi.org/10.18653/v1/2021.acl-short.97) |  | 0 |  | Chunliu Wang, Rik van Noord, Arianna Bisazza, Johan Bos |  |
| 135 |  |  [Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data](https://doi.org/10.18653/v1/2021.acl-short.98) |  | 0 |  | Sajad Norouzi, Keyi Tang, Yanshuai Cao |  |
| 136 |  |  [Issues with Entailment-based Zero-shot Text Classification](https://doi.org/10.18653/v1/2021.acl-short.99) |  | 0 |  | Tingting Ma, JinGe Yao, ChinYew Lin, Tiejun Zhao |  |
| 137 |  |  [Neural-Symbolic Commonsense Reasoner with Relation Predictors](https://doi.org/10.18653/v1/2021.acl-short.100) |  | 0 |  | Farhad Moghimifar, Lizhen Qu, Yue Zhuo, Gholamreza Haffari, Mahsa Baktashmotlagh |  |
| 138 |  |  [What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts](https://doi.org/10.18653/v1/2021.acl-short.101) |  | 0 |  | Sanja Stajner, Seren Yenikent, Bilal Ghanem, Marc FrancoSalvador |  |
| 139 |  |  [Semantic Frame Induction using Masked Word Embeddings and Two-Step Clustering](https://doi.org/10.18653/v1/2021.acl-short.102) |  | 0 |  | Kosuke Yamada, Ryohei Sasano, Koichi Takeda |  |
| 140 |  |  [Lightweight Adapter Tuning for Multilingual Speech Translation](https://doi.org/10.18653/v1/2021.acl-short.103) |  | 0 |  | Hang Le, Juan Miguel Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent Besacier |  |
| 141 |  |  [Parameter Selection: Why We Should Pay More Attention to It](https://doi.org/10.18653/v1/2021.acl-short.104) |  | 0 |  | JieJyun Liu, TsungHan Yang, SiAn Chen, ChihJen Lin |  |
| 142 |  |  [Distinct Label Representations for Few-Shot Text Classification](https://doi.org/10.18653/v1/2021.acl-short.105) |  | 0 |  | Sora Ohashi, Junya Takayama, Tomoyuki Kajiwara, Yuki Arase |  |
| 143 |  |  [Learning to Solve NLP Tasks in an Incremental Number of Languages](https://doi.org/10.18653/v1/2021.acl-short.106) |  | 0 |  | Giuseppe Castellucci, Simone Filice, Danilo Croce, Roberto Basili |  |
| 144 |  |  [Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](https://doi.org/10.18653/v1/2021.acl-short.107) |  | 0 |  | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 145 |  |  [Robust Transfer Learning with Pretrained Language Models through Adapters](https://doi.org/10.18653/v1/2021.acl-short.108) |  | 0 |  | Wenjuan Han, Bo Pang, Ying Nian Wu |  |
| 146 |  |  [Embracing Ambiguity: Shifting the Training Target of NLI Models](https://doi.org/10.18653/v1/2021.acl-short.109) |  | 0 |  | Johannes Mario Meissner, Napat Thumwanit, Saku Sugawara, Akiko Aizawa |  |
| 147 |  |  [Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-short.110) |  | 0 |  | Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, Weiran Xu |  |
| 148 |  |  [Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-short.111) |  | 0 |  | Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu |  |
| 149 |  |  [On the Generation of Medical Dialogs for COVID-19](https://doi.org/10.18653/v1/2021.acl-short.112) |  | 0 |  | Meng Zhou, Zechen Li, Bowen Tan, Guangtao Zeng, Wenmian Yang, Xuehai He, Zeqian Ju, Subrato Chakravorty, Shu Chen, Xingyi Yang, Yichen Zhang, Qingyang Wu, Zhou Yu, Kun Xu, Eric P. Xing, Pengtao Xie |  |
| 150 |  |  [Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images](https://doi.org/10.18653/v1/2021.acl-short.113) |  | 0 |  | Nyoungwoo Lee, Suwon Shin, Jaegul Choo, HoJin Choi, SungHyon Myaeng |  |
| 151 |  |  [Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection](https://doi.org/10.18653/v1/2021.acl-short.114) |  | 0 |  | Debora Nozza |  |
| 152 |  |  [BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://doi.org/10.18653/v1/2021.acl-short.115) |  | 0 |  | Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi |  |
| 153 |  |  [Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain](https://doi.org/10.18653/v1/2021.acl-short.116) |  | 0 |  | Wei Shi, Vera Demberg |  |
| 154 |  |  [Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction](https://doi.org/10.18653/v1/2021.acl-short.117) |  | 0 |  | Ming Shen, Pratyay Banerjee, Chitta Baral |  |
| 155 |  |  [Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction](https://doi.org/10.18653/v1/2021.acl-short.118) |  | 0 |  | Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang |  |
| 156 |  |  [Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning](https://doi.org/10.18653/v1/2021.acl-short.119) |  | 0 |  | Linzi Xing, Wen Xiao, Giuseppe Carenini |  |
| 157 |  |  [DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications](https://doi.org/10.18653/v1/2021.acl-short.120) |  | 0 |  | Hongxuan Tang, Hongyu Li, Jing Liu, Yu Hong, Hua Wu, Haifeng Wang |  |
| 158 |  |  [Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving](https://doi.org/10.18653/v1/2021.acl-short.121) |  | 0 |  | Shihhung Tsai, ChaoChun Liang, HsinMin Wang, KehYih Su |  |
| 159 |  |  [Multi-Scale Progressive Attention Network for Video Question Answering](https://doi.org/10.18653/v1/2021.acl-short.122) |  | 0 |  | Zhicheng Guo, Jiaxuan Zhao, Licheng Jiao, Xu Liu, Lingling Li |  |
| 160 |  |  [Efficient Passage Retrieval with Hashing for Open-domain Question Answering](https://doi.org/10.18653/v1/2021.acl-short.123) |  | 0 |  | Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi |  |
| 161 |  |  [Entity Concept-enhanced Few-shot Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.124) |  | 0 |  | Shan Yang, Yongfei Zhang, Guanglin Niu, Qinghua Zhao, Shiliang Pu |  |
| 162 |  |  [Improving Model Generalization: A Chinese Named Entity Recognition Case Study](https://doi.org/10.18653/v1/2021.acl-short.125) |  | 0 |  | Guanqing Liang, Cane WingKi Leung |  |
| 163 |  |  [Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.126) |  | 0 |  | Quzhe Huang, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, Dongyan Zhao |  |
| 164 |  |  [Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders](https://doi.org/10.18653/v1/2021.acl-short.127) |  | 0 |  | Irene Li, Vanessa Yan, Tianxiao Li, Rihao Qu, Dragomir R. Radev |  |
| 165 |  |  [Attentive Multiview Text Representation for Differential Diagnosis](https://doi.org/10.18653/v1/2021.acl-short.128) |  | 0 |  | Hadi Amiri, Mitra Mohtarami, Isaac S. Kohane |  |
| 166 |  |  [MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain](https://doi.org/10.18653/v1/2021.acl-short.129) |  | 0 |  | Christine Herlihy, Rachel Rudinger |  |
| 167 |  |  [Towards a more Robust Evaluation for Conversational Question Answering](https://doi.org/10.18653/v1/2021.acl-short.130) |  | 0 |  | Wissam Siblini, Baris Sayil, Yacine Kessaci |  |
| 168 |  |  [VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.acl-short.131) |  | 0 |  | Haoyang Wen, Anthony Ferritto, Heng Ji, Radu Florian, Avi Sil |  |
| 169 |  |  [Avoiding Overlap in Data Augmentation for AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-short.132) |  | 0 |  | Wenchao Du, Jeffrey Flanigan |  |
| 170 |  |  [Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains](https://doi.org/10.18653/v1/2021.acl-short.133) |  | 0 |  | Chenghao Yang, Yudong Zhang, Smaranda Muresan |  |
| 171 |  |  [Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test](https://doi.org/10.18653/v1/2021.acl-short.134) |  | 0 |  | Philippe Laban, Luke Dai, Lucas Bandarkar, Marti A. Hearst |  |
| 172 |  |  [SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization](https://doi.org/10.18653/v1/2021.acl-short.135) |  | 0 |  | Yixin Liu, Pengfei Liu |  |
| 173 |  |  [SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles](https://doi.org/10.18653/v1/2021.acl-short.136) |  | 0 |  | AnaCristina Rogoz, Mihaela Gaman, Radu Tudor Ionescu |  |
| 174 |  |  [Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents](https://doi.org/10.18653/v1/2021.acl-short.137) |  | 0 |  | Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He |  |
| 175 |  |  [Replicating and Extending "Because Their Treebanks Leak": Graph Isomorphism, Covariants, and Parser Performance](https://doi.org/10.18653/v1/2021.acl-short.138) |  | 0 |  | Mark Anderson, Anders Søgaard, Carlos GómezRodríguez |  |
| 176 |  |  [Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data](https://doi.org/10.18653/v1/2021.acl-short.139) |  | 0 |  | Rajat Bhatnagar, Ananya Ganesh, Katharina Kann |  |
| 177 |  |  [Frontmatter](https://aclanthology.org/2021.acl-long.0) |  | 0 |  |  |  |
| 178 |  |  [Investigating label suggestions for opinion mining in German Covid-19 social media](https://doi.org/10.18653/v1/2021.acl-long.1) |  | 0 |  | Tilman Beck, JiUng Lee, Christina Viehmann, Marcus Maurer, Oliver Quiring, Iryna Gurevych |  |
| 179 |  |  [How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements](https://doi.org/10.18653/v1/2021.acl-long.2) |  | 0 |  | Chen Shani, Nadav Borenstein, Dafna Shahaf |  |
| 180 |  |  [Engage the Public: Poll Question Generation for Social Media Posts](https://doi.org/10.18653/v1/2021.acl-long.3) |  | 0 |  | Zexin Lu, Keyang Ding, Yuji Zhang, Jing Li, Baolin Peng, Lemao Liu |  |
| 181 |  |  [HateCheck: Functional Tests for Hate Speech Detection Models](https://doi.org/10.18653/v1/2021.acl-long.4) |  | 0 |  | Paul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Z. Margetts, Janet B. Pierrehumbert |  |
| 182 |  |  [Unified Dual-view Cognitive Model for Interpretable Claim Verification](https://doi.org/10.18653/v1/2021.acl-long.5) |  | 0 |  | Lianwei Wu, Yuan Rao, Yuqian Lan, Ling Sun, Zhaoyin Qi |  |
| 183 |  |  [DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](https://doi.org/10.18653/v1/2021.acl-long.6) |  | 0 |  | Lanqing Xue, Kaitao Song, Duocai Wu, Xu Tan, Nevin L. Zhang, Tao Qin, WeiQiang Zhang, TieYan Liu |  |
| 184 |  |  [PENS: A Dataset and Generic Framework for Personalized News Headline Generation](https://doi.org/10.18653/v1/2021.acl-long.7) |  | 0 |  | Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, Xing Xie |  |
| 185 |  |  [Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization](https://doi.org/10.18653/v1/2021.acl-long.8) |  | 0 |  | Dongkyu Lee, Zhiliang Tian, Lanqing Xue, Nevin L. Zhang |  |
| 186 |  |  [Mention Flags (MF): Constraining Transformer-based Text Generators](https://doi.org/10.18653/v1/2021.acl-long.9) |  | 0 |  | Yufei Wang, Ian D. Wood, Stephen Wan, Mark Dras, Mark Johnson |  |
| 187 |  |  [Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation](https://doi.org/10.18653/v1/2021.acl-long.10) |  | 0 |  | Giulio Zhou, Gerasimos Lampouras |  |
| 188 |  |  [Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances](https://doi.org/10.18653/v1/2021.acl-long.11) |  | 0 |  | Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, Jie Zhou |  |
| 189 |  |  [Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-long.12) |  | 0 |  | Jinyu Guo, Kai Shuang, Jijie Li, Zihan Wang |  |
| 190 |  |  [Transferable Dialogue Systems and User Simulators](https://doi.org/10.18653/v1/2021.acl-long.13) |  | 0 |  | BoHsiang Tseng, Yinpei Dai, Florian Kreyssig, Bill Byrne |  |
| 191 |  |  [BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data](https://doi.org/10.18653/v1/2021.acl-long.14) |  | 0 |  | Haoyu Song, Yan Wang, Kaiyan Zhang, WeiNan Zhang, Ting Liu |  |
| 192 |  |  [GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2021.acl-long.15) |  | 0 |  | Libo Qin, Fuxuan Wei, Tianbao Xie, Xiao Xu, Wanxiang Che, Ting Liu |  |
| 193 |  |  [Accelerating BERT Inference for Sequence Labeling via Early-Exit](https://doi.org/10.18653/v1/2021.acl-long.16) |  | 0 |  | Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, Xuanjing Huang |  |
| 194 |  |  [Modularized Interaction Network for Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.17) |  | 0 |  | Fei Li, Zheng Wang, Siu Cheung Hui, Lejian Liao, Dandan Song, Jing Xu, Guoxiu He, Meihuizi Jia |  |
| 195 |  |  [Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder](https://doi.org/10.18653/v1/2021.acl-long.18) |  | 0 |  | Xiangyu Xi, Wei Ye, Shikun Zhang, Quanxiu Wang, Huixing Jiang, Wei Wu |  |
| 196 |  |  [UniRE: A Unified Label Space for Entity Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.19) |  | 0 |  | Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, Junchi Yan |  |
| 197 |  |  [Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.20) |  | 0 |  | Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang Cheng, Jingjie Yi, Yanghua Xiao |  |
| 198 |  |  [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.21) |  | 0 |  | Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li |  |
| 199 |  |  [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.22) |  | 0 |  | Mathias Müller, Rico Sennrich |  |
| 200 |  |  [Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.23) |  | 0 |  | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Meng Zhang |  |
| 201 |  |  [A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.24) |  | 0 |  | Jingyi Zhang, Josef van Genabith |  |
| 202 |  |  [Learning Language Specific Sub-network for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.25) |  | 0 |  | Zehui Lin, Liwei Wu, Mingxuan Wang, Lei Li |  |
| 203 |  |  [Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.26) |  | 0 |  | Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, Ruihai Dong |  |
| 204 |  |  [Bridge-Based Active Domain Adaptation for Aspect Term Extraction](https://doi.org/10.18653/v1/2021.acl-long.27) |  | 0 |  | Zhuang Chen, Tieyun Qian |  |
| 205 |  |  [Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks](https://doi.org/10.18653/v1/2021.acl-long.28) |  | 0 |  | Xiaocui Yang, Shi Feng, Yifei Zhang, Daling Wang |  |
| 206 |  |  [Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions](https://doi.org/10.18653/v1/2021.acl-long.29) |  | 0 |  | Hongjie Cai, Rui Xia, Jianfei Yu |  |
| 207 |  |  [PASS: Perturb-and-Select Summarizer for Product Reviews](https://doi.org/10.18653/v1/2021.acl-long.30) |  | 0 |  | Nadav Oved, Ran Levy |  |
| 208 |  |  [Deep Differential Amplifier for Extractive Summarization](https://doi.org/10.18653/v1/2021.acl-long.31) |  | 0 |  | Ruipeng Jia, Yanan Cao, Fang Fang, Yuchen Zhou, Zheng Fang, Yanbing Liu, Shi Wang |  |
| 209 |  |  [Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries](https://doi.org/10.18653/v1/2021.acl-long.32) |  | 0 |  | Yi Yu, Adam Jatowt, Antoine Doucet, Kazunari Sugiyama, Masatoshi Yoshikawa |  |
| 210 |  |  [Self-Supervised Multimodal Opinion Summarization](https://doi.org/10.18653/v1/2021.acl-long.33) |  | 0 |  | Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung |  |
| 211 |  |  [A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy](https://doi.org/10.18653/v1/2021.acl-long.34) |  | 0 |  | Wang Chen, Piji Li, Irwin King |  |
| 212 |  |  [DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions](https://doi.org/10.18653/v1/2021.acl-long.35) |  | 0 |  | Weijia Shi, Mandar Joshi, Luke Zettlemoyer |  |
| 213 |  |  [Introducing Orthogonal Constraint in Structural Probes](https://doi.org/10.18653/v1/2021.acl-long.36) |  | 0 |  | Tomasz Limisiewicz, David Marecek |  |
| 214 |  |  [Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger](https://doi.org/10.18653/v1/2021.acl-long.37) |  | 0 |  | Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun |  |
| 215 |  |  [Examining the Inductive Bias of Neural Language Models with Artificial Languages](https://doi.org/10.18653/v1/2021.acl-long.38) |  | 0 |  | Jennifer C. White, Ryan Cotterell |  |
| 216 |  |  [Explaining Contextualization in Language Models using Visual Analytics](https://doi.org/10.18653/v1/2021.acl-long.39) |  | 0 |  | Rita Sevastjanova, AikateriniLida Kalouli, Christin Beck, Hanna Schäfer, Mennatallah ElAssady |  |
| 217 |  |  [Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.40) |  | 0 |  | George Chrysostomou, Nikolaos Aletras |  |
| 218 |  |  [Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem](https://doi.org/10.18653/v1/2021.acl-long.41) |  | 0 |  | Raphael Schumann, Stefan Riezler |  |
| 219 |  |  [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://doi.org/10.18653/v1/2021.acl-long.42) |  | 0 |  | Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, Fei Huang |  |
| 220 |  |  [Learning Relation Alignment for Calibrated Cross-modal Retrieval](https://doi.org/10.18653/v1/2021.acl-long.43) |  | 0 |  | Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang |  |
| 221 |  |  [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://doi.org/10.18653/v1/2021.acl-long.44) |  | 0 |  | Yiran Xing, Zai Shi, Zhao Meng, Gerhard Lakemeyer, Yunpu Ma, Roger Wattenhofer |  |
| 222 |  |  [Cascaded Head-colliding Attention](https://doi.org/10.18653/v1/2021.acl-long.45) |  | 0 |  | Lin Zheng, Zhiyong Wu, Lingpeng Kong |  |
| 223 |  |  [Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor](https://doi.org/10.18653/v1/2021.acl-long.46) |  | 0 |  | Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 224 |  |  [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://doi.org/10.18653/v1/2021.acl-long.47) |  | 0 |  | Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James Henderson |  |
| 225 |  |  [COSY: COunterfactual SYntax for Cross-Lingual Understanding](https://doi.org/10.18653/v1/2021.acl-long.48) |  | 0 |  | Sicheng Yu, Hao Zhang, Yulei Niu, Qianru Sun, Jing Jiang |  |
| 226 |  |  [OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.49) |  | 0 |  | Seonghyeon Lee, Dongha Lee, Hwanjo Yu |  |
| 227 |  |  [Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model](https://doi.org/10.18653/v1/2021.acl-long.50) |  | 0 |  | Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko |  |
| 228 |  |  [Structurizing Misinformation Stories via Rationalizing Fact-Checks](https://doi.org/10.18653/v1/2021.acl-long.51) |  | 0 |  | Shan Jiang, Christo Wilson |  |
| 229 |  |  [Modeling Language Usage and Listener Engagement in Podcasts](https://doi.org/10.18653/v1/2021.acl-long.52) |  | 0 |  | Sravana Reddy, Mariya Lazarova, Yongze Yu, Rosie Jones |  |
| 230 |  |  [Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions](https://doi.org/10.18653/v1/2021.acl-long.53) |  | 0 |  | Saumya Sahai, Oana Balalau, Roxana Horincar |  |
| 231 |  |  [SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues](https://doi.org/10.18653/v1/2021.acl-long.54) |  | 0 |  | Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying Nian Wu, SongChun Zhu |  |
| 232 |  |  [TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems](https://doi.org/10.18653/v1/2021.acl-long.55) |  | 0 |  | Bill Byrne, Karthik Krishnamoorthi, Saravanan Ganesh, Mihir Sanjay Kale |  |
| 233 |  |  [Improving Dialog Systems for Negotiation with Personality Modeling](https://doi.org/10.18653/v1/2021.acl-long.56) |  | 0 |  | Runzhe Yang, Jingxiao Chen, Karthik Narasimhan |  |
| 234 |  |  [Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training](https://doi.org/10.18653/v1/2021.acl-long.57) |  | 0 |  | Wangchunshu Zhou, Qifei Li, Chenle Li |  |
| 235 |  |  [Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features](https://doi.org/10.18653/v1/2021.acl-long.58) |  | 0 |  | Hannah Rashkin, David Reitter, Gaurav Singh Tomar, Dipanjan Das |  |
| 236 |  |  [CitationIE: Leveraging the Citation Graph for Scientific Information Extraction](https://doi.org/10.18653/v1/2021.acl-long.59) |  | 0 |  | Vijay Viswanathan, Graham Neubig, Pengfei Liu |  |
| 237 |  |  [From Discourse to Narrative: Knowledge Projection for Event Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.60) |  | 0 |  | Jialong Tang, Hongyu Lin, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun, Weijian Xie, Jin Xu |  |
| 238 |  |  [AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER](https://doi.org/10.18653/v1/2021.acl-long.61) |  | 0 |  | Weile Chen, Huiqiang Jiang, Qianhui Wu, Börje Karlsson, Yi Guan |  |
| 239 |  |  [Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge](https://doi.org/10.18653/v1/2021.acl-long.62) |  | 0 |  | Linmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu Tang, Chuan Shi, Nan Duan, Ming Zhou |  |
| 240 |  |  [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://doi.org/10.18653/v1/2021.acl-long.63) |  | 0 |  | Yucheng Wang, Bowen Yu, Hongsong Zhu, Tingwen Liu, Nan Yu, Limin Sun |  |
| 241 |  |  [LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking](https://doi.org/10.18653/v1/2021.acl-long.64) |  | 0 |  | Hang Jiang, Sairam Gurajada, Qiuhao Lu, Sumit Neelam, Lucian Popa, Prithviraj Sen, Yunyao Li, Alexander G. Gray |  |
| 242 |  |  [Do Context-Aware Translation Models Pay the Right Attention?](https://doi.org/10.18653/v1/2021.acl-long.65) |  | 0 |  | Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig |  |
| 243 |  |  [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://doi.org/10.18653/v1/2021.acl-long.66) |  | 0 |  | WeiJen Ko, Ahmed ElKishky, Adithya Renduchintala, Vishrav Chaudhary, Naman Goyal, Francisco Guzmán, Pascale Fung, Philipp Koehn, Mona T. Diab |  |
| 244 |  |  [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.67) |  | 0 |  | Haoyue Shi, Luke Zettlemoyer, Sida I. Wang |  |
| 245 |  |  [Multilingual Speech Translation from Efficient Finetuning of Pretrained Models](https://doi.org/10.18653/v1/2021.acl-long.68) |  | 0 |  | Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Miguel Pino, Alexei Baevski, Alexis Conneau, Michael Auli |  |
| 246 |  |  [Learning Faithful Representations of Causal Graphs](https://doi.org/10.18653/v1/2021.acl-long.69) |  | 0 |  | Ananth Balashankar, Lakshminarayanan Subramanian |  |
| 247 |  |  [What Context Features Can Transformer Language Models Use?](https://doi.org/10.18653/v1/2021.acl-long.70) |  | 0 |  | Joe O'Connor, Jacob Andreas |  |
| 248 |  |  [Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models](https://doi.org/10.18653/v1/2021.acl-long.71) |  | 0 |  | Sandipan Sikdar, Parantapa Bhattacharya, Kieran Heese |  |
| 249 |  |  [DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations](https://doi.org/10.18653/v1/2021.acl-long.72) |  | 0 |  | John M. Giorgi, Osvald Nitski, Bo Wang, Gary D. Bader |  |
| 250 |  |  [XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation](https://doi.org/10.18653/v1/2021.acl-long.73) |  | 0 |  | Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, Guodong Zhou |  |
| 251 |  |  [Span-based Semantic Parsing for Compositional Generalization](https://doi.org/10.18653/v1/2021.acl-long.74) |  | 0 |  | Jonathan Herzig, Jonathan Berant |  |
| 252 |  |  [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](https://doi.org/10.18653/v1/2021.acl-long.75) |  | 0 |  | Peter Shaw, MingWei Chang, Panupong Pasupat, Kristina Toutanova |  |
| 253 |  |  [A Targeted Assessment of Incremental Processing in Neural Language Models and Humans](https://doi.org/10.18653/v1/2021.acl-long.76) |  | 0 |  | Ethan Wilcox, Pranali Vani, Roger Levy |  |
| 254 |  |  [The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing](https://doi.org/10.18653/v1/2021.acl-long.77) |  | 0 |  | Valentina Pyatkin, Shoval Sadde, Aynat Rubinstein, Paul Portner, Reut Tsarfaty |  |
| 255 |  |  [To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings](https://doi.org/10.18653/v1/2021.acl-long.78) |  | 0 |  | Sarah R. Moeller, Ling Liu, Mans Hulden |  |
| 256 |  |  [Prosodic segmentation for parsing spoken dialogue](https://doi.org/10.18653/v1/2021.acl-long.79) |  | 0 |  | Elizabeth Nielsen, Mark Steedman, Sharon Goldwater |  |
| 257 |  |  [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://doi.org/10.18653/v1/2021.acl-long.80) |  | 0 |  | Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, Emmanuel Dupoux |  |
| 258 |  |  [Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets](https://doi.org/10.18653/v1/2021.acl-long.81) |  | 0 |  | Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, Hanna M. Wallach |  |
| 259 |  |  [Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network](https://doi.org/10.18653/v1/2021.acl-long.82) |  | 0 |  | Justin Lovelace, Denis NewmanGriffis, Shikhar Vashishth, Jill Fain Lehman, Carolyn P. Rosé |  |
| 260 |  |  [A DQN-based Approach to Finding Precise Evidences for Fact Verification](https://doi.org/10.18653/v1/2021.acl-long.83) |  | 0 |  | Hai Wan, Haicheng Chen, Jianfeng Du, Weilin Luo, Rongzhen Ye |  |
| 261 |  |  [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://doi.org/10.18653/v1/2021.acl-long.84) |  | 0 |  | Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin |  |
| 262 |  |  [Unsupervised Out-of-Domain Detection via Pre-trained Transformers](https://doi.org/10.18653/v1/2021.acl-long.85) |  | 0 |  | Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, Caiming Xiong |  |
| 263 |  |  [MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation](https://doi.org/10.18653/v1/2021.acl-long.86) |  | 0 |  | Ahmad Rashid, Vasileios Lioutas, Mehdi Rezagholizadeh |  |
| 264 |  |  [Selecting Informative Contexts Improves Language Model Fine-tuning](https://doi.org/10.18653/v1/2021.acl-long.87) |  | 0 |  | Richard J. Antonello, Nicole Beckage, Javier Turek, Alexander Huth |  |
| 265 |  |  [Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification](https://doi.org/10.18653/v1/2021.acl-long.88) |  | 0 |  | Cristina Garbacea, Mengtian Guo, Samuel Carton, Qiaozhu Mei |  |
| 266 |  |  [Multi-Task Retrieval for Knowledge-Intensive Tasks](https://doi.org/10.18653/v1/2021.acl-long.89) |  | 0 |  | Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wentau Yih, Barlas Oguz, Veselin Stoyanov, Gargi Ghosh |  |
| 267 |  |  [When Do You Need Billions of Words of Pretraining Data?](https://doi.org/10.18653/v1/2021.acl-long.90) |  | 0 |  | Yian Zhang, Alex Warstadt, Xiaocheng Li, Samuel R. Bowman |  |
| 268 |  |  [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.91) |  | 0 |  | Elena Voita, Rico Sennrich, Ivan Titov |  |
| 269 |  |  [Comparing Test Sets with Item Response Theory](https://doi.org/10.18653/v1/2021.acl-long.92) |  | 0 |  | Clara Vania, Phu Mon Htut, William Huang, Dhara A. Mungra, Richard Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, Samuel R. Bowman |  |
| 270 |  |  [Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.93) |  | 0 |  | Forrest Davis, Marten van Schijndel |  |
| 271 |  |  [More Identifiable yet Equally Performant Transformers for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.94) |  | 0 |  | Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria, Eduard H. Hovy |  |
| 272 |  |  [AugNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://doi.org/10.18653/v1/2021.acl-long.95) |  | 0 |  | Xinnuo Xu, Guoyin Wang, YoungBum Kim, Sungjin Lee |  |
| 273 |  |  [Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability](https://doi.org/10.18653/v1/2021.acl-long.96) |  | 0 |  | Venelin Kovatchev, Phillip Smith, Mark G. Lee, Rory T. Devine |  |
| 274 |  |  [A Dataset and Baselines for Multilingual Reply Suggestion](https://doi.org/10.18653/v1/2021.acl-long.97) |  | 0 |  | Mozhi Zhang, Wei Wang, Budhaditya Deb, Guoqing Zheng, Milad Shokouhi, Ahmed Hassan Awadallah |  |
| 275 |  |  [What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?](https://doi.org/10.18653/v1/2021.acl-long.98) |  | 0 |  | Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara Vania, Samuel R. Bowman |  |
| 276 |  |  [Align Voting Behavior with Public Statements for Legislator Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.99) |  | 0 |  | Xinyi Mou, Zhongyu Wei, Lei Chen, Shangyi Ning, Yancheng He, Changjian Jiang, Xuanjing Huang |  |
| 277 |  |  [Measure and Evaluation of Semantic Divergence across Two Languages](https://doi.org/10.18653/v1/2021.acl-long.100) |  | 0 |  | Syrielle Montariol, Alexandre Allauzen |  |
| 278 |  |  [Improving Zero-Shot Translation by Disentangling Positional Information](https://doi.org/10.18653/v1/2021.acl-long.101) |  | 0 |  | Danni Liu, Jan Niehues, James Cross, Francisco Guzmán, Xian Li |  |
| 279 |  |  [Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning](https://doi.org/10.18653/v1/2021.acl-long.102) |  | 0 |  | Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, Xiang Ren |  |
| 280 |  |  [Attention Calibration for Transformer in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.103) |  | 0 |  | Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li |  |
| 281 |  |  [Diverse Pretrained Context Encodings Improve Document Translation](https://doi.org/10.18653/v1/2021.acl-long.104) |  | 0 |  | Domenic Donato, Lei Yu, Chris Dyer |  |
| 282 |  |  [Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study](https://doi.org/10.18653/v1/2021.acl-long.105) |  | 0 |  | Yash Khemchandani, Sarvesh Mehtani, Vaidehi Patil, Abhijeet Awasthi, Partha P. Talukdar, Sunita Sarawagi |  |
| 283 |  |  [On Finding the K-best Non-projective Dependency Trees](https://doi.org/10.18653/v1/2021.acl-long.106) |  | 0 |  | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 284 |  |  [Towards Argument Mining for Social Good: A Survey](https://doi.org/10.18653/v1/2021.acl-long.107) |  | 0 |  | Eva Maria Vecchi, Neele Falk, Iman Jundi, Gabriella Lapesa |  |
| 285 |  |  [Automated Generation of Storytelling Vocabulary from Photographs for use in AAC](https://doi.org/10.18653/v1/2021.acl-long.108) |  | 0 |  | Maurício Fontana de Vargas, Karyn Moffatt |  |
| 286 |  |  [CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes](https://doi.org/10.18653/v1/2021.acl-long.109) |  | 0 |  | James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David A. Sontag |  |
| 287 |  |  [Assessing Emoji Use in Modern Text Processing Tools](https://doi.org/10.18653/v1/2021.acl-long.110) |  | 0 |  | Abu Awal Md Shoeb, Gerard de Melo |  |
| 288 |  |  [Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention](https://doi.org/10.18653/v1/2021.acl-long.111) |  | 0 |  | Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, KaiWei Chang |  |
| 289 |  |  [Factorising Meaning and Form for Intent-Preserving Paraphrasing](https://doi.org/10.18653/v1/2021.acl-long.112) |  | 0 |  | Tom Hosking, Mirella Lapata |  |
| 290 |  |  [AggGen: Ordering and Aggregating while Generating](https://doi.org/10.18653/v1/2021.acl-long.113) |  | 0 |  | Xinnuo Xu, Ondrej Dusek, Verena Rieser, Ioannis Konstas |  |
| 291 |  |  [Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models](https://doi.org/10.18653/v1/2021.acl-long.114) |  | 0 |  | Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena D. Hwang, Yejin Choi |  |
| 292 |  |  [Towards Table-to-Text Generation with Numerical Reasoning](https://doi.org/10.18653/v1/2021.acl-long.115) |  | 0 |  | Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura, Hiroya Takamura |  |
| 293 |  |  [BACO: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation](https://doi.org/10.18653/v1/2021.acl-long.116) |  | 0 |  | Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner |  |
| 294 |  |  [Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization](https://doi.org/10.18653/v1/2021.acl-long.117) |  | 0 |  | Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu |  |
| 295 |  |  [Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval](https://doi.org/10.18653/v1/2021.acl-long.118) |  | 0 |  | Akari Asai, Eunsol Choi |  |
| 296 |  |  [A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding](https://doi.org/10.18653/v1/2021.acl-long.119) |  | 0 |  | Khalil Mrini, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter Chang, Emilia Farcas, Ndapa Nakashole |  |
| 297 |  |  [Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification](https://doi.org/10.18653/v1/2021.acl-long.120) |  | 0 |  | Rami Aly, Andreas Vlachos, Ryan McDonald |  |
| 298 |  |  [MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.121) |  | 0 |  | Shuang Wu, Xiaoning Song, ZhenHua Feng |  |
| 299 |  |  [Factuality Assessment as Modal Dependency Parsing](https://doi.org/10.18653/v1/2021.acl-long.122) |  | 0 |  | Jiarui Yao, Haoling Qiu, Jin Zhao, Bonan Min, Nianwen Xue |  |
| 300 |  |  [Directed Acyclic Graph Network for Conversational Emotion Recognition](https://doi.org/10.18653/v1/2021.acl-long.123) |  | 0 |  | Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan |  |
| 301 |  |  [Improving Formality Style Transfer with Context-Aware Rule Injection](https://doi.org/10.18653/v1/2021.acl-long.124) |  | 0 |  | Zonghai Yao, Hong Yu |  |
| 302 |  |  [Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection](https://doi.org/10.18653/v1/2021.acl-long.125) |  | 0 |  | Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, Yulan He |  |
| 303 |  |  [Syntopical Graphs for Computational Argumentation Tasks](https://doi.org/10.18653/v1/2021.acl-long.126) |  | 0 |  | Joe Barrow, Rajiv Jain, Nedim Lipka, Franck Dernoncourt, Vlad I. Morariu, Varun Manjunatha, Douglas W. Oard, Philip Resnik, Henning Wachsmuth |  |
| 304 |  |  [Stance Detection in COVID-19 Tweets](https://doi.org/10.18653/v1/2021.acl-long.127) |  | 0 |  | Kyle Glandt, Sarthak Khanal, Yingjie Li, Doina Caragea, Cornelia Caragea |  |
| 305 |  |  [Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification](https://doi.org/10.18653/v1/2021.acl-long.128) |  | 0 |  | Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He |  |
| 306 |  |  [Changes in European Solidarity Before and During COVID-19: Evidence from a Large Crowd- and Expert-Annotated Twitter Dataset](https://doi.org/10.18653/v1/2021.acl-long.129) |  | 0 |  | Alexandra Ils, Dan Liu, Daniela Grunow, Steffen Eger |  |
| 307 |  |  [Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions](https://doi.org/10.18653/v1/2021.acl-long.130) |  | 0 |  | Dorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill, Dan Jurafsky, Tatsunori Hashimoto |  |
| 308 |  |  [A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies](https://doi.org/10.18653/v1/2021.acl-long.131) |  | 0 |  | A. Seza Dogruöz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio |  |
| 309 |  |  [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://doi.org/10.18653/v1/2021.acl-long.132) |  | 0 |  | Bertie Vidgen, Tristan Thrush, Zeerak Waseem, Douwe Kiela |  |
| 310 |  |  [InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection](https://doi.org/10.18653/v1/2021.acl-long.133) |  | 0 |  | Yi R. Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, ShihFu Chang, Kathleen R. McKeown, Mohit Bansal, Avi Sil |  |
| 311 |  |  [I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling](https://doi.org/10.18653/v1/2021.acl-long.134) |  | 0 |  | Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston |  |
| 312 |  |  [A Sequence-to-Sequence Approach to Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-long.135) |  | 0 |  | Yue Feng, Yang Wang, Hang Li |  |
| 313 |  |  [Discovering Dialog Structure Graph for Coherent Dialog Generation](https://doi.org/10.18653/v1/2021.acl-long.136) |  | 0 |  | Jun Xu, Zeyang Lei, Haifeng Wang, ZhengYu Niu, Hua Wu, Wanxiang Che |  |
| 314 |  |  [Dialogue Response Selection with Hierarchical Curriculum Learning](https://doi.org/10.18653/v1/2021.acl-long.137) |  | 0 |  | Yixuan Su, Deng Cai, Qingyu Zhou, Zibo Lin, Simon Baker, Yunbo Cao, Shuming Shi, Nigel Collier, Yan Wang |  |
| 315 |  |  [A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech](https://doi.org/10.18653/v1/2021.acl-long.138) |  | 0 |  | Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, Nianwen Xue, JiRong Wen |  |
| 316 |  |  [A Systematic Investigation of KB-Text Embedding Alignment at Scale](https://doi.org/10.18653/v1/2021.acl-long.139) |  | 0 |  | Vardaan Pahuja, Yu Gu, Wenhu Chen, Mehdi Bahrami, Lei Liu, WeiPeng Chen, Yu Su |  |
| 317 |  |  [Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data](https://doi.org/10.18653/v1/2021.acl-long.140) |  | 0 |  | Haoming Jiang, Danqing Zhang, Tianyu Cao, Bing Yin, Tuo Zhao |  |
| 318 |  |  [Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model](https://doi.org/10.18653/v1/2021.acl-long.141) |  | 0 |  | Hongliang Dai, Yangqiu Song, Haixun Wang |  |
| 319 |  |  [Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](https://doi.org/10.18653/v1/2021.acl-long.142) |  | 0 |  | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 320 |  |  [Implicit Representations of Meaning in Neural Language Models](https://doi.org/10.18653/v1/2021.acl-long.143) |  | 0 |  | Belinda Z. Li, Maxwell I. Nye, Jacob Andreas |  |
| 321 |  |  [Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models](https://doi.org/10.18653/v1/2021.acl-long.144) |  | 0 |  | Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart M. Shieber, Tal Linzen, Yonatan Belinkov |  |
| 322 |  |  [Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach](https://doi.org/10.18653/v1/2021.acl-long.145) |  | 0 |  | Yifan Hou, Mrinmaya Sachan |  |
| 323 |  |  [Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases](https://doi.org/10.18653/v1/2021.acl-long.146) |  | 0 |  | Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu |  |
| 324 |  |  [Poisoning Knowledge Graph Embeddings via Relation Inference Patterns](https://doi.org/10.18653/v1/2021.acl-long.147) |  | 0 |  | Peru Bhardwaj, John D. Kelleher, Luca Costabello, Declan O'Sullivan |  |
| 325 |  |  [Bad Seeds: Evaluating Lexical Methods for Bias Measurement](https://doi.org/10.18653/v1/2021.acl-long.148) |  | 0 |  | Maria Antoniak, David Mimno |  |
| 326 |  |  [A Survey of Race, Racism, and Anti-Racism in NLP](https://doi.org/10.18653/v1/2021.acl-long.149) |  | 0 |  | Anjalie Field, Su Lin Blodgett, Zeerak Waseem, Yulia Tsvetkov |  |
| 327 |  |  [Intrinsic Bias Metrics Do Not Correlate with Application Bias](https://doi.org/10.18653/v1/2021.acl-long.150) |  | 0 |  | Seraphina GoldfarbTarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, Adam Lopez |  |
| 328 |  |  [RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://doi.org/10.18653/v1/2021.acl-long.151) |  | 0 |  | Soumya Barikeri, Anne Lauscher, Ivan Vulic, Goran Glavas |  |
| 329 |  |  [Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks](https://doi.org/10.18653/v1/2021.acl-long.152) |  | 0 |  | Weicheng Ma, Kai Zhang, Renze Lou, Lili Wang, Soroush Vosoughi |  |
| 330 |  |  [Crafting Adversarial Examples for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.153) |  | 0 |  | Xinze Zhang, Junzhe Zhang, Zhenhua Chen, Kun He |  |
| 331 |  |  [UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP](https://doi.org/10.18653/v1/2021.acl-long.154) |  | 0 |  | M. Saiful Bari, Tasnim Mohiuddin, Shafiq R. Joty |  |
| 332 |  |  [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.155) |  | 0 |  | Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li |  |
| 333 |  |  [Hierarchical Context-aware Network for Dense Video Event Captioning](https://doi.org/10.18653/v1/2021.acl-long.156) |  | 0 |  | Lei Ji, Xianglin Guo, Haoyang Huang, Xilin Chen |  |
| 334 |  |  [Control Image Captioning Spatially and Temporally](https://doi.org/10.18653/v1/2021.acl-long.157) |  | 0 |  | Kun Yan, Lei Ji, Huaishao Luo, Ming Zhou, Nan Duan, Shuai Ma |  |
| 335 |  |  [Edited Media Understanding Frames: Reasoning About the Intent and Implications of Visual Misinformation](https://doi.org/10.18653/v1/2021.acl-long.158) |  | 0 |  | Jeff Da, Maxwell Forbes, Rowan Zellers, Anthony Zheng, Jena D. Hwang, Antoine Bosselut, Yejin Choi |  |
| 336 |  |  [PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World](https://doi.org/10.18653/v1/2021.acl-long.159) |  | 0 |  | Rowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi |  |
| 337 |  |  [Modeling Fine-Grained Entity Types with Box Embeddings](https://doi.org/10.18653/v1/2021.acl-long.160) |  | 0 |  | Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett |  |
| 338 |  |  [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://doi.org/10.18653/v1/2021.acl-long.161) |  | 0 |  | Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, Jiwei Li |  |
| 339 |  |  [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://doi.org/10.18653/v1/2021.acl-long.162) |  | 0 |  | Ye Lin, Yanyang Li, Ziyang Wang, Bei Li, Quan Du, Tong Xiao, Jingbo Zhu |  |
| 340 |  |  [Optimizing Deeper Transformers on Small Datasets](https://doi.org/10.18653/v1/2021.acl-long.163) |  | 0 |  | Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon J. D. Prince, Yanshuai Cao |  |
| 341 |  |  [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://doi.org/10.18653/v1/2021.acl-long.164) |  | 0 |  | JongHoon Oh, Ryu Iida, Julien Kloetzer, Kentaro Torisawa |  |
| 342 |  |  [COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic](https://doi.org/10.18653/v1/2021.acl-long.165) |  | 0 |  | Arkadiy Saakyan, Tuhin Chakrabarty, Smaranda Muresan |  |
| 343 |  |  [Explaining Relationships Between Scientific Documents](https://doi.org/10.18653/v1/2021.acl-long.166) |  | 0 |  | Kelvin Luu, Xinyi Wu, Rik KoncelKedziorski, Kyle Lo, Isabel Cachola, Noah A. Smith |  |
| 344 |  |  [IrEne: Interpretable Energy Prediction for Transformers](https://doi.org/10.18653/v1/2021.acl-long.167) |  | 0 |  | Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian |  |
| 345 |  |  [Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach](https://doi.org/10.18653/v1/2021.acl-long.168) |  | 0 |  | Lu Cheng, Ahmadreza Mosallanezhad, Yasin N. Silva, Deborah L. Hall, Huan Liu |  |
| 346 |  |  [PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context](https://doi.org/10.18653/v1/2021.acl-long.169) |  | 0 |  | Xinyun Chen, Linyuan Gong, Alvin Cheung, Dawn Song |  |
| 347 |  |  [Changing the World by Changing the Data](https://doi.org/10.18653/v1/2021.acl-long.170) |  | 0 |  | Anna Rogers |  |
| 348 |  |  [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://doi.org/10.18653/v1/2021.acl-long.171) |  | 0 |  | Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, Jingjing Liu |  |
| 349 |  |  [On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://doi.org/10.18653/v1/2021.acl-long.172) |  | 0 |  | Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, JiaWei Low, Lidong Bing, Luo Si |  |
| 350 |  |  [Data Augmentation for Text Generation Without Any Augmented Data](https://doi.org/10.18653/v1/2021.acl-long.173) |  | 0 |  | Wei Bi, Huayang Li, Jiacheng Huang |  |
| 351 |  |  [Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval](https://doi.org/10.18653/v1/2021.acl-long.174) |  | 0 |  | Zijing Ou, Qinliang Su, Jianxing Yu, Bang Liu, Jingwen Wang, Ruihui Zhao, Changyou Chen, Yefeng Zheng |  |
| 352 |  |  [SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis](https://doi.org/10.18653/v1/2021.acl-long.175) |  | 0 |  | Joshua Feinglass, Yezhou Yang |  |
| 353 |  |  [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2021.acl-long.176) |  | 0 |  | ChiaHsuan Lee, Oleksandr Polozov, Matthew Richardson |  |
| 354 |  |  [QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus](https://doi.org/10.18653/v1/2021.acl-long.177) |  | 0 |  | Hamdy Mubarak, Amir Hussein, Shammur Absar Chowdhury, Ahmed Ali |  |
| 355 |  |  [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.178) |  | 0 |  | Xueqing Liu, Chi Wang |  |
| 356 |  |  [Better than Average: Paired Evaluation of NLP systems](https://doi.org/10.18653/v1/2021.acl-long.179) |  | 0 |  | Maxime Peyrard, Wei Zhao, Steffen Eger, Robert West |  |
| 357 |  |  [Chase: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL](https://doi.org/10.18653/v1/2021.acl-long.180) |  | 0 |  | Jiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, JianGuang Lou, Zijiang Yang, Ting Liu |  |
| 358 |  |  [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://doi.org/10.18653/v1/2021.acl-long.181) |  | 0 |  | Dong Wang, Ning Ding, Piji Li, Haitao Zheng |  |
| 359 |  |  [Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference](https://doi.org/10.18653/v1/2021.acl-long.182) |  | 0 |  | Ziye Chen, Cheng Ding, Zusheng Zhang, Yanghui Rao, Haoran Xie |  |
| 360 |  |  [ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning](https://doi.org/10.18653/v1/2021.acl-long.183) |  | 0 |  | Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin |  |
| 361 |  |  [Distributed Representations of Emotion Categories in Emotion Space](https://doi.org/10.18653/v1/2021.acl-long.184) |  | 0 |  | Xiangyu Wang, Chengqing Zong |  |
| 362 |  |  [Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding](https://doi.org/10.18653/v1/2021.acl-long.185) |  | 0 |  | Dongyeop Kang, Eduard H. Hovy |  |
| 363 |  |  [DynaSent: A Dynamic Benchmark for Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.186) |  | 0 |  | Christopher Potts, Zhengxuan Wu, Atticus Geiger, Douwe Kiela |  |
| 364 |  |  [A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow](https://doi.org/10.18653/v1/2021.acl-long.187) |  | 0 |  | Bidisha Samanta, Mohit Agrawal, Niloy Ganguly |  |
| 365 |  |  [A Unified Generative Framework for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.188) |  | 0 |  | Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, Zheng Zhang |  |
| 366 |  |  [Discovering Dialogue Slots with Weak Supervision](https://doi.org/10.18653/v1/2021.acl-long.189) |  | 0 |  | Vojtech Hudecek, Ondrej Dusek, Zhou Yu |  |
| 367 |  |  [Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU](https://doi.org/10.18653/v1/2021.acl-long.190) |  | 0 |  | Yilin Shen, YenChang Hsu, Avik Ray, Hongxia Jin |  |
| 368 |  |  [ProtAugment: Intent Detection Meta-Learning through Unsupervised Diverse Paraphrasing](https://doi.org/10.18653/v1/2021.acl-long.191) |  | 0 |  | Thomas Dopierre, Christophe Gravier, Wilfried Logerais |  |
| 369 |  |  [Robustness Testing of Language Understanding in Task-Oriented Dialog](https://doi.org/10.18653/v1/2021.acl-long.192) |  | 0 |  | Jiexi Liu, Ryuichi Takanobu, Jiaxin Wen, Dazhen Wan, Hongguang Li, Weiran Nie, Cheng Li, Wei Peng, Minlie Huang |  |
| 370 |  |  [Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?](https://doi.org/10.18653/v1/2021.acl-long.193) |  | 0 |  | Puhai Yang, Heyan Huang, XianLing Mao |  |
| 371 |  |  [OTTers: One-turn Topic Transitions for Open-Domain Dialogue](https://doi.org/10.18653/v1/2021.acl-long.194) |  | 0 |  | Karin Sevegnani, David M. Howcroft, Ioannis Konstas, Verena Rieser |  |
| 372 |  |  [Towards Robustness of Text-to-SQL Models against Synonym Substitution](https://doi.org/10.18653/v1/2021.acl-long.195) |  | 0 |  | Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, Pengsheng Huang |  |
| 373 |  |  [KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.196) |  | 0 |  | Qianglong Chen, Feng Ji, Xiangji Zeng, FengLin Li, Ji Zhang, Haiqing Chen, Yin Zhang |  |
| 374 |  |  [Self-Guided Contrastive Learning for BERT Sentence Representations](https://doi.org/10.18653/v1/2021.acl-long.197) |  | 0 |  | Taeuk Kim, Kang Min Yoo, Sanggoo Lee |  |
| 375 |  |  [LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations](https://doi.org/10.18653/v1/2021.acl-long.198) |  | 0 |  | Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, Kai Yu |  |
| 376 |  |  [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://doi.org/10.18653/v1/2021.acl-long.199) |  | 0 |  | Tongtong Liu, Fangxiang Feng, Xiaojie Wang |  |
| 377 |  |  [Beyond Sentence-Level End-to-End Speech Translation: Context Helps](https://doi.org/10.18653/v1/2021.acl-long.200) |  | 0 |  | Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich |  |
| 378 |  |  [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://doi.org/10.18653/v1/2021.acl-long.201) |  | 0 |  | Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou |  |
| 379 |  |  [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-long.202) |  | 0 |  | Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang |  |
| 380 |  |  [Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities](https://doi.org/10.18653/v1/2021.acl-long.203) |  | 0 |  | Jinming Zhao, Ruichen Li, Qin Jin |  |
| 381 |  |  [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://doi.org/10.18653/v1/2021.acl-long.204) |  | 0 |  | Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu |  |
| 382 |  |  [N-ary Constituent Tree Parsing with Recursive Semi-Markov Model](https://doi.org/10.18653/v1/2021.acl-long.205) |  | 0 |  | Xin Xin, Jinlong Li, Zeqi Tan |  |
| 383 |  |  [Automated Concatenation of Embeddings for Structured Prediction](https://doi.org/10.18653/v1/2021.acl-long.206) |  | 0 |  | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 384 |  |  [Multi-View Cross-Lingual Structured Prediction with Minimum Supervision](https://doi.org/10.18653/v1/2021.acl-long.207) |  | 0 |  | Zechuan Hu, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 385 |  |  [The Limitations of Limited Context for Constituency Parsing](https://doi.org/10.18653/v1/2021.acl-long.208) |  | 0 |  | Yuchen Li, Andrej Risteski |  |
| 386 |  |  [Neural Bi-Lexicalized PCFG Induction](https://doi.org/10.18653/v1/2021.acl-long.209) |  | 0 |  | Songlin Yang, Yanpeng Zhao, Kewei Tu |  |
| 387 |  |  [Ruddit: Norms of Offensiveness for English Reddit Comments](https://doi.org/10.18653/v1/2021.acl-long.210) |  | 0 |  | Rishav Hada, Sohi Sudhir, Pushkar Mishra, Helen Yannakoudakis, Saif M. Mohammad, Ekaterina Shutova |  |
| 388 |  |  [Towards Quantifiable Dialogue Coherence Evaluation](https://doi.org/10.18653/v1/2021.acl-long.211) |  | 0 |  | Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, Xiaodan Liang |  |
| 389 |  |  [Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels](https://doi.org/10.18653/v1/2021.acl-long.212) |  | 0 |  | Marcos García, Tiago Kramer Vieira, Carolina Scarton, Marco Idiart, Aline Villavicencio |  |
| 390 |  |  [Factoring Statutory Reasoning as Language Understanding Challenges](https://doi.org/10.18653/v1/2021.acl-long.213) |  | 0 |  | Nils Holzenberger, Benjamin Van Durme |  |
| 391 |  |  [Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification](https://doi.org/10.18653/v1/2021.acl-long.214) |  | 0 |  | Tetsuya Sakai |  |
| 392 |  |  [Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making](https://doi.org/10.18653/v1/2021.acl-long.215) |  | 0 |  | Zijun Yao, Chengjiang Li, Tiansi Dong, Xin Lv, Jifan Yu, Lei Hou, Juanzi Li, Yichi Zhang, Zelin Dai |  |
| 393 |  |  [Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.216) |  | 0 |  | Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang, Wen Wang, Weiming Lu |  |
| 394 |  |  [Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction](https://doi.org/10.18653/v1/2021.acl-long.217) |  | 0 |  | Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, Shaoyi Chen |  |
| 395 |  |  [A Large-Scale Chinese Multimodal NER Dataset with Speech Clues](https://doi.org/10.18653/v1/2021.acl-long.218) |  | 0 |  | Dianbo Sui, Zhengkun Tian, Yubo Chen, Kang Liu, Jun Zhao |  |
| 396 |  |  [A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization](https://doi.org/10.18653/v1/2021.acl-long.219) |  | 0 |  | Zongcheng Ji, Tian Xia, Mei Han, Jing Xiao |  |
| 397 |  |  [OntoED: Low-resource Event Detection with Ontology Embedding](https://doi.org/10.18653/v1/2021.acl-long.220) |  | 0 |  | Shumin Deng, Ningyu Zhang, Luoqiu Li, Hui Chen, Huaixiao Tou, Mosha Chen, Fei Huang, Huajun Chen |  |
| 398 |  |  [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.221) |  | 0 |  | Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu, Irwin King |  |
| 399 |  |  [Breaking the Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://doi.org/10.18653/v1/2021.acl-long.222) |  | 0 |  | Linqing Chen, Junhui Li, Zhengxian Gong, Boxing Chen, Weihua Luo, Min Zhang, Guodong Zhou |  |
| 400 |  |  [Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.223) |  | 0 |  | Yang Feng, Shuhao Gu, Dengji Guo, Zhengxin Yang, Chenze Shao |  |
| 401 |  |  [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://doi.org/10.18653/v1/2021.acl-long.224) |  | 0 |  | Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, Matteo Negri, Marco Turchi |  |
| 402 |  |  [Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning](https://doi.org/10.18653/v1/2021.acl-long.225) |  | 0 |  | Cheonbok Park, Yunwon Tae, Taehee Kim, Soyoung Yang, Mohammad Azam Khan, Lucy Park, Jaegul Choo |  |
| 403 |  |  [Lightweight Cross-Lingual Sentence Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.226) |  | 0 |  | Zhuoyuan Mao, Prakhar Gupta, Chenhui Chu, Martin Jaggi, Sadao Kurohashi |  |
| 404 |  |  [ERNIE-Doc: A Retrospective Long-Document Modeling Transformer](https://doi.org/10.18653/v1/2021.acl-long.227) |  | 0 |  | Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 405 |  |  [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://doi.org/10.18653/v1/2021.acl-long.228) |  | 0 |  | Yuanxin Liu, Fandong Meng, Zheng Lin, Weiping Wang, Jie Zhou |  |
| 406 |  |  [Rational LAMOL: A Rationale-based Lifelong Learning Framework](https://doi.org/10.18653/v1/2021.acl-long.229) |  | 0 |  | Kasidis Kanwatchara, Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Boonserm Kijsirikul, Peerapon Vateekul |  |
| 407 |  |  [EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering](https://doi.org/10.18653/v1/2021.acl-long.230) |  | 0 |  | Zhibin Duan, Hao Zhang, Chaojie Wang, Zhengjue Wang, Bo Chen, Mingyuan Zhou |  |
| 408 |  |  [LeeBERT: Learned Early Exit for BERT with cross-level optimization](https://doi.org/10.18653/v1/2021.acl-long.231) |  | 0 |  | Wei Zhu |  |
| 409 |  |  [Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering](https://doi.org/10.18653/v1/2021.acl-long.232) |  | 0 |  | Reinald Adrian Pugoy, HungYu Kao |  |
| 410 |  |  [PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction](https://doi.org/10.18653/v1/2021.acl-long.233) |  | 0 |  | Shulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, Di Wang |  |
| 411 |  |  [Competence-based Multimodal Curriculum Learning for Medical Report Generation](https://doi.org/10.18653/v1/2021.acl-long.234) |  | 0 |  | Fenglin Liu, Shen Ge, Xian Wu |  |
| 412 |  |  [Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment](https://doi.org/10.18653/v1/2021.acl-long.235) |  | 0 |  | Xinying Qiu, Yuan Chen, Hanwu Chen, JianYun Nie, Yuming Shen, Dawei Lu |  |
| 413 |  |  [Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains](https://doi.org/10.18653/v1/2021.acl-long.236) |  | 0 |  | Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang |  |
| 414 |  |  [A Semantic-based Method for Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2021.acl-long.237) |  | 0 |  | Yilin Niu, Fei Huang, Jiaming Liang, Wenkai Chen, Xiaoyan Zhu, Minlie Huang |  |
| 415 |  |  [Explanations for CommonsenseQA: New Dataset and Models](https://doi.org/10.18653/v1/2021.acl-long.238) |  | 0 |  | Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg |  |
| 416 |  |  [Few-Shot Question Answering by Pretraining Span Selection](https://doi.org/10.18653/v1/2021.acl-long.239) |  | 0 |  | Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy |  |
| 417 |  |  [UnitedQA: A Hybrid Approach for Open Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.240) |  | 0 |  | Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao |  |
| 418 |  |  [Database reasoning over text](https://doi.org/10.18653/v1/2021.acl-long.241) |  | 0 |  | James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Y. Halevy |  |
| 419 |  |  [Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](https://doi.org/10.18653/v1/2021.acl-long.242) |  | 0 |  | Vânia Mendonça, Ricardo Rei, Luísa Coheur, Alberto Sardinha, Ana Lúcia Santos |  |
| 420 |  |  [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://doi.org/10.18653/v1/2021.acl-long.243) |  | 0 |  | Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, Iryna Gurevych |  |
| 421 |  |  [Evaluating morphological typology in zero-shot cross-lingual transfer](https://doi.org/10.18653/v1/2021.acl-long.244) |  | 0 |  | Antonio MartínezGarcía, Toni Badia, Jeremy Barnes |  |
| 422 |  |  [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://doi.org/10.18653/v1/2021.acl-long.245) |  | 0 |  | Ishan Tarunesh, Syamantak Kumar, Preethi Jyothi |  |
| 423 |  |  [Fast and Accurate Neural Machine Translation with Translation Memory](https://doi.org/10.18653/v1/2021.acl-long.246) |  | 0 |  | Qiuxiang He, Guoping Huang, Qu Cui, Li Li, Lemao Liu |  |
| 424 |  |  [Annotating Online Misogyny](https://doi.org/10.18653/v1/2021.acl-long.247) |  | 0 |  | Philine Zeinert, Nanna Inie, Leon Derczynski |  |
| 425 |  |  [Few-NERD: A Few-shot Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2021.acl-long.248) |  | 0 |  | Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, Zhiyuan Liu |  |
| 426 |  |  [MultiMET: A Multimodal Dataset for Metaphor Understanding](https://doi.org/10.18653/v1/2021.acl-long.249) |  | 0 |  | Dongyu Zhang, Minghao Zhang, Heting Zhang, Liang Yang, Hongfei Lin |  |
| 427 |  |  [Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech](https://doi.org/10.18653/v1/2021.acl-long.250) |  | 0 |  | Margherita Fanton, Helena Bonaldi, Serra Sinem Tekiroglu, Marco Guerini |  |
| 428 |  |  [Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?](https://doi.org/10.18653/v1/2021.acl-long.251) |  | 0 |  | Cunxiang Wang, Pai Liu, Yue Zhang |  |
| 429 |  |  [Joint Models for Answer Verification in Question Answering Systems](https://doi.org/10.18653/v1/2021.acl-long.252) |  | 0 |  | Zeyu Zhang, Thuy Vu, Alessandro Moschitti |  |
| 430 |  |  [Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction](https://doi.org/10.18653/v1/2021.acl-long.253) |  | 0 |  | Yifan Gao, Henghui Zhu, Patrick Ng, Cícero Nogueira dos Santos, Zhiguo Wang, Feng Nan, Dejiao Zhang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang |  |
| 431 |  |  [TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance](https://doi.org/10.18653/v1/2021.acl-long.254) |  | 0 |  | Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, TatSeng Chua |  |
| 432 |  |  [Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.acl-long.255) |  | 0 |  | Yunshi Lan, Jing Jiang |  |
| 433 |  |  [Evidence-based Factual Error Correction](https://doi.org/10.18653/v1/2021.acl-long.256) |  | 0 |  | James Thorne, Andreas Vlachos |  |
| 434 |  |  [Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments](https://doi.org/10.18653/v1/2021.acl-long.257) |  | 0 |  | Austin Blodgett, Nathan Schneider |  |
| 435 |  |  [Meta-Learning to Compositionally Generalize](https://doi.org/10.18653/v1/2021.acl-long.258) |  | 0 |  | Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov |  |
| 436 |  |  [Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation](https://doi.org/10.18653/v1/2021.acl-long.259) |  | 0 |  | Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, Tong Zhang |  |
| 437 |  |  [ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-long.260) |  | 0 |  | Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou |  |
| 438 |  |  [Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction](https://doi.org/10.18653/v1/2021.acl-long.261) |  | 0 |  | Hanqi Yan, Lin Gui, Gabriele Pergola, Yulan He |  |
| 439 |  |  [Every Bite Is an Experience: Key Point Analysis of Business Reviews](https://doi.org/10.18653/v1/2021.acl-long.262) |  | 0 |  | Roy BarHaim, Lilach Eden, Yoav Kantor, Roni Friedman, Noam Slonim |  |
| 440 |  |  [Structured Sentiment Analysis as Dependency Graph Parsing](https://doi.org/10.18653/v1/2021.acl-long.263) |  | 0 |  | Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja Øvrelid, Erik Velldal |  |
| 441 |  |  [Consistency Regularization for Cross-Lingual Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.264) |  | 0 |  | Bo Zheng, Li Dong, Shaohan Huang, Wenhui Wang, Zewen Chi, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, Furu Wei |  |
| 442 |  |  [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.265) |  | 0 |  | Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, XianLing Mao, Heyan Huang, Furu Wei |  |
| 443 |  |  [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://doi.org/10.18653/v1/2021.acl-long.266) |  | 0 |  | Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, Zhaopeng Tu |  |
| 444 |  |  [G-Transformer for Document-Level Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.267) |  | 0 |  | Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, Weihua Luo |  |
| 445 |  |  [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.268) |  | 0 |  | Mengqi Miao, Fandong Meng, Yijin Liu, XiaoHua Zhou, Jie Zhou |  |
| 446 |  |  [Towards Emotional Support Dialog Systems](https://doi.org/10.18653/v1/2021.acl-long.269) |  | 0 |  | Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, Minlie Huang |  |
| 447 |  |  [Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2021.acl-long.270) |  | 0 |  | Yanan Wu, Zhiyuan Zeng, Keqing He, Hong Xu, Yuanmeng Yan, Huixing Jiang, Weiran Xu |  |
| 448 |  |  [GTM: A Generative Triple-wise Model for Conversational Question Generation](https://doi.org/10.18653/v1/2021.acl-long.271) |  | 0 |  | Lei Shen, Fandong Meng, Jinchao Zhang, Yang Feng, Jie Zhou |  |
| 449 |  |  [Diversifying Dialog Generation via Adaptive Label Smoothing](https://doi.org/10.18653/v1/2021.acl-long.272) |  | 0 |  | Yida Wang, Yinhe Zheng, Yong Jiang, Minlie Huang |  |
| 450 |  |  [Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training](https://doi.org/10.18653/v1/2021.acl-long.273) |  | 0 |  | LiMing Zhan, Haowen Liang, Bo Liu, Lu Fan, XiaoMing Wu, Albert Y. S. Lam |  |
| 451 |  |  [Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker](https://doi.org/10.18653/v1/2021.acl-long.274) |  | 0 |  | Runxin Xu, Tianyu Liu, Lei Li, Baobao Chang |  |
| 452 |  |  [Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path](https://doi.org/10.18653/v1/2021.acl-long.275) |  | 0 |  | Yiran Wang, Hiroyuki Shindo, Yuji Matsumoto, Taro Watanabe |  |
| 453 |  |  [LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification](https://doi.org/10.18653/v1/2021.acl-long.276) |  | 0 |  | Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng, Yuguang Chen |  |
| 454 |  |  [Revisiting the Negative Data of Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.277) |  | 0 |  | Chenhao Xie, Jiaqing Liang, Jingping Liu, Chengsong Huang, Wenhao Huang, Yanghua Xiao |  |
| 455 |  |  [Knowing the No-match: Entity Alignment with Dangling Cases](https://doi.org/10.18653/v1/2021.acl-long.278) |  | 0 |  | Zequn Sun, Muhao Chen, Wei Hu |  |
| 456 |  |  [Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words](https://doi.org/10.18653/v1/2021.acl-long.279) |  | 0 |  | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 457 |  |  [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://doi.org/10.18653/v1/2021.acl-long.280) |  | 0 |  | Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, José CamachoCollados |  |
| 458 |  |  [Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy](https://doi.org/10.18653/v1/2021.acl-long.281) |  | 0 |  | Marcos García |  |
| 459 |  |  [Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach](https://doi.org/10.18653/v1/2021.acl-long.282) |  | 0 |  | Jie Huang, Kevin Chang, Jinjun Xiong, WenMei Hwu |  |
| 460 |  |  [HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations](https://doi.org/10.18653/v1/2021.acl-long.283) |  | 0 |  | Weixin Liang, Kaihui Liang, Zhou Yu |  |
| 461 |  |  [Value-Agnostic Conversational Semantic Parsing](https://doi.org/10.18653/v1/2021.acl-long.284) |  | 0 |  | Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Andrew Wolfe, Jacob Andreas, Dan Klein |  |
| 462 |  |  [MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding](https://doi.org/10.18653/v1/2021.acl-long.285) |  | 0 |  | JiaChen Gu, Chongyang Tao, ZhenHua Ling, Can Xu, Xiubo Geng, Daxin Jiang |  |
| 463 |  |  [Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental](https://doi.org/10.18653/v1/2021.acl-long.286) |  | 0 |  | Morteza Rohanian, Julian Hough |  |
| 464 |  |  [NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation](https://doi.org/10.18653/v1/2021.acl-long.287) |  | 0 |  | Sungdong Kim, Minsuk Chang, SangWoo Lee |  |
| 465 |  |  [CDRNN: Discovering Complex Dynamics in Human Language Processing](https://doi.org/10.18653/v1/2021.acl-long.288) |  | 0 |  | Cory Shain |  |
| 466 |  |  [Structural Guidance for Transformer Language Models](https://doi.org/10.18653/v1/2021.acl-long.289) |  | 0 |  | Peng Qian, Tahira Naseem, Roger Levy, Ramón Fernandez Astudillo |  |
| 467 |  |  [Surprisal Estimators for Human Reading Times Need Character Models](https://doi.org/10.18653/v1/2021.acl-long.290) |  | 0 |  | ByungDoh Oh, Christian Clark, William Schuler |  |
| 468 |  |  [CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals](https://doi.org/10.18653/v1/2021.acl-long.291) |  | 0 |  | Yuqi Ren, Deyi Xiong |  |
| 469 |  |  [Self-Attention Networks Can Process Bounded Hierarchical Languages](https://doi.org/10.18653/v1/2021.acl-long.292) |  | 0 |  | Shunyu Yao, Binghui Peng, Christos H. Papadimitriou, Karthik Narasimhan |  |
| 470 |  |  [TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling](https://doi.org/10.18653/v1/2021.acl-long.293) |  | 0 |  | Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David C. Uthus, Zarana Parekh |  |
| 471 |  |  [H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](https://doi.org/10.18653/v1/2021.acl-long.294) |  | 0 |  | Zhenhai Zhu, Radu Soricut |  |
| 472 |  |  [Making Pre-trained Language Models Better Few-shot Learners](https://doi.org/10.18653/v1/2021.acl-long.295) |  | 0 |  | Tianyu Gao, Adam Fisch, Danqi Chen |  |
| 473 |  |  [A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks](https://doi.org/10.18653/v1/2021.acl-long.296) |  | 0 |  | Thai Le, Noseong Park, Dongwon Lee |  |
| 474 |  |  [Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection](https://doi.org/10.18653/v1/2021.acl-long.297) |  | 0 |  | Lingwei Wei, Dou Hu, Wei Zhou, Zhaojuan Yue, Songlin Hu |  |
| 475 |  |  [Label-Specific Dual Graph Neural Network for Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.acl-long.298) |  | 0 |  | Qianwen Ma, Chunyuan Yuan, Wei Zhou, Songlin Hu |  |
| 476 |  |  [TAN-NTM: Topic Attention Networks for Neural Topic Modeling](https://doi.org/10.18653/v1/2021.acl-long.299) |  | 0 |  | Madhur Panwar, Shashank Shailabh, Milan Aggarwal, Balaji Krishnamurthy |  |
| 477 |  |  [Cross-language Sentence Selection via Data Augmentation and Rationale Training](https://doi.org/10.18653/v1/2021.acl-long.300) |  | 0 |  | Yanda Chen, Chris Kedzie, Suraj Nair, Petra Galuscáková, Rui Zhang, Douglas W. Oard, Kathleen R. McKeown |  |
| 478 |  |  [A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections](https://doi.org/10.18653/v1/2021.acl-long.301) |  | 0 |  | Dimitris Pappas, Ion Androutsopoulos |  |
| 479 |  |  [W-RST: Towards a Weighted RST-style Discourse Framework](https://doi.org/10.18653/v1/2021.acl-long.302) |  | 0 |  | Patrick Huber, Wen Xiao, Giuseppe Carenini |  |
| 480 |  |  [ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences](https://doi.org/10.18653/v1/2021.acl-long.303) |  | 0 |  | Yanjun Gao, TingHao Kenneth Huang, Rebecca J. Passonneau |  |
| 481 |  |  [Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering](https://doi.org/10.18653/v1/2021.acl-long.304) |  | 0 |  | Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, Deepak Ramachandran |  |
| 482 |  |  [Adversarial Learning for Discourse Rhetorical Structure Parsing](https://doi.org/10.18653/v1/2021.acl-long.305) |  | 0 |  | Longyin Zhang, Fang Kong, Guodong Zhou |  |
| 483 |  |  [Exploring Discourse Structures for Argument Impact Classification](https://doi.org/10.18653/v1/2021.acl-long.306) |  | 0 |  | Xin Liu, Jiefu Ou, Yangqiu Song, Xin Jiang |  |
| 484 |  |  [Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.307) |  | 0 |  | Tong Zhang, Long Zhang, Wei Ye, Bo Li, Jinan Sun, Xiaoyu Zhu, Wen Zhao, Shikun Zhang |  |
| 485 |  |  [VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation](https://doi.org/10.18653/v1/2021.acl-long.308) |  | 0 |  | Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, Luo Si |  |
| 486 |  |  [A unified approach to sentence segmentation of punctuated text in many languages](https://doi.org/10.18653/v1/2021.acl-long.309) |  | 0 |  | Rachel Wicks, Matt Post |  |
| 487 |  |  [Towards User-Driven Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.310) |  | 0 |  | Huan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Degen Huang, Jinsong Su |  |
| 488 |  |  [End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](https://doi.org/10.18653/v1/2021.acl-long.311) |  | 0 |  | Josef Jon, João Paulo Aires, Dusan Varis, Ondrej Bojar |  |
| 489 |  |  [Handling Extreme Class Imbalance in Technical Logbook Datasets](https://doi.org/10.18653/v1/2021.acl-long.312) |  | 0 |  | Farhad Akhbardeh, Cecilia Ovesdotter Alm, Marcos Zampieri, Travis Desell |  |
| 490 |  |  [ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation](https://doi.org/10.18653/v1/2021.acl-long.313) |  | 0 |  | Vijit Malik, Rishabh Sanjay, Shubham Kumar Nigam, Kripabandhu Ghosh, Shouvik Kumar Guha, Arnab Bhattacharya, Ashutosh Modi |  |
| 491 |  |  [Supporting Cognitive and Emotional Empathic Writing of Students](https://doi.org/10.18653/v1/2021.acl-long.314) |  | 0 |  | Thiemo Wambsganss, Christina Niklaus, Matthias Söllner, Siegfried Handschuh, Jan Marco Leimeister |  |
| 492 |  |  [Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.315) |  | 0 |  | Alexander Hanbo Li, Patrick Ng, Peng Xu, Henghui Zhu, Zhiguo Wang, Bing Xiang |  |
| 493 |  |  [Generation-Augmented Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.316) |  | 0 |  | Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen |  |
| 494 |  |  [Check It Again: Progressive Visual Question Answering via Visual Entailment](https://doi.org/10.18653/v1/2021.acl-long.317) |  | 0 |  | Qingyi Si, Zheng Lin, Mingyu Zheng, Peng Fu, Weiping Wang |  |
| 495 |  |  [A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering](https://doi.org/10.18653/v1/2021.acl-long.318) |  | 0 |  | Zhihong Shao, Lifeng Shang, Qun Liu, Minlie Huang |  |
| 496 |  |  [Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?](https://doi.org/10.18653/v1/2021.acl-long.319) |  | 0 |  | Abhilasha Ravichander, Alan W. Black, Thomas B. Norton, Shomir Wilson, Norman M. Sadeh |  |
| 497 |  |  [Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning](https://doi.org/10.18653/v1/2021.acl-long.320) |  | 0 |  | Christopher Schröder, Kim Bürgl, Yves Annanias, Andreas Niekler, Lydia Müller, Daniel Wiegreffe, Christian Bender, Christoph Mengs, Gerik Scheuermann, Gerhard Heyer |  |
| 498 |  |  [Reliability Testing for Natural Language Processing Systems](https://doi.org/10.18653/v1/2021.acl-long.321) |  | 0 |  | Samson Tan, Shafiq R. Joty, Kathy Baxter, Araz Taeihagh, Gregory A. Bennett, MinYen Kan |  |
| 499 |  |  [Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data](https://doi.org/10.18653/v1/2021.acl-long.322) |  | 0 |  | Paul Pu Liang, Terrance Liu, Anna Cai, Michal Muszynski, Ryo Ishii, Nicholas B. Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 500 |  |  [Anonymisation Models for Text Data: State of the art, Challenges and Future Directions](https://doi.org/10.18653/v1/2021.acl-long.323) |  | 0 |  | Pierre Lison, Ildikó Pilán, David Sánchez, Montserrat Batet, Lilja Øvrelid |  |
| 501 |  |  [End-to-End AMR Corefencence Resolution](https://doi.org/10.18653/v1/2021.acl-long.324) |  | 0 |  | Qiankun Fu, Linfeng Song, Wenyu Du, Yue Zhang |  |
| 502 |  |  [How is BERT surprised? Layerwise detection of linguistic anomalies](https://doi.org/10.18653/v1/2021.acl-long.325) |  | 0 |  | Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, Frank Rudzicz |  |
| 503 |  |  [Psycholinguistic Tripartite Graph Network for Personality Detection](https://doi.org/10.18653/v1/2021.acl-long.326) |  | 0 |  | Tao Yang, Feifan Yang, Haolan Ouyang, Xiaojun Quan |  |
| 504 |  |  [Verb Metaphor Detection via Contextual Relation Learning](https://doi.org/10.18653/v1/2021.acl-long.327) |  | 0 |  | Wei Song, Shuhui Zhou, Ruiji Fu, Ting Liu, Lizhen Liu |  |
| 505 |  |  [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://doi.org/10.18653/v1/2021.acl-long.328) |  | 0 |  | Yun Tang, Juan Miguel Pino, Xian Li, Changhan Wang, Dmitriy Genzel |  |
| 506 |  |  [Probing Toxic Content in Large Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.329) |  | 0 |  | Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, DitYan Yeung |  |
| 507 |  |  [Societal Biases in Language Generation: Progress and Challenges](https://doi.org/10.18653/v1/2021.acl-long.330) |  | 0 |  | Emily Sheng, KaiWei Chang, Prem Natarajan, Nanyun Peng |  |
| 508 |  |  [Reservoir Transformers](https://doi.org/10.18653/v1/2021.acl-long.331) |  | 0 |  | Sheng Shen, Alexei Baevski, Ari S. Morcos, Kurt Keutzer, Michael Auli, Douwe Kiela |  |
| 509 |  |  [Subsequence Based Deep Active Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.332) |  | 0 |  | Puria Radmard, Yassir Fathullah, Aldo Lipani |  |
| 510 |  |  [Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.333) |  | 0 |  | Tyler A. Chang, Yifan Xu, Weijian Xu, Zhuowen Tu |  |
| 511 |  |  [BinaryBERT: Pushing the Limit of BERT Quantization](https://doi.org/10.18653/v1/2021.acl-long.334) |  | 0 |  | Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R. Lyu, Irwin King |  |
| 512 |  |  [Are Pretrained Convolutions Better than Pretrained Transformers?](https://doi.org/10.18653/v1/2021.acl-long.335) |  | 0 |  | Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, Donald Metzler |  |
| 513 |  |  [PairRE: Knowledge Graph Embeddings via Paired Relation Vectors](https://doi.org/10.18653/v1/2021.acl-long.336) |  | 0 |  | Linlin Chao, Jianshan He, Taifeng Wang, Wei Chu |  |
| 514 |  |  [Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification](https://doi.org/10.18653/v1/2021.acl-long.337) |  | 0 |  | Haibin Chen, Qianli Ma, Zhenxi Lin, Jiangyue Yan |  |
| 515 |  |  [HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability](https://doi.org/10.18653/v1/2021.acl-long.338) |  | 0 |  | Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang |  |
| 516 |  |  [Neural Stylistic Response Generation with Disentangled Latent Variables](https://doi.org/10.18653/v1/2021.acl-long.339) |  | 0 |  | Qingfu Zhu, WeiNan Zhang, Ting Liu, William Yang Wang |  |
| 517 |  |  [Intent Classification and Slot Filling for Privacy Policies](https://doi.org/10.18653/v1/2021.acl-long.340) |  | 0 |  | Wasi Uddin Ahmad, Jianfeng Chi, Tu Le, Thomas Norton, Yuan Tian, KaiWei Chang |  |
| 518 |  |  [RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2021.acl-long.341) |  | 0 |  | Baolin Peng, Chunyuan Li, Zhu Zhang, Chenguang Zhu, Jinchao Li, Jianfeng Gao |  |
| 519 |  |  [Semantic Representation for Dialogue Modeling](https://doi.org/10.18653/v1/2021.acl-long.342) |  | 0 |  | Xuefeng Bai, Yulong Chen, Linfeng Song, Yue Zhang |  |
| 520 |  |  [A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations](https://doi.org/10.18653/v1/2021.acl-long.343) |  | 0 |  | Chongyang Tao, Changyu Chen, Jiazhan Feng, JiRong Wen, Rui Yan |  |
| 521 |  |  [Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks](https://doi.org/10.18653/v1/2021.acl-long.344) |  | 0 |  | Yuanhe Tian, Guimin Chen, Yan Song, Xiang Wan |  |
| 522 |  |  [Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP](https://doi.org/10.18653/v1/2021.acl-long.345) |  | 0 |  | Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer Singh |  |
| 523 |  |  [Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?](https://doi.org/10.18653/v1/2021.acl-long.346) |  | 0 |  | Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, Jordan L. BoydGraber |  |
| 524 |  |  [Claim Matching Beyond English to Scale Global Fact-Checking](https://doi.org/10.18653/v1/2021.acl-long.347) |  | 0 |  | Ashkan Kazemi, Kiran Garimella, Devin Gaffney, Scott Hale |  |
| 525 |  |  [SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.348) |  | 0 |  | Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming Zhou, Shuai Ma |  |
| 526 |  |  [Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models](https://doi.org/10.18653/v1/2021.acl-long.349) |  | 0 |  | Sumanta Bhattacharyya, Amirmohammad Rooshenas, Subhajit Naskar, Simeng Sun, Mohit Iyyer, Andrew McCallum |  |
| 527 |  |  [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://doi.org/10.18653/v1/2021.acl-long.350) |  | 0 |  | Wasi Uddin Ahmad, Haoran Li, KaiWei Chang, Yashar Mehdad |  |
| 528 |  |  [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://doi.org/10.18653/v1/2021.acl-long.351) |  | 0 |  | Abteen Ebrahimi, Katharina Kann |  |
| 529 |  |  [Weakly Supervised Named Entity Tagging with Learnable Logical Rules](https://doi.org/10.18653/v1/2021.acl-long.352) |  | 0 |  | Jiacheng Li, Haibo Ding, Jingbo Shang, Julian J. McAuley, Zhe Feng |  |
| 530 |  |  [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://doi.org/10.18653/v1/2021.acl-long.353) |  | 0 |  | Xiang Lisa Li, Percy Liang |  |
| 531 |  |  [One2Set: Generating Diverse Keyphrases as a Set](https://doi.org/10.18653/v1/2021.acl-long.354) |  | 0 |  | Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, Qi Zhang |  |
| 532 |  |  [Continuous Language Generative Flow](https://doi.org/10.18653/v1/2021.acl-long.355) |  | 0 |  | Zineng Tang, Shiyue Zhang, Hyounghun Kim, Mohit Bansal |  |
| 533 |  |  [TWAG: A Topic-Guided Wikipedia Abstract Generator](https://doi.org/10.18653/v1/2021.acl-long.356) |  | 0 |  | Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui |  |
| 534 |  |  [ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data](https://doi.org/10.18653/v1/2021.acl-long.357) |  | 0 |  | Woojeong Jin, Rahul Khanna, Suji Kim, DongHo Lee, Fred Morstatter, Aram Galstyan, Xiang Ren |  |
| 535 |  |  [Recursive Tree-Structured Self-Attention for Answer Sentence Selection](https://doi.org/10.18653/v1/2021.acl-long.358) |  | 0 |  | Khalil Mrini, Emilia Farcas, Ndapa Nakashole |  |
| 536 |  |  [How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.359) |  | 0 |  | Zikun Hu, Yixin Cao, Lifu Huang, TatSeng Chua |  |
| 537 |  |  [Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction](https://doi.org/10.18653/v1/2021.acl-long.360) |  | 0 |  | Kaiwen Wei, Xian Sun, Zequn Zhang, Jingyuan Zhang, Zhi Guo, Li Jin |  |
| 538 |  |  [Element Intervention for Open Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.361) |  | 0 |  | Fangchao Liu, Lingyong Yan, Hongyu Lin, Xianpei Han, Le Sun |  |
| 539 |  |  [AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding](https://doi.org/10.18653/v1/2021.acl-long.362) |  | 0 |  | Jun Yan, Nasser Zalmout, Yan Liang, Christan Grant, Xiang Ren, Xin Luna Dong |  |
| 540 |  |  [CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction](https://doi.org/10.18653/v1/2021.acl-long.363) |  | 0 |  | Zhengbao Jiang, Jialong Han, Bunyamin Sisman, Xin Luna Dong |  |
| 541 |  |  [Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference](https://doi.org/10.18653/v1/2021.acl-long.364) |  | 0 |  | Robert L. Logan IV, Andrew McCallum, Sameer Singh, Daniel M. Bikel |  |
| 542 |  |  [Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.acl-long.365) |  | 0 |  | Zixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng |  |
| 543 |  |  [Employing Argumentation Knowledge Graphs for Neural Argument Generation](https://doi.org/10.18653/v1/2021.acl-long.366) |  | 0 |  | Khalid Al Khatib, Lukas Trautner, Henning Wachsmuth, Yufang Hou, Benno Stein |  |
| 544 |  |  [Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2021.acl-long.367) |  | 0 |  | Lu Xu, Yew Ken Chia, Lidong Bing |  |
| 545 |  |  [On Compositional Generalization of Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.368) |  | 0 |  | Yafu Li, Yongjing Yin, Yulong Chen, Yue Zhang |  |
| 546 |  |  [Mask-Align: Self-Supervised Neural Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.369) |  | 0 |  | Chi Chen, Maosong Sun, Yang Liu |  |
| 547 |  |  [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://doi.org/10.18653/v1/2021.acl-long.370) |  | 0 |  | Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi |  |
| 548 |  |  [De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](https://doi.org/10.18653/v1/2021.acl-long.371) |  | 0 |  | Wenkai Zhang, Hongyu Lin, Xianpei Han, Le Sun |  |
| 549 |  |  [A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.372) |  | 0 |  | Fei Li, Zhichao Lin, Meishan Zhang, Donghong Ji |  |
| 550 |  |  [MLBiNet: A Cross-Sentence Collective Event Detection Network](https://doi.org/10.18653/v1/2021.acl-long.373) |  | 0 |  | Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, Huajun Chen |  |
| 551 |  |  [Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution](https://doi.org/10.18653/v1/2021.acl-long.374) |  | 0 |  | Hieu Minh Tran, Duy Phung, Thien Huu Nguyen |  |
| 552 |  |  [StereoRel: Relational Triple Extraction from a Stereoscopic Perspective](https://doi.org/10.18653/v1/2021.acl-long.375) |  | 0 |  | Xuetao Tian, Liping Jing, Lu He, Feng Liu |  |
| 553 |  |  [Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks](https://doi.org/10.18653/v1/2021.acl-long.376) |  | 0 |  | Pengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao, Yuguang Chen, Weihua Peng |  |
| 554 |  |  [Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution](https://doi.org/10.18653/v1/2021.acl-long.377) |  | 0 |  | Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun |  |
| 555 |  |  [Parameter-Efficient Transfer Learning with Diff Pruning](https://doi.org/10.18653/v1/2021.acl-long.378) |  | 0 |  | Demi Guo, Alexander M. Rush, Yoon Kim |  |
| 556 |  |  [R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.379) |  | 0 |  | Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard de Melo |  |
| 557 |  |  [Risk Minimization for Zero-shot Sequence Labeling](https://doi.org/10.18653/v1/2021.acl-long.380) |  | 0 |  | Zechuan Hu, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 558 |  |  [WARP: Word-level Adversarial ReProgramming](https://doi.org/10.18653/v1/2021.acl-long.381) |  | 0 |  | Karen Hambardzumyan, Hrant Khachatrian, Jonathan May |  |
| 559 |  |  [Lexicon Learning for Few Shot Sequence Modeling](https://doi.org/10.18653/v1/2021.acl-long.382) |  | 0 |  | Ekin Akyürek, Jacob Andreas |  |
| 560 |  |  [Personalized Transformer for Explainable Recommendation](https://doi.org/10.18653/v1/2021.acl-long.383) |  | 0 |  | Lei Li, Yongfeng Zhang, Li Chen |  |
| 561 |  |  [Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques](https://doi.org/10.18653/v1/2021.acl-long.384) |  | 0 |  | Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 562 |  |  [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2021.acl-long.385) |  | 0 |  | Piji Li, Shuming Shi |  |
| 563 |  |  [Early Detection of Sexual Predators in Chats](https://doi.org/10.18653/v1/2021.acl-long.386) |  | 0 |  | Matthias Vogt, Ulf Leser, Alan Akbik |  |
| 564 |  |  [Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation](https://doi.org/10.18653/v1/2021.acl-long.387) |  | 0 |  | Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma |  |
| 565 |  |  [Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification](https://doi.org/10.18653/v1/2021.acl-long.388) |  | 0 |  | Xuepeng Wang, Li Zhao, Bing Liu, Tao Chen, Feng Zhang, Di Wang |  |
| 566 |  |  [VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words](https://doi.org/10.18653/v1/2021.acl-long.389) |  | 0 |  | Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee |  |
| 567 |  |  [Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision](https://doi.org/10.18653/v1/2021.acl-long.390) |  | 0 |  | Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie Bao, Zhiyuan Liu, Paul Bennett |  |
| 568 |  |  [Semi-Supervised Text Classification with Balanced Deep Representation Distributions](https://doi.org/10.18653/v1/2021.acl-long.391) |  | 0 |  | Changchun Li, Ximing Li, Jihong Ouyang |  |
| 569 |  |  [Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval](https://doi.org/10.18653/v1/2021.acl-long.392) |  | 0 |  | Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, Wei Wu |  |
| 570 |  |  [ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](https://doi.org/10.18653/v1/2021.acl-long.393) |  | 0 |  | Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu |  |
| 571 |  |  [Exploring Dynamic Selection of Branch Expansion Orders for Code Generation](https://doi.org/10.18653/v1/2021.acl-long.394) |  | 0 |  | Hui Jiang, Chulun Zhou, Fandong Meng, Biao Zhang, Jie Zhou, Degen Huang, Qingqiang Wu, Jinsong Su |  |
| 572 |  |  [COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion](https://doi.org/10.18653/v1/2021.acl-long.395) |  | 0 |  | Debjit Paul, Anette Frank |  |
| 573 |  |  [Reasoning over Entity-Action-Location Graph for Procedural Text Understanding](https://doi.org/10.18653/v1/2021.acl-long.396) |  | 0 |  | Hao Huang, Xiubo Geng, Jian Pei, Guodong Long, Daxin Jiang |  |
| 574 |  |  [From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding](https://doi.org/10.18653/v1/2021.acl-long.397) |  | 0 |  | Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun, Weipeng Zhang, Jiansong Chen, Fan Yang, Xunliang Cai |  |
| 575 |  |  [Pre-training Universal Language Representation](https://doi.org/10.18653/v1/2021.acl-long.398) |  | 0 |  | Yian Li, Hai Zhao |  |
| 576 |  |  [Structural Pre-training for Dialogue Comprehension](https://doi.org/10.18653/v1/2021.acl-long.399) |  | 0 |  | Zhuosheng Zhang, Hai Zhao |  |
| 577 |  |  [AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.400) |  | 0 |  | Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu |  |
| 578 |  |  [Data Augmentation with Adversarial Training for Cross-Lingual NLI](https://doi.org/10.18653/v1/2021.acl-long.401) |  | 0 |  | Xin Dong, Yaxin Zhu, Zuohui Fu, Dongkuan Xu, Gerard de Melo |  |
| 579 |  |  [Bootstrapped Unsupervised Sentence Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.402) |  | 0 |  | Yan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, Haizhou Li |  |
| 580 |  |  [Learning Event Graph Knowledge for Abductive Reasoning](https://doi.org/10.18653/v1/2021.acl-long.403) |  | 0 |  | Li Du, Xiao Ding, Ting Liu, Bing Qin |  |
| 581 |  |  [A Cognitive Regularizer for Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.404) |  | 0 |  | Jason Wei, Clara Meister, Ryan Cotterell |  |
| 582 |  |  [Lower Perplexity is Not Always Human-Like](https://doi.org/10.18653/v1/2021.acl-long.405) |  | 0 |  | Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara, Kentaro Inui |  |
| 583 |  |  [Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives](https://doi.org/10.18653/v1/2021.acl-long.406) |  | 0 |  | Ming Wang, Yinglin Wang |  |
| 584 |  |  [A Knowledge-Guided Framework for Frame Identification](https://doi.org/10.18653/v1/2021.acl-long.407) |  | 0 |  | Xuefeng Su, Ru Li, Xiaoli Li, Jeff Z. Pan, Hu Zhang, Qinghua Chai, Xiaoqi Han |  |
| 585 |  |  [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://doi.org/10.18653/v1/2021.acl-long.408) |  | 0 |  | Prakhar Gupta, Martin Jaggi |  |
| 586 |  |  [Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.acl-long.409) |  | 0 |  | YingJun Du, Nithin Holla, Xiantong Zhen, Cees Snoek, Ekaterina Shutova |  |
| 587 |  |  [LexFit: Lexical Fine-Tuning of Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-long.410) |  | 0 |  | Ivan Vulic, Edoardo Maria Ponti, Anna Korhonen, Goran Glavas |  |
| 588 |  |  [Text-Free Image-to-Speech Synthesis Using Learned Segmental Units](https://doi.org/10.18653/v1/2021.acl-long.411) |  | 0 |  | WeiNing Hsu, David Harwath, Tyler Miller, Christopher Song, James R. Glass |  |
| 589 |  |  [CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network](https://doi.org/10.18653/v1/2021.acl-long.412) |  | 0 |  | Jiajia Tang, Kang Li, Xuanyu Jin, Andrzej Cichocki, Qibin Zhao, Wanzeng Kong |  |
| 590 |  |  [Positional Artefacts Propagate Through Masked Language Model Embeddings](https://doi.org/10.18653/v1/2021.acl-long.413) |  | 0 |  | Ziyang Luo, Artur Kulmizev, Xiaoxi Mao |  |
| 591 |  |  [Language Model Evaluation Beyond Perplexity](https://doi.org/10.18653/v1/2021.acl-long.414) |  | 0 |  | Clara Meister, Ryan Cotterell |  |
| 592 |  |  [Learning to Explain: Generating Stable Explanations Fast](https://doi.org/10.18653/v1/2021.acl-long.415) |  | 0 |  | Xuelin Situ, Ingrid Zukerman, Cécile Paris, Sameen Maruf, Gholamreza Haffari |  |
| 593 |  |  [StereoSet: Measuring stereotypical bias in pretrained language models](https://doi.org/10.18653/v1/2021.acl-long.416) |  | 0 |  | Moin Nadeem, Anna Bethke, Siva Reddy |  |
| 594 |  |  [Alignment Rationale for Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.417) |  | 0 |  | Zhongtao Jiang, Yuanzhe Zhang, Zhao Yang, Jun Zhao, Kang Liu |  |
| 595 |  |  [Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators](https://doi.org/10.18653/v1/2021.acl-long.418) |  | 0 |  | Peiyu Liu, ZeFeng Gao, Wayne Xin Zhao, ZhiYuan Xie, ZhongYi Lu, JiRong Wen |  |
| 596 |  |  [On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation](https://doi.org/10.18653/v1/2021.acl-long.419) |  | 0 |  | Wei Zhang, Ziming Huang, Yada Zhu, Guangnan Ye, Xiaodong Cui, Fan Zhang |  |
| 597 |  |  [Syntax-Enhanced Pre-trained Model](https://doi.org/10.18653/v1/2021.acl-long.420) |  | 0 |  | Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun Quan, Daxin Jiang, Nan Duan |  |
| 598 |  |  [Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation](https://doi.org/10.18653/v1/2021.acl-long.421) |  | 0 |  | Bo Zhang, Xiaoming Zhang, Yun Liu, Lei Cheng, Zhoujun Li |  |
| 599 |  |  [Counterfactual Inference for Text Classification Debiasing](https://doi.org/10.18653/v1/2021.acl-long.422) |  | 0 |  | Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, Pengjun Xie |  |
| 600 |  |  [HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation](https://doi.org/10.18653/v1/2021.acl-long.423) |  | 0 |  | Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, Yongfeng Huang |  |
| 601 |  |  [PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity](https://doi.org/10.18653/v1/2021.acl-long.424) |  | 0 |  | Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang |  |
| 602 |  |  [Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims](https://doi.org/10.18653/v1/2021.acl-long.425) |  | 0 |  | Qiang Sheng, Juan Cao, Xueyao Zhang, Xirong Li, Lei Zhong |  |
| 603 |  |  [Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble](https://doi.org/10.18653/v1/2021.acl-long.426) |  | 0 |  | Yi Zhou, Xiaoqing Zheng, ChoJui Hsieh, KaiWei Chang, Xuanjing Huang |  |
| 604 |  |  [Shortformer: Better Language Modeling using Shorter Inputs](https://doi.org/10.18653/v1/2021.acl-long.427) |  | 0 |  | Ofir Press, Noah A. Smith, Mike Lewis |  |
| 605 |  |  [BanditMTL: Bandit-based Multi-task Learning for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.428) |  | 0 |  | Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, Wenbin Hu |  |
| 606 |  |  [Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2021.acl-long.429) |  | 0 |  | Hidetaka Kamigaito, Katsuhiko Hayashi |  |
| 607 |  |  [De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-long.430) |  | 0 |  | Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin |  |
| 608 |  |  [Rethinking Stealthiness of Backdoor Attack against NLP Models](https://doi.org/10.18653/v1/2021.acl-long.431) |  | 0 |  | Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun |  |
| 609 |  |  [Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.432) |  | 0 |  | Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Pengjun Xie |  |
| 610 |  |  [Exploring Distantly-Labeled Rationales in Neural Network Models](https://doi.org/10.18653/v1/2021.acl-long.433) |  | 0 |  | Quzhe Huang, Shengqi Zhu, Yansong Feng, Dongyan Zhao |  |
| 611 |  |  [Learning to Perturb Word Embeddings for Out-of-distribution QA](https://doi.org/10.18653/v1/2021.acl-long.434) |  | 0 |  | Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang |  |
| 612 |  |  [Maria: A Visual Experience Powered Conversational Agent](https://doi.org/10.18653/v1/2021.acl-long.435) |  | 0 |  | Zujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xiubo Geng, Yining Chen, Fan Liang, Daxin Jiang |  |
| 613 |  |  [A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues](https://doi.org/10.18653/v1/2021.acl-long.436) |  | 0 |  | Yangjun Zhang, Pengjie Ren, Maarten de Rijke |  |
| 614 |  |  [Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders](https://doi.org/10.18653/v1/2021.acl-long.437) |  | 0 |  | Bin Sun, Shaoxiong Feng, Yiwei Li, Jiamou Liu, Kan Li |  |
| 615 |  |  [Learning to Ask Conversational Questions by Optimizing Levenshtein Distance](https://doi.org/10.18653/v1/2021.acl-long.438) |  | 0 |  | Zhongkun Liu, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Maarten de Rijke, Ming Zhou |  |
| 616 |  |  [DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue](https://doi.org/10.18653/v1/2021.acl-long.439) |  | 0 |  | Hung Le, Chinnadhurai Sankar, Seungwhan Moon, Ahmad Beirami, Alborz Geramifard, Satwik Kottur |  |
| 617 |  |  [MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2021.acl-long.440) |  | 0 |  | Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin |  |
| 618 |  |  [DynaEval: Unifying Turn and Dialogue Level Evaluation](https://doi.org/10.18653/v1/2021.acl-long.441) |  | 0 |  | Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, Haizhou Li |  |
| 619 |  |  [CoSQA: 20, 000+ Web Queries for Code Search and Question Answering](https://doi.org/10.18653/v1/2021.acl-long.442) |  | 0 |  | Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, Nan Duan |  |
| 620 |  |  [Rewriter-Evaluator Architecture for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.443) |  | 0 |  | Yangming Li, Kaisheng Yao |  |
| 621 |  |  [Modeling Bilingual Conversational Characteristics for Neural Chat Translation](https://doi.org/10.18653/v1/2021.acl-long.444) |  | 0 |  | Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 622 |  |  [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.445) |  | 0 |  | Wanying Xie, Yang Feng, Shuhao Gu, Dong Yu |  |
| 623 |  |  [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://doi.org/10.18653/v1/2021.acl-long.446) |  | 0 |  | Xuancheng Huang, Jingfang Xu, Maosong Sun, Yang Liu |  |
| 624 |  |  [A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters](https://doi.org/10.18653/v1/2021.acl-long.447) |  | 0 |  | Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic, Roi Reichart, Anna Korhonen, Hinrich Schütze |  |
| 625 |  |  [Coreference Reasoning in Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.acl-long.448) |  | 0 |  | Mingzhu Wu, Nafise Sadat Moosavi, Dan Roth, Iryna Gurevych |  |
| 626 |  |  [Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing](https://doi.org/10.18653/v1/2021.acl-long.449) |  | 0 |  | Liwen Zhang, Ge Wang, Wenjuan Han, Kewei Tu |  |
| 627 |  |  [A Conditional Splitting Framework for Efficient Constituency Parsing](https://doi.org/10.18653/v1/2021.acl-long.450) |  | 0 |  | ThanhTung Nguyen, XuanPhi Nguyen, Shafiq R. Joty, Xiaoli Li |  |
| 628 |  |  [A Unified Generative Framework for Various NER Subtasks](https://doi.org/10.18653/v1/2021.acl-long.451) |  | 0 |  | Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, Xipeng Qiu |  |
| 629 |  |  [An In-depth Study on Internal Structure of Chinese Words](https://doi.org/10.18653/v1/2021.acl-long.452) |  | 0 |  | Chen Gong, Saihao Huang, Houquan Zhou, Zhenghua Li, Min Zhang, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan |  |
| 630 |  |  [MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER](https://doi.org/10.18653/v1/2021.acl-long.453) |  | 0 |  | Linlin Liu, Bosheng Ding, Lidong Bing, Shafiq R. Joty, Luo Si, Chunyan Miao |  |
| 631 |  |  [Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter](https://doi.org/10.18653/v1/2021.acl-long.454) |  | 0 |  | Wei Liu, Xiyan Fu, Yue Zhang, Wenming Xiao |  |
| 632 |  |  [Math Word Problem Solving with Explicit Numerical Values](https://doi.org/10.18653/v1/2021.acl-long.455) |  | 0 |  | Qinzhuo Wu, Qi Zhang, Zhongyu Wei, Xuanjing Huang |  |
| 633 |  |  [Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks](https://doi.org/10.18653/v1/2021.acl-long.456) |  | 0 |  | Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, Liang Lin |  |
| 634 |  |  [SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining](https://doi.org/10.18653/v1/2021.acl-long.457) |  | 0 |  | Taolin Zhang, Zerui Cai, Chengyu Wang, Minghui Qiu, Bite Yang, Xiaofeng He |  |
| 635 |  |  [What is Your Article Based On? Inferring Fine-grained Provenance](https://doi.org/10.18653/v1/2021.acl-long.458) |  | 0 |  | Yi Zhang, Zachary G. Ives, Dan Roth |  |
| 636 |  |  [Cross-modal Memory Networks for Radiology Report Generation](https://doi.org/10.18653/v1/2021.acl-long.459) |  | 0 |  | Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan |  |
| 637 |  |  [Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection](https://doi.org/10.18653/v1/2021.acl-long.460) |  | 0 |  | Kamil Kanclerz, Alicja Figas, Marcin Gruza, Tomasz Kajdanowicz, Jan Kocon, Daria Puchalska, Przemyslaw Kazienko |  |
| 638 |  |  [Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews](https://doi.org/10.18653/v1/2021.acl-long.461) |  | 0 |  | Junhao Liu, Zhen Hai, Min Yang, Lidong Bing |  |
| 639 |  |  [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://doi.org/10.18653/v1/2021.acl-long.462) |  | 0 |  | Xin Sun, Tao Ge, Furu Wei, Houfeng Wang |  |
| 640 |  |  [Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism](https://doi.org/10.18653/v1/2021.acl-long.463) |  | 0 |  | Tong Zhou, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Kun Niu, Weifeng Chong, Shengping Liu |  |
| 641 |  |  [PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check](https://doi.org/10.18653/v1/2021.acl-long.464) |  | 0 |  | Li Huang, Junjie Li, Weiwei Jiang, Zhiyu Zhang, Minchuan Chen, Shaojun Wang, Jing Xiao |  |
| 642 |  |  [Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting](https://doi.org/10.18653/v1/2021.acl-long.465) |  | 0 |  | Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li, Chenghua Lin, Yefeng Zheng |  |
| 643 |  |  [Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-long.466) |  | 0 |  | Liang Li, Can Ma, Yinliang Yue, Dayong Hu |  |
| 644 |  |  [POS-Constrained Parallel Decoding for Non-autoregressive Generation](https://doi.org/10.18653/v1/2021.acl-long.467) |  | 0 |  | Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi, Jiancheng Lv |  |
| 645 |  |  [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://doi.org/10.18653/v1/2021.acl-long.468) |  | 0 |  | Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Min Zhang, Haiying Zhang, Jinsong Su |  |
| 646 |  |  [TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-long.469) |  | 0 |  | Jie He, Bo Peng, Yi Liao, Qun Liu, Deyi Xiong |  |
| 647 |  |  [Long-Span Summarization via Local Attention and Content Selection](https://doi.org/10.18653/v1/2021.acl-long.470) |  | 0 |  | Potsawee Manakul, Mark J. F. Gales |  |
| 648 |  |  [RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy](https://doi.org/10.18653/v1/2021.acl-long.471) |  | 0 |  | Xiyan Fu, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Changlong Sun, Zhenglu Yang |  |
| 649 |  |  [BASS: Boosting Abstractive Summarization with Unified Semantic Graph](https://doi.org/10.18653/v1/2021.acl-long.472) |  | 0 |  | Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, Haifeng Wang |  |
| 650 |  |  [Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation](https://doi.org/10.18653/v1/2021.acl-long.473) |  | 0 |  | Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan |  |
| 651 |  |  [Focus Attention: Promoting Faithfulness and Diversity in Summarization](https://doi.org/10.18653/v1/2021.acl-long.474) |  | 0 |  | Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan T. McDonald |  |
| 652 |  |  [Generating Query Focused Summaries from Query-Free Resources](https://doi.org/10.18653/v1/2021.acl-long.475) |  | 0 |  | Yumo Xu, Mirella Lapata |  |
| 653 |  |  [Robustifying Multi-hop QA through Pseudo-Evidentiality Training](https://doi.org/10.18653/v1/2021.acl-long.476) |  | 0 |  | Kyungjae Lee, Seungwon Hwang, Sangeun Han, Dohyeon Lee |  |
| 654 |  |  [xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.477) |  | 0 |  | Nan Yang, Furu Wei, Binxing Jiao, Daxing Jiang, Linjun Yang |  |
| 655 |  |  [Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering](https://doi.org/10.18653/v1/2021.acl-long.478) |  | 0 |  | Gangwoo Kim, Hyunjae Kim, Jungsoo Park, Jaewoo Kang |  |
| 656 |  |  [PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling](https://doi.org/10.18653/v1/2021.acl-long.479) |  | 0 |  | Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, Jindong Chen |  |
| 657 |  |  [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.480) |  | 0 |  | Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao |  |
| 658 |  |  [Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering](https://doi.org/10.18653/v1/2021.acl-long.481) |  | 0 |  | Ahjeong Seo, GiCheon Kang, Joonhan Park, ByoungTak Zhang |  |
| 659 |  |  [BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.482) |  | 0 |  | Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, Le Song |  |
| 660 |  |  [CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.483) |  | 0 |  | Tao Chen, Haizhou Shi, Siliang Tang, Zhigang Chen, Fei Wu, Yueting Zhuang |  |
| 661 |  |  [SENT: Sentence-level Distant Relation Extraction via Negative Training](https://doi.org/10.18653/v1/2021.acl-long.484) |  | 0 |  | Ruotian Ma, Tao Gui, Linyang Li, Qi Zhang, Xuanjing Huang, Yaqian Zhou |  |
| 662 |  |  [An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization](https://doi.org/10.18653/v1/2021.acl-long.485) |  | 0 |  | Baohang Zhou, Xiangrui Cai, Ying Zhang, Xiaojie Yuan |  |
| 663 |  |  [PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2021.acl-long.486) |  | 0 |  | Hengyi Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Xu Ming, Yefeng Zheng |  |
| 664 |  |  [Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.487) |  | 0 |  | Meihan Tong, Shuai Wang, Bin Xu, Yixin Cao, Minghui Liu, Lei Hou, Juanzi Li |  |
| 665 |  |  [Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference](https://doi.org/10.18653/v1/2021.acl-long.488) |  | 0 |  | Tuan Manh Lai, Heng Ji, ChengXiang Zhai, Quan Hung Tran |  |
| 666 |  |  [Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation](https://doi.org/10.18653/v1/2021.acl-long.489) |  | 0 |  | Zixuan Zhang, Nikolaus Nova Parulian, Heng Ji, Ahmed Elsayed, Skatje Myers, Martha Palmer |  |
| 667 |  |  [Unleash GPT-2 Power for Event Detection](https://doi.org/10.18653/v1/2021.acl-long.490) |  | 0 |  | Amir Pouran Ben Veyseh, Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen |  |
| 668 |  |  [CLEVE: Contrastive Pre-training for Event Extraction](https://doi.org/10.18653/v1/2021.acl-long.491) |  | 0 |  | Ziqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei Hou, Zhiyuan Liu, Peng Li, Juanzi Li, Jie Zhou |  |
| 669 |  |  [Document-level Event Extraction via Parallel Prediction Networks](https://doi.org/10.18653/v1/2021.acl-long.492) |  | 0 |  | Hang Yang, Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Taifeng Wang |  |
| 670 |  |  [StructuralLM: Structural Pre-training for Form Understanding](https://doi.org/10.18653/v1/2021.acl-long.493) |  | 0 |  | Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si |  |
| 671 |  |  [Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.494) |  | 0 |  | Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang, Eduard H. Hovy |  |
| 672 |  |  [Multi-Label Few-Shot Learning for Aspect Category Detection](https://doi.org/10.18653/v1/2021.acl-long.495) |  | 0 |  | Mengting Hu, Shiwan Zhao, Honglei Guo, Chao Xue, Hang Gao, Tiegang Gao, Renhong Cheng, Zhong Su |  |
| 673 |  |  [Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding](https://doi.org/10.18653/v1/2021.acl-long.496) |  | 0 |  | Liying Cheng, Tianyu Wu, Lidong Bing, Luo Si |  |
| 674 |  |  [A Neural Transition-based Model for Argumentation Mining](https://doi.org/10.18653/v1/2021.acl-long.497) |  | 0 |  | Jianzhu Bao, Chuang Fan, Jipeng Wu, Yixue Dang, Jiachen Du, Ruifeng Xu |  |
| 675 |  |  [Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text](https://doi.org/10.18653/v1/2021.acl-long.498) |  | 0 |  | Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst |  |
| 676 |  |  [Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence](https://doi.org/10.18653/v1/2021.acl-long.499) |  | 0 |  | Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, Minlie Huang |  |
| 677 |  |  [OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics](https://doi.org/10.18653/v1/2021.acl-long.500) |  | 0 |  | Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang |  |
| 678 |  |  [DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation](https://doi.org/10.18653/v1/2021.acl-long.501) |  | 0 |  | Xinyu Hua, Ashwin Sreevatsa, Lu Wang |  |
| 679 |  |  [Controllable Open-ended Question Generation with A New Question Type Ontology](https://doi.org/10.18653/v1/2021.acl-long.502) |  | 0 |  | Shuyang Cao, Lu Wang |  |
| 680 |  |  [BERTGen: Multi-task Generation through BERT](https://doi.org/10.18653/v1/2021.acl-long.503) |  | 0 |  | Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, Lucia Specia |  |
| 681 |  |  [Selective Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.504) |  | 0 |  | Fusheng Wang, Jianhao Yan, Fandong Meng, Jie Zhou |  |
| 682 |  |  [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.505) |  | 0 |  | Patrick Fernandes, Kayo Yin, Graham Neubig, André F. T. Martins |  |
| 683 |  |  [Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring](https://doi.org/10.18653/v1/2021.acl-long.506) |  | 0 |  | Aitor Ormazabal, Mikel Artetxe, Aitor Soroa, Gorka Labaka, Eneko Agirre |  |
| 684 |  |  [CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web](https://doi.org/10.18653/v1/2021.acl-long.507) |  | 0 |  | Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, Angela Fan |  |
| 685 |  |  [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://doi.org/10.18653/v1/2021.acl-long.508) |  | 0 |  | Gyuwan Kim, Kyunghyun Cho |  |
| 686 |  |  [GhostBERT: Generate More Features with Cheap Operations for BERT](https://doi.org/10.18653/v1/2021.acl-long.509) |  | 0 |  | Zhiqi Huang, Lu Hou, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu |  |
| 687 |  |  [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://doi.org/10.18653/v1/2021.acl-long.510) |  | 0 |  | Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, Weizhu Chen |  |
| 688 |  |  [A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations](https://doi.org/10.18653/v1/2021.acl-long.511) |  | 0 |  | Pierre Colombo, Pablo Piantanida, Chloé Clavel |  |
| 689 |  |  [Determinantal Beam Search](https://doi.org/10.18653/v1/2021.acl-long.512) |  | 0 |  | Clara Meister, Martina Forster, Ryan Cotterell |  |
| 690 |  |  [Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning](https://doi.org/10.18653/v1/2021.acl-long.513) |  | 0 |  | Shuoran Jiang, Qingcai Chen, Xin Liu, Baotian Hu, Lisai Zhang |  |
| 691 |  |  [Accelerating Text Communication via Abbreviated Sentence Input](https://doi.org/10.18653/v1/2021.acl-long.514) |  | 0 |  | Jiban Adhikary, Jamie Berger, Keith Vertanen |  |
| 692 |  |  [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://doi.org/10.18653/v1/2021.acl-long.515) |  | 0 |  | Yuqing Xie, YiAn Lai, Yuanjun Xiong, Yi Zhang, Stefano Soatto |  |
| 693 |  |  [Detecting Propaganda Techniques in Memes](https://doi.org/10.18653/v1/2021.acl-long.516) |  | 0 |  | Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino |  |
| 694 |  |  [On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study](https://doi.org/10.18653/v1/2021.acl-long.517) |  | 0 |  | Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wentau Yih |  |
| 695 |  |  [Learning Dense Representations of Phrases at Scale](https://doi.org/10.18653/v1/2021.acl-long.518) |  | 0 |  | Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen |  |
| 696 |  |  [End-to-End Training of Neural Retrievers for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.519) |  | 0 |  | Devendra Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamilton, Bryan Catanzaro |  |
| 697 |  |  [Question Answering Over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.acl-long.520) |  | 0 |  | Apoorv Saxena, Soumen Chakrabarti, Partha P. Talukdar |  |
| 698 |  |  [Language Model Augmented Relevance Score](https://doi.org/10.18653/v1/2021.acl-long.521) |  | 0 |  | Ruibo Liu, Jason Wei, Soroush Vosoughi |  |
| 699 |  |  [DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts](https://doi.org/10.18653/v1/2021.acl-long.522) |  | 0 |  | Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, Yejin Choi |  |
| 700 |  |  [Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models](https://doi.org/10.18653/v1/2021.acl-long.523) |  | 0 |  | Tongshuang Wu, Marco Túlio Ribeiro, Jeffrey Heer, Daniel S. Weld |  |
| 701 |  |  [Metaphor Generation with Conceptual Mappings](https://doi.org/10.18653/v1/2021.acl-long.524) |  | 0 |  | Kevin Stowe, Tuhin Chakrabarty, Nanyun Peng, Smaranda Muresan, Iryna Gurevych |  |
| 702 |  |  [Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols](https://doi.org/10.18653/v1/2021.acl-long.525) |  | 0 |  | Chaitanya Kulkarni, Jany Chan, Eric FoslerLussier, Raghu Machiraju |  |
| 703 |  |  [Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines](https://doi.org/10.18653/v1/2021.acl-long.526) |  | 0 |  | Ramit Sawhney, Mihir Goyal, Prakhar Goel, Puneet Mathur, Rajiv Ratn Shah |  |
| 704 |  |  [Mid-Air Hand Gestures for Post-Editing of Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.527) |  | 0 |  | Rashad Albo Jamara, Nico Herbig, Antonio Krüger, Josef van Genabith |  |
| 705 |  |  [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://doi.org/10.18653/v1/2021.acl-long.528) |  | 0 |  | Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, SongChun Zhu |  |
| 706 |  |  [Joint Verification and Reranking for Open Fact Checking Over Tables](https://doi.org/10.18653/v1/2021.acl-long.529) |  | 0 |  | Michael Sejr Schlichtkrull, Vladimir Karpukhin, Barlas Oguz, Mike Lewis, Wentau Yih, Sebastian Riedel |  |
| 707 |  |  [Evaluation of Thematic Coherence in Microblogs](https://doi.org/10.18653/v1/2021.acl-long.530) |  | 0 |  | Iman Munire Bilal, Bo Wang, Maria Liakata, Rob Procter, Adam Tsakalidis |  |
| 708 |  |  [Neural semi-Markov CRF for Monolingual Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.531) |  | 0 |  | Wuwei Lan, Chao Jiang, Wei Xu |  |
| 709 |  |  [Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies](https://doi.org/10.18653/v1/2021.acl-long.532) |  | 0 |  | Mukund Srinath, Shomir Wilson, C. Lee Giles |  |
| 710 |  |  [The statistical advantage of automatic NLG metrics at the system level](https://doi.org/10.18653/v1/2021.acl-long.533) |  | 0 |  | Johnny TianZheng Wei, Robin Jia |  |
| 711 |  |  [Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.acl-long.534) |  | 0 |  | Yixin Cao, Xiang Ji, Xin Lv, Juanzi Li, Yonggang Wen, Hanwang Zhang |  |
| 712 |  |  [ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining](https://doi.org/10.18653/v1/2021.acl-long.535) |  | 0 |  | Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir R. Radev |  |
| 713 |  |  [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://doi.org/10.18653/v1/2021.acl-long.536) |  | 0 |  | Feng Nan, Cícero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen R. McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang |  |
| 714 |  |  [EmailSum: Abstractive Email Thread Summarization](https://doi.org/10.18653/v1/2021.acl-long.537) |  | 0 |  | Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal |  |
| 715 |  |  [Cross-Lingual Abstractive Summarization with Limited Parallel Resources](https://doi.org/10.18653/v1/2021.acl-long.538) |  | 0 |  | Yu Bai, Yang Gao, Heyan Huang |  |
| 716 |  |  [Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution](https://doi.org/10.18653/v1/2021.acl-long.539) |  | 0 |  | Jiacheng Xu, Greg Durrett |  |
| 717 |  |  [Learning Prototypical Functions for Physical Artifacts](https://doi.org/10.18653/v1/2021.acl-long.540) |  | 0 |  | Tianyu Jiang, Ellen Riloff |  |
| 718 |  |  [Verb Knowledge Injection for Multilingual Event Processing](https://doi.org/10.18653/v1/2021.acl-long.541) |  | 0 |  | Olga Majewska, Ivan Vulic, Goran Glavas, Edoardo Maria Ponti, Anna Korhonen |  |
| 719 |  |  [Dynamic Contextualized Word Embeddings](https://doi.org/10.18653/v1/2021.acl-long.542) |  | 0 |  | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 720 |  |  [Lexical Semantic Change Discovery](https://doi.org/10.18653/v1/2021.acl-long.543) |  | 0 |  | Sinan Kurtyigit, Maike Park, Dominik Schlechtweg, Jonas Kuhn, Sabine Schulte im Walde |  |
| 721 |  |  [The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity](https://doi.org/10.18653/v1/2021.acl-long.544) |  | 0 |  | David Gros, Yu Li, Zhou Yu |  |
| 722 |  |  [Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems](https://doi.org/10.18653/v1/2021.acl-long.545) |  | 0 |  | Claudio S. Pinhanez, Paulo Rodrigo Cavalin, Victor Henrique Alves Ribeiro, Ana Paula Appel, Heloisa Candello, Julio Nogima, Mauro Pichiliani, Melina Alberio Guerra, Maíra de Bayser, Gabriel Louzada Malfatti, Henrique Ferreira |  |
| 723 |  |  [Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer](https://doi.org/10.18653/v1/2021.acl-long.546) |  | 0 |  | Fabian Galetzka, Jewgeni Rose, David Schlangen, Jens Lehmann |  |
| 724 |  |  [DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2021.acl-long.547) |  | 0 |  | Dou Hu, Lingwei Wei, Xiaoyong Huai |  |
| 725 |  |  [Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability](https://doi.org/10.18653/v1/2021.acl-long.548) |  | 0 |  | Ka Wong, Praveen K. Paritosh, Lora Aroyo |  |
| 726 |  |  [TIMEDIAL: Temporal Commonsense Reasoning in Dialog](https://doi.org/10.18653/v1/2021.acl-long.549) |  | 0 |  | Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, Manaal Faruqui |  |
| 727 |  |  [RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)](https://doi.org/10.18653/v1/2021.acl-long.550) |  | 0 |  | Sean Trott, Benjamin K. Bergen |  |
| 728 |  |  [ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic](https://doi.org/10.18653/v1/2021.acl-long.551) |  | 0 |  | Muhammad AbdulMageed, AbdelRahim A. Elmadany, El Moatez Billah Nagoudi |  |
| 729 |  |  [Improving Paraphrase Detection with the Adversarial Paraphrasing Task](https://doi.org/10.18653/v1/2021.acl-long.552) |  | 0 |  | Animesh Nighojkar, John Licato |  |
| 730 |  |  [ADEPT: An Adjective-Dependent Plausibility Task](https://doi.org/10.18653/v1/2021.acl-long.553) |  | 0 |  | Ali Emami, Ian Porada, Alexandra Olteanu, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung |  |
| 731 |  |  [ReadOnce Transformers: Reusable Representations of Text for Transformers](https://doi.org/10.18653/v1/2021.acl-long.554) |  | 0 |  | ShihTing Lin, Ashish Sabharwal, Tushar Khot |  |
| 732 |  |  [Conditional Generation of Temporally-ordered Event Sequences](https://doi.org/10.18653/v1/2021.acl-long.555) |  | 0 |  | ShihTing Lin, Nathanael Chambers, Greg Durrett |  |
| 733 |  |  [Hate Speech Detection Based on Sentiment Knowledge Sharing](https://doi.org/10.18653/v1/2021.acl-long.556) |  | 0 |  | Xianbing Zhou, Yang Yong, Xiaochao Fan, Ge Ren, Yunfeng Song, Yufeng Diao, Liang Yang, Hongfei Lin |  |
| 734 |  |  [Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction](https://doi.org/10.18653/v1/2021.acl-long.557) |  | 0 |  | Tianze Shi, Lillian Lee |  |
| 735 |  |  [SpanNER: Named Entity Re-/Recognition as Span Prediction](https://doi.org/10.18653/v1/2021.acl-long.558) |  | 0 |  | Jinlan Fu, Xuanjing Huang, Pengfei Liu |  |
| 736 |  |  [StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.559) |  | 0 |  | Yikang Shen, Yi Tay, Che Zheng, Dara Bahri, Donald Metzler, Aaron C. Courville |  |
| 737 |  |  [Language Embeddings for Typology and Cross-lingual Transfer Learning](https://doi.org/10.18653/v1/2021.acl-long.560) |  | 0 |  | Dian Yu, Taiqi He, Kenji Sagae |  |
| 738 |  |  [Can Sequence-to-Sequence Models Crack Substitution Ciphers?](https://doi.org/10.18653/v1/2021.acl-long.561) |  | 0 |  | Nada Aldarrab, Jonathan May |  |
| 739 |  |  [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.562) |  | 0 |  | Eleftheria Briakou, Marine Carpuat |  |
| 740 |  |  [Discriminative Reranking for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.563) |  | 0 |  | Ann Lee, Michael Auli, Marc'Aurelio Ranzato |  |
| 741 |  |  [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://doi.org/10.18653/v1/2021.acl-long.564) |  | 0 |  | Siddharth Karamcheti, Ranjay Krishna, Li FeiFei, Christopher D. Manning |  |
| 742 |  |  [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://doi.org/10.18653/v1/2021.acl-long.565) |  | 0 |  | Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A. Smith |  |
| 743 |  |  [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://doi.org/10.18653/v1/2021.acl-long.566) |  | 0 |  | Benjamin Marie, Atsushi Fujita, Raphael Rubino |  |
| 744 |  |  [Neural Machine Translation with Monolingual Translation Memory](https://doi.org/10.18653/v1/2021.acl-long.567) |  | 0 |  | Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu |  |
| 745 |  |  [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.568) |  | 0 |  | Armen Aghajanyan, Sonal Gupta, Luke Zettlemoyer |  |
| 746 |  |  [UnNatural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.569) |  | 0 |  | Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams |  |
| 747 |  |  [Including Signed Languages in Natural Language Processing](https://doi.org/10.18653/v1/2021.acl-long.570) |  | 0 |  | Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe Alikhani |  |
| 748 |  |  [Vocabulary Learning via Optimal Transport for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.571) |  | 0 |  | Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li |  |
