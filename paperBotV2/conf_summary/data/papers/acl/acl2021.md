# ACL2021

## 会议论文列表

本会议共有 748 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Investigation on Data Adaptation Techniques for Neural Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-srw.1) |  | 0 | Data processing is an important step in various natural language processing tasks. As the commonly used datasets in named entity recognition contain only a limited number of samples, it is important to obtain additional labeled data in an efficient and reliable manner. A common practice is to utilize large monolingual unlabeled corpora. Another popular technique is to create synthetic data from the original labeled data (data augmentation). In this work, we investigate the impact of these two... | Evgeniia Tokarchuk, David Thulke, Weiyue Wang, Christian Dugast, Hermann Ney |  |
| 2 |  |  [Stage-wise Fine-tuning for Graph-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-srw.2) |  | 0 | Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In... | Qingyun Wang, Semih Yavuz, Xi Victoria Lin, Heng Ji, Nazneen Fatema Rajani |  |
| 3 |  |  [Transformer-Based Direct Hidden Markov Model for Machine Translation](https://doi.org/10.18653/v1/2021.acl-srw.3) |  | 0 | The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge... | Weiyue Wang, Zijian Yang, Yingbo Gao, Hermann Ney |  |
| 4 |  |  [AutoRC: Improving BERT Based Relation Classification Models via Architecture Search](https://doi.org/10.18653/v1/2021.acl-srw.4) |  | 0 | Although BERT based relation classification (RC) models have achieved significant improvements over the traditional deep learning models, it seems that no consensus can be reached on what is the optimal architecture, since there are many design choices available. In this work, we design a comprehensive search space for BERT based RC models and employ a modified version of efficient neural architecture search (ENAS) method to automatically discover the design choices mentioned above. Experiments... | Wei Zhu |  |
| 5 |  |  [How Low is Too Low? A Computational Perspective on Extremely Low-Resource Languages](https://doi.org/10.18653/v1/2021.acl-srw.5) |  | 0 | Despite the recent advancements of attention-based deep learning architectures across a majority of Natural Language Processing tasks, their application remains limited in a low-resource setting because of a lack of pre-trained models for such languages. In this study, we make the first attempt to investigate the challenges of adapting these techniques to an extremely low-resource language – Sumerian cuneiform – one of the world’s oldest written languages attested from at least the beginning of... | Rachit Bansal, Himanshu Choudhary, Ravneet Punia, Niko Schenk, Émilie PagéPerron, Jacob L. Dahl |  |
| 6 |  |  [On the Relationship between Zipf's Law of Abbreviation and Interfering Noise in Emergent Languages](https://doi.org/10.18653/v1/2021.acl-srw.6) |  | 0 | This paper studies whether emergent languages in a signaling game follow Zipf’s law of abbreviation (ZLA), especially when the communication ability of agents is limited because of interfering noises. ZLA is a well-known tendency in human languages where the more frequently a word is used, the shorter it will be. Surprisingly, previous work demonstrated that emergent languages do not obey ZLA at all when neural agents play a signaling game. It also reported that a ZLA-like tendency appeared by... | Ryo Ueda, Koki Washio |  |
| 7 |  |  [Long Document Summarization in a Low Resource Setting using Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-srw.7) |  | 0 | Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an... | Ahsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Pradhiksha Ashok Kumar, Rheeya Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer, Rajarshi Das, Andrew McCallum |  |
| 8 |  |  [Attending Self-Attention: A Case Study of Visually Grounded Supervision in Vision-and-Language Transformers](https://doi.org/10.18653/v1/2021.acl-srw.8) |  | 0 | The impressive performances of pre-trained visually grounded language models have motivated a growing body of research investigating what has been learned during the pre-training. As a lot of these models are based on Transformers, several studies on the attention mechanisms used by the models to learn to associate phrases with their visual grounding in the image have been conducted. In this work, we investigate how supervising attention directly to learn visual grounding can affect the... | Jules Samaran, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima |  |
| 9 |  |  [Video-guided Machine Translation with Spatial Hierarchical Attention Network](https://doi.org/10.18653/v1/2021.acl-srw.9) |  | 0 | Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial... | Weiqi Gu, Haiyue Song, Chenhui Chu, Sadao Kurohashi |  |
| 10 |  |  [Stylistic approaches to predicting Reddit popularity in diglossia](https://doi.org/10.18653/v1/2021.acl-srw.10) |  | 0 | Past work investigating what makes a Reddit post popular has indicated that style is a far better predictor than content, where posts conforming to a subreddit’s community style are better received. However, what about a diglossia, when there are two community styles? In Singapore, the basilect (‘Singlish’) co-exists with an acrolect (standard English), each with contrasting advantages of community identity and prestige respectively. In this paper, I apply stylistic approaches to predicting... | Huikai Chua |  |
| 11 |  |  ["I've Seen Things You People Wouldn't Believe": Hallucinating Entities in GuessWhat?!](https://doi.org/10.18653/v1/2021.acl-srw.11) |  | 0 | Natural language generation systems have witnessed important progress in the last years, but they are shown to generate tokens that are unrelated to the source input. This problem affects computational models in many NLP tasks, and it is particularly unpleasant in multimodal systems. In this work, we assess the rate of object hallucination in multimodal conversational agents playing the GuessWhat?! referential game. Better visual processing has been shown to mitigate this issue in image... | Alberto Testoni, Raffaella Bernardi |  |
| 12 |  |  [How do different factors Impact the Inter-language Similarity? A Case Study on Indian languages](https://doi.org/10.18653/v1/2021.acl-srw.12) |  | 0 | India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time. Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation, sentiment analysis systems, etc. So in this paper, we... | Sourav Kumar, Salil Aggarwal, Dipti Misra Sharma, Radhika Mamidi |  |
| 13 |  |  [COVID-19 and Misinformation: A Large-Scale Lexical Analysis on Twitter](https://doi.org/10.18653/v1/2021.acl-srw.13) |  | 0 | Social media is often used by individuals and organisations as a platform to spread misinformation. With the recent coronavirus pandemic we have seen a surge of misinformation on Twitter, posing a danger to public health. In this paper, we compile a large COVID-19 Twitter misinformation corpus and perform an analysis to discover patterns with respect to vocabulary usage. Among others, our analysis reveals that the variety of topics and vocabulary usage are considerably more limited and negative... | Dimosthenis Antypas, José CamachoCollados, Alun D. Preece, David Rogers |  |
| 14 |  |  [Situation-Based Multiparticipant Chat Summarization: a Concept, an Exploration-Annotation Tool and an Example Collection](https://doi.org/10.18653/v1/2021.acl-srw.14) |  | 0 | Currently, text chatting is one of the primary means of communication. However, modern text chat still in general does not offer any navigation or even full-featured search, although the high volumes of messages demand it. In order to mitigate these inconveniences, we formulate the problem of situation-based summarization and propose a special data annotation tool intended for developing training and gold-standard data. A situation is a subset of messages revolving around a single event in both... | Anna N. Smirnova, Evgeniy Slobodkin, George A. Chernishev |  |
| 15 |  |  [Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings](https://doi.org/10.18653/v1/2021.acl-srw.15) |  | 0 | In this study, we propose a model that extends the continuous space topic model (CSTM), which flexibly controls word probability in a document, using pre-trained word embeddings. To develop the proposed model, we pre-train word embeddings, which capture the semantics of words and plug them into the CSTM. Intrinsic experimental results show that the proposed model exhibits a superior performance over the CSTM in terms of perplexity and convergence speed. Furthermore, extrinsic experimental... | Seiichi Inoue, Taichi Aida, Mamoru Komachi, Manabu Asai |  |
| 16 |  |  [Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2](https://doi.org/10.18653/v1/2021.acl-srw.16) |  | 0 | The semantics of a text is manifested not only by what is read but also by what is not read. In this article, we will study how those implicit “not read” information such as end-of-paragraph () and end-of-sequence () affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the in the fine-tuning stage. Experimental results on English story generation show that can lead to higher BLEU scores... | He Bai, Peng Shi, Jimmy Lin, Luchen Tan, Kun Xiong, Wen Gao, Jie Liu, Ming Li |  |
| 17 |  |  [Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings](https://doi.org/10.18653/v1/2021.acl-srw.17) |  | 0 | Unsupervised cross-lingual word embedding(CLWE) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora. This method relies on the assumption that the two embedding spaces are structurally similar, which does not necessarily hold true in general. In this paper, we argue that using a pseudo-parallel corpus generated by an unsupervised machine translation model facilitates the structural similarity of the two... | Sosuke Nishikawa, Ryokan Ri, Yoshimasa Tsuruoka |  |
| 18 |  |  [Joint Detection and Coreference Resolution of Entities and Events with Document-level Context Aggregation](https://doi.org/10.18653/v1/2021.acl-srw.18) |  | 0 | Constructing knowledge graphs from unstructured text is an important task that is relevant to many domains. Most previous work focuses on extracting information from sentences or paragraphs, due to the difficulty of analyzing longer contexts. In this paper we propose a new jointly trained model that can be used for various information extraction tasks at the document level. The tasks performed by this system are entity and event identification, typing, and coreference resolution. In order to... | Samuel Kriman, Heng Ji |  |
| 19 |  |  ["Hold on honey, men at work": A semi-supervised approach to detecting sexism in sitcoms](https://doi.org/10.18653/v1/2021.acl-srw.19) |  | 0 | Television shows play an important role inpropagating societal norms. Owing to the popularity of the situational comedy (sitcom) genre, it contributes significantly to the over-all development of society. In an effort to analyze the content of television shows belong-ing to this genre, we present a dataset of dialogue turns from popular sitcoms annotated for the presence of sexist remarks. We train a text classification model to detect sexism using domain adaptive learning. We apply the model... | Smriti Singh, Tanvi Anand, Arijit Ghosh Chowdhury, Zeerak Waseem |  |
| 20 |  |  [Observing the Learning Curve of NMT Systems With Regard to Linguistic Phenomena](https://doi.org/10.18653/v1/2021.acl-srw.20) |  | 0 | In this paper we present our observations and evaluations by observing the linguistic performance of the system on several steps on the training process of various English-to-German Neural Machine Translation models. The linguistic performance is measured through a semi-automatic process using a test suite. Among several linguistic observations, we find that the translation quality of some linguistic categories decreased within the recorded iterations. Additionally, we notice some drops of the... | Patrick Stadler, Vivien Macketanz, Eleftherios Avramidis |  |
| 21 |  |  [Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation](https://doi.org/10.18653/v1/2021.acl-srw.21) |  | 0 | Question answering (QA) models for reading comprehension have achieved human-level accuracy on in-distribution test sets. However, they have been demonstrated to lack robustness to challenge sets, whose distribution is different from that of training sets. Existing data augmentation methods mitigate this problem by simply augmenting training sets with synthetic examples sampled from the same distribution as the challenge sets. However, these methods assume that the distribution of a challenge... | Kazutoshi Shinoda, Saku Sugawara, Akiko Aizawa |  |
| 22 |  |  [Tools Impact on the Quality of Annotations for Chat Untangling](https://doi.org/10.18653/v1/2021.acl-srw.22) |  | 0 | The quality of the annotated data directly influences in the success of supervised NLP models. However, creating annotated datasets is often time-consuming and expensive. Although the annotation tool takes an important role, we know little about how it influences annotation quality. We compare the quality of annotations for the task of chat-untangling made by non-experts annotators using two different tools. The first is SLATE, an existing command-line based tool, and the second is Parlay, a... | Jhonny Cerezo, Felipe BravoMarquez, Alexandre Bergel |  |
| 23 |  |  [How Many Layers and Why? An Analysis of the Model Depth in Transformers](https://doi.org/10.18653/v1/2021.acl-srw.23) |  | 0 | In this study, we investigate the role of the multiple layers in deep transformer models. We design a variant of Albert that dynamically adapts the number of layers for each token of the input. The key specificity of Albert is that weights are tied across layers. Therefore, the stack of encoder layers iteratively repeats the application of the same transformation function on the input. We interpret the repetition of this application as an iterative process where the token contextualized... | Antoine Simoulin, Benoît Crabbé |  |
| 24 |  |  [Edit Distance Based Curriculum Learning for Paraphrase Generation](https://doi.org/10.18653/v1/2021.acl-srw.24) |  | 0 | Curriculum learning has improved the quality of neural machine translation, where only source-side features are considered in the metrics to determine the difficulty of translation. In this study, we apply curriculum learning to paraphrase generation for the first time. Different from machine translation, paraphrase generation allows a certain level of discrepancy in semantics between source and target, which results in diverse transformations from lexical substitution to reordering of clauses.... | Sora Kadotani, Tomoyuki Kajiwara, Yuki Arase, Makoto Onizuka |  |
| 25 |  |  [Changing the Basis of Contextual Representations with Explicit Semantics](https://doi.org/10.18653/v1/2021.acl-srw.25) |  | 0 | The application of transformer-based contextual representations has became a de facto solution for solving complex NLP tasks. Despite their successes, such representations are arguably opaque as their latent dimensions are not directly interpretable. To alleviate this limitation of contextual representations, we devise such an algorithm where the output representation expresses human-interpretable information of each dimension. We achieve this by constructing a transformation matrix based on... | Tamás Ficsor, Gábor Berend |  |
| 26 |  |  [Personal Bias in Prediction of Emotions Elicited by Textual Opinions](https://doi.org/10.18653/v1/2021.acl-srw.26) |  | 0 | Analysis of emotions elicited by opinions, comments, or articles commonly exploits annotated corpora, in which the labels assigned to documents average the views of all annotators, or represent a majority decision. The models trained on such data are effective at identifying the general views of the population. However, their usefulness for predicting the emotions evoked by the textual content in a particular individual is limited. In this paper, we present a study performed on a dataset... | Piotr Milkowski, Marcin Gruza, Kamil Kanclerz, Przemyslaw Kazienko, Damian Grimling, Jan Kocon |  |
| 27 |  |  [MVP-BERT: Multi-Vocab Pre-training for Chinese BERT](https://doi.org/10.18653/v1/2021.acl-srw.27) |  | 0 | Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary (vocab) for these Chinese PLMs remains to be the one provided by Google Chinese BERT (CITATION), which is based on Chinese characters (chars). Second, the masked language model pre-training is based on a single vocab, limiting its downstream task performances. In this work, we first experimentally demonstrate that building... | Wei Zhu |  |
| 28 |  |  [CMTA: COVID-19 Misinformation Multilingual Analysis on Twitter](https://doi.org/10.18653/v1/2021.acl-srw.28) |  | 0 | The internet has actually come to be an essential resource of health knowledge for individuals around the world in the present situation of the coronavirus condition pandemic(COVID-19). During pandemic situations, myths, sensationalism, rumours and misinformation, generated intentionally or unintentionally, spread rapidly through social networks. Twitter is one of these popular social networks people use to share COVID-19 related news, information, and thoughts that reflect their perception and... | Raj Ratn Pranesh, Mehrdad Farokhenajd, Ambesh Shekhar, Genoveva VargasSolar |  |
| 29 |  |  [Predicting pragmatic discourse features in the language of adults with autism spectrum disorder](https://doi.org/10.18653/v1/2021.acl-srw.29) |  | 0 | Individuals with autism spectrum disorder (ASD) experience difficulties in social aspects of communication, but the linguistic characteristics associated with deficits in discourse and pragmatic expression are often difficult to precisely identify and quantify. We are currently collecting a corpus of transcribed natural conversations produced in an experimental setting in which participants with and without ASD complete a number of collaborative tasks with their neurotypical peers. Using this... | Christine Yang, Duanchen Liu, Qingyun Yang, Zoey Liu, Emily Prud'hommeaux |  |
| 30 |  |  [SumPubMed: Summarization Dataset of PubMed Scientific Articles](https://doi.org/10.18653/v1/2021.acl-srw.30) |  | 0 | Most earlier work on text summarization is carried out on news article datasets. The summary in these datasets is naturally located at the beginning of the text. Hence, a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize. To address this issue, we constructed a new dataset, SumPubMed , using scientific articles from the PubMed archive. We conducted a human analysis of summary coverage, redundancy, readability, coherence, and... | Vivek Gupta, Prerna Bharti, Pegah Nokhiz, Harish Karnick |  |
| 31 |  |  [A Case Study of Analysis of Construals in Language on Social Media Surrounding a Crisis Event](https://doi.org/10.18653/v1/2021.acl-srw.31) |  | 0 | The events that took place at the Unite the Right rally held in Charlottesville, Virginia on August 11-12, 2017 caused intense reaction on social media from users across the political spectrum. We present a novel application of psycholinguistics - specifically, construal level theory - to analyze the language on social media around this event of social import through topic models. We find that including psycholinguistic measures of concreteness as covariates in topic models can lead to informed... | Lolo Aboufoul, Khyati Mahajan, Tiffany Gallicano, Sara Levens, Samira Shaikh |  |
| 32 |  |  [Cross-lingual Evidence Improves Monolingual Fake News Detection](https://doi.org/10.18653/v1/2021.acl-srw.32) |  | 0 | Misleading information spreads on the Internet at an incredible speed, which can lead to irreparable consequences in some cases. Therefore, it is becoming essential to develop fake news detection technologies. While substantial work has been done in this direction, one of the limitations of the current approaches is that these models are focused only on one language and do not use multilingual information. In this work, we propose a new technique based on cross-lingual evidence (CE) that can be... | Daryna Dementieva, Alexander Panchenko |  |
| 33 |  |  [Neural Machine Translation with Synchronous Latent Phrase Structure](https://doi.org/10.18653/v1/2021.acl-srw.33) |  | 0 | It is reported that grammatical information is useful for machine translation (MT) task. However, the annotation of grammatical information requires the highly human resources. Furthermore, it is not trivial to adapt grammatical information to MT since grammatical annotation usually adapts tokenization standards which might not be suitable to capture the relation of two languages, and the use of sub-word tokenization, e.g., Byte-Pair-Encoding, to alleviate out-of-vocabulary problem might not be... | Shintaro Harada, Taro Watanabe |  |
| 34 |  |  [Zero Pronouns Identification based on Span prediction](https://doi.org/10.18653/v1/2021.acl-srw.34) |  | 0 | The presence of zero-pronoun (ZP) greatly affects the downstream tasks of NLP in pro-drop languages such as Japanese and Chinese. To tackle the problem, the previous works identified ZPs as sequence labeling on the word sequence or the linearlized tree nodes of the input. We propose a novel approach to ZP identification by casting it as a query-based argument span prediction task. Given a predicate as a query, our model predicts the omission with ZP. In the experiments, our model surpassed the... | Sei Iwata, Taro Watanabe, Masaaki Nagata |  |
| 35 |  |  [On the differences between BERT and MT encoder spaces and how to address them in translation tasks](https://doi.org/10.18653/v1/2021.acl-srw.35) |  | 0 | Various studies show that pretrained language models such as BERT cannot straightforwardly replace encoders in neural machine translation despite their enormous success in other tasks. This is even more astonishing considering the similarities between the architectures. This paper sheds some light on the embedding spaces they create, using average cosine similarity, contextuality metrics and measures for representational similarity for comparison, revealing that BERT and NMT encoder... | Raúl Vázquez, Hande Çelikkanat, Mathias Creutz, Jörg Tiedemann |  |
| 36 |  |  [Synchronous Syntactic Attention for Transformer Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-srw.36) |  | 0 | This paper proposes a novel attention mechanism for Transformer Neural Machine Translation, “Synchronous Syntactic Attention,” inspired by synchronous dependency grammars. The mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side self-attentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. The experiments show that the proposed method improves the translation performance on WMT14... | Hiroyuki Deguchi, Akihiro Tamura, Takashi Ninomiya |  |
| 37 |  |  [Frontmatter](https://aclanthology.org/2021.acl-short.0) |  | 0 |  |  |  |
| 38 |  |  [Catchphrase: Automatic Detection of Cultural References](https://doi.org/10.18653/v1/2021.acl-short.1) |  | 0 | A snowclone is a customizable phrasal template that can be realized in multiple, instantly recognized variants. For example, “\* is the new \*" (Orange is the new black, 40 is the new 30). Snowclones are extensively used in social media. In this paper, we study snowclones originating from pop-culture quotes; our goal is to automatically detect cultural references in text. We introduce a new, publicly available data set of pop-culture quotes and their corresponding snowclone usages and train... | Nir Sweed, Dafna Shahaf |  |
| 39 |  |  [On Training Instance Selection for Few-Shot Neural Text Generation](https://doi.org/10.18653/v1/2021.acl-short.2) |  | 0 | Large-scale pretrained language models have led to dramatic improvements in text generation. Impressive performance can be achieved by finetuning only on a small number of instances (few-shot setting). Nonetheless, almost all previous work simply applies random sampling to select the few-shot training instances. Little to no attention has been paid to the selection strategies and how they would affect model performance. In this work, we present a study on training instance selection in few-shot... | Ernie Chang, Xiaoyu Shen, HuiSyuan Yeh, Vera Demberg |  |
| 40 |  |  [Coreference Resolution without Span Representations](https://doi.org/10.18653/v1/2021.acl-short.3) |  | 0 | The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint – primarily due to dynamically-constructed span and span-pair representations – which hinders the processing of complete documents and the ability to train on... | Yuval Kirstain, Ori Ram, Omer Levy |  |
| 41 |  |  [Enhancing Entity Boundary Detection for Better Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-short.4) |  | 0 | In comparison with English, due to the lack of explicit word boundary and tenses information, Chinese Named Entity Recognition (NER) is much more challenging. In this paper, we propose a boundary enhanced approach for better Chinese NER. In particular, our approach enhances the boundary information from two perspectives. On one hand, we enhance the representation of the internal dependency of phrases by an additional Graph Attention Network(GAT) layer. On the other hand, taking the entity... | Chun Chen, Fang Kong |  |
| 42 |  |  [Difficulty-Aware Machine Translation Evaluation](https://doi.org/10.18653/v1/2021.acl-short.5) |  | 0 | The high-quality translation results produced by machine translation (MT) systems still pose a huge challenge for automatic evaluation. Current MT evaluation pays the same attention to each sentence component, while the questions of real-world examinations (e.g., university examinations) have different difficulties and weightings. In this paper, we propose a novel difficulty-aware MT evaluation metric, expanding the evaluation dimension by taking translation difficulty into consideration. A... | Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao |  |
| 43 |  |  [Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition](https://doi.org/10.18653/v1/2021.acl-short.6) |  | 0 | Humor recognition has been widely studied as a text classification problem using data-driven approaches. However, most existing work does not examine the actual joke mechanism to understand humor. We break down any joke into two distinct components: the set-up and the punchline, and further explore the special relationship between them. Inspired by the incongruity theory of humor, we model the set-up as the part developing semantic uncertainty, and the punchline disrupting audience... | Yubo Xie, Junze Li, Pearl Pu |  |
| 44 |  |  [Counterfactuals to Control Latent Disentangled Text Representations for Style Transfer](https://doi.org/10.18653/v1/2021.acl-short.7) |  | 0 | Disentanglement of latent representations into content and style spaces has been a commonly employed method for unsupervised text style transfer. These techniques aim to learn the disentangled representations and tweak them to modify the style of a sentence. In this paper, we propose a counterfactual-based method to modify the latent representation, by posing a ‘what-if’ scenario. This simple and disciplined approach also enables a fine-grained control on the transfer strength. We conduct... | Sharmila Reddy Nangi, Niyati Chhaya, Sopan Khosla, Nikhil Kaushik, Harshit Nyati |  |
| 45 |  |  [Attention Flows are Shapley Value Explanations](https://doi.org/10.18653/v1/2021.acl-short.8) |  | 0 | Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that — save for the degenerate case — attention weights and leave-one-out values cannot be Shapley Values.... | Kawin Ethayarajh, Dan Jurafsky |  |
| 46 |  |  [Video Paragraph Captioning as a Text Summarization Task](https://doi.org/10.18653/v1/2021.acl-short.9) |  | 0 | Video paragraph captioning aims to generate a set of coherent sentences to describe a video that contains several events. Most previous methods simplify this task by using ground-truth event segments. In this work, we propose a novel framework by taking this task as a text summarization task. We first generate lots of sentence-level captions focusing on different video clips and then summarize these captions to obtain the final paragraph caption. Our method does not depend on ground-truth event... | Hui Liu, Xiaojun Wan |  |
| 47 |  |  [Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions](https://doi.org/10.18653/v1/2021.acl-short.10) |  | 0 | Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these... | Daniel Rosenberg, Itai Gat, Amir Feder, Roi Reichart |  |
| 48 |  |  [How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?](https://doi.org/10.18653/v1/2021.acl-short.11) |  | 0 | Existing approaches for the Table-to-Text task suffer from issues such as missing information, hallucination and repetition. Many approaches to this problem use Reinforcement Learning (RL), which maximizes a single manually defined reward, such as BLEU. In this work, we instead pose the Table-to-Text task as Inverse Reinforcement Learning (IRL) problem. We explore using multiple interpretable unsupervised reward components that are combined linearly to form a composite reward function. The... | Sayan Ghosh, Zheng Qi, Snigdha Chaturvedi, Shashank Srivastava |  |
| 49 |  |  [Automatic Fake News Detection: Are Models Learning to Reason?](https://doi.org/10.18653/v1/2021.acl-short.12) |  | 0 | Most fact checking models for automatic fake news detection are based on reasoning: given a claim with associated evidence, the models aim to estimate the claim veracity based on the supporting or refuting content within the evidence. When these models perform well, it is generally assumed to be due to the models having learned to reason over the evidence with regards to the claim. In this paper, we investigate this assumption of reasoning, by exploring the relationship and importance of both... | Casper Hansen, Christian Hansen, Lucas Chaves Lima |  |
| 50 |  |  [Saying No is An Art: Contextualized Fallback Responses for Unanswerable Dialogue Queries](https://doi.org/10.18653/v1/2021.acl-short.13) |  | 0 | Despite end-to-end neural systems making significant progress in the last decade for task-oriented as well as chit-chat based dialogue systems, most dialogue systems rely on hybrid approaches which use a combination of rule-based, retrieval and generative approaches for generating a set of ranked responses. Such dialogue systems need to rely on a fallback mechanism to respond to out-of-domain or novel user queries which are not answerable within the scope of the dialogue system. While, dialogue... | Ashish Shrivastava, Kaustubh D. Dhole, Abhinav Bhatt, Sharvani Raghunath |  |
| 51 |  |  [N-Best ASR Transformer: Enhancing SLU Performance using Multiple ASR Hypotheses](https://doi.org/10.18653/v1/2021.acl-short.14) |  | 0 | Spoken Language Understanding (SLU) systems parse speech into semantic structures like dialog acts and slots. This involves the use of an Automatic Speech Recognizer (ASR) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in ASRs, impact downstream SLU performance negatively. Common approaches to mitigate such errors involve using richer information from the ASR, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer... | Karthik Ganesan, Pakhi Bamdev, Jaivarsan B, Amresh Venugopal, Abhinav Tushar |  |
| 52 |  |  [Gender bias amplification during Speed-Quality optimization in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.15) |  | 0 | Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU? We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation. We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a... | Adithya Renduchintala, Denise Díaz, Kenneth Heafield, Xian Li, Mona T. Diab |  |
| 53 |  |  [Machine Translation into Low-resource Language Varieties](https://doi.org/10.18653/v1/2021.acl-short.16) |  | 0 | State-of-the-art machine translation (MT) systems are typically trained to generate “standard” target language; however, many languages have multiple varieties (regional varieties, dialects, sociolects, non-native varieties) that are different from the standard language. Such varieties are often low-resource, and hence do not benefit from contemporary NLP solutions, MT included. We propose a general framework to rapidly adapt MT systems to generate language varieties that are close to, but... | Sachin Kumar, Antonios Anastasopoulos, Shuly Wintner, Yulia Tsvetkov |  |
| 54 |  |  [Is Sparse Attention more Interpretable?](https://doi.org/10.18653/v1/2021.acl-short.17) |  | 0 | Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability... | Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell |  |
| 55 |  |  [The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models](https://doi.org/10.18653/v1/2021.acl-short.18) |  | 0 | Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which... | Ulme Wennberg, Gustav Eje Henter |  |
| 56 |  |  [Relative Importance in Sentence Processing](https://doi.org/10.18653/v1/2021.acl-short.19) |  | 0 | Determining the relative importance of the elements in a sentence is a key factor for effortless natural language understanding. For human language processing, we can approximate patterns of relative importance by measuring reading fixations using eye-tracking technology. In neural language models, gradient-based saliency methods indicate the relative importance of a token for the target objective. In this work, we compare patterns of relative importance in English language processing by humans... | Nora Hollenstein, Lisa Beinborn |  |
| 57 |  |  [Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models](https://doi.org/10.18653/v1/2021.acl-short.20) |  | 0 | Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more... | Mingyue Han, Yinglin Wang |  |
| 58 |  |  [AND does not mean OR: Using Formal Languages to Study Language Models' Representations](https://doi.org/10.18653/v1/2021.acl-short.21) |  | 0 | A current open question in natural language processing is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between meaning and form, but rather meaning constrains form in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion: Under what conditions should we expect that meaning and form... | Aaron Traylor, Roman Feiman, Ellie Pavlick |  |
| 59 |  |  [Enforcing Consistency in Weakly Supervised Semantic Parsing](https://doi.org/10.18653/v1/2021.acl-short.22) |  | 0 | The predominant challenge in weakly supervised semantic parsing is that of spurious programs that evaluate to correct answers for the wrong reasons. Prior work uses elaborate search strategies to mitigate the prevalence of spurious programs; however, they typically consider only one input at a time. In this work we explore the use of consistency between the output programs for related inputs to reduce the impact of spurious programs. We bias the program search (and thus the model’s training... | Nitish Gupta, Sameer Singh, Matt Gardner |  |
| 60 |  |  [An Improved Model for Voicing Silent Speech](https://doi.org/10.18653/v1/2021.acl-short.23) |  | 0 | In this paper, we present an improved model for voicing silent speech, where audio is synthesized from facial electromyography (EMG) signals. To give our model greater flexibility to learn its own input features, we directly use EMG signals as input in the place of hand-designed features used by prior work. Our model uses convolutional layers to extract features from the signals and Transformer layers to propagate information across longer distances. To provide better signal for learning, we... | David Gaddy, Dan Klein |  |
| 61 |  |  [What's in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus](https://doi.org/10.18653/v1/2021.acl-short.24) |  | 0 | Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit... | Alexandra Sasha Luccioni, Joseph D. Viviano |  |
| 62 |  |  [Continual Quality Estimation with Online Bayesian Meta-Learning](https://doi.org/10.18653/v1/2021.acl-short.25) |  | 0 | Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of... | Abiola Obamuyide, Marina Fomicheva, Lucia Specia |  |
| 63 |  |  [A Span-based Dynamic Local Attention Model for Sequential Sentence Classification](https://doi.org/10.18653/v1/2021.acl-short.26) |  | 0 | Sequential sentence classification aims to classify each sentence in the document based on the context in which sentences appear. Most existing work addresses this problem using a hierarchical sequence labeling network. However, they ignore considering the latent segment structure of the document, in which contiguous sentences often have coherent semantics. In this paper, we proposed a span-based dynamic local attention model that could explicitly capture the structural information by the... | Xichen Shang, Qianli Ma, Zhenxi Lin, Jiangyue Yan, Zipeng Chen |  |
| 64 |  |  [How effective is BERT without word ordering? Implications for language understanding and data privacy](https://doi.org/10.18653/v1/2021.acl-short.27) |  | 0 | Ordered word sequences contain the rich structures that define language. However, it’s often not clear if or how modern pretrained language models utilize these structures. We show that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens, and that for several GLUE language understanding tasks, shuffling only minimally degrades performance, e.g., by 4% for QNLI. While bleak from the perspective of language... | Jack Hessel, Alexandra Schofield |  |
| 65 |  |  [WikiSum: Coherent Summarization Dataset for Efficient Human-Evaluation](https://doi.org/10.18653/v1/2021.acl-short.28) |  | 0 | Recent works made significant advances on summarization tasks, facilitated by summarization datasets. Several existing datasets have the form of coherent-paragraph summaries. However, these datasets were curated from academic documents that were written for experts, thus making the essential step of assessing the summarization output through human-evaluation very demanding. To overcome these limitations, we present a dataset based on article summaries appearing on the WikiHow website, composed... | Nachshon Cohen, Oren Kalinsky, Yftah Ziser, Alessandro Moschitti |  |
| 66 |  |  [UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-short.29) |  | 0 | Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical... | Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin Jung |  |
| 67 |  |  [Anchor-based Bilingual Word Embeddings for Low-Resource Languages](https://doi.org/10.18653/v1/2021.acl-short.30) |  | 0 | Good quality monolingual word embeddings (MWEs) can be built for languages which have large amounts of unlabeled text. MWEs can be aligned to bilingual spaces using only a few thousand word translation pairs. For low resource languages training MWEs monolingually results in MWEs of poor quality, and thus poor bilingual word embeddings (BWEs) as well. This paper proposes a new approach for building BWEs in which the vector space of the high resource source language is used as a starting point... | Tobias Eder, Viktor Hangya, Alexander M. Fraser |  |
| 68 |  |  [Multilingual Agreement for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.31) |  | 0 | Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the... | Jian Yang, Yuwei Yin, Shuming Ma, Haoyang Huang, Dongdong Zhang, Zhoujun Li, Furu Wei |  |
| 69 |  |  [Higher-order Derivatives of Weighted Finite-state Machines](https://doi.org/10.18653/v1/2021.acl-short.32) |  | 0 | Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 70 |  |  [Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards](https://doi.org/10.18653/v1/2021.acl-short.33) |  | 0 | The growth of online consumer health questions has led to the necessity for reliable and accurate question answering systems. A recent study showed that manual summarization of consumer health questions brings significant improvement in retrieving relevant answers. However, the automatic summarization of long questions is a challenging task due to the lack of training data and the complexity of the related subtasks, such as the question focus and type recognition. In this paper, we introduce a... | Shweta Yadav, Deepak Gupta, Asma Ben Abacha, Dina DemnerFushman |  |
| 71 |  |  [A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.acl-short.34) |  | 0 | Relation linking is a crucial component of Knowledge Base Question Answering systems. Existing systems use a wide variety of heuristics, or ensembles of multiple systems, heavily relying on the surface question text. However, the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. We propose a simple transformer-based neural model for relation linking that leverages the AMR semantic parse of a sentence. Our system significantly... | Tahira Naseem, Srinivas Ravishankar, Nandana Mihindukulasooriya, Ibrahim Abdelaziz, YoungSuk Lee, Pavan Kapanipathi, Salim Roukos, Alfio Gliozzo, Alexander G. Gray |  |
| 72 |  |  [Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation](https://doi.org/10.18653/v1/2021.acl-short.35) |  | 0 | Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The cross-attention model is... | Yinfei Yang, Ning Jin, Kuo Lin, Mandy Guo, Daniel Cer |  |
| 73 |  |  [Enhancing Descriptive Image Captioning with Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-short.36) |  | 0 | Generating descriptive sentences that convey non-trivial, detailed, and salient information about images is an important goal of image captioning. In this paper we propose a novel approach to encourage captioning models to produce more detailed captions using natural language inference, based on the motivation that, among different captions of an image, descriptive captions are more likely to entail less descriptive captions. Specifically, we construct directed inference graphs for reference... | Zhan Shi, Hui Liu, Xiaodan Zhu |  |
| 74 |  |  [MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network](https://doi.org/10.18653/v1/2021.acl-short.37) |  | 0 | We present an instance-based nearest neighbor approach to entity linking. In contrast to most prior entity retrieval systems which represent each entity with a single vector, we build a contextualized mention-encoder that learns to place similar mentions of the same entity closer in vector space than mentions of different entities. This approach allows all mentions of an entity to serve as “class prototypes” as inference involves retrieving from the full set of labeled entity mentions in the... | Nicholas FitzGerald, Daniel M. Bikel, Jan A. Botha, Daniel Gillick, Tom Kwiatkowski, Andrew McCallum |  |
| 75 |  |  [eMLM: A New Pre-training Objective for Emotion Related Tasks](https://doi.org/10.18653/v1/2021.acl-short.38) |  | 0 | BERT has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of BERT do not induce any sentiment or emotion-specific biases into the model. In this paper, we present Emotion Masked Language Modelling, a variation of Masked Language Modelling aimed at improving the BERT language representation model for emotion detection and sentiment analysis tasks. Using the... | Tiberiu Sosea, Cornelia Caragea |  |
| 76 |  |  [On Positivity Bias in Negative Reviews](https://doi.org/10.18653/v1/2021.acl-short.39) |  | 0 | Prior work has revealed that positive words occur more frequently than negative words in human expressions, which is typically attributed to positivity bias, a tendency for people to report positive views of reality. But what about the language used in negative reviews? Consistent with prior work, we show that English negative reviews tend to contain more positive words than negative words, using a variety of datasets. We reconcile this observation with prior findings on the pragmatics of... | Madhusudhan Aithal, Chenhao Tan |  |
| 77 |  |  [PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation](https://doi.org/10.18653/v1/2021.acl-short.40) |  | 0 | Large pre-trained language generation models such as GPT-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a Pre-trainedRole Alternating Language model (PRAL), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge... | Jing Gu, Qingyang Wu, Chongruo Wu, Weiyan Shi, Zhou Yu |  |
| 78 |  |  [ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction](https://doi.org/10.18653/v1/2021.acl-short.41) |  | 0 | Natural reading orders of words are crucial for information extraction from form-like documents. Despite recent advances in Graph Convolutional Networks (GCNs) on modeling spatial layout patterns of documents, they have limited ability to capture reading orders of given word-level node representations in a graph. We propose Reading Order Equivariant Positional Encoding (ROPE), a new positional encoding technique designed to apprehend the sequential presentation of words in documents. ROPE... | ChenYu Lee, ChunLiang Li, Chu Wang, Renshen Wang, Yasuhisa Fujii, Siyang Qin, Ashok C. Popat, Tomas Pfister |  |
| 79 |  |  [Zero-shot Event Extraction via Transfer Learning: Challenges and Insights](https://doi.org/10.18653/v1/2021.acl-short.42) |  | 0 | Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. “A city was attacked” entails “There is an attack”), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves... | Qing Lyu, Hongming Zhang, Elior Sulem, Dan Roth |  |
| 80 |  |  [Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models](https://doi.org/10.18653/v1/2021.acl-short.43) |  | 0 | Pre-trained language models have achieved human-level performance on many Machine Reading Comprehension (MRC) tasks, but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. Here, we demonstrate a simple yet effective method to attack MRC models and reveal the statistical biases in these models. We apply the method to the RACE dataset, for which the answer to each MRC question is selected from 4 options. It is found... | Jieyu Lin, Jiajie Zou, Nai Ding |  |
| 81 |  |  [Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing](https://doi.org/10.18653/v1/2021.acl-short.44) |  | 0 | Extensive work has argued in favour of paying crowd workers a wage that is at least equivalent to the U.S. federal minimum wage. Meanwhile, research on collecting high quality annotations suggests using a qualification that requires workers to have previously completed a certain number of tasks. If most requesters who pay fairly require workers to have completed a large number of tasks already then workers need to complete a substantial amount of poorly paid work before they can earn a fair... | Jonathan K. Kummerfeld |  |
| 82 |  |  [Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia](https://doi.org/10.18653/v1/2021.acl-short.45) |  | 0 | Human activities can be seen as sequences of events, which are crucial to understanding societies. Disproportional event distribution for different demographic groups can manifest and amplify social stereotypes, and potentially jeopardize the ability of members in some groups to pursue certain goals. In this paper, we present the first event-centric study of gender biases in a Wikipedia corpus. To facilitate the study, we curate a corpus of career and personal life descriptions with demographic... | Jiao Sun, Nanyun Peng |  |
| 83 |  |  [Modeling Task-Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.46) |  | 0 | Neural machine translation has achieved great success in bilingual settings, as well as in multilingual settings. With the increase of the number of languages, multilingual systems tend to underperform their bilingual counterparts. Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics. Previous work increases the modeling capacity by deepening or widening the Transformer. However, modeling cardinality based on... | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong |  |
| 84 |  |  [Adaptive Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.47) |  | 0 | kNN-MT, recently proposed by Khandelwal et al. (2020a), successfully combines pre-trained neural machine translation (NMT) model with token-level k-nearest-neighbor (kNN) retrieval to improve the translation accuracy. However, the traditional kNN algorithm used in kNN-MT simply retrieves a same number of nearest neighbors for each target token, which may cause prediction errors when the retrieved neighbors include noises. In this paper, we propose Adaptive kNN-MT to dynamically determine the... | Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang, Boxing Chen, Weihua Luo, Jiajun Chen |  |
| 85 |  |  [On Orthogonality Constraints for Transformers](https://doi.org/10.18653/v1/2021.acl-short.48) |  | 0 | Orthogonality constraints encourage matrices to be orthogonal for numerical stability. These plug-and-play constraints, which can be conveniently incorporated into model training, have been studied for popular architectures in natural language processing, such as convolutional neural networks and recurrent neural networks. However, a dedicated study on such constraints for transformers has been absent. To fill this gap, this paper studies orthogonality constraints for transformers, showing the... | Aston Zhang, Alvin Chan, Yi Tay, Jie Fu, Shuohang Wang, Shuai Zhang, Huajie Shao, Shuochao Yao, Roy KaWei Lee |  |
| 86 |  |  [Measuring and Improving BERT's Mathematical Abilities by Predicting the Order of Reasoning](https://doi.org/10.18653/v1/2021.acl-short.49) |  | 0 | Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical... | Piotr Piekos, Mateusz Malinowski, Henryk Michalewski |  |
| 87 |  |  [Happy Dance, Slow Clap: Using Reaction GIFs to Predict Induced Affect on Twitter](https://doi.org/10.18653/v1/2021.acl-short.50) |  | 0 | Datasets with induced emotion labels are scarce but of utmost importance for many NLP tasks. We present a new, automated method for collecting texts along with their induced reaction labels. The method exploits the online use of reaction GIFs, which capture complex affective states. We show how to augment the data with induced emotion and induced sentiment labels. We use our method to create and publish ReactionGIF, a first-of-its-kind affective dataset of 30K tweets. We provide baselines for... | Boaz Shmueli, Soumya Ray, LunWei Ku |  |
| 88 |  |  [Exploring Listwise Evidence Reasoning with T5 for Fact Verification](https://doi.org/10.18653/v1/2021.acl-short.51) |  | 0 | This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex... | Kelvin Jiang, Ronak Pradeep, Jimmy Lin |  |
| 89 |  |  [DefSent: Sentence Embeddings using Definition Sentences](https://doi.org/10.18653/v1/2021.acl-short.52) |  | 0 | Sentence embedding methods using natural language inference (NLI) datasets have been successfully applied to various tasks. However, these methods are only available for limited languages due to relying heavily on the large NLI datasets. In this paper, we propose DefSent, a sentence embedding method that uses definition sentences from a word dictionary, which performs comparably on unsupervised semantics textual similarity (STS) tasks and slightly better on SentEval tasks than conventional... | Hayato Tsukagoshi, Ryohei Sasano, Koichi Takeda |  |
| 90 |  |  [Discrete Cosine Transform as Universal Sentence Encoder](https://doi.org/10.18653/v1/2021.acl-short.53) |  | 0 | Modern sentence encoders are used to generate dense vector representations that capture the underlying linguistic characteristics for a sequence of words, including phrases, sentences, or paragraphs. These kinds of representations are ideal for training a classifier for an end task such as sentiment analysis, question answering and text classification. Different models have been proposed to efficiently generate general purpose sentence representations to be used in pretraining protocols. While... | Nada AlMarwani, Mona T. Diab |  |
| 91 |  |  [AligNarr: Aligning Narratives on Movies](https://doi.org/10.18653/v1/2021.acl-short.54) |  | 0 | High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. Experimental results on ten movies show the viability of our method with 76% F1-score and... | Paramita Mirza, Mostafa Abouhamra, Gerhard Weikum |  |
| 92 |  |  [An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers](https://doi.org/10.18653/v1/2021.acl-short.55) |  | 0 | Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and... | Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov |  |
| 93 |  |  [Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models](https://doi.org/10.18653/v1/2021.acl-short.56) |  | 0 | A sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for Chinese Spelling Correction (CSC), which takes a sentence with some spelling errors as input and outputs the corrected one. However, CSC models may fail to correct spelling errors covered by the confusion sets, and also will encounter unseen ones. We propose a method, which continually identifies the weak spots of a model to generate more valuable training instances, and apply a... | Chong Li, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang |  |
| 94 |  |  [Training Adaptive Computation for Open-Domain Question Answering with Computational Constraints](https://doi.org/10.18653/v1/2021.acl-short.57) |  | 0 | Adaptive Computation (AC) has been shown to be effective in improving the efficiency of Open-Domain Question Answering (ODQA) systems. However, the current AC approaches require tuning of all model parameters, and training state-of-the-art ODQA models requires significant computational resources that may not be available for most researchers. We propose Adaptive Passage Encoder, an AC method that can be applied to an existing ODQA model and can be trained efficiently on a single GPU. It keeps... | Yuxiang Wu, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel |  |
| 95 |  |  [An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter](https://doi.org/10.18653/v1/2021.acl-short.58) |  | 0 | In this paper, we empirically investigate adversarial attack on NMT from two aspects: languages (the source vs. the target language) and positions (front vs. rear). For autoregressive NMT models that generate target words from left to right, we observe that adversarial attack on the source language is more effective than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front positions of corresponding target sentences... | Zhiyuan Zeng, Deyi Xiong |  |
| 96 |  |  [OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres](https://doi.org/10.18653/v1/2021.acl-short.59) |  | 0 | SOTA coreference resolution produces increasingly impressive scores on the OntoNotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. This paper provides a dataset and comprehensive evaluation showing that the latest neural LM based end-to-end systems degrade very substantially out of domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly available, converted from GUM,... | Yilun Zhu, Sameer Pradhan, Amir Zeldes |  |
| 97 |  |  [In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering](https://doi.org/10.18653/v1/2021.acl-short.60) |  | 0 | Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark... | Peter Vickers, Nikolaos Aletras, Emilio Monti, Loïc Barrault |  |
| 98 |  |  [Zero-shot Fact Verification by Claim Generation](https://doi.org/10.18653/v1/2021.acl-short.61) |  | 0 | Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from... | Liangming Pan, Wenhu Chen, Wenhan Xiong, MinYen Kan, William Yang Wang |  |
| 99 |  |  [Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer](https://doi.org/10.18653/v1/2021.acl-short.62) |  | 0 | Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content –the two core aspects of the task– we achieve a new state-of-the-art. | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 100 |  |  [Deep Context- and Relation-Aware Learning for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-short.63) |  | 0 | Existing works for aspect-based sentiment analysis (ABSA) have adopted a unified approach, which allows the interactive relations among subtasks. However, we observe that these methods tend to predict polarities based on the literal meaning of aspect and opinion terms and mainly consider relations implicitly among subtasks at the word level. In addition, identifying multiple aspect–opinion pairs with their polarities is much more challenging. Therefore, a comprehensive understanding of... | Shinhyeok Oh, Dongyub Lee, Taesun Whang, IlNam Park, Gaeun Seo, EungGyun Kim, Harksoo Kim |  |
| 101 |  |  [Towards Generative Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-short.64) |  | 0 | Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and... | Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, Wai Lam |  |
| 102 |  |  [Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.65) |  | 0 | Recently, token-level adaptive training has achieved promising improvement in machine translation, where the cross-entropy loss function is adjusted by assigning different training weights to different tokens, in order to alleviate the token imbalance problem. However, previous approaches only use static word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks like machine translation. In this paper, we propose a novel... | Yangyifan Xu, Yijin Liu, Fandong Meng, Jiajun Zhang, Jinan Xu, Jie Zhou |  |
| 103 |  |  [Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking](https://doi.org/10.18653/v1/2021.acl-short.66) |  | 0 | This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. This paper proposes an effective continual learning method for the task-oriented dialogue system with iterative network pruning, expanding, and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Specifically, TPEM (i) leverages network pruning to keep the... | Binzong Geng, Fajie Yuan, Qiancheng Xu, Ying Shen, Ruifeng Xu, Min Yang |  |
| 104 |  |  [TIMERS: Document-level Temporal Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.67) |  | 0 | We present TIMERS - a TIME, Rhetorical and Syntactic-aware model for document-level temporal relation classification in the English language. Our proposed method leverages rhetorical discourse features and temporal arguments from semantic role labels, in addition to traditional local syntactic features, trained through a Gated Relational-GCN. Extensive experiments show that the proposed model outperforms previous methods by 5-18% on the TDDiscourse, TimeBank-Dense, and MATRES datasets due to... | Puneet Mathur, Rajiv Jain, Franck Dernoncourt, Vlad I. Morariu, Quan Hung Tran, Dinesh Manocha |  |
| 105 |  |  [Improving Arabic Diacritization with Regularized Decoding and Adversarial Training](https://doi.org/10.18653/v1/2021.acl-short.68) |  | 0 | Arabic diacritization is a fundamental task for Arabic language processing. Previous studies have demonstrated that automatically generated knowledge can be helpful to this task. However, these studies regard the auto-generated knowledge instances as gold references, which limits their effectiveness since such knowledge is not always accurate and inferior instances can lead to incorrect predictions. In this paper, we propose to use regularized decoding and adversarial training to appropriately... | Han Qin, Guimin Chen, Yuanhe Tian, Yan Song |  |
| 106 |  |  [When is Char Better Than Subword: A Systematic Study of Segmentation Algorithms for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-short.69) |  | 0 | Subword segmentation algorithms have been a de facto choice when building neural machine translation systems. However, most of them need to learn a segmentation model based on some heuristics, which may produce sub-optimal segmentation. This can be problematic in some scenarios when the target language has rich morphological changes or there is not enough data for learning compact composition rules. Translating at fully character level has the potential to alleviate the issue, but empirical... | Jiahuan Li, Yutong Shen, Shujian Huang, Xinyu Dai, Jiajun Chen |  |
| 107 |  |  [More than Text: Multi-modal Chinese Word Segmentation](https://doi.org/10.18653/v1/2021.acl-short.70) |  | 0 | Chinese word segmentation (CWS) is undoubtedly an important basic task in natural language processing. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new dataset for CWS... | Dong Zhang, Zheng Hu, Shoushan Li, Hanqian Wu, Qiaoming Zhu, Guodong Zhou |  |
| 108 |  |  [A Mixture-of-Experts Model for Antonym-Synonym Discrimination](https://doi.org/10.18653/v1/2021.acl-short.71) |  | 0 | Discrimination between antonyms and synonyms is an important and challenging NLP task. Antonyms and synonyms often share the same or similar contexts and thus are hard to make a distinction. This paper proposes two underlying hypotheses and employs the mixture-of-experts framework as a solution. It works on the basis of a divide-and-conquer strategy, where a number of localized experts focus on their own domains (or subspaces) to learn their specialties, and a gating mechanism determines the... | Zhipeng Xie, Nan Zeng |  |
| 109 |  |  [Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking](https://doi.org/10.18653/v1/2021.acl-short.72) |  | 0 | Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate... | Fangyu Liu, Ivan Vulic, Anna Korhonen, Nigel Collier |  |
| 110 |  |  [A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space](https://doi.org/10.18653/v1/2021.acl-short.73) |  | 0 | The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due... | Sara Rajaee, Mohammad Taher Pilehvar |  |
| 111 |  |  [Unsupervised Enrichment of Persona-grounded Dialog with Background Stories](https://doi.org/10.18653/v1/2021.acl-short.74) |  | 0 | Humans often refer to personal narratives, life experiences, and events to make a conversation more engaging and rich. While persona-grounded dialog models are able to generate responses that follow a given persona, they often miss out on stating detailed experiences or events related to a persona, often leaving conversations shallow and dull. In this work, we equip dialog models with ‘background stories’ related to a persona by leveraging fictional narratives from existing story datasets (e.g.... | Bodhisattwa Prasad Majumder, Taylor BergKirkpatrick, Julian J. McAuley, Harsh Jhamtani |  |
| 112 |  |  [Beyond Laurel/Yanny: An Autoencoder-Enabled Search for Polyperceivable Audio](https://doi.org/10.18653/v1/2021.acl-short.75) |  | 0 | The famous “laurel/yanny” phenomenon references an audio clip that elicits dramatically different responses from different listeners. For the original clip, roughly half the population hears the word “laurel,” while the other half hears “yanny.” How common are such “polyperceivable” audio clips? In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language. We devise a metric that correlates with polyperceivability of audio clips, use it to efficiently... | Kartik Chandra, Chuma Kabaghe, Gregory Valiant |  |
| 113 |  |  [Don't Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models](https://doi.org/10.18653/v1/2021.acl-short.76) |  | 0 | Event language models represent plausible sequences of events. Most existing approaches train autoregressive models on text, which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of ordering (e.g., temporal) or not care about ordering at all (e.g., when predicting related events in a... | Mahnaz Koupaee, Greg Durrett, Nathanael Chambers, Niranjan Balasubramanian |  |
| 114 |  |  [The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes](https://doi.org/10.18653/v1/2021.acl-short.77) |  | 0 | Information Retrieval using dense low-dimensional representations recently became popular and showed out-performance to traditional sparse-representations like BM25. However, no previous work investigated how dense representations perform with large index sizes. We show theoretically and empirically that the performance for dense representations decreases quicker than sparse representations for increasing index sizes. In extreme cases, this can even lead to a tipping point where at a certain... | Nils Reimers, Iryna Gurevych |  |
| 115 |  |  [Cross-lingual Text Classification with Heterogeneous Graph Neural Network](https://doi.org/10.18653/v1/2021.acl-short.78) |  | 0 | Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate... | Ziyun Wang, Xuan Liu, Peiji Yang, Shixing Liu, Zhisheng Wang |  |
| 116 |  |  [Towards more equitable question answering systems: How much more data do you need?](https://doi.org/10.18653/v1/2021.acl-short.79) |  | 0 | Question answering (QA) in English has been widely explored, but multilingual datasets are relatively new, with several methods attempting to bridge the gap between high- and low-resourced languages using data augmentation through translation and cross-lingual transfer. In this project we take a step back and study which approaches allow us to take the most advantage of existing resources in order to produce QA systems in many languages. Specifically, we perform extensive analysis to measure... | Arnab Debnath, Navid Rajabi, Fardina Fathmiul Alam, Antonios Anastasopoulos |  |
| 117 |  |  [Embedding Time Differences in Context-sensitive Neural Networks for Learning Time to Event](https://doi.org/10.18653/v1/2021.acl-short.80) |  | 0 | We propose an effective context-sensitive neural model for time to event (TTE) prediction task, which aims to predict the amount of time to/from the occurrence of given events in streaming content. We investigate this problem in the context of a multi-task learning framework, which we enrich with time difference embeddings. In addition, we develop a multi-genre dataset of English events about soccer competitions and academy awards ceremonies, and their relevant tweets obtained from Twitter. Our... | Nazanin Dehghani, Hassan Hajipoor, Hadi Amiri |  |
| 118 |  |  [Improving Compositional Generalization in Classification Tasks via Structure Annotations](https://doi.org/10.18653/v1/2021.acl-short.81) |  | 0 | Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires... | Juyong Kim, Pradeep Ravikumar, Joshua Ainslie, Santiago Ontañón |  |
| 119 |  |  [Learning to Generate Task-Specific Adapters from Task Description](https://doi.org/10.18653/v1/2021.acl-short.82) |  | 0 | Pre-trained text-to-text transformers such as BART have achieved impressive performance across a range of NLP tasks. Recent study further shows that they can learn to generalize to novel tasks, by including task descriptions as part of the source sequence and training the model with (source, target) examples. At test time, these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. However, this approach has potential limitations, as the model... | Qinyuan Ye, Xiang Ren |  |
| 120 |  |  [QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining](https://doi.org/10.18653/v1/2021.acl-short.83) |  | 0 | Slot-filling is an essential component for building task-oriented dialog systems. In this work, we focus on the zero-shot slot-filling problem, where the model needs to predict slots and their values, given utterances from new domains without training on the target domain. Prior methods directly encode slot descriptions to generalize to unseen slot types. However, raw slot descriptions are often ambiguous and do not encode enough semantic information, limiting the models’ zero-shot capability.... | Xinya Du, Luheng He, Qi Li, Dian Yu, Panupong Pasupat, Yuan Zhang |  |
| 121 |  |  [Domain-Adaptive Pretraining Methods for Dialogue Understanding](https://doi.org/10.18653/v1/2021.acl-short.84) |  | 0 | Language models like BERT and SpanBERT pretrained on open-domain data have obtained impressive gains on various NLP tasks. In this paper, we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. In particular, three objectives, including a novel objective focusing on modeling predicate-argument relations, are evaluated on two challenging dialogue understanding tasks. Experimental results demonstrate that domain-adaptive pretraining with proper objectives can... | Han Wu, Kun Xu, Linfeng Song, Lifeng Jin, Haisong Zhang, Linqi Song |  |
| 122 |  |  [Targeting the Benchmark: On Methodology in Current Natural Language Processing Research](https://doi.org/10.18653/v1/2021.acl-short.85) |  | 0 | It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment... | David Schlangen |  |
| 123 |  |  [X-Fact: A New Benchmark Dataset for Multilingual Fact Checking](https://doi.org/10.18653/v1/2021.acl-short.86) |  | 0 | In this work, we introduce : the largest publicly available multilingual dataset for factual verification of naturally existing real-world claims. The dataset contains short statements in 25 languages and is labeled for veracity by expert fact-checkers. The dataset includes a multilingual evaluation benchmark that measures both out-of-domain generalization, and zero-shot capabilities of the multilingual models. Using state-of-the-art multilingual transformer-based models, we develop several... | Ashim Gupta, Vivek Srikumar |  |
| 124 |  |  [nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?](https://doi.org/10.18653/v1/2021.acl-short.87) |  | 0 | Recently, mT5 - a massively multilingual version of T5 - leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as machine translation during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains... | Mihir Kale, Aditya Siddhant, Rami AlRfou, Linting Xue, Noah Constant, Melvin Johnson |  |
| 125 |  |  [Question Generation for Adaptive Education](https://doi.org/10.18653/v1/2021.acl-short.88) |  | 0 | Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students. However, existing systems usually depend on a pool of hand-made questions, limiting how fine-grained and open-ended they can be in adapting to individual students. We explore targeted question generation as a controllable sequence generation task. We first show how to fine-tune pre-trained language models for deep knowledge tracing (LM-KT). This model accurately... | Megha Srivastava, Noah D. Goodman |  |
| 126 |  |  [A Simple Recipe for Multilingual Grammatical Error Correction](https://doi.org/10.18653/v1/2021.acl-short.89) |  | 0 | This paper presents a simple recipe to trainstate-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and... | Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, Aliaksei Severyn |  |
| 127 |  |  [Towards Visual Question Answering on Pathology Images](https://doi.org/10.18653/v1/2021.acl-short.90) |  | 0 | Pathology imaging is broadly used for identifying the causes and effects of diseases or injuries. Given a pathology image, being able to answer questions about the clinical findings contained in the image is very important for medical decision making. In this paper, we aim to develop a pathological visual question answering framework to analyze pathology images and answer medical questions related to these images. To build such a framework, we create PathVQA, a VQA dataset with 32,795 questions... | Xuehai He, Zhuo Cai, Wenlan Wei, Yichen Zhang, Luntian Mou, Eric P. Xing, Pengtao Xie |  |
| 128 |  |  [Efficient Text-based Reinforcement Learning by Jointly Leveraging State and Commonsense Graph Representations](https://doi.org/10.18653/v1/2021.acl-short.91) |  | 0 | Text-based games (TBGs) have emerged as useful benchmarks for evaluating progress at the intersection of grounded language understanding and reinforcement learning (RL). Recent work has proposed the use of external knowledge to improve the efficiency of RL agents for TBGs. In this paper, we posit that to act efficiently in TBGs, an agent must be able to track the state of the game while retrieving and using relevant commonsense knowledge. Thus, we propose an agent for TBGs that induces a graph... | Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell |  |
| 129 |  |  [mTVR: Multilingual Moment Retrieval in Videos](https://doi.org/10.18653/v1/2021.acl-short.92) |  | 0 | We introduce mTVR, a large-scale multilingual video moment retrieval dataset, containing 218K English and Chinese queries from 21.8K TV show video clips. The dataset is collected by extending the popular TVR dataset (in English) with paired Chinese queries and subtitles. Compared to existing moment retrieval datasets, mTVR is multilingual, larger, and comes with diverse annotations. We further propose mXML, a multilingual moment retrieval model that learns and operates on data from both... | Jie Lei, Tamara L. Berg, Mohit Bansal |  |
| 130 |  |  [Explicitly Capturing Relations between Entity Mentions via Graph Neural Networks for Domain-specific Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-short.93) |  | 0 | Named entity recognition (NER) is well studied for the general domain, and recent systems have achieved human-level performance for identifying common entity types. However, the NER performance is still moderate for specialized domains that tend to feature complicated contexts and jargonistic entity types. To address these challenges, we propose explicitly connecting entity mentions based on both global coreference relations and local dependency relations for building better entity mention... | Pei Chen, Haibo Ding, Jun Araki, Ruihong Huang |  |
| 131 |  |  [Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction](https://doi.org/10.18653/v1/2021.acl-short.94) |  | 0 | Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni- and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domain-specific... | Gyubok Lee, Seongjun Yang, Edward Choi |  |
| 132 |  |  [Quotation Recommendation and Interpretation Based on Transformation from Queries to Quotations](https://doi.org/10.18653/v1/2021.acl-short.95) |  | 0 | To help individuals express themselves better, quotation recommendation is receiving growing attention. Nevertheless, most prior efforts focus on modeling quotations and queries separately and ignore the relationship between the quotations and the queries. In this work, we introduce a transformation matrix that directly maps the query representations to quotation representations. To better learn the mapping relationship, we employ a mapping loss that minimizes the distance of two semantic... | Lingzhi Wang, Xingshan Zeng, KamFai Wong |  |
| 133 |  |  [Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence](https://doi.org/10.18653/v1/2021.acl-short.96) |  | 0 | Topic models extract groups of words from documents, whose interpretation as a topic hopefully allows for a better understanding of the data. However, the resulting word groups are often not coherent, making them harder to interpret. Recently, neural topic models have shown improvements in overall coherence. Concurrently, contextual embeddings have advanced the state of the art of neural models in general. In this paper, we combine contextualized representations with neural topic models. We... | Federico Bianchi, Silvia Terragni, Dirk Hovy |  |
| 134 |  |  [Input Representations for Parsing Discourse Representation Structures: Comparing English with Chinese](https://doi.org/10.18653/v1/2021.acl-short.97) |  | 0 | Neural semantic parsers have obtained acceptable results in the context of parsing DRSs (Discourse Representation Structures). In particular models with character sequences as input showed remarkable performance for English. But how does this approach perform on languages with a different writing system, like Chinese, a language with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are... | Chunliu Wang, Rik van Noord, Arianna Bisazza, Johan Bos |  |
| 135 |  |  [Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data](https://doi.org/10.18653/v1/2021.acl-short.98) |  | 0 | Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance... | Sajad Norouzi, Keyi Tang, Yanshuai Cao |  |
| 136 |  |  [Issues with Entailment-based Zero-shot Text Classification](https://doi.org/10.18653/v1/2021.acl-short.99) |  | 0 | The general format of natural language inference (NLI) makes it tempting to be used for zero-shot text classification by casting any target label into a sentence of hypothesis and verifying whether or not it could be entailed by the input, aiming at generic classification applicable on any specified label space. In this opinion piece, we point out a few overlooked issues that are yet to be discussed in this line of work. We observe huge variance across different classification datasets amongst... | Tingting Ma, JinGe Yao, ChinYew Lin, Tiejun Zhao |  |
| 137 |  |  [Neural-Symbolic Commonsense Reasoner with Relation Predictors](https://doi.org/10.18653/v1/2021.acl-short.100) |  | 0 | Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited... | Farhad Moghimifar, Lizhen Qu, Yue Zhuo, Gholamreza Haffari, Mahsa Baktashmotlagh |  |
| 138 |  |  [What Motivates You? Benchmarking Automatic Detection of Basic Needs from Short Posts](https://doi.org/10.18653/v1/2021.acl-short.101) |  | 0 | According to the self-determination theory, the levels of satisfaction of three basic needs (competence, autonomy and relatedness) have implications on people’s everyday life and career. We benchmark the novel task of automatically detecting those needs on short posts in English, by modelling it as a ternary classification task, and as three binary classification tasks. A detailed manual analysis shows that the latter has advantages in the real-world scenario, and that our best models achieve... | Sanja Stajner, Seren Yenikent, Bilal Ghanem, Marc FrancoSalvador |  |
| 139 |  |  [Semantic Frame Induction using Masked Word Embeddings and Two-Step Clustering](https://doi.org/10.18653/v1/2021.acl-short.102) |  | 0 | Recent studies on semantic frame induction show that relatively high performance has been achieved by using clustering-based methods with contextualized word embeddings. However, there are two potential drawbacks to these methods: one is that they focus too much on the superficial information of the frame-evoking verb and the other is that they tend to divide the instances of the same verb into too many different frame clusters. To overcome these drawbacks, we propose a semantic frame induction... | Kosuke Yamada, Ryohei Sasano, Koichi Takeda |  |
| 140 |  |  [Lightweight Adapter Tuning for Multilingual Speech Translation](https://doi.org/10.18653/v1/2021.acl-short.103) |  | 0 | Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP. Adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers, resulting in the addition of only a small number of task-specific trainable parameters. While adapter tuning was investigated for multilingual neural machine translation, this paper proposes a comprehensive analysis of adapters for multilingual speech translation (ST). Starting from... | Hang Le, Juan Miguel Pino, Changhan Wang, Jiatao Gu, Didier Schwab, Laurent Besacier |  |
| 141 |  |  [Parameter Selection: Why We Should Pay More Attention to It](https://doi.org/10.18653/v1/2021.acl-short.104) |  | 0 | The importance of parameter selection in supervised learning is well known. However, due to the many parameter combinations, an incomplete or an insufficient procedure is often applied. This situation may cause misleading or confusing conclusions. In this opinion paper, through an intriguing example we point out that the seriousness goes beyond what is generally recognized. In the topic of multilabel classification for medical code prediction, one influential paper conducted a proper parameter... | JieJyun Liu, TsungHan Yang, SiAn Chen, ChihJen Lin |  |
| 142 |  |  [Distinct Label Representations for Few-Shot Text Classification](https://doi.org/10.18653/v1/2021.acl-short.105) |  | 0 | Few-shot text classification aims to classify inputs whose label has only a few examples. Previous studies overlooked the semantic relevance between label representations. Therefore, they are easily confused by labels that are relevant. To address this problem, we propose a method that generates distinct label representations that embed information specific to each label. Our method is applicable to conventional few-shot classification models. Experimental results show that our method... | Sora Ohashi, Junya Takayama, Tomoyuki Kajiwara, Yuki Arase |  |
| 143 |  |  [Learning to Solve NLP Tasks in an Incremental Number of Languages](https://doi.org/10.18653/v1/2021.acl-short.106) |  | 0 | In real scenarios, a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time. Unfortunately, the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming, especially when the number of target languages grows. Moreover, the original annotated material may no longer be available due to storage or business constraints. Re-training only with the new language data... | Giuseppe Castellucci, Simone Filice, Danilo Croce, Roberto Basili |  |
| 144 |  |  [Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling](https://doi.org/10.18653/v1/2021.acl-short.107) |  | 0 | Transformer is important for text modeling. However, it has difficulty in handling long documents due to the quadratic complexity with input text length. In order to handle this problem, we propose a hierarchical interactive Transformer (Hi-Transformer) for efficient and effective long document modeling. Hi-Transformer models documents in a hierarchical way, i.e., first learns sentence representations and then learns document representations. It can effectively reduce the complexity and... | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 145 |  |  [Robust Transfer Learning with Pretrained Language Models through Adapters](https://doi.org/10.18653/v1/2021.acl-short.108) |  | 0 | Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple... | Wenjuan Han, Bo Pang, Ying Nian Wu |  |
| 146 |  |  [Embracing Ambiguity: Shifting the Training Target of NLI Models](https://doi.org/10.18653/v1/2021.acl-short.109) |  | 0 | Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels. While many research works do not pay much attention to this fact, several recent efforts have been made to acknowledge and embrace the existence of ambiguity, such as UNLI and ChaosNLI. In this paper, we explore the option of training directly on the estimated label distribution of the annotators in the NLI task, using a learning loss based on this ambiguity distribution instead of the gold-labels. We... | Johannes Mario Meissner, Napat Thumwanit, Saku Sugawara, Akiko Aizawa |  |
| 147 |  |  [Modeling Discriminative Representations for Out-of-Domain Detection with Supervised Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-short.110) |  | 0 | Detecting Out-of-Domain (OOD) or unknown intents from user queries is essential in a task-oriented dialog system. A key challenge of OOD detection is to learn discriminative semantic features. Traditional cross-entropy loss only focuses on whether a sample is correctly classified, and does not explicitly distinguish the margins between categories. In this paper, we propose a supervised contrastive learning objective to minimize intra-class variance by pulling together in-domain intents... | Zhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu, Yanan Wu, Hong Xu, Huixing Jiang, Weiran Xu |  |
| 148 |  |  [Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-short.111) |  | 0 | Existing dialog state tracking (DST) models are trained with dialog data in a random order, neglecting rich structural information in a dataset. In this paper, we propose to use curriculum learning (CL) to better leverage both the curriculum structure and schema structure for task-oriented dialogs. Specifically, we propose a model-agnostic framework called Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), which consists of a preview module that pre-trains a DST model with... | Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Xiaodan Zhu |  |
| 149 |  |  [On the Generation of Medical Dialogs for COVID-19](https://doi.org/10.18653/v1/2021.acl-short.112) |  | 0 | Under the pandemic of COVID-19, people experiencing COVID19-related symptoms have a pressing need to consult doctors. Because of the shortage of medical professionals, many people cannot receive online consultations timely. To address this problem, we aim to develop a medical dialog system that can provide COVID19-related consultations. We collected two dialog datasets – CovidDialog – (in English and Chinese respectively) containing conversations between doctors and patients about COVID-19.... | Meng Zhou, Zechen Li, Bowen Tan, Guangtao Zeng, Wenmian Yang, Xuehai He, Zeqian Ju, Subrato Chakravorty, Shu Chen, Xingyi Yang, Yichen Zhang, Qingyang Wu, Zhou Yu, Kun Xu, Eric P. Xing, Pengtao Xie |  |
| 150 |  |  [Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images](https://doi.org/10.18653/v1/2021.acl-short.113) |  | 0 | In multi-modal dialogue systems, it is important to allow the use of images as part of a multi-turn conversation. Training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images, but such datasets rarely exist. In response, this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. Our method to create such a dataset consists of (1) preparing and pre-processing text dialogue datasets, (2)... | Nyoungwoo Lee, Suwon Shin, Jaegul Choo, HoJin Choi, SungHyon Myaeng |  |
| 151 |  |  [Exposing the limits of Zero-shot Cross-lingual Hate Speech Detection](https://doi.org/10.18653/v1/2021.acl-short.114) |  | 0 | Reducing and counter-acting hate speech on Social Media is a significant concern. Most of the proposed automatic methods are conducted exclusively on English and very few consistently labeled, non-English resources have been proposed. Learning to detect hate speech on English and transferring to unseen languages seems an immediate solution. This work is the first to shed light on the limits of this zero-shot, cross-lingual transfer learning framework for hate speech detection. We use benchmark... | Debora Nozza |  |
| 152 |  |  [BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://doi.org/10.18653/v1/2021.acl-short.115) |  | 0 | Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the... | Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi |  |
| 153 |  |  [Entity Enhancement for Implicit Discourse Relation Classification in the Biomedical Domain](https://doi.org/10.18653/v1/2021.acl-short.116) |  | 0 | Implicit discourse relation classification is a challenging task, in particular when the text domain is different from the standard Penn Discourse Treebank (PDTB; Prasad et al., 2008) training corpus domain (Wall Street Journal in 1990s). We here tackle the task of implicit discourse relation classification on the biomedical domain, for which the Biomedical Discourse Relation Bank (BioDRB; Prasad et al., 2011) is available. We show that entity information can be used to improve discourse... | Wei Shi, Vera Demberg |  |
| 154 |  |  [Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction](https://doi.org/10.18653/v1/2021.acl-short.117) |  | 0 | In this work, we propose Masked Noun-Phrase Prediction (MNPP), a pre-training strategy to tackle pronoun resolution in a fully unsupervised setting. Firstly, We evaluate our pre-trained model on various pronoun resolution datasets without any finetuning. Our method outperforms all previous unsupervised methods on all datasets by large margins. Secondly, we proceed to a few-shot setting where we finetune our pre-trained model on WinoGrande-S and XS separately. Our method outperforms... | Ming Shen, Pratyay Banerjee, Chitta Baral |  |
| 155 |  |  [Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction](https://doi.org/10.18653/v1/2021.acl-short.118) |  | 0 | Recently, question answering (QA) based on machine reading comprehension has become popular. This work focuses on generative QA which aims to generate an abstractive answer to a given question instead of extracting an answer span from a provided passage. Generative QA often suffers from two critical problems: (1) summarizing content irrelevant to a given question, (2) drifting away from a correct answer during generation. In this paper, we address these problems by a novel Rationale-Enriched... | Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang |  |
| 156 |  |  [Demoting the Lead Bias in News Summarization via Alternating Adversarial Learning](https://doi.org/10.18653/v1/2021.acl-short.119) |  | 0 | In news articles the lead bias is a common phenomenon that usually dominates the learning signals for neural extractive summarizers, severely limiting their performance on data with different or even no bias. In this paper, we introduce a novel technique to demote lead bias and make the summarizer focus more on the content semantics. Experiments on two news corpora with different degrees of lead bias show that our method can effectively demote the model’s learned lead bias and improve its... | Linzi Xing, Wen Xiao, Giuseppe Carenini |  |
| 157 |  |  [DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications](https://doi.org/10.18653/v1/2021.acl-short.120) |  | 0 | Machine reading comprehension (MRC) is a crucial task in natural language processing and has achieved remarkable advancements. However, most of the neural MRC models are still far from robust and fail to generalize well in real-world applications. In order to comprehensively verify the robustness and generalization of MRC models, we introduce a real-world Chinese dataset – DuReader_robust . It is designed to evaluate the MRC models from three aspects: over-sensitivity, over-stability and... | Hongxuan Tang, Hongyu Li, Jing Liu, Yu Hong, Hua Wu, Haifeng Wang |  |
| 158 |  |  [Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving](https://doi.org/10.18653/v1/2021.acl-short.121) |  | 0 | With the recent advancements in deep learning, neural solvers have gained promising results in solving math word problems. However, these SOTA solvers only generate binary expression trees that contain basic arithmetic operators and do not explicitly use the math formulas. As a result, the expression trees they produce are lengthy and uninterpretable because they need to use multiple operators and constants to represent one single formula. In this paper, we propose sequence-to-general tree... | Shihhung Tsai, ChaoChun Liang, HsinMin Wang, KehYih Su |  |
| 159 |  |  [Multi-Scale Progressive Attention Network for Video Question Answering](https://doi.org/10.18653/v1/2021.acl-short.122) |  | 0 | Understanding the multi-scale visual information in a video is essential for Video Question Answering (VideoQA). Therefore, we propose a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. We construct clips of different lengths to represent different scales of the video. Then, the clip-level features are aggregated into node features by using max-pool, and a graph is generated for each scale of clips. For cross-scale... | Zhicheng Guo, Jiaxuan Zhao, Licheng Jiao, Xu Liu, Lingling Li |  |
| 160 |  |  [Efficient Passage Retrieval with Hashing for Open-domain Question Answering](https://doi.org/10.18653/v1/2021.acl-short.123) |  | 0 | Most state-of-the-art open-domain question answering systems use a neural retrieval model to encode passages into continuous vectors and extract them from a knowledge source. However, such retrieval models often require large memory to run because of the massive size of their passage index. In this paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural retrieval model that integrates a learning-to-hash technique into the state-of-the-art Dense Passage Retriever (DPR) to... | Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi |  |
| 161 |  |  [Entity Concept-enhanced Few-shot Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.124) |  | 0 | Few-shot relation extraction (FSRE) is of great importance in long-tail distribution problem, especially in special domain with low-resource data. Most existing FSRE algorithms fail to accurately classify the relations merely based on the information of the sentences together with the recognized entity pairs, due to limited samples and lack of knowledge. To address this problem, in this paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction scheme (ConceptFERE), which... | Shan Yang, Yongfei Zhang, Guanglin Niu, Qinghua Zhao, Shiliang Pu |  |
| 162 |  |  [Improving Model Generalization: A Chinese Named Entity Recognition Case Study](https://doi.org/10.18653/v1/2021.acl-short.125) |  | 0 | Generalization is an important ability that helps to ensure that a machine learning model can perform well on unseen data. In this paper, we study the effect of data bias on model generalization, using Chinese Named Entity Recognition (NER) as a case study. Specifically, we analyzed five benchmarking datasets for Chinese NER, and observed the following two types of data bias that can compromise model generalization ability. Firstly, the test sets of all the five datasets contain a significant... | Guanqing Liang, Cane WingKi Leung |  |
| 163 |  |  [Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction](https://doi.org/10.18653/v1/2021.acl-short.126) |  | 0 | Document-level Relation Extraction (RE) is a more challenging task than sentence RE as it often requires reasoning over multiple sentences. Yet, human annotators usually use a small number of sentences to identify the relationship between a given entity pair. In this paper, we present an embarrassingly simple but effective method to heuristically select evidence sentences for document-level RE, which can be easily combined with BiLSTM to achieve good performance on benchmark datasets, even... | Quzhe Huang, Shengqi Zhu, Yansong Feng, Yuan Ye, Yuxuan Lai, Dongyan Zhao |  |
| 164 |  |  [Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders](https://doi.org/10.18653/v1/2021.acl-short.127) |  | 0 | Learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. For example, one may be an expert in the natural language processing (NLP) domain, but want to determine the best order in which to learn new concepts in an unfamiliar Computer Vision domain (CV). Both domains share some common concepts, such as machine learning basics and deep learning models. In this paper, we solve the task of unsupervised cross-domain concept... | Irene Li, Vanessa Yan, Tianxiao Li, Rihao Qu, Dragomir R. Radev |  |
| 165 |  |  [Attentive Multiview Text Representation for Differential Diagnosis](https://doi.org/10.18653/v1/2021.acl-short.128) |  | 0 | We present a text representation approach that can combine different views (representations) of the same input through effective data fusion and attention strategies for ranking purposes. We apply our model to the problem of differential diagnosis, which aims to find the most probable diseases that match with clinical descriptions of patients, using data from the Undiagnosed Diseases Network. Our model outperforms several ranking approaches (including a commercially-supported system) by... | Hadi Amiri, Mitra Mohtarami, Isaac S. Kohane |  |
| 166 |  |  [MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain](https://doi.org/10.18653/v1/2021.acl-short.129) |  | 0 | Crowdworker-constructed natural language inference (NLI) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis-only classifiers to achieve better-than-random performance (CITATION). We investigate whether MedNLI, a physician-annotated dataset with premises extracted from clinical notes, contains such artifacts (CITATION). We find that entailed hypotheses contain generic versions of specific concepts in the premise, as well as... | Christine Herlihy, Rachel Rudinger |  |
| 167 |  |  [Towards a more Robust Evaluation for Conversational Question Answering](https://doi.org/10.18653/v1/2021.acl-short.130) |  | 0 | With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers... | Wissam Siblini, Baris Sayil, Yacine Kessaci |  |
| 168 |  |  [VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.acl-short.131) |  | 0 | Existing models on Machine Reading Comprehension (MRC) require complex model architecture for effectively modeling long texts with paragraph representation and classification, thereby making inference computationally inefficient for production use. In this work, we propose VAULT: a light-weight and parallel-efficient paragraph representation for MRC based on contextualized representation from long document input, trained using a new Gaussian distribution-based objective that pays close... | Haoyang Wen, Anthony Ferritto, Heng Ji, Radu Florian, Avi Sil |  |
| 169 |  |  [Avoiding Overlap in Data Augmentation for AMR-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-short.132) |  | 0 | Leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing. For generation tasks, if there is overlap between the additional data and the target text evaluation data, then training on the additional data is training on answers of the test set. This leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models. We study the AMR dataset and Gigaword,... | Wenchao Du, Jeffrey Flanigan |  |
| 170 |  |  [Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains](https://doi.org/10.18653/v1/2021.acl-short.133) |  | 0 | Social media has become a valuable resource for the study of suicidal ideation and the assessment of suicide risk. Among social media platforms, Reddit has emerged as the most promising one due to its anonymity and its focus on topic-based communities (subreddits) that can be indicative of someone’s state of mind or interest regarding mental health disorders such as r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on suicide risk assessment has been the small amount of... | Chenghao Yang, Yudong Zhang, Smaranda Muresan |  |
| 171 |  |  [Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test](https://doi.org/10.18653/v1/2021.acl-short.134) |  | 0 | The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8%, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being... | Philippe Laban, Luke Dai, Lucas Bandarkar, Marti A. Hearst |  |
| 172 |  |  [SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization](https://doi.org/10.18653/v1/2021.acl-short.135) |  | 0 | In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring... | Yixin Liu, Pengfei Liu |  |
| 173 |  |  [SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles](https://doi.org/10.18653/v1/2021.acl-short.136) |  | 0 | In this work, we introduce a corpus for satire detection in Romanian news. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest corpora for satire detection regardless of language and the only one for the Romanian language. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that models do not achieve high performance simply due to... | AnaCristina Rogoz, Mihaela Gaman, Radu Tudor Ionescu |  |
| 174 |  |  [Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents](https://doi.org/10.18653/v1/2021.acl-short.137) |  | 0 | Faceted summarization provides briefings of a document from different perspectives. Readers can quickly comprehend the main points of a long document with the help of a structured outline. However, little research has been conducted on this subject, partially due to the lack of large-scale faceted summarization datasets. In this study, we present FacetSum, a faceted summarization benchmark built on Emerald journal articles, covering a diverse range of domains. Different from traditional... | Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong, Xingdi Yuan, Tong Wang, Daqing He |  |
| 175 |  |  [Replicating and Extending "Because Their Treebanks Leak": Graph Isomorphism, Covariants, and Parser Performance](https://doi.org/10.18653/v1/2021.acl-short.138) |  | 0 | Søgaard (2020) obtained results suggesting the fraction of trees occurring in the test data isomorphic to trees in the training set accounts for a non-trivial variation in parser performance. Similar to other statistical analyses in NLP, the results were based on evaluating linear regressions. However, the study had methodological issues and was undertaken using a small sample size leading to unreliable results. We present a replication study in which we also bin sentences by length and find... | Mark Anderson, Anders Søgaard, Carlos GómezRodríguez |  |
| 176 |  |  [Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data](https://doi.org/10.18653/v1/2021.acl-short.139) |  | 0 | High-performing machine translation (MT) systems can help overcome language barriers while making it possible for everyone to communicate and use language technologies in the language of their choice. However, such systems require large amounts of parallel sentences for training, and translators can be difficult to find and expensive. Here, we present a data collection strategy for MT which, in contrast, is cheap and simple, as it does not require bilingual speakers. Based on the insight that... | Rajat Bhatnagar, Ananya Ganesh, Katharina Kann |  |
| 177 |  |  [Frontmatter](https://aclanthology.org/2021.acl-long.0) |  | 0 |  |  |  |
| 178 |  |  [Investigating label suggestions for opinion mining in German Covid-19 social media](https://doi.org/10.18653/v1/2021.acl-long.1) |  | 0 | This work investigates the use of interactively updated label suggestions to improve upon the efficiency of gathering annotations on the task of opinion mining in German Covid-19 social media data. We develop guidelines to conduct a controlled annotation study with social science students and find that suggestions from a model trained on a small, expert-annotated dataset already lead to a substantial improvement – in terms of inter-annotator agreement (+.14 Fleiss’ κ) and annotation quality –... | Tilman Beck, JiUng Lee, Christina Viehmann, Marcus Maurer, Oliver Quiring, Iryna Gurevych |  |
| 179 |  |  [How Did This Get Funded?! Automatically Identifying Quirky Scientific Achievements](https://doi.org/10.18653/v1/2021.acl-long.2) |  | 0 | Humor is an important social phenomenon, serving complex social and psychological functions. However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem. In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers. We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: “Are cows more... | Chen Shani, Nadav Borenstein, Dafna Shahaf |  |
| 180 |  |  [Engage the Public: Poll Question Generation for Social Media Posts](https://doi.org/10.18653/v1/2021.acl-long.3) |  | 0 | This paper presents a novel task to generate poll questions for social media posts. It offers an easy way to hear the voice from the public and learn from their feelings to important social topics. While most related work tackles formal languages (e.g., exam papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity. To deal with that, we propose to encode user comments and discover latent topics therein as contexts. They are then... | Zexin Lu, Keyang Ding, Yuji Zhang, Jing Li, Baolin Peng, Lemao Liu |  |
| 181 |  |  [HateCheck: Functional Tests for Hate Speech Detection Models](https://doi.org/10.18653/v1/2021.acl-long.4) |  | 0 | Detecting online hate is a difficult task that even state-of-the-art models struggle with. Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score. However, this approach makes it difficult to identify specific model weak points. It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets. To enable more targeted... | Paul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak Waseem, Helen Z. Margetts, Janet B. Pierrehumbert |  |
| 182 |  |  [Unified Dual-view Cognitive Model for Interpretable Claim Verification](https://doi.org/10.18653/v1/2021.acl-long.5) |  | 0 | Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification. Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition. However, individuals’ cognition of social things is not always able to truly reflect the objective. There... | Lianwei Wu, Yuan Rao, Yuqian Lan, Ling Sun, Zhaoyin Qi |  |
| 183 |  |  [DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling](https://doi.org/10.18653/v1/2021.acl-long.6) |  | 0 | Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance. In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms. Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap... | Lanqing Xue, Kaitao Song, Duocai Wu, Xu Tan, Nevin L. Zhang, Tao Qin, WeiQiang Zhang, TieYan Liu |  |
| 184 |  |  [PENS: A Dataset and Generic Framework for Personalized News Headline Generation](https://doi.org/10.18653/v1/2021.acl-long.7) |  | 0 | In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user’s reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair... | Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, Xing Xie |  |
| 185 |  |  [Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization](https://doi.org/10.18653/v1/2021.acl-long.8) |  | 0 | Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the... | Dongkyu Lee, Zhiliang Tian, Lanqing Xue, Nevin L. Zhang |  |
| 186 |  |  [Mention Flags (MF): Constraining Transformer-based Text Generators](https://doi.org/10.18653/v1/2021.acl-long.9) |  | 0 | This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs. Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction. Constrained decoding algorithms always produce hypotheses satisfying all constraints. However, they are computationally expensive and can lower the... | Yufei Wang, Ian D. Wood, Stephen Wan, Mark Dras, Mark Johnson |  |
| 187 |  |  [Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation](https://doi.org/10.18653/v1/2021.acl-long.10) |  | 0 | Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language. Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. However, this often requires that the input appears verbatim in the output text. This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input. In this paper,... | Giulio Zhou, Gerasimos Lampouras |  |
| 188 |  |  [Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances](https://doi.org/10.18653/v1/2021.acl-long.11) |  | 0 | Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models. However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances. In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and... | Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, Jie Zhou |  |
| 189 |  |  [Dual Slot Selector via Local Reliability Verification for Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-long.12) |  | 0 | The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts. Existing approaches generally predict the dialogue state at every turn from scratch. However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn. Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value... | Jinyu Guo, Kai Shuang, Jijie Li, Zihan Wang |  |
| 190 |  |  [Transferable Dialogue Systems and User Simulators](https://doi.org/10.18653/v1/2021.acl-long.13) |  | 0 | One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural... | BoHsiang Tseng, Yinpei Dai, Florian Kreyssig, Bill Byrne |  |
| 191 |  |  [BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data](https://doi.org/10.18653/v1/2021.acl-long.14) |  | 0 | Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two... | Haoyu Song, Yan Wang, Kaiyan Zhang, WeiNan Zhang, Ting Liu |  |
| 192 |  |  [GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2021.acl-long.15) |  | 0 | Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local... | Libo Qin, Fuxuan Wei, Tianbao Xie, Xiao Xu, Wanxiang Che, Ting Liu |  |
| 193 |  |  [Accelerating BERT Inference for Sequence Labeling via Early-Exit](https://doi.org/10.18653/v1/2021.acl-long.16) |  | 0 | Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios. Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive. To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks. However, existing early-exit mechanisms are specifically designed for sequence-level... | Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, Xuanjing Huang |  |
| 194 |  |  [Modularized Interaction Network for Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.17) |  | 0 | Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks. The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment. Moreover, as boundary detection and type prediction may cooperate with... | Fei Li, Zheng Wang, Siu Cheung Hui, Lejian Liao, Dandan Song, Jing Xu, Guoxiu He, Meihuizi Jia |  |
| 195 |  |  [Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder](https://doi.org/10.18653/v1/2021.acl-long.18) |  | 0 | Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE). However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an... | Xiangyu Xi, Wei Ye, Shikun Zhang, Quanxiu Wang, Huixing Jiang, Wei Wu |  |
| 196 |  |  [UniRE: A Unified Label Space for Entity Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.19) |  | 0 | Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks’ label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the... | Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, Junchi Yan |  |
| 197 |  |  [Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.20) |  | 0 | Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications. As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously. Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable... | Li Cui, Deqing Yang, Jiaxin Yu, Chengwei Hu, Jiayang Cheng, Jingjie Yi, Yanghua Xiao |  |
| 198 |  |  [Contrastive Learning for Many-to-many Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.21) |  | 0 | Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified... | Xiao Pan, Mingxuan Wang, Liwei Wu, Lei Li |  |
| 199 |  |  [Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.22) |  | 0 | Neural Machine Translation (NMT) currently exhibits biases such as producing translations that are too short and overgenerating frequent words, and shows poor robustness to copy noise in training data or domain shift. Recent work has tied these shortcomings to beam search – the de facto standard inference algorithm in NMT – and Eikema & Aziz (2020) propose to use Minimum Bayes Risk (MBR) decoding on unbiased samples instead. In this paper, we empirically investigate the properties of MBR... | Mathias Müller, Rico Sennrich |  |
| 200 |  |  [Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.23) |  | 0 | One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level. However, the computational complexity of a self-attention network is O(n2), increasing quadratically with sequence length. By contrast, the complexity of LSTM-based approaches is only O(n). In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model... | Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Meng Zhang |  |
| 201 |  |  [A Bidirectional Transformer Based Alignment Model for Unsupervised Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.24) |  | 0 | Word alignment and machine translation are two closely related tasks. Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy. High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection. Existing methods for learning word alignment include statistical word aligners... | Jingyi Zhang, Josef van Genabith |  |
| 202 |  |  [Learning Language Specific Sub-network for Multilingual Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.25) |  | 0 | Multilingual neural machine translation aims at learning a single translation model for multiple languages. These jointly trained models often suffer from performance degradationon rich-resource language pairs. We attribute this degeneration to parameter interference. In this paper, we propose LaSS to jointly train a single unified multilingual MT model. LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference. Comprehensive experiments on IWSLT... | Zehui Lin, Liwei Wu, Mingxuan Wang, Lei Li |  |
| 203 |  |  [Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.26) |  | 0 | While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance... | Linyi Yang, Jiazheng Li, Padraig Cunningham, Yue Zhang, Barry Smyth, Ruihai Dong |  |
| 204 |  |  [Bridge-Based Active Domain Adaptation for Aspect Term Extraction](https://doi.org/10.18653/v1/2021.acl-long.27) |  | 0 | As a fine-grained task, the annotation cost of aspect term extraction is extremely high. Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains. Since most aspect terms are domain-specific, they cannot be transferred directly. Existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots). However, all these methods... | Zhuang Chen, Tieyun Qian |  |
| 205 |  |  [Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks](https://doi.org/10.18653/v1/2021.acl-long.28) |  | 0 | With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose... | Xiaocui Yang, Shi Feng, Yifei Zhang, Daling Wang |  |
| 206 |  |  [Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions](https://doi.org/10.18653/v1/2021.acl-long.29) |  | 0 | Product reviews contain a large number of implicit aspects and implicit opinions. However, most of the existing studies in aspect-based sentiment analysis ignored this problem. In this work, we introduce a new task, named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions. We furthermore... | Hongjie Cai, Rui Xia, Jianfei Yu |  |
| 207 |  |  [PASS: Perturb-and-Select Summarizer for Product Reviews](https://doi.org/10.18653/v1/2021.acl-long.30) |  | 0 | The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product. Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner. This challenging task gives rise to two shortcomings in existing work. First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries. Second, as reviewers often... | Nadav Oved, Ran Levy |  |
| 208 |  |  [Deep Differential Amplifier for Extractive Summarization](https://doi.org/10.18653/v1/2021.acl-long.31) |  | 0 | For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy. The imbalanced classification of summarization is inherent, which can’t be addressed by common algorithms easily. In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework. Specifically, we first calculate and amplify... | Ruipeng Jia, Yanan Cao, Fang Fang, Yuchen Zhou, Zheng Fang, Yanbing Liu, Shi Wang |  |
| 209 |  |  [Multi-TimeLine Summarization (MTLS): Improving Timeline Summarization by Generating Multiple Summaries](https://doi.org/10.18653/v1/2021.acl-long.32) |  | 0 | In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of Time-Line Summarization (TLS). Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story. To achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation. We also introduce a quantitative evaluation measure for... | Yi Yu, Adam Jatowt, Antoine Doucet, Kazunari Sugiyama, Masatoshi Yoshikawa |  |
| 210 |  |  [Self-Supervised Multimodal Opinion Summarization](https://doi.org/10.18653/v1/2021.acl-long.33) |  | 0 | Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. However, non-text data such as image and metadata related to reviews have been considered less often. To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum. Our framework obtains a representation of each... | Jinbae Im, Moonki Kim, Hoyeop Lee, Hyunsouk Cho, Sehee Chung |  |
| 211 |  |  [A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy](https://doi.org/10.18653/v1/2021.acl-long.34) |  | 0 | In recent years, reference-based and supervised summarization evaluation metrics have been widely explored. However, collecting human-annotated references and ratings are costly and time-consuming. To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric. Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score. The relevance score is computed between the pseudo reference built from the source document... | Wang Chen, Piji Li, Irwin King |  |
| 212 |  |  [DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions](https://doi.org/10.18653/v1/2021.acl-long.35) |  | 0 | Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate... | Weijia Shi, Mandar Joshi, Luke Zettlemoyer |  |
| 213 |  |  [Introducing Orthogonal Constraint in Structural Probes](https://doi.org/10.18653/v1/2021.acl-long.36) |  | 0 | With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that... | Tomasz Limisiewicz, David Marecek |  |
| 214 |  |  [Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger](https://doi.org/10.18653/v1/2021.acl-long.37) |  | 0 | Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert... | Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, Maosong Sun |  |
| 215 |  |  [Examining the Inductive Bias of Neural Language Models with Artificial Languages](https://doi.org/10.18653/v1/2021.acl-long.38) |  | 0 | Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel... | Jennifer C. White, Ryan Cotterell |  |
| 216 |  |  [Explaining Contextualization in Language Models using Visual Analytics](https://doi.org/10.18653/v1/2021.acl-long.39) |  | 0 | Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights. In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and... | Rita Sevastjanova, AikateriniLida Kalouli, Christin Beck, Hanna Schäfer, Mennatallah ElAssady |  |
| 217 |  |  [Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.40) |  | 0 | Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful... | George Chrysostomou, Nikolaos Aletras |  |
| 218 |  |  [Generating Landmark Navigation Instructions from Maps as a Graph-to-Text Problem](https://doi.org/10.18653/v1/2021.acl-long.41) |  | 0 | Car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks. We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions. Routes on the map are encoded in a location- and rotation-invariant graph representation that is... | Raphael Schumann, Stefan Riezler |  |
| 219 |  |  [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://doi.org/10.18653/v1/2021.acl-long.42) |  | 0 | Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train. However, these methods face problems of using task-specific visual representation of the... | Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, Fei Huang |  |
| 220 |  |  [Learning Relation Alignment for Calibrated Cross-modal Retrieval](https://doi.org/10.18653/v1/2021.acl-long.43) |  | 0 | Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task. To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions. The neglect of such relation consistency impairs the contextualized representation of image-text... | Shuhuai Ren, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun, Hongxia Yang |  |
| 221 |  |  [KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation](https://doi.org/10.18653/v1/2021.acl-long.44) |  | 0 | We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of... | Yiran Xing, Zai Shi, Zhao Meng, Gerhard Lakemeyer, Yunpu Ma, Roger Wattenhofer |  |
| 222 |  |  [Cascaded Head-colliding Attention](https://doi.org/10.18653/v1/2021.acl-long.45) |  | 0 | Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks. At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence. Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model. To... | Lin Zheng, Zhiyong Wu, Lingpeng Kong |  |
| 223 |  |  [Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor](https://doi.org/10.18653/v1/2021.acl-long.46) |  | 0 | Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student’s output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this... | Xinyu Wang, Yong Jiang, Zhaohui Yan, Zixia Jia, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 224 |  |  [Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://doi.org/10.18653/v1/2021.acl-long.47) |  | 0 | State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient... | Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, James Henderson |  |
| 225 |  |  [COSY: COunterfactual SYntax for Cross-Lingual Understanding](https://doi.org/10.18653/v1/2021.acl-long.48) |  | 0 | Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance. However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task. We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency... | Sicheng Yu, Hao Zhang, Yulei Niu, Qianru Sun, Jing Jiang |  |
| 226 |  |  [OoMMix: Out-of-manifold Regularization in Contextual Embedding Space for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.49) |  | 0 | Recent studies on neural networks with pre-trained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located. In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which cannot be accessed through the words. Specifically, we synthesize the out-of-manifold embeddings based on two embeddings obtained from... | Seonghyeon Lee, Dongha Lee, Hwanjo Yu |  |
| 227 |  |  [Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model](https://doi.org/10.18653/v1/2021.acl-long.50) |  | 0 | Stereotypical language expresses widely-held beliefs about different social categories. Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences. In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology. The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and... | Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko |  |
| 228 |  |  [Structurizing Misinformation Stories via Rationalizing Fact-Checks](https://doi.org/10.18653/v1/2021.acl-long.51) |  | 0 | Misinformation has recently become a well-documented matter of public concern. Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks. This paper aims to structurize these misinformation stories by leveraging fact-check articles. Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images,... | Shan Jiang, Christo Wilson |  |
| 229 |  |  [Modeling Language Usage and Listener Engagement in Podcasts](https://doi.org/10.18653/v1/2021.acl-long.52) |  | 0 | While there is an abundance of advice to podcast creators on how to speak in ways that engage their listeners, there has been little data-driven analysis of podcasts that relates linguistic style with engagement. In this paper, we investigate how various factors – vocabulary diversity, distinctiveness, emotion, and syntax, among others – correlate with engagement, based on analysis of the creators’ written descriptions and transcripts of the audio. We build models with different textual... | Sravana Reddy, Mariya Lazarova, Yongze Yu, Rosie Jones |  |
| 230 |  |  [Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions](https://doi.org/10.18653/v1/2021.acl-long.53) |  | 0 | People debate on a variety of topics on online platforms such as Reddit, or Facebook. Debates can be lengthy, with users exchanging a wealth of information and opinions. However, conversations do not always go smoothly, and users sometimes engage in unsound argumentation techniques to prove a claim. These techniques are called fallacies. Fallacies are persuasive arguments that provide insufficient or incorrect evidence to support the claim. In this paper, we study the most frequent fallacies on... | Saumya Sahai, Oana Balalau, Roxana Horincar |  |
| 231 |  |  [SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues](https://doi.org/10.18653/v1/2021.acl-long.54) |  | 0 | Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly. We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues. Moreover, we formulate a sequential structure prediction task, and propose an 𝛼-𝛽-𝛾 strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an 𝛼 process... | Liang Qiu, Yuan Liang, Yizhou Zhao, Pan Lu, Baolin Peng, Zhou Yu, Ying Nian Wu, SongChun Zhu |  |
| 232 |  |  [TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems](https://doi.org/10.18653/v1/2021.acl-long.55) |  | 0 | We present a data-driven, end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy. We show that two essential components of the system produce these results: a sufficiently large and diverse, in-domain labeled dataset, and a neural network-based, pre-trained model that generates both verbal responses and API call predictions. In terms of data, we introduce TicketTalk, a movie ticketing dialog... | Bill Byrne, Karthik Krishnamoorthi, Saravanan Ganesh, Mihir Sanjay Kale |  |
| 233 |  |  [Improving Dialog Systems for Negotiation with Personality Modeling](https://doi.org/10.18653/v1/2021.acl-long.56) |  | 0 | In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent’s high-level strategy in negotiation tasks. Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent’s personality type during both learning and inference. We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our... | Runzhe Yang, Jingxiao Chen, Karthik Narasimhan |  |
| 234 |  |  [Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training](https://doi.org/10.18653/v1/2021.acl-long.57) |  | 0 | In this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better. In contrast to standard adversarial training algorithms, IAT encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations. By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the model is... | Wangchunshu Zhou, Qifei Li, Chenle Li |  |
| 235 |  |  [Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features](https://doi.org/10.18653/v1/2021.acl-long.58) |  | 0 | Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these... | Hannah Rashkin, David Reitter, Gaurav Singh Tomar, Dipanjan Das |  |
| 236 |  |  [CitationIE: Leveraging the Citation Graph for Scientific Information Extraction](https://doi.org/10.18653/v1/2021.acl-long.59) |  | 0 | Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. Despite the importance of this task, most existing works on scientific information extraction (SciIE)... | Vijay Viswanathan, Graham Neubig, Pengfei Liu |  |
| 237 |  |  [From Discourse to Narrative: Knowledge Projection for Event Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.60) |  | 0 | Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events. Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs. The lack of high-quality labelled corpora further exacerbates that problem. In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them. Specifically, we propose... | Jialong Tang, Hongyu Lin, Meng Liao, Yaojie Lu, Xianpei Han, Le Sun, Weijian Xie, Jin Xu |  |
| 238 |  |  [AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER](https://doi.org/10.18653/v1/2021.acl-long.61) |  | 0 | Neural methods have been shown to achieve high performance in Named Entity Recognition (NER), but rely on costly high-quality labeled data for training, which is not always available across languages. While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (AdvPicker) to better leverage such data and further improve results. We design an adversarial learning framework in which an... | Weile Chen, Huiqiang Jiang, Qianhui Wu, Börje Karlsson, Yi Guan |  |
| 239 |  |  [Compare to The Knowledge: Graph Neural Fake News Detection with External Knowledge](https://doi.org/10.18653/v1/2021.acl-long.62) |  | 0 | Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important. Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted. In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake... | Linmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong, Duyu Tang, Chuan Shi, Nan Duan, Ming Zhou |  |
| 240 |  |  [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://doi.org/10.18653/v1/2021.acl-long.63) |  | 0 | Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a... | Yucheng Wang, Bowen Yu, Hongsong Zhu, Tingwen Liu, Nan Yu, Limin Sun |  |
| 241 |  |  [LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking](https://doi.org/10.18653/v1/2021.acl-long.64) |  | 0 | Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the... | Hang Jiang, Sairam Gurajada, Qiuhao Lu, Sumit Neelam, Lucian Popa, Prithviraj Sen, Yunyao Li, Alexander G. Gray |  |
| 242 |  |  [Do Context-Aware Translation Models Pay the Right Attention?](https://doi.org/10.18653/v1/2021.acl-long.65) |  | 0 | Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting... | Kayo Yin, Patrick Fernandes, Danish Pruthi, Aditi Chaudhary, André F. T. Martins, Graham Neubig |  |
| 243 |  |  [Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data](https://doi.org/10.18653/v1/2021.acl-long.66) |  | 0 | The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related... | WeiJen Ko, Ahmed ElKishky, Adithya Renduchintala, Vishrav Chaudhary, Naman Goyal, Francisco Guzmán, Pascale Fung, Philipp Koehn, Mona T. Diab |  |
| 244 |  |  [Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.67) |  | 0 | Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces. In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment. Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are... | Haoyue Shi, Luke Zettlemoyer, Sida I. Wang |  |
| 245 |  |  [Multilingual Speech Translation from Efficient Finetuning of Pretrained Models](https://doi.org/10.18653/v1/2021.acl-long.68) |  | 0 | We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder. Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic... | Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Miguel Pino, Alexei Baevski, Alexis Conneau, Michael Auli |  |
| 246 |  |  [Learning Faithful Representations of Causal Graphs](https://doi.org/10.18653/v1/2021.acl-long.69) |  | 0 | Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation. However, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks. In this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of... | Ananth Balashankar, Lakshminarayanan Subramanian |  |
| 247 |  |  [What Context Features Can Transformer Language Models Use?](https://doi.org/10.18653/v1/2021.acl-long.70) |  | 0 | Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling... | Joe O'Connor, Jacob Andreas |  |
| 248 |  |  [Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models](https://doi.org/10.18653/v1/2021.acl-long.71) |  | 0 | In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input. The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions. Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature. In this... | Sandipan Sikdar, Parantapa Bhattacharya, Kieran Heese |  |
| 249 |  |  [DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations](https://doi.org/10.18653/v1/2021.acl-long.72) |  | 0 | Sentence embeddings are an important component of many natural language processing (NLP) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present DeCLUTR:... | John M. Giorgi, Osvald Nitski, Bo Wang, Gary D. Bader |  |
| 250 |  |  [XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation](https://doi.org/10.18653/v1/2021.acl-long.73) |  | 0 | Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English. Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation. Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text... | Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, Guodong Zhou |  |
| 251 |  |  [Span-based Semantic Parsing for Compositional Generalization](https://doi.org/10.18653/v1/2021.acl-long.74) |  | 0 | Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the... | Jonathan Herzig, Jonathan Berant |  |
| 252 |  |  [Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?](https://doi.org/10.18653/v1/2021.acl-long.75) |  | 0 | Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language... | Peter Shaw, MingWei Chang, Panupong Pasupat, Kristina Toutanova |  |
| 253 |  |  [A Targeted Assessment of Incremental Processing in Neural Language Models and Humans](https://doi.org/10.18653/v1/2021.acl-long.76) |  | 0 | We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set... | Ethan Wilcox, Pranali Vani, Roger Levy |  |
| 254 |  |  [The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing](https://doi.org/10.18653/v1/2021.acl-long.77) |  | 0 | Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more. Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these... | Valentina Pyatkin, Shoval Sadde, Aynat Rubinstein, Paul Portner, Reut Tsarfaty |  |
| 255 |  |  [To POS Tag or Not to POS Tag: The Impact of POS Tags on Morphological Learning in Low-Resource Settings](https://doi.org/10.18653/v1/2021.acl-long.78) |  | 0 | Part-of-Speech (POS) tags are routinely included as features in many NLP tasks. However, the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS. This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture. Each task is tested twice on... | Sarah R. Moeller, Ling Liu, Mans Hulden |  |
| 256 |  |  [Prosodic segmentation for parsing spoken dialogue](https://doi.org/10.18653/v1/2021.acl-long.79) |  | 0 | Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units. Previous work has shown that prosody can help with parsing disfluent speech (Tran et al. 2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn’t true in existing speech applications. We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold... | Elizabeth Nielsen, Mark Steedman, Sharon Goldwater |  |
| 257 |  |  [VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation](https://doi.org/10.18653/v1/2021.acl-long.80) |  | 0 | We introduce VoxPopuli, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours. We provide speech recognition (ASR) baselines and validate the versatility of VoxPopuli unlabeled data... | Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Miguel Pino, Emmanuel Dupoux |  |
| 258 |  |  [Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets](https://doi.org/10.18653/v1/2021.acl-long.81) |  | 0 | Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social... | Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, Hanna M. Wallach |  |
| 259 |  |  [Robust Knowledge Graph Completion with Stacked Convolutions and a Student Re-Ranking Network](https://doi.org/10.18653/v1/2021.acl-long.82) |  | 0 | Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG... | Justin Lovelace, Denis NewmanGriffis, Shikhar Vashishth, Jill Fain Lehman, Carolyn P. Rosé |  |
| 260 |  |  [A DQN-based Approach to Finding Precise Evidences for Fact Verification](https://doi.org/10.18653/v1/2021.acl-long.83) |  | 0 | Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV. Despite being important, precise evidences are rarely studied by existing methods for FV. It is challenging to find precise evidences due to a large search space with lots of local optimums. Inspired... | Hai Wan, Haicheng Chen, Jianfeng Du, Weilin Luo, Rongzhen Ye |  |
| 261 |  |  [The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing](https://doi.org/10.18653/v1/2021.acl-long.84) |  | 0 | In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without... | Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin |  |
| 262 |  |  [Unsupervised Out-of-Domain Detection via Pre-trained Transformers](https://doi.org/10.18653/v1/2021.acl-long.85) |  | 0 | Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs. Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues. Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios. Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data. We utilize the latent representations of... | Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, Caiming Xiong |  |
| 263 |  |  [MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation](https://doi.org/10.18653/v1/2021.acl-long.86) |  | 0 | The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator... | Ahmad Rashid, Vasileios Lioutas, Mehdi Rezagholizadeh |  |
| 264 |  |  [Selecting Informative Contexts Improves Language Model Fine-tuning](https://doi.org/10.18653/v1/2021.acl-long.87) |  | 0 | Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the... | Richard J. Antonello, Nicole Beckage, Javier Turek, Alexander Huth |  |
| 265 |  |  [Explainable Prediction of Text Complexity: The Missing Preliminaries for Text Simplification](https://doi.org/10.18653/v1/2021.acl-long.88) |  | 0 | Text simplification reduces the language complexity of professional content for accessibility purposes. End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox. We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process. The first two steps in this pipeline are often neglected: 1) to predict whether a given piece of... | Cristina Garbacea, Mengtian Guo, Samuel Carton, Qiaozhu Mei |  |
| 266 |  |  [Multi-Task Retrieval for Knowledge-Intensive Tasks](https://doi.org/10.18653/v1/2021.acl-long.89) |  | 0 | Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking. Although neural retrieval outperforms traditional methods like tf-idf and BM25, its performance degrades considerably when applied to out-of-domain data. Driven by the question of whether a neural retrieval model can be _universal_ and perform robustly on a wide variety of problems, we propose a multi-task trained model. Our approach not only outperforms... | Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wentau Yih, Barlas Oguz, Veselin Stoyanov, Gargi Ghosh |  |
| 267 |  |  [When Do You Need Billions of Words of Pretraining Data?](https://doi.org/10.18653/v1/2021.acl-long.90) |  | 0 | NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves... | Yian Zhang, Alex Warstadt, Xiaocheng Li, Samuel R. Bowman |  |
| 268 |  |  [Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.91) |  | 0 | In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance... | Elena Voita, Rico Sennrich, Ivan Titov |  |
| 269 |  |  [Comparing Test Sets with Item Response Theory](https://doi.org/10.18653/v1/2021.acl-long.92) |  | 0 | Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks. Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements? To measure this uniformly... | Clara Vania, Phu Mon Htut, William Huang, Dhara A. Mungra, Richard Yuanzhe Pang, Jason Phang, Haokun Liu, Kyunghyun Cho, Samuel R. Bowman |  |
| 270 |  |  [Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.93) |  | 0 | A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models. Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations. We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge. We tested this claim by exploring a single phenomenon in four languages: English,... | Forrest Davis, Marten van Schijndel |  |
| 271 |  |  [More Identifiable yet Equally Performant Transformers for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.94) |  | 0 | Interpretability is an important aspect of the trustworthiness of a model’s predictions. Transformer’s predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head). Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique. A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights.... | Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria, Eduard H. Hovy |  |
| 272 |  |  [AugNLG: Few-shot Natural Language Generation using Self-trained Data Augmentation](https://doi.org/10.18653/v1/2021.acl-long.95) |  | 0 | Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language. For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable. Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings. This paper proposes... | Xinnuo Xu, Guoyin Wang, YoungBum Kim, Sungjin Lee |  |
| 273 |  |  [Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability](https://doi.org/10.18653/v1/2021.acl-long.96) |  | 0 | In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children’s ability to understand others’ thoughts, feelings, and desires (or “mindreading”). We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating. We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems. To determine... | Venelin Kovatchev, Phillip Smith, Mark G. Lee, Rory T. Devine |  |
| 274 |  |  [A Dataset and Baselines for Multilingual Reply Suggestion](https://doi.org/10.18653/v1/2021.acl-long.97) |  | 0 | Reply suggestion models help users process emails and chats faster. Previous work only studies English reply suggestion. Instead, we present MRS, a multilingual reply suggestion dataset with ten languages. MRS can be used to compare two families of models: 1) retrieval models that select the reply from a fixed set and 2) generation models that produce the reply from scratch. Therefore, MRS complements existing cross-lingual generalization benchmarks that focus on classification and sequence... | Mozhi Zhang, Wei Wang, Budhaditya Deb, Guoqing Zheng, Milad Shokouhi, Ahmed Hassan Awadallah |  |
| 275 |  |  [What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?](https://doi.org/10.18653/v1/2021.acl-long.98) |  | 0 | Crowdsourcing is widely used to create data for common natural language understanding tasks. Despite the importance of these datasets for measuring and refining model understanding of language, there has been little focus on the crowdsourcing methods used for collecting the datasets. In this paper, we compare the efficacy of interventions that have been proposed in prior work as ways of improving data quality. We use multiple-choice question answering as a testbed and run a randomized trial by... | Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara Vania, Samuel R. Bowman |  |
| 276 |  |  [Align Voting Behavior with Public Statements for Legislator Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.99) |  | 0 | Ideology of legislators is typically estimated by ideal point models from historical records of votes. It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior. However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories. In order to mitigate these two problems, we explore to incorporate both voting behavior and public... | Xinyi Mou, Zhongyu Wei, Lei Chen, Shangyi Ning, Yancheng He, Changjian Jiang, Xuanjing Huang |  |
| 277 |  |  [Measure and Evaluation of Semantic Divergence across Two Languages](https://doi.org/10.18653/v1/2021.acl-long.100) |  | 0 | Languages are dynamic systems: word usage may change over time, reflecting various societal factors. However, all languages do not evolve identically: the impact of an event, the influence of a trend or thinking, can differ between communities. In this paper, we propose to track these divergences by comparing the evolution of a word and its translation across two languages. We investigate several methods of building time-varying and bilingual word embeddings, using contextualised and... | Syrielle Montariol, Alexandre Allauzen |  |
| 278 |  |  [Improving Zero-Shot Translation by Disentangling Positional Information](https://doi.org/10.18653/v1/2021.acl-long.101) |  | 0 | Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional... | Danni Liu, Jan Niehues, James Cross, Francisco Guzmán, Xian Li |  |
| 279 |  |  [Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning](https://doi.org/10.18653/v1/2021.acl-long.102) |  | 0 | Commonsense reasoning research has so far been limited to English. We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English. We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs. We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages. In addition,... | Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, Xiang Ren |  |
| 280 |  |  [Attention Calibration for Transformer in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.103) |  | 0 | Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions. However, recent studies have questioned the attention mechanisms’ capability for discovering decisive inputs. In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input’s contribution to the model outputs. We increase the attention weights assigned to the... | Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li |  |
| 281 |  |  [Diverse Pretrained Context Encodings Improve Document Translation](https://doi.org/10.18653/v1/2021.acl-long.104) |  | 0 | We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts. Experiments on the NIST Chinese-English, and IWSLT and WMT English-German... | Domenic Donato, Lei Yu, Chris Dyer |  |
| 282 |  |  [Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study](https://doi.org/10.18653/v1/2021.acl-long.105) |  | 0 | Recent research in multilingual language models (LM) has demonstrated their ability to effectively handle multiple languages in a single model. This holds promise for low web-resource languages (LRL) as multilingual models can enable transfer of supervision from high resource languages to LRLs. However, incorporating a new language in an LM still remains a challenge, particularly for languages with limited corpora and in unseen scripts. In this paper we argue that relatedness among languages in... | Yash Khemchandani, Sarvesh Mehtani, Vaidehi Patil, Abhijeet Awasthi, Partha P. Talukdar, Sunita Sarawagi |  |
| 283 |  |  [On Finding the K-best Non-projective Dependency Trees](https://doi.org/10.18653/v1/2021.acl-long.106) |  | 0 | The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency... | Ran Zmigrod, Tim Vieira, Ryan Cotterell |  |
| 284 |  |  [Towards Argument Mining for Social Good: A Survey](https://doi.org/10.18653/v1/2021.acl-long.107) |  | 0 | This survey builds an interdisciplinary picture of Argument Mining (AM), with a strong focus on its potential to address issues related to Social and Political Science. More specifically, we focus on AM challenges related to its applications to social media and in the multilingual domain, and then proceed to the widely debated notion of argument quality. We propose a novel definition of argument quality which is integrated with that of deliberative quality from the Social Science literature.... | Eva Maria Vecchi, Neele Falk, Iman Jundi, Gabriella Lapesa |  |
| 285 |  |  [Automated Generation of Storytelling Vocabulary from Photographs for use in AAC](https://doi.org/10.18653/v1/2021.acl-long.108) |  | 0 | Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce. We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences. Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of... | Maurício Fontana de Vargas, Karyn Moffatt |  |
| 286 |  |  [CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes](https://doi.org/10.18653/v1/2021.acl-long.109) |  | 0 | Continuity of care is crucial to ensuring positive health outcomes for patients discharged from an inpatient hospital setting, and improved information sharing can help. To share information, caregivers write discharge notes containing action items to share with patients and their future caregivers, but these action items are easily lost due to the lengthiness of the documents. In this work, we describe our creation of a dataset of clinical action items annotated over MIMIC-III, the largest... | James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David A. Sontag |  |
| 287 |  |  [Assessing Emoji Use in Modern Text Processing Tools](https://doi.org/10.18653/v1/2021.acl-long.110) |  | 0 | Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors. This also leads to an increased need for systems and tools to operate on text containing emojis. In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them. In... | Abu Awal Md Shoeb, Gerard de Melo |  |
| 288 |  |  [Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention](https://doi.org/10.18653/v1/2021.acl-long.111) |  | 0 | Natural language processing techniques have demonstrated promising results in keyphrase generation. However, one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks. Generally, documents are truncated before given as inputs to neural networks. Consequently, the models may miss essential points conveyed in the target document. To overcome this limitation, we propose SEG-Net, a neural keyphrase generation model that is composed of two... | Wasi Uddin Ahmad, Xiao Bai, Soomin Lee, KaiWei Chang |  |
| 289 |  |  [Factorising Meaning and Form for Intent-Preserving Paraphrasing](https://doi.org/10.18653/v1/2021.acl-long.112) |  | 0 | We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form. Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form. We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces. We use a... | Tom Hosking, Mirella Lapata |  |
| 290 |  |  [AggGen: Ordering and Aggregating while Generating](https://doi.org/10.18653/v1/2021.acl-long.113) |  | 0 | We present AggGen (pronounced ‘again’) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation. In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text. Experiments on the WebNLG and E2E challenge data show... | Xinnuo Xu, Ondrej Dusek, Verena Rieser, Ioannis Konstas |  |
| 291 |  |  [Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models](https://doi.org/10.18653/v1/2021.acl-long.114) |  | 0 | Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right. As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision. In this paper, we present Reflective Decoding, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks. Our... | Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena D. Hwang, Yejin Choi |  |
| 292 |  |  [Towards Table-to-Text Generation with Numerical Reasoning](https://doi.org/10.18653/v1/2021.acl-long.115) |  | 0 | Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy... | Lya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura, Hiroya Takamura |  |
| 293 |  |  [BACO: A Background Knowledge- and Content-Based Framework for Citing Sentence Generation](https://doi.org/10.18653/v1/2021.acl-long.116) |  | 0 | In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper. We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information... | Yubin Ge, Ly Dinh, Xiaofeng Liu, Jinsong Su, Ziyao Lu, Ante Wang, Jana Diesner |  |
| 294 |  |  [Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization](https://doi.org/10.18653/v1/2021.acl-long.117) |  | 0 | Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations. In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of... | Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu |  |
| 295 |  |  [Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval](https://doi.org/10.18653/v1/2021.acl-long.118) |  | 0 | Recent pretrained language models “solved” many reading comprehension benchmarks, where questions are written with access to the evidence document. However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA. Our controlled experiments suggest... | Akari Asai, Eunsol Choi |  |
| 296 |  |  [A Gradually Soft Multi-Task and Data-Augmented Approach to Medical Question Understanding](https://doi.org/10.18653/v1/2021.acl-long.119) |  | 0 | Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval. To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding. We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain. Based on this equivalence, we propose a data... | Khalil Mrini, Franck Dernoncourt, Seunghyun Yoon, Trung Bui, Walter Chang, Emilia Farcas, Ndapa Nakashole |  |
| 297 |  |  [Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification](https://doi.org/10.18653/v1/2021.acl-long.120) |  | 0 | A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for the target entity classes during training. Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it. This paper presents the first approach for zero-shot NERC, introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally. We... | Rami Aly, Andreas Vlachos, Ryan McDonald |  |
| 298 |  |  [MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.121) |  | 0 | Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel... | Shuang Wu, Xiaoning Song, ZhenHua Feng |  |
| 299 |  |  [Factuality Assessment as Modal Dependency Parsing](https://doi.org/10.18653/v1/2021.acl-long.122) |  | 0 | As the sources of information that we consume everyday rapidly diversify, it is becoming increasingly important to develop NLP tools that help to evaluate the credibility of the information we receive. A critical step towards this goal is to determine the factuality of events in text. In this paper, we frame factuality assessment as a modal dependency parsing task that identifies the events and their sources, formally known as conceivers, and then determine the level of certainty that the... | Jiarui Yao, Haoling Qiu, Jin Zhao, Bonan Min, Nianwen Xue |  |
| 300 |  |  [Directed Acyclic Graph Network for Conversational Emotion Recognition](https://doi.org/10.18653/v1/2021.acl-long.123) |  | 0 | The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC). In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea. In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a... | Weizhou Shen, Siyue Wu, Yunyi Yang, Xiaojun Quan |  |
| 301 |  |  [Improving Formality Style Transfer with Context-Aware Rule Injection](https://doi.org/10.18653/v1/2021.acl-long.124) |  | 0 | Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model. CARI is able to learn to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest... | Zonghai Yao, Hong Yu |  |
| 302 |  |  [Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection](https://doi.org/10.18653/v1/2021.acl-long.125) |  | 0 | Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states. In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above. We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection. The topic-augmented LM is then combined with... | Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, Yulan He |  |
| 303 |  |  [Syntopical Graphs for Computational Argumentation Tasks](https://doi.org/10.18653/v1/2021.acl-long.126) |  | 0 | Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims, losing out on potentially valuable context provided by the rest of the collection. We introduce a general approach to these tasks motivated by syntopical reading, a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding. To capture collection-level context, we introduce the syntopical graph, a... | Joe Barrow, Rajiv Jain, Nedim Lipka, Franck Dernoncourt, Vlad I. Morariu, Varun Manjunatha, Douglas W. Oard, Philip Resnik, Henning Wachsmuth |  |
| 304 |  |  [Stance Detection in COVID-19 Tweets](https://doi.org/10.18653/v1/2021.acl-long.127) |  | 0 | The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called... | Kyle Glandt, Sarthak Khanal, Yingjie Li, Doina Caragea, Cornelia Caragea |  |
| 305 |  |  [Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification](https://doi.org/10.18653/v1/2021.acl-long.128) |  | 0 | Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim. Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without... | Jiasheng Si, Deyu Zhou, Tongzhe Li, Xingyu Shi, Yulan He |  |
| 306 |  |  [Changes in European Solidarity Before and During COVID-19: Evidence from a Large Crowd- and Expert-Annotated Twitter Dataset](https://doi.org/10.18653/v1/2021.acl-long.129) |  | 0 | We introduce the well-established social scientific concept of social solidarity and its contestation, anti-solidarity, as a new problem setting to supervised machine learning in NLP to assess how European solidarity discourses changed before and after the COVID-19 outbreak was declared a global pandemic. To this end, we annotate 2.3k English and German tweets for (anti-)solidarity expressions, utilizing multiple human annotators and two annotation approaches (experts vs. crowds). We use these... | Alexandra Ils, Dan Liu, Daniela Grunow, Steffen Eger |  |
| 307 |  |  [Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions](https://doi.org/10.18653/v1/2021.acl-long.130) |  | 0 | In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said. In education, teachers’ uptake of student contributions has been linked to higher student achievement. Yet measuring and improving teachers’ uptake at scale is challenging, as existing methods require expensive annotation by experts. We propose a framework for computationally measuring uptake, by (1) releasing a dataset... | Dorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill, Dan Jurafsky, Tatsunori Hashimoto |  |
| 308 |  |  [A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies](https://doi.org/10.18653/v1/2021.acl-long.131) |  | 0 | The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on... | A. Seza Dogruöz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio |  |
| 309 |  |  [Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection](https://doi.org/10.18653/v1/2021.acl-long.132) |  | 0 | We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of 40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes 15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than... | Bertie Vidgen, Tristan Thrush, Zeerak Waseem, Douwe Kiela |  |
| 310 |  |  [InfoSurgeon: Cross-Media Fine-grained Information Consistency Checking for Fake News Detection](https://doi.org/10.18653/v1/2021.acl-long.133) |  | 0 | To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to... | Yi R. Fung, Christopher Thomas, Revanth Gangi Reddy, Sandeep Polisetty, Heng Ji, ShihFu Chang, Kathleen R. McKeown, Mohit Bansal, Avi Sil |  |
| 311 |  |  [I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling](https://doi.org/10.18653/v1/2021.acl-long.134) |  | 0 | To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii)... | Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, Jason Weston |  |
| 312 |  |  [A Sequence-to-Sequence Approach to Dialogue State Tracking](https://doi.org/10.18653/v1/2021.acl-long.135) |  | 0 | This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system. Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently. This paper proposes a new approach to dialogue state tracking, referred to as Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of... | Yue Feng, Yang Wang, Hang Li |  |
| 313 |  |  [Discovering Dialog Structure Graph for Coherent Dialog Generation](https://doi.org/10.18653/v1/2021.acl-long.136) |  | 0 | Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. However, this problem is less studied in open-domain dialogue. In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. To this end, we present an unsupervised model,... | Jun Xu, Zeyang Lei, Haifeng Wang, ZhengYu Niu, Hua Wu, Wanxiang Che |  |
| 314 |  |  [Dialogue Response Selection with Hierarchical Curriculum Learning](https://doi.org/10.18653/v1/2021.acl-long.137) |  | 0 | We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an “easy-to-difficult” scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its... | Yixuan Su, Deng Cai, Qingyu Zhou, Zibo Lin, Simon Baker, Yunbo Cao, Shuming Shi, Nigel Collier, Yan Wang |  |
| 315 |  |  [A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech](https://doi.org/10.18653/v1/2021.acl-long.138) |  | 0 | In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech. We show that DPR and CDP are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN). The token states for an utterance are then aggregated to produce a single state for each... | Jingxuan Yang, Kerui Xu, Jun Xu, Si Li, Sheng Gao, Jun Guo, Nianwen Xue, JiRong Wen |  |
| 316 |  |  [A Systematic Investigation of KB-Text Embedding Alignment at Scale](https://doi.org/10.18653/v1/2021.acl-long.139) |  | 0 | Knowledge bases (KBs) and text often contain complementary knowledge: KBs store structured knowledge that can support long range reasoning, while text stores more comprehensive and timely knowledge in an unstructured way. Separately embedding the individual knowledge sources into vector spaces has demonstrated tremendous successes in encoding the respective knowledge, but how to jointly embed and reason with both knowledge sources to fully leverage the complementary information is still largely... | Vardaan Pahuja, Yu Gu, Wenhu Chen, Mehdi Bahrami, Lei Liu, WeiPeng Chen, Yu Su |  |
| 317 |  |  [Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data](https://doi.org/10.18653/v1/2021.acl-long.140) |  | 0 | Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a... | Haoming Jiang, Danqing Zhang, Tianyu Cao, Bing Yin, Tuo Zhao |  |
| 318 |  |  [Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model](https://doi.org/10.18653/v1/2021.acl-long.141) |  | 0 | Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for... | Hongliang Dai, Yangqiu Song, Haixun Wang |  |
| 319 |  |  [Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning](https://doi.org/10.18653/v1/2021.acl-long.142) |  | 0 | Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view,... | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 320 |  |  [Implicit Representations of Meaning in Neural Language Models](https://doi.org/10.18653/v1/2021.acl-long.143) |  | 0 | Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as \*models of entities and situations\* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a... | Belinda Z. Li, Maxwell I. Nye, Jacob Andreas |  |
| 321 |  |  [Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models](https://doi.org/10.18653/v1/2021.acl-long.144) |  | 0 | Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models’ preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic... | Matthew Finlayson, Aaron Mueller, Sebastian Gehrmann, Stuart M. Shieber, Tal Linzen, Yonatan Belinkov |  |
| 322 |  |  [Bird's Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach](https://doi.org/10.18653/v1/2021.acl-long.145) |  | 0 | NLP has a rich history of representing our prior understanding of language in the form of graphs. Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon. However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs... | Yifan Hou, Mrinmaya Sachan |  |
| 323 |  |  [Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases](https://doi.org/10.18653/v1/2021.acl-long.146) |  | 0 | Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which... | Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu |  |
| 324 |  |  [Poisoning Knowledge Graph Embeddings via Relation Inference Patterns](https://doi.org/10.18653/v1/2021.acl-long.147) |  | 0 | We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs. To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph. Specifically, to degrade the model’s prediction confidence on target facts, we propose to improve the model’s prediction confidence on a set of decoy... | Peru Bhardwaj, John D. Kelleher, Luca Costabello, Declan O'Sullivan |  |
| 325 |  |  [Bad Seeds: Evaluating Lexical Methods for Bias Measurement](https://doi.org/10.18653/v1/2021.acl-long.148) |  | 0 | A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection. We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements. Seeds developed in one context are often re-used in other contexts,... | Maria Antoniak, David Mimno |  |
| 326 |  |  [A Survey of Race, Racism, and Anti-Racism in NLP](https://doi.org/10.18653/v1/2021.acl-long.149) |  | 0 | Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains... | Anjalie Field, Su Lin Blodgett, Zeerak Waseem, Yulia Tsvetkov |  |
| 327 |  |  [Intrinsic Bias Metrics Do Not Correlate with Application Bias](https://doi.org/10.18653/v1/2021.acl-long.150) |  | 0 | Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics... | Seraphina GoldfarbTarrant, Rebecca Marchant, Ricardo Muñoz Sánchez, Mugdha Pandya, Adam Lopez |  |
| 328 |  |  [RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://doi.org/10.18653/v1/2021.acl-long.151) |  | 0 | Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited... | Soumya Barikeri, Anne Lauscher, Ivan Vulic, Goran Glavas |  |
| 329 |  |  [Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks](https://doi.org/10.18653/v1/2021.acl-long.152) |  | 0 | This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model. However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. Through... | Weicheng Ma, Kai Zhang, Renze Lou, Lili Wang, Soroush Vosoughi |  |
| 330 |  |  [Crafting Adversarial Examples for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.153) |  | 0 | Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems. In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples. We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics... | Xinze Zhang, Junzhe Zhang, Zhenhua Chen, Kun He |  |
| 331 |  |  [UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP](https://doi.org/10.18653/v1/2021.acl-long.154) |  | 0 | Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. In particular, UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no... | M. Saiful Bari, Tasnim Mohiuddin, Shafiq R. Joty |  |
| 332 |  |  [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.155) |  | 0 | Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality. However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup. We propose the Glancing Language Model (GLM) for single-pass parallel generation models. With GLM, we develop Glancing Transformer (GLAT) for machine translation. With only single-pass parallel decoding, GLAT is... | Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li |  |
| 333 |  |  [Hierarchical Context-aware Network for Dense Video Event Captioning](https://doi.org/10.18653/v1/2021.acl-long.156) |  | 0 | Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video. Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events. In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture context from various aspects. In detail, the model leverages local and global context with different mechanisms... | Lei Ji, Xianglin Guo, Haoyang Huang, Xilin Chen |  |
| 334 |  |  [Control Image Captioning Spatially and Temporally](https://doi.org/10.18653/v1/2021.acl-long.157) |  | 0 | Generating image captions with user intention is an emerging need. The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image. However, how to effectively employ traces to improve generation quality and controllability is still under exploration. This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects... | Kun Yan, Lei Ji, Huaishao Luo, Ming Zhou, Nan Duan, Shuai Ma |  |
| 335 |  |  [Edited Media Understanding Frames: Reasoning About the Intent and Implications of Visual Misinformation](https://doi.org/10.18653/v1/2021.acl-long.158) |  | 0 | Understanding manipulated media, from automatically generated ‘deepfakes’ to manually edited ones, raises novel research challenges. Because the vast majority of edited or manipulated images are benign, such as photoshopped images for visual enhancements, the key challenge is to understand the complex layers of underlying intents of media edits and their implications with respect to disinformation. In this paper, we study Edited Media Frames, a new formalism to understand visual media... | Jeff Da, Maxwell Forbes, Rowan Zellers, Anthony Zheng, Jena D. Hwang, Antoine Bosselut, Yejin Choi |  |
| 336 |  |  [PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World](https://doi.org/10.18653/v1/2021.acl-long.159) |  | 0 | We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence,... | Rowan Zellers, Ari Holtzman, Matthew E. Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali Farhadi, Yejin Choi |  |
| 337 |  |  [Modeling Fine-Grained Entity Types with Box Embeddings](https://doi.org/10.18653/v1/2021.acl-long.160) |  | 0 | Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types’ complex interdependencies. We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology. Our model represents both types and entity mentions as boxes. Each mention and its context are... | Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett |  |
| 338 |  |  [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://doi.org/10.18653/v1/2021.acl-long.161) |  | 0 | Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding. In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining. The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual... | Zijun Sun, Xiaoya Li, Xiaofei Sun, Yuxian Meng, Xiang Ao, Qing He, Fei Wu, Jiwei Li |  |
| 339 |  |  [Weight Distillation: Transferring the Knowledge in Neural Network Parameters](https://doi.org/10.18653/v1/2021.acl-long.162) |  | 0 | Knowledge distillation has been proven to be effective in model acceleration and compression. It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. But this way ignores the knowledge inside the large neural networks, e.g., parameters. Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. In this paper,... | Ye Lin, Yanyang Li, Ziyang Wang, Bei Li, Quan Du, Tong Xiao, Jingbo Zhu |  |
| 340 |  |  [Optimizing Deeper Transformers on Small Datasets](https://doi.org/10.18653/v1/2021.acl-long.163) |  | 0 | It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading... | Peng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi Tang, Chenyang Huang, Jackie Chi Kit Cheung, Simon J. D. Prince, Yanshuai Cao |  |
| 341 |  |  [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://doi.org/10.18653/v1/2021.acl-long.164) |  | 0 | Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP. However, many researchers wonder whether these models can maintain their dominance forever. Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard... | JongHoon Oh, Ryu Iida, Julien Kloetzer, Kentaro Torisawa |  |
| 342 |  |  [COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic](https://doi.org/10.18653/v1/2021.acl-long.165) |  | 0 | We introduce a FEVER-like dataset COVID-Fact of 4,086 claims concerning the COVID-19 pandemic. The dataset contains claims, evidence for the claims, and contradictory claims refuted by the evidence. Unlike previous approaches, we automatically detect true claims and their source articles and then generate counter-claims using automatic methods rather than employing human annotators. Along with our constructed resource, we formally present the task of identifying relevant evidence for the claims... | Arkadiy Saakyan, Tuhin Chakrabarty, Smaranda Muresan |  |
| 343 |  |  [Explaining Relationships Between Scientific Documents](https://doi.org/10.18653/v1/2021.acl-long.166) |  | 0 | We address the task of explaining relationships between two scientific documents using natural language text. This task requires modeling the complex content of long technical documents, deducing a relationship between these documents, and expressing the details of that relationship in text. In addition to the theoretical interest of this task, successful solutions can help improve researcher efficiency in search and review. In this paper we establish a dataset of 622K examples from 154K... | Kelvin Luu, Xinyi Wu, Rik KoncelKedziorski, Kyle Lo, Isabel Cachola, Noah A. Smith |  |
| 344 |  |  [IrEne: Interpretable Energy Prediction for Transformers](https://doi.org/10.18653/v1/2021.acl-long.167) |  | 0 | Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models. IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning... | Qingqing Cao, Yash Kumar Lal, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian |  |
| 345 |  |  [Mitigating Bias in Session-based Cyberbullying Detection: A Non-Compromising Approach](https://doi.org/10.18653/v1/2021.acl-long.168) |  | 0 | The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session. In contrast to a single text, a session may consist of an initial post and an associated sequence of comments. Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets. For example, a session containing certain... | Lu Cheng, Ahmadreza Mosallanezhad, Yasin N. Silva, Deborah L. Hall, Huan Liu |  |
| 346 |  |  [PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context](https://doi.org/10.18653/v1/2021.acl-long.169) |  | 0 | Creating effective visualization is an important part of data analytics. While there are many libraries for creating visualization, writing such code remains difficult given the myriad of parameters that users need to provide. In this paper, we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context. To tackle the learning problem, we introduce PlotCoder, a new hierarchical encoder-decoder architecture that models both the... | Xinyun Chen, Linyuan Gong, Alvin Cheung, Dawn Song |  |
| 347 |  |  [Changing the World by Changing the Data](https://doi.org/10.18653/v1/2021.acl-long.170) |  | 0 | NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments... | Anna Rogers |  |
| 348 |  |  [EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets](https://doi.org/10.18653/v1/2021.acl-long.171) |  | 0 | Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks. However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process. Other works use extremely large batch sizes to shorten the... | Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, Jingjing Liu |  |
| 349 |  |  [On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://doi.org/10.18653/v1/2021.acl-long.172) |  | 0 | Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on... | Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, JiaWei Low, Lidong Bing, Luo Si |  |
| 350 |  |  [Data Augmentation for Text Generation Without Any Augmented Data](https://doi.org/10.18653/v1/2021.acl-long.173) |  | 0 | Data augmentation is an effective way to improve the performance of many neural text generation models. However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples. In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions. Our proposed objective can be efficiently optimized... | Wei Bi, Huayang Li, Jiacheng Huang |  |
| 351 |  |  [Integrating Semantics and Neighborhood Information with Graph-Driven Generative Models for Document Retrieval](https://doi.org/10.18653/v1/2021.acl-long.174) |  | 0 | With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval. To generate high-quality hashing code, both semantics and neighborhood information are crucial. However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process. In this paper, we encode the neighborhood information with a graph-induced... | Zijing Ou, Qinliang Su, Jianxing Yu, Bang Liu, Jingwen Wang, Ruihui Zhao, Changyou Chen, Yefeng Zheng |  |
| 352 |  |  [SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis](https://doi.org/10.18653/v1/2021.acl-long.175) |  | 0 | The open-ended nature of visual captioning makes it a challenging area for evaluation. The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty. We introduce “typicality”, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth. Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as... | Joshua Feinglass, Yezhou Yang |  |
| 353 |  |  [KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers](https://doi.org/10.18653/v1/2021.acl-long.176) |  | 0 | The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of... | ChiaHsuan Lee, Oleksandr Polozov, Matthew Richardson |  |
| 354 |  |  [QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus](https://doi.org/10.18653/v1/2021.acl-long.177) |  | 0 | We introduce the largest transcribed Arabic speech corpus, QASR, collected from the broadcast domain. This multi-dialect speech dataset contains 2,000 hours of speech sampled at 16kHz crawled from Aljazeera news channel. The dataset is released with lightly supervised transcriptions, aligned with the audio segments. Unlike previous datasets, QASR contains linguistically motivated segmentation, punctuation, speaker information among others. QASR is suitable for training and evaluating speech... | Hamdy Mubarak, Amir Hussein, Shammur Absar Chowdhury, Ahmed Ali |  |
| 355 |  |  [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.178) |  | 0 | The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration. In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models. First, we study and report three HPO algorithms’ performances on fine-tuning two state-of-the-art language models on the GLUE dataset. We find that using the same time budget, HPO often fails to outperform grid search due to two reasons:... | Xueqing Liu, Chi Wang |  |
| 356 |  |  [Better than Average: Paired Evaluation of NLP systems](https://doi.org/10.18653/v1/2021.acl-long.179) |  | 0 | Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instancelevel... | Maxime Peyrard, Wei Zhao, Steffen Eger, Robert West |  |
| 357 |  |  [Chase: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL](https://doi.org/10.18653/v1/2021.acl-long.180) |  | 0 | The cross-database context-dependent Text-to-SQL (XDTS) problem has attracted considerable attention in recent years due to its wide range of potential applications. However, we identify two biases in existing datasets for XDTS: (1) a high proportion of context-independent questions and (2) a high proportion of easy SQL queries. These biases conceal the major challenges in XDTS to some extent. In this work, we present Chase, a large-scale and pragmatic Chinese dataset for XDTS. It consists of... | Jiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, JianGuang Lou, Zijiang Yang, Ting Liu |  |
| 358 |  |  [CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding](https://doi.org/10.18653/v1/2021.acl-long.181) |  | 0 | Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations. Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics. Different from the image processing field, the text is discrete and few word substitutions can cause significant... | Dong Wang, Ning Ding, Piji Li, Haitao Zheng |  |
| 359 |  |  [Tree-Structured Topic Modeling with Nonparametric Neural Variational Inference](https://doi.org/10.18653/v1/2021.acl-long.182) |  | 0 | Topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure. Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process, such methods have data scalability issues. In this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference. Particularly, the latent components of the... | Ziye Chen, Cheng Ding, Zusheng Zhang, Yanghui Rao, Haoran Xie |  |
| 360 |  |  [ExCAR: Event Graph Knowledge Enhanced Explainable Causal Reasoning](https://doi.org/10.18653/v1/2021.acl-long.183) |  | 0 | Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning... | Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin |  |
| 361 |  |  [Distributed Representations of Emotion Categories in Emotion Space](https://doi.org/10.18653/v1/2021.acl-long.184) |  | 0 | Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories. The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors. However, emotion relations are ignored in one-hot representations. In this article, we first propose a general framework to learn the... | Xiangyu Wang, Chengqing Zong |  |
| 362 |  |  [Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding](https://doi.org/10.18653/v1/2021.acl-long.185) |  | 0 | Every natural text is written in some style. Style is formed by a complex combination of different stylistic factors, including formality markers, emotions, metaphors, etc. One cannot form a complete understanding of a text without considering these factors. The factors combine and co-vary in complex ways to form styles. Studying the nature of the covarying combinations sheds light on stylistic language in general, sometimes called cross-style language understanding. This paper provides the... | Dongyeop Kang, Eduard H. Hovy |  |
| 363 |  |  [DynaSent: A Dynamic Benchmark for Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.186) |  | 0 | We introduce DynaSent (‘Dynamic Sentiment’), a new English-language benchmark task for ternary (positive/negative/neutral) sentiment analysis. DynaSent combines naturally occurring sentences with sentences created using the open-source Dynabench Platform, which facilities human-and-model-in-the-loop dataset creation. DynaSent has a total of 121,634 sentences, each validated by five crowdworkers, and its development and test splits are designed to produce chance performance for even the best... | Christopher Potts, Zhengxuan Wu, Atticus Geiger, Douwe Kiela |  |
| 364 |  |  [A Hierarchical VAE for Calibrating Attributes while Generating Text using Normalizing Flow](https://doi.org/10.18653/v1/2021.acl-long.187) |  | 0 | In this digital age, online users expect personalized content. To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.). Though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer. In this paper we propose a hierarchical architecture for finer... | Bidisha Samanta, Mohit Agrawal, Niloy Ganguly |  |
| 365 |  |  [A Unified Generative Framework for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.188) |  | 0 | Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a... | Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, Zheng Zhang |  |
| 366 |  |  [Discovering Dialogue Slots with Weak Supervision](https://doi.org/10.18653/v1/2021.acl-long.189) |  | 0 | Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain. We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms. Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging... | Vojtech Hudecek, Ondrej Dusek, Zhou Yu |  |
| 367 |  |  [Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU](https://doi.org/10.18653/v1/2021.acl-long.190) |  | 0 | Intent classification is a major task in spoken language understanding (SLU). Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use. Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data. This paper proposes to train a model with only IND data while supporting both IND... | Yilin Shen, YenChang Hsu, Avik Ray, Hongxia Jin |  |
| 368 |  |  [ProtAugment: Intent Detection Meta-Learning through Unsupervised Diverse Paraphrasing](https://doi.org/10.18653/v1/2021.acl-long.191) |  | 0 | Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes. In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task). ProtAugment is a novel extension of Prototypical Networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode. It relies on diverse paraphrasing: a... | Thomas Dopierre, Christophe Gravier, Wilfried Logerais |  |
| 369 |  |  [Robustness Testing of Language Understanding in Task-Oriented Dialog](https://doi.org/10.18653/v1/2021.acl-long.192) |  | 0 | Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution. However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important... | Jiexi Liu, Ryuichi Takanobu, Jiaxin Wen, Dazhen Wan, Hongguang Li, Weiran Nie, Cheng Li, Wei Peng, Minlie Huang |  |
| 370 |  |  [Comprehensive Study: How the Context Information of Different Granularity Affects Dialogue State Tracking?](https://doi.org/10.18653/v1/2021.acl-long.193) |  | 0 | Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal. In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state. The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state. However, it is hard for the scratch-based strategy to correctly... | Puhai Yang, Heyan Huang, XianLing Mao |  |
| 371 |  |  [OTTers: One-turn Topic Transitions for Open-Domain Dialogue](https://doi.org/10.18653/v1/2021.acl-long.194) |  | 0 | Mixed initiative in open-domain dialogue requires a system to pro-actively introduce new topics. The one-turn topic transition task explores how a system connects two topics in a cooperative and coherent manner. The goal of the task is to generate a “bridging” utterance connecting the new topic to the topic of the previous conversation turn. We are especially interested in commonsense explanations of how a new topic relates to what has been mentioned before. We first collect a new dataset of... | Karin Sevegnani, David M. Howcroft, Ioannis Konstas, Verena Rieser |  |
| 372 |  |  [Towards Robustness of Text-to-SQL Models against Synonym Substitution](https://doi.org/10.18653/v1/2021.acl-long.195) |  | 0 | Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym... | Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, Pengsheng Huang |  |
| 373 |  |  [KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.196) |  | 0 | In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations. However, existing approaches in NLP mainly focus on “WHY A” rather than contrastive “WHY A NOT B”, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields. In this paper, we focus on generating contrastive explanations with counterfactual examples in... | Qianglong Chen, Feng Ji, Xiangji Zeng, FengLin Li, Ji Zhang, Haiqing Chen, Yin Zhang |  |
| 374 |  |  [Self-Guided Contrastive Learning for BERT Sentence Representations](https://doi.org/10.18653/v1/2021.acl-long.197) |  | 0 | Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers. In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations. Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors. Moreover, we redesign... | Taeuk Kim, Kang Min Yoo, Sanggoo Lee |  |
| 375 |  |  [LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations](https://doi.org/10.18653/v1/2021.acl-long.198) |  | 0 | This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task. Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without... | Ruisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, Kai Yu |  |
| 376 |  |  [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://doi.org/10.18653/v1/2021.acl-long.199) |  | 0 | Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks. However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations. How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. In this paper, we propose a new Multi-stage Pre-training (MSP)... | Tongtong Liu, Fangxiang Feng, Xiaojie Wang |  |
| 377 |  |  [Beyond Sentence-Level End-to-End Speech Translation: Context Helps](https://doi.org/10.18653/v1/2021.acl-long.200) |  | 0 | Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs... | Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich |  |
| 378 |  |  [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://doi.org/10.18653/v1/2021.acl-long.201) |  | 0 | Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked... | Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou |  |
| 379 |  |  [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-long.202) |  | 0 | Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to... | Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang |  |
| 380 |  |  [Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities](https://doi.org/10.18653/v1/2021.acl-long.203) |  | 0 | Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can... | Jinming Zhao, Ruichen Li, Qin Jin |  |
| 381 |  |  [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://doi.org/10.18653/v1/2021.acl-long.204) |  | 0 | Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked... | Chen Xu, Bojie Hu, Yanyang Li, Yuhao Zhang, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu |  |
| 382 |  |  [N-ary Constituent Tree Parsing with Recursive Semi-Markov Model](https://doi.org/10.18653/v1/2021.acl-long.205) |  | 0 | In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children. Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction. The limitation is that the hidden nodes break the sibling relations of the n-ary node’s... | Xin Xin, Jinlong Li, Zeqi Tan |  |
| 383 |  |  [Automated Concatenation of Embeddings for Structured Prediction](https://doi.org/10.18653/v1/2021.acl-long.206) |  | 0 | Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose... | Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 384 |  |  [Multi-View Cross-Lingual Structured Prediction with Minimum Supervision](https://doi.org/10.18653/v1/2021.acl-long.207) |  | 0 | In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages. However, not all source models are created equal and some may hurt performance on the target language. Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. In this paper, we propose a... | Zechuan Hu, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 385 |  |  [The Limitations of Limited Context for Constituency Parsing](https://doi.org/10.18653/v1/2021.acl-long.208) |  | 0 | Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits. For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing. Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified... | Yuchen Li, Andrej Risteski |  |
| 386 |  |  [Neural Bi-Lexicalized PCFG Induction](https://doi.org/10.18653/v1/2021.acl-long.209) |  | 0 | Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction. However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored. In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions. Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs.... | Songlin Yang, Yanpeng Zhao, Kewei Tu |  |
| 387 |  |  [Ruddit: Norms of Offensiveness for English Reddit Comments](https://doi.org/10.18653/v1/2021.acl-long.210) |  | 0 | On social media platforms, hateful and offensive language negatively impact the mental well-being of users and the participation of people from diverse backgrounds. Automatic methods to detect offensive language have largely relied on datasets with categorical labels. However, comments can vary in their degree of offensiveness. We create the first dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive).... | Rishav Hada, Sohi Sudhir, Pushkar Mishra, Helen Yannakoudakis, Saif M. Mohammad, Ekaterina Shutova |  |
| 388 |  |  [Towards Quantifiable Dialogue Coherence Evaluation](https://doi.org/10.18653/v1/2021.acl-long.211) |  | 0 | Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems. However, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give Likert-type multi-level coherence scores, dubbed as “quantifiable”; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during... | Zheng Ye, Liucun Lu, Lishan Huang, Liang Lin, Xiaodan Liang |  |
| 389 |  |  [Assessing the Representations of Idiomaticity in Vector Models with a Noun Compound Dataset Labeled at Type and Token Levels](https://doi.org/10.18653/v1/2021.acl-long.212) |  | 0 | Accurate assessment of the ability of embedding models to capture idiomaticity may require evaluation at token rather than type level, to account for degrees of idiomaticity and possible ambiguity between literal and idiomatic usages. However, most existing resources with annotation of idiomaticity include ratings only at type level. This paper presents the Noun Compound Type and Token Idiomaticity (NCTTI) dataset, with human annotations for 280 noun compounds in English and 180 in Portuguese... | Marcos García, Tiago Kramer Vieira, Carolina Scarton, Marco Idiart, Aline Villavicencio |  |
| 390 |  |  [Factoring Statutory Reasoning as Language Understanding Challenges](https://doi.org/10.18653/v1/2021.acl-long.213) |  | 0 | Statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case. Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance. To address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in Prolog... | Nils Holzenberger, Benjamin Van Durme |  |
| 391 |  |  [Evaluating Evaluation Measures for Ordinal Classification and Ordinal Quantification](https://doi.org/10.18653/v1/2021.acl-long.214) |  | 0 | Ordinal Classification (OC) is an important classification task where the classes are ordinal. For example, an OC task for sentiment analysis could have the following classes: highly positive, positive, neutral, negative, highly negative. Clearly, evaluation measures for an OC task should penalise misclassifications by considering the ordinal nature of the classes. Ordinal Quantification (OQ) is a related task where the gold data is a distribution over ordinal classes, and the system is... | Tetsuya Sakai |  |
| 392 |  |  [Interpretable and Low-Resource Entity Matching via Decoupling Feature Learning from Decision Making](https://doi.org/10.18653/v1/2021.acl-long.215) |  | 0 | Entity Matching (EM) aims at recognizing entity records that denote the same real-world object. Neural EM models learn vector representation of entity descriptions and match entities end-to-end. Though robust, these methods require many annotated resources for training, and lack of interpretability. In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching... | Zijun Yao, Chengjiang Li, Tiansi Dong, Xin Lv, Jifan Yu, Lei Hou, Juanzi Li, Yichi Zhang, Zelin Dai |  |
| 393 |  |  [Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.216) |  | 0 | Named entity recognition (NER) is a well-studied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity... | Yongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang, Wen Wang, Weiming Lu |  |
| 394 |  |  [Text2Event: Controllable Sequence-to-Structure Generation for End-to-end Event Extraction](https://doi.org/10.18653/v1/2021.acl-long.217) |  | 0 | Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a... | Yaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, Shaoyi Chen |  |
| 395 |  |  [A Large-Scale Chinese Multimodal NER Dataset with Speech Clues](https://doi.org/10.18653/v1/2021.acl-long.218) |  | 0 | In this paper, we aim to explore an uncharted territory, which is Chinese multimodal named entity recognition (NER) with both textual and acoustic contents. To achieve this, we construct a large-scale human-annotated Chinese multimodal NER dataset, named CNERTA. Our corpus totally contains 42,987 annotated sentences accompanying by 71 hours of speech data. Based on this dataset, we propose a family of strong and representative baseline models, which can leverage textual features or multimodal... | Dianbo Sui, Zhengkun Tian, Yubo Chen, Kang Liu, Jun Zhao |  |
| 396 |  |  [A Neural Transition-based Joint Model for Disease Named Entity Recognition and Normalization](https://doi.org/10.18653/v1/2021.acl-long.219) |  | 0 | Disease is one of the fundamental entities in biomedical research. Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications. Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart. Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers... | Zongcheng Ji, Tian Xia, Mei Han, Jing Xiao |  |
| 397 |  |  [OntoED: Low-resource Event Detection with Ontology Embedding](https://doi.org/10.18653/v1/2021.acl-long.220) |  | 0 | Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED... | Shumin Deng, Ningyu Zhang, Luoqiu Li, Hui Chen, Huaixiao Tou, Mosha Chen, Fei Huang, Huajun Chen |  |
| 398 |  |  [Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.221) |  | 0 | Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences... | Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Shuming Shi, Michael R. Lyu, Irwin King |  |
| 399 |  |  [Breaking the Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://doi.org/10.18653/v1/2021.acl-long.222) |  | 0 | Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora. To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. To this end, we propose two pre-training tasks. One learns to translate a sentence from source language to target language on the sentence-level... | Linqing Chen, Junhui Li, Zhengxian Gong, Boxing Chen, Weihua Luo, Min Zhang, Guodong Zhou |  |
| 400 |  |  [Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.223) |  | 0 | Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge... | Yang Feng, Shuhao Gu, Dengji Guo, Zhengxin Yang, Chenze Shao |  |
| 401 |  |  [Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?](https://doi.org/10.18653/v1/2021.acl-long.224) |  | 0 | Five years after the first published proofs of concept, direct approaches to speech translation (ST) are now competing with traditional cascade solutions. In light of this steady progress, can we claim that the performance gap between the two is closed? Starting from this question, we present a systematic comparison between state-of-the-art systems representative of the two paradigms. Focusing on three language directions (English-German/Italian/Spanish), we conduct automatic and manual... | Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, Matteo Negri, Marco Turchi |  |
| 402 |  |  [Unsupervised Neural Machine Translation for Low-Resource Domains via Meta-Learning](https://doi.org/10.18653/v1/2021.acl-long.225) |  | 0 | Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation. However, it still suffers from data-scarce domains. To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data. We assume that domain-general knowledge is a... | Cheonbok Park, Yunwon Tae, Taehee Kim, Soyoung Yang, Mohammad Azam Khan, Lucy Park, Jaegul Choo |  |
| 403 |  |  [Lightweight Cross-Lingual Sentence Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.226) |  | 0 | Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore... | Zhuoyuan Mao, Prakhar Gupta, Chenhui Chu, Martin Jaggi, Sadao Kurohashi |  |
| 404 |  |  [ERNIE-Doc: A Retrospective Long-Document Modeling Transformer](https://doi.org/10.18653/v1/2021.acl-long.227) |  | 0 | Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption. Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers. Two well-designed techniques, namely the retrospective feed... | Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang |  |
| 405 |  |  [Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation](https://doi.org/10.18653/v1/2021.acl-long.228) |  | 0 | Recently, knowledge distillation (KD) has shown great success in BERT compression. Instead of only learning from the teacher’s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student’s performance. To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner. In this paper, however, we observe that... | Yuanxin Liu, Fandong Meng, Zheng Lin, Weiping Wang, Jie Zhou |  |
| 406 |  |  [Rational LAMOL: A Rationale-based Lifelong Learning Framework](https://doi.org/10.18653/v1/2021.acl-long.229) |  | 0 | Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks. In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying... | Kasidis Kanwatchara, Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Boonserm Kijsirikul, Peerapon Vateekul |  |
| 407 |  |  [EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering](https://doi.org/10.18653/v1/2021.acl-long.230) |  | 0 | Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on. Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples. To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity. Having obtained the clustering assignment for each sample,... | Zhibin Duan, Hao Zhang, Chaojie Wang, Zhengjue Wang, Bo Chen, Mingyuan Zhou |  |
| 408 |  |  [LeeBERT: Learned Early Exit for BERT with cross-level optimization](https://doi.org/10.18653/v1/2021.acl-long.231) |  | 0 | Pre-trained language models like BERT are performant in a wide range of natural language tasks. However, they are resource exhaustive and computationally expensive for industrial scenarios. Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT).... | Wei Zhu |  |
| 409 |  |  [Unsupervised Extractive Summarization-Based Representations for Accurate and Explainable Collaborative Filtering](https://doi.org/10.18653/v1/2021.acl-long.232) |  | 0 | We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT. Our proposed model specifically produces extractive summaries for each item and user. Unlike other types of explanations, summary-level explanations closely resemble real-life explanations. The strength of ESCOFILT lies in the fact that it unifies representation and explanation. In other words, extractive summaries both represent and explain the items and users. Our model uniquely integrates BERT,... | Reinald Adrian Pugoy, HungYu Kao |  |
| 410 |  |  [PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction](https://doi.org/10.18653/v1/2021.acl-long.233) |  | 0 | Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts. CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task. In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors. To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the... | Shulin Liu, Tao Yang, Tianchi Yue, Feng Zhang, Di Wang |  |
| 411 |  |  [Competence-based Multimodal Curriculum Learning for Medical Report Generation](https://doi.org/10.18653/v1/2021.acl-long.234) |  | 0 | Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently. Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models. This is mainly due to 1) the serious data bias and 2) the limited medical data. To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning... | Fenglin Liu, Shen Ge, Xian Wu |  |
| 412 |  |  [Learning Syntactic Dense Embedding with Correlation Graph for Automatic Readability Assessment](https://doi.org/10.18653/v1/2021.acl-long.235) |  | 0 | Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task. We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features. To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings.... | Xinying Qiu, Yuan Chen, Hanwu Chen, JianYun Nie, Yuming Shen, Dawei Lu |  |
| 413 |  |  [Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains](https://doi.org/10.18653/v1/2021.acl-long.236) |  | 0 | Pre-trained language models have been applied to various NLP tasks with considerable performance gains. However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications. One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains. We notice that... | Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang |  |
| 414 |  |  [A Semantic-based Method for Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2021.acl-long.237) |  | 0 | Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data. Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context. However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc. These distracting factors may not only mislead the model to choose a wrong answer but also make it... | Yilin Niu, Fei Huang, Jiaming Liang, Wenkai Chen, Xiaoyan Zhu, Minlie Huang |  |
| 415 |  |  [Explanations for CommonsenseQA: New Dataset and Models](https://doi.org/10.18653/v1/2021.acl-long.238) |  | 0 | CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and... | Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg |  |
| 416 |  |  [Few-Shot Question Answering by Pretraining Span Selection](https://doi.org/10.18653/v1/2021.acl-long.239) |  | 0 | In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span... | Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy |  |
| 417 |  |  [UnitedQA: A Hybrid Approach for Open Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.240) |  | 0 | To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively. In this paper, we study a hybrid approach for leveraging the strengths of both models. We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models. We demonstrate that a simple... | Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao |  |
| 418 |  |  [Database reasoning over text](https://doi.org/10.18653/v1/2021.acl-long.241) |  | 0 | Neural models have shown impressive performance gains in answering queries from natural language text. However, existing works are unable to support database queries, such as “List/Count all female athletes who were born in 20th century”, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation. We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data,... | James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Y. Halevy |  |
| 419 |  |  [Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort](https://doi.org/10.18653/v1/2021.acl-long.242) |  | 0 | In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging. Automatic metrics are not reliable when it comes to high performing systems. In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems. To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of... | Vânia Mendonça, Ricardo Rei, Luísa Coheur, Alberto Sardinha, Ana Lúcia Santos |  |
| 420 |  |  [How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models](https://doi.org/10.18653/v1/2021.acl-long.243) |  | 0 | In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding... | Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, Iryna Gurevych |  |
| 421 |  |  [Evaluating morphological typology in zero-shot cross-lingual transfer](https://doi.org/10.18653/v1/2021.acl-long.244) |  | 0 | Cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance. However, this progress has also brought to light the differences in performance across languages. Specifically, certain language families and typologies seem to consistently perform worse in these models. In this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks:... | Antonio MartínezGarcía, Toni Badia, Jeremy Barnes |  |
| 422 |  |  [From Machine Translation to Code-Switching: Generating High-Quality Code-Switched Text](https://doi.org/10.18653/v1/2021.acl-long.245) |  | 0 | Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text. In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences. We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality... | Ishan Tarunesh, Syamantak Kumar, Preethi Jyothi |  |
| 423 |  |  [Fast and Accurate Neural Machine Translation with Translation Memory](https://doi.org/10.18653/v1/2021.acl-long.246) |  | 0 | It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual... | Qiuxiang He, Guoping Huang, Qu Cui, Li Li, Lemao Liu |  |
| 424 |  |  [Annotating Online Misogyny](https://doi.org/10.18653/v1/2021.acl-long.247) |  | 0 | Online misogyny, a category of online abusive language, has serious and harmful social consequences. Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse. This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook. Secondly, we present a... | Philine Zeinert, Nanna Inie, Leon Derczynski |  |
| 425 |  |  [Few-NERD: A Few-shot Named Entity Recognition Dataset](https://doi.org/10.18653/v1/2021.acl-long.248) |  | 0 | Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and re-organize them to the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this... | Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, Zhiyuan Liu |  |
| 426 |  |  [MultiMET: A Multimodal Dataset for Metaphor Understanding](https://doi.org/10.18653/v1/2021.acl-long.249) |  | 0 | Metaphor involves not only a linguistic phenomenon, but also a cognitive phenomenon structuring human thought, which makes understanding it challenging. As a means of cognition, metaphor is rendered by more than texts alone, and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. However, previous metaphor processing and understanding has focused on texts, partly due to the unavailability of... | Dongyu Zhang, Minghao Zhang, Heting Zhang, Liang Yang, Hongfei Lin |  |
| 427 |  |  [Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech](https://doi.org/10.18653/v1/2021.acl-long.250) |  | 0 | Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities. Thus, some NLP studies have started addressing the task of counter narrative generation. Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity. In this paper, we propose a... | Margherita Fanton, Helena Bonaldi, Serra Sinem Tekiroglu, Marco Guerini |  |
| 428 |  |  [Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?](https://doi.org/10.18653/v1/2021.acl-long.251) |  | 0 | Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions. However, existing work is limited in using small benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant... | Cunxiang Wang, Pai Liu, Yue Zhang |  |
| 429 |  |  [Joint Models for Answer Verification in Question Answering Systems](https://doi.org/10.18653/v1/2021.acl-long.252) |  | 0 | This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems. Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers. For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to... | Zeyu Zhang, Thuy Vu, Alessandro Moschitti |  |
| 430 |  |  [Answering Ambiguous Questions through Generative Evidence Fusion and Round-Trip Prediction](https://doi.org/10.18653/v1/2021.acl-long.253) |  | 0 | In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them. Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers. When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity. In this paper, we present a model that aggregates and combines evidence from multiple passages... | Yifan Gao, Henghui Zhu, Patrick Ng, Cícero Nogueira dos Santos, Zhiguo Wang, Feng Nan, Dejiao Zhang, Ramesh Nallapati, Andrew O. Arnold, Bing Xiang |  |
| 431 |  |  [TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance](https://doi.org/10.18653/v1/2021.acl-long.254) |  | 0 | Hybrid data combining both tabular and textual content (e.g., financial reports) are quite pervasive in the real world. However, Question Answering (QA) over such hybrid data is largely neglected in existing research. In this work, we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data, named TAT-QA, where numerical reasoning is usually required to infer the answer, such as addition, subtraction, multiplication, division,... | Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, TatSeng Chua |  |
| 432 |  |  [Modeling Transitions of Focal Entities for Conversational Knowledge Base Question Answering](https://doi.org/10.18653/v1/2021.acl-long.255) |  | 0 | Conversational KBQA is about answering a sequence of questions related to a KB. Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history. In this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation. We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities... | Yunshi Lan, Jing Jiang |  |
| 433 |  |  [Evidence-based Factual Error Correction](https://doi.org/10.18653/v1/2021.acl-long.256) |  | 0 | This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence. This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence. We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the... | James Thorne, Andreas Vlachos |  |
| 434 |  |  [Probabilistic, Structure-Aware Algorithms for Improved Variety, Accuracy, and Coverage of AMR Alignments](https://doi.org/10.18653/v1/2021.acl-long.257) |  | 0 | We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences. We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners. Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse. Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and... | Austin Blodgett, Nathan Schneider |  |
| 435 |  |  [Meta-Learning to Compositionally Generalize](https://doi.org/10.18653/v1/2021.acl-long.258) |  | 0 | Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts. This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience. Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional... | Henry Conklin, Bailin Wang, Kenny Smith, Ivan Titov |  |
| 436 |  |  [Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation](https://doi.org/10.18653/v1/2021.acl-long.259) |  | 0 | Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain. Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks. However, this practice requires significant domain-specific data and computational resources which may not always be available. In this paper, we aim to adapt... | Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, Tong Zhang |  |
| 437 |  |  [ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](https://doi.org/10.18653/v1/2021.acl-long.260) |  | 0 | Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks. However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding. To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text. Specifically, we define two novel pre-training tasks to better understand... | Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou |  |
| 438 |  |  [Position Bias Mitigation: A Knowledge-Aware Graph Model for Emotion Cause Extraction](https://doi.org/10.18653/v1/2021.acl-long.261) |  | 0 | The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text. We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves. Existing models for ECE tend to explore such relative position information and suffer from the dataset bias. To investigate the degree of reliance... | Hanqi Yan, Lin Gui, Gabriele Pergola, Yulan He |  |
| 439 |  |  [Every Bite Is an Experience: Key Point Analysis of Business Reviews](https://doi.org/10.18653/v1/2021.acl-long.262) |  | 0 | Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has... | Roy BarHaim, Lilach Eden, Yoav Kantor, Roni Friedman, Noam Slonim |  |
| 440 |  |  [Structured Sentiment Analysis as Dependency Graph Parsing](https://doi.org/10.18653/v1/2021.acl-long.263) |  | 0 | Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification. We argue that this division has become counterproductive and propose a new unified framework to remedy the situation. We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs... | Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja Øvrelid, Erik Velldal |  |
| 441 |  |  [Consistency Regularization for Cross-Lingual Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.264) |  | 0 | Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize... | Bo Zheng, Li Dong, Shaohan Huang, Wenhui Wang, Zewen Chi, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, Furu Wei |  |
| 442 |  |  [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.265) |  | 0 | The cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. In this paper, we introduce denoising word alignment as a new cross-lingual pre-training task. Specifically, the model first self-label word alignments for parallel sentences. Then we randomly mask tokens in a bitext pair. Given a masked token, the model uses a pointer network to predict the aligned token in the other language. We alternately perform the above two... | Zewen Chi, Li Dong, Bo Zheng, Shaohan Huang, XianLing Mao, Heyan Huang, Furu Wei |  |
| 443 |  |  [Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation](https://doi.org/10.18653/v1/2021.acl-long.266) |  | 0 | Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more... | Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, Zhaopeng Tu |  |
| 444 |  |  [G-Transformer for Document-Level Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.267) |  | 0 | Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the... | Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, Weihua Luo |  |
| 445 |  |  [Prevent the Language Model from being Overconfident in Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.268) |  | 0 | The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation. Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation. Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations. The main reason is that NMT pays excessive attention to the partial translation while... | Mengqi Miao, Fandong Meng, Yijin Liu, XiaoHua Zhou, Jie Zhou |  |
| 446 |  |  [Towards Emotional Support Dialog Systems](https://doi.org/10.18653/v1/2021.acl-long.269) |  | 0 | Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats. Following reasonable procedures and using various support skills can help to effectively provide support. However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking. In this paper, we define the Emotional Support... | Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, Minlie Huang |  |
| 447 |  |  [Novel Slot Detection: A Benchmark for Discovering Unknown Slot Types in the Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2021.acl-long.270) |  | 0 | Existing slot filling models can only recognize pre-defined in-domain slot types from a limited slot set. In the practical application, a reliable dialogue system should know what it does not know. In this paper, we introduce a new task, Novel Slot Detection (NSD), in the task-oriented dialogue system. NSD aims to discover unknown or out-of-domain slot types to strengthen the capability of a dialogue system based on in-domain training data. Besides, we construct two public NSD datasets, propose... | Yanan Wu, Zhiyuan Zeng, Keqing He, Hong Xu, Yuanmeng Yan, Huixing Jiang, Weiran Xu |  |
| 448 |  |  [GTM: A Generative Triple-wise Model for Conversational Question Generation](https://doi.org/10.18653/v1/2021.acl-long.271) |  | 0 | Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction. To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation. However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence. Besides, the QA... | Lei Shen, Fandong Meng, Jinchao Zhang, Yang Feng, Jie Zhou |  |
| 449 |  |  [Diversifying Dialog Generation via Adaptive Label Smoothing](https://doi.org/10.18653/v1/2021.acl-long.272) |  | 0 | Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature. Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts. In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts. The... | Yida Wang, Yinhe Zheng, Yong Jiang, Minlie Huang |  |
| 450 |  |  [Out-of-Scope Intent Detection with Self-Supervision and Discriminative Training](https://doi.org/10.18653/v1/2021.acl-long.273) |  | 0 | Out-of-distribution (OOD) intent detection is of practical importance in task-oriented dialogue systems. Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection.In this paper, we propose a simple... | LiMing Zhan, Haowen Liang, Bo Liu, Lu Fan, XiaoMing Wu, Albert Y. S. Lam |  |
| 451 |  |  [Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker](https://doi.org/10.18653/v1/2021.acl-long.274) |  | 0 | Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph... | Runxin Xu, Tianyu Liu, Lei Li, Baobao Chang |  |
| 452 |  |  [Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path](https://doi.org/10.18653/v1/2021.acl-long.275) |  | 0 | This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional... | Yiran Wang, Hiroyuki Shindo, Yuji Matsumoto, Taro Watanabe |  |
| 453 |  |  [LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification](https://doi.org/10.18653/v1/2021.acl-long.276) |  | 0 | Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the... | Xinyu Zuo, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Weihua Peng, Yuguang Chen |  |
| 454 |  |  [Revisiting the Negative Data of Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.277) |  | 0 | Distantly supervision automatically generates plenty of training samples for relation extraction. However, it also incurs two major problems: noisy labels and imbalanced training data. Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives). Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations.... | Chenhao Xie, Jiaqing Liang, Jingping Liu, Chengsong Huang, Wenhao Huang, Yanghua Xiao |  |
| 455 |  |  [Knowing the No-match: Entity Alignment with Dangling Cases](https://doi.org/10.18653/v1/2021.acl-long.278) |  | 0 | This paper studies a new problem setting of entity alignment for knowledge graphs (KGs). Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities. As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection. The framework can opt to abstain from predicting alignment for the detected dangling... | Zequn Sun, Muhao Chen, Wei Hu |  |
| 456 |  |  [Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words](https://doi.org/10.18653/v1/2021.acl-long.279) |  | 0 | How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words? We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should... | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 457 |  |  [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://doi.org/10.18653/v1/2021.acl-long.280) |  | 0 | Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as “eye is to seeing what ear is to hearing”, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using... | Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, José CamachoCollados |  |
| 458 |  |  [Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy](https://doi.org/10.18653/v1/2021.acl-long.281) |  | 0 | This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment... | Marcos García |  |
| 459 |  |  [Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach](https://doi.org/10.18653/v1/2021.acl-long.282) |  | 0 | We propose to measure fine-grained domain relevance– the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain. Such measurement is crucial for many downstream tasks in natural language processing. To handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically. To support a fine-grained domain without relying on a matching corpus... | Jie Huang, Kevin Chang, Jinjun Xiong, WenMei Hwu |  |
| 460 |  |  [HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations](https://doi.org/10.18653/v1/2021.acl-long.283) |  | 0 | Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience. User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning. Existing work on detecting user disengagement typically requires hand-labeling many dialog samples. We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a... | Weixin Liang, Kaihui Liang, Zhou Yu |  |
| 461 |  |  [Value-Agnostic Conversational Semantic Parsing](https://doi.org/10.18653/v1/2021.acl-long.284) |  | 0 | Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted... | Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte, Alan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Andrew Wolfe, Jacob Andreas, Dan Klein |  |
| 462 |  |  [MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding](https://doi.org/10.18653/v1/2021.acl-long.285) |  | 0 | Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we... | JiaChen Gu, Chongyang Tao, ZhenHua Ling, Can Xu, Xiubo Geng, Daxin Jiang |  |
| 463 |  |  [Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental](https://doi.org/10.18653/v1/2021.acl-long.286) |  | 0 | While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs. We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order. We... | Morteza Rohanian, Julian Hough |  |
| 464 |  |  [NeuralWOZ: Learning to Collect Task-Oriented Dialogue via Model-Based Simulation](https://doi.org/10.18653/v1/2021.acl-long.287) |  | 0 | We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation. NeuralWOZ has two pipelined models, Collector and Labeler. Collector generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language, and (2) system’s API call results, which is a list of possible query responses for user requests from the given knowledge base. Labeler annotates the generated dialogue by formulating the annotation as a... | Sungdong Kim, Minsuk Chang, SangWoo Lee |  |
| 465 |  |  [CDRNN: Discovering Complex Dynamics in Human Language Processing](https://doi.org/10.18653/v1/2021.acl-long.288) |  | 0 | The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain & Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g.... | Cory Shain |  |
| 466 |  |  [Structural Guidance for Transformer Language Models](https://doi.org/10.18653/v1/2021.acl-long.289) |  | 0 | Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations. Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data. We explore two general ideas. The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of... | Peng Qian, Tahira Naseem, Roger Levy, Ramón Fernandez Astudillo |  |
| 467 |  |  [Surprisal Estimators for Human Reading Times Need Character Models](https://doi.org/10.18653/v1/2021.acl-long.290) |  | 0 | While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than... | ByungDoh Oh, Christian Clark, William Schuler |  |
| 468 |  |  [CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals](https://doi.org/10.18653/v1/2021.acl-long.291) |  | 0 | Most previous studies integrate cognitive language processing signals (e.g., eye-tracking or EEG data) into neural models of natural language processing (NLP) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features. In this paper, we propose a CogAlign approach to these issues, which learns to align textual neural representations to cognitive features. In CogAlign, we use a... | Yuqi Ren, Deyi Xiong |  |
| 469 |  |  [Self-Attention Networks Can Process Bounded Hierarchical Languages](https://doi.org/10.18653/v1/2021.acl-long.292) |  | 0 | Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention... | Shunyu Yao, Binghui Peng, Christos H. Papadimitriou, Karthik Narasimhan |  |
| 470 |  |  [TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling](https://doi.org/10.18653/v1/2021.acl-long.293) |  | 0 | We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our... | Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David C. Uthus, Zarana Parekh |  |
| 471 |  |  [H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences](https://doi.org/10.18653/v1/2021.acl-long.294) |  | 0 | We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural... | Zhenhai Zhu, Radu Soricut |  |
| 472 |  |  [Making Pre-trained Language Models Better Few-shot Learners](https://doi.org/10.18653/v1/2021.acl-long.295) |  | 0 | The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models... | Tianyu Gao, Adam Fisch, Danqi Chen |  |
| 473 |  |  [A Sweet Rabbit Hole by DARCY: Using Honeypots to Detect Universal Trigger's Adversarial Attacks](https://doi.org/10.18653/v1/2021.acl-long.296) |  | 0 | The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method. Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class. To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based... | Thai Le, Noseong Park, Dongwon Lee |  |
| 474 |  |  [Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection](https://doi.org/10.18653/v1/2021.acl-long.297) |  | 0 | Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc. Previous works generally capture effective features from texts and the propagation structure. However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data. Most approaches neglect it and may seriously limit the learning of features. Towards this issue, this... | Lingwei Wei, Dou Hu, Wei Zhou, Zhaojuan Yue, Songlin Hu |  |
| 475 |  |  [Label-Specific Dual Graph Neural Network for Multi-Label Text Classification](https://doi.org/10.18653/v1/2021.acl-long.298) |  | 0 | Multi-label text classification is one of the fundamental tasks in natural language processing. Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents. Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels. In this paper, we propose... | Qianwen Ma, Chunyuan Yuan, Wei Zhou, Songlin Hu |  |
| 476 |  |  [TAN-NTM: Topic Attention Networks for Neural Topic Modeling](https://doi.org/10.18653/v1/2021.acl-long.299) |  | 0 | Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework TAN-NTM, which... | Madhur Panwar, Shashank Shailabh, Milan Aggarwal, Balaji Krishnamurthy |  |
| 477 |  |  [Cross-language Sentence Selection via Data Augmentation and Rationale Training](https://doi.org/10.18653/v1/2021.acl-long.300) |  | 0 | This paper proposes an approach to cross-language sentence selection in a low-resource setting. It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model. Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data. Moreover, when a rationale training secondary objective is... | Yanda Chen, Chris Kedzie, Suraj Nair, Petra Galuscáková, Rui Zhang, Douglas W. Oard, Kathleen R. McKeown |  |
| 478 |  |  [A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections](https://doi.org/10.18653/v1/2021.acl-long.301) |  | 0 | Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet... | Dimitris Pappas, Ion Androutsopoulos |  |
| 479 |  |  [W-RST: Towards a Weighted RST-style Discourse Framework](https://doi.org/10.18653/v1/2021.acl-long.302) |  | 0 | Aiming for a better integration of data-driven and linguistically-inspired approaches, we explore whether RST Nuclearity, assigning a binary assessment of importance between text segments, can be replaced by automatically generated, real-valued scores, in what we call a Weighted-RST framework. In particular, we find that weighted discourse trees from auxiliary tasks can benefit key NLP downstream applications, compared to nuclearity-centered approaches. We further show that real-valued... | Patrick Huber, Wen Xiao, Giuseppe Carenini |  |
| 480 |  |  [ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences](https://doi.org/10.18653/v1/2021.acl-long.303) |  | 0 | Atomic clauses are fundamental text units for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation... | Yanjun Gao, TingHao Kenneth Huang, Rebecca J. Passonneau |  |
| 481 |  |  [Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering](https://doi.org/10.18653/v1/2021.acl-long.304) |  | 0 | Many Question-Answering (QA) datasets contain unanswerable questions, but their treatment in QA systems remains primitive. Our analysis of the Natural Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (~21%) can be explained based on the presence of unverifiable presuppositions. Through a user preference study, we demonstrate that the oracle behavior of our proposed system—which provides responses based on presupposition failure—is... | Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, Deepak Ramachandran |  |
| 482 |  |  [Adversarial Learning for Discourse Rhetorical Structure Parsing](https://doi.org/10.18653/v1/2021.acl-long.305) |  | 0 | Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data. Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect. To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point.... | Longyin Zhang, Fang Kong, Guodong Zhou |  |
| 483 |  |  [Exploring Discourse Structures for Argument Impact Classification](https://doi.org/10.18653/v1/2021.acl-long.306) |  | 0 | Discourse relations among arguments reveal logical structures of a debate conversation. However, no prior work has explicitly studied how the sequence of discourse relations influence a claim’s impact. This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument. We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized... | Xin Liu, Jiefu Ou, Yangqiu Song, Xin Jiang |  |
| 484 |  |  [Point, Disambiguate and Copy: Incorporating Bilingual Dictionaries for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.307) |  | 0 | This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation (NMT) models. By introducing three novel components: Pointer, Disambiguator, and Copier, our method PDC achieves the following merits inherently compared with previous efforts: (1) Pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; (2)... | Tong Zhang, Long Zhang, Wei Ye, Bo Li, Jinan Sun, Xiaoyu Zhu, Wen Zhao, Shikun Zhang |  |
| 485 |  |  [VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation](https://doi.org/10.18653/v1/2021.acl-long.308) |  | 0 | Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the... | Fuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi, Songfang Huang, Fei Huang, Luo Si |  |
| 486 |  |  [A unified approach to sentence segmentation of punctuated text in many languages](https://doi.org/10.18653/v1/2021.acl-long.309) |  | 0 | The sentence is a fundamental unit of text processing. Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents. Therefore, the first step in many NLP pipelines is sentence segmentation. Despite its importance, this step is the subject of relatively little research. There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the... | Rachel Wicks, Matt Post |  |
| 487 |  |  [Towards User-Driven Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.310) |  | 0 | A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text. For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs). However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2)... | Huan Lin, Liang Yao, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Degen Huang, Jinsong Su |  |
| 488 |  |  [End-to-End Lexically Constrained Machine Translation for Morphologically Rich Languages](https://doi.org/10.18653/v1/2021.acl-long.311) |  | 0 | Lexically constrained machine translation allows the user to manipulate the output sentence by enforcing the presence or absence of certain words and phrases. Although current approaches can enforce terms to appear in the translation, they often struggle to make the constraint word form agree with the rest of the generated output. Our manual analysis shows that 46% of the errors in the output of a baseline constrained model for English to Czech translation are related to agreement. We... | Josef Jon, João Paulo Aires, Dusan Varis, Ondrej Bojar |  |
| 489 |  |  [Handling Extreme Class Imbalance in Technical Logbook Datasets](https://doi.org/10.18653/v1/2021.acl-long.312) |  | 0 | Technical logbooks are a challenging and under-explored text type in automated event identification. These texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf NLP pipelines. The granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes. In this paper we focus on the problem of technical issue... | Farhad Akhbardeh, Cecilia Ovesdotter Alm, Marcos Zampieri, Travis Desell |  |
| 490 |  |  [ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation](https://doi.org/10.18653/v1/2021.acl-long.313) |  | 0 | An automated system that could assist a judge in predicting the outcome of a case would help expedite the judicial process. For such a system to be practically useful, predictions by the system should be explainable. To promote research in developing such a system, we introduce ILDC (Indian Legal Documents Corpus). ILDC is a large corpus of 35k Indian Supreme Court cases annotated with original court decisions. A portion of the corpus (a separate test set) is annotated with gold standard... | Vijit Malik, Rishabh Sanjay, Shubham Kumar Nigam, Kripabandhu Ghosh, Shouvik Kumar Guha, Arnab Bhattacharya, Ashutosh Modi |  |
| 491 |  |  [Supporting Cognitive and Emotional Empathic Writing of Students](https://doi.org/10.18653/v1/2021.acl-long.314) |  | 0 | We present an annotation approach to capturing emotional and cognitive empathy in student-written peer reviews on business models in German. We propose an annotation scheme that allows us to model emotional and cognitive empathy scores based on three types of review components. Also, we conducted an annotation study with three annotators based on 92 student essays to evaluate our annotation scheme. The obtained inter-rater agreement of α=0.79 for the components and the multi-π=0.41 for the... | Thiemo Wambsganss, Christina Niklaus, Matthias Söllner, Siegfried Handschuh, Jan Marco Leimeister |  |
| 492 |  |  [Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.315) |  | 0 | The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world’s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both... | Alexander Hanbo Li, Patrick Ng, Peng Xu, Henghui Zhu, Zhiguo Wang, Bing Xiang |  |
| 493 |  |  [Generation-Augmented Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.316) |  | 0 | We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts... | Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen |  |
| 494 |  |  [Check It Again: Progressive Visual Question Answering via Visual Entailment](https://doi.org/10.18653/v1/2021.acl-long.317) |  | 0 | While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer. Several recent approaches have been developed to address this language priors problem. However, most of them predict the correct answer according to one best output without checking the authenticity of answers. Besides, they only explore the interaction between image and... | Qingyi Si, Zheng Lin, Mingyu Zheng, Peng Fu, Weiping Wang |  |
| 495 |  |  [A Mutual Information Maximization Approach for the Spurious Solution Problem in Weakly Supervised Question Answering](https://doi.org/10.18653/v1/2021.acl-long.318) |  | 0 | Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided. This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers). For example, for discrete reasoning tasks as on DROP, there may exist many equations to... | Zhihong Shao, Lifeng Shang, Qun Liu, Minlie Huang |  |
| 496 |  |  [Breaking Down Walls of Text: How Can NLP Benefit Consumer Privacy?](https://doi.org/10.18653/v1/2021.acl-long.319) |  | 0 | Privacy plays a crucial role in preserving democratic ideals and personal autonomy. The dominant legal approach to privacy in many jurisdictions is the “Notice and Choice” paradigm, where privacy policies are the primary instrument used to convey information to users. However, privacy policies are long and complex documents that are difficult for users to read and comprehend. We discuss how language technologies can play an important role in addressing this information gap, reporting on initial... | Abhilasha Ravichander, Alan W. Black, Thomas B. Norton, Shomir Wilson, Norman M. Sadeh |  |
| 497 |  |  [Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning](https://doi.org/10.18653/v1/2021.acl-long.320) |  | 0 | Open pit mines left many regions worldwide inhospitable or uninhabitable. Many sites are left behind in a hazardous or contaminated state, show remnants of waste, or have other restrictions imposed upon them, e.g., for the protection of human or nature. Such information has to be permanently managed in order to reuse those areas in the future. In this work we present and evaluate an automated workflow for supporting the post-mining management of former lignite open pit mines in the eastern part... | Christopher Schröder, Kim Bürgl, Yves Annanias, Andreas Niekler, Lydia Müller, Daniel Wiegreffe, Christian Bender, Christoph Mengs, Gerik Scheuermann, Gerhard Heyer |  |
| 498 |  |  [Reliability Testing for Natural Language Processing Systems](https://doi.org/10.18653/v1/2021.acl-long.321) |  | 0 | Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for... | Samson Tan, Shafiq R. Joty, Kathy Baxter, Araz Taeihagh, Gregory A. Bennett, MinYen Kan |  |
| 499 |  |  [Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data](https://doi.org/10.18653/v1/2021.acl-long.322) |  | 0 | Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care. The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders. One promising data source to help monitor human behavior is daily smartphone usage. However, care must be taken to summarize behaviors without identifying the user through personal (e.g.,... | Paul Pu Liang, Terrance Liu, Anna Cai, Michal Muszynski, Ryo Ishii, Nicholas B. Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 500 |  |  [Anonymisation Models for Text Data: State of the art, Challenges and Future Directions](https://doi.org/10.18653/v1/2021.acl-long.323) |  | 0 | This position paper investigates the problem of automated text anonymisation, which is a prerequisite for secure sharing of documents containing sensitive information about individuals. We summarise the key concepts behind text anonymisation and provide a review of current approaches. Anonymisation methods have so far been developed in two fields with little mutual interaction, namely natural language processing and privacy-preserving data publishing. Based on a case study, we outline the... | Pierre Lison, Ildikó Pilán, David Sánchez, Montserrat Batet, Lilja Øvrelid |  |
| 501 |  |  [End-to-End AMR Corefencence Resolution](https://doi.org/10.18653/v1/2021.acl-long.324) |  | 0 | Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both... | Qiankun Fu, Linfeng Song, Wenyu Du, Yue Zhang |  |
| 502 |  |  [How is BERT surprised? Layerwise detection of linguistic anomalies](https://doi.org/10.18653/v1/2021.acl-long.325) |  | 0 | Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in... | Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, Frank Rudzicz |  |
| 503 |  |  [Psycholinguistic Tripartite Graph Network for Personality Detection](https://doi.org/10.18653/v1/2021.acl-long.326) |  | 0 | Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one’s language use and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph... | Tao Yang, Feifan Yang, Haolan Ouyang, Xiaojun Quan |  |
| 504 |  |  [Verb Metaphor Detection via Contextual Relation Learning](https://doi.org/10.18653/v1/2021.acl-long.327) |  | 0 | Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word. Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling. In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts. We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and... | Wei Song, Shuhui Zhou, Ruiji Fu, Ting Liu, Lizhen Liu |  |
| 505 |  |  [Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task](https://doi.org/10.18653/v1/2021.acl-long.328) |  | 0 | Pretraining and multitask learning are widely used to improve the speech translation performance. In this study, we are interested in training a speech translation model along with an auxiliary text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more... | Yun Tang, Juan Miguel Pino, Xian Li, Changhan Wang, Dmitriy Genzel |  |
| 506 |  |  [Probing Toxic Content in Large Pre-Trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.329) |  | 0 | Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use... | Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, DitYan Yeung |  |
| 507 |  |  [Societal Biases in Language Generation: Progress and Challenges](https://doi.org/10.18653/v1/2021.acl-long.330) |  | 0 | Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the... | Emily Sheng, KaiWei Chang, Prem Natarajan, Nanyun Peng |  |
| 508 |  |  [Reservoir Transformers](https://doi.org/10.18653/v1/2021.acl-long.331) |  | 0 | We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear “reservoir” layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks. | Sheng Shen, Alexei Baevski, Ari S. Morcos, Kurt Keutzer, Michael Auli, Douwe Kiela |  |
| 509 |  |  [Subsequence Based Deep Active Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.332) |  | 0 | Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance. Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data. However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences.... | Puria Radmard, Yassir Fathullah, Aldo Lipani |  |
| 510 |  |  [Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.333) |  | 0 | In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments... | Tyler A. Chang, Yifan Xu, Weijian Xu, Zhuowen Tu |  |
| 511 |  |  [BinaryBERT: Pushing the Limit of BERT Quantization](https://doi.org/10.18653/v1/2021.acl-long.334) |  | 0 | The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution. In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization. We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. Therefore, we propose ternary weight splitting, which initializes BinaryBERT by... | Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R. Lyu, Irwin King |  |
| 512 |  |  [Are Pretrained Convolutions Better than Pretrained Transformers?](https://doi.org/10.18653/v1/2021.acl-long.335) |  | 0 | In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-fine-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting findings. Across an extensive set of experiments on... | Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, Donald Metzler |  |
| 513 |  |  [PairRE: Knowledge Graph Embeddings via Paired Relation Vectors](https://doi.org/10.18653/v1/2021.acl-long.336) |  | 0 | Distance based knowledge graph embedding methods show promising results on link prediction task, on which two topics have been widely studied: one is the ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the other is to encode various relation patterns, such as symmetry/antisymmetry. However, the existing methods fail to solve these two problems at the same time, which leads to unsatisfactory results. To mitigate this problem, we propose PairRE, a model with paired vectors... | Linlin Chao, Jianshan He, Taifeng Wang, Wei Chu |  |
| 514 |  |  [Hierarchy-aware Label Semantics Matching Network for Hierarchical Text Classification](https://doi.org/10.18653/v1/2021.acl-long.337) |  | 0 | Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy. Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information. To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch). First, we project text semantics and label semantics into a joint... | Haibin Chen, Qianli Ma, Zhenxi Lin, Jiangyue Yan |  |
| 515 |  |  [HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability](https://doi.org/10.18653/v1/2021.acl-long.338) |  | 0 | Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP. However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. This leads to inferior results when generalizing the obtained models to out-of-domain distributions. To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and... | Jiaao Chen, Dinghan Shen, Weizhu Chen, Diyi Yang |  |
| 516 |  |  [Neural Stylistic Response Generation with Disentangled Latent Variables](https://doi.org/10.18653/v1/2021.acl-long.339) |  | 0 | Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style. Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance. In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then... | Qingfu Zhu, WeiNan Zhang, Ting Liu, William Yang Wang |  |
| 517 |  |  [Intent Classification and Slot Filling for Privacy Policies](https://doi.org/10.18653/v1/2021.acl-long.340) |  | 0 | Understanding privacy policies is crucial for users as it empowers them to learn about the information that matters to them. Sentences written in a privacy policy document explain privacy practices, and the constituent text spans convey further specific information about that practice. We refer to predicting the privacy practice explained in a sentence as intent classification and identifying the text spans sharing specific information as slot filling. In this work, we propose PolicyIE, an... | Wasi Uddin Ahmad, Jianfeng Chi, Tu Le, Thomas Norton, Yuan Tian, KaiWei Chang |  |
| 518 |  |  [RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems](https://doi.org/10.18653/v1/2021.acl-long.341) |  | 0 | For task-oriented dialog systems to be maximally useful, it must be able to process conversations in a way that is (1) generalizable with a small number of training examples for new task domains, and (2) robust to user input in various styles, modalities, or domains. In pursuit of these goals, we introduce the RADDLE benchmark, a collection of corpora and tools for evaluating the performance of models across a diverse set of domains. By including tasks with limited training data, RADDLE is... | Baolin Peng, Chunyuan Li, Zhu Zhang, Chenguang Zhu, Jinchao Li, Jianfeng Gao |  |
| 519 |  |  [Semantic Representation for Dialogue Modeling](https://doi.org/10.18653/v1/2021.acl-long.342) |  | 0 | Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities. To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling. Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity. We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate... | Xuefeng Bai, Yulong Chen, Linfeng Song, Yue Zhang |  |
| 520 |  |  [A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations](https://doi.org/10.18653/v1/2021.acl-long.343) |  | 0 | Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans. However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching. To overcome the challenge, we consider decomposing the training of the knowledge-grounded... | Chongyang Tao, Changyu Chen, Jiazhan Feng, JiRong Wen, Rui Yan |  |
| 521 |  |  [Dependency-driven Relation Extraction with Attentive Graph Convolutional Networks](https://doi.org/10.18653/v1/2021.acl-long.344) |  | 0 | Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities. However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great... | Yuanhe Tian, Guimin Chen, Yan Song, Xiang Wan |  |
| 522 |  |  [Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP](https://doi.org/10.18653/v1/2021.acl-long.345) |  | 0 | Retrieval is a core component for open-domain NLP tasks. In open-domain tasks, multiple entities can share a name, making disambiguation an inherent yet under-explored problem. We propose an evaluation benchmark for assessing the entity disambiguation capabilities of these retrievers, which we call Ambiguous Entity Retrieval (AmbER) sets. We define an AmbER set as a collection of entities that share a name along with queries about those entities. By covering the set of entities for polysemous... | Anthony Chen, Pallavi Gudipati, Shayne Longpre, Xiao Ling, Sameer Singh |  |
| 523 |  |  [Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?](https://doi.org/10.18653/v1/2021.acl-long.346) |  | 0 | Leaderboards are widely used in NLP and push the field forward. While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models). Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made. Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. Using... | Pedro Rodriguez, Joe Barrow, Alexander Miserlis Hoyle, John P. Lalor, Robin Jia, Jordan L. BoydGraber |  |
| 524 |  |  [Claim Matching Beyond English to Scale Global Fact-Checking](https://doi.org/10.18653/v1/2021.acl-long.347) |  | 0 | Manual fact-checking does not scale well to serve the needs of the internet. This issue is further compounded in non-English contexts. In this paper, we discuss claim matching as a possible solution to scale fact-checking. We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check. We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for... | Ashkan Kazemi, Kiran Garimella, Devin Gaffney, Scott Hale |  |
| 525 |  |  [SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.348) |  | 0 | While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose... | Shuo Ren, Long Zhou, Shujie Liu, Furu Wei, Ming Zhou, Shuai Ma |  |
| 526 |  |  [Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models](https://doi.org/10.18653/v1/2021.acl-long.349) |  | 0 | The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability. Despite this mismatch between the training objective and task... | Sumanta Bhattacharyya, Amirmohammad Rooshenas, Subhajit Naskar, Simeng Sun, Mohit Iyyer, Andrew McCallum |  |
| 527 |  |  [Syntax-augmented Multilingual BERT for Cross-lingual Transfer](https://doi.org/10.18653/v1/2021.acl-long.350) |  | 0 | In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. However, due to typological differences across languages, the cross-lingual transfer is challenging. Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax,... | Wasi Uddin Ahmad, Haoran Li, KaiWei Chang, Yashar Mehdad |  |
| 528 |  |  [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://doi.org/10.18653/v1/2021.acl-long.351) |  | 0 | Pretrained multilingual models (PMMs) enable zero-shot learning via cross-lingual transfer, performing best for languages seen during pretraining. While methods exist to improve performance for unseen languages, they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the world’s languages. In this paper, we evaluate the performance of existing methods to adapt PMMs to new languages using a resource available for close to 1600 languages: the... | Abteen Ebrahimi, Katharina Kann |  |
| 529 |  |  [Weakly Supervised Named Entity Tagging with Learnable Logical Rules](https://doi.org/10.18653/v1/2021.acl-long.352) |  | 0 | We study the problem of building entity tagging systems by using a few rules as weak supervision. Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given. In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner. Specifically, we introduce compound rules that are composed from simple rules to increase the precision of... | Jiacheng Li, Haibo Ding, Jingbo Shang, Julian J. McAuley, Zhe Feng |  |
| 530 |  |  [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://doi.org/10.18653/v1/2021.acl-long.353) |  | 0 | Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix.... | Xiang Lisa Li, Percy Liang |  |
| 531 |  |  [One2Set: Generating Diverse Keyphrases as a Set](https://doi.org/10.18653/v1/2021.acl-long.354) |  | 0 | Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set... | Jiacheng Ye, Tao Gui, Yichao Luo, Yige Xu, Qi Zhang |  |
| 532 |  |  [Continuous Language Generative Flow](https://doi.org/10.18653/v1/2021.acl-long.355) |  | 0 | Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models. However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG. In this paper, we propose a flow-based... | Zineng Tang, Shiyue Zhang, Hyounghun Kim, Mohit Bansal |  |
| 533 |  |  [TWAG: A Topic-Guided Wikipedia Abstract Generator](https://doi.org/10.18653/v1/2021.acl-long.356) |  | 0 | Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques. However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics. In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information. First, we detect the topic of each input... | Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui |  |
| 534 |  |  [ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data](https://doi.org/10.18653/v1/2021.acl-long.357) |  | 0 | Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news... | Woojeong Jin, Rahul Khanna, Suji Kim, DongHo Lee, Fred Morstatter, Aram Galstyan, Xiang Ren |  |
| 535 |  |  [Recursive Tree-Structured Self-Attention for Answer Sentence Selection](https://doi.org/10.18653/v1/2021.acl-long.358) |  | 0 | Syntactic structure is an important component of natural language text. Recent top-performing models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure. Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. We investigate whether tree structures can boost performance in AS2. We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2. The... | Khalil Mrini, Emilia Farcas, Ndapa Nakashole |  |
| 536 |  |  [How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.359) |  | 0 | Knowledge Graph (KG) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. However, only qualitative analysis and ablation study are provided as evidence. In this paper, we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and KG on bag-level relation extraction (RE). We find that (1) higher attention accuracy may lead to worse performance as it may harm the model’s... | Zikun Hu, Yixin Cao, Lifu Huang, TatSeng Chua |  |
| 537 |  |  [Trigger is Not Sufficient: Exploiting Frame-aware Knowledge for Implicit Event Argument Extraction](https://doi.org/10.18653/v1/2021.acl-long.360) |  | 0 | Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method... | Kaiwen Wei, Xian Sun, Zequn Zhang, Jingyuan Zhang, Zhi Guo, Li Jin |  |
| 538 |  |  [Element Intervention for Open Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.361) |  | 0 | Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction. Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems... | Fangchao Liu, Lingyong Yan, Hongyu Lin, Xianpei Han, Le Sun |  |
| 539 |  |  [AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding](https://doi.org/10.18653/v1/2021.acl-long.362) |  | 0 | Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multi-attribute model,... | Jun Yan, Nasser Zalmout, Yan Liang, Christan Grant, Xiang Ren, Xin Luna Dong |  |
| 540 |  |  [CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction](https://doi.org/10.18653/v1/2021.acl-long.363) |  | 0 | Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a... | Zhengbao Jiang, Jialong Han, Bunyamin Sisman, Xin Luna Dong |  |
| 541 |  |  [Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference](https://doi.org/10.18653/v1/2021.acl-long.364) |  | 0 | Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering. Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time. Thus, it is well-suited for processing streams of data where new entities are frequently introduced. Despite these benefits, this task is currently difficult to study, as existing... | Robert L. Logan IV, Andrew McCallum, Sameer Singh, Daniel M. Bikel |  |
| 542 |  |  [Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.acl-long.365) |  | 0 | Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing models. When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning,... | Zixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng |  |
| 543 |  |  [Employing Argumentation Knowledge Graphs for Neural Argument Generation](https://doi.org/10.18653/v1/2021.acl-long.366) |  | 0 | Generating high-quality arguments, while being challenging, may benefit a wide range of downstream applications, such as writing assistants and argument search engines. Motivated by the effectiveness of utilizing knowledge graphs for supporting general text generation tasks, this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments. In particular, we construct and populate three knowledge graphs, employing several compositions of them to... | Khalid Al Khatib, Lukas Trautner, Henning Wachsmuth, Yufang Hou, Benno Stein |  |
| 544 |  |  [Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2021.acl-long.367) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term. Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between... | Lu Xu, Yew Ken Chia, Lidong Bing |  |
| 545 |  |  [On Compositional Generalization of Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.368) |  | 0 | Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT. However, there still exist significant issues such as robustness, domain generalization, etc. In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs. We quantitatively analyze effects of various factors using compound translation error rate, then... | Yafu Li, Yongjing Yin, Yulong Chen, Yue Zhang |  |
| 546 |  |  [Mask-Align: Self-Supervised Neural Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.369) |  | 0 | Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks. Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence. In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side. Our... | Chi Chen, Maosong Sun, Yang Liu |  |
| 547 |  |  [GWLAN: General Word-Level AutocompletioN for Computer-Aided Translation](https://doi.org/10.18653/v1/2021.acl-long.370) |  | 0 | Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators. Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT. There are two limitations in previous research in this line. First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole... | Huayang Li, Lemao Liu, Guoping Huang, Shuming Shi |  |
| 548 |  |  [De-biasing Distantly Supervised Named Entity Recognition via Causal Intervention](https://doi.org/10.18653/v1/2021.acl-long.371) |  | 0 | Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching. Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models. In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and... | Wenkai Zhang, Hongyu Lin, Xianpei Han, Le Sun |  |
| 549 |  |  [A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.372) |  | 0 | Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention. The majority of previous work focuses on either overlapped or discontinuous entities. In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly. The model includes two major steps. First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized. Second, we perform... | Fei Li, Zhichao Lin, Meishan Zhang, Donghong Ji |  |
| 550 |  |  [MLBiNet: A Cross-Sentence Collective Event Detection Network](https://doi.org/10.18653/v1/2021.acl-long.373) |  | 0 | We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model... | Dongfang Lou, Zhilin Liao, Shumin Deng, Ningyu Zhang, Huajun Chen |  |
| 551 |  |  [Exploiting Document Structures and Cluster Consistencies for Event Coreference Resolution](https://doi.org/10.18653/v1/2021.acl-long.374) |  | 0 | We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters. Deep learning methods have recently been applied for this task to deliver state-of-the-art performance. However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context. In addition, consistency... | Hieu Minh Tran, Duy Phung, Thien Huu Nguyen |  |
| 552 |  |  [StereoRel: Relational Triple Extraction from a Stereoscopic Perspective](https://doi.org/10.18653/v1/2021.acl-long.375) |  | 0 | Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest. However, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation. To intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic... | Xuetao Tian, Liping Jing, Lu He, Feng Liu |  |
| 553 |  |  [Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks](https://doi.org/10.18653/v1/2021.acl-long.376) |  | 0 | Identifying causal relations of events is an important task in natural language processing area. However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues. Existing methods cannot handle well the problem, especially in the condition of lacking training data. Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge. Inspired by it,... | Pengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao, Yuguang Chen, Weihua Peng |  |
| 554 |  |  [Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution](https://doi.org/10.18653/v1/2021.acl-long.377) |  | 0 | Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks. Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications. Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked. In this work, we present invisible backdoors... | Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, Maosong Sun |  |
| 555 |  |  [Parameter-Efficient Transfer Learning with Diff Pruning](https://doi.org/10.18653/v1/2021.acl-long.378) |  | 0 | The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. Diff pruning enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a task-specific “diff” vector that extends the original pretrained parameters. This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity. As the number of tasks increases, diff pruning... | Demi Guo, Alexander M. Rush, Yoon Kim |  |
| 556 |  |  [R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.379) |  | 0 | Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training... | Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard de Melo |  |
| 557 |  |  [Risk Minimization for Zero-shot Sequence Labeling](https://doi.org/10.18653/v1/2021.acl-long.380) |  | 0 | Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. One straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. In this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and... | Zechuan Hu, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, Kewei Tu |  |
| 558 |  |  [WARP: Word-level Adversarial ReProgramming](https://doi.org/10.18653/v1/2021.acl-long.381) |  | 0 | Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings... | Karen Hambardzumyan, Hrant Khachatrian, Jonathan May |  |
| 559 |  |  [Lexicon Learning for Few Shot Sequence Modeling](https://doi.org/10.18653/v1/2021.acl-long.382) |  | 0 | Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following. The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets. Past work has shown that many failures of systematic generalization arise from neural models’ inability to disentangle lexical... | Ekin Akyürek, Jacob Andreas |  |
| 560 |  |  [Personalized Transformer for Explainable Recommendation](https://doi.org/10.18653/v1/2021.acl-long.383) |  | 0 | Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems. In these tasks, user and item IDs are important identifiers for personalization. Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words. To address this... | Lei Li, Yongfeng Zhang, Li Chen |  |
| 561 |  |  [Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques](https://doi.org/10.18653/v1/2021.acl-long.384) |  | 0 | Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive... | Kundan Krishna, Sopan Khosla, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 562 |  |  [Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2021.acl-long.385) |  | 0 | We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (TtT) non-autoregressive sequence prediction to address the deep issues hidden in CGEC. Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct... | Piji Li, Shuming Shi |  |
| 563 |  |  [Early Detection of Sexual Predators in Chats](https://doi.org/10.18653/v1/2021.acl-long.386) |  | 0 | An important risk that children face today is online grooming, where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse. Prior work has sought to automatically identify grooming chats, but only after an incidence has already happened in the context of legal prosecution. In this work, we instead investigate this problem from the point of view of prevention. We define and study the task of early sexual predator detection (eSPD)... | Matthias Vogt, Ulf Leser, Alan Akbik |  |
| 564 |  |  [Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation](https://doi.org/10.18653/v1/2021.acl-long.387) |  | 0 | Medical report generation is one of the most challenging tasks in medical image analysis. Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation. To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report... | Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma |  |
| 565 |  |  [Concept-Based Label Embedding via Dynamic Routing for Hierarchical Text Classification](https://doi.org/10.18653/v1/2021.acl-long.388) |  | 0 | Hierarchical Text Classification (HTC) is a challenging task that categorizes a textual description within a taxonomic hierarchy. Most of the existing methods focus on modeling the text. Recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries). However, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work. In this paper, we propose a novel concept-based label... | Xuepeng Wang, Li Zhao, Bing Liu, Tao Chen, Feng Zhang, Di Wang |  |
| 566 |  |  [VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words](https://doi.org/10.18653/v1/2021.acl-long.389) |  | 0 | Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency. VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K. We also show that it achieves substantial... | Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee |  |
| 567 |  |  [Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision](https://doi.org/10.18653/v1/2021.acl-long.390) |  | 0 | The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios. To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains. Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large... | Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie Bao, Zhiyuan Liu, Paul Bennett |  |
| 568 |  |  [Semi-Supervised Text Classification with Balanced Deep Representation Distributions](https://doi.org/10.18653/v1/2021.acl-long.391) |  | 0 | Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training. They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts. Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts. Unfortunately, they often suffer from low accuracy because of the margin bias... | Changchun Li, Ximing Li, Jihong Ouyang |  |
| 569 |  |  [Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval](https://doi.org/10.18653/v1/2021.acl-long.392) |  | 0 | Recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models. To obtain high efficiency, the basic structure of these models is Bi-encoder in most cases. However, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic. To address this problem, we design a method to mimic the... | Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, Wei Wu |  |
| 570 |  |  [ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer](https://doi.org/10.18653/v1/2021.acl-long.393) |  | 0 | Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks. In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive... | Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, Weiran Xu |  |
| 571 |  |  [Exploring Dynamic Selection of Branch Expansion Orders for Code Generation](https://doi.org/10.18653/v1/2021.acl-long.394) |  | 0 | Due to the great potential in facilitating software development, code generation has attracted increasing attention recently. Generally, dominant models are Seq2Tree models, which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal order may not be suitable for handling all multi-branch nodes. In this paper, we propose to equip the Seq2Tree model with a... | Hui Jiang, Chulun Zhou, Fandong Meng, Biao Zhang, Jie Zhou, Degen Huang, Qingqiang Wu, Jinsong Su |  |
| 572 |  |  [COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion](https://doi.org/10.18653/v1/2021.acl-long.395) |  | 0 | Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and... | Debjit Paul, Anette Frank |  |
| 573 |  |  [Reasoning over Entity-Action-Location Graph for Procedural Text Understanding](https://doi.org/10.18653/v1/2021.acl-long.396) |  | 0 | Procedural text understanding aims at tracking the states (e.g., create, move, destroy) and locations of the entities mentioned in a given paragraph. To effectively track the states and locations, it is essential to capture the rich semantic relations between entities, actions, and locations in the paragraph. Although recent works have achieved substantial progress, most of them focus on leveraging the inherent constraints or incorporating external knowledge for state prediction. The rich... | Hao Huang, Xiubo Geng, Jian Pei, Guodong Long, Daxin Jiang |  |
| 574 |  |  [From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding](https://doi.org/10.18653/v1/2021.acl-long.397) |  | 0 | Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously... | Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun, Weipeng Zhang, Jiansong Chen, Fan Yang, Xunliang Cai |  |
| 575 |  |  [Pre-training Universal Language Representation](https://doi.org/10.18653/v1/2021.acl-long.398) |  | 0 | Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units. This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for... | Yian Li, Hai Zhao |  |
| 576 |  |  [Structural Pre-training for Dialogue Comprehension](https://doi.org/10.18653/v1/2021.acl-long.399) |  | 0 | Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE Reader, to capture dialogue exclusive... | Zhuosheng Zhang, Hai Zhao |  |
| 577 |  |  [AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](https://doi.org/10.18653/v1/2021.acl-long.400) |  | 0 | Pre-trained language models (PLMs) have achieved great success in natural language processing. Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT. Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained... | Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu |  |
| 578 |  |  [Data Augmentation with Adversarial Training for Cross-Lingual NLI](https://doi.org/10.18653/v1/2021.acl-long.401) |  | 0 | Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. In practice, however, we still face the problem of scarce labeled data, leading to subpar results. In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful... | Xin Dong, Yaxin Zhu, Zuohui Fu, Dongkuan Xu, Gerard de Melo |  |
| 579 |  |  [Bootstrapped Unsupervised Sentence Representation Learning](https://doi.org/10.18653/v1/2021.acl-long.402) |  | 0 | As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention. In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence. Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. Meanwhile, the... | Yan Zhang, Ruidan He, Zuozhu Liu, Lidong Bing, Haizhou Li |  |
| 580 |  |  [Learning Event Graph Knowledge for Abductive Reasoning](https://doi.org/10.18653/v1/2021.acl-long.403) |  | 0 | Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering. To facilitate this task, a narrative text based abductive reasoning task 𝛼NLI is proposed, together with explorations about building reasoning framework using pretrained language models. However, abundant event commonsense knowledge is not well exploited for this task. To fill this gap, we... | Li Du, Xiao Ding, Ting Liu, Bing Qin |  |
| 581 |  |  [A Cognitive Regularizer for Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.404) |  | 0 | The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling. Specifically, we augment the canonical MLE objective for training language models... | Jason Wei, Clara Meister, Ryan Cotterell |  |
| 582 |  |  [Lower Perplexity is Not Always Human-Like](https://doi.org/10.18653/v1/2021.acl-long.405) |  | 0 | In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages.... | Tatsuki Kuribayashi, Yohei Oseki, Takumi Ito, Ryo Yoshida, Masayuki Asahara, Kentaro Inui |  |
| 583 |  |  [Word Sense Disambiguation: Towards Interactive Context Exploitation from Both Word and Sense Perspectives](https://doi.org/10.18653/v1/2021.acl-long.406) |  | 0 | Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks. However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context. In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation... | Ming Wang, Yinglin Wang |  |
| 584 |  |  [A Knowledge-Guided Framework for Frame Identification](https://doi.org/10.18653/v1/2021.acl-long.407) |  | 0 | Frame Identification (FI) is a fundamental and challenging task in frame semantic parsing. The task aims to find the exact frame evoked by a target word in a given sentence. It is generally regarded as a classification task in existing work, where frames are treated as discrete labels or represented using onehot embeddings. However, the valuable knowledge about frames is neglected. In this paper, we propose a Knowledge-Guided Frame Identification framework (KGFI) that integrates three types... | Xuefeng Su, Ru Li, Xiaoli Li, Jeff Z. Pan, Hu Zhang, Qinghua Chai, Xiaoqi Han |  |
| 585 |  |  [Obtaining Better Static Word Embeddings Using Contextual Embedding Models](https://doi.org/10.18653/v1/2021.acl-long.408) |  | 0 | The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve... | Prakhar Gupta, Martin Jaggi |  |
| 586 |  |  [Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation](https://doi.org/10.18653/v1/2021.acl-long.409) |  | 0 | A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. This inspired recent research on few-shot WSD using meta-learning. While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart. Aiming to further close this gap, we propose a model of semantic memory for WSD in... | YingJun Du, Nithin Holla, Xiantong Zhen, Cees Snoek, Ekaterina Shutova |  |
| 587 |  |  [LexFit: Lexical Fine-Tuning of Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-long.410) |  | 0 | Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal “decontextualized” word encoders even when fed... | Ivan Vulic, Edoardo Maria Ponti, Anna Korhonen, Goran Glavas |  |
| 588 |  |  [Text-Free Image-to-Speech Synthesis Using Learned Segmental Units](https://doi.org/10.18653/v1/2021.acl-long.411) |  | 0 | In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision. Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task. We conduct experiments on the Flickr8k spoken caption dataset in addition to a... | WeiNing Hsu, David Harwath, Tyler Miller, Christopher Song, James R. Glass |  |
| 589 |  |  [CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network](https://doi.org/10.18653/v1/2021.acl-long.412) |  | 0 | Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities. The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure. However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time. In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning,... | Jiajia Tang, Kang Li, Xuanyu Jin, Andrzej Cichocki, Qibin Zhao, Wanzeng Kong |  |
| 590 |  |  [Positional Artefacts Propagate Through Masked Language Model Embeddings](https://doi.org/10.18653/v1/2021.acl-long.413) |  | 0 | In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers. Namely, we find cases of persistent outlier neurons within BERT and RoBERTa’s hidden state vectors that consistently bear the smallest or largest values in said vectors. In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are... | Ziyang Luo, Artur Kulmizev, Xiaoxi Mao |  |
| 591 |  |  [Language Model Evaluation Beyond Perplexity](https://doi.org/10.18653/v1/2021.acl-long.414) |  | 0 | We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework–paired with significance tests–for evaluating the fit of language models to these trends. We find that neural language... | Clara Meister, Ryan Cotterell |  |
| 592 |  |  [Learning to Explain: Generating Stable Explanations Fast](https://doi.org/10.18653/v1/2021.acl-long.415) |  | 0 | The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose... | Xuelin Situ, Ingrid Zukerman, Cécile Paris, Sameen Maruf, Gholamreza Haffari |  |
| 593 |  |  [StereoSet: Measuring stereotypical bias in pretrained language models](https://doi.org/10.18653/v1/2021.acl-long.416) |  | 0 | A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects:... | Moin Nadeem, Anna Bethke, Siva Reddy |  |
| 594 |  |  [Alignment Rationale for Natural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.417) |  | 0 | Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors. Existing explanation methods usually pick prominent features such as words or phrases from the input text. However, for NLI, alignments among words or phrases are more enlightening clues to explain the model. To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models... | Zhongtao Jiang, Yuanzhe Zhang, Zhao Yang, Jun Zhao, Kang Liu |  |
| 595 |  |  [Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators](https://doi.org/10.18653/v1/2021.acl-long.418) |  | 0 | This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics. It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters). With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm... | Peiyu Liu, ZeFeng Gao, Wayne Xin Zhao, ZhiYuan Xie, ZhongYi Lu, JiRong Wen |  |
| 596 |  |  [On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation](https://doi.org/10.18653/v1/2021.acl-long.419) |  | 0 | In the recent advances of natural language processing, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as explanation interpretability, efficiency, and faithfulness. In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit. On top of this, we implement a hessian-free method with a model... | Wei Zhang, Ziming Huang, Yada Zhu, Guangnan Ye, Xiaodong Cui, Fan Zhang |  |
| 597 |  |  [Syntax-Enhanced Pre-trained Model](https://doi.org/10.18653/v1/2021.acl-long.420) |  | 0 | We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa. Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages. Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios. To address this, we present a model that utilizes... | Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun Quan, Daxin Jiang, Nan Duan |  |
| 598 |  |  [Matching Distributions between Model and Data: Cross-domain Knowledge Distillation for Unsupervised Domain Adaptation](https://doi.org/10.18653/v1/2021.acl-long.421) |  | 0 | Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain. Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains. However, this pipeline makes the source data risky and is inflexible for deploying the target model. This paper tackles a novel setting where only a trained source model is available and different network architectures can be... | Bo Zhang, Xiaoming Zhang, Yun Liu, Lei Cheng, Zhoujun Li |  |
| 599 |  |  [Counterfactual Inference for Text Classification Debiasing](https://doi.org/10.18653/v1/2021.acl-long.422) |  | 0 | Today’s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models’ generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of... | Chen Qian, Fuli Feng, Lijie Wen, Chunping Ma, Pengjun Xie |  |
| 600 |  |  [HieRec: Hierarchical User Interest Modeling for Personalized News Recommendation](https://doi.org/10.18653/v1/2021.acl-long.423) |  | 0 | User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec. Instead of a single user... | Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, Yongfeng Huang |  |
| 601 |  |  [PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity](https://doi.org/10.18653/v1/2021.acl-long.424) |  | 0 | Personalized news recommendation methods are widely used in online news services. These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors. However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read. In general, popular news usually contain important information and can attract users with different interests.... | Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang |  |
| 602 |  |  [Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims](https://doi.org/10.18653/v1/2021.acl-long.425) |  | 0 | False claims that have been previously fact-checked can still spread on social media. To mitigate their continual spread, detecting previously fact-checked claims is indispensable. Given a claim, existing works focus on providing evidence for detection by reranking candidate fact-checking articles (FC-articles) retrieved by BM25. However, these performances may be limited because they ignore the following characteristics of FC-articles: (1) claims are often quoted to describe the checked... | Qiang Sheng, Juan Cao, Xueyao Zhang, Xirong Li, Lei Zhong |  |
| 603 |  |  [Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble](https://doi.org/10.18653/v1/2021.acl-long.426) |  | 0 | Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a... | Yi Zhou, Xiaoqing Zheng, ChoJui Hsieh, KaiWei Chang, Xuanjing Huang |  |
| 604 |  |  [Shortformer: Better Language Modeling using Shorter Inputs](https://doi.org/10.18653/v1/2021.acl-long.427) |  | 0 | Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency... | Ofir Press, Noah A. Smith, Mike Lewis |  |
| 605 |  |  [BanditMTL: Bandit-based Multi-task Learning for Text Classification](https://doi.org/10.18653/v1/2021.acl-long.428) |  | 0 | Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification. Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit. The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm. Adopting... | Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, Wenbin Hu |  |
| 606 |  |  [Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2021.acl-long.429) |  | 0 | In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair... | Hidetaka Kamigaito, Katsuhiko Hayashi |  |
| 607 |  |  [De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-long.430) |  | 0 | Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables. The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text. The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table x and the sentence y. Specifically, in the training stage, a model can get a low empirical loss... | Wenqing Chen, Jidong Tian, Yitian Li, Hao He, Yaohui Jin |  |
| 608 |  |  [Rethinking Stealthiness of Backdoor Attack against NLP Models](https://doi.org/10.18653/v1/2021.acl-long.431) |  | 0 | Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack. Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking... | Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, Xu Sun |  |
| 609 |  |  [Crowdsourcing Learning as Domain Adaptation: A Case Study on Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.432) |  | 0 | Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers. Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models. We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators. In this way, we find that crowdsourcing could be highly similar to domain... | Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Pengjun Xie |  |
| 610 |  |  [Exploring Distantly-Labeled Rationales in Neural Network Models](https://doi.org/10.18653/v1/2021.acl-long.433) |  | 0 | Recent studies strive to incorporate various human rationales into neural networks to improve model performance, but few pay attention to the quality of the rationales. Most existing methods distribute their models’ focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words. In this paper, we propose two novel auxiliary loss functions to make better use of... | Quzhe Huang, Shengqi Zhu, Yansong Feng, Dongyan Zhao |  |
| 611 |  |  [Learning to Perturb Word Embeddings for Out-of-distribution QA](https://doi.org/10.18653/v1/2021.acl-long.434) |  | 0 | QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets. However, QA models do not generalize well to unseen data that falls outside the training distribution, due to distributional shifts. Data augmentation (DA) techniques which drop/replace words have shown to be effective in regularizing the model from overfitting to the training data. Yet, they may adversely affect the QA tasks since they incur semantic changes that may lead to wrong... | Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang |  |
| 612 |  |  [Maria: A Visual Experience Powered Conversational Agent](https://doi.org/10.18653/v1/2021.acl-long.435) |  | 0 | Arguably, the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence. Image-grounded conversation is thus proposed to address this challenge. Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image. In this paper, we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available.... | Zujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xiubo Geng, Yining Chen, Fan Liang, Daxin Jiang |  |
| 613 |  |  [A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues](https://doi.org/10.18653/v1/2021.acl-long.436) |  | 0 | Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language. Automatic evaluation of dialogues often shows insufficient correlation with human judgements. Human evaluation is reliable but labor-intensive. We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort. HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a... | Yangjun Zhang, Pengjie Ren, Maarten de Rijke |  |
| 614 |  |  [Generating Relevant and Coherent Dialogue Responses using Self-Separated Conditional Variational AutoEncoders](https://doi.org/10.18653/v1/2021.acl-long.437) |  | 0 | Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables. However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts’ semantics, leading to irrelevant and incoherent generated responses. To resolve this problem, we propose Self-separated... | Bin Sun, Shaoxiong Feng, Yiwei Li, Jiamou Liu, Kan Li |  |
| 615 |  |  [Learning to Ask Conversational Questions by Optimizing Levenshtein Distance](https://doi.org/10.18653/v1/2021.acl-long.438) |  | 0 | Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis. Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training. In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit... | Zhongkun Liu, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Maarten de Rijke, Ming Zhou |  |
| 616 |  |  [DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded Dialogue](https://doi.org/10.18653/v1/2021.acl-long.439) |  | 0 | A video-grounded dialogue system is required to understand both dialogue, which contains semantic dependencies from turn to turn, and video, which contains visual cues of spatial and temporal scene variations. Building such dialogue systems is a challenging problem, involving various reasoning types on both visual and language inputs. Existing benchmarks do not have enough annotations to thoroughly analyze dialogue systems and understand their capabilities and limitations in isolation. These... | Hung Le, Chinnadhurai Sankar, Seungwhan Moon, Ahmad Beirami, Alborz Geramifard, Satwik Kottur |  |
| 617 |  |  [MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2021.acl-long.440) |  | 0 | Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses. However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model... | Jingwen Hu, Yuchen Liu, Jinming Zhao, Qin Jin |  |
| 618 |  |  [DynaEval: Unifying Turn and Dialogue Level Evaluation](https://doi.org/10.18653/v1/2021.acl-long.441) |  | 0 | A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose DynaEval, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In DynaEval, the graph convolutional... | Chen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, Haizhou Li |  |
| 619 |  |  [CoSQA: 20, 000+ Web Queries for Code Search and Question Answering](https://doi.org/10.18653/v1/2021.acl-long.442) |  | 0 | Finding codes given natural language query is beneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce CoSQA dataset. It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance text-code matching, which works as a... | Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, Nan Duan |  |
| 620 |  |  [Rewriter-Evaluator Architecture for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.443) |  | 0 | A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this... | Yangming Li, Kaisheng Yao |  |
| 621 |  |  [Modeling Bilingual Conversational Characteristics for Neural Chat Translation](https://doi.org/10.18653/v1/2021.acl-long.444) |  | 0 | Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of... | Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou |  |
| 622 |  |  [Importance-based Neuron Allocation for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.445) |  | 0 | Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual... | Wanying Xie, Yang Feng, Shuhao Gu, Dong Yu |  |
| 623 |  |  [Transfer Learning for Sequence Generation: from Single-source to Multi-source](https://doi.org/10.18653/v1/2021.acl-long.446) |  | 0 | Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc. As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential. Although directly finetuning pretrained models on MSG tasks... | Xuancheng Huang, Jingfang Xu, Maosong Sun, Yang Liu |  |
| 624 |  |  [A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters](https://doi.org/10.18653/v1/2021.acl-long.447) |  | 0 | Few-shot crosslingual transfer has been shown to outperform its zero-shot counterpart with pretrained encoders like multilingual BERT. Despite its growing popularity, little to no attention has been paid to standardizing and analyzing the design of few-shot experiments. In this work, we highlight a fundamental risk posed by this shortcoming, illustrating that the model exhibits a high degree of sensitivity to the selection of few shots. We conduct a large-scale experimental study on 40 sets of... | Mengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vulic, Roi Reichart, Anna Korhonen, Hinrich Schütze |  |
| 625 |  |  [Coreference Reasoning in Machine Reading Comprehension](https://doi.org/10.18653/v1/2021.acl-long.448) |  | 0 | Coreference resolution is essential for natural language understanding and has been long studied in NLP. In recent years, as the format of Question Answering (QA) became a standard for machine reading comprehension (MRC), there have been data collection efforts, e.g., Dasigi et al. (2019), that attempt to evaluate the ability of MRC models to reason about coreference. However, as we show, coreference reasoning in MRC is a greater challenge than earlier thought; MRC datasets do not reflect the... | Mingzhu Wu, Nafise Sadat Moosavi, Dan Roth, Iryna Gurevych |  |
| 626 |  |  [Adapting Unsupervised Syntactic Parsing Methodology for Discourse Dependency Parsing](https://doi.org/10.18653/v1/2021.acl-long.449) |  | 0 | One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data. A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing. Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing. In this paper, we propose a simple yet effective method to adapt... | Liwen Zhang, Ge Wang, Wenjuan Han, Kewei Tu |  |
| 627 |  |  [A Conditional Splitting Framework for Efficient Constituency Parsing](https://doi.org/10.18653/v1/2021.acl-long.450) |  | 0 | We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions. Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes. The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without... | ThanhTung Nguyen, XuanPhi Nguyen, Shafiq R. Joty, Xiaoli Li |  |
| 628 |  |  [A Unified Generative Framework for Various NER Subtasks](https://doi.org/10.18653/v1/2021.acl-long.451) |  | 0 | Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER... | Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, Xipeng Qiu |  |
| 629 |  |  [An In-depth Study on Internal Structure of Chinese Words](https://doi.org/10.18653/v1/2021.acl-long.452) |  | 0 | Unlike English letters, Chinese characters have rich and specific meanings. Usually, the meaning of a word can be derived from its constituent characters in some way. Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information. This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships. First, based on newly compiled... | Chen Gong, Saihao Huang, Houquan Zhou, Zhenghua Li, Min Zhang, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan |  |
| 630 |  |  [MulDA: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual NER](https://doi.org/10.18653/v1/2021.acl-long.453) |  | 0 | Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the... | Linlin Liu, Bosheng Ding, Lidong Bing, Shafiq R. Joty, Luo Si, Chunyan Miao |  |
| 631 |  |  [Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter](https://doi.org/10.18653/v1/2021.acl-long.454) |  | 0 | Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labeling tasks due to their respective strengths. However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter... | Wei Liu, Xiyan Fu, Yue Zhang, Wenming Xiao |  |
| 632 |  |  [Math Word Problem Solving with Explicit Numerical Values](https://doi.org/10.18653/v1/2021.acl-long.455) |  | 0 | In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration. Most methods treat the numerical values in the problems as number symbols, and ignore the prominent role of the numerical values in solving the problem. In this paper, we propose a novel approach called NumS2T, which enhances math word problem solving performance by explicitly incorporating numerical values into a... | Qinzhuo Wu, Qi Zhang, Zhongyu Wei, Xuanjing Huang |  |
| 633 |  |  [Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks](https://doi.org/10.18653/v1/2021.acl-long.456) |  | 0 | Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain... | Jinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, Liang Lin |  |
| 634 |  |  [SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining](https://doi.org/10.18653/v1/2021.acl-long.457) |  | 0 | Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge... | Taolin Zhang, Zerui Cai, Chengyu Wang, Minghui Qiu, Bite Yang, Xiaofeng He |  |
| 635 |  |  [What is Your Article Based On? Inferring Fine-grained Provenance](https://doi.org/10.18653/v1/2021.acl-long.458) |  | 0 | When evaluating an article and the claims it makes, a critical reader must be able to assess where the information presented comes from, and whether the various claims are mutually consistent and support the conclusion. This motivates the study of claim provenance, which seeks to trace and explain the origins of claims. In this paper, we introduce new techniques to model and reason about the provenance of multiple interacting claims, including how to capture fine-grained information about the... | Yi Zhang, Zachary G. Ives, Dan Roth |  |
| 636 |  |  [Cross-modal Memory Networks for Radiology Report Generation](https://doi.org/10.18653/v1/2021.acl-long.459) |  | 0 | Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments. By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain. Previous studies mainly follow the encoder-decoder paradigm and... | Zhihong Chen, Yaling Shen, Yan Song, Xiang Wan |  |
| 637 |  |  [Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection](https://doi.org/10.18653/v1/2021.acl-long.460) |  | 0 | There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems. Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions.... | Kamil Kanclerz, Alicja Figas, Marcin Gruza, Tomasz Kajdanowicz, Jan Kocon, Daria Puchalska, Przemyslaw Kazienko |  |
| 638 |  |  [Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews](https://doi.org/10.18653/v1/2021.acl-long.461) |  | 0 | As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic. Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information. This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from... | Junhao Liu, Zhen Hai, Min Yang, Lidong Bing |  |
| 639 |  |  [Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding](https://doi.org/10.18653/v1/2021.acl-long.462) |  | 0 | In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with... | Xin Sun, Tao Ge, Furu Wei, Houfeng Wang |  |
| 640 |  |  [Automatic ICD Coding via Interactive Shared Representation Networks with Self-distillation Mechanism](https://doi.org/10.18653/v1/2021.acl-long.463) |  | 0 | The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes. Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task. However, existing works either ignore the long-tail of code frequency or the noisy clinical notes. To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism. Specifically, an interactive shared... | Tong Zhou, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Kun Niu, Weifeng Chong, Shengping Liu |  |
| 641 |  |  [PHMOSpell: Phonological and Morphological Knowledge Guided Chinese Spelling Check](https://doi.org/10.18653/v1/2021.acl-long.464) |  | 0 | Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of Chinese characters. Statistics reveal that most Chinese spelling errors belong to phonological or visual errors. However, previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of... | Li Huang, Junjie Li, Weiwei Jiang, Zhiyu Zhang, Minchuan Chen, Shaojun Wang, Jing Xiao |  |
| 642 |  |  [Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting](https://doi.org/10.18653/v1/2021.acl-long.465) |  | 0 | This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems... | Yi Cheng, Siyao Li, Bang Liu, Ruihui Zhao, Sujian Li, Chenghua Lin, Yefeng Zheng |  |
| 643 |  |  [Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation](https://doi.org/10.18653/v1/2021.acl-long.466) |  | 0 | Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked. Previous methods cannot deduce the factual results from the entity’s (player or team) performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to... | Liang Li, Can Ma, Yinliang Yue, Dayong Hu |  |
| 644 |  |  [POS-Constrained Parallel Decoding for Non-autoregressive Generation](https://doi.org/10.18653/v1/2021.acl-long.467) |  | 0 | The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems. A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”). The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model. However, in this work, we experimentally reveal that this assumption does not always... | Kexin Yang, Wenqiang Lei, Dayiheng Liu, Weizhen Qi, Jiancheng Lv |  |
| 645 |  |  [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://doi.org/10.18653/v1/2021.acl-long.468) |  | 0 | A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary. This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy. Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. Specifically, a plug-and-play... | Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang, Weihua Luo, Min Zhang, Haiying Zhang, Jinsong Su |  |
| 646 |  |  [TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models](https://doi.org/10.18653/v1/2021.acl-long.469) |  | 0 | In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs). We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We... | Jie He, Bo Peng, Yi Liao, Qun Liu, Deyi Xiong |  |
| 647 |  |  [Long-Span Summarization via Local Attention and Content Selection](https://doi.org/10.18653/v1/2021.acl-long.470) |  | 0 | Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization. Typically these systems are trained by fine-tuning a large pre-trained model to the target task. One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. Thus, for long document summarization, it can be challenging to train or fine-tune these models.... | Potsawee Manakul, Mark J. F. Gales |  |
| 648 |  |  [RepSum: Unsupervised Dialogue Summarization based on Replacement Strategy](https://doi.org/10.18653/v1/2021.acl-long.471) |  | 0 | In the field of dialogue summarization, due to the lack of training data, it is often difficult for supervised summary generation methods to learn vital information from dialogue context with limited data. Several attempts on unsupervised summarization for text by leveraging semantic information solely or auto-encoder strategy (i.e., sentence compression), it however cannot be adapted to the dialogue scene due to the limited words in utterances and huge gap between the dialogue and its summary.... | Xiyan Fu, Yating Zhang, Tianyi Wang, Xiaozhong Liu, Changlong Sun, Zhenglu Yang |  |
| 649 |  |  [BASS: Boosting Abstractive Summarization with Unified Semantic Graph](https://doi.org/10.18653/v1/2021.acl-long.472) |  | 0 | Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve... | Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, Haifeng Wang |  |
| 650 |  |  [Capturing Relations between Scientific Papers: An Abstractive Model for Related Work Section Generation](https://doi.org/10.18653/v1/2021.acl-long.473) |  | 0 | Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order. Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion. Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an... | Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan |  |
| 651 |  |  [Focus Attention: Promoting Faithfulness and Diversity in Summarization](https://doi.org/10.18653/v1/2021.acl-long.474) |  | 0 | Professional summaries are written with document-level information, such as the theme of the document, in mind. This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step. With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. Further, we... | Rahul Aralikatte, Shashi Narayan, Joshua Maynez, Sascha Rothe, Ryan T. McDonald |  |
| 652 |  |  [Generating Query Focused Summaries from Query-Free Resources](https://doi.org/10.18653/v1/2021.acl-long.475) |  | 0 | The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents. In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available. We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary... | Yumo Xu, Mirella Lapata |  |
| 653 |  |  [Robustifying Multi-hop QA through Pseudo-Evidentiality Training](https://doi.org/10.18653/v1/2021.acl-long.476) |  | 0 | This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences,... | Kyungjae Lee, Seungwon Hwang, Sangeun Han, Dohyeon Lee |  |
| 654 |  |  [xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.477) |  | 0 | Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering. Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time. In this paper, we propose a new contrastive learning method called Cross Momentum... | Nan Yang, Furu Wei, Binxing Jiao, Daxing Jiang, Linjun Yang |  |
| 655 |  |  [Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering](https://doi.org/10.18653/v1/2021.acl-long.478) |  | 0 | One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational... | Gangwoo Kim, Hyunjae Kim, Jungsoo Park, Jaewoo Kang |  |
| 656 |  |  [PhotoChat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling](https://doi.org/10.18653/v1/2021.acl-long.479) |  | 0 | We present a new human-human dialogue dataset - PhotoChat, the first dataset that casts light on the photo sharing behavior in online messaging. PhotoChat contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task... | Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, Jindong Chen |  |
| 657 |  |  [Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.480) |  | 0 | A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although... | Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, Ben Kao |  |
| 658 |  |  [Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering](https://doi.org/10.18653/v1/2021.acl-long.481) |  | 0 | Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and... | Ahjeong Seo, GiCheon Kang, Joonhan Park, ByoungTak Zhang |  |
| 659 |  |  [BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.482) |  | 0 | We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the... | Yinghao Li, Pranav Shetty, Lucas Liu, Chao Zhang, Le Song |  |
| 660 |  |  [CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2021.acl-long.483) |  | 0 | The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can... | Tao Chen, Haizhou Shi, Siliang Tang, Zhigang Chen, Fei Wu, Yueting Zhuang |  |
| 661 |  |  [SENT: Sentence-level Distant Relation Extraction via Negative Training](https://doi.org/10.18653/v1/2021.acl-long.484) |  | 0 | Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type. Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance. In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that “the instance does not belong to... | Ruotian Ma, Tao Gui, Linyang Li, Qi Zhang, Xuanjing Huang, Yaqian Zhou |  |
| 662 |  |  [An End-to-End Progressive Multi-Task Learning Framework for Medical Named Entity Recognition and Normalization](https://doi.org/10.18653/v1/2021.acl-long.485) |  | 0 | Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems. Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks. The mispredicted mentions from NER will directly influence the results of NEN. Therefore, the NER module is the bottleneck of the whole system. Besides, the learnable features for both tasks are beneficial to improving the model performance. To... | Baohang Zhou, Xiangrui Cai, Ying Zhang, Xiaojie Yuan |  |
| 663 |  |  [PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2021.acl-long.486) |  | 0 | Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint... | Hengyi Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Xu Ming, Yefeng Zheng |  |
| 664 |  |  [Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2021.acl-long.487) |  | 0 | Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to iden- tify and classify named entity mentions. Pro- totypical network shows superior performance on few-shot NER. However, existing prototyp- ical methods fail to differentiate rich seman- tics in other-class words, which will aggravate overfitting under few shot scenario. To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different unde-... | Meihan Tong, Shuai Wang, Bin Xu, Yixin Cao, Minghui Liu, Lei Hou, Juanzi Li |  |
| 665 |  |  [Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference](https://doi.org/10.18653/v1/2021.acl-long.488) |  | 0 | Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge. However, many previous IE methods do not utilize any external knowledge during inference. Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind. Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external... | Tuan Manh Lai, Heng Ji, ChengXiang Zhai, Quan Hung Tran |  |
| 666 |  |  [Fine-grained Information Extraction from Biomedical Literature based on Knowledge-enriched Abstract Meaning Representation](https://doi.org/10.18653/v1/2021.acl-long.489) |  | 0 | Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges. First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements. Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge. In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and... | Zixuan Zhang, Nikolaus Nova Parulian, Heng Ji, Ahmed Elsayed, Skatje Myers, Martha Palmer |  |
| 667 |  |  [Unleash GPT-2 Power for Event Detection](https://doi.org/10.18653/v1/2021.acl-long.490) |  | 0 | Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To... | Amir Pouran Ben Veyseh, Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen |  |
| 668 |  |  [CLEVE: Contrastive Pre-training for Event Extraction](https://doi.org/10.18653/v1/2021.acl-long.491) |  | 0 | Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic... | Ziqi Wang, Xiaozhi Wang, Xu Han, Yankai Lin, Lei Hou, Zhiyuan Liu, Peng Li, Juanzi Li, Jie Zhou |  |
| 669 |  |  [Document-level Event Extraction via Parallel Prediction Networks](https://doi.org/10.18653/v1/2021.acl-long.492) |  | 0 | Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can... | Hang Yang, Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Taifeng Wang |  |
| 670 |  |  [StructuralLM: Structural Pre-training for Form Understanding](https://doi.org/10.18653/v1/2021.acl-long.493) |  | 0 | Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, they almost exclusively focus on text-only representation, while neglecting cell-level layout information that is important for form image understanding. In this paper, we propose a new pre-training approach, StructuralLM, to jointly leverage cell and layout information from scanned documents. Specifically, we pre-train StructuralLM with two new designs to make the most of the... | Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, Luo Si |  |
| 671 |  |  [Dual Graph Convolutional Networks for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2021.acl-long.494) |  | 0 | Aspect-based sentiment analysis is a fine-grained sentiment classification task. Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words. However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews. To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers... | Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang, Eduard H. Hovy |  |
| 672 |  |  [Multi-Label Few-Shot Learning for Aspect Category Detection](https://doi.org/10.18653/v1/2021.acl-long.495) |  | 0 | Aspect category detection (ACD) in sentiment analysis aims to identify the aspect categories mentioned in a sentence. In this paper, we formulate ACD in the few-shot learning scenario. However, existing few-shot learning approaches mainly focus on single-label predictions. These methods can not work well for the ACD task since a sentence may contain multiple aspect categories. Therefore, we propose a multi-label few-shot learning method based on the prototypical network. To alleviate the noise,... | Mengting Hu, Shiwan Zhao, Honglei Guo, Chao Xue, Hang Gao, Tiegang Gao, Renhong Cheng, Zhong Su |  |
| 673 |  |  [Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding](https://doi.org/10.18653/v1/2021.acl-long.496) |  | 0 | Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs. Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages. This paper proposes a novel attention-guided multi-layer multi-cross encoding... | Liying Cheng, Tianyu Wu, Lidong Bing, Luo Si |  |
| 674 |  |  [A Neural Transition-based Model for Argumentation Mining](https://doi.org/10.18653/v1/2021.acl-long.497) |  | 0 | The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts. Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance. Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation. Towards these issues, we propose a neural... | Jianzhu Bao, Chuang Fan, Jipeng Wu, Yixue Dang, Jiachen Du, Ruifeng Xu |  |
| 675 |  |  [Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text](https://doi.org/10.18653/v1/2021.acl-long.498) |  | 0 | This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate’s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for... | Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst |  |
| 676 |  |  [Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence](https://doi.org/10.18653/v1/2021.acl-long.499) |  | 0 | Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation. Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text. We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level... | Jian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wenbiao Ding, Minlie Huang |  |
| 677 |  |  [OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics](https://doi.org/10.18653/v1/2021.acl-long.500) |  | 0 | Automatic metrics are essential for developing natural language generation (NLG) models, particularly for open-ended language generation tasks such as story generation. However, existing automatic metrics are observed to correlate poorly with human evaluation. The lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. Therefore, we propose OpenMEVA, a benchmark for evaluating open-ended story generation... | Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, Minlie Huang |  |
| 678 |  |  [DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation](https://doi.org/10.18653/v1/2021.acl-long.501) |  | 0 | We study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language... | Xinyu Hua, Ashwin Sreevatsa, Lu Wang |  |
| 679 |  |  [Controllable Open-ended Question Generation with A New Question Type Ontology](https://doi.org/10.18653/v1/2021.acl-long.502) |  | 0 | We investigate the less-explored task of generating open-ended questions that are typically answered by multiple sentences. We first define a new question type ontology which differentiates the nuanced nature of questions better than widely used question words. A new dataset with 4,959 questions is labeled based on the new ontology. We then propose a novel question type-aware question generation framework, augmented by a semantic graph representation, to jointly predict question focuses and... | Shuyang Cao, Lu Wang |  |
| 680 |  |  [BERTGen: Multi-task Generation through BERT](https://doi.org/10.18653/v1/2021.acl-long.503) |  | 0 | We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively. BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored. We also show BERTGen’s... | Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, Lucia Specia |  |
| 681 |  |  [Selective Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.504) |  | 0 | Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks. As an active research field in NMT, knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample. However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge. In this paper, we design a novel protocol that can... | Fusheng Wang, Jianhao Yan, Fandong Meng, Jie Zhou |  |
| 682 |  |  [Measuring and Increasing Context Usage in Context-Aware Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.505) |  | 0 | Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by... | Patrick Fernandes, Kayo Yin, Graham Neubig, André F. T. Martins |  |
| 683 |  |  [Beyond Offline Mapping: Learning Cross-lingual Word Embeddings through Context Anchoring](https://doi.org/10.18653/v1/2021.acl-long.506) |  | 0 | Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the... | Aitor Ormazabal, Mikel Artetxe, Aitor Soroa, Gorka Labaka, Eneko Agirre |  |
| 684 |  |  [CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web](https://doi.org/10.18653/v1/2021.acl-long.507) |  | 0 | We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create... | Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, Angela Fan |  |
| 685 |  |  [Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search](https://doi.org/10.18653/v1/2021.acl-long.508) |  | 0 | Despite transformers’ impressive accuracy, their computational cost is often prohibitive to use with limited computational resources. Most previous approaches to improve inference efficiency require a separate model for each possible computational budget. In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training. We train a transformer with LengthDrop, a structural variant of dropout,... | Gyuwan Kim, Kyunghyun Cho |  |
| 686 |  |  [GhostBERT: Generate More Features with Cheap Operations for BERT](https://doi.org/10.18653/v1/2021.acl-long.509) |  | 0 | Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model’s representation ability. In this paper, we propose GhostBERT, which generates more... | Zhiqi Huang, Lu Hou, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu |  |
| 687 |  |  [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://doi.org/10.18653/v1/2021.acl-long.510) |  | 0 | The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of ”lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as ”winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only... | Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, Weizhu Chen |  |
| 688 |  |  [A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations](https://doi.org/10.18653/v1/2021.acl-long.511) |  | 0 | Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others. The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. However,... | Pierre Colombo, Pablo Piantanida, Chloé Clavel |  |
| 689 |  |  [Determinantal Beam Search](https://doi.org/10.18653/v1/2021.acl-long.512) |  | 0 | Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a... | Clara Meister, Martina Forster, Ryan Cotterell |  |
| 690 |  |  [Multi-hop Graph Convolutional Network with High-order Chebyshev Approximation for Text Reasoning](https://doi.org/10.18653/v1/2021.acl-long.513) |  | 0 | Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in long-term and non-consecutive word interactions. However, existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies. In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct... | Shuoran Jiang, Qingcai Chen, Xin Liu, Baotian Hu, Lisai Zhang |  |
| 691 |  |  [Accelerating Text Communication via Abbreviated Sentence Input](https://doi.org/10.18653/v1/2021.acl-long.514) |  | 0 | Typing every character in a text message may require more time or effort than strictly necessary. Skipping spaces or other characters may be able to speed input and reduce a user’s physical input effort. This can be particularly important for people with motor impairments. In a large crowdsourced study, we found workers frequently abbreviated text by omitting mid-word vowels. We designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word... | Jiban Adhikary, Jamie Berger, Keith Vertanen |  |
| 692 |  |  [Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates](https://doi.org/10.18653/v1/2021.acl-long.515) |  | 0 | Behavior of deep neural networks can be inconsistent between different versions. Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain. This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates. Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark. We formulate the regression-free model updates into a... | Yuqing Xie, YiAn Lai, Yuanjun Xiong, Yi Zhang, Stefano Soatto |  |
| 693 |  |  [Detecting Propaganda Techniques in Memes](https://doi.org/10.18653/v1/2021.acl-long.516) |  | 0 | Propaganda can be defined as a form of communication that aims to influence the opinions or the actions of people towards a specific goal; this is achieved by means of well-defined rhetorical and psychological devices. Propaganda, in the form we know it today, can be dated back to the beginning of the 17th century. However, it is with the advent of the Internet and the social media that propaganda has started to spread on a much larger scale than before, thus becoming major societal and... | Dimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, Giovanni Da San Martino |  |
| 694 |  |  [On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study](https://doi.org/10.18653/v1/2021.acl-long.517) |  | 0 | In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC’s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on... | Divyansh Kaushik, Douwe Kiela, Zachary C. Lipton, Wentau Yih |  |
| 695 |  |  [Learning Dense Representations of Phrases at Scale](https://doi.org/10.18653/v1/2021.acl-long.518) |  | 0 | Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to... | Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen |  |
| 696 |  |  [End-to-End Training of Neural Retrievers for Open-Domain Question Answering](https://doi.org/10.18653/v1/2021.acl-long.519) |  | 0 | Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches. However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers. In this work, we systematically study retriever pre-training. We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context... | Devendra Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamilton, Bryan Catanzaro |  |
| 697 |  |  [Question Answering Over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2021.acl-long.520) |  | 0 | Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset,... | Apoorv Saxena, Soumen Chakrabarti, Partha P. Talukdar |  |
| 698 |  |  [Language Model Augmented Relevance Score](https://doi.org/10.18653/v1/2021.acl-long.521) |  | 0 | Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on n-gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model... | Ruibo Liu, Jason Wei, Soroush Vosoughi |  |
| 699 |  |  [DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts](https://doi.org/10.18653/v1/2021.acl-long.522) |  | 0 | Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and/or “anti-expert” LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to... | Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, Yejin Choi |  |
| 700 |  |  [Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models](https://doi.org/10.18653/v1/2021.acl-long.523) |  | 0 | While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces... | Tongshuang Wu, Marco Túlio Ribeiro, Jeffrey Heer, Daniel S. Weld |  |
| 701 |  |  [Metaphor Generation with Conceptual Mappings](https://doi.org/10.18653/v1/2021.acl-long.524) |  | 0 | Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions. To achieve this, we develop two methods: 1) using FrameNet-based embeddings to... | Kevin Stowe, Tuhin Chakrabarty, Nanyun Peng, Smaranda Muresan, Iryna Gurevych |  |
| 702 |  |  [Learning Latent Structures for Cross Action Phrase Relations in Wet Lab Protocols](https://doi.org/10.18653/v1/2021.acl-long.525) |  | 0 | Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research. They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions. This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions. Here, we propose methods to automatically generate a... | Chaitanya Kulkarni, Jany Chan, Eric FoslerLussier, Raghu Machiraju |  |
| 703 |  |  [Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines](https://doi.org/10.18653/v1/2021.acl-long.526) |  | 0 | Risk prediction is an essential task in financial markets. Merger and Acquisition (M&A) calls provide key insights into the claims made by company executives about the restructuring of the financial firms. Extracting vocal and textual cues from M&A calls can help model the risk associated with such financial activities. To aid the analysis of M&A calls, we curate a dataset of conference call transcripts and their corresponding audio recordings for the time period ranging from 2016 to 2020. We... | Ramit Sawhney, Mihir Goyal, Prakhar Goel, Puneet Mathur, Rajiv Ratn Shah |  |
| 704 |  |  [Mid-Air Hand Gestures for Post-Editing of Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.527) |  | 0 | To translate large volumes of text in a globally connected world, more and more translators are integrating machine translation (MT) and post-editing (PE) into their translation workflows to generate publishable quality translations. While this process has been shown to save time and reduce errors, the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output. This is affecting the interface design of translation... | Rashad Albo Jamara, Nico Herbig, Antonio Krüger, Josef van Genabith |  |
| 705 |  |  [Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning](https://doi.org/10.18653/v1/2021.acl-long.528) |  | 0 | Geometry problem solving has attracted much attention in the NLP community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language... | Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, SongChun Zhu |  |
| 706 |  |  [Joint Verification and Reranking for Open Fact Checking Over Tables](https://doi.org/10.18653/v1/2021.acl-long.529) |  | 0 | Structured information is an important knowledge source for automatic verification of factual claims. Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved. In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint... | Michael Sejr Schlichtkrull, Vladimir Karpukhin, Barlas Oguz, Mike Lewis, Wentau Yih, Sebastian Riedel |  |
| 707 |  |  [Evaluation of Thematic Coherence in Microblogs](https://doi.org/10.18653/v1/2021.acl-long.530) |  | 0 | Collecting together microblogs representing opinions about the same topics within the same timeframe is useful to a number of different tasks and practitioners. A major question is how to evaluate the quality of such thematic clusters. Here we create a corpus of microblog clusters from three different domains and time windows and define the task of evaluating thematic coherence. We provide annotation guidelines and human annotations of thematic coherence by journalist experts. We subsequently... | Iman Munire Bilal, Bo Wang, Maria Liakata, Rob Procter, Adam Tsakalidis |  |
| 708 |  |  [Neural semi-Markov CRF for Monolingual Word Alignment](https://doi.org/10.18653/v1/2021.acl-long.531) |  | 0 | Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc. In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans. We also create a new benchmark with human annotations that cover four different text genres to... | Wuwei Lan, Chao Jiang, Wei Xu |  |
| 709 |  |  [Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies](https://doi.org/10.18653/v1/2021.acl-long.532) |  | 0 | Organisations disclose their privacy practices by posting privacy policies on their websites. Even though internet users often care about their digital privacy, they usually do not read privacy policies, since understanding them requires a significant investment of time and effort. Natural language processing has been used to create experimental tools to interpret privacy policies, but there has been a lack of large privacy policy corpora to facilitate the creation of large-scale... | Mukund Srinath, Shomir Wilson, C. Lee Giles |  |
| 710 |  |  [The statistical advantage of automatic NLG metrics at the system level](https://doi.org/10.18653/v1/2021.acl-long.533) |  | 0 | Estimating the expected output quality of generation systems is central to NLG. This paper qualifies the notion that automatic metrics are not as good as humans in estimating system-level quality. Statistically, humans are unbiased, high variance estimators, while metrics are biased, low variance estimators. We compare these estimators by their error in pairwise prediction (which generation system is better?) using the bootstrap. Measuring this error is complicated: predictions are evaluated... | Johnny TianZheng Wei, Robin Jia |  |
| 711 |  |  [Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion](https://doi.org/10.18653/v1/2021.acl-long.534) |  | 0 | We present InferWiki, a Knowledge Graph Completion (KGC) dataset that improves upon existing benchmarks in inferential ability, assumptions, and patterns. First, each testing sample is predictable with supportive data in the training set. To ensure it, we propose to utilize rule-guided train/test generation, instead of conventional random split. Second, InferWiki initiates the evaluation following the open-world assumption and improves the inferential difficulty of the closed-world assumption,... | Yixin Cao, Xiang Ji, Xin Lv, Juanzi Li, Yonggang Wen, Hanwang Zhang |  |
| 712 |  |  [ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining](https://doi.org/10.18653/v1/2021.acl-long.535) |  | 0 | While online conversations can cover a vast amount of information in many different formats, abstractive text summarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues–viewpoints–assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion... | Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, Dragomir R. Radev |  |
| 713 |  |  [Improving Factual Consistency of Abstractive Summarization via Question Answering](https://doi.org/10.18653/v1/2021.acl-long.536) |  | 0 | A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. In this paper we present an approach to address factual consistency in summarization. We first propose an efficient automatic evaluation metric to measure factual consistency;... | Feng Nan, Cícero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen R. McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang |  |
| 714 |  |  [EmailSum: Abstractive Email Thread Summarization](https://doi.org/10.18653/v1/2021.acl-long.537) |  | 0 | Recent years have brought about an interest in the challenging task of summarizing conversation threads (meetings, online discussions, etc.). Such summaries help analysis of the long text to quickly catch up with the decisions made and thus improve our work or communication efficiency. To spur research in thread summarization, we have developed an abstractive Email Thread Summarization (EmailSum) dataset, which contains human-annotated short (<30 words) and long (<100 words) summaries of 2,549... | Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal |  |
| 715 |  |  [Cross-Lingual Abstractive Summarization with Limited Parallel Resources](https://doi.org/10.18653/v1/2021.acl-long.538) |  | 0 | Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the... | Yu Bai, Yang Gao, Heyan Huang |  |
| 716 |  |  [Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution](https://doi.org/10.18653/v1/2021.acl-long.539) |  | 0 | Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model’s behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in... | Jiacheng Xu, Greg Durrett |  |
| 717 |  |  [Learning Prototypical Functions for Physical Artifacts](https://doi.org/10.18653/v1/2021.acl-long.540) |  | 0 | Humans create things for a reason. Ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc. The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language. For example, if someone says “She borrowed the book” then you would assume that she intends to read the book, or if someone asks “Can I use your knife?” then you would assume that they need to cut something. In this paper, we... | Tianyu Jiang, Ellen Riloff |  |
| 718 |  |  [Verb Knowledge Injection for Multilingual Event Processing](https://doi.org/10.18653/v1/2021.acl-long.541) |  | 0 | Linguistic probing of pretrained Transformer-based language models (LMs) revealed that they encode a range of syntactic and semantic properties of a language. However, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information. In this paper, we target a specific facet of linguistic knowledge, the interplay between verb meaning and argument structure. We investigate whether injecting explicit... | Olga Majewska, Ivan Vulic, Goran Glavas, Edoardo Maria Ponti, Anna Korhonen |  |
| 719 |  |  [Dynamic Contextualized Word Embeddings](https://doi.org/10.18653/v1/2021.acl-long.542) |  | 0 | Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which... | Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Schütze |  |
| 720 |  |  [Lexical Semantic Change Discovery](https://doi.org/10.18653/v1/2021.acl-long.543) |  | 0 | While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models. In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary. By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied... | Sinan Kurtyigit, Maike Park, Dominik Schlechtweg, Jonas Kuhn, Sabine Schulte im Walde |  |
| 721 |  |  [The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity](https://doi.org/10.18653/v1/2021.acl-long.544) |  | 0 | Humans are increasingly interacting with machines through language, sometimes in contexts where the user may not know they are talking to a machine (like over the phone or a text chatbot). We aim to understand how system designers and researchers might allow their systems to confirm its non-human identity. We collect over 2,500 phrasings related to the intent of “Are you a robot?”. This is paired with over 2,500 adversarially selected utterances where only confirming the system is non-human... | David Gros, Yu Li, Zhou Yu |  |
| 722 |  |  [Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems](https://doi.org/10.18653/v1/2021.acl-long.545) |  | 0 | In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw... | Claudio S. Pinhanez, Paulo Rodrigo Cavalin, Victor Henrique Alves Ribeiro, Ana Paula Appel, Heloisa Candello, Julio Nogima, Mauro Pichiliani, Melina Alberio Guerra, Maíra de Bayser, Gabriel Louzada Malfatti, Henrique Ferreira |  |
| 723 |  |  [Space Efficient Context Encoding for Non-Task-Oriented Dialogue Generation with Graph Attention Transformer](https://doi.org/10.18653/v1/2021.acl-long.546) |  | 0 | To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context. This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context. However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept.... | Fabian Galetzka, Jewgeni Rose, David Schlangen, Jens Lehmann |  |
| 724 |  |  [DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2021.acl-long.547) |  | 0 | Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive... | Dou Hu, Lingwei Wei, Xiaoyong Huai |  |
| 725 |  |  [Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability](https://doi.org/10.18653/v1/2021.acl-long.548) |  | 0 | When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012). Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of 0.6 (Landis and Koch, 1977). These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics. We present a new alternative to... | Ka Wong, Praveen K. Paritosh, Lora Aroyo |  |
| 726 |  |  [TIMEDIAL: Temporal Commonsense Reasoning in Dialog](https://doi.org/10.18653/v1/2021.acl-long.549) |  | 0 | Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a... | Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, Manaal Faruqui |  |
| 727 |  |  [RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)](https://doi.org/10.18653/v1/2021.acl-long.550) |  | 0 | Most words are ambiguous—-i.e., they convey distinct meanings in different contexts—-and even the meanings of unambiguous words are context-dependent. Both phenomena present a challenge for NLP. Recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as Word Sense Disambiguation. However, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word... | Sean Trott, Benjamin K. Bergen |  |
| 728 |  |  [ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic](https://doi.org/10.18653/v1/2021.acl-long.551) |  | 0 | Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models,... | Muhammad AbdulMageed, AbdelRahim A. Elmadany, El Moatez Billah Nagoudi |  |
| 729 |  |  [Improving Paraphrase Detection with the Adversarial Paraphrasing Task](https://doi.org/10.18653/v1/2021.acl-long.552) |  | 0 | If two sentences have the same meaning, it should follow that they are equivalent in their inferential properties, i.e., each sentence should textually entail the other. However, many paraphrase datasets currently in widespread use rely on a sense of paraphrase based on word overlap and syntax. Can we teach them instead to identify paraphrases in a way that draws on the inferential properties of the sentences, and is not over-reliant on lexical and syntactic similarities of a sentence pair? We... | Animesh Nighojkar, John Licato |  |
| 730 |  |  [ADEPT: An Adjective-Dependent Plausibility Task](https://doi.org/10.18653/v1/2021.acl-long.553) |  | 0 | A false contract is more likely to be rejected than a contract is, yet a false key is less likely than a key to open doors. While correctly interpreting and assessing the effects of such adjective-noun pairs (e.g., false key) on the plausibility of given events (e.g., opening doors) underpins many natural language understanding tasks, doing so often requires a significant degree of world knowledge and common-sense reasoning. We introduce ADEPT – a large-scale semantic plausibility task... | Ali Emami, Ian Porada, Alexandra Olteanu, Kaheer Suleman, Adam Trischler, Jackie Chi Kit Cheung |  |
| 731 |  |  [ReadOnce Transformers: Reusable Representations of Text for Transformers](https://doi.org/10.18653/v1/2021.acl-long.554) |  | 0 | We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text. The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once. This leads to faster training and evaluation of models. Additionally, we extend standard text-to-text transformer models to... | ShihTing Lin, Ashish Sabharwal, Tushar Khot |  |
| 732 |  |  [Conditional Generation of Temporally-ordered Event Sequences](https://doi.org/10.18653/v1/2021.acl-long.555) |  | 0 | Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events. We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence. We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence,... | ShihTing Lin, Nathanael Chambers, Greg Durrett |  |
| 733 |  |  [Hate Speech Detection Based on Sentiment Knowledge Sharing](https://doi.org/10.18653/v1/2021.acl-long.556) |  | 0 | The wanton spread of hate speech on the internet brings great harm to society and families. It is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. In other words, getting more affective features from other affective resources will significantly affect the performance of hate speech detection. In this paper, we propose a... | Xianbing Zhou, Yang Yong, Xiaochao Fan, Ge Ren, Yunfeng Song, Yufeng Diao, Liang Yang, Hongfei Lin |  |
| 734 |  |  [Transition-based Bubble Parsing: Improvements on Coordination Structure Prediction](https://doi.org/10.18653/v1/2021.acl-long.557) |  | 0 | We propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously. Bubble representations were proposed in the formal linguistics literature decades ago; they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly. In this paper, we introduce a transition system and neural models for parsing these bubble-enhanced structures. Experimental... | Tianze Shi, Lillian Lee |  |
| 735 |  |  [SpanNER: Named Entity Re-/Recognition as Span Prediction](https://doi.org/10.18653/v1/2021.acl-long.558) |  | 0 | Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction. Despite its preliminary effectiveness, the span prediction model’s architectural bias has not been fully understood. In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary... | Jinlan Fu, Xuanjing Huang, Pengfei Liu |  |
| 736 |  |  [StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling](https://doi.org/10.18653/v1/2021.acl-long.559) |  | 0 | There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can induce dependency and constituency structure at the same time. To achieve this, we propose a new parsing framework that can... | Yikang Shen, Yi Tay, Che Zheng, Dara Bahri, Donald Metzler, Aaron C. Courville |  |
| 737 |  |  [Language Embeddings for Typology and Cross-lingual Transfer Learning](https://doi.org/10.18653/v1/2021.acl-long.560) |  | 0 | Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot... | Dian Yu, Taiqi He, Kenji Sagae |  |
| 738 |  |  [Can Sequence-to-Sequence Models Crack Substitution Ciphers?](https://doi.org/10.18653/v1/2021.acl-long.561) |  | 0 | Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that... | Nada Aldarrab, Jonathan May |  |
| 739 |  |  [Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.562) |  | 0 | While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on... | Eleftheria Briakou, Marine Carpuat |  |
| 740 |  |  [Discriminative Reranking for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.563) |  | 0 | Reranking models enable the integration of rich features to select a better output hypothesis within an n-best list or lattice. These models have a long history in NLP, and we revisit discriminative reranking for modern neural machine translation models by training a large transformer architecture. This takes as input both the source sentence as well as a list of hypotheses to output a ranked list. The reranker is trained to predict the observed distribution of a desired metric, e.g. BLEU, over... | Ann Lee, Michael Auli, Marc'Aurelio Ranzato |  |
| 741 |  |  [Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering](https://doi.org/10.18653/v1/2021.acl-long.564) |  | 0 | Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active... | Siddharth Karamcheti, Ranjay Krishna, Li FeiFei, Christopher D. Manning |  |
| 742 |  |  [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://doi.org/10.18653/v1/2021.acl-long.565) |  | 0 | Human evaluations are typically considered the gold standard in natural language generation, but as models’ fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts’ ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore... | Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, Noah A. Smith |  |
| 743 |  |  [Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers](https://doi.org/10.18653/v1/2021.acl-long.566) |  | 0 | This paper presents the first large-scale meta-evaluation of machine translation (MT). We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020. Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends. An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor... | Benjamin Marie, Atsushi Fujita, Raphael Rubino |  |
| 744 |  |  [Neural Machine Translation with Monolingual Translation Memory](https://doi.org/10.18653/v1/2021.acl-long.567) |  | 0 | Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT). In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner. Our framework has unique advantages. First, the cross-lingual memory retriever allows abundant monolingual data to be TM. Second, the... | Deng Cai, Yan Wang, Huayang Li, Wai Lam, Lemao Liu |  |
| 745 |  |  [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://doi.org/10.18653/v1/2021.acl-long.568) |  | 0 | Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing... | Armen Aghajanyan, Sonal Gupta, Luke Zettlemoyer |  |
| 746 |  |  [UnNatural Language Inference](https://doi.org/10.18653/v1/2021.acl-long.569) |  | 0 | Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This... | Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams |  |
| 747 |  |  [Including Signed Languages in Natural Language Processing](https://doi.org/10.18653/v1/2021.acl-long.570) |  | 0 | Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community... | Kayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, Malihe Alikhani |  |
| 748 |  |  [Vocabulary Learning via Optimal Transport for Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.571) |  | 0 | The choice of token vocabulary affects the performance of machine translation. This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory. It motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport (OT) problem. We... | Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li |  |
