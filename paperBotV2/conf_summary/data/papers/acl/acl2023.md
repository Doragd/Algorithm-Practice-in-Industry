# ACL2023

## 会议论文列表

本会议共有 2153 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2023.acl-demo.0) |  | 0 |  |  |  |
| 2 |  |  [Human-in-the-loop Schema Induction](https://doi.org/10.18653/v1/2023.acl-demo.1) |  | 0 | Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By... | Tianyi Zhang, Isaac Tham, Zhaoyi Hou, Jiaxuan Ren, Leon Zhou, Hainiu Xu, Li Zhang, Lara J. Martin, Rotem Dror, Sha Li, Heng Ji, Martha Palmer, Susan Windisch Brown, Reece Suchocki, Chris CallisonBurch |  |
| 3 |  |  [PersLEARN: Research Training through the Lens of Perspective Cultivation](https://doi.org/10.18653/v1/2023.acl-demo.2) |  | 0 | Scientific research is inherently shaped by its authors’ perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce PersLEARN , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a... | YuZhe Shi, Shiqian Li, Xinyi Niu, Qiao Xu, Jiawen Liu, Yifan Xu, Shiyu Gu, Bingru He, Xinyang Li, Xinyu Zhao, Zijian Zhao, Yidong Lyu, Zhen Li, Sijia Liu, Lin Qiu, Jinhao Ji, Lecheng Ruan, Yuxi Ma, Wenjuan Han, Yixin Zhu |  |
| 4 |  |  [LAVIS: A One-stop Library for Language-Vision Intelligence](https://doi.org/10.18653/v1/2023.acl-demo.3) |  | 0 | We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and... | Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, Steven C. H. Hoi |  |
| 5 |  |  [Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models](https://doi.org/10.18653/v1/2023.acl-demo.4) |  | 0 | Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. To address this issue, we propose Finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models.... | Bum Chul Kwon, Nandana Mihindukulasooriya |  |
| 6 |  |  [PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development](https://doi.org/10.18653/v1/2023.acl-demo.5) |  | 0 | The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce PrimeQA: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods. PrimeQA supports core QA functionalities... | Avi Sil, Jaydeep Sen, Bhavani Iyer, Martin Franz, Kshitij Fadnis, Mihaela A. Bornea, Sara Rosenthal, J. Scott McCarley, Rong Zhang, Vishwajeet Kumar, Yulong Li, Md. Arafat Sultan, Riyaz A. Bhat, Jürgen Broß, Radu Florian, Salim Roukos |  |
| 7 |  |  [Lingxi: A Diversity-aware Chinese Modern Poetry Generation System](https://doi.org/10.18653/v1/2023.acl-demo.6) |  | 0 | Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose... | Xinran Zhang, Maosong Sun, Jiafeng Liu, Xiaobing Li |  |
| 8 |  |  [Autodive: An Integrated Onsite Scientific Literature Annotation Tool](https://doi.org/10.18653/v1/2023.acl-demo.7) |  | 0 | Scientific literature is always available in Adobe’s Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, we present Autodive, an integrated onsite scientific literature annotation tool for natural scientists and Natural Language Processing (NLP) researchers. This tool provides six core functions of... | Yi Du, Ludi Wang, Mengyi Huang, Dongze Song, Wenjuan Cui, Yuanchun Zhou |  |
| 9 |  |  [A Practical Toolkit for Multilingual Question and Answer Generation](https://doi.org/10.18653/v1/2023.acl-demo.8) |  | 0 | Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e. a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 10 |  |  [OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-demo.9) |  | 0 | Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models... | Libo Qin, Qiguang Chen, Xiao Xu, Yunlong Feng, Wanxiang Che |  |
| 11 |  |  [SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes](https://doi.org/10.18653/v1/2023.acl-demo.10) |  | 0 | We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the... | Jivnesh Sandhan, Anshul Agarwal, Laxmidhar Behera, Tushar Sandhan, Pawan Goyal |  |
| 12 |  |  [LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models](https://doi.org/10.18653/v1/2023.acl-demo.11) |  | 0 | Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for... | Victor Dibia |  |
| 13 |  |  [MetaPro Online: A Computational Metaphor Processing Online System](https://doi.org/10.18653/v1/2023.acl-demo.12) |  | 0 | Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept mappings, attracting great attention from cognitive science and psychology communities. Thus, we aim to develop a computational metaphor processing online system, termed MetaPro Online, that allows... | Rui Mao, Xiao Li, Kai He, Mengshi Ge, Erik Cambria |  |
| 14 |  |  [DIAGRAPH: An Open-Source Graphic Interface for Dialog Flow Design](https://doi.org/10.18653/v1/2023.acl-demo.13) |  | 0 | In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g., for research, and 3) To provide a hands-on test bed for students learning about dialog systems. To facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating... | Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu |  |
| 15 |  |  [disco: a toolkit for Distributional Control of Generative Models](https://doi.org/10.18653/v1/2023.acl-demo.14) |  | 0 | Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researchers have introduced distributional control techniques. These techniques, not limited to language, allow controlling the prevalence (i.e. expectations) of any features of interest in the model’s... | Germán Kruszewski, Jos Rozen, Marc Dymetman |  |
| 16 |  |  [A Hyperparameter Optimization Toolkit for Neural Machine Translation Research](https://doi.org/10.18653/v1/2023.acl-demo.15) |  | 0 | Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the... | Xuan Zhang, Kevin Duh, Paul McNamee |  |
| 17 |  |  [Japanese-to-English Simultaneous Dubbing Prototype](https://doi.org/10.18653/v1/2023.acl-demo.16) |  | 0 | Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces the original speech of a live video stream in a simultaneous manner. Tests on a collection of 90 public videos show that our system achieves a low average latency of 11.90 seconds for smooth playback.... | Xiaolin Wang, Masao Utiyama, Eiichiro Sumita |  |
| 18 |  |  [VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-demo.17) |  | 0 | We present Visual Knowledge oriented Programming platform (VisKoP), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as ”dragging” to add... | Zijun Yao, Yuanyong Chen, Xin Lv, Shulin Cao, Amy Xin, Jifan Yu, Hailong Jin, Jianjun Xu, Peng Zhang, Lei Hou, Juanzi Li |  |
| 19 |  |  [PEEP-Talk: A Situational Dialogue-based Chatbot for English Education](https://doi.org/10.18653/v1/2023.acl-demo.18) |  | 0 | English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-world situational dialogue-based chatbot designed for English education. It also naturally switches to a new topic or situation in response to out-of-topic utterances, which are common among English... | Seungjun Lee, Yoonna Jang, Chanjun Park, Jungseob Lee, Jaehyung Seo, Hyeonseok Moon, Sugyeong Eo, Seounghoon Lee, Bernardo Yahya, Heuiseok Lim |  |
| 20 |  |  [OpenTIPE: An Open-source Translation Framework for Interactive Post-Editing Research](https://doi.org/10.18653/v1/2023.acl-demo.19) |  | 0 | Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, current approaches do not consider the human interactions that occur in real post-editing scenarios. To address this issue, we present OpenTIPE, a flexible and extensible framework that aims at... | Fabian Landwehr, Thomas Steinmann, Laura Mascarell |  |
| 21 |  |  [TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities](https://doi.org/10.18653/v1/2023.acl-demo.20) |  | 0 | Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the... | Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong Tian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan Liu, Weiquan Mao, Han Guo, Weigang Guo, Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen, Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xiaoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoyong Du, Linlin Shen, Kimmo Yan |  |
| 22 |  |  [NeuroX Library for Neuron Analysis of Deep NLP Models](https://doi.org/10.18653/v1/2023.acl-demo.21) |  | 0 | Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and... | Fahim Dalvi, Hassan Sajjad, Nadir Durrani |  |
| 23 |  |  [SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation](https://doi.org/10.18653/v1/2023.acl-demo.22) |  | 0 | Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and... | Nianlong Gu, Richard H. R. Hahnloser |  |
| 24 |  |  [Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search](https://doi.org/10.18653/v1/2023.acl-demo.23) |  | 0 | In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing... | Chris Jenkins, Shantanu Agarwal, Joel Barry, Steven Fincke, Elizabeth Boschee |  |
| 25 |  |  [YANMTT: Yet Another Neural Machine Translation Toolkit](https://doi.org/10.18653/v1/2023.acl-demo.24) |  | 0 | In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight... | Raj Dabre, Diptesh Kanojia, Chinmay Sawant, Eiichiro Sumita |  |
| 26 |  |  [XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models](https://doi.org/10.18653/v1/2023.acl-demo.25) |  | 0 | NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we... | DongHo Lee, Akshen Kadakia, Brihi Joshi, Aaron Chan, Ziyi Liu, Kiran Narahari, Takashi Shibuya, Ryosuke Mitani, Toshiyuki Sekiya, Jay Pujara, Xiang Ren |  |
| 27 |  |  [OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-demo.26) |  | 0 | The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as “delta tuning” in Ding et al. (2022), which updates only a small subset of parameters, known as “delta modules”, while keeping the backbone model’s parameters fixed. However, the practicality and... | Shengding Hu, Ning Ding, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu, Maosong Sun |  |
| 28 |  |  [Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation](https://doi.org/10.18653/v1/2023.acl-demo.27) |  | 0 | Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what’s available, and achance to dive deeper on some aspects. The sys-tem works by grouping similar items together,and arranging the remaining items into a hierar-chical navigable DAG structure. We apply themethod to medical information extraction. | Itay Yair, Hillel TaubTabib, Yoav Goldberg |  |
| 29 |  |  [CARE: Collaborative AI-Assisted Reading Environment](https://doi.org/10.18653/v1/2023.acl-demo.28) |  | 0 | Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as... | Dennis Zyska, Nils Dycke, Jan Buchmann, Ilia Kuznetsov, Iryna Gurevych |  |
| 30 |  |  [The ROOTS Search Tool: Data Transparency for LLMs](https://doi.org/10.18653/v1/2023.acl-demo.29) |  | 0 | ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces:... | Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurençon, Gérard Dupont, Sasha Luccioni, Yacine Jernite, Anna Rogers |  |
| 31 |  |  [The OPUS-MT Dashboard - A Toolkit for a Systematic Evaluation of Open Machine Translation Models](https://doi.org/10.18653/v1/2023.acl-demo.30) |  | 0 | The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS-MT and Tatoeba models as well as external models from the Huggingface repository and user-contributed translations. The functionalities of the evaluation tool include summaries of benchmarks for... | Jörg Tiedemann, Ona de Gibert |  |
| 32 |  |  [The D-WISE Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis](https://doi.org/10.18653/v1/2023.acl-demo.31) |  | 0 | This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in which the discourses of contemporary societies are encoded. To provide meaningful insights from such data, our system leverages and combines state-of-the-art machine learning technologies from Natural... | Florian Schneider, Tim Fischer, Fynn PetersenFrey, Isabel Eiser, Gertraud Koch, Chris Biemann |  |
| 33 |  |  [OpenRT: An Open-source Framework for Reasoning Over Tabular Data](https://doi.org/10.18653/v1/2023.acl-demo.32) |  | 0 | There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing... | Yilun Zhao, Boyu Mi, Zhenting Qi, Linyong Nan, Minghao Guo, Arman Cohan, Dragomir Radev |  |
| 34 |  |  [UINAUIL: A Unified Benchmark for Italian Natural Language Understanding](https://doi.org/10.18653/v1/2023.acl-demo.33) |  | 0 | This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available Italian and multilingual... | Valerio Basile, Livio Bioglio, Alessio Bosca, Cristina Bosco, Viviana Patti |  |
| 35 |  |  [Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction](https://doi.org/10.18653/v1/2023.acl-demo.34) |  | 0 | The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research... | Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Thanh Lam Hoang |  |
| 36 |  |  [BiSync: A Bilingual Editor for Synchronized Monolingual Texts](https://doi.org/10.18653/v1/2023.acl-demo.35) |  | 0 | In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process.... | Josep Maria Crego, Jitao Xu, François Yvon |  |
| 37 |  |  [Riveter: Measuring Power and Social Dynamics Between Entities](https://doi.org/10.18653/v1/2023.acl-demo.36) |  | 0 | Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted... | Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap |  |
| 38 |  |  [Fast Whitespace Correction with Encoder-Only Transformers](https://doi.org/10.18653/v1/2023.acl-demo.37) |  | 0 | The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with... | Hannah Bast, Matthias Hertel, Sebastian Walter |  |
| 39 |  |  [ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit](https://doi.org/10.18653/v1/2023.acl-demo.38) |  | 0 | ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers... | Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe |  |
| 40 |  |  [CB2: Collaborative Natural Language Interaction Research Platform](https://doi.org/10.18653/v1/2023.acl-demo.39) |  | 0 | CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model. | Jacob Sharf, Mustafa Omer Gul, Yoav Artzi |  |
| 41 |  |  [Inseq: An Interpretability Toolkit for Sequence Generation Models](https://doi.org/10.18653/v1/2023.acl-demo.40) |  | 0 | Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models’ internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers... | Gabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar van der Wal |  |
| 42 |  |  [Pipeline for modeling causal beliefs from natural language](https://doi.org/10.18653/v1/2023.acl-demo.41) |  | 0 | We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets. | John Priniski, Ishaan Verma, Fred Morstatter |  |
| 43 |  |  [TabGenie: A Toolkit for Table-to-Text Generation](https://doi.org/10.18653/v1/2023.acl-demo.42) |  | 0 | Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie – a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In TabGenie, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text... | Zdenek Kasner, Ekaterina Garanina, Ondrej Plátek, Ondrej Dusek |  |
| 44 |  |  [An Efficient Conversational Smart Compose System](https://doi.org/10.18653/v1/2023.acl-demo.43) |  | 0 | Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency. From the modeling side, the decoder-only model exploited the previous turns of conversational history in a... | Yun Zhu, Xiayu Chen, Lei Shu, Bowen Tan, Xinying Song, Lijuan Liu, Maria Wang, Jindong Chen, Ning Ruan |  |
| 45 |  |  [Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals](https://doi.org/10.18653/v1/2023.acl-demo.44) |  | 0 | We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that... | Robin Chan, Afra Amini, Mennatallah ElAssady |  |
| 46 |  |  [LaTeX2Solver: a Hierarchical Semantic Parsing of LaTeX Document into Code for an Assistive Optimization Modeling Application](https://doi.org/10.18653/v1/2023.acl-demo.45) |  | 0 | We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an in-depth understanding of the problem description and a technical expertise to produce the modeling code. Thus, our proposed system TeX2Solver helps partially automate this conversion and help the... | Rindra Ramamonjison, Timothy T. L. Yu, Linzi Xing, Mahdi Mostajabdaveh, Xiaorui Li, Xiaojin Fu, Xiongwei Han, Yuanzhe Chen, Ren Li, Kun Mao, Yong Zhang |  |
| 47 |  |  [Alfred: A System for Prompted Weak Supervision](https://doi.org/10.18653/v1/2023.acl-demo.46) |  | 0 | Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data... | Peilin Yu, Stephen H. Bach |  |
| 48 |  |  [OpenICL: An Open-Source Framework for In-context Learning](https://doi.org/10.18653/v1/2023.acl-demo.47) |  | 0 | In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and... | Zhenyu Wu, Yaoxiang Wang, Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Jingjing Xu, Yu Qiao |  |
| 49 |  |  [Self-Supervised Sentence Polishing by Adding Engaging Modifiers](https://doi.org/10.18653/v1/2023.acl-demo.48) |  | 0 | Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones.... | Zhexin Zhang, Jian Guan, Xin Cui, Yu Ran, Bo Liu, Minlie Huang |  |
| 50 |  |  [Effidit: An Assistant for Improving Writing Efficiency](https://doi.org/10.18653/v1/2023.acl-demo.49) |  | 0 | Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) technologies. We significantly expand the capacities of a writing assistantby providing functions in three modules: text completion, hint recommendation, and writing... | Shuming Shi, Enbo Zhao, Wei Bi, Deng Cai, Leyang Cui, Xinting Huang, Haiyun Jiang, Duyu Tang, Kaiqiang Song, Longyue Wang, Chenyan Huang, Guoping Huang, Yan Wang, Piji Li |  |
| 51 |  |  [WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings](https://doi.org/10.18653/v1/2023.acl-demo.50) |  | 0 | Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help... | Zijie J. Wang, Fred Hohman, Duen Horng Chau |  |
| 52 |  |  [A System for Answering Simple Questions in Multiple Languages](https://doi.org/10.18653/v1/2023.acl-demo.51) |  | 0 | Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question’s text embeddings and the answer’s graph... | Anton Razzhigaev, Mikhail Salnikov, Valentin Malykh, Pavel Braslavski, Alexander Panchenko |  |
| 53 |  |  [KWJA: A Unified Japanese Analyzer Based on Foundation Models](https://doi.org/10.18653/v1/2023.acl-demo.52) |  | 0 | We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.KWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS analysis, bridging reference resolution, coreference resolution, and discourse relation analysis, making it the most versatile among existing Japanese text analyzers.KWJA solves these tasks in a... | Nobuhiro Ueda, Kazumasa Omura, Takashi Kodama, Hirokazu Kiyomaru, Yugo Murawaki, Daisuke Kawahara, Sadao Kurohashi |  |
| 54 |  |  [Disease Network Constructor: a Pathway Extraction and Visualization](https://doi.org/10.18653/v1/2023.acl-demo.53) |  | 0 | We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end... | Mohammad Golam Sohrab, Khoa Duong, Goran Topic, Ikeda Masami, Nozomi Nagano, Yayoi NatsumeKitatani, Masakata Kuroda, Mari Nogami Itoh, Hiroya Takamura |  |
| 55 |  |  [Petals: Collaborative Inference and Fine-tuning of Large Models](https://doi.org/10.18653/v1/2023.acl-demo.54) |  | 0 | Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are... | Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Maksim Riabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, Colin Raffel |  |
| 56 |  |  [UKP-SQuARE v3: A Platform for Multi-Agent QA Research](https://doi.org/10.18653/v1/2023.acl-demo.55) |  | 0 | The continuous development of Question Answering (QA) datasets has drawn the research community’s attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert... | Haritz Puerto, Tim Baumgärtner, Rachneet Sachdeva, Haishuo Fang, Hao Zhang, Sewin Tariverdian, Kexin Wang, Iryna Gurevych |  |
| 57 |  |  [Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation](https://doi.org/10.18653/v1/2023.acl-demo.56) |  | 0 | In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical... | Mete Sertkan, Sophia Althammer, Sebastian Hofstätter |  |
| 58 |  |  [GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration](https://doi.org/10.18653/v1/2023.acl-demo.57) |  | 0 | Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We... | Aleksandra Piktus, Odunayo Ogundepo, Christopher Akiki, Akintunde Oladipo, Xinyu Zhang, Hailey Schoelkopf, Stella Biderman, Martin Potthast, Jimmy Lin |  |
| 59 |  |  [DeepPavlov Dream: Platform for Building Generative AI Assistants](https://doi.org/10.18653/v1/2023.acl-demo.58) |  | 0 | An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the... | Diliara Zharikova, Daniel Kornev, Fedor Ignatov, Maxim Talimanchuk, Dmitry Evseev, Ksenya Petukhova, Veronika Smilga, Dmitry Karpov, Yana Shishkina, Dmitry Kosenko, Mikhail Burtsev |  |
| 60 |  |  [Frontmatter](https://aclanthology.org/2023.acl-industry.0) |  | 0 |  |  |  |
| 61 |  |  [CWSeg: An Efficient and General Approach to Chinese Word Segmentation](https://doi.org/10.18653/v1/2023.acl-industry.1) |  | 0 | In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we... | Dedong Li, Rui Zhao, Fei Tan |  |
| 62 |  |  ["Knowledge is Power": Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation](https://doi.org/10.18653/v1/2023.acl-industry.2) |  | 0 | In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists’ dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our... | Kaveri Kale, Pushpak Bhattacharyya, Aditya Shetty, Milind Gune, Kush Shrivastava, Rustom Lawyer, Spriha Biswas |  |
| 63 |  |  [Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications](https://doi.org/10.18653/v1/2023.acl-industry.3) |  | 0 | Patent applicants write patent specificationsthat describe embodiments of inventions. Some embodiments are claimed for a patent,while others may be unclaimeddue to strategic considerations. Unclaimed embodiments may be extracted byapplicants later and claimed incontinuing applications togain advantages over competitors. Despite being essential for corporate intellectual property (IP) strategies,unclaimed embodiment extraction is conducted manually,and little research has been conducted on its... | Chikara Hashimoto, Gautam Kumar, Shuichiro Hashimoto, Jun Suzuki |  |
| 64 |  |  [MathPrompter: Mathematical Reasoning using Large Language Models](https://doi.org/10.18653/v1/2023.acl-industry.4) |  | 0 | Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address... | Shima Imani, Liang Du, Harsh Shrivastava |  |
| 65 |  |  [Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems](https://doi.org/10.18653/v1/2023.acl-industry.5) |  | 0 | Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined... | Mohammad Kachuee, Sungjin Lee |  |
| 66 |  |  [pNLP-Mixer: an Efficient all-MLP Architecture for Language](https://doi.org/10.18653/v1/2023.acl-industry.6) |  | 0 | Large pre-trained language models based on transformer architectureƒhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot... | Francesco Fusco, Damian Pascual, Peter W. J. Staar, Diego Antognini |  |
| 67 |  |  [Extracting Text Representations for Terms and Phrases in Technical Domains](https://doi.org/10.18653/v1/2023.acl-industry.7) |  | 0 | Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In... | Francesco Fusco, Diego Antognini |  |
| 68 |  |  [CocaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval](https://doi.org/10.18653/v1/2023.acl-industry.8) |  | 0 | Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial indexing and inference time and the large consumption of computational resources. Although knowledge distillation techniques have been widely utilized for uni-modal model compression, how to expand... | Jiapeng Wang, Chengyu Wang, Xiaodan Wang, Jun Huang, Lianwen Jin |  |
| 69 |  |  [KG-FLIP: Knowledge-guided Fashion-domain Language-Image Pre-training for E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.9) |  | 0 | Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without leveraging external knowledge. Adapting these models to downstream applications in specific domains like fashion requires fine-grained in-domain image-text corpus, which are usually less semantically... | Qinjin Jia, Yang Liu, Daoping Wu, Shaoyuan Xu, Huidong Liu, Jinmiao Fu, Roland Vollgraf, Bryan Wang |  |
| 70 |  |  [Domain-specific transformer models for query translation](https://doi.org/10.18653/v1/2023.acl-industry.10) |  | 0 | Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products. Due to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to... | Mandar Kulkarni, Nikesh Garera, Anusua Trivedi |  |
| 71 |  |  [Label efficient semi-supervised conversational intent classification](https://doi.org/10.18653/v1/2023.acl-industry.11) |  | 0 | To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e.,... | Mandar Kulkarni, Kyung Kim, Nikesh Garera, Anusua Trivedi |  |
| 72 |  |  [xPQA: Cross-Lingual Product Question Answering in 12 Languages](https://doi.org/10.18653/v1/2023.acl-industry.12) |  | 0 | Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers’ questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate... | Xiaoyu Shen, Akari Asai, Bill Byrne, Adrià de Gispert |  |
| 73 |  |  [Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection](https://doi.org/10.18653/v1/2023.acl-industry.13) |  | 0 | Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on past data and testing on future data. In this paper, we observe that the appearances of news events on the same topic may display discernible patterns over time, and posit that such patterns can... | Beizhe Hu, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Zhengjia Wang, Zhiwei Jin |  |
| 74 |  |  [AVEN-GR: Attribute Value Extraction and Normalization using product GRaphs](https://doi.org/10.18653/v1/2023.acl-industry.14) |  | 0 | Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient and able to capture early emerging market trends. Query Attribute Understanding (QAU) is a sub-component of QU that involves extracting named attributes from user queries and linking them to existing... | Thomas Ricatte, Donato Crisostomi |  |
| 75 |  |  [GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model](https://doi.org/10.18653/v1/2023.acl-industry.15) |  | 0 | Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these... | Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu Zhao, Peng Zhang, Jie Tang |  |
| 76 |  |  [FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph](https://doi.org/10.18653/v1/2023.acl-industry.16) |  | 0 | Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained... | Xiaodan Wang, Chengyu Wang, Lei Li, Zhixu Li, Ben Chen, Linbo Jin, Jun Huang, Yanghua Xiao, Ming Gao |  |
| 77 |  |  [Entity Contrastive Learning in a Large-Scale Virtual Assistant System](https://doi.org/10.18653/v1/2023.acl-industry.17) |  | 0 | Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation... | Jonathan Rubin, Jason Crowley, George Leung, Morteza Ziyadi, Maria Minakova |  |
| 78 |  |  [Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog](https://doi.org/10.18653/v1/2023.acl-industry.18) |  | 0 | Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present... | Kewei Cheng, Xian Li, Zhengyang Wang, Chenwei Zhang, Binxuan Huang, Yifan Ethan Xu, Xin Luna Dong, Yizhou Sun |  |
| 79 |  |  [Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs](https://doi.org/10.18653/v1/2023.acl-industry.19) |  | 0 | Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs... | Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel, Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, Aram Galstyan |  |
| 80 |  |  [Tab-CQA: A Tabular Conversational Question Answering Dataset on Financial Reports](https://doi.org/10.18653/v1/2023.acl-industry.20) |  | 0 | Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally, we select 4,578 tables, from which 4,578 conversations... | Chuang Liu, Junzhuo Li, Deyi Xiong |  |
| 81 |  |  [KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications](https://doi.org/10.18653/v1/2023.acl-industry.21) |  | 0 | Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the... | Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Gunhee Kim, JungWoo Ha |  |
| 82 |  |  [Improving Knowledge Production Efficiency With Question Answering on Conversation](https://doi.org/10.18653/v1/2023.acl-industry.22) |  | 0 | Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in... | Changlin Yang, Siye Liu, Sen Hu, Wangshu Zhang, Teng Xu, Jing Zheng |  |
| 83 |  |  [Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses](https://doi.org/10.18653/v1/2023.acl-industry.23) |  | 0 | Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times. This paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets. Our approach eliminates duplicates at the batch level, without altering the data distribution observed by the model, making it model-agnostic and easy to implement as a plug-and-play module. We also provide a... | Donato Crisostomi, Andrea Caciolai, Alessandro Pedrani, Kay Rottmann, Alessandro Manzotti, Enrico Palumbo, Davide Bernardi |  |
| 84 |  |  [Distilled Language Models are economically efficient for the enterprise. ...mostly](https://doi.org/10.18653/v1/2023.acl-industry.24) |  | 0 | Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and... | Kristen Howell, Gwen Christian, Pavel Fomitchov, Gitit Kehat, Julianne Marzulla, Leanne Rolston, Jadin Tredup, Ilana Zimmerman, Ethan Selfridge, Joseph Bradley |  |
| 85 |  |  [Application-Agnostic Language Modeling for On-Device ASR](https://doi.org/10.18653/v1/2023.acl-industry.25) |  | 0 | On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an... | Markus NußbaumThom, Lyan Verwimp, Youssef Oualil |  |
| 86 |  |  [Building Accurate Low Latency ASR for Streaming Voice Search in E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.26) |  | 0 | Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish... | Abhinav Goyal, Nikesh Garera |  |
| 87 |  |  [PLAtE: A Large-scale Dataset for List Page Web Extraction](https://doi.org/10.18653/v1/2023.acl-industry.27) |  | 0 | Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items... | Aidan San, Yuan Zhuang, Jan Bakus, Colin Lockard, David M. Ciemiewicz, Sandeep Atluri, Kevin Small, Yangfeng Ji, Heba Elfardy |  |
| 88 |  |  [Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed](https://doi.org/10.18653/v1/2023.acl-industry.28) |  | 0 | Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the open-source community cannot support industrial domain-specific applications due to the lack of entity knowledge and low inference speed. In this paper, we propose Rapid Diffusion, a novel framework... | Bingyan Liu, Weifeng Lin, Zhongjie Duan, Chengyu Wang, Ziheng Wu, Zhang Zipeng, Kui Jia, Lianwen Jin, Cen Chen, Jun Huang |  |
| 89 |  |  [Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes](https://doi.org/10.18653/v1/2023.acl-industry.29) |  | 0 | E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don’t label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we... | Anant Khandelwal, Happy Mittal, Shreyas Sunil Kulkarni, Deepak Gupta |  |
| 90 |  |  [Consistent Text Categorization using Data Augmentation in e-Commerce](https://doi.org/10.18653/v1/2023.acl-industry.30) |  | 0 | The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we... | Noa Avigdor, Guy Horowitz, Ariel Raviv, Stav Yanovsky Daye |  |
| 91 |  |  [An efficient method for Natural Language Querying on Structured Data](https://doi.org/10.18653/v1/2023.acl-industry.31) |  | 0 | We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following “bread and butter” NLP tasks: (a) Domain classification, for choosing which DB table to query, whether the question is out-of-scope (b) Multi-head slot/entity extraction (SE) to extract the field criteria and other attributes such as its role (filter, sort etc) from... | Hanoz Bhathena, Aviral Joshi, Prateek Singh |  |
| 92 |  |  [Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy](https://doi.org/10.18653/v1/2023.acl-industry.32) |  | 0 | Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients’ clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine... | Zekai Chen, Mariann Micsinai Balan, Kevin Brown |  |
| 93 |  |  [EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only](https://doi.org/10.18653/v1/2023.acl-industry.33) |  | 0 | This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method’s online learning capability enables it to adapt to changes in the domain or MT engines dynamically,... | Kamer Ali Yüksel, Ahmet Gunduz, Mohamed AlBadrashiny, Hassan Sawaf |  |
| 94 |  |  [A Static Evaluation of Code Completion by Large Language Models](https://doi.org/10.18653/v1/2023.acl-industry.34) |  | 0 | Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program,... | Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang |  |
| 95 |  |  [Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems](https://doi.org/10.18653/v1/2023.acl-industry.35) |  | 0 | Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has... | Sarthak Ahuja, Mohammad Kachuee, Fatemeh Sheikholeslami, Weiqing Liu, Jaeyoung Do |  |
| 96 |  |  [MobileNMT: Enabling Translation in 15MB and 30ms](https://doi.org/10.18653/v1/2023.acl-industry.36) |  | 0 | Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We... | Ye Lin, Xiaohui Wang, Zhexi Zhang, Mingxuan Wang, Tong Xiao, Jingbo Zhu |  |
| 97 |  |  [Multi-doc Hybrid Summarization via Salient Representation Learning](https://doi.org/10.18653/v1/2023.acl-industry.37) |  | 0 | Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint... | Min Xiao |  |
| 98 |  |  [SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels](https://doi.org/10.18653/v1/2023.acl-industry.38) |  | 0 | Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we... | Zhenting Qi, Xiaoyu Tan, Chao Qu, Yinghui Xu, Yuan Qi |  |
| 99 |  |  [Chemical Language Understanding Benchmark](https://doi.org/10.18653/v1/2023.acl-industry.39) |  | 0 | In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from... | Yunsoo Kim, Hyuk Ko, Jane Lee, Hyun Young Heo, Jinyoung Yang, Sungsoo Lee, KyuHwang Lee |  |
| 100 |  |  [HyperT5: Towards Compute-Efficient Korean Language Modeling](https://doi.org/10.18653/v1/2023.acl-industry.40) |  | 0 | Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and... | Dongju Park, Soonwon Ka, Kang Min Yoo, Gichang Lee, Jaewook Kang |  |
| 101 |  |  [Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings](https://doi.org/10.18653/v1/2023.acl-industry.41) |  | 0 | Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design considering ambiguity is possible. We utilize similarity in a semantic space to detect ambiguity in service scenarios and training data. In addition, we apply task-specific embedding to improve... | Jong Myoung Kim, YoungJun Lee, Sangkeun Jung, HoJin Choi |  |
| 102 |  |  [Reliable and Interpretable Drift Detection in Streams of Short Texts](https://doi.org/10.18653/v1/2023.acl-industry.42) |  | 0 | Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards effective re-training of the model. In this study we propose an end-to-end framework for reliable model-agnostic change-point detection and interpretation in large task-oriented dialog systems, proven... | Ella Rabinovich, Matan Vetzler, Samuel Ackerman, Ateret AnabyTavor |  |
| 103 |  |  [Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-industry.43) |  | 0 | Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks. Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).We evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT... | Jonathan J. Hüser, Judith Gaspers, Thomas Gueudré, Chandana Satya Prakash, Jin Cao, Daniil Sorokin, Quynh Do, Nicolas Anastassacos, Tobias Falke, Turan Gojayev |  |
| 104 |  |  [Annotating Research Infrastructure in Scientific Papers: An NLP-driven Approach](https://doi.org/10.18653/v1/2023.acl-industry.44) |  | 0 | In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such as evaluating the impact of RI investment, and supporting Open Science and research reproducibility. These links can also be used to establish a profile of the RI portfolio of each institution and... | Seyed Amin Tabatabaei, Georgios Cheirmpos, Marius A. Doornenbal, Alberto Zigoni, Véronique Moore, Georgios Tsatsaronis |  |
| 105 |  |  [Event-Centric Query Expansion in Web Search](https://doi.org/10.18653/v1/2023.acl-industry.45) |  | 0 | In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the QE system used in a famous Chinese search engine. EQE utilizes a novel event retrieval framework that consists of four stages, i.e., event collection, event reformulation, semantic retrieval and... | Yanan Zhang, Weijie Cui, Yangfan Zhang, Xiaoling Bai, Zhe Zhang, Jin Ma, Xiang Chen, Tianhua Zhou |  |
| 106 |  |  [Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization](https://doi.org/10.18653/v1/2023.acl-industry.46) |  | 0 | As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. In order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. To further enhance... | Shansan Gong, Zelin Zhou, Shuo Wang, Fengjiao Chen, Xiujie Song, Xuezhi Cao, Yunsen Xian, Kenny Q. Zhu |  |
| 107 |  |  [DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction](https://doi.org/10.18653/v1/2023.acl-industry.47) |  | 0 | Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge Base (KB) databases are an effective way of storing and accessing such information to scale. In this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris... | Paul Darm, Antonio Valerio Miceli Barone, Shay B. Cohen, Annalisa Riccardi |  |
| 108 |  |  [BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting](https://doi.org/10.18653/v1/2023.acl-industry.48) |  | 0 | Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs’ early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of... | Wei Zhu, Peng Wang, Yuan Ni, Guotong Xie, Xiaoling Wang |  |
| 109 |  |  [K-pop and fake facts: from texts to smart alerting for maritime security](https://doi.org/10.18653/v1/2023.acl-industry.49) |  | 0 | Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard and soft data (sensors and texts), either by tweaking their AIS emitters or by emitting false information on pseudo-newspapers. Many techniques exist to identify these pieces of false information,... | Maxime Prieur, Souhir Gahbiche, Guillaume Gadek, Sylvain Gatepaille, Kilian Vasnier, Valerian Justine |  |
| 110 |  |  [Evaluating Embedding APIs for Information Retrieval](https://doi.org/10.18653/v1/2023.acl-industry.50) |  | 0 | The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist... | Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David AlfonsoHermelo, Mehdi Rezagholizadeh, Jimmy Lin |  |
| 111 |  |  [Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform](https://doi.org/10.18653/v1/2023.acl-industry.51) |  | 0 | Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason,... | Mateusz Wójcik, Witold Kosciukiewicz, Mateusz Baran, Tomasz Kajdanowicz, Adam Gonczarek |  |
| 112 |  |  [Regression-Free Model Updates for Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-industry.52) |  | 0 | In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, focal distillation, to model updates in a goal-oriented dialog system and assess its usefulness in practice. In particular, we evaluate its effectiveness for key language understanding tasks,... | Andrea Caciolai, Verena Weber, Tobias Falke, Alessandro Pedrani, Davide Bernardi |  |
| 113 |  |  [Reducing cohort bias in natural language understanding systems with targeted self-training scheme](https://doi.org/10.18653/v1/2023.acl-industry.53) |  | 0 | Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is observed that natural language understanding models often have lower performance when dealing with requests coming from new users rather than experienced users. To mitigate this problem, we propose a... | DieuThu Le, Gabriela Hernández, Bei Chen, Melanie Bradford |  |
| 114 |  |  [Content Moderation for Evolving Policies using Binary Question Answering](https://doi.org/10.18653/v1/2023.acl-industry.54) |  | 0 | Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging. We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes... | Sankha Subhra Mullick, Mohan Bhambhani, Suhit Sinha, Akshat Mathur, Somya Gupta, Jidnya Shah |  |
| 115 |  |  [Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification](https://doi.org/10.18653/v1/2023.acl-industry.55) |  | 0 | Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task’s long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and... | Tianqi Wang, Lei Chen, Xiaodan Zhu, Younghun Lee, Jing Gao |  |
| 116 |  |  [Towards Building a Robust Toxicity Predictor](https://doi.org/10.18653/v1/2023.acl-industry.56) |  | 0 | Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal... | Dmitriy Bespalov, Sourav Bhabesh, Yi Xiang, Liutong Zhou, Yanjun Qi |  |
| 117 |  |  [AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching](https://doi.org/10.18653/v1/2023.acl-industry.57) |  | 0 | In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based... | Md. Tahmid Rahman Laskar, Cheng Chen, XueYong Fu, Mahsa Azizi, Shashi Bhushan TN, Simon CorstonOliver |  |
| 118 |  |  [Unified Contextual Query Rewriting](https://doi.org/10.18653/v1/2023.acl-industry.58) |  | 0 | Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To... | Yingxue Zhou, Jie Hao, Mukund Rungta, Yang Liu, Eunah Cho, Xing Fan, Yanbin Lu, Vishal Thanvantri Vasudevan, Kellen Gillespie, Zeynab Raeesy |  |
| 119 |  |  [Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites](https://doi.org/10.18653/v1/2023.acl-industry.59) |  | 0 | E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users’ true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore... | Simiao Zuo, Qingyu Yin, Haoming Jiang, Shaohui Xi, Bing Yin, Chao Zhang, Tuo Zhao |  |
| 120 |  |  [Federated Learning of Gboard Language Models with Differential Privacy](https://doi.org/10.18653/v1/2023.acl-industry.60) |  | 0 | We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based... | Zheng Xu, Yanxiang Zhang, Galen Andrew, Christopher A. ChoquetteChoo, Peter Kairouz, H. Brendan McMahan, Jesse Rosenstock, Yuanbo Zhang |  |
| 121 |  |  [RadLing: Towards Efficient Radiology Report Understanding](https://doi.org/10.18653/v1/2023.acl-industry.61) |  | 0 | Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for... | Rikhiya Ghosh, Oladimeji Farri, Sanjeev Kumar Karn, Manuela Daniela Danu, Ramya Vunikili, Larisa Micu |  |
| 122 |  |  [Predicting Customer Satisfaction with Soft Labels for Ordinal Classification](https://doi.org/10.18653/v1/2023.acl-industry.62) |  | 0 | In a typical call center, only up to 8% of callersleave a Customer Satisfaction (CSAT) surveyresponse at the end of the call, and these tend tobe customers with strongly positive or negativeexperiences. To manage this data sparsity andresponse bias, we outline a predictive CSATdeep learning algorithm that infers CSAT onthe 1-5 scale on inbound calls to the call centerwith minimal latency. The key metric to maximize is the precision for CSAT = 1 (lowestCSAT). We maximize this metric in two ways.... | Etienne Manderscheid, Matthias Lee |  |
| 123 |  |  [Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users](https://doi.org/10.18653/v1/2023.acl-industry.63) |  | 0 | Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models. Unfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback data collected from deployed QA systems. This data can be collected from tens of millions of user with no additional cost, for real-world QA services, e.g., Alexa, Google Home, and etc. The main... | Liang Wang, Ivano Lauriola, Alessandro Moschitti |  |
| 124 |  |  [SPM: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2023.acl-industry.64) |  | 0 | In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label classification, which cannot generalize to utterances with more intents than training samples. Meanwhile, it lacks the ability to assign slots to each corresponding intent. To overcome these... | Sheng Jiang, Su Zhu, Ruisheng Cao, Qingliang Miao, Kai Yu |  |
| 125 |  |  [NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks](https://doi.org/10.18653/v1/2023.acl-industry.65) |  | 0 | Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to... | Xinpeng Zhang, Ming Tan, Jingfan Zhang, Wei Zhu |  |
| 126 |  |  [Search Query Spell Correction with Weak Supervision in E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.66) |  | 0 | Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling patterns for a single correct query. In this paper, we first systematically analyze and group different spell errors into error classes and then leverage the state-of-the-art Transformer model for... | Vishal Kakkar, Chinmay Sharma, Madhura Pande, Surender Kumar |  |
| 127 |  |  ["Let's not Quote out of Context": Unified Vision-Language Pretraining for Context Assisted Image Captioning](https://doi.org/10.18653/v1/2023.acl-industry.67) |  | 0 | Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach... | Abisek Rajakumar Kalarani, Pushpak Bhattacharyya, Niyati Chhaya, Sumit Shekhar |  |
| 128 |  |  [What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue](https://doi.org/10.18653/v1/2023.acl-industry.68) |  | 0 | This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized,... | Deuk Sin Kwon, Sunwoo Lee, Ki Hyun Kim, Seojin Lee, Taeyoon Kim, Eric Davis |  |
| 129 |  |  [CUPID: Curriculum Learning Based Real-Time Prediction using Distillation](https://doi.org/10.18653/v1/2023.acl-industry.69) |  | 0 | Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction... | Arindam Bhattacharya, Ankith M. S, Ankit Gandhi, Vijay Huddar, Atul Saroop, Rahul Bhagat |  |
| 130 |  |  [Answering Unanswered Questions through Semantic Reformulations in Spoken QA](https://doi.org/10.18653/v1/2023.acl-industry.70) |  | 0 | Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a... | Pedro Faustini, Zhiyu Chen, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi |  |
| 131 |  |  [Exploring Zero and Few-shot Techniques for Intent Classification](https://doi.org/10.18653/v1/2023.acl-industry.71) |  | 0 | Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4)... | Soham Parikh, Mitul Tiwari, Prashil Tumbade, Quaizar Vohra |  |
| 132 |  |  [Referring to Screen Texts with Voice Assistants](https://doi.org/10.18653/v1/2023.acl-industry.72) |  | 0 | Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their users’ context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are... | Shruti Bhargava, Anand Dhoot, IngMarie Jonsson, Hoang Long Nguyen, Alkesh Patel, Hong Yu, Vincent Renkens |  |
| 133 |  |  [Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search](https://doi.org/10.18653/v1/2023.acl-industry.73) |  | 0 | Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user’s shopping experience poses challenges on deciding when and how to show FAQ results. Our proposed intent-aware FAQ retrieval consists of (1) an intent... | Zhiyu Chen, Jason Ingyu Choi, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi |  |
| 134 |  |  [KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-industry.74) |  | 0 | Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained... | Zhiwei Jia, Pradyumna Narayana, Arjun R. Akula, Garima Pruthi, Hao Su, Sugato Basu, Varun Jampani |  |
| 135 |  |  [Weakly supervised hierarchical multi-task classification of customer questions](https://doi.org/10.18653/v1/2023.acl-industry.75) |  | 0 | Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification... | Jitenkumar Rana, Promod Yenigalla, Chetan Aggarwal, Sandeep Sricharan Mukku, Manan Soni, Rashmi Patange |  |
| 136 |  |  [Automated Digitization of Unstructured Medical Prescriptions](https://doi.org/10.18653/v1/2023.acl-industry.76) |  | 0 | Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and... | Megha Sharma, Tushar Vatsal, Srujana Merugu, Aruna Rajan |  |
| 137 |  |  [Frontmatter](https://aclanthology.org/2023.acl-tutorials.0) |  | 0 |  |  |  |
| 138 |  |  [Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond](https://doi.org/10.18653/v1/2023.acl-tutorials.1) |  | 0 | Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target... | Yang Deng, Wenqiang Lei, Minlie Huang, TatSeng Chua |  |
| 139 |  |  [Complex Reasoning in Natural Languag](https://doi.org/10.18653/v1/2023.acl-tutorials.2) |  | 0 | Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a... | Wenting Zhao, Mor Geva, Bill Yuchen Lin, Michihiro Yasunaga, Aman Madaan, Tao Yu |  |
| 140 |  |  [Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world](https://doi.org/10.18653/v1/2023.acl-tutorials.3) |  | 0 | This tutorial will describe various aspects of scaling up language technologies to many of the world’s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also... | Sunayana Sitaram, Monojit Choudhury, Barun Patra, Vishrav Chaudhary, Kabir Ahuja, Kalika Bali |  |
| 141 |  |  [Generating Text from Language Models](https://doi.org/10.18653/v1/2023.acl-tutorials.4) |  | 0 | An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms—like top-p... | Afra Amini, Ryan Cotterell, John Hewitt, Clara Meister, Tiago Pimentel |  |
| 142 |  |  [Indirectly Supervised Natural Language Processing](https://doi.org/10.18653/v1/2023.acl-tutorials.5) |  | 0 | This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a “related” task T′? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from... | Wenpeng Yin, Muhao Chen, Ben Zhou, Qiang Ning, KaiWei Chang, Dan Roth |  |
| 143 |  |  [Retrieval-based Language Models and Applications](https://doi.org/10.18653/v1/2023.acl-tutorials.6) |  | 0 | Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning... | Akari Asai, Sewon Min, Zexuan Zhong, Danqi Chen |  |
| 144 |  |  [ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer](https://doi.org/10.18653/v1/2023.acl-srw.1) |  | 0 | Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT’s performance in two controllable generation tasks, with respect to ChatGPT’s ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate... | Dongqi Liu, Vera Demberg |  |
| 145 |  |  [Multi-Dialectal Representation Learning of Sinitic Phonology](https://doi.org/10.18653/v1/2023.acl-srw.2) |  | 0 | Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data... | Zhibai Jia |  |
| 146 |  |  [Prompt-based Zero-shot Text Classification with Conceptual Knowledge](https://doi.org/10.18653/v1/2023.acl-srw.4) |  | 0 | In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the... | Yuqi Wang, Wei Wang, Qi Chen, Kaizhu Huang, Anh Nguyen, Suparna De |  |
| 147 |  |  [How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese](https://doi.org/10.18653/v1/2023.acl-srw.5) |  | 0 | This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers,... | Takuro Fujii, Koki Shibata, Atsuki Yamaguchi, Terufumi Morishita, Yasuhiro Sogawa |  |
| 148 |  |  [Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering](https://doi.org/10.18653/v1/2023.acl-srw.7) |  | 0 | Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a... | Chenyang Lyu, Tianbo Ji, Yvette Graham, Jennifer Foster |  |
| 149 |  |  [Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models](https://doi.org/10.18653/v1/2023.acl-srw.8) |  | 0 | Natural Language Inference (NLI) tasks involving temporal inference remain challenging for pre-trained language models (LMs). Although various datasets have been created for this task, they primarily focus on English and do not address the need for resources in other languages. It is unclear whether current LMs realize the generalization capacity for temporal inference across languages. In this paper, we present Jamp, a Japanese NLI benchmark focused on temporal inference. Our dataset includes... | Tomoki Sugimoto, Yasumasa Onoe, Hitomi Yanaka |  |
| 150 |  |  [Constructing Multilingual Code Search Dataset Using Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-srw.10) |  | 0 | Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate... | Ryo Sekizawa, Nan Duan, Shuai Lu, Hitomi Yanaka |  |
| 151 |  |  [Multimodal Neural Machine Translation Using Synthetic Images Transformed by Latent Diffusion Model](https://doi.org/10.18653/v1/2023.acl-srw.12) |  | 0 | This study proposes a new multimodal neural machine translation (MNMT) model using synthetic images transformed by a latent diffusion model. MNMT translates a source language sentence based on its related image, but the image usually contains noisy information that are not relevant to the source language sentence. Our proposed method first generates a synthetic image corresponding to the content of the source language sentence by using a latent diffusion model and then performs translation... | Ryoya Yuasa, Akihiro Tamura, Tomoyuki Kajiwara, Takashi Ninomiya, Tsuneo Kato |  |
| 152 |  |  [Enhancing Ancient Chinese Understanding with Derived Noisy Syntax Trees](https://doi.org/10.18653/v1/2023.acl-srw.15) |  | 0 | Despite the rapid development of neural-based models, syntax still plays a crucial role in modern natural language processing. However, few studies have incorporated syntactic information into ancient Chinese understanding tasks due to the lack of syntactic annotation. This paper explores the role of syntax in ancient Chinese understanding based on the noisy syntax trees from unsupervised derivation and modern Chinese syntax parsers. On top of that, we propose a novel syntax encoding component... | Ping Wang, Shitou Zhang, Zuchao Li, Jingrui Hou |  |
| 153 |  |  [The Turing Quest: Can Transformers Make Good NPCs?](https://doi.org/10.18653/v1/2023.acl-srw.17) |  | 0 | In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of... | Qi Chen Gao, Ali Emami |  |
| 154 |  |  [Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section](https://doi.org/10.18653/v1/2023.acl-srw.18) |  | 0 | Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs... | Hongyi Zheng, Yixin Zhu, Lavender Y. Jiang, Kyunghyun Cho, Eric K. Oermann |  |
| 155 |  |  [Intriguing Effect of the Correlation Prior on ICD-9 Code Assignment](https://doi.org/10.18653/v1/2023.acl-srw.19) |  | 0 | The Ninth Revision of the International Classification of Diseases (ICD-9) is a standardized coding system used to classify health conditions. It is used for billing, tracking individual patient conditions, and for epidemiology. The highly detailed and technical nature of the codes and their associated medical conditions make it difficult for humans to accurately record them. Researchers have explored the use of neural networks, particularly language models, for automated ICD-9 code assignment.... | Zihao Yang, Chenkang Zhang, Muru Wu, Xujin Liu, Lavender Y. Jiang, Kyunghyun Cho, Eric K. Oermann |  |
| 156 |  |  [Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks](https://doi.org/10.18653/v1/2023.acl-srw.20) |  | 0 | State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our... | Mateusz Baran, Joanna Baran, Mateusz Wójcik, Maciej Zieba, Adam Gonczarek |  |
| 157 |  |  [Can LMs Store and Retrieve 1-to-N Relational Knowledge?](https://doi.org/10.18653/v1/2023.acl-srw.22) |  | 0 | It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational knowledge, such as ”country and its capital,” with high memorization accuracy. On the other hand, world knowledge includes not only 1-to-1 but also 1-to-N relational knowledge, such as ”parent and... | Haruki Nagasawa, Benjamin Heinzerling, Kazuma Kokuta, Kentaro Inui |  |
| 158 |  |  [Theoretical Linguistics Rivals Embeddings in Language Clustering for Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-srw.24) |  | 0 | While embedding-based methods have been dominant in language clustering for multilingual tasks, clustering based on linguistic features has not yet been explored much, as it remains baselines (Tan et al., 2019; Shaffer, 2021). This study investigates whether and how theoretical linguistics improves language clustering for multilingual named entity recognition (NER). We propose two types of language groupings: one based on morpho-syntactic features in a nominal domain and one based on a head... | Sakura Imai, Daisuke Kawahara, Naho Orita, Hiromune Oda |  |
| 159 |  |  [Native Language Prediction from Gaze: a Reproducibility Study](https://doi.org/10.18653/v1/2023.acl-srw.26) |  | 0 | Numerous studies found that the linguistic properties of a person’s native language affect the cognitive processing of other languages. However, only one study has shown that it was possible to identify the native language based on eye-tracking records of natural L2 reading using machine learning. A new corpus allows us to replicate these results on a more interrelated and larger set of native languages. Our results show that comparable classification performance is maintained despite using... | Lina Skerath, Paulina Toborek, Anita Zielinska, Maria Barrett, Rob van der Goot |  |
| 160 |  |  [MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries](https://doi.org/10.18653/v1/2023.acl-srw.27) |  | 0 | Discharge summaries are comprehensive medical records that encompass vital information about a patient’s hospital stay. A crucial aspect of discharge summaries is the temporal information of treatments administered throughout the patient’s illness. With an extensive volume of clinical documents, manually extracting and compiling a patient’s medication list can be laborious, time-consuming, and susceptible to errors. The objective of this paper is to build upon the recent development on clinical... | Yang Cui, Lifeng Han, Goran Nenadic |  |
| 161 |  |  [Sudden Semantic Shifts in Swedish NATO discourse](https://doi.org/10.18653/v1/2023.acl-srw.28) |  | 0 | In this paper, we investigate a type of semantic shift that occurs when a sudden event radically changes public opinion on a topic. Looking at Sweden’s decision to apply for NATO membership in 2022, we use word embeddings to study how the associations users on Twitter have regarding NATO evolve. We identify several changes that we successfully validate against real-world events. However, the low engagement of the public with the issue often made it challenging to distinguish true signals from... | Brian Bonafilia, Bastiaan Bruinsma, Denitsa Saynova, Moa Johansson |  |
| 162 |  |  [Building a Buzzer-quiz Answering System](https://doi.org/10.18653/v1/2023.acl-srw.29) |  | 0 | A buzzer quiz is a genre of quiz in which multiple players simultaneously listen to a quiz being read aloud and respond it by buzzing in as soon as they can predict the answer. Because incorrect answers often result in penalties, a buzzer-quiz answering system must not only predict the answer from only part of a question but also estimate the predicted answer’s accuracy. In this paper, we introduce two types of buzzer-quiz answering systems: (1) a system that directly generates an answer from... | Naoya Sugiura, Kosuke Yamada, Ryohei Sasano, Koichi Takeda, Katsuhiko Toyama |  |
| 163 |  |  [Probing for Hyperbole in Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-srw.30) |  | 0 | Hyperbole is a common figure of speech, which is under-explored in NLP research. In this study, we conduct edge and minimal description length (MDL) probing experiments on three pre-trained language models (PLMs) in an attempt to explore the extent to which hyperbolic information is encoded in these models. We use both word-in-context and sentence-level representations as model inputs as a basis for comparison. We also annotate 63 hyperbole sentences from the HYPO dataset according to an... | Nina Schneidermann, Daniel Hershcovich, Bolette S. Pedersen |  |
| 164 |  |  [Towards Efficient Dialogue Processing in the Emergency Response Domain](https://doi.org/10.18653/v1/2023.acl-srw.31) |  | 0 | In this paper we describe the task of adapting NLP models to dialogue processing in the emergency response domain. Our goal is to provide a recipe for building a system that performs dialogue act classification and domain-specific slot tagging while being efficient, flexible and robust. We show that adapter models Pfeiffer et al. (2020) perform well in the emergency response domain and benefit from additional dialogue context and speaker information. Comparing adapters to standard fine-tuned... | Tatiana Anikina |  |
| 165 |  |  [I already said that! Degenerating redundant questions in open-domain dialogue systems](https://doi.org/10.18653/v1/2023.acl-srw.33) |  | 0 | Neural text generation models have achieved remarkable success in carrying on short open-domain conversations. However, their performance degrades significantly in the long term, especially in their ability to ask coherent questions. A significant issue is the generation of redundant questions where the answer has already been provided by the user. We adapt and evaluate different methods, including negative training, decoding, and classification, to mitigate the redundancy problem. We also... | Long Mai, Julie CarsonBerndsen |  |
| 166 |  |  [Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation](https://doi.org/10.18653/v1/2023.acl-srw.34) |  | 0 | Currently, most knowledge-grounded dialogue response generation models focus on reflecting given external knowledge. However, even when conveying external knowledge, humans integrate their own knowledge, experiences, and opinions with external knowledge to make their utterances engaging. In this study, we analyze such human behavior by annotating the utterances in an existing knowledge-grounded dialogue corpus. Each entity in the corpus is annotated with its information source, either derived... | Takashi Kodama, Hirokazu Kiyomaru, Yin Jou Huang, Taro Okahisa, Sadao Kurohashi |  |
| 167 |  |  [Choosing What to Mask: More Informed Masking for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.acl-srw.35) |  | 0 | Pre-trained language models have achieved remarkable results on several NLP tasks. Most of them adopt masked language modeling to learn representations by randomly masking tokens and predicting them based on their context. However, this random selection of tokens to be masked is inefficient to learn some language patterns as it may not consider linguistic information that can be helpful for many NLP tasks, such as multimodal machine translation (MMT). Hence, we propose three novel masking... | Júlia Sato, Helena de Medeiros Caseli, Lucia Specia |  |
| 168 |  |  [Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference](https://doi.org/10.18653/v1/2023.acl-srw.36) |  | 0 | Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal–Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best existing methods encode texts and images either with independent encoders, or with object-level multimodal encoders using blackbox transformers. This stands in contrast to early, linguistically inspired... | Chong Shen, Carina Silberer |  |
| 169 |  |  [Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values](https://doi.org/10.18653/v1/2023.acl-srw.37) |  | 0 | Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the... | Stephanie Schoch, Ritwick Mishra, Yangfeng Ji |  |
| 170 |  |  [Distractor Generation for Fill-in-the-Blank Exercises by Question Type](https://doi.org/10.18653/v1/2023.acl-srw.38) |  | 0 | This study addresses the automatic generation of distractors for English fill-in-the-blank exercises in the entrance examinations for Japanese universities. While previous studies applied the same method to all questions, actual entrance examinations have multiple question types that reflect the purpose of the questions. Therefore, we define three types of questions (grammar, function word, and context) and propose a method to generate distractors according to the characteristics of each... | Nana Yoshimi, Tomoyuki Kajiwara, Satoru Uchida, Yuki Arase, Takashi Ninomiya |  |
| 171 |  |  [Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://doi.org/10.18653/v1/2023.acl-srw.40) |  | 0 | Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows... | Gabriel Simmons |  |
| 172 |  |  [LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism](https://doi.org/10.18653/v1/2023.acl-srw.43) |  | 0 | Recently, dynamic early exiting has attracted much attention since it can accelerate the inference speed of pre-trained models (PTMs). However, previous work on early exiting has neglected the intermediate exits’ architectural designs. In this work, we propose a novel framework, Learned Exits and COmparison-based early exiting (LECO) to improve PTMs’ early exiting performances. First, to fully uncover the potentials of multi-exit BERT, we design a novel search space for intermediate exits and... | Jingfan Zhang, Ming Tan, Pengyu Dai, Wei Zhu |  |
| 173 |  |  [Authorship Attribution of Late 19th Century Novels using GAN-BERT](https://doi.org/10.18653/v1/2023.acl-srw.44) |  | 0 | Authorship attribution aims to identify the author of an anonymous text. The task becomes even more worthwhile when it comes to literary works. For example, pen names were commonly used by female authors in the 19th century resulting in some literary works being incorrectly attributed or claimed. With this motivation, we collated a dataset of late 19th century novels in English. Due to the imbalance in the dataset and the unavailability of enough data per author, we employed the GANBERT model... | Kanishka Silva, Burcu Can, Frédéric Blain, Raheem Sarwar, Laura Ugolini, Ruslan Mitkov |  |
| 174 |  |  [How-to Guides for Specific Audiences: A Corpus and Initial Findings](https://doi.org/10.18653/v1/2023.acl-srw.46) |  | 0 | Instructional texts for specific target groups should ideally take into account the prior knowledge and needs of the readers in order to guide them efficiently to their desired goals. However, targeting specific groups also carries the risk of reflecting disparate social norms and subtle stereotypes. In this paper, we investigate the extent to which how-to guides from one particular platform, wikiHow, differ in practice depending on the intended audience. We conduct two case studies in which we... | Nicola Fanton, Agnieszka Falenska, Michael Roth |  |
| 175 |  |  ["When Words Fail, Emojis Prevail": A Novel Architecture for Generating Sarcastic Sentences With Emoji Using Valence Reversal and Semantic Incongruity](https://doi.org/10.18653/v1/2023.acl-srw.47) |  | 0 | Sarcasm is a form of figurative language that serves as a humorous tool for mockery and ridicule. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence in English. We divide the generation task into two sub tasks: one for generating textual sarcasm and another for collecting emojis associated with those sarcastic sentences. Two key elements of sarcasm are incorporated into the textual sarcasm generation task: valence reversal and semantic... | Faria Binte Kader, Nafisa Hossain Nujat, Tasmia Binte Sogir, Mohsinul Kabir, Hasan Mahmud, Md. Kamrul Hasan |  |
| 176 |  |  [Semantic Accuracy in Natural Language Generation: A Thesis Proposal](https://doi.org/10.18653/v1/2023.acl-srw.48) |  | 0 | With the fast-growing popularity of current large pre-trained language models (LLMs), it is necessary to dedicate efforts to making them more reliable. In this thesis proposal, we aim to improve the reliability of natural language generation systems (NLG) by researching the semantic accuracy of their outputs. We look at this problem from the outside (evaluation) and from the inside (interpretability). We propose a novel method for evaluating semantic accuracy and discuss the importance of... | Patrícia Schmidtová |  |
| 177 |  |  [Should you marginalize over possible tokenizations?](https://doi.org/10.18653/v1/2023.acl-short.1) |  | 0 | Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the... | Nadezhda Chirkova, Germán Kruszewski, Jos Rozen, Marc Dymetman |  |
| 178 |  |  [Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie](https://doi.org/10.18653/v1/2023.acl-short.2) |  | 0 | Accurate neural models are much less efficient than non-neural models and are useless for processing billions of social media posts or handling user queries in real time with a limited budget. This study revisits the fastest pattern-based NLP methods to make them as accurate as possible, thus yielding a strikingly simple yet surprisingly accurate morphological analyzer for Japanese. The proposed method induces reliable patterns from a morphological dictionary and annotated data. Experimental... | Naoki Yoshinaga |  |
| 179 |  |  [Transformed Protoform Reconstruction](https://doi.org/10.18653/v1/2023.acl-short.3) |  | 0 | Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates... | Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen |  |
| 180 |  |  [Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models](https://doi.org/10.18653/v1/2023.acl-short.4) |  | 0 | We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures. | Daniel Hardt |  |
| 181 |  |  [Bootstrapping Neural Relation and Explanation Classifiers](https://doi.org/10.18653/v1/2023.acl-short.5) |  | 0 | We introduce a method that self trains (or bootstraps) neural relation and explanation classifiers. Our work expands the supervised approach of CITATION, which jointly trains a relation classifier with an explanation classifier that identifies context words important for the relation at hand, to semi-supervised scenarios. In particular, our approach iteratively converts the explainable models’ outputs to rules and applies them to unlabeled text to produce new annotations. Our evaluation on the... | Zheng Tang, Mihai Surdeanu |  |
| 182 |  |  [A Fast Algorithm for Computing Prefix Probabilities](https://doi.org/10.18653/v1/2023.acl-short.6) |  | 0 | Multiple algorithms are known for efficiently calculating the prefix probability of a string under a probabilistic context-free grammar (PCFG). Good algorithms for the problem have a runtime cubic in the length of the input string. However, some proposed algorithms are suboptimal with respect to the size of the grammar. This paper proposes a new speed-up of Jelinek and Lafferty’s (1991) algorithm, which runs in O(n3\|N\|3 + \|N\|4), where n is the input length and \|N\| is the number of... | Franz Nowak, Ryan Cotterell |  |
| 183 |  |  [Analyzing Text Representations by Measuring Task Alignment](https://doi.org/10.18653/v1/2023.acl-short.7) |  | 0 | Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim. To test it, we develop a task alignment score based on hierarchical clustering that measures alignment at different levels of granularity. Our experiments on text classification validate our... | César GonzálezGutiérrez, Audi Primadhanty, Francesco Cazzaro, Ariadna Quattoni |  |
| 184 |  |  [Tracing Linguistic Markers of Influence in a Large Online Organisation](https://doi.org/10.18653/v1/2023.acl-short.8) |  | 0 | Social science and psycholinguistic research have shown that power and status affect how people use language in a range of domains. Here, we investigate a similar question in a large, distributed, consensus-driven community with little traditional power hierarchy – the Internet Engineering Task Force (IETF), a collaborative organisation that designs internet standards. Our analysis based on lexical categories (LIWC) and BERT, shows that participants’ levels of influence can be predicted from... | Prashant Khare, Ravi Shekhar, Mladen Karan, Stephen McQuistin, Colin Perkins, Ignacio Castro, Gareth Tyson, Patrick Healey, Matthew Purver |  |
| 185 |  |  [Metaphor Detection via Explicit Basic Meanings Modelling](https://doi.org/10.18653/v1/2023.acl-short.9) |  | 0 | One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design. While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its contextual meaning and its basic meaning, existing work does not strictly follow this principle, typically using the aggregated meaning to approximate the basic meaning of target words. In this paper, we propose a novel... | Yucheng Li, Shun Wang, Chenghua Lin, Frank Guerin |  |
| 186 |  |  [xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages](https://doi.org/10.18653/v1/2023.acl-short.10) |  | 0 | We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and... | Mingda Chen, Kevin Heffernan, Onur Çelebi, Alexandre Mourachko, Holger Schwenk |  |
| 187 |  |  [Graph Propagation based Data Augmentation for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.11) |  | 0 | Data augmentation is an effective solution to improve model performance and robustness for low-resource named entity recognition (NER). However, synthetic data often suffer from poor diversity, which leads to performance limitations. In this paper, we propose a novel Graph Propagated Data Augmentation (GPDA) framework for Named Entity Recognition (NER), leveraging graph propagation to build relationships between labeled data and unlabeled natural texts. By projecting the annotations from the... | Jiong Cai, Shen Huang, Yong Jiang, Zeqi Tan, Pengjun Xie, Kewei Tu |  |
| 188 |  |  [Dataset Distillation with Attention Labels for Fine-tuning BERT](https://doi.org/10.18653/v1/2023.acl-short.12) |  | 0 | Dataset distillation aims to create a small dataset of informative synthetic samples to rapidly train neural networks that retain the performance of the original dataset. In this paper, we focus on constructing distilled few-shot datasets for natural language processing (NLP) tasks to fine-tune pre-trained transformers. Specifically, we propose to introduce attention labels, which can efficiently distill the knowledge from the original dataset and transfer it to the transformer models via... | Aru Maekawa, Naoki Kobayashi, Kotaro Funakoshi, Manabu Okumura |  |
| 189 |  |  [Multi-Document Summarization with Centroid-Based Pretraining](https://doi.org/10.18653/v1/2023.acl-short.13) |  | 0 | In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through... | Ratish Surendran Puduppully, Parag Jain, Nancy Chen, Mark Steedman |  |
| 190 |  |  [Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times](https://doi.org/10.18653/v1/2023.acl-short.14) |  | 0 | Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate... | Andrea Gregor de Varda, Marco Marelli |  |
| 191 |  |  [Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques](https://doi.org/10.18653/v1/2023.acl-short.15) |  | 0 | Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM’s generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to... | Daking Rai, Bailin Wang, Yilun Zhou, Ziyu Yao |  |
| 192 |  |  [HiPool: Modeling Long Documents Using Graph Neural Networks](https://doi.org/10.18653/v1/2023.acl-short.16) |  | 0 | Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we... | Irene Li, Aosong Feng, Dragomir Radev, Rex Ying |  |
| 193 |  |  [A Weakly Supervised Classifier and Dataset of White Supremacist Language](https://doi.org/10.18653/v1/2023.acl-short.17) |  | 0 | We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias. | Michael Yoder, Ahmad Diab, David West Brown, Kathleen M. Carley |  |
| 194 |  |  [BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases](https://doi.org/10.18653/v1/2023.acl-short.18) |  | 0 | Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model’s output logits. Unlike prior... | Xin Liu, Muhammad Khalifa, Lu Wang |  |
| 195 |  |  [mOKB6: A Multilingual Open Knowledge Base Completion Benchmark](https://doi.org/10.18653/v1/2023.acl-short.19) |  | 0 | Automated completion of open knowledge bases (Open KBs), which are constructed from triples of the form (subject phrase, relation phrase, object phrase), obtained via open information extraction (Open IE) system, are useful for discovering novel facts that may not be directly present in the text. However, research in Open KB completion (Open KBC) has so far been limited to resource-rich languages like English. Using the latest advances in multilingual Open IE, we construct the first... | Shubham Mittal, Keshav Kolluru, Soumen Chakrabarti, Mausam |  |
| 196 |  |  [Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment](https://doi.org/10.18653/v1/2023.acl-short.20) |  | 0 | Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically.... | Roni Rabin, Alexandre Djerbetian, Roee Engelberg, Lidan Hackmon, Gal Elidan, Reut Tsarfaty, Amir Globerson |  |
| 197 |  |  [Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts](https://doi.org/10.18653/v1/2023.acl-short.21) |  | 0 | Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We... | Skyler Hallinan, Alisa Liu, Yejin Choi, Maarten Sap |  |
| 198 |  |  [A Natural Bias for Language Generation Models](https://doi.org/10.18653/v1/2023.acl-short.22) |  | 0 | After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this... | Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, Adhiguna Kuncoro |  |
| 199 |  |  [Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-short.23) |  | 0 | High-quality and high-coverage rule sets are imperative to the success of Neuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form the basis of all symbolic inferences. Recent literature builds neural models for generating rule sets, however, preliminary experiments show that they struggle with maintaining high coverage. In this work, we suggest three simple augmentations to existing rule sets: (1) transforming rules to their abductive forms, (2) generating equivalent rules... | Ananjan Nandi, Navdeep Kaur, Parag Singla, Mausam |  |
| 200 |  |  [Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer](https://doi.org/10.18653/v1/2023.acl-short.24) |  | 0 | Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks.... | Xingtai Lv, Ning Ding, Yujia Qin, Zhiyuan Liu, Maosong Sun |  |
| 201 |  |  [Faithfulness Tests for Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-short.25) |  | 0 | Explanations of neural models aim to reveal a model’s decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model’s inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a... | Pepa Atanasova, OanaMaria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein |  |
| 202 |  |  [COGEN: Abductive Commonsense Language Generation](https://doi.org/10.18653/v1/2023.acl-short.26) |  | 0 | Reasoning is one of the most important elements in achieving Artificial General Intelligence (AGI), specifically when it comes to Abductive and counterfactual reasoning. In order to introduce these capabilities of reasoning in Natural Language Processing (NLP) models, there have been recent advances towards training NLP models to better perform on two main tasks - Abductive Natural Language Inference (alphaNLI) and Abductive Natural Language Generation Task (alphaNLG). This paper proposes... | Rohola Zandie, Diwanshu Shekhar, Mohammad H. Mahoor |  |
| 203 |  |  [Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis](https://doi.org/10.18653/v1/2023.acl-short.27) |  | 0 | Multimodal relation extraction (MRE) is the task of identifying the semantic relationships between two entities based on the context of the sentence image pair. Existing retrieval-augmented approaches mainly focused on modeling the retrieved textual knowledge, but this may not be able to accurately identify complex relations. To improve the prediction, this research proposes to retrieve textual and visual evidence based on the object, sentence, and whole image. We further develop a novel... | Xuming Hu, Zhijiang Guo, Zhiyang Teng, Irwin King, Philip S. Yu |  |
| 204 |  |  [Characterization of Stigmatizing Language in Medical Records](https://doi.org/10.18653/v1/2023.acl-short.28) |  | 0 | Widespread disparities in clinical outcomes exist between different demographic groups in the United States. A new line of work in medical sociology has demonstrated physicians often use stigmatizing language in electronic medical records within certain groups, such as black patients, which may exacerbate disparities. In this study, we characterize these instances at scale using a series of domain-informed NLP techniques. We highlight important differences between this task and analogous... | Keith Harrigian, Ayah Zirikly, Brant Chee, Alya Ahmad, Anne Links, Somnath Saha, Mary Catherine Beach, Mark Dredze |  |
| 205 |  |  [Abstractive Summarizers are Excellent Extractive Summarizers](https://doi.org/10.18653/v1/2023.acl-short.29) |  | 0 | Extractive and abstractive summarization designs have historically been fragmented, limiting the benefits that often arise from compatible model architectures. In this paper, we explore the potential synergies of modeling extractive summarization with an abstractive summarization system and propose three novel inference algorithms using the sequence-to-sequence architecture. We evaluate them on the CNN & Dailymail dataset and show that recent advancements in abstractive system designs enable... | Daniel Varab, Yumo Xu |  |
| 206 |  |  [Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions](https://doi.org/10.18653/v1/2023.acl-short.30) |  | 0 | Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the... | Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, LouisPhilippe Morency |  |
| 207 |  |  [PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English](https://doi.org/10.18653/v1/2023.acl-short.31) |  | 0 | Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy... | Jianfeng Chi, Wasi Uddin Ahmad, Yuan Tian, KaiWei Chang |  |
| 208 |  |  [Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages](https://doi.org/10.18653/v1/2023.acl-short.32) |  | 0 | Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In... | Yasmine Karoui, Rémi Lebret, Negar Foroutan Eghlidi, Karl Aberer |  |
| 209 |  |  [BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-short.33) |  | 0 | Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by... | Jie He, Simon Chi Lok U, Víctor GutiérrezBasulto, Jeff Z. Pan |  |
| 210 |  |  [Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases](https://doi.org/10.18653/v1/2023.acl-short.34) |  | 0 | Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors. In this paper, we conduct a new series... | Haozhe An, Rachel Rudinger |  |
| 211 |  |  [Improving Syntactic Probing Correctness and Robustness with Control Tasks](https://doi.org/10.18653/v1/2023.acl-short.35) |  | 0 | Syntactic probing methods have been used to examine whether and how pre-trained language models (PLMs) encode syntactic features. However, the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations. This paper presents a random-word-substitution and random-label-matching control task to reduce these biases and improve the robustness of syntactic probing methods. Our control tasks are also shown to notably improve... | Weicheng Ma, Brian Wang, Hefan Zhang, Lili Wang, Rolando CotoSolano, Saeed Hassanpour, Soroush Vosoughi |  |
| 212 |  |  [Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications](https://doi.org/10.18653/v1/2023.acl-short.36) |  | 0 | In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both... | Jatin Arora, Youngja Park |  |
| 213 |  |  [Credible without Credit: Domain Experts Assess Generative Language Models](https://doi.org/10.18653/v1/2023.acl-short.37) |  | 0 | Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy,... | Denis Peskoff, Brandon M. Stewart |  |
| 214 |  |  [Grokking of Hierarchical Structure in Vanilla Transformers](https://doi.org/10.18653/v1/2023.acl-short.38) |  | 0 | For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods—far beyond the point when in-domain accuracy has saturated. We call this... | Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning |  |
| 215 |  |  [Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data](https://doi.org/10.18653/v1/2023.acl-short.39) |  | 0 | Adapters have emerged as a parameter-efficient Transformer-based framework for cross-lingual transfer by inserting lightweight language-specific modules (language adapters) and task-specific modules (task adapters) within pretrained multilingual models. Zero-shot transfer is enabled by pairing the language adapter in the target language with an appropriate task adapter in a source language. If our target languages are known apriori, we explore how zero-shot transfer can be further improved... | Ujan Deb, Ridayesh Parab, Preethi Jyothi |  |
| 216 |  |  [Context-Aware Transformer Pre-Training for Answer Sentence Selection](https://doi.org/10.18653/v1/2023.acl-short.40) |  | 0 | Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large annotated datasets, while using local contextual information around the candidate sentence. In this paper, we propose three pre-training objectives designed to mimic the downstream fine-tuning task of... | Luca Di Liello, Siddhant Garg, Alessandro Moschitti |  |
| 217 |  |  [Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities](https://doi.org/10.18653/v1/2023.acl-short.41) |  | 0 | Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations. First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior... | Zhihong Chen, Maya Varma, Xiang Wan, Curtis P. Langlotz, JeanBenoit Delbrouck |  |
| 218 |  |  [Efficient Diagnosis Assignment Using Unstructured Clinical Notes](https://doi.org/10.18653/v1/2023.acl-short.42) |  | 0 | Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these... | Louis Blankemeier, Jason A. Fries, Robert Tinn, Joseph Preston, Nigam Shah, Akshay Chaudhari |  |
| 219 |  |  [MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-short.43) |  | 0 | Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain?... | Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang, KaiWei Chang |  |
| 220 |  |  [On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach](https://doi.org/10.18653/v1/2023.acl-short.44) |  | 0 | In recent years, word embeddings have been widely used to measure biases in texts. Even if they have proven to be effective in detecting a wide variety of biases, metrics based on word embeddings lack transparency and interpretability. We analyze an alternative PMI-based metric to quantify biases in texts. It can be expressed as a function of conditional probabilities, which provides a simple interpretation in terms of word co-occurrences. We also prove that it can be approximated by an odds... | Francisco Valentini, Germán Rosati, Damián E. Blasi, Diego Fernández Slezak, Edgar Altszyler |  |
| 221 |  |  [Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models](https://doi.org/10.18653/v1/2023.acl-short.45) |  | 0 | Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap.... | Ehsan Doostmohammadi, Tobias Norlund, Marco Kuhlmann, Richard Johansson |  |
| 222 |  |  [MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents](https://doi.org/10.18653/v1/2023.acl-short.46) |  | 0 | Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns highquality representations of scientific papers by fine-tuning transformer model to predict the target journal... | Anastasia Razdaibiedina, Aleksander Brechalov |  |
| 223 |  |  [KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-short.47) |  | 0 | While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not... | Myeongjun Jang, Bodhisattwa Prasad Majumder, Julian J. McAuley, Thomas Lukasiewicz, OanaMaria Camburu |  |
| 224 |  |  [Measuring the Effect of Influential Messages on Varying Personas](https://doi.org/10.18653/v1/2023.acl-short.48) |  | 0 | Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the... | Chenkai Sun, Jinning Li, Hou Pong Chan, ChengXiang Zhai, Heng Ji |  |
| 225 |  |  [Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity](https://doi.org/10.18653/v1/2023.acl-short.49) |  | 0 | Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability. This paper presents a token-level matching inference algorithm, which can be... | Hongwei Wang, Dong Yu |  |
| 226 |  |  [Robust Learning for Multi-party Addressee Recognition with Discrete Addressee Codebook](https://doi.org/10.18653/v1/2023.acl-short.50) |  | 0 | Addressee recognition aims to identify addressees in multi-party conversations. While state-of-the-art addressee recognition models have achieved promising performance, they still suffer from the issue of robustness when applied in real-world scenes. When exposed to a noisy environment, these models regard the noise as input and identify the addressee in a pre-given addressee closed set, while the addressees of the noise do not belong to this closed set, thus leading to the wrong identification... | Pengcheng Zhu, Wei Zhou, Kuncai Zhang, Yuankai Ma, Haiqing Chen |  |
| 227 |  |  [TwistList: Resources and Baselines for Tongue Twister Generation](https://doi.org/10.18653/v1/2023.acl-short.51) |  | 0 | Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present TwistList, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We... | Tyler Loakman, Chen Tang, Chenghua Lin |  |
| 228 |  |  [Substitution-based Semantic Change Detection using Contextual Embeddings](https://doi.org/10.18653/v1/2023.acl-short.52) |  | 0 | Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly... | Dallas Card |  |
| 229 |  |  [Probing Physical Reasoning with Counter-Commonsense Context](https://doi.org/10.18653/v1/2023.acl-short.53) |  | 0 | In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates. We measure the ability of several masked... | Kazushi Kondo, Saku Sugawara, Akiko Aizawa |  |
| 230 |  |  [Morphological Inflection with Phonological Features](https://doi.org/10.18653/v1/2023.acl-short.54) |  | 0 | Recent years have brought great advances into solving morphological tasks, mostly due to powerful neural models applied to various tasks as (re)inflection and analysis. Yet, such morphological tasks cannot be considered solved, especially when little training data is available or when generalizing to previously unseen lemmas. This work explores effects on performance obtained through various ways in which morphological models get access to sub-character phonological features that are often the... | David Guriel, Omer Goldman, Reut Tsarfaty |  |
| 231 |  |  [A Holistic Approach to Reference-Free Evaluation of Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.55) |  | 0 | Traditional machine translation evaluation relies on reference written by humans. While reference-free evaluation gets rid of the constraints of labor-intensive annotations, which can pivot easily to new domains and is more scalable. In this paper, we propose a reference-free evaluation approach that characterizes evaluation as two aspects: (1) fluency: how well the translated text conforms to normal human language usage; (2) faithfulness: how well the translated text reflects the source data.... | Hanming Wu, Wenjuan Han, Hui Di, Yufeng Chen, Jinan Xu |  |
| 232 |  |  [Balancing Lexical and Semantic Quality in Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-short.56) |  | 0 | An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we... | Jeewoo Sul, Yong Suk Choi |  |
| 233 |  |  [Learning Neuro-Symbolic World Models with Conversational Proprioception](https://doi.org/10.18653/v1/2023.acl-short.57) |  | 0 | The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we... | Don Joven Agravante, Daiki Kimura, Michiaki Tatsubori, Asim Munawar, Alexander Gray |  |
| 234 |  |  [In and Out-of-Domain Text Adversarial Robustness via Label Smoothing](https://doi.org/10.18653/v1/2023.acl-short.58) |  | 0 | Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the... | Yahan Yang, Soham Dan, Dan Roth, Insup Lee |  |
| 235 |  |  [LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-short.59) |  | 0 | In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more... | Amirhossein Abaskohi, Sascha Rothe, Yadollah Yaghoobzadeh |  |
| 236 |  |  [Considerations for meaningful sign language machine translation based on glosses](https://doi.org/10.18653/v1/2023.acl-short.60) |  | 0 | Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation. To address these... | Mathias Müller, Zifan Jiang, Amit Moryossef, Annette Rios, Sarah Ebling |  |
| 237 |  |  [Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature](https://doi.org/10.18653/v1/2023.acl-short.61) |  | 0 | The COVID-19 pandemic created a deluge of questionable and contradictory scientific claims about drug efficacy – an “infodemic” with lasting consequences for science and society. In this work, we argue that NLP models can help domain experts distill and understand the literature in this complex, high-stakes area. Our task is to automatically identify contradictory claims about COVID-19 drug efficacy. We frame this as a natural language inference problem and offer a new NLI dataset created by... | Daniel N. Sosa, Malavika Suresh, Christopher Potts, Russ B. Altman |  |
| 238 |  |  [The Role of Global and Local Context in Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.62) |  | 0 | Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the... | Arthur Amalvy, Vincent Labatut, Richard Dufour |  |
| 239 |  |  [Joint End-to-end Semantic Proto-role Labeling](https://doi.org/10.18653/v1/2023.acl-short.63) |  | 0 | Semantic proto-role labeling (SPRL) assigns properties to arguments based on a series of binary labels. While multiple studies have evaluated various approaches to SPRL, it has only been studied in-depth as a standalone task using gold predicate/argument pairs. How do SPRL systems perform as part of an information extraction pipeline? We model SPRL jointly with predicate-argument extraction using a deep transformer model. We find that proto-role labeling is surprisingly robust in this setting,... | Elizabeth Spaulding, Gary Kazantsev, Mark Dredze |  |
| 240 |  |  [Improving Automatic Quotation Attribution in Literary Novels](https://doi.org/10.18653/v1/2023.acl-short.64) |  | 0 | Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution. We benchmark state-of-the-art models on each of these sub-tasks independently, using a large dataset of annotated... | Krishnapriya Vishnubhotla, Frank Rudzicz, Graeme Hirst, Adam Hammond |  |
| 241 |  |  [Modular Visual Question Answering via Code Generation](https://doi.org/10.18653/v1/2023.acl-short.65) |  | 0 | We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy... | Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein |  |
| 242 |  |  [Target-Based Offensive Language Identification](https://doi.org/10.18653/v1/2023.acl-short.66) |  | 0 | We present TBO, a new dataset for Target-based Offensive language identification. TBO contains post-level annotations regarding the harmfulness of an offensive post and token-level annotations comprising of the target and the offensive argument expression. Popular offensive language identification datasets for social media focus on annotation taxonomies only at the post level and more recently, some datasets have been released that feature only token-level annotations. TBO is an important... | Marcos Zampieri, Skye Morgan, Kai North, Tharindu Ranasinghe, Austin Simmmons, Paridhi Khandelwal, Sara Rosenthal, Preslav Nakov |  |
| 243 |  |  [Unsupervised Subtitle Segmentation with Masked Language Models](https://doi.org/10.18653/v1/2023.acl-short.67) |  | 0 | We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points. Our approach obtained competitive results in terms of segmentation accuracy across metrics, while also fully preserving the original text and complying with length constraints. Although supervised models trained on in-domain data and with access to... | David Ponce, Thierry Etchegoyhen, Victor Ruiz Gómez |  |
| 244 |  |  [Exploring Continual Learning for Code Generation Models](https://doi.org/10.18653/v1/2023.acl-short.68) |  | 0 | Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and... | Prateek Yadav, Qing Sun, Hantian Ding, Xiaopeng Li, Dejiao Zhang, Ming Tan, Parminder Bhatia, Xiaofei Ma, Ramesh Nallapati, Murali Krishna Ramanathan, Mohit Bansal, Bing Xiang |  |
| 245 |  |  [Deep Active Learning for Morphophonological Processing](https://doi.org/10.18653/v1/2023.acl-short.69) |  | 0 | Building a system for morphological processing is a challenging task in morphologically complex languages like Arabic. Although there are some deep learning based models that achieve successful results, these models rely on a large amount of annotated data. Building such datasets, specially for some of the lower-resource Arabic dialects, is very difficult, time-consuming, and expensive. In addition, some parts of the annotated data do not contain useful information for training machine learning... | Seyed Morteza Mirbostani, Yasaman Boreshban, Salam Khalifa, Seyed Abolghasem Mirroshandel, Owen Rambow |  |
| 246 |  |  [Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios](https://doi.org/10.18653/v1/2023.acl-short.70) |  | 0 | Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as... | Jiaxuan Li, Lang Yu, Allyson Ettinger |  |
| 247 |  |  [Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages](https://doi.org/10.18653/v1/2023.acl-short.71) |  | 0 | We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better... | Yash Madhani, Mitesh M. Khapra, Anoop Kunchukuttan |  |
| 248 |  |  [Using contradictions improves question answering systems](https://doi.org/10.18653/v1/2023.acl-short.72) |  | 0 | This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is entailed (supported) by some background context. But is it useful to also determine if an answer contradicts the context? We test this in two settings, multiple choice and extractive QA, and find that systems that incorporate contradiction can do slightly better than entailment-only systems on certain... | Etienne FortierDubois, Domenic Rosati |  |
| 249 |  |  [Token-Level Self-Evolution Training for Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2023.acl-short.73) |  | 0 | Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g. word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal. In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method... | Keqin Peng, Liang Ding, Qihuang Zhong, Yuanxin Ouyang, Wenge Rong, Zhang Xiong, Dacheng Tao |  |
| 250 |  |  [Gradient Ascent Post-training Enhances Language Model Generalization](https://doi.org/10.18653/v1/2023.acl-short.74) |  | 0 | In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks. We also show that applying GAP on out-of-distribution corpora leads to the most reliable performance improvements. Our... | Dongkeun Yoon, Joel Jang, Sungdong Kim, Minjoon Seo |  |
| 251 |  |  [An Open Dataset and Model for Language Identification](https://doi.org/10.18653/v1/2023.acl-short.75) |  | 0 | Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033% across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, which we audit manually to ensure reliability. We make both the model and the... | Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, Kenneth Heafield |  |
| 252 |  |  [Evaluating Paraphrastic Robustness in Textual Entailment Models](https://doi.org/10.18653/v1/2023.acl-short.76) |  | 0 | We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models’ predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that... | Dhruv Verma, Yash Kumar Lal, Shreyashee Sinha, Benjamin Van Durme, Adam Poliak |  |
| 253 |  |  [Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?](https://doi.org/10.18653/v1/2023.acl-short.77) |  | 0 | Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models. However, the performance does not improve but even gets worse after the PLM-based ensemble. This surprising result sets us doing a... | Chenming Tang, Xiuyu Wu, Yunfang Wu |  |
| 254 |  |  [Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality](https://doi.org/10.18653/v1/2023.acl-short.78) |  | 0 | Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro ‘MODEL’}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries... | Tanay Dixit, Fei Wang, Muhao Chen |  |
| 255 |  |  [With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness](https://doi.org/10.18653/v1/2023.acl-short.79) |  | 0 | Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional... | Julius Steen, Juri Opitz, Anette Frank, Katja Markert |  |
| 256 |  |  [A Better Way to Do Masked Language Model Scoring](https://doi.org/10.18653/v1/2023.acl-short.80) |  | 0 | Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its... | Carina Kauf, Anna A. Ivanova |  |
| 257 |  |  [ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?](https://doi.org/10.18653/v1/2023.acl-short.81) |  | 0 | Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT... | Michael Heck, Nurul Lubis, Benjamin Matthias Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, HsienChin Lin, Carel van Niekerk, Milica Gasic |  |
| 258 |  |  [Controllable Mixed-Initiative Dialogue Generation through Prompting](https://doi.org/10.18653/v1/2023.acl-short.82) |  | 0 | Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large... | Maximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi, Zhou Yu |  |
| 259 |  |  [Enhancing Event Causality Identification with Counterfactual Reasoning](https://doi.org/10.18653/v1/2023.acl-short.83) |  | 0 | Existing methods for event causality identification (ECI) focus on mining potential causal signals, i.e., causal context keywords and event pairs. However, causal signals are ambiguous, which may lead to the context-keywords bias and the event-pairs bias. To solve this issue, we propose the counterfactual reasoning that explicitly estimates the influence of context keywords and event pairs in training, so that we are able to eliminate the biases in inference.Experiments are conducted on two... | Feiteng Mu, Wenjie Li |  |
| 260 |  |  [Contrastive Bootstrapping for Label Refinement](https://doi.org/10.18653/v1/2023.acl-short.84) |  | 0 | Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively... | Shudi Hou, Yu Xia, Muhao Chen, Sujian Li |  |
| 261 |  |  [NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification](https://doi.org/10.18653/v1/2023.acl-short.85) |  | 0 | Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create a new dataset, Nollywood movie reviews for five languages widely spoken in Nigeria (English,... | Iyanuoluwa Shode, David Ifeoluwa Adelani, Jing Peng, Anna Feldman |  |
| 262 |  |  [Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement](https://doi.org/10.18653/v1/2023.acl-short.86) |  | 0 | State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to be effective at representing rare words or words with insufficient context information. To address this issue,... | Samuel Mensah, Kai Sun, Nikolaos Aletras |  |
| 263 |  |  [An (unhelpful) guide to selecting the best ASR architecture for your under-resourced language](https://doi.org/10.18653/v1/2023.acl-short.87) |  | 0 | Advances in deep neural models for automatic speech recognition (ASR) have yielded dramatic improvements in ASR quality for resource-rich languages, with English ASR now achieving word error rates comparable to that of human transcribers. The vast majority of the world’s languages, however, lack the quantity of data necessary to approach this level of accuracy. In this paper we use four of the most popular ASR toolkits to train ASR models for eleven languages with limited ASR training... | Robert Jimerson, Zoey Liu, Emily Prud'hommeaux |  |
| 264 |  |  [The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics](https://doi.org/10.18653/v1/2023.acl-short.88) |  | 0 | Many NLP tasks exhibit human label variation, where different annotators give different labels to the same texts. This variation is known to depend, at least in part, on the sociodemographics of annotators. Recent research aims to model individual annotator behaviour rather than predicting aggregated labels, and we would expect that sociodemographic information is useful for these models. On the other hand, the ecological fallacy states that aggregate group behaviour, such as the behaviour of... | Matthias Orlikowski, Paul Röttger, Philipp Cimiano, Dirk Hovy |  |
| 265 |  |  [Decomposed scoring of CCG dependencies](https://doi.org/10.18653/v1/2023.acl-short.89) |  | 0 | In statistical parsing with CCG, the standard evaluation method is based on predicate-argument structure and evaluates dependencies labelled in part by lexical categories. When a predicate has multiple argument slots that can be filled, the same lexical category is used for the label of multiple dependencies. In this paper, we show that this evaluation can result in disproportionate penalization of supertagging errors and obfuscate the truly erroneous dependencies. Enabled by the compositional... | Aditya Bhargava, Gerald Penn |  |
| 266 |  |  [Do GPTs Produce Less Literal Translations?](https://doi.org/10.18653/v1/2023.acl-short.90) |  | 0 | Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT)... | Vikas Raunak, Arul Menezes, Matt Post, Hany Hassan |  |
| 267 |  |  [Environmental Claim Detection](https://doi.org/10.18653/v1/2023.acl-short.91) |  | 0 | To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable. To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this. Thus, this paper introduces the task of environmental claim detection. To accompany the task, we release an expert-annotated dataset and models trained on this dataset. We preview one potential application of such models: We... | Dominik Stammbach, Nicolas Webersinke, Julia Anna Bingler, Mathias Kraus, Markus Leippold |  |
| 268 |  |  [Black-box language model explanation by context length probing](https://doi.org/10.18653/v1/2023.acl-short.92) |  | 0 | The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present \*context length probing\*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign \*differential importance scores\* to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing... | Ondrej Cífka, Antoine Liutkus |  |
| 269 |  |  [Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation](https://doi.org/10.18653/v1/2023.acl-short.93) |  | 0 | Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings. Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any additional operation, neglecting the prompt-demonstration dependencies. Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that... | Sirui Wang, Kaiwen Wei, Hongzhi Zhang, Yuntao Li, Wei Wu |  |
| 270 |  |  [The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics](https://doi.org/10.18653/v1/2023.acl-short.94) |  | 0 | Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, “black boxes” returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting... | Ricardo Rei, Nuno Miguel Guerreiro, Marcos V. Treviso, Luísa Coheur, Alon Lavie, André F. T. Martins |  |
| 271 |  |  [Typo-Robust Representation Learning for Dense Retrieval](https://doi.org/10.18653/v1/2023.acl-short.95) |  | 0 | Dense retrieval is a basic building block of information retrieval applications. One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words. A popular approach for handling misspelled queries is minimizing the representations discrepancy between misspelled queries and their pristine ones. Unlike the existing approaches, which only focus on the alignment between misspelled and pristine queries, our method also improves the contrast... | Panuthep Tasawong, Wuttikorn Ponwitayarat, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong |  |
| 272 |  |  [Focused Prefix Tuning for Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-short.96) |  | 0 | In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks,... | Congda Ma, Tianyu Zhao, Makoto Shing, Kei Sawada, Manabu Okumura |  |
| 273 |  |  [ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-short.97) |  | 0 | Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base... | Jianyi Zhang, Aashiq Muhamed, Aditya Anantharaman, Guoyin Wang, Changyou Chen, Kai Zhong, Qingjun Cui, Yi Xu, Belinda Zeng, Trishul Chilimbi, Yiran Chen |  |
| 274 |  |  [Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood](https://doi.org/10.18653/v1/2023.acl-short.98) |  | 0 | Recognizing flat, overlapped and discontinuous entities uniformly has been paid increasing attention. Among these works, Seq2Seq formulation prevails for its flexibility and effectiveness. It arranges the output entities into a specific target sequence. However, it introduces bias by assigning all the probability mass to the observed sequence. To alleviate the bias, previous works either augment the data with possible sequences or resort to other formulations. In this paper, we stick to the... | Yu Xia, Yongwei Zhao, Wenhao Wu, Sujian Li |  |
| 275 |  |  [Deriving Language Models from Masked Language Models](https://doi.org/10.18653/v1/2023.acl-short.99) |  | 0 | Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions from MLMs, focusing on distributions over two tokens, which makes it possible to calculate exact distributional properties. We find that an approach based on identifying joints whose conditionals are... | Lucas Torroba Hennigen, Yoon Kim |  |
| 276 |  |  [UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation](https://doi.org/10.18653/v1/2023.acl-short.100) |  | 0 | Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our... | Zhiming Mao, Huimin Wang, Yiming Du, KamFai Wong |  |
| 277 |  |  [Reasoning Implicit Sentiment with Chain-of-Thought Prompting](https://doi.org/10.18653/v1/2023.acl-short.101) |  | 0 | While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic... | Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, TatSeng Chua |  |
| 278 |  |  [Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://doi.org/10.18653/v1/2023.acl-short.102) |  | 0 | The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within... | TaChung Chi, TingHan Fan, LiWei Chen, Alexander Rudnicky, Peter J. Ramadge |  |
| 279 |  |  [Is Anisotropy Truly Harmful? A Case Study on Text Clustering](https://doi.org/10.18653/v1/2023.acl-short.103) |  | 0 | In the last few years, several studies have been devoted to dissecting dense text representations in order to understand their effectiveness and further improve their quality. Particularly, the anisotropy of such representations has been observed, which means that the directions of the word vectors are not evenly distributed across the space but rather concentrated in a narrow cone. This has led to several attempts to counteract this phenomenon both on static and contextualized text... | Mira Ait Saada, Mohamed Nadif |  |
| 280 |  |  [Class based Influence Functions for Error Detection](https://doi.org/10.18653/v1/2023.acl-short.104) |  | 0 | Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs.Extensive experiments show that our modification significantly... | Thang NguyenDuc, Hoang ThanhTung, Quan Hung Tran, HuuTien Dang, Hieu Nguyen, Anh T. V. Dau, Nghi D. Q. Bui |  |
| 281 |  |  [Leveraging Prefix Transfer for Multi-Intent Text Revision](https://doi.org/10.18653/v1/2023.acl-short.105) |  | 0 | Text revision is a necessary process to improve text quality. During this process, writers constantly edit texts out of different edit intentions. Identifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts according to one specific edit intention. In this work, we aim to build a multi-intent text revision system that could revise texts without explicit intent annotation. Our system is based on prefix-tuning,... | Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye Jin, Liner Yang, Yange Fan, Hanghang Fan, Erhong Yang |  |
| 282 |  |  [Learning Multi-Step Reasoning by Solving Arithmetic Tasks](https://doi.org/10.18653/v1/2023.acl-short.106) |  | 0 | Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs’ impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of... | Tianduo Wang, Wei Lu |  |
| 283 |  |  [Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning](https://doi.org/10.18653/v1/2023.acl-short.107) |  | 0 | Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation... | Zhenru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, Songfang Huang |  |
| 284 |  |  [Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.acl-short.108) |  | 0 | Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin. In this work, we empirically show that... | Zahra Fatemi, Chen Xing, Wenhao Liu, Caiming Xiong |  |
| 285 |  |  [Class-Incremental Learning based on Label Generation](https://doi.org/10.18653/v1/2023.acl-short.109) |  | 0 | Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the... | Yijia Shao, Yiduo Guo, Dongyan Zhao, Bing Liu |  |
| 286 |  |  [Evaluating pragmatic abilities of image captioners on A3DS](https://doi.org/10.18653/v1/2023.acl-short.110) |  | 0 | Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans. To enable such evaluation, we present a novel open source image-text dataset “Annotated 3D Shapes” (A3DS) comprising over nine million exhaustive natural language annotations and over 12 million variable-granularity captions for the 480,000... | Polina Tsvilodub, Michael Franke |  |
| 287 |  |  [The Art of Prompting: Event Detection based on Type Specific Prompts](https://doi.org/10.18653/v1/2023.acl-short.111) |  | 0 | We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zero-shot event detection. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (few-shot event detection) or not available (zero-shot event detection). By leveraging the semantics of event... | Sijia Wang, Mo Yu, Lifu Huang |  |
| 288 |  |  [Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.112) |  | 0 | This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on... | Zhuoyuan Mao, Raj Dabre, Qianying Liu, Haiyue Song, Chenhui Chu, Sadao Kurohashi |  |
| 289 |  |  [Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning](https://doi.org/10.18653/v1/2023.acl-short.113) |  | 0 | Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive performance gains, what models learn from IT remains understudied. In this work, we analyze how models utilize instructions during IT by comparing model training with altered vs. original instructions.... | PoNien Kung, Nanyun Peng |  |
| 290 |  |  [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models](https://doi.org/10.18653/v1/2023.acl-short.114) |  | 0 | We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-RBase and InfoXLMBase and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance... | James O'Neill, Sourav Dutta |  |
| 291 |  |  [Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.acl-short.115) |  | 0 | Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace ”modality gap” between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap occurs in the early stages of fine-tuning, but does not have a major impact on the final performance. On the other hand, we find that there has another gap, which we call the ”capacity gap”: high resource... | Yuchen Han, Chen Xu, Tong Xiao, Jingbo Zhu |  |
| 292 |  |  [Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data](https://doi.org/10.18653/v1/2023.acl-short.116) |  | 0 | Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves... | Yufei Li, Xiao Yu, Yanchi Liu, Haifeng Chen, Cong Liu |  |
| 293 |  |  [Text-to-SQL Error Correction with Language Models of Code](https://doi.org/10.18653/v1/2023.acl-short.117) |  | 0 | Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming... | Ziru Chen, Shijie Chen, Michael White, Raymond J. Mooney, Ali Payani, Jayanth Srinivasa, Yu Su, Huan Sun |  |
| 294 |  |  [The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks](https://doi.org/10.18653/v1/2023.acl-short.118) |  | 0 | How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing... | Nikil Roashan Selvam, Sunipa Dev, Daniel Khashabi, Tushar Khot, KaiWei Chang |  |
| 295 |  |  [Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)](https://doi.org/10.18653/v1/2023.acl-short.119) |  | 0 | Large language models, particularly GPT-3, are able to produce high quality summaries ofgeneral domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine. In this paper we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given no supervision. We consider bothsingle- and multi-document settings. In the former, GPT-3 is... | Chantal Shaib, Millicent L. Li, Sebastian Joseph, Iain James Marshall, Junyi Jessy Li, Byron C. Wallace |  |
| 296 |  |  [Prefix Propagation: Parameter-Efficient Tuning for Long Sequences](https://doi.org/10.18653/v1/2023.acl-short.120) |  | 0 | Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model’s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose... | Jonathan Li, Will Aitken, Rohan Bhambhoria, Xiaodan Zhu |  |
| 297 |  |  [Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain](https://doi.org/10.18653/v1/2023.acl-short.121) |  | 0 | PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common. It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively. Methods developed in the literature, however, cannot be deployed to real gameplaysince they only tackle some subtasks of the game,and they require additional reference chains inputs, whose extraction... | ShihLun Wu, YiHui Chou, Liangze Li |  |
| 298 |  |  [Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations](https://doi.org/10.18653/v1/2023.acl-short.122) |  | 0 | Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English–German: the better the given MT is, the harder it is to decide what parts to edit and how to fix these errors. One possible solution to this problem is to instill deeper knowledge about the target language into the model. Thus, we propose a... | Baikjin Jung, Myungji Lee, JongHyeok Lee, Yunsu Kim |  |
| 299 |  |  [An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.123) |  | 0 | Named entity recognition (NER) is the task to detect and classify entity spans in the text. When entity spans overlap between each other, the task is named as nested NER. Span-based methods have been widely used to tackle nested NER. Most of these methods get a score matrix, where each entry corresponds to a span. However, previous work ignores spatial relations in the score matrix. In this paper, we propose using Convolutional Neural Network (CNN) to model these spatial relations. Despite... | Hang Yan, Yu Sun, Xiaonan Li, Xipeng Qiu |  |
| 300 |  |  [Hexatagging: Projective Dependency Parsing as Tagging](https://doi.org/10.18653/v1/2023.acl-short.124) |  | 0 | We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a... | Afra Amini, Tianyu Liu, Ryan Cotterell |  |
| 301 |  |  [Understanding Demonstration-based Learning from a Causal Perspective](https://doi.org/10.18653/v1/2023.acl-short.125) |  | 0 | Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate... | Ruiyi Zhang, Tong Yu |  |
| 302 |  |  [RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation](https://doi.org/10.18653/v1/2023.acl-short.126) |  | 0 | Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting... | Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, Maria Nadejde |  |
| 303 |  |  [Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation](https://doi.org/10.18653/v1/2023.acl-short.127) |  | 0 | Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target... | Haoyang Wen, Alexander G. Hauptmann |  |
| 304 |  |  [Discourse-Level Representations can Improve Prediction of Degree of Anxiety](https://doi.org/10.18653/v1/2023.acl-short.128) |  | 0 | Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourse-level information in addition to lexical-level large language model... | Swanie Juhng, Matthew Matero, Vasudha Varadarajan, Johannes C. Eichstaedt, Adithya V. Ganesan, H. Andrew Schwartz |  |
| 305 |  |  [Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://doi.org/10.18653/v1/2023.acl-short.129) |  | 0 | Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the... | Mustafa Özdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta |  |
| 306 |  |  [MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting](https://doi.org/10.18653/v1/2023.acl-short.130) |  | 0 | Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our... | Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, Sadao Kurohashi |  |
| 307 |  |  [mPMR: A Multilingual Pre-trained Machine Reader at Scale](https://doi.org/10.18653/v1/2023.acl-short.131) |  | 0 | We present multilingual Pre-trained Machine Reader (mPMR), a novel method for multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims to guide multilingual pre-trained language models (mPLMs) to perform natural language understanding (NLU) including both sequence classification and span extraction in multiple languages. To achieve cross-lingual generalization when only source-language fine-tuning data is available, existing mPLMs solely transfer NLU capability from a... | Weiwen Xu, Xin Li, Wai Lam, Lidong Bing |  |
| 308 |  |  [MOSPC: MOS Prediction Based on Pairwise Comparison](https://doi.org/10.18653/v1/2023.acl-short.132) |  | 0 | As a subjective metric to evaluate the quality of synthesized speech, Mean opinion score(MOS) usually requires multiple annotators to score the same speech. Such an annotation approach requires a lot of manpower and is also time-consuming. MOS prediction model for automatic evaluation can significantly reduce labor cost. In previous works, it is difficult to accurately rank the quality of speech when the MOS scores are close. However, in practical applications, it is more important to correctly... | Kexin Wang, Yunlong Zhao, Qianqian Dong, Tom Ko, Mingxuan Wang |  |
| 309 |  |  [LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering](https://doi.org/10.18653/v1/2023.acl-short.133) |  | 0 | Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each. These fixed vectors can be insufficient to capture fine-grained features of potentially very big tables with heterogeneous row/column information. We address this limitation by 1) applying late interaction models which enforce a... | Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert, Gonzalo Iglesias |  |
| 310 |  |  [How Well Apply Simple MLP to Incomplete Utterance Rewriting?](https://doi.org/10.18653/v1/2023.acl-short.134) |  | 0 | Incomplete utterance rewriting (IUR) aims to restore the incomplete utterance with sufficient context information for comprehension. This paper introduces a simple yet efficient IUR method. Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR). After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance. The well-designed network and... | Jiang Li, Xiangdong Su, Xinlan Ma, Guanglai Gao |  |
| 311 |  |  [XL-LEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE](https://doi.org/10.18653/v1/2023.acl-short.135) |  | 0 | The recent introduction of large-scale datasets for the WiC (Word in Context) task enables the creation of more reliable and meaningful contextualized word embeddings.However, most of the approaches to the WiC task use cross-encoders, which prevent the possibility of deriving comparable word embeddings.In this work, we introduce XL-LEXEME, a Lexical Semantic Change Detection model.XL-LEXEME extends SBERT, highlighting the target word in the sentence. We evaluate XL-LEXEME on the multilingual... | Pierluigi Cassotti, Lucia Siciliani, Marco de Gemmis, Giovanni Semeraro, Pierpaolo Basile |  |
| 312 |  |  [Theory-Grounded Computational Text Analysis](https://doi.org/10.18653/v1/2023.acl-short.136) |  | 0 | In this position paper, we argue that computational text analysis lacks and requires organizing principles. A broad space separates its two constituent disciplines—natural language processing and social science—which has to date been sidestepped rather than filled by applying increasingly complex computational models to problems in social science research. We contrast descriptive and integrative findings, and our review of approximately 60 papers on computational text analysis reveals that... | Arya D. McCarthy, Giovanna Maria Dora Dore |  |
| 313 |  |  [AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing](https://doi.org/10.18653/v1/2023.acl-short.137) |  | 0 | In this paper, we examine the current state-of-the-art in AMR parsing, which relies on ensemble strategies by merging multiple graph predictions. Our analysis reveals that the present models often violate AMR structural constraints. To address this issue, we develop a validation method, and show how ensemble models can exploit SMATCH metric weaknesses to obtain higher scores, but sometimes result in corrupted graphs. Additionally, we highlight the demanding need to compute the SMATCH score... | Abelardo Carlos Martinez Lorenzo, PereLluís Huguet Cabot, Roberto Navigli |  |
| 314 |  |  [MolXPT: Wrapping Molecules with Text for Generative Pre-training](https://doi.org/10.18653/v1/2023.acl-short.138) |  | 0 | Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the... | Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, TieYan Liu |  |
| 315 |  |  [A Study on the Efficiency and Generalization of Light Hybrid Retrievers](https://doi.org/10.18653/v1/2023.acl-short.139) |  | 0 | Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study “Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance”? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge... | Man Luo, Shashank Jain, Anchit Gupta, Arash Einolghozati, Barlas Oguz, Debojeet Chatterjee, Xilun Chen, Chitta Baral, Peyman Heidari |  |
| 316 |  |  [The Mechanical Bard: An Interpretable Machine Learning Approach to Shakespearean Sonnet Generation](https://doi.org/10.18653/v1/2023.acl-short.140) |  | 0 | We consider the automated generation of sonnets, a poetic form constrained according to meter, rhyme scheme, and length. Sonnets generally also use rhetorical figures, expressive language, and a consistent theme or narrative. Our constrained decoding approach allows for the generation of sonnets within preset poetic constraints, while using a relatively modest neural backbone. Human evaluation confirms that our approach produces Shakespearean sonnets that resemble human-authored sonnets, and... | Edwin Agnew, Michelle Qiu, Lily Zhu, Sam Wiseman, Cynthia Rudin |  |
| 317 |  |  [When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants](https://doi.org/10.18653/v1/2023.acl-short.141) |  | 0 | We present the first unified study of the efficiency of self-attention-based Transformer variants spanning text, speech and vision. We identify input length thresholds (tipping points) at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency metrics (latency, throughput, and memory). To conduct this analysis for speech, we introduce L-HuBERT, a novel local-attention variant of a self-supervised speech model. We observe that these... | Anuj Diwan, Eunsol Choi, David Harwath |  |
| 318 |  |  [Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction (ACE) Annotations](https://doi.org/10.18653/v1/2023.acl-short.142) |  | 0 | Zero-shot event extraction (EE) methods infer richly structured event records from text, based only on a minimal user specification and no training examples, which enables flexibility in exploring and developing applications. Most event extraction research uses the Automatic Content Extraction (ACE) annotated dataset to evaluate supervised EE methods, but can it be used to evaluate zero-shot and other low-supervision EE? We describe ACE’s event structures and identify significant ambiguities... | Erica Cai, Brendan T. O'Connor |  |
| 319 |  |  [Event Extraction as Question Generation and Answering](https://doi.org/10.18653/v1/2023.acl-short.143) |  | 0 | Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches... | Di Lu, Shihao Ran, Joel R. Tetreault, Alejandro Jaimes |  |
| 320 |  |  [Are Sample-Efficient NLP Models More Robust?](https://doi.org/10.18653/v1/2023.acl-short.144) |  | 0 | Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-ofdistribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship... | Nelson F. Liu, Ananya Kumar, Percy Liang, Robin Jia |  |
| 321 |  |  [Diversity-Aware Coherence Loss for Improving Neural Topic Models](https://doi.org/10.18653/v1/2023.acl-short.145) |  | 0 | The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level... | Raymond Li, Felipe GonzálezPizarro, Linzi Xing, Gabriel Murray, Giuseppe Carenini |  |
| 322 |  |  [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://doi.org/10.18653/v1/2023.acl-short.146) |  | 0 | Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the... | Haoxin Li, Phillip Keung, Daniel Cheng, Jungo Kasai, Noah A. Smith |  |
| 323 |  |  [S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering](https://doi.org/10.18653/v1/2023.acl-short.147) |  | 0 | Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever,... | Fangyu Lei, Xiang Li, Yifan Wei, Shizhu He, Yiming Huang, Jun Zhao, Kang Liu |  |
| 324 |  |  [Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue](https://doi.org/10.18653/v1/2023.acl-short.148) |  | 0 | Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts. We argue that this problem mainly stems from the polarized optimization objectives and weak knowledge generation ability. To mitigate the hallucination, we take inspiration from human communicating that people will replay euphemistic responses for the unclear... | Bin Sun, Yitong Li, Fei Mi, Fanhu Bie, Yiwei Li, Kan Li |  |
| 325 |  |  [AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models](https://doi.org/10.18653/v1/2023.acl-short.149) |  | 0 | Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language... | Siheng Li, Cheng Yang, Yichun Yin, Xinyu Zhu, Zesen Cheng, Lifeng Shang, Xin Jiang, Qun Liu, Yujiu Yang |  |
| 326 |  |  [STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions](https://doi.org/10.18653/v1/2023.acl-short.150) |  | 0 | We present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The data is collected using a web app in which the speakers are shown Standard German sentences, which they translate to Swiss German and record. We make the corpus publicly available. It contains 343 hours of speech from all dialect regions and is the largest public speech corpus for Swiss German to date. Application areas include automatic speech recognition (ASR),... | Michel Plüss, Jan Deriu, Yanick Schraner, Claudio Paonessa, Julia Hartmann, Larissa Schmidt, Christian Scheller, Manuela Hürlimann, Tanja Samardzic, Manfred Vogel, Mark Cieliebak |  |
| 327 |  |  [Teaching Small Language Models to Reason](https://doi.org/10.18653/v1/2023.acl-short.151) |  | 0 | Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the... | Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adámek, Eric Malmi, Aliaksei Severyn |  |
| 328 |  |  [A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification](https://doi.org/10.18653/v1/2023.acl-short.152) |  | 0 | In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To... | Rohan Bhambhoria, Lei Chen, Xiaodan Zhu |  |
| 329 |  |  [A Simple Concatenation can Effectively Improve Speech Translation](https://doi.org/10.18653/v1/2023.acl-short.153) |  | 0 | A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to construct the ST model architecture, however, the two modalities are independently fed and trained via different losses. This situation does not match ST’s properties across two modalities and also... | Linlin Zhang, Kai Fan, Boxing Chen, Luo Si |  |
| 330 |  |  [ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning](https://doi.org/10.18653/v1/2023.acl-short.154) |  | 0 | A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We... | Jingyuan Selena She, Christopher Potts, Samuel R. Bowman, Atticus Geiger |  |
| 331 |  |  [Revisiting Automated Prompting: Are We Actually Doing Better?](https://doi.org/10.18653/v1/2023.acl-short.155) |  | 0 | Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different... | Yulin Zhou, Yiren Zhao, Ilia Shumailov, Robert D. Mullins, Yarin Gal |  |
| 332 |  |  [Mind the Gap between the Application Track and the Real World](https://doi.org/10.18653/v1/2023.acl-short.156) |  | 0 | Recent advances in NLP have led to a rise in inter-disciplinary and application-oriented research. While this demonstrates the growing real-world impact of the field, research papers frequently feature experiments that do not account for the complexities of realistic data and environments. To explore the extent of this gap, we investigate the relationship between the real-world motivations described in NLP papers and the models and evaluation which comprise the proposed solution. We first... | Ananya Ganesh, Jie Cao, E. Margaret Perkoff, Rosy Southwell, Martha Palmer, Katharina Kann |  |
| 333 |  |  [How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives](https://doi.org/10.18653/v1/2023.acl-short.157) |  | 0 | Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice... | Xinpeng Wang, Leonie Weissweiler, Hinrich Schütze, Barbara Plank |  |
| 334 |  |  [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-short.158) |  | 0 | Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of... | Anastasiia Sedova, Benjamin Roth |  |
| 335 |  |  [Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-short.159) |  | 0 | Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such... | Hao Cheng, Hao Fang, Xiaodong Liu, Jianfeng Gao |  |
| 336 |  |  [Linear Classifier: An Often-Forgotten Baseline for Text Classification](https://doi.org/10.18653/v1/2023.acl-short.160) |  | 0 | Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text... | YuChen Lin, SiAn Chen, JieJyun Liu, ChihJen Lin |  |
| 337 |  |  [Randomized Positional Encodings Boost Length Generalization of Transformers](https://doi.org/10.18653/v1/2023.acl-short.161) |  | 0 | Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences... | Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi GrauMoya, Róbert Csordás, Mehdi Bennani, Shane Legg, Joel Veness |  |
| 338 |  |  [Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models](https://doi.org/10.18653/v1/2023.acl-short.162) |  | 0 | In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V & L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform... | Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe |  |
| 339 |  |  [Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints](https://doi.org/10.18653/v1/2023.acl-short.163) |  | 0 | Neural QCFG is a grammar-based sequence-to-sequence model with strong inductive biases on hierarchical structures. It excels in interpretability and generalization but suffers from expensive inference. In this paper, we study two low-rank variants of Neural QCFG for faster inference with different trade-offs between efficiency and expressiveness. Furthermore, utilizing the symbolic interface provided by the grammar, we introduce two soft constraints over tree hierarchy and source coverage. We... | Chao Lou, Kewei Tu |  |
| 340 |  |  [TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.164) |  | 0 | Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model’s mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a corresponding benchmark, tense prediction accuracy. With the tense test set and the benchmark, researchers are able to measure the tense consistency performance of machine translation systems for the first... | Yiming Ai, Zhiwei He, Kai Yu, Rui Wang |  |
| 341 |  |  [Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023](https://aclanthology.org/volumes/2023.findings-acl/) |  | 0 |  | Anna Rogers, Jordan L. BoydGraber, Naoaki Okazaki |  |
| 342 |  |  [Frontmatter](https://aclanthology.org/2023.findings-acl.0) |  | 0 |  |  |  |
| 343 |  |  [Investigating Glyph-Phonetic Information for Chinese Spell Checking: What Works and What's Next?](https://doi.org/10.18653/v1/2023.findings-acl.1) |  | 0 | While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability of CSC models to distinguish misspelled characters, with good results at the accuracy level on public datasets. However, the generalization ability of these CSC models has not been well understood: it is unclear whether they... | Xiaotian Zhang, Yanjun Zheng, Hang Yan, Xipeng Qiu |  |
| 344 |  |  [A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions](https://doi.org/10.18653/v1/2023.findings-acl.2) |  | 0 | We investigate the representation of pretrained language models and humans, using the idea of word definition modeling–how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences. We then present a simple method DefBERT that integrates pretrained models with word semantics in dictionaries. We show its benefits on... | Hwiyeol Jo |  |
| 345 |  |  [Conformal Nucleus Sampling](https://doi.org/10.18653/v1/2023.findings-acl.3) |  | 0 | Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-p) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. In this work, we assess whether a top-p set is indeed aligned with its probabilistic meaning in various linguistic contexts.We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired... | Shauli Ravfogel, Yoav Goldberg, Jacob Goldberger |  |
| 346 |  |  [DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.4) |  | 0 | Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation process (Prasad et al., 2008), forming a hierarchy structure. Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives... | Chunkit Chan, Xin Liu, Jiayang Cheng, Zihan Li, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 347 |  |  [Modularized Zero-shot VQA with Pre-trained Models](https://doi.org/10.18653/v1/2023.findings-acl.5) |  | 0 | Large-scale pre-trained models (PTMs) show great zero-shot capabilities. In this paper, we study how to leverage them for zero-shot visual question answering (VQA).Our approach is motivated by a few observations. First, VQA questions often require multiple steps of reasoning, which is still a capability that most PTMs lack. Second, different steps in VQA reasoning chains require different skills such as object detection and relational reasoning, but a single PTM may not possess all these... | Rui Cao, Jing Jiang |  |
| 348 |  |  [TimelineQA: A Benchmark for Question Answering over Timelines](https://doi.org/10.18653/v1/2023.findings-acl.6) |  | 0 | Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of... | WangChiew Tan, Jane DwivediYu, Yuliang Li, Lambert Mathias, Marzieh Saeidi, Jing Nathan Yan, Alon Y. Halevy |  |
| 349 |  |  [Abstractive Text Summarization Using the BRIO Training Paradigm](https://doi.org/10.18653/v1/2023.findings-acl.7) |  | 0 | Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model’s dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the... | Khang Nhut Lam, Thieu Gia Doan, Khang Thua Pham, Jugal Kalita |  |
| 350 |  |  [Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization](https://doi.org/10.18653/v1/2023.findings-acl.8) |  | 0 | Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution... | Ting Wu, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 351 |  |  [Pre-training Language Model as a Multi-perspective Course Learner](https://doi.org/10.18653/v1/2023.findings-acl.9) |  | 0 | ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm... | Beiduo Chen, Shaohan Huang, Zihan Zhang, Wu Guo, Zhenhua Ling, Haizhen Huang, Furu Wei, Weiwei Deng, Qi Zhang |  |
| 352 |  |  [Layerwise universal adversarial attack on NLP models](https://doi.org/10.18653/v1/2023.findings-acl.10) |  | 0 | In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models and three datasets from the GLUE benchmark, we demonstrate that our method provides better transferability in a model-to-model setting with an average gain of 9.3% in the fooling rate over the... | Olga Tsymboi, Danil Malaev, Andrei Petrovskii, Ivan V. Oseledets |  |
| 353 |  |  [Scene-robust Natural Language Video Localization via Learning Domain-invariant Representations](https://doi.org/10.18653/v1/2023.findings-acl.11) |  | 0 | Natural language video localization(NLVL) task involves the semantic matching of a text query with a moment from an untrimmed video. Previous methods primarily focus on improving performance with the assumption of independently identical data distribution while ignoring the out-of-distribution data. Therefore, these approaches often fail when handling the videos and queries in novel scenes, which is inevitable in real-world scenarios. In this paper, we, for the first time, formulate the... | Zehan Wang, Yang Zhao, Haifeng Huang, Yan Xia, Zhou Zhao |  |
| 354 |  |  [Exploiting Pseudo Image Captions for Multimodal Summarization](https://doi.org/10.18653/v1/2023.findings-acl.12) |  | 0 | Multimodal summarization with multimodal output (MSMO) faces a challenging semantic gap between visual and textual modalities due to the lack of reference images for training. Our pilot investigation indicates that image captions, which naturally connect texts and images, can significantly benefit MSMO. However, exposure of image captions during training is inconsistent with MSMO’s task settings, where prior cross-modal alignment information is excluded to guarantee the generalization of... | Chaoya Jiang, Rui Xie, Wei Ye, Jinan Sun, Shikun Zhang |  |
| 355 |  |  [Cross-Lingual Transfer with Target Language-Ready Task Adapters](https://doi.org/10.18653/v1/2023.findings-acl.13) |  | 0 | Adapters have emerged as a modular and parameter-efficient approach to (zero-shot) cross-lingual transfer. The established MAD-X framework employs separate language and task adapters which can be arbitrarily combined to perform the transfer of any task to any target language. Subsequently, BAD-X, an extension of the MAD-X framework, achieves improved transfer at the cost of MAD-X’s modularity by creating ‘bilingual’ adapters specific to the source-target language pair. In this work, we aim to... | Marinela Parovic, Alan Ansell, Ivan Vulic, Anna Korhonen |  |
| 356 |  |  [DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance](https://doi.org/10.18653/v1/2023.findings-acl.14) |  | 0 | Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user’s needs. Further, the topic evolutions produced by DTMs tend to contain generic terms that are not indicative of their designated time steps. To address these issues, we propose the task of discriminative dynamic topic... | Nishant Balepur, Shivam Agarwal, Karthik Venkat Ramanan, Susik Yoon, Diyi Yang, Jiawei Han |  |
| 357 |  |  [Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization](https://doi.org/10.18653/v1/2023.findings-acl.15) |  | 0 | Along with the performance improvement in NLP domain, the sizes of transformer-based language models (TLM) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally... | Chong Yu, Tao Chen, Zhongxue Gan |  |
| 358 |  |  [RMSSinger: Realistic-Music-Score based Singing Voice Synthesis](https://doi.org/10.18653/v1/2023.findings-acl.16) |  | 0 | We are interested in a challenging task, Realistic-Music-Score based Singing Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent singing voice synthesis (SVS) methods are limited to fine-grained music scores, which require a complicated data collection pipeline with time-consuming manual annotation to align music notes with phonemes.... | Jinzheng He, Jinglin Liu, Zhenhui Ye, Rongjie Huang, Chenye Cui, Huadai Liu, Zhou Zhao |  |
| 359 |  |  [Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.17) |  | 0 | The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions. To bridge this gap, this paper proposes a framework for multi-domain dialogue systems. This framework automatically infers implicit intents from user utterances, and prompts a large... | HuiChi Kuo, YunNung Chen |  |
| 360 |  |  [MTGP: Multi-turn Target-oriented Dialogue Guided by Generative Global Path with Flexible Turns](https://doi.org/10.18653/v1/2023.findings-acl.18) |  | 0 | Target-oriented dialogue guides the dialogue to a target quickly and smoothly. The latest approaches focus on global planning, which plans toward the target before the conversation instead of adopting a greedy strategy during the conversation. However, the global plan in existing works is fixed to certain turns by generating paths with certain nodes, which limits the optimization of turns and coherence of the target-oriented process. Toward flexible global planning, we propose to generate a... | Anqi Liu, Bo Wang, Yue Tan, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou |  |
| 361 |  |  [The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python](https://doi.org/10.18653/v1/2023.findings-acl.19) |  | 0 | Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even... | Antonio Valerio Miceli Barone, Fazl Barez, Shay B. Cohen, Ioannis Konstas |  |
| 362 |  |  [Class Lifelong Learning for Intent Detection via Structure Consolidation Networks](https://doi.org/10.18653/v1/2023.findings-acl.20) |  | 0 | Intent detection, which estimates diverse intents behind user utterances, is an essential component of task-oriented dialogue systems. Previous intent detection models are usually trained offline, which can only handle predefined intent classes. In the real world, new intents may keep challenging deployed models. For example, with the prevalence of the COVID-19 pandemic, users may pose various issues related to the pandemic to conversational systems, which brings many new intents. A general... | Qingbin Liu, Yanchao Hao, Xiaolong Liu, Bo Li, Dianbo Sui, Shizhu He, Kang Liu, Jun Zhao, Xi Chen, Ningyu Zhang, Jiaoyan Chen |  |
| 363 |  |  [On Evaluating and Mitigating Gender Biases in Multilingual Settings](https://doi.org/10.18653/v1/2023.findings-acl.21) |  | 0 | While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating... | Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram |  |
| 364 |  |  [Rethinking Round-Trip Translation for Machine Translation Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.22) |  | 0 | Automatic evaluation methods for translation often require model training, and thus the availability of parallel corpora limits their applicability to low-resource settings. Round-trip translation is a potential workaround, which can reframe bilingual evaluation into a much simpler monolingual task. Early results from the era of statistical machine translation (SMT) raised fundamental concerns about the utility of this approach, based on poor correlation with human translation quality... | Terry Yue Zhuo, Qiongkai Xu, Xuanli He, Trevor Cohn |  |
| 365 |  |  [G³R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.findings-acl.23) |  | 0 | We present a framework called G3R for complex and cross-domain Text-to-SQL generation. G3R aims to address two limitations of current approaches: (1) The structure of the abstract syntax tree (AST) is not fully explored during the decoding process which is crucial for complex SQL generation; (2) Domain knowledge is not incorporated to enhance their ability to generalise to unseen domains. G3R consists of a graph-guided SQL generator and a knowledge-enhanced re-ranking mechanism. Firstly, during... | Yanzheng Xiang, QianWen Zhang, Xu Zhang, Zejie Liu, Yunbo Cao, Deyu Zhou |  |
| 366 |  |  [A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks](https://doi.org/10.18653/v1/2023.findings-acl.24) |  | 0 | By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can... | Ruiqing Ding, Xiao Han, Leye Wang |  |
| 367 |  |  [Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.25) |  | 0 | Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence... | Jian Wang, Dongding Lin, Wenjie Li |  |
| 368 |  |  [A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection](https://doi.org/10.18653/v1/2023.findings-acl.26) |  | 0 | Hyperbole and metaphor are common in day-to-day communication (e.g., “I am in deep trouble”: how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor... | Naveen Badathala, Abisek Rajakumar Kalarani, Tejpalsingh Siledar, Pushpak Bhattacharyya |  |
| 369 |  |  [Prompt Tuning for Unified Multimodal Pretrained Models](https://doi.org/10.18653/v1/2023.findings-acl.27) |  | 0 | Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified... | Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou |  |
| 370 |  |  [Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.28) |  | 0 | Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the contextual nature of the TKG structure, i.e., query-specific subgraph contains both structural and temporal neighboring facts. This paper presents the SToKE, a novel method that employs the pre-trained... | Yifu Gao, Yongquan He, Zhigang Kan, Yi Han, Linbo Qiao, Dongsheng Li |  |
| 371 |  |  [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets](https://doi.org/10.18653/v1/2023.findings-acl.29) |  | 0 | The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT’s performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense... | Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang |  |
| 372 |  |  [Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference](https://doi.org/10.18653/v1/2023.findings-acl.30) |  | 0 | This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may not be explicitly mentioned in the text but is used by most humans for problem-shooting. Such complex reasoning with hidden contexts involves deep semantic understanding. Thus, this task has great... | Jianxing Yu, Shiqi Wang, Libin Zheng, Qinliang Su, Wei Liu, Baoquan Zhao, Jian Yin |  |
| 373 |  |  [TADA: Efficient Task-Agnostic Domain Adaptation for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.31) |  | 0 | Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been developed. However, these require additional parameters for each layer, and are criticized for their limited expressiveness. In this work, we introduce TADA, a novel task-agnostic domain adaptation method... | ChiaChien Hung, Lukas Lange, Jannik Strötgen |  |
| 374 |  |  [Robust Natural Language Understanding with Residual Attention Debiasing](https://doi.org/10.18653/v1/2023.findings-acl.32) |  | 0 | Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial... | Fei Wang, James Y. Huang, Tianyi Yan, Wenxuan Zhou, Muhao Chen |  |
| 375 |  |  [MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.33) |  | 0 | Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizes all history information, the dialogue state in the previous turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that... | Haoning Zhang, Junwei Bao, Haipeng Sun, Youzheng Wu, Wenye Li, Shuguang Cui, Xiaodong He |  |
| 376 |  |  [PAL: Persona-Augmented Emotional Support Conversation Generation](https://doi.org/10.18653/v1/2023.findings-acl.34) |  | 0 | Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers’ persona is an important factor for effective support, we investigate whether there are benefits to modeling such information in dialogue models for support. In this paper, our empirical analysis verifies that... | Jiale Cheng, Sahand Sabour, Hao Sun, Zhuang Chen, Minlie Huang |  |
| 377 |  |  [Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model](https://doi.org/10.18653/v1/2023.findings-acl.35) |  | 0 | Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selection (ISS) for language model, which explicitly utilizes end-task knowledge to select a tiny subset of the pretraining corpus. Specifically, the ISS selects the samples that will provide the most... | Xiao Wang, Weikang Zhou, Qi Zhang, Jie Zhou, Songyang Gao, Junzhe Wang, Menghan Zhang, Xiang Gao, Yunwen Chen, Tao Gui |  |
| 378 |  |  [Exclusive Supermask Subnetwork Training for Continual Learning](https://doi.org/10.18653/v1/2023.findings-acl.36) |  | 0 | Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal... | Prateek Yadav, Mohit Bansal |  |
| 379 |  |  [Transferring General Multimodal Pretrained Models to Text Recognition](https://doi.org/10.18653/v1/2023.findings-acl.37) |  | 0 | This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark. Additionally, we construct an OCR pipeline with... | Junyang Lin, Xuancheng Ren, Yichang Zhang, Gao Liu, Peng Wang, An Yang, Chang Zhou |  |
| 380 |  |  [A Formal Perspective on Byte-Pair Encoding](https://doi.org/10.18653/v1/2023.findings-acl.38) |  | 0 | Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a 1/sigma\*(1-e(-sigma))-approximation of an optimal merge sequence, where sigma is the... | Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, Ryan Cotterell |  |
| 381 |  |  [Automatic Named Entity Obfuscation in Speech](https://doi.org/10.18653/v1/2023.findings-acl.39) |  | 0 | Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator. While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech. This work focuses on identifying, replacing and inserting replacement named entities synthesized using voice cloning into original audio thereby retaining prosodic information while reducing the likelihood of deanonymization. The approach... | Judita Preiss |  |
| 382 |  |  [Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models](https://doi.org/10.18653/v1/2023.findings-acl.40) |  | 0 | Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models’ (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called... | Soochan Lee, Gunhee Kim |  |
| 383 |  |  [UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.41) |  | 0 | Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal... | Heqing Zou, Meng Shen, Chen Chen, Yuchen Hu, Deepu Rajan, Eng Siong Chng |  |
| 384 |  |  [Robustness-Aware Word Embedding Improves Certified Robustness to Adversarial Word Substitutions](https://doi.org/10.18653/v1/2023.findings-acl.42) |  | 0 | Natural Language Processing (NLP) models have gained great success on clean texts, but they are known to be vulnerable to adversarial examples typically crafted by synonym substitutions. In this paper, we target to solve this problem and find that word embedding is important to the certified robustness of NLP models. Given the findings, we propose the Embedding Interval Bound Constraint (EIBC) triplet loss to train robustness-aware word embeddings for better certified robustness. We optimize... | Yibin Wang, Yichen Yang, Di He, Kun He |  |
| 385 |  |  [Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-acl.43) |  | 0 | In the context-dependent Text-to-SQL task, the generated SQL statements are refined iteratively based on the user input utterance from each interaction. The input text from each interaction can be viewed as component modifications to the previous SQL statements, which could be further extracted as the modification patterns. Since these modification patterns could also be combined with other SQL statements, the models are supposed to have the compositional generalization to these novel... | Aiwei Liu, Wei Liu, Xuming Hu, Shuang Li, Fukun Ma, Yawen Yang, Lijie Wen |  |
| 386 |  |  [Towards Generative Event Factuality Prediction](https://doi.org/10.18653/v1/2023.findings-acl.44) |  | 0 | We present a novel end-to-end generative task and system for predicting event factuality holders, targets, and their associated factuality values. We perform the first experiments using all sources and targets of factuality statements from the FactBank corpus. We perform multi-task learning with other tasks and event-factuality corpora to improve on the FactBank source and target task. We argue that careful domain specific target text output format in generative systems is important and verify... | John Murzaku, Tyler Osborne, Amittai Aviram, Owen Rambow |  |
| 387 |  |  [Can Language Models Be Specific? How?](https://doi.org/10.18653/v1/2023.findings-acl.45) |  | 0 | “He is a person”, “Paris is located on the earth”. Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given “Toronto is located in [MASK].”, we want to test whether a more specific answer will be better filled... | Jie Huang, Kevin ChenChuan Chang, Jinjun Xiong, WenMei Hwu |  |
| 388 |  |  [The Web Can Be Your Oyster for Improving Language Models](https://doi.org/10.18653/v1/2023.findings-acl.46) |  | 0 | Pretrained language models (PLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of PLMs for knowledge-intensive tasks, we consider augmenting PLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly... | Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang, JianYun Nie, JiRong Wen |  |
| 389 |  |  [Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples](https://doi.org/10.18653/v1/2023.findings-acl.47) |  | 0 | Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent performance in the target language. However, if no labeled data in the target language are available, they need to be created through human annotations. In this study, we devise a metric to select annotation candidates from an unlabeled data pool that efficiently enhance accuracy for few-shot cross-lingual... | Hwichan Kim, Mamoru Komachi |  |
| 390 |  |  [Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning](https://doi.org/10.18653/v1/2023.findings-acl.48) |  | 0 | Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past. In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering... | Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, Daniel PreotiucPietro |  |
| 391 |  |  [UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.49) |  | 0 | Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model’s reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since Contrastive Language-Image Pre-training (CLIP) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong... | Rui Sun, Zhecan Wang, Haoxuan You, Noel Codella, KaiWei Chang, ShihFu Chang |  |
| 392 |  |  [Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors](https://doi.org/10.18653/v1/2023.findings-acl.50) |  | 0 | Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE’s low incidence in... | Kai Zhang, Bernal Jimenez Gutierrez, Yu Su |  |
| 393 |  |  [TADA : Task Agnostic Dialect Adapters for English](https://doi.org/10.18653/v1/2023.findings-acl.51) |  | 0 | Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for... | William Held, Caleb Ziems, Diyi Yang |  |
| 394 |  |  [Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting](https://doi.org/10.18653/v1/2023.findings-acl.52) |  | 0 | Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we... | Xuefeng Li, Liwen Wang, Guanting Dong, Keqing He, Jinzheng Zhao, Hao Lei, Jiachi Liu, Weiran Xu |  |
| 395 |  |  [Re-appraising the Schema Linking for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.53) |  | 0 | Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking: aligning the entity references in the question with the schema columns or tables. This is generally approached via Exact Match based Schema Linking (EMSL) within a neural network-based schema linking module. EMSL has become standard in text-to-SQL: many state-of-the-art models employ... | Yujian Gan, Xinyun Chen, Matthew Purver |  |
| 396 |  |  [Echoes from Alexandria: A Large Resource for Multilingual Book Summarization](https://doi.org/10.18653/v1/2023.findings-acl.54) |  | 0 | In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and availability in English only. To overcome these limitations, we present “Echoes from Alexandria”, or in shortened form, “Echoes”, a large resource for multilingual book summarization. Echoes... | Alessandro Scirè, Simone Conia, Simone Ciciliano, Roberto Navigli |  |
| 397 |  |  [When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario](https://doi.org/10.18653/v1/2023.findings-acl.55) |  | 0 | Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which... | Chengcheng Han, Liqing Cui, Renyu Zhu, Jianing Wang, Nuo Chen, Qiushi Sun, Xiang Li, Ming Gao |  |
| 398 |  |  [Align-then-Enhance: Multilingual Entailment Graph Enhancement with Soft Predicate Alignment](https://doi.org/10.18653/v1/2023.findings-acl.56) |  | 0 | Entailment graphs (EGs) with predicates as nodes and entailment relations as edges are typically incomplete, while EGs in different languages are often complementary to each other. In this paper, we propose a new task, multilingual entailment graph enhancement, which aims to utilize the entailment information from one EG to enhance another EG in a different language. The ultimate goal is to obtain an enhanced EG containing richer and more accurate entailment information. We present an... | Yuting Wu, Yutong Hu, Yansong Feng, Tianyi Li, Mark Steedman, Dongyan Zhao |  |
| 399 |  |  [Few-shot Classification with Hypersphere Modeling of Prototypes](https://doi.org/10.18653/v1/2023.findings-acl.57) |  | 0 | Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However, using embeddings in space lacks expressivity and cannot capture class information robustly, while statistical complex modeling poses difficulty to metric designs. In this work, we use tensor fields... | Ning Ding, Yulin Chen, Ganqu Cui, Xiaobin Wang, Haitao Zheng, Zhiyuan Liu, Pengjun Xie |  |
| 400 |  |  [Structured Mean-Field Variational Inference for Higher-Order Span-Based Semantic Role](https://doi.org/10.18653/v1/2023.findings-acl.58) |  | 0 | In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling. To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2H), predicate-to-tail (P2T), and head-to-tail (H2T), where head/tail means the first/last word of the semantic argument span. As such, we use a CRF-based higher-order dependency parser and leverage... | Wei Liu, Songlin Yang, Kewei Tu |  |
| 401 |  |  [AQE: Argument Quadruplet Extraction via a Quad-Tagging Augmented Generative Approach](https://doi.org/10.18653/v1/2023.findings-acl.59) |  | 0 | Argument mining involves multiple sub-tasks that automatically identify argumentative elements, such as claim detection, evidence extraction, stance classification, etc. However, each subtask alone is insufficient for a thorough understanding of the argumentative structure and reasoning process. To learn a complete view of an argument essay and capture the interdependence among argumentative components, we need to know what opinions people hold (i.e., claims), why those opinions are valid... | Jia Guo, Liying Cheng, Wenxuan Zhang, Stanley Kok, Xin Li, Lidong Bing |  |
| 402 |  |  [The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.60) |  | 0 | Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question... | Sabrina Chiesurin, Dimitris Dimakopoulos, Marco Antonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis Papaioannou, Verena Rieser, Ioannis Konstas |  |
| 403 |  |  [Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker](https://doi.org/10.18653/v1/2023.findings-acl.61) |  | 0 | Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the... | Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Jong C. Park |  |
| 404 |  |  [Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks](https://doi.org/10.18653/v1/2023.findings-acl.62) |  | 0 | Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks over structured knowledge graphs. Specifically, we use soft-prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random-walk paths that lead to the... | Kanishka Misra, Cícero Nogueira dos Santos, Siamak Shakeri |  |
| 405 |  |  [Multimedia Generative Script Learning for Task Planning](https://doi.org/10.18653/v1/2023.findings-acl.63) |  | 0 | Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provides detailed information that is not covered by text and will guide subsequent steps. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking... | Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary, Heng Ji |  |
| 406 |  |  [Label Agnostic Pre-training for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.64) |  | 0 | Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot... | Christopher Clarke, Yuzhao Heng, Yiping Kang, Krisztián Flautner, Lingjia Tang, Jason Mars |  |
| 407 |  |  [Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.65) |  | 0 | It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Leo for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations... | Chujie Zheng, Pei Ke, Zheng Zhang, Minlie Huang |  |
| 408 |  |  [Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information](https://doi.org/10.18653/v1/2023.findings-acl.66) |  | 0 | Keyphrase extraction aims to extract a set of phrases with the central idea of the source document. In a structured document, there are certain locations (e.g., the title or the first sentence) where a keyphrase is most likely to appear. However, when extracting keyphrases from the document, most existing embedding-based unsupervised keyphrase extraction models ignore the indicative role of the highlights in certain locations, leading to wrong keyphrases extraction. In this paper, we propose a... | Mingyang Song, Huafeng Liu, Yi Feng, Liping Jing |  |
| 409 |  |  [Towards Reasoning in Large Language Models: A Survey](https://doi.org/10.18653/v1/2023.findings-acl.67) |  | 0 | Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview... | Jie Huang, Kevin ChenChuan Chang |  |
| 410 |  |  [Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications](https://doi.org/10.18653/v1/2023.findings-acl.68) |  | 0 | Although recent years have been marked by incredible advances in the whole development process of NLP systems, there are still blind spots in characterizing what is still hampering real-world adoption of models in knowledge-intensive settings. In this paper, we illustrate through a real-world zero-shot text search case for information seeking in scientific papers, the masked phenomena that the current process of measuring performance might not reflect, even when benchmarks are, in appearance,... | Chyrine Tahri, Aurore Bochnakian, Patrick Haouat, Xavier Tannier |  |
| 411 |  |  [CLIPText: A New Paradigm for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.69) |  | 0 | While CLIP models are useful for zero-shot vision-and-language (VL) tasks or computer vision tasks, little attention has been paid to the application of CLIP for language tasks. Intuitively, CLIP model have a rich representation pre-trained with natural language supervision, in which we argue that it is useful for language tasks. Hence, this work bridge this gap by investigating a CLIP model for zero-shot text classification. Specifically, we introduce CLIPText, a novel paradigm for zero-shot... | Libo Qin, Weiyun Wang, Qiguang Chen, Wanxiang Che |  |
| 412 |  |  [Rethinking Dictionaries and Glyphs for Chinese Language Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.70) |  | 0 | We introduce CDBert, a new learning paradigm that enhances the semantics understanding ability of the Chinese PLMs with dictionary knowledge and structure of Chinese characters. We name the two core modules of CDBert as Shuowen and Jiezi, where Shuowen refers to the process of retrieving the most appropriate meaning from Chinese dictionaries and Jiezi refers to the process of enhancing characters’ glyph representations with structure understanding. To facilitate dictionary understanding, we... | Yuxuan Wang, Jianghui Wang, Dongyan Zhao, Zilong Zheng |  |
| 413 |  |  [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.71) |  | 0 | We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this... | Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wentau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu |  |
| 414 |  |  [Towards Speech Dialogue Translation Mediating Speakers of Different Languages](https://doi.org/10.18653/v1/2023.findings-acl.72) |  | 0 | We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context. We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our... | Shuichiro Shimizu, Chenhui Chu, Sheng Li, Sadao Kurohashi |  |
| 415 |  |  [Adaptation Approaches for Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2023.findings-acl.73) |  | 0 | Semi-parametric Nearest Neighbor Language Models (kNN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting kNN-LMs — 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation... | Rishabh Bhardwaj, George Polovets, Monica Sunkara |  |
| 416 |  |  [Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.74) |  | 0 | Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the... | Miriam Anschütz, Joshua Oehms, Thomas Wimmer, Bartlomiej Jezierski, Georg Groh |  |
| 417 |  |  [Client-Customized Adaptation for Parameter-Efficient Federated Learning](https://doi.org/10.18653/v1/2023.findings-acl.75) |  | 0 | Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be the use of parameter-efficient fine-tuning (PEFT) in the context of FL. However, we have observed that typical PEFT tends to severely suffer from heterogeneity among clients in FL scenarios,... | Yeachan Kim, Junho Kim, WingLam Mok, JunHyung Park, SangKeun Lee |  |
| 418 |  |  [FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery](https://doi.org/10.18653/v1/2023.findings-acl.76) |  | 0 | Understanding users’ intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans’ minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models (LLMs) and human-in-the-loop... | Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng Li, Yifan Gao, Tianyu Cao, Bing Yin |  |
| 419 |  |  [I am PsyAM: Modeling Happiness with Cognitive Appraisal Dimensions](https://doi.org/10.18653/v1/2023.findings-acl.77) |  | 0 | This paper proposes and evaluates PsyAM (https://anonymous.4open.science/r/BERT-PsyAM-10B9), a framework that incorporates adaptor modules in a sequential multi-task learning setup to generate high-dimensional feature representations of hedonic well-being (momentary happiness) in terms of its psychological underpinnings. PsyAM models emotion in text through its cognitive antecedents through auxiliary models that achieve multi-task learning through novel feature fusion methods. We show that... | Xuan Liu, Kokil Jaidka |  |
| 420 |  |  [Value type: the bridge to a better DST model](https://doi.org/10.18653/v1/2023.findings-acl.78) |  | 0 | Value type of the slots can provide lots of useful information for DST tasks. However, it has been ignored in most previous works. In this paper, we propose a new framework for DST task based on these value types. Firstly, we extract the type of token from each turn. Specifically, we divide the slots in the dataset into 9 categories according to the type of slot value, and then train a Ner model to extract the corresponding type-entity from each turn of conversation according to the token.... | QiXiang Gao, Mingyang Sun, Yutao Mou, Chen Zeng, Weiran Xu |  |
| 421 |  |  [Hypothetical Training for Robust Machine Reading Comprehension of Tabular Context](https://doi.org/10.18653/v1/2023.findings-acl.79) |  | 0 | Machine Reading Comprehension (MRC) models easily learn spurious correlations from complex contexts such as tabular data. Counterfactual training—using the factual and counterfactual data by augmentation—has become a promising solution. However, it is costly to construct faithful counterfactual examples because it is tricky to maintain the consistency and dependency of the tabular data. In this paper, we take a more efficient fashion to ask hypothetical questions like “in which year would the... | Moxin Li, Wenjie Wang, Fuli Feng, Hanwang Zhang, Qifan Wang, TatSeng Chua |  |
| 422 |  |  [BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews](https://doi.org/10.18653/v1/2023.findings-acl.80) |  | 0 | The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into... | Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md. Kamrul Hasan |  |
| 423 |  |  [Risks and NLP Design: A Case Study on Procedural Document QA](https://doi.org/10.18653/v1/2023.findings-acl.81) |  | 0 | As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications. We argue that clearer assessments of risks and harms to users—and concrete strategies to mitigate them—will be possible when we specialize the analysis to more concrete applications and their plausible users. As an illustration, this... | Nikita Haduong, Alice Gao, Noah A. Smith |  |
| 424 |  |  [The Diminishing Returns of Masked Language Models to Science](https://doi.org/10.18653/v1/2023.findings-acl.82) |  | 0 | Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770Mparameter... | Zhi Hong, Aswathy Ajith, J. Gregory Pauloski, Eamon Duede, Kyle Chard, Ian T. Foster |  |
| 425 |  |  [Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies](https://doi.org/10.18653/v1/2023.findings-acl.83) |  | 0 | A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year to the next. However, in these settings the content of the manuscript is a confounding variable—each year has a different distribution of scientific content which may naturally affect the distribution of reviewer scores. To address this textual confounding, we extend variable ratio nearest neighbor matching to... | Raymond Zhang, Neha Nayak Kennard, Daniel Scott Smith, Daniel A. McFarland, Andrew McCallum, Katherine Keith |  |
| 426 |  |  [Learning to Generalize for Cross-domain QA](https://doi.org/10.18653/v1/2023.findings-acl.84) |  | 0 | There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing with fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to... | Yingjie Niu, Linyi Yang, Ruihai Dong, Yue Zhang |  |
| 427 |  |  [Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs](https://doi.org/10.18653/v1/2023.findings-acl.85) |  | 0 | Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel... | Mingyang Zhou, Yi Ren Fung, Long Chen, Christopher Thomas, Heng Ji, ShihFu Chang |  |
| 428 |  |  [Importance of Synthesizing High-quality Data for Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-acl.86) |  | 0 | There has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented synthetic data. We observed three shortcomings: illogical synthetic SQL queries from independent column sampling, arbitrary table joins, and language gaps between the synthesized SQL and natural language... | Yiqun Hu, Yiyun Zhao, Jiarong Jiang, Wuwei Lan, Henghui Zhu, Anuj Chauhan, Alexander Hanbo Li, Lin Pan, Jun Wang, ChungWei Hang, Sheng Zhang, Jiang Guo, Mingwen Dong, Joseph Lilien, Patrick Ng, Zhiguo Wang, Vittorio Castelli, Bing Xiang |  |
| 429 |  |  [Exploring Schema Generalizability of Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.87) |  | 0 | Exploring the generalizability of a text-to-SQL parser is essential for a system to automatically adapt the real-world databases. Previous investigation works mostly focus on lexical diversity, including the influence of the synonym and perturbations in both natural language questions and databases. However, the structural variability of database schema (DS), as a widely seen real-world scenario, is yet underexplored. Specifically, confronted with the same input question, the target SQL may be... | Jieyu Li, Lu Chen, Ruisheng Cao, Su Zhu, Hongshen Xu, Zhi Chen, Hanchong Zhang, Kai Yu |  |
| 430 |  |  [Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer](https://doi.org/10.18653/v1/2023.findings-acl.88) |  | 0 | Cross-lingual natural language inference is a fundamental problem in cross-lingual language understanding. Many recent works have used prompt learning to address the lack of annotated parallel corpora in XNLI.However, these methods adopt discrete prompting by simply translating the templates to the target language and need external expert knowledge to design the templates. Besides, discrete prompts of human-designed template words are not trainable vectors and can not be migrated to target... | Shuang Li, Xuming Hu, Aiwei Liu, Yawen Yang, Fukun Ma, Philip S. Yu, Lijie Wen |  |
| 431 |  |  [A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.89) |  | 0 | Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity of this task. To address this problem, we aim to utilize the original multi-annotator labels directly. Particularly, we propose a CONfidence-based partial Label Learning (CONLL) method to integrate... | Limao Xiong, Jie Zhou, Qunxi Zhu, Xiao Wang, Yuanbin Wu, Qi Zhang, Tao Gui, Xuanjing Huang, Jin Ma, Ying Shan |  |
| 432 |  |  [Towards Zero-Shot Persona Dialogue Generation with In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.90) |  | 0 | Much work has been done to improve persona consistency by finetuning a pretrained dialogue model on high-quality human-annoated persona datasets. However, these methods still face the challenges of high cost and poor scalability. To this end, we propose a simple-yet-effective approach to significantly improve zero-shot persona consistency via in-context learning. Specifically, we first pre-train a persona-augmented dialogue generation model and then utilize in-context prompting mechanism to... | Xinchao Xu, Zeyang Lei, Wenquan Wu, ZhengYu Niu, Hua Wu, Haifeng Wang |  |
| 433 |  |  [Grammar-based Decoding for Improved Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.91) |  | 0 | Sequence-to-sequence (seq2seq) models have achieved great success in semantic parsing tasks, but they tend to struggle on out-of-distribution (OOD) data. Despite recent progress, robust semantic parsing on large-scale tasks with combined challenges from both compositional generalization and natural language variations remains an unsolved problem. To promote research in this area, this work presents CUDON, a large-scale dialogue dataset in Chinese language, particularly designed for evaluating... | Jing Zheng, JyhHerng Chow, Zhongnan Shen, Peng Xu |  |
| 434 |  |  [Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.92) |  | 0 | User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architectureor do not make full use of user/product associations. The contribution of this work is twofold: i) a method to explicitly employ... | Chenyang Lyu, Linyi Yang, Yue Zhang, Yvette Graham, Jennifer Foster |  |
| 435 |  |  [Efficient Out-of-Domain Detection for Sequence to Sequence Models](https://doi.org/10.18653/v1/2023.findings-acl.93) |  | 0 | Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering). However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable. Uncertainty... | Artem Vazhentsev, Akim Tsvigun, Roman Vashurin, Sergey Petrakov, Daniil Vasilev, Maxim Panov, Alexander Panchenko, Artem Shelmanov |  |
| 436 |  |  [Emotion Cause Extraction on Social Media without Human Annotation](https://doi.org/10.18653/v1/2023.findings-acl.94) |  | 0 | In social media, there is a vast amount of information pertaining to people’s emotions and the corresponding causes. The emotion cause extraction (ECE) from social media data is an important research area that has not been thoroughly explored due to the lack of fine-grained annotations. Early studies referred to either unsupervised rule-based methods or supervised machine learning methods using a number of manually annotated data in specific domains. However, the former suffers from limitations... | Debin Xiao, Rui Xia, Jianfei Yu |  |
| 437 |  |  [Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers](https://doi.org/10.18653/v1/2023.findings-acl.95) |  | 0 | For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.A rejection network can be trained with ID and diverse... | Jaeyoung Kim, Kyuheon Jung, Dongbin Na, Sion Jang, Eunbin Park, Sungchul Choi |  |
| 438 |  |  [Adversarial Multi-task Learning for End-to-end Metaphor Detection](https://doi.org/10.18653/v1/2023.findings-acl.96) |  | 0 | Metaphor detection (MD) suffers from limited training data. In this paper, we started with a linguistic rule called Metaphor Identification Procedure and then proposed a novel multi-task learning framework to transfer knowledge in basic sense discrimination (BSD) to MD. BSD is constructed from word sense disambiguation (WSD), which has copious amounts of data. We leverage adversarial training to align the data distributions of MD and BSD in the same feature space, so task-invariant... | Shenglong Zhang, Ying Liu |  |
| 439 |  |  [SERENGETI: Massively Multilingual Language Models for Africa](https://doi.org/10.18653/v1/2023.findings-acl.97) |  | 0 | Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language... | Ife Adebara, AbdelRahim A. Elmadany, Muhammad AbdulMageed, Alcides Alcoba Inciarte |  |
| 440 |  |  [Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring](https://doi.org/10.18653/v1/2023.findings-acl.98) |  | 0 | Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay... | Heejin Do, Yunsu Kim, Gary Geunbae Lee |  |
| 441 |  |  [AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation](https://doi.org/10.18653/v1/2023.findings-acl.99) |  | 0 | Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts... | Chujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng Zhang, Minlie Huang |  |
| 442 |  |  [2\*n is better than n²: Decomposing Event Coreference Resolution into Two Tractable Problems](https://doi.org/10.18653/v1/2023.findings-acl.100) |  | 0 | Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in which they appear. Existing methods for training coreference systems sample from a largely skewed distribution, making it difficult for the algorithm to learn coreference beyond surface matching.... | Shafiuddin Rehan Ahmed, Abhijnan Nath, James H. Martin, Nikhil Krishnaswamy |  |
| 443 |  |  [SCCS: Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment](https://doi.org/10.18653/v1/2023.findings-acl.101) |  | 0 | Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However, existing methods extract features from the whole video and article and use fusion methods to select the representative one, thus usually ignoring the critical structure and varying semantics with... | Jielin Qiu, Jiacheng Zhu, Mengdi Xu, Franck Dernoncourt, Trung Bui, Zhaowen Wang, Bo Li, Ding Zhao, Hailin Jin |  |
| 444 |  |  [General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.102) |  | 0 | Training keyphrase generation (KPG) models require a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG models. We then propose a three-stage pipeline, which gradually guides KPG models’ learning focus from general syntactical features to domain-related semantics, in a data-efficient manner. With... | Rui Meng, Tong Wang, Xingdi Yuan, Yingbo Zhou, Daqing He |  |
| 445 |  |  [E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.103) |  | 0 | Most named entity recognition (NER) systems focus on improving model performance, ignoring the need to quantify model uncertainty, which is critical to the reliability of NER systems in open environments. Evidential deep learning (EDL) has recently been proposed as a promising solution to explicitly model predictive uncertainty for classification tasks. However, directly applying EDL to NER applications faces two challenges, i.e., the problems of sparse entities and OOV/OOD entities in NER... | Zhen Zhang, Mengting Hu, Shiwan Zhao, Minlie Huang, Haotian Wang, Lemao Liu, Zhirui Zhang, Zhe Liu, Bingzhe Wu |  |
| 446 |  |  [LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting](https://doi.org/10.18653/v1/2023.findings-acl.104) |  | 0 | Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCap first retrieves the captions of similar images... | Rita Ramos, Bruno Martins, Desmond Elliott |  |
| 447 |  |  [Boosting Text Augmentation via Hybrid Instance Filtering Framework](https://doi.org/10.18653/v1/2023.findings-acl.105) |  | 0 | Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2% in aspect-based... | Heng Yang, Ke Li |  |
| 448 |  |  [Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction](https://doi.org/10.18653/v1/2023.findings-acl.106) |  | 0 | Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from... | Thong Nguyen, Xiaobao Wu, Xinshuai Dong, CongDuy Nguyen, Zhen Hai, Lidong Bing, Anh Tuan Luu |  |
| 449 |  |  [Extract and Attend: Improving Entity Translation in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.107) |  | 0 | While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we usually first look up in a dictionary and then organize the entity translation together with the translations of other parts to form a smooth target sentence. Inspired by this translation process, we... | Zixin Zeng, Rui Wang, Yichong Leng, Junliang Guo, Shufang Xie, Xu Tan, Tao Qin, TieYan Liu |  |
| 450 |  |  [Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.108) |  | 0 | Compositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We introduce two key modifications to this model which encourage more disentangled representations and improve its compute and memory efficiency, allowing us to tackle compositional generalization in a more... | Hao Zheng, Mirella Lapata |  |
| 451 |  |  [Cross-lingual AMR Aligner: Paying Attention to Cross-Attention](https://doi.org/10.18653/v1/2023.findings-acl.109) |  | 0 | This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm... | Abelardo Carlos Martinez Lorenzo, PereLluís Huguet Cabot, Roberto Navigli |  |
| 452 |  |  [Zero-Shot Text Classification via Self-Supervised Tuning](https://doi.org/10.18653/v1/2023.findings-acl.110) |  | 0 | Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a... | Chaoqun Liu, Wenxuan Zhang, Guizhen Chen, Xiaobao Wu, Anh Tuan Luu, ChipHong Chang, Lidong Bing |  |
| 453 |  |  [Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.111) |  | 0 | Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP models’ performance on many important NLU and NLG tasks. Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language... | Borui Wang, Qiuyuan Huang, Budhaditya Deb, Aaron Halfaker, Liqun Shao, Daniel McDuff, Ahmed Hassan Awadallah, Dragomir Radev, Jianfeng Gao |  |
| 454 |  |  [Large Language Models with Controllable Working Memory](https://doi.org/10.18653/v1/2023.findings-acl.112) |  | 0 | Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining. While many downstream applications provide the model with an informational context to aid its underlying task, how the model’s world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context... | Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, Sanjiv Kumar |  |
| 455 |  |  [A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution](https://doi.org/10.18653/v1/2023.findings-acl.113) |  | 0 | State-of-the-art natural language processing models have been shown to achieve remarkable performance in ‘closed-world’ settings where all the labels in the evaluation set are known at training time. However, in real-world settings, ‘novel’ instances that do not belong to any known class are often observed. This renders the ability to deal with novelties crucial. To initiate a systematic research in this important area of ‘dealing with novelties’, we introduce NoveltyTask, a multi-stage task to... | Neeraj Varshney, Himanshu Gupta, Eric Robertson, Bing Liu, Chitta Baral |  |
| 456 |  |  [CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection](https://doi.org/10.18653/v1/2023.findings-acl.114) |  | 0 | Alzheimer’s Disease (AD) is a neurodegenerative disorder that significantly impacts a patient’s ability to communicate and organize language. Traditional methods for detecting AD, such as physical screening or neurological testing, can be challenging and time-consuming. Recent research has explored the use of deep learning techniques to distinguish AD patients from non-AD patients by analysing the spontaneous speech. These models, however, are limited by the availability of data. To address... | Junwen Duan, Fangyuan Wei, Jin Liu, Hongdong Li, Tianming Liu, Jianxin Wang |  |
| 457 |  |  [Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions](https://doi.org/10.18653/v1/2023.findings-acl.115) |  | 0 | Mining public opinions about vaccines from social media has been increasingly relevant to analyse trends in public debates and to provide quick insights to policy-makers. However, the application of existing models has been hindered by the wide variety of users’ attitudes and the new aspects continuously arising in the public debate. Existing approaches, frequently framed via well-known tasks, such as aspect classification or text span detection, make direct usage of the supervision information... | Lixing Zhu, Runcong Zhao, Gabriele Pergola, Yulan He |  |
| 458 |  |  [Temporal Relation Classification using Boolean Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.116) |  | 0 | Classifying temporal relations between a pair of events is crucial to natural language understanding and a well-known natural language processing task. Given a document and two event mentions, the task is aimed at finding which one started first. We propose an efficient approach for temporal relation classification (TRC) using a boolean question answering (QA) model which we fine-tune on questions that we carefully design based on the TRC annotation guidelines, thereby mimicking the way human... | Omer Cohen, Kfir Bar |  |
| 459 |  |  [Are Synonym Substitution Attacks Really Synonym Substitution Attacks?](https://doi.org/10.18653/v1/2023.findings-acl.117) |  | 0 | In this paper, we explore the following question: Are synonym substitution attacks really synonym substitution attacks (SSAs)?We approach this question by examining how SSAs replace words in the original sentence and show that there are still unresolved obstacles that make current SSAs generate invalid adversarial samples. We reveal that four widely used word substitution methods generate a large fraction of invalid substitution words that are ungrammatical or do not preserve the original... | David ChengHan Chiang, Hungyi Lee |  |
| 460 |  |  [DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection](https://doi.org/10.18653/v1/2023.findings-acl.118) |  | 0 | Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called DIVHSK. It has two components:KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating... | Venkatesh Elangovan, Kaushal Maurya, Deepak Kumar, Maunendra Sankar Desarkar |  |
| 461 |  |  [Similarity-Based Content Scoring - A more Classroom-Suitable Alternative to Instance-Based Scoring?](https://doi.org/10.18653/v1/2023.findings-acl.119) |  | 0 | Automatically scoring student answers is an important task that is usually solved using instance-based supervised learning. Recently, similarity-based scoring has been proposed as an alternative approach yielding similar perfor- mance. It has hypothetical advantages such as a lower need for annotated training data and better zero-shot performance, both of which are properties that would be highly beneficial when applying content scoring in a realistic classroom setting. In this paper we take a... | Marie Bexte, Andrea Horbach, Torsten Zesch |  |
| 462 |  |  [Pragmatic Inference with a CLIP Listener for Contrastive Captioning](https://doi.org/10.18653/v1/2023.findings-acl.120) |  | 0 | We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener... | Jiefu Ou, Benno Krojer, Daniel Fried |  |
| 463 |  |  [A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus](https://doi.org/10.18653/v1/2023.findings-acl.121) |  | 0 | We present a pipeline for a statistical stylometric exploration of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and... | Gideon Yoffe, Axel Bühler, Nachum Dershowitz, Thomas Römer, Eli Piasetzky, Israel Finkelstein, Barak Sober |  |
| 464 |  |  [A Language-First Approach for Procedure Planning](https://doi.org/10.18653/v1/2023.findings-acl.122) |  | 0 | Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. Encouraged by the recent success of language models (LMs) for zero-shot and few-shot planning, we hypothesize that LMs may be equipped with stronger priors for planning compared to their visual counterparts. To this end, we propose a language-first procedure planning... | Jiateng Liu, Sha Li, Zhenhailong Wang, Manling Li, Heng Ji |  |
| 465 |  |  [An Empirical Analysis of Leveraging Knowledge for Low-Resource Task-Oriented Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.123) |  | 0 | Task-oriented semantic parsing has drawn a lot of interest from the NLP community, and especially the voice assistant industry as it enables representing the meaning of user requests with arbitrarily nested semantics, including multiple intents and compound entities. SOTA models are large seq2seq transformers and require hundreds of thousands of annotated examples to be trained. However annotating such data to bootstrap new domains or languages is expensive and error-prone, especially for... | Mayank Kulkarni, Aoxiao Zhong, Nicolas Guenon des Mesnards, Sahar Movaghati, Mukund Sridhar, He Xie, Jianhua Lu |  |
| 466 |  |  [TempLM: Distilling Language Models into Template-Based Generators](https://doi.org/10.18653/v1/2023.findings-acl.124) |  | 0 | While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content. In contrast, classic template-based systems provide strong guarantees of faithfulness at the cost of fluency. We propose TempLM, which achieves the best of both worlds by distilling a PLM into a template-based generator. On the E2E and SynthBio data-to-text datasets, we show that TempLM is more faithful than the original PLM and is more fluent... | Tianyi Zhang, Mina Lee, Xiang Lisa Li, Ende Shen, Tatsunori Hashimoto |  |
| 467 |  |  [Incorporating Graph Information in Transformer-based AMR Parsing](https://doi.org/10.18653/v1/2023.findings-acl.125) |  | 0 | Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly... | Pavlo Vasylenko, PereLluís Huguet Cabot, Abelardo Carlos Martinez Lorenzo, Roberto Navigli |  |
| 468 |  |  [Rethinking the Word-level Quality Estimation for Machine Translation from Human Judgement](https://doi.org/10.18653/v1/2023.findings-acl.126) |  | 0 | Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing effort, where the word labels in the dataset, i.e., OK or BAD, are automatically generated by comparing words between MT sentences and the post-edited sentences through a Translation Error Rate (TER)... | Zhen Yang, Fandong Meng, Yuanmeng Yan, Jie Zhou |  |
| 469 |  |  [PV2TEA: Patching Visual Modality to Textual-Established Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.127) |  | 0 | Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute in- formation extractor. The cross-modality integration faces several unique... | Hejie Cui, Rongmei Lin, Nasser Zalmout, Chenwei Zhang, Jingbo Shang, Carl Yang, Xian Li |  |
| 470 |  |  [Structural Contrastive Pretraining for Cross-Lingual Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.128) |  | 0 | To present, multilingual language models trained using various pre-training tasks like mask language modeling (MLM) have yielded encouraging results on a wide range of downstream tasks. Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current works, leading to the semantic misalignment. In this paper, we propose a new pre-training task named Structural Contrast Pretraining (SCP) to align the structural words in a parallel sentence, enhancing... | Nuo Chen, Linjun Shou, Tengtao Song, Ming Gong, Jian Pei, Jianhui Chang, Daxin Jiang, Jia Li |  |
| 471 |  |  [Reducing Sensitivity on Speaker Names for Text Generation from Dialogues](https://doi.org/10.18653/v1/2023.findings-acl.129) |  | 0 | Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model’s sensitivity on speaker names,... | Qi Jia, Haifeng Tang, Kenny Q. Zhu |  |
| 472 |  |  [Topic and Style-aware Transformer for Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.130) |  | 0 | Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language... | Shuwen Qiu, Nitesh Sekhar, Prateek Singhal |  |
| 473 |  |  [Exploiting Abstract Meaning Representation for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.131) |  | 0 | The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity in surface form expressions can hinder the model’s ability to capture accurate correlations, especially within complex contexts. Therefore, we utilize Abstract Meaning Representation (AMR) graphs to... | Cunxiang Wang, Zhikun Xu, Qipeng Guo, Xiangkun Hu, Xuefeng Bai, Zheng Zhang, Yue Zhang |  |
| 474 |  |  [Nonparametric Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.132) |  | 0 | Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to... | Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 475 |  |  [Pay More Attention to Relation Exploration for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.133) |  | 0 | Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the... | Yong Cao, Xianzhi Li, Huiwen Liu, Wen Dai, Shuai Chen, Bin Wang, Min Chen, Daniel Hershcovich |  |
| 476 |  |  [Speaking Multiple Languages Affects the Moral Bias of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.134) |  | 0 | Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus... | Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindrich Libovický, Constantin A. Rothkopf, Alexander Fraser, Kristian Kersting |  |
| 477 |  |  [Retrieving Relevant Context to Align Representations for Cross-lingual Event Detection](https://doi.org/10.18653/v1/2023.findings-acl.135) |  | 0 | We study the problem of cross-lingual transfer learning for event detection (ED) where models trained on a source language are expected to perform well on data for a new target language. Among a few recent works for this problem, the main approaches involve representation matching (e.g., adversarial training) that aims to eliminate language-specific features from the representations to achieve the language-invariant representations. However, due to the mix of language-specific features with... | Chien Nguyen, Linh Ngo Van, Thien Huu Nguyen |  |
| 478 |  |  [NormNet: Normalize Noun Phrases for More Robust NLP](https://doi.org/10.18653/v1/2023.findings-acl.136) |  | 0 | A critical limitation of deep NLP models is their over-fitting over spurious features. Previous work has proposed several approaches to debunk such features and reduce their impact on the learned models. In this work, a normalization strategy is proposed to eliminate the false features caused by the textual surfaces of noun phrases. The motivation for this strategy is that noun phrases often play the role of slots in textual expressions and their exact forms are often not that important for... | Minlong Peng, Mingming Sun |  |
| 479 |  |  [Cross Encoding as Augmentation: Towards Effective Educational Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.137) |  | 0 | Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag space and 2) it is multi-label. Though a retrieval approach is reportedly good at low-resource scenarios, there have been fewer efforts to directly address the data scarcity problem. To mitigate these... | Hyun Seung Lee, Seungtaek Choi, Yunsung Lee, Hyeongdon Moon, Shinhyeok Oh, Myeongho Jeong, Hyojun Go, Christian Wallraven |  |
| 480 |  |  [Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.138) |  | 0 | State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of... | Venkata Prabhakara Sarath Nookala, Gaurav Verma, Subhabrata Mukherjee, Srijan Kumar |  |
| 481 |  |  [This prompt is measuring \textlessmask\textgreater: evaluating bias evaluation in language models](https://doi.org/10.18653/v1/2023.findings-acl.139) |  | 0 | Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. We analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. By applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that... | Seraphina GoldfarbTarrant, Eddie Ungless, Esma Balkir, Su Lin Blodgett |  |
| 482 |  |  [Towards Open Environment Intent Prediction](https://doi.org/10.18653/v1/2023.findings-acl.140) |  | 0 | Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not further differentiate detected OOD intents in fine granularity. Discovering focuses on how to cluster unlabeled samples according to their semantic representation, which relies heavily on prior... | Yunhua Zhou, Jiawei Hong, Xipeng Qiu |  |
| 483 |  |  [Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.141) |  | 0 | Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for... | Minqian Liu, Lifu Huang |  |
| 484 |  |  [C-XNLI: Croatian Extension of XNLI Dataset](https://doi.org/10.18653/v1/2023.findings-acl.142) |  | 0 | Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Facebook’s 1.2B parameter m2m_100 model. We thoroughly... | Leo Obadic, Andrej Jertec, Marko Rajnovic, Branimir Dropuljic |  |
| 485 |  |  [AVATAR: A Parallel Corpus for Java-Python Program Translation](https://doi.org/10.18653/v1/2023.findings-acl.143) |  | 0 | Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables... | Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, KaiWei Chang |  |
| 486 |  |  [On Dataset Transferability in Active Learning for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.144) |  | 0 | Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled... | Fran Jelenic, Josip Jukic, Nina Drobac, Jan Snajder |  |
| 487 |  |  [Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions](https://doi.org/10.18653/v1/2023.findings-acl.145) |  | 0 | We present an annotation approach for capturing structured components and arguments inlegal case solutions of German students. Based on the appraisal style, which dictates the structured way of persuasive writing in German law, we propose an annotation scheme with annotation guidelines that identify structured writing in legal case solutions. We conducted an annotation study with two annotators and annotated legal case solutions to capture the structures of a persuasive legal text. Based on our... | Florian Weber, Thiemo Wambsganss, Seyed Parsa Neshaei, Matthias Söllner |  |
| 488 |  |  [Characterizing the Impacts of Instances on Robustness](https://doi.org/10.18653/v1/2023.findings-acl.146) |  | 0 | Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with robust patterns carring innate robustness. In this paper, we show that robust and non-robust instances in the training dataset, though are both important for test performance, have contrary impacts on... | Rui Zheng, Zhiheng Xi, Qin Liu, Wenbin Lai, Tao Gui, Qi Zhang, Xuanjing Huang, Jin Ma, Ying Shan, Weifeng Ge |  |
| 489 |  |  [Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge](https://doi.org/10.18653/v1/2023.findings-acl.147) |  | 0 | The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias – the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality –... | Xingyu Fu, Sheng Zhang, Gukyeong Kwon, Pramuditha Perera, Henghui Zhu, Yuhao Zhang, Alexander Hanbo Li, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Dan Roth, Bing Xiang |  |
| 490 |  |  [Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.148) |  | 0 | Syllogistic reasoning, a typical form of deductive reasoning, is a critical capability widely required in natural language understanding tasks, such as text entailment and question answering. To better facilitate research on syllogistic reasoning, we develop a benchmark called SylloBase that differs from existing syllogistic datasets in three aspects: (1) Covering a complete taxonomy of syllogism reasoning patterns; (2) Containing both automatically and manually constructed samples; and (3)... | Yongkang Wu, Meng Han, Yutao Zhu, Lei Li, Xinyu Zhang, Ruofei Lai, Xiaoguang Li, Yuanhang Ren, Zhicheng Dou, Zhao Cao |  |
| 491 |  |  [Categorial grammar induction from raw data](https://doi.org/10.18653/v1/2023.findings-acl.149) |  | 0 | Grammar induction, the task of learning a set of grammatical rules from raw or minimally labeled text data, can provide clues about what kinds of syntactic structures are learnable without prior knowledge. Recent work (e.g., Kim et al., 2019; Zhu et al., 2020; Jin et al., 2021a) has achieved advances in unsupervised induction of probabilistic context-free grammars (PCFGs). However, categorial grammar induction has received less recent attention, despite allowing inducers to support a larger set... | Christian Clark, William Schuler |  |
| 492 |  |  [Attribute Controlled Dialogue Prompting](https://doi.org/10.18653/v1/2023.findings-acl.150) |  | 0 | Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based... | Runcheng Liu, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart |  |
| 493 |  |  [Open-World Factually Consistent Question Generation](https://doi.org/10.18653/v1/2023.findings-acl.151) |  | 0 | Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem of hallucination. This is a critical issue for any natural language application doing question generation. In this work, we propose an effective data processing technique based on de-lexicalization for... | Himanshu Maheshwari, Sumit Shekhar, Apoorv Saxena, Niyati Chhaya |  |
| 494 |  |  [Contrastive Learning of Sociopragmatic Meaning in Social Media](https://doi.org/10.18653/v1/2023.findings-acl.152) |  | 0 | Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data,... | Chiyu Zhang, Muhammad AbdulMageed, Ganesh Jawahar |  |
| 495 |  |  [Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.153) |  | 0 | This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as... | Ruijie Wang, Baoyu Li, Yichen Lu, Dachun Sun, Jinning Li, Yuchen Yan, Shengzhong Liu, Hanghang Tong, Tarek F. Abdelzaher |  |
| 496 |  |  [ACROSS: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.findings-acl.154) |  | 0 | This research addresses the challenges of Cross-Lingual Summarization (CLS) in low-resource scenarios and over imbalanced multilingual data. Existing CLS studies mostly resort to pipeline frameworks or multi-task methods in bilingual settings. However, they ignore the data imbalance in multilingual scenarios and do not utilize the high-resource monolingual summarization data. In this paper, we propose the Aligned CROSs-lingual Summarization (ACROSS) model to tackle these issues. Our framework... | Peiyao Li, Zhengkun Zhang, Jun Wang, Liang Li, Adam Jatowt, Zhenglu Yang |  |
| 497 |  |  [RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.155) |  | 0 | Open-Domain Question Answering (ODQA) systems necessitate a reader model capable of generating answers by simultaneously referring to multiple passages. Although representative models like Fusion-in-Decoder (FiD) have been proposed to address this challenge, these systems can inadvertently rely on spurious features instead of genuine causal relationships between the question and the passages to generate answers. To counter this problem, we introduce the Rational Fusion-in-Decoder (RFiD) model.... | Cunxiang Wang, Haofei Yu, Yue Zhang |  |
| 498 |  |  [Unsupervised Keyphrase Extraction by Learning Neural Keyphrase Set Function](https://doi.org/10.18653/v1/2023.findings-acl.156) |  | 0 | We create a paradigm shift concerning building unsupervised keyphrase extraction systems in this paper. Instead of modeling the relevance between an individual candidate phrase and the document as in the commonly used framework, we formulate the unsupervised keyphrase extraction task as a document-set matching problem from a set-wise perspective, in which the document and the candidate set are globally matched in the semantic space to particularly take into account the interactions among all... | Mingyang Song, Haiyun Jiang, Lemao Liu, Shuming Shi, Liping Jing |  |
| 499 |  |  [Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias](https://doi.org/10.18653/v1/2023.findings-acl.157) |  | 0 | Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. According to the relationship between parameter drifts and Hessians... | Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun |  |
| 500 |  |  [Retrieving Multimodal Prompts for Generative Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.158) |  | 0 | Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model... | Timothy Ossowski, Junjie Hu |  |
| 501 |  |  [InfoSync: Information Synchronization across Multilingual Semi-structured Tables](https://doi.org/10.18653/v1/2023.findings-acl.159) |  | 0 | Information Synchronization of semi-structured data across languages is challenging. For example, Wikipedia tables in one language need to be synchronized with others. To address this problem, we introduce a new dataset InfoSync and a two-step method for tabular synchronization. InfoSync contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages, of which a subset (~3.5K pairs) are manually annotated. The proposed method includes 1) Information Alignment to map rows and 2)... | Siddharth Khincha, Chelsi Jain, Vivek Gupta, Tushar Kataria, Shuo Zhang |  |
| 502 |  |  [T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation](https://doi.org/10.18653/v1/2023.findings-acl.160) |  | 0 | \*Warning: This paper contains several contents that may be toxic, harmful, or offensive.\*In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this... | Jialu Wang, Xinyue Liu, Zonglin Di, Yang Liu, Xin Eric Wang |  |
| 503 |  |  [An Investigation of Evaluation Methods in Automatic Medical Note Generation](https://doi.org/10.18653/v1/2023.findings-acl.161) |  | 0 | Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries,... | Asma Ben Abacha, Wenwai Yim, George Michalopoulos, Thomas Lin |  |
| 504 |  |  [Rethinking Translation Memory Augmented Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.162) |  | 0 | This paper rethinks translation memory augmented neural machine translation (TM-augmented NMT) from two perspectives, i.e., a probabilistic view of retrieval and the variance-bias decomposition principle. The finding demonstrates that TM-augmented NMT is good at the ability of fitting data (i.e., lower bias) but is more sensitive to the fluctuations in the training data (i.e., higher variance), which provides an explanation to a recently reported contradictory phenomenon on the same translation... | Hongkun Hao, Guoping Huang, Lemao Liu, Zhirui Zhang, Shuming Shi, Rui Wang |  |
| 505 |  |  [Controlling Styles in Neural Machine Translation with Activation Prompt](https://doi.org/10.18653/v1/2023.findings-acl.163) |  | 0 | Controlling styles in neural machine translation (NMT) has attracted wide attention, as it is crucial for enhancing user experience. Earlier studies on this topic typically concentrate on regulating the level of formality and achieve some progress in this area. However, they still encounter two major challenges. The first is the difficulty in style evaluation. The style comprises various aspects such as lexis, syntax, and others that provide abundant information. Nevertheless, only formality... | Yifan Wang, Zewei Sun, Shanbo Cheng, Weiguo Zheng, Mingxuan Wang |  |
| 506 |  |  [Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.164) |  | 0 | Few-shot named entity recognition (NER), identifying named entities with a small number of labeled data, has attracted much attention. Frequently, entities are nested within each other. However, most of the existing work on few-shot NER addresses flat entities instead of nested entities. To tackle nested NER in a few-shot setting, it is crucial to utilize the limited labeled data to mine unique features of nested entities, such as the relationship between inner and outer entities and contextual... | Yuanyuan Xu, Zeng Yang, Linhai Zhang, Deyu Zhou, Tiandeng Wu, Rong Zhou |  |
| 507 |  |  [Together We Make Sense-Learning Meta-Sense Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.165) |  | 0 | Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses. For this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning methods. However, not all existing sense embeddings cover all senses of ambiguous words equally well due to the discrepancies in their training resources. To address this problem, we propose the... | Haochen Luo, Yi Zhou, Danushka Bollegala |  |
| 508 |  |  [Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels](https://doi.org/10.18653/v1/2023.findings-acl.166) |  | 0 | Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models. However, for novel products, especially in a different domain, there are few existing labelled data. In this paper, we propose a prompt-based approach, i.e., the Multimodal Prompt Learning framework, to... | Bang Yang, Fenglin Liu, Zheng Li, Qingyu Yin, Chenyu You, Bing Yin, Yuexian Zou |  |
| 509 |  |  [Large Language Models are Built-in Autoregressive Search Engines](https://doi.org/10.18653/v1/2023.findings-acl.167) |  | 0 | Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the... | Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang |  |
| 510 |  |  [Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.168) |  | 0 | Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems focus on better access and use of visual information and tend to validate their methods on image-related datasets. However, these studies face two challenges. First, they can only utilize a limited amount of data that is composed of bilingual texts and images (referred to as “triple data”), which is scarce. Second, current benchmarks... | Yaoming Zhu, Zewei Sun, Shanbo Cheng, Luyang Huang, Liwei Wu, Mingxuan Wang |  |
| 511 |  |  [From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?](https://doi.org/10.18653/v1/2023.findings-acl.169) |  | 0 | Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g.,... | Albert Coil, Vered Shwartz |  |
| 512 |  |  [Measuring Intersectional Biases in Historical Documents](https://doi.org/10.18653/v1/2023.findings-acl.170) |  | 0 | Data-driven analyses of biases in historical texts can help illuminate the origin and development of biases prevailing in modern society. However, digitised historical documents pose a challenge for NLP practitioners as these corpora suffer from errors introduced by optical character recognition (OCR) and are written in an archaic language. In this paper, we investigate the continuities and transformations of bias in historical newspapers published in the Caribbean during the colonial era (18th... | Nadav Borenstein, Karolina Stanczak, Thea Rolskov, Natacha Klein Käfer, Natalia da Silva Perez, Isabelle Augenstein |  |
| 513 |  |  [Incomplete Utterance Rewriting by A Two-Phase Locate-and-Fill Regime](https://doi.org/10.18653/v1/2023.findings-acl.171) |  | 0 | Rewriting incomplete and ambiguous utterances can improve dialogue models’ understanding of the context and help them generate better results. However, the existing end-to-end models will have the problem of too large search space, resulting in poor quality of rewriting results. We propose a 2-phase rewriting framework which first predicts the empty slots in the utterance that need to be completed, and then generate the part to be filled into each positions. Our framework is simple to... | Zitong Li, Jiawei Li, Haifeng Tang, Kenny Q. Zhu, Ruolan Yang |  |
| 514 |  |  [Exploring Variation of Results from Different Experimental Conditions](https://doi.org/10.18653/v1/2023.findings-acl.172) |  | 0 | It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP groups to run the training and testing pipeline for three neural text simplification models under varying experimental conditions, including different random seeds, run-time environments, and dependency... | Maja Popovic, Mohammad Arvan, Natalie Parde, Anya Belz |  |
| 515 |  |  [Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2023.findings-acl.173) |  | 0 | Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a... | Nicolás Benjamín Ocampo, Elena Cabrio, Serena Villata |  |
| 516 |  |  [X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents](https://doi.org/10.18653/v1/2023.findings-acl.174) |  | 0 | Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. To reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language.X-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language,... | Mehrad Moradshahi, Tianhao Shen, Kalika Bali, Monojit Choudhury, Gaël de Chalendar, Anmol Goel, Sungkyun Kim, Prashant Kodali, Ponnurangam Kumaraguru, Nasredine Semmar, Sina J. Semnani, Jiwon Seo, Vivek Seshadri, Manish Shrivastava, Michael Sun, Aditya Yadavalli, Chaobin You, Deyi Xiong, Monica S. Lam |  |
| 517 |  |  [Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation](https://doi.org/10.18653/v1/2023.findings-acl.175) |  | 0 | Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to... | Francois Meyer, Jan Buys |  |
| 518 |  |  [Measuring and Mitigating Local Instability in Deep Neural Networks](https://doi.org/10.18653/v1/2023.findings-acl.176) |  | 0 | Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as... | Arghya Datta, Subhrangshu Nandi, Jingcheng Xu, Greg Ver Steeg, He Xie, Anoop Kumar, Aram Galstyan |  |
| 519 |  |  [What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.177) |  | 0 | kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single... | Wenhao Zhu, Shujian Huang, Yunzhe Lv, Xin Zheng, Jiajun Chen |  |
| 520 |  |  [Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-acl.178) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is manually annotated to better fit real-world scenarios by providing more diverse and realistic reviews for the task. The dataset includes various lengths, diverse expressions, more aspect types, and more... | Ting Xu, Huiyun Yang, Zhen Wu, Jiaze Chen, Fei Zhao, Xinyu Dai |  |
| 521 |  |  [Grounding the Lexical Substitution Task in Entailment](https://doi.org/10.18653/v1/2023.findings-acl.179) |  | 0 | Existing definitions of lexical substitutes are often vague or inconsistent with the gold annotations. We propose a new definition which is grounded in the relation of entailment; namely, that the sentence that results from the substitution should be in the relation of mutual entailment with the original sentence. We argue that the new definition is well-founded and supported by previous work on lexical entailment. We empirically validate our definition by verifying that it covers the majority... | Talgat Omarov, Grzegorz Kondrak |  |
| 522 |  |  [Operator Selection and Ordering in a Pipeline Approach to Efficiency Optimizations for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.180) |  | 0 | There exists a wide variety of efficiency methods for natural language processing (NLP) tasks, such as pruning, distillation, dynamic inference, quantization, etc. From a different perspective, we can consider an efficiency method as an operator applied on a model. Naturally, we may construct a pipeline of operators, i.e., to apply multiple efficiency methods on the model sequentially. In this paper, we study the plausibility of this idea, and more importantly, the commutativity and... | Ji Xin, Raphael Tang, Zhiying Jiang, Yaoliang Yu, Jimmy Lin |  |
| 523 |  |  [AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.181) |  | 0 | Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing... | Asaad Alghamdi, Xinyu Duan, Wei Jiang, Zhenhai Wang, Yimeng Wu, Qingrong Xia, Zhefeng Wang, Yi Zheng, Mehdi Rezagholizadeh, Baoxing Huai, Peilun Cheng, Abbas Ghaddar |  |
| 524 |  |  [Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction](https://doi.org/10.18653/v1/2023.findings-acl.182) |  | 0 | Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as... | Julia White, Arushi Raghuvanshi, Yada Pruksachatkun |  |
| 525 |  |  [Quantifying Train-Evaluation Overlap with Nearest Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.183) |  | 0 | Characterizing benchmark datasets is crucial to interpreting model performance. In this work, we study train-evaluation overlap as a measure of an individual dataset’s adequacy to evaluate model generalization over a wide range of datasets. We quantify the overlap with a simple novel metric based on a nearest neighbors approach between the training and evaluation sets. We identify nearest training examples for each evaluation example by mapping instances with generic and task-specific embedding... | Gauri Kambhatla, Thuy Nguyen, Eunsol Choi |  |
| 526 |  |  [Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels](https://doi.org/10.18653/v1/2023.findings-acl.184) |  | 0 | Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more... | Aviv Weinstein, Yoav Goldberg |  |
| 527 |  |  [The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges](https://doi.org/10.18653/v1/2023.findings-acl.185) |  | 0 | Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the... | Genta Indra Winata, Alham Fikri Aji, Zheng Xin Yong, Thamar Solorio |  |
| 528 |  |  [Learning to Predict Persona Information for Dialogue Personalization without Explicit Persona Description](https://doi.org/10.18653/v1/2023.findings-acl.186) |  | 0 | Personalizing dialogue agents is important for dialogue systems to generate more specific,consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on the dialogue history to personalize the dialogue agent without relying on any explicit persona descriptions during... | Wangchunshu Zhou, Qifei Li, Chenle Li |  |
| 529 |  |  [Automated Refugee Case Analysis: A NLP Pipeline for Supporting Legal Practitioners](https://doi.org/10.18653/v1/2023.findings-acl.187) |  | 0 | In this paper, we introduce an end-to-end pipeline for retrieving, processing, and extracting targeted information from legal cases. We investigate an under-studied legal domain with a case study on refugee law Canada. Searching case law for past similar cases is a key part of legal work for both lawyers and judges, the potential end-users of our prototype. While traditional named-entity recognition labels such as dates are meaningful information in law, we propose to extend existing models and... | Claire Barale, Michael Rovatsos, Nehal Bhuta |  |
| 530 |  |  [Recurrent Attention Networks for Long-text Modeling](https://doi.org/10.18653/v1/2023.findings-acl.188) |  | 0 | Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising... | Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li |  |
| 531 |  |  [Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers](https://doi.org/10.18653/v1/2023.findings-acl.189) |  | 0 | Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for... | Félix Gaschi, Patricio Cerda, Parisa Rastin, Yannick Toussaint |  |
| 532 |  |  [Aerial Vision-and-Dialog Navigation](https://doi.org/10.18653/v1/2023.findings-acl.190) |  | 0 | The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people’s burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous... | Yue Fan, Winson Chen, Tongzhou Jiang, Chun Zhou, Yi Zhang, Xin Wang |  |
| 533 |  |  [Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming](https://doi.org/10.18653/v1/2023.findings-acl.191) |  | 0 | Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning... | Hanlin Zhang, Jiani Huang, Ziyang Li, Mayur Naik, Eric P. Xing |  |
| 534 |  |  [B2T Connection: Serving Stability and Performance in Deep Transformers](https://doi.org/10.18653/v1/2023.findings-acl.192) |  | 0 | In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.Recent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, resulting in useless models. However, in contrast, Post-LN has also consistently achieved better performance than Pre-LN in relatively shallow Transformers, e.g., six or fewer layers. This study first... | Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki |  |
| 535 |  |  [Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data](https://doi.org/10.18653/v1/2023.findings-acl.193) |  | 0 | Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with... | Robert Litschko, Ekaterina Artemova, Barbara Plank |  |
| 536 |  |  [Domain-specific Attention with Distributional Signatures for Multi-Domain End-to-end Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.194) |  | 0 | The end-to-end task-oriented dialogue system has achieved great success in recent years. Most of these dialogue systems need to accommodate multi-domain dialogue in real-world scenarios. However, due to the high cost of dialogue data annotation and the scarcity of labeled dialogue data, existing methods are difficult to extend to new domains. Therefore, it is important to use limited data to construct multi-domain dialogue systems. To solve this problem, we propose a novel domain attention... | Xing Ma, Peng Zhang, Feifei Zhao |  |
| 537 |  |  [CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.195) |  | 0 | Distilling knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation. However, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from machine translation. In this paper, we propose a comprehensive knowledge distillation framework for speech translation, CKDST, which is capable of comprehensively and effectively distilling knowledge... | Yikun Lei, Zhengshan Xue, Xiaohu Zhao, Haoran Sun, Shaolin Zhu, Xiaodong Lin, Deyi Xiong |  |
| 538 |  |  [Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance](https://doi.org/10.18653/v1/2023.findings-acl.196) |  | 0 | Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one is actually better (i.e. ranked higher) than the other. What has made this difficult in the past is that each system must itself be evaluated by comparison to a gold standard. We define a null... | Shira Wein, Christopher Homan, Lora Aroyo, Chris Welty |  |
| 539 |  |  [Parallel Data Helps Neural Entity Coreference Resolution](https://doi.org/10.18653/v1/2023.findings-acl.197) |  | 0 | Coreference resolution is the task of finding expressions that refer to the same entity in a text. Coreference models are generally trained on monolingual annotated data but annotating coreference is expensive and challenging. Hardmeier et al. (2013) have shown that parallel data contains latent anaphoric knowledge, but it has not been explored in end-to-end neural models yet. In this paper, we propose a simple yet effective model to exploit coreference knowledge from parallel data. In addition... | Gongbo Tang, Christian Hardmeier |  |
| 540 |  |  [Towards Open-Domain Twitter User Profile Inference](https://doi.org/10.18653/v1/2023.findings-acl.198) |  | 0 | Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users’ privacy. Therefore, it is important for researchers to determine the extent of profiling in a safe environment to facilitate proper use and make the public aware of the potential risks. Contrary to existing approaches on limited attributes, we explore open-domain... | Haoyang Wen, Zhenxin Xiao, Eduard H. Hovy, Alexander G. Hauptmann |  |
| 541 |  |  [Eliciting Affective Events from Language Models by Multiple View Co-prompting](https://doi.org/10.18653/v1/2023.findings-acl.199) |  | 0 | Prior research on affective event classification showed that exploiting weakly labeled data for training can improve model performance. In this work, we propose a simpler and more effective approach for generating training data by automatically acquiring and labeling affective events with Multiple View Co-prompting, which leverages two language model prompts that provide independent views of an event. The approach starts with a modest amount of gold data and prompts pre-trained language models... | Yuan Zhuang, Ellen Riloff |  |
| 542 |  |  [ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.200) |  | 0 | Many text classification tasks require handling unseen domains with plenty of unlabeled data, thus giving rise to the self-adaption or the so-called transductive zero-shot learning (TZSL) problem. However, current methods based solely on encoders or decoders overlook the possibility that these two modules may promote each other. As a first effort to bridge this gap, we propose an autoencoder named ZeroAE. Specifically, the text is encoded with two separate BERT-based encoders into two... | Kaihao Guo, Hang Yu, Cong Liao, Jianguo Li, Haipeng Zhang |  |
| 543 |  |  [PRAM: An End-to-end Prototype-based Representation Alignment Model for Zero-resource Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.201) |  | 0 | Zero-resource cross-lingual named entity recognition (ZRCL-NER) aims to leverage rich labeled source language data to address the NER problem in the zero-resource target language. Existing methods are built either based on data transfer or representation transfer. However, the former usually leads to additional computation costs, and the latter lacks explicit optimization specific to the NER task. To overcome the above limitations, we propose a novel prototype-based representation alignment... | Yucheng Huang, Wenqiang Liu, Xianli Zhang, Jun Lang, Tieliang Gong, Chen Li |  |
| 544 |  |  [It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance](https://doi.org/10.18653/v1/2023.findings-acl.202) |  | 0 | Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant... | Arjun Subramonian, Xingdi Yuan, Hal Daumé III, Su Lin Blodgett |  |
| 545 |  |  [Task-adaptive Label Dependency Transfer for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.203) |  | 0 | Named Entity Recognition (NER), as a crucial subtask in natural language processing (NLP), suffers from limited labeled samples (a.k.a. few-shot). Meta-learning methods are widely used for few-shot NER, but these existing methods overlook the importance of label dependency for NER, resulting in suboptimal performance. However, applying meta-learning methods to label dependency learning faces a special challenge, that is, due to the discrepancy of label sets in different domains, the label... | Shan Zhang, Bin Cao, Tianming Zhang, Yuqi Liu, Jing Fan |  |
| 546 |  |  [WYWEB: A NLP Evaluation Benchmark For Classical Chinese](https://doi.org/10.18653/v1/2023.findings-acl.204) |  | 0 | To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as "wen yan wen (文言文)", which has a rich history spanning thousands of years and... | Bo Zhou, Qianglong Chen, Xiaomi Zhong, Yin Zhang |  |
| 547 |  |  [A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment](https://doi.org/10.18653/v1/2023.findings-acl.205) |  | 0 | Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint... | Jianheng Tang, Kangfei Zhao, Jia Li |  |
| 548 |  |  [Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-acl.206) |  | 0 | Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is gradient-based search, which iteratively updates a (randomly) initialized prompt towards the optimal one with the guide of gradients. We propose a novel regularization method, CoRe, for gradient-based... | Hyeonmin Ha, Soyoung Jung, Jinsol Park, Minjoon Seo, Seungwon Hwang, ByungGon Chun |  |
| 549 |  |  [An Investigation of Noise in Morphological Inflection](https://doi.org/10.18653/v1/2023.findings-acl.207) |  | 0 | With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of... | Adam Wiemerslage, Changbing Yang, Garrett Nicolai, Miikka Silfverberg, Katharina Kann |  |
| 550 |  |  [Graph Reasoning for Question Answering with Triplet Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.208) |  | 0 | Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for... | Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li, Xifeng Yan, Chao Zhang, Bing Yin |  |
| 551 |  |  [End-to-End Argument Mining over Varying Rhetorical Structures](https://doi.org/10.18653/v1/2023.findings-acl.209) |  | 0 | Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically similar texts with varying rhetorical structures. In this work, the differences between paraphrases within the same argument scheme are evaluated from a rhetorical perspective. The study proposes a deep... | Elena Chistova |  |
| 552 |  |  [Unsupervised Task Graph Generation from Instructional Video Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.210) |  | 0 | This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned... | Lajanugen Logeswaran, Sungryull Sohn, Yunseok Jang, Moontae Lee, Honglak Lee |  |
| 553 |  |  [Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.211) |  | 0 | Chinese Named Entity Recognition (CNER) is a widely used technology in various applications. While recent studies have focused on utilizing additional information of the Chinese language and characters to enhance CNER performance, this paper focuses on a specific aspect of CNER known as fine-grained CNER (FG-CNER). FG-CNER involves the use of hierarchical, fine-grained categories (e.g. Person-MovieStar) to label named entities. To promote research in this area, we introduce the FiNE dataset, a... | Jiuding Yang, Jinwen Luo, Weidong Guo, Di Niu, Yu Xu |  |
| 554 |  |  [Adversarial Textual Robustness on Visual Dialog](https://doi.org/10.18653/v1/2023.findings-acl.212) |  | 0 | Adversarial robustness evaluates the worst-case performance scenario of a machine learning model to ensure its safety and reliability. For example, cases where the user input contains a minimal change, e.g. a synonym, which causes the previously correct model to return a wrong answer. Using this scenario, this study is the first to investigate the robustness of visually grounded dialog models towards textual attacks. We first aim to understand how multimodal input components contribute to model... | Lu Yu, Verena Rieser |  |
| 555 |  |  [Language Model Analysis for Ontology Subsumption Inference](https://doi.org/10.18653/v1/2023.findings-acl.213) |  | 0 | Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM’s knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We... | Yuan He, Jiaoyan Chen, Ernesto JiménezRuiz, Hang Dong, Ian Horrocks |  |
| 556 |  |  [Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.214) |  | 0 | Previous research has demonstrated that natural language explanations provide valuable inductive biases that guide models, thereby improving the generalization ability and data efficiency. In this paper, we undertake a systematic examination of the effectiveness of these explanations. Remarkably, we find that corrupted explanations with diminished inductive biases can achieve competitive or superior performance compared to the original explanations. Our findings furnish novel insights into the... | Wanyun Cui, Xingran Chen |  |
| 557 |  |  [Varta: A Large-Scale Headline-Generation Dataset for Indic Languages](https://doi.org/10.18653/v1/2023.findings-acl.215) |  | 0 | We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our knowledge, this is the largest collection of curated news articles for Indic languages currently available. We use the collected data in a series of experiments to answer important questions related to... | Rahul Aralikatte, Ziling Cheng, Sumanth Doddapaneni, Jackie Chi Kit Cheung |  |
| 558 |  |  [Better Zero-Shot Reasoning with Self-Adaptive Prompting](https://doi.org/10.18653/v1/2023.findings-acl.216) |  | 0 | Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities – they can effectively learn from a handful of handcrafted, completed responses (“in-context examples”), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting... | Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan Ö. Arik, Tomas Pfister |  |
| 559 |  |  [Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark](https://doi.org/10.18653/v1/2023.findings-acl.217) |  | 0 | Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with Subjective Preference), which contains 12K shopping dialogs in complex store scenes. The data is built in two phases with human annotations to ensure quality and diversity. SURE is well-annotated with subjective preferences and... | Yuxing Long, Binyuan Hui, Caixia Yuan, Fei Huang, Yongbin Li, Xiaojie Wang |  |
| 560 |  |  [ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.218) |  | 0 | Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to... | Thilini Wijesiriwardene, Ruwan Wickramarachchi, Bimal G. Gajera, Shreeyash Mukul Gowaikar, Chandan Gupta, Aman Chadha, Aishwarya Naresh Reganti, Amit P. Sheth, Amitava Das |  |
| 561 |  |  [Financial Numeric Extreme Labelling: A dataset and benchmarking](https://doi.org/10.18653/v1/2023.findings-acl.219) |  | 0 | The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral span in a sentence from an extremely large label set. Towards this task, we release a dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794 labels. We benchmark the performance of... | Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan Goyal, Niloy Ganguly |  |
| 562 |  |  [Multilingual Summarization with Factual Consistency Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.220) |  | 0 | Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on... | Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, Mirella Lapata |  |
| 563 |  |  [Enhancing Out-of-Vocabulary Estimation with Subword Attention](https://doi.org/10.18653/v1/2023.findings-acl.221) |  | 0 | Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words. However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words. As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words. However, while most of these... | Raj Patel, Carlotta Domeniconi |  |
| 564 |  |  [Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT](https://doi.org/10.18653/v1/2023.findings-acl.222) |  | 0 | Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs. In this paper, we propose a new monolingual PLM-sponsored NMT model to let both... | Sufeng Duan, Hai Zhao |  |
| 565 |  |  [TransGEC: Improving Grammatical Error Correction with Translationese](https://doi.org/10.18653/v1/2023.findings-acl.223) |  | 0 | Data augmentation is an effective way to improve model performance of grammatical error correction (GEC). This paper identifies a critical side-effect of GEC data augmentation, which is due to the style discrepancy between the data used in GEC tasks (i.e., texts produced by non-native speakers) and data augmentation (i.e., native texts). To alleviate this issue, we propose to use an alternative data source, translationese (i.e., human-translated texts), as input for GEC data augmentation, which... | Tao Fang, Xuebo Liu, Derek F. Wong, Runzhe Zhan, Liang Ding, Lidia S. Chao, Dacheng Tao, Min Zhang |  |
| 566 |  |  [NewsDialogues: Towards Proactive News Grounded Conversation](https://doi.org/10.18653/v1/2023.findings-acl.224) |  | 0 | Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of... | Siheng Li, Yichun Yin, Cheng Yang, Wangjie Jiang, Yiwei Li, Zesen Cheng, Lifeng Shang, Xin Jiang, Qun Liu, Yujiu Yang |  |
| 567 |  |  [Task-aware Retrieval with Instructions](https://doi.org/10.18653/v1/2023.findings-acl.225) |  | 0 | We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task... | Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, Wentau Yih |  |
| 568 |  |  [Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in NLP](https://doi.org/10.18653/v1/2023.findings-acl.226) |  | 0 | Human evaluation is widely regarded as the litmus test of quality in NLP. A basic requirementof all evaluations, but in particular where they are used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually never queried, let alone formally tested, in NLP which means that their repeatability and the reproducibility of their results is currently an open question. This focused contribution reports our review... | Anya Belz, Craig Thomson, Ehud Reiter, Simon Mille |  |
| 569 |  |  [Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models](https://doi.org/10.18653/v1/2023.findings-acl.227) |  | 0 | Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models... | Lingjun Zhao, Khanh Nguyen, Hal Daumé III |  |
| 570 |  |  [Robustness of Multi-Source MT to Transcription Errors](https://doi.org/10.18653/v1/2023.findings-acl.228) |  | 0 | Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation quality if the sources complement one another in terms of correct information they contain. To this end, we first show that on a 10-hour ESIC corpus, the ASR errors in the original English speech... | Dominik Machácek, Peter Polak, Ondrej Bojar, Raj Dabre |  |
| 571 |  |  [Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings](https://doi.org/10.18653/v1/2023.findings-acl.229) |  | 0 | As the number of vulnerabilities increases day by day, security management requires more and more structured data. In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques. Vulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications. Accurate VDM is... | Xin Liu, Yuan Tan, Zhenghang Xiao, Jianwei Zhuge, Rui Zhou |  |
| 572 |  |  [Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.230) |  | 0 | Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support. These systems are typically constructed for a single domain or language and do not generalise well beyond this. To support work on Natural Language Understanding (NLU) in ToD across multiple languages and domains simultaneously, we constructed Multi3NLU++, a multilingual, multi-intent, multi-domain dataset. Multi3NLU++ extends the English-only NLU++ dataset to... | Nikita Moghe, Evgeniia Razumovskaia, Liane Guillou, Ivan Vulic, Anna Korhonen, Alexandra Birch |  |
| 573 |  |  [A Robust Information-Masking Approach for Domain Counterfactual Generation](https://doi.org/10.18653/v1/2023.findings-acl.231) |  | 0 | Domain shift is a big challenge in NLP. Many approaches, thus, resort to learning domain-invariant features to mitigate the hurdles of domain shift during inference. Such methods, however, inexorably fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation has recently been proposed that aims to transform a text from the source domain to a given target domain. To achieve this, the existing method uses a frequency-based... | Pengfei Hong, Rishabh Bhardwaj, Navonil Majumder, Somak Aditya, Soujanya Poria |  |
| 574 |  |  [Misleading Relation Classifiers by Substituting Words in Texts](https://doi.org/10.18653/v1/2023.findings-acl.232) |  | 0 | Relation classification is to determine the semantic relationship between two entities in a given sentence. However, many relation classifiers are vulnerable to adversarial attacks, which is using adversarial examples to lead victim models to output wrong results. In this paper, we propose a simple but effective method for misleading relation classifiers. We first analyze the most important parts of speech (POSs) from the syntax and morphology perspectives, then we substitute words labeled with... | Tian Jiang, Yunqi Liu, Yan Feng, Yuqing Li, Xiaohui Cui |  |
| 575 |  |  [Automatic Table Union Search with Tabular Representation Learning](https://doi.org/10.18653/v1/2023.findings-acl.233) |  | 0 | Given a data lake of tabular data as well as a query table, how can we retrieve all the tables in the data lake that can be unioned with the query table? Table union search constitutes an essential task in data discovery and preparation as it enables data scientists to navigate massive open data repositories. Existing methods identify uniability based on column representations (word surface forms or token embeddings) and column relation represented by column representation similarity. However,... | Xuming Hu, Shen Wang, Xiao Qin, Chuan Lei, Zhengyuan Shen, Christos Faloutsos, Asterios Katsifodimos, George Karypis, Lijie Wen, Philip S. Yu |  |
| 576 |  |  [Bidirectional Transformer Reranker for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.234) |  | 0 | Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability of each candidate sentence generated by the pre-trained seq2seq model. The BTR preserves the seq2seq-style Transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to... | Ying Zhang, Hidetaka Kamigaito, Manabu Okumura |  |
| 577 |  |  [Not Enough Data to Pre-train Your Language Model? MT to the Rescue!](https://doi.org/10.18653/v1/2023.findings-acl.235) |  | 0 | In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for pre-training LMs. We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the... | Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi, Ander Corral |  |
| 578 |  |  [UMSE: Unified Multi-scenario Summarization Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.236) |  | 0 | Summarization quality evaluation is a non-trivial task in text summarization. Contemporary methods can be mainly categorized into two scenarios: (1) reference-based: evaluating with human-labeled reference summary; (2) reference-free: evaluating the summary consistency of the document. Recent studies mainly focus on one of these scenarios and explore training neural models built on PLMs to align with human criteria. However, the models from different scenarios are optimized individually, which... | Shen Gao, Zhitao Yao, Chongyang Tao, Xiuying Chen, Pengjie Ren, Zhaochun Ren, Zhumin Chen |  |
| 579 |  |  [Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.237) |  | 0 | Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples. The stealthiness of backdoor attacks is commonly attained through minimal cross-entropy loss fine-tuning on a union of poisoned and clean samples. Existing defense paradigms provide a workaround by detecting and removing poisoned samples at pre-training or inference time. On the contrary, we provide a new... | Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, Weiping Wang |  |
| 580 |  |  [Improving Named Entity Recognition via Bridge-based Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.238) |  | 0 | Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification. To this end, we attempt to utilize contrastive learning to refine the original representations and propose a model-agnostic... | Jingyun Xu, Changmeng Zheng, Yi Cai, TatSeng Chua |  |
| 581 |  |  [SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.239) |  | 0 | Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete one or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise... | Shuzheng Si, Zefan Cai, Shuang Zeng, Guoqiang Feng, Jiaxing Lin, Baobao Chang |  |
| 582 |  |  [The State of Profanity Obfuscation in Natural Language Processing Scientific Publications](https://doi.org/10.18653/v1/2023.findings-acl.240) |  | 0 | Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers and increases its internet frequency. While maintaining publications’ professional appearance, obfuscating profanities makes it challenging to evaluate the content, especially for non-native... | Debora Nozza, Dirk Hovy |  |
| 583 |  |  [Teacher and Student Models of Offensive Language in Social Media](https://doi.org/10.18653/v1/2023.findings-acl.241) |  | 0 | State-of-the-art approaches to identifying offensive language online make use of large pre-trained transformer models. However, the inference time, disk, and memory requirements of these transformer models present challenges for their wide usage in the real world. Even the distilled transformer models remain prohibitively large for many usage scenarios. To cope with these challenges, in this paper, we propose transferring knowledge from transformer models to much smaller neural models to make... | Tharindu Ranasinghe, Marcos Zampieri |  |
| 584 |  |  [A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-acl.242) |  | 0 | Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen. Second, in order to adapt to different domains, re-designing prompt templates is usually required, which is time-consuming and inefficient. To remedy... | Yang Zhao, Tetsuya Nasukawa, Masayasu Muraoka, Bishwaranjan Bhattacharjee |  |
| 585 |  |  [Balancing the Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer](https://doi.org/10.18653/v1/2023.findings-acl.243) |  | 0 | Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the considered stylistic attributes, adding to the challenges of training a style transfer model. This paper explores the impact of training data input diversity on the quality of the generated text from... | Debarati Das, David Ma, Dongyeop Kang |  |
| 586 |  |  [A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches](https://doi.org/10.18653/v1/2023.findings-acl.244) |  | 0 | Extremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1) training classifiers based on pseudo-labels generated by (softly) matching seed words (Seed) and (2) prompting (and calibrating) language models using classification instruction (and raw texts) to... | Zihan Wang, Tianle Wang, Dheeraj Mekala, Jingbo Shang |  |
| 587 |  |  [Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-acl.245) |  | 0 | Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty estimation (UE) in the real-world setting, where the data is always noisy and out of distribution. This paper extensively studies UE on the benchmark designed for WSD. Specifically, we first compare... | Zhu Liu, Ying Liu |  |
| 588 |  |  [Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks](https://doi.org/10.18653/v1/2023.findings-acl.246) |  | 0 | Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into the model parameters. Although existing semi-parametric language models have demonstrated promising language modeling capabilities, it remains unclear whether they can exhibit competitive zero-shot... | Zhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu, Jianshu Chen, Heng Ji |  |
| 589 |  |  [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://doi.org/10.18653/v1/2023.findings-acl.247) |  | 0 | Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of... | Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, Furu Wei |  |
| 590 |  |  [Dramatic Conversation Disentanglement](https://doi.org/10.18653/v1/2023.findings-acl.248) |  | 0 | We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in face-to-face multi-party interactions. In this work, we draw on theoretical research in sociolinguistics, sociology, and film studies to operationalize a conversational thread (including the notion of a floor... | Kent K. Chang, Danica Chen, David Bamman |  |
| 591 |  |  [Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation](https://doi.org/10.18653/v1/2023.findings-acl.249) |  | 0 | In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select and present to the user. Existing TOD systems handle this ambiguity by randomly selecting one or few results and presenting their names to the user. However, in a real scenario, users do not always... | Yongil Kim, Yerin Hwang, Joongbo Shin, Hyunkyung Bae, Kyomin Jung |  |
| 592 |  |  [Emergent Modularity in Pre-trained Transformers](https://doi.org/10.18653/v1/2023.findings-acl.250) |  | 0 | This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore to find a structure that groups neurons into modules... | Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Jie Zhou |  |
| 593 |  |  [Universal Information Extraction with Meta-Pretrained Self-Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.251) |  | 0 | Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge bases may help models to overcome this problem but it is impossible to construct a knowledge base suitable for various IE tasks. Inspired by the fact that large amount of knowledge are stored in the... | Xin Cong, Bowen Yu, Mengcheng Fang, Tingwen Liu, Haiyang Yu, Zhongkai Hu, Fei Huang, Yongbin Li, Bin Wang |  |
| 594 |  |  [SETI: Systematicity Evaluation of Textual Inference](https://doi.org/10.18653/v1/2023.findings-acl.252) |  | 0 | We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and corresponding datasets to evaluate various types of systematicity in reasoning processes. In order to solve these tasks, models are required to perform compositional inference based on known primitive... | Xiyan Fu, Anette Frank |  |
| 595 |  |  [Coarse-to-fine Few-shot Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.253) |  | 0 | Recently, Few-shot Named Entity Recognition has received wide attention with the growing need for NER models to learn new classes with minimized annotation costs. However, one common yet understudied situation is to transfer a model trained with coarse-grained classes to recognize fine-grained classes, such as separating a product category into sub-classes. We find that existing few-shot NER solutions are not suitable for such a situation since they do not consider the sub-class discrimination... | Ruotian Ma, Zhang Lin, Xuanting Chen, Xin Zhou, Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao, Yun Wen Chen |  |
| 596 |  |  [Self-Evolution Learning for Discriminative Language Model Pretraining](https://doi.org/10.18653/v1/2023.findings-acl.254) |  | 0 | Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model... | Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao |  |
| 597 |  |  [QueryForm: A Simple Zero-shot Form Entity Query Framework](https://doi.org/10.18653/v1/2023.findings-acl.255) |  | 0 | Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity... | Zifeng Wang, Zizhao Zhang, Jacob Devlin, ChenYu Lee, Guolong Su, Hao Zhang, Jennifer G. Dy, Vincent Perot, Tomas Pfister |  |
| 598 |  |  [Search-Oriented Conversational Query Editing](https://doi.org/10.18653/v1/2023.findings-acl.256) |  | 0 | Conversational query rewriting (CQR) realizes conversational search by reformulating the search dialogue into a standalone rewrite. However, existing CQR models either are not learned toward improving the downstream search performance or inefficiently generate the rewrite token-by-token from scratch while neglecting the fact that the search dialogue often has a large overlap with the rewrite. In this paper, we propose EdiRCS, a new text editing-based CQR model tailored for conversational... | Kelong Mao, Zhicheng Dou, Bang Liu, Hongjin Qian, Fengran Mo, Xiangli Wu, Xiaohua Cheng, Zhao Cao |  |
| 599 |  |  [TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model](https://doi.org/10.18653/v1/2023.findings-acl.257) |  | 0 | Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences,... | Patrick Kahardipraja, Brielen Madureira, David Schlangen |  |
| 600 |  |  [Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind](https://doi.org/10.18653/v1/2023.findings-acl.258) |  | 0 | Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this paper, we model a visually grounded referential game between a knowledgeable speaker and a listener with more limited visual and linguistic experience. Inspired by psycholinguistic theories, we endow... | Ece Takmaz, Nicolo' Brandizzi, Mario Giulianelli, Sandro Pezzelle, Raquel Fernández |  |
| 601 |  |  [A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing](https://doi.org/10.18653/v1/2023.findings-acl.259) |  | 0 | Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should be factorized into multiple sequentially dependent components, then it can be built up component by component. And the latter assumes these components to be independent so that they can be outputted... | Ye Ma, Mingming Sun, Ping Li |  |
| 602 |  |  [AMR-TST: Abstract Meaning Representation-based Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-acl.260) |  | 0 | Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph and generates the transferred text based on the AMR graph modified by a TST policy named style rewriting. Our method combines both the explainability and diversity of explicit and implicit TST... | Kaize Shi, Xueyao Sun, Li He, Dingxian Wang, Qing Li, Guandong Xu |  |
| 603 |  |  [Understanding the Cooking Process with English Recipe Text](https://doi.org/10.18653/v1/2023.findings-acl.261) |  | 0 | Translating procedural text, like recipes, into a graphical representation can be important for visualizing the text, and can offer a machine-readable formalism for use in software. There are proposals for translating recipes into a flow graph representation, where each node represents an ingredient, action, location, or equipment, and each arc between the nodes denotes the steps of the recipe. However, these proposals have had performance problems with both named entity recognition and... | Yi Fan, Anthony Hunter |  |
| 604 |  |  [Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding](https://doi.org/10.18653/v1/2023.findings-acl.262) |  | 0 | In open-ended natural-language generation, existing text decoding methods typically struggle to produce text which is both diverse and high-quality. Greedy and beam search are known to suffer from text degeneration and linguistic diversity issues, while temperature, top-k, and nucleus sampling yield diverse but often lower-quality outputs. In this work, we build upon Minimum Bayes Risk Decoding (MBRD), a family of decoding methods based on Bayesian risk minimization, to address this... | Mirac Suzgun, Luke MelasKyriazi, Dan Jurafsky |  |
| 605 |  |  [RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.263) |  | 0 | Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models’ generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose \*\*RobustQA\*\*, a novel benchmark consisting of... | Rujun Han, Peng Qi, Yuhao Zhang, Lan Liu, Juliette Burger, William Yang Wang, Zhiheng Huang, Bing Xiang, Dan Roth |  |
| 606 |  |  [SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations](https://doi.org/10.18653/v1/2023.findings-acl.264) |  | 0 | Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model’s decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language... | Victoria Lin, LouisPhilippe Morency |  |
| 607 |  |  [Reinforcement Learning for Topic Models](https://doi.org/10.18653/v1/2023.findings-acl.265) |  | 0 | We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modifications: modernize the neural network architecture, weight the ELBO loss, use contextual embeddings, and monitor the learning process via computing topic diversity and coherence for each training step. Experiments... | Jeremy Costello, Marek Z. Reformat |  |
| 608 |  |  [Contextualized Soft Prompts for Extraction of Event Arguments](https://doi.org/10.18653/v1/2023.findings-acl.266) |  | 0 | Event argument extraction (EAE) is a sub-task of event extraction where the goal is to identify roles of entity mentions for events in text. The current state-of-the-art approaches for this problem explore prompt-based methods to prompt pre-trained language models for arguments over input context. However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance. In... | Chien Van Nguyen, Hieu Man, Thien Huu Nguyen |  |
| 609 |  |  [TextVerifier: Robustness Verification for Textual Classifiers with Certifiable Guarantees](https://doi.org/10.18653/v1/2023.findings-acl.267) |  | 0 | When textual classifiers are deployed in safety-critical workflows, they must withstand the onslaught of AI-enabled model confusion caused by adversarial examples with minor alterations. In this paper, the main objective is to provide a formal verification framework, called TextVerifier, with certifiable guarantees on deep neural networks in natural language processing against word-level alteration attacks. We aim to provide an approximation of the maximal safe radius by deriving provable... | Siqi Sun, Wenjie Ruan |  |
| 610 |  |  [OASum: Large-Scale Open Domain Aspect-based Summarization](https://doi.org/10.18653/v1/2023.findings-acl.268) |  | 0 | Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users’ interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia and automatically create a high-quality,... | Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda R. Petzold, Dong Yu |  |
| 611 |  |  [On the Limitations of Simulating Active Learning](https://doi.org/10.18653/v1/2023.findings-acl.269) |  | 0 | Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve data efficiency over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position... | Katerina Margatina, Nikolaos Aletras |  |
| 612 |  |  [Towards Alleviating the Object Bias in Prompt Tuning-based Factual Knowledge Extraction](https://doi.org/10.18653/v1/2023.findings-acl.270) |  | 0 | Many works employed prompt tuning methods to automatically optimize prompt queries and extract the factual knowledge stored in Pre-trained Language Models. In this paper, we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias. To handle this problem, we propose a novel prompt tuning method called MeCoD consisting of three modules: Prompt Encoder, Object Equalization and Biased Object Obstruction. Experimental results show that... | Yuhang Wang, Dongyuan Lu, Chao Kong, Jitao Sang |  |
| 613 |  |  [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport](https://doi.org/10.18653/v1/2023.findings-acl.271) |  | 0 | Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates... | Weijie Xu, Xiaoyu Jiang, Srinivasan Sengamedu Hanumantha Rao, Francis Iannacci, Jinjin Zhao |  |
| 614 |  |  [Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages](https://doi.org/10.18653/v1/2023.findings-acl.272) |  | 0 | Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. We demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: What biases do systems... | Seraphina GoldfarbTarrant, Adam Lopez, Roi Blanco, Diego Marcheggiani |  |
| 615 |  |  [Complementary Explanations for Effective In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.273) |  | 0 | Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two dif- ferent factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and... | Xi Ye, Srinivasan Iyer, Asli Celikyilmaz, Veselin Stoyanov, Greg Durrett, Ramakanth Pasunuru |  |
| 616 |  |  [MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types](https://doi.org/10.18653/v1/2023.findings-acl.274) |  | 0 | With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated text captured by the existing metrics. In this work, we propose a new evaluation scheme to model human judgments in 7 NLP tasks, based on the fine-grained mismatches between a pair of texts. Inspired... | Keerthiram Murugesan, Sarathkrishna Swaminathan, Soham Dan, Subhajit Chaudhury, R. Chulaka Gunasekara, Maxwell Crouse, Diwakar Mahajan, Ibrahim Abdelaziz, Achille Fokoue, Pavan Kapanipathi, Salim Roukos, Alexander Gray |  |
| 617 |  |  [RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding](https://doi.org/10.18653/v1/2023.findings-acl.275) |  | 0 | Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, which further contributes to unfaithfulness. To handle this challenge and generate more faithful... | Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, Pascale Fung |  |
| 618 |  |  [Transformer Language Models Handle Word Frequency in Prediction Head](https://doi.org/10.18653/v1/2023.findings-acl.276) |  | 0 | Prediction head is a crucial component of Transformer language models. Despite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.In this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters. Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the... | Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, Kentaro Inui |  |
| 619 |  |  [Prompted LLMs as Chatbot Modules for Long Open-domain Conversation](https://doi.org/10.18653/v1/2023.findings-acl.277) |  | 0 | In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it... | Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, Kangwook Lee |  |
| 620 |  |  [Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.278) |  | 0 | Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to Prompt pre-trained language models (PLMs) To be Consistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible... | Fengzhu Zeng, Wei Gao |  |
| 621 |  |  [Model Analysis & Evaluation for Ambiguous Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.279) |  | 0 | Ambiguous questions are a challenge for Question Answering models, as they require answers that cover multiple interpretations of the original query. To this end, these models are required to generate long-form answers that often combine conflicting pieces of information. Although recent advances in the field have shown strong capabilities in generating fluent responses, certain research questions remain unanswered. Does model/data scaling improve the answers’ quality? Do automated metrics... | Konstantinos Papakostas, Irene Papadopoulou |  |
| 622 |  |  [Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models](https://doi.org/10.18653/v1/2023.findings-acl.280) |  | 0 | Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is... | Robert Morabito, Jad Kabbara, Ali Emami |  |
| 623 |  |  [Critic-Guided Decoding for Controlled Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.281) |  | 0 | Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted... | Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joonsuk Park, Hwaran Lee, Kyomin Jung |  |
| 624 |  |  [MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations](https://doi.org/10.18653/v1/2023.findings-acl.282) |  | 0 | Patients who effectively manage their symptoms often demonstrate higher levels of engagement in conversations and interventions with healthcare practitioners. This engagement is multifaceted, encompassing cognitive and social dimensions. Consequently, it is crucial for AI systems to understand the engagement in natural conversations between patients and practitioners to better contribute toward patient care. In this paper, we present a novel dataset (MedNgage), which consists of patient-nurse... | Yan Wang, Heidi Ann Scharf Donovan, Sabit Hassan, Malihe Alikhani |  |
| 625 |  |  [SEAG: Structure-Aware Event Causality Generation](https://doi.org/10.18653/v1/2023.findings-acl.283) |  | 0 | Extracting event causality underlies a broad spectrum of natural language processing applications. Cutting-edge methods break this task into Event Detection and Event Causality Identification. Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of separating the task incurs limitations. On the one hand, it suffers from the lack of cross-task dependencies and may cause error propagation. On the other hand, it predicts events and relations separately,... | Zhengwei Tao, Zhi Jin, Xiaoying Bai, Haiyan Zhao, Chengfeng Dou, Yongqiang Zhao, Fang Wang, Chongyang Tao |  |
| 626 |  |  [Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.284) |  | 0 | Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments... | Ruixiang Tang, Dehan Kong, Longtao Huang, Hui Xue |  |
| 627 |  |  [A Two-Stage Decoder for Efficient ICD Coding](https://doi.org/10.18653/v1/2023.findings-acl.285) |  | 0 | Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how... | ThanhTung Nguyen, Viktor Schlegel, Abhinav Ramesh Kashyap, Stefan Winkler |  |
| 628 |  |  [Asymmetric feature interaction for interpreting model predictions](https://doi.org/10.18653/v1/2023.findings-acl.286) |  | 0 | In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction... | Xiaolei Lu, Jianghong Ma, Haode Zhang |  |
| 629 |  |  [Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo](https://doi.org/10.18653/v1/2023.findings-acl.287) |  | 0 | Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard “ground truth” as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution.... | Tharindu Cyril Weerasooriya, Alexander Ororbia, Raj Bhensadadia, Ashiqur R. KhudaBukhsh, Christopher Homan |  |
| 630 |  |  [Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.288) |  | 0 | Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number... | Pranav Ajit Nair, Sukomal Pal, Pradeepika Verma |  |
| 631 |  |  [ClaimDiff: Comparing and Contrasting Claims on Contentious Issues](https://doi.org/10.18653/v1/2023.findings-acl.289) |  | 0 | With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one’s argument is not necessarily more true than the other, requiring comparison rather than... | Miyoung Ko, Ingyu Seong, Hwaran Lee, Joonsuk Park, Minsuk Chang, Minjoon Seo |  |
| 632 |  |  [Unsupervised Paraphrasing of Multiword Expressions](https://doi.org/10.18653/v1/2023.findings-acl.290) |  | 0 | We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems. | Takashi Wada, Yuji Matsumoto, Timothy Baldwin, Jey Han Lau |  |
| 633 |  |  [G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network](https://doi.org/10.18653/v1/2023.findings-acl.291) |  | 0 | The generalization ability of pre-trained language models (Plms) in downstream tasks is heavily influenced by fine-tuning. The objective of fine-tuning is to transform the latent representation of Plms from a universal space to a target space, allowing the model to be applied to downstream tasks with the capability of generalizing to unseen samples. However, the effect of Plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the... | Rongxiang Weng, Wensen Cheng, Min Zhang |  |
| 634 |  |  [Unified Language Representation for Question Answering over Text, Tables, and Images](https://doi.org/10.18653/v1/2023.findings-acl.292) |  | 0 | When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task... | Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, Yongbin Li |  |
| 635 |  |  [A Set Prediction Network For Extractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.293) |  | 0 | Extractive summarization focuses on extracting salient sentences from the source document and incorporating them in the summary without changing their wording or structure. The naive approach for extractive summarization is sentence classification, which makes independent binary decisions for each sentence, resulting in the model cannot detect the dependencies between sentences in the summary. Recent approaches introduce an autoregressive decoder to detect redundancy relationship between... | Xiaoxia Cheng, Yongliang Shen, Weiming Lu |  |
| 636 |  |  [Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.294) |  | 0 | Location information can support social media analyses by providing geographic context. Some of the most accurate and popular Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which fail to handle informal or noisy location names. We propose Geo-Seq2seq, a sequence-to-sequence (seq2seq) model for Twitter user geolocation that rewrites noisy, multilingual user-provided location strings into structured English location names. We train our... | Jingyu Zhang, Alexandra DeLucia, Chenyu Zhang, Mark Dredze |  |
| 637 |  |  [Predicting Numerals in Text Using Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2023.findings-acl.295) |  | 0 | Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals. However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense. In this study, we apply the k-nearest neighbor LM (kNN-LM) to the masked numeral prediction (MNP) task, which measures the quantitative commonsense of LMs.kNN-LM extends pre-trained neural LMs... | Taku Sakamoto, Akiko Aizawa |  |
| 638 |  |  [HonestBait: Forward References for Attractive but Faithful Headline Generation](https://doi.org/10.18653/v1/2023.findings-acl.296) |  | 0 | Current methods for generating attractive headlines often learn directly from data, which bases attractiveness on the number of user clicks and views. Although clicks or views do reflect user interest, they can fail to reveal how much interest is raised by the writing style and how much is due to the event or topic itself. Also, such approaches can lead to harmful inventions by over-exaggerating the content, aggravating the spread of false information. In this work, we propose HonestBait, a... | ChihYao Chen, Dennis Wu, LunWei Ku |  |
| 639 |  |  [Few Shot Rationale Generation using Self-Training with Dual Teachers](https://doi.org/10.18653/v1/2023.findings-acl.297) |  | 0 | Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly process, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that... | Aditya Srikanth Veerubhotla, Lahari Poddar, Jun Yin, György Szarvas, Sharanya Eswaran |  |
| 640 |  |  [Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints](https://doi.org/10.18653/v1/2023.findings-acl.298) |  | 0 | Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the current evaluation process of LNMT studies. The model needs to cope with challenging lexical constraints that are “homographs” or “unseen” during training. To this end, we first design a homograph... | Yujin Baek, Koanho Lee, Dayeon Ki, Cheonbok Park, HyoungGyu Lee, Jaegul Choo |  |
| 641 |  |  [NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.299) |  | 0 | Large-scale datasets in the real world inevitably involve label noise. Deep models can gradually overfit noisy labels and thus degrade model generalization. To mitigate the effects of label noise, learning with noisy labels (LNL) methods are designed to achieve better generalization performance. Due to the lack of suitable datasets, previous studies have frequently employed synthetic label noise to mimic real-world label noise. However, synthetic noise is not instance-dependent, making this... | Tingting Wu, Xiao Ding, Minji Tang, Hao Zhang, Bing Qin, Ting Liu |  |
| 642 |  |  [Sampling Better Negatives for Distantly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.300) |  | 0 | Distantly supervised named entity recognition (DS-NER) has been proposed to exploit the automatically labeled training data instead of human annotations. The distantly annotated datasets are often noisy and contain a considerable number of false negatives. The recent approach uses a weighted sampling approach to select a subset of negative samples for training. However, it requires a good classifier to assign weights to the negative samples. In this paper, we propose a simple and... | Lu Xu, Lidong Bing, Wei Lu |  |
| 643 |  |  [Prototype-Based Interpretability for Legal Citation Prediction](https://doi.org/10.18653/v1/2023.findings-acl.301) |  | 0 | Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work, we attempt to address these requirements applied to the important problem of legal citation prediction (LCP). We design the task with parallels to the thought-process of lawyers, i.e., with... | Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu |  |
| 644 |  |  [LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models](https://doi.org/10.18653/v1/2023.findings-acl.302) |  | 0 | Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation... | Philipp Wicke |  |
| 645 |  |  [Making Better Use of Training Corpus: Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation](https://doi.org/10.18653/v1/2023.findings-acl.303) |  | 0 | In this paper, we aim to adapt the idea of retrieval-based neural approaches to the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicting the sentiment polarity and it is usually aspect-dependent. Semantic similar neighbors with different polarities will be infeasible even counterproductive. To tackle this issue, we propose a... | Guoxin Yu, Lemao Liu, Haiyun Jiang, Shuming Shi, Xiang Ao |  |
| 646 |  |  [Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention](https://doi.org/10.18653/v1/2023.findings-acl.304) |  | 0 | As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems. Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider. In recent mainstream approaches, each domain and slot are aggregated and regarded as a single query feeding into attention with the dialogue history to obtain domain-slot specific representations. In this work,... | Longfei Yang, Jiyi Li, Sheng Li, Takahiro Shinozaki |  |
| 647 |  |  [Improved Visual Story Generation with Adaptive Context Modeling](https://doi.org/10.18653/v1/2023.findings-acl.305) |  | 0 | Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all preceding images contribute equally to the generation of the characters and scenes at the current stage. To address this, we present a simple method that improves the leading system with adaptive context... | Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming Shi, Bing Qin |  |
| 648 |  |  [Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.306) |  | 0 | We focus on dialogue reading comprehension (DRC) that extracts answers from dialogues. Compared to standard RC tasks, DRC has raised challenges because of the complex speaker information and noisy dialogue context. Essentially, the challenges come from the speaker-centric nature of dialogue utterances — an utterance is usually insufficient in its surface form, but requires to incorporate the role of its speaker and the dialogue context to fill the latent pragmatic and intention information. We... | Jiangnan Li, Mo Yu, Fandong Meng, Zheng Lin, Peng Fu, Weiping Wang, Jie Zhou |  |
| 649 |  |  [Speech-to-Speech Translation for a Real-world Unwritten Language](https://doi.org/10.18653/v1/2023.findings-acl.307) |  | 0 | We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting... | PengJen Chen, Kevin Tran, Yilin Yang, Jingfei Du, Justine Kao, YuAn Chung, Paden Tomasello, PaulAmbroise Duquenne, Holger Schwenk, Hongyu Gong, Hirofumi Inaguma, Sravya Popuri, Changhan Wang, Juan Pino, WeiNing Hsu, Ann Lee |  |
| 650 |  |  [Code Execution with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.308) |  | 0 | Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution,... | Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, Nan Duan |  |
| 651 |  |  [BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.309) |  | 0 | It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge... | Shibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan Shao, Hengzhe Zhang, Eric P. Xing, Zhiting Hu |  |
| 652 |  |  [Sequential Path Signature Networks for Personalised Longitudinal Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.310) |  | 0 | Longitudinal user modeling can provide a strong signal for various downstream tasks. Despite the rapid progress in representation learning, dynamic aspects of modelling individuals’ language have only been sparsely addressed. We present a novel extension of neural sequential models using the notion of path signatures from rough path theory, which constitute graduated summaries of continuous paths and have the ability to capture non-linearities in trajectories. By combining path signatures of... | Talia Tseriotou, Adam Tsakalidis, Peter Foster, Terence Lyons, Maria Liakata |  |
| 653 |  |  [A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.311) |  | 0 | Recent studies have pointed out that many well-developed Visual Question Answering (VQA) systems suffer from bias problem. Despite the remarkable performance gained on In-Distribution (ID) datasets, the VQA model might merely capture the superficial correlation from question to answer rather than showing real reasoning abilities. Therefore, when switching to Out-of-Distribution (OOD) dataset, whose test distribution is unknown or even reversed with the training set, significant drops might be... | Yu Li, Bojie Hu, Fengshuo Zhang, Yahan Yu, Jian Liu, Yufeng Chen, Jinan Xu |  |
| 654 |  |  [Trigger-Argument based Explanation for Event Detection](https://doi.org/10.18653/v1/2023.findings-acl.312) |  | 0 | Event Detection (ED) is a critical task that aims to identify events of certain types in plain text. Neural models have achieved great success on ED, thus coming with a desire for higher interpretability. Existing works mainly exploit words or phrases of the input text to explain models’ inner mechanisms. However, for ED, the event structure, comprising of an event trigger and a set of arguments, are more enlightening clues to explain model behaviors. To this end, we propose a Trigger-Argument... | Yong Guan, Jiaoyan Chen, Freddy Lécué, Jeff Z. Pan, Juanzi Li, Ru Li |  |
| 655 |  |  [Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections](https://doi.org/10.18653/v1/2023.findings-acl.313) |  | 0 | Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a theme to account for more than just a word distribution, and include generalized concepts deemed relevant by domain experts. Then, we propose an interactive framework that receives and encodes expert... | Maria Leonor Pacheco, Tunazzina Islam, Lyle H. Ungar, Ming Yin, Dan Goldwasser |  |
| 656 |  |  [NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery](https://doi.org/10.18653/v1/2023.findings-acl.314) |  | 0 | Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose NormMark, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These... | Farhad Moghimifar, Shilin Qu, Tongtong Wu, YuanFang Li, Gholamreza Haffari |  |
| 657 |  |  [VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations](https://doi.org/10.18653/v1/2023.findings-acl.315) |  | 0 | Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without training by voting on hard labels from predictions of transformations, namely, VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the hard labels of input text and its... | HoangQuoc NguyenSon, Seira Hidano, Kazuhide Fukushima, Shinsaku Kiyomoto, Isao Echizen |  |
| 658 |  |  [Fusion or Defusion? Flexible Vision-and-Language Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.316) |  | 0 | Existing approaches in the vision-and-language pre-training (VLP) paradigm mainly deploy either fusion-based encoders or dual-encoders, failing to achieve both effectiveness and efficiency in downstream multimodal tasks. In this paper, we build a flexible VLP model by incorporating cross-modal fusions into a dual-encoder architecture, where the introduced fusion modules can be easily decoupled from the dual encoder so as to switch the model to a fusion-free one. To better absorb cross-modal... | Rongyi Sun, Ziran Li, Yifeng Ding, Qifan Wang, Jingang Wang, Haitao Zheng, Wei Wu, Yunsen Xian |  |
| 659 |  |  [COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP](https://doi.org/10.18653/v1/2023.findings-acl.317) |  | 0 | Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that... | Fanny Jourdan, Agustin Martin Picard, Thomas Fel, Laurent Risser, JeanMichel Loubes, Nicholas Asher |  |
| 660 |  |  [Code-Switched Text Synthesis in Unseen Language Pairs](https://doi.org/10.18653/v1/2023.findings-acl.318) |  | 0 | Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an... | IHung Hsu, Avik Ray, Shubham Garg, Nanyun Peng, Jing Huang |  |
| 661 |  |  [Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning](https://doi.org/10.18653/v1/2023.findings-acl.319) |  | 0 | Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of... | JustusJonas Erker, Stefan Schaffer, Gerasimos Spanakis |  |
| 662 |  |  [Data-Efficient French Language Modeling with CamemBERTa](https://doi.org/10.18653/v1/2023.findings-acl.320) |  | 0 | Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model’s performance on a variety of French... | Wissam Antoun, Benoît Sagot, Djamé Seddah |  |
| 663 |  |  [Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text](https://doi.org/10.18653/v1/2023.findings-acl.321) |  | 0 | While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism.... | Zhun Yang, Adam Ishay, Joohyung Lee |  |
| 664 |  |  [Evaluating the Factual Consistency of Large Language Models Through News Summarization](https://doi.org/10.18653/v1/2023.findings-acl.322) |  | 0 | While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news... | Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel |  |
| 665 |  |  [Text Generation Model Enhanced with Semantic Information in Aspect Category Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.323) |  | 0 | Aspect Category Sentiment Analysis (ACSA) is one of the main subtasks of sentiment analysis, which aims at predicting polarity over a given aspect category. Recently, generative methods emerge as an efficient way to utilize a pre-trained language model for solving ACSA. However, those methods fail to model relations of target words and opinion words in a sentence including multiple aspects. To tackle this problem, this paper proposes a method to incorporate Abstract Meaning Representation... | Tu Tran, Kiyoaki Shirai, Natthawut Kertkeidkachorn |  |
| 666 |  |  [Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting](https://doi.org/10.18653/v1/2023.findings-acl.324) |  | 0 | We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in... | Ruixi Lin, Hwee Tou Ng |  |
| 667 |  |  [CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.325) |  | 0 | In order to solve the inefficient parameter update and storage issues of fine-tuning in Natural Language Generation (NLG) tasks, prompt-tuning methods have emerged as lightweight alternatives. Furthermore, efforts to reduce the gap between pre-training and fine-tuning have shown successful results in low-resource settings. As large Pre-trained Language Models (PLMs) for Program and Language Generation (PLG) tasks are constantly being developed, prompt tuning methods are necessary for the tasks.... | YunSeok Choi, JeeHyong Lee |  |
| 668 |  |  [Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale](https://doi.org/10.18653/v1/2023.findings-acl.326) |  | 0 | In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the... | Vijeta Deshpande, Dan Pechi, Shree Thatte, Vladislav Lialin, Anna Rumshisky |  |
| 669 |  |  [Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter](https://doi.org/10.18653/v1/2023.findings-acl.327) |  | 0 | Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the... | Yi Liu, Xiaohan Bi, Lei Li, Sishuo Chen, Wenkai Yang, Xu Sun |  |
| 670 |  |  [Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.328) |  | 0 | Text classification with extremely weak supervision (EWS) imposes stricter supervision constraints compared to regular weakly supervise classification. Absolutely no labeled training samples or hand-crafted rules specific to the evaluation data are allowed. Such restrictions limit state-of-the-art EWS classification methods to indirect weak labeling techniques that assign unnatural label uncertainty estimates. We present PLAT, a framework that creates weak labels by leveraging recent... | Seongmin Park, Kyungho Kim, Jihwa Lee |  |
| 671 |  |  [GVdoc - Graph-based Visual DOcument Classification](https://doi.org/10.18653/v1/2023.findings-acl.329) |  | 0 | The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization... | Fnu Mohbat, Mohammed J. Zaki, Catherine FineganDollak, Ashish Verma |  |
| 672 |  |  [A Sequence-to-Sequence&Set Model for Text-to-Table Generation](https://doi.org/10.18653/v1/2023.findings-acl.330) |  | 0 | Recently, the text-to-table generation task has attracted increasing attention due to its wide applications. In this aspect, the dominant model formalizes this task as a sequence-to-sequence generation task and serializes each table into a token sequence during training by concatenating all rows in a top-down order. However, it suffers from two serious defects: 1) the predefined order introduces a wrong bias during training, which highly penalizes shifts in the order between rows; 2) the error... | Tong Li, Zhihao Wang, Liangying Shao, Xuling Zheng, Xiaoli Wang, Jinsong Su |  |
| 673 |  |  [Automatic Readability Assessment for Closely Related Languages](https://doi.org/10.18653/v1/2023.findings-acl.331) |  | 0 | In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models’ accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how... | Joseph Marvin Imperial, Ekaterina Kochmar |  |
| 674 |  |  [Towards Robust Ranker for Text Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.332) |  | 0 | A neural ranker plays an indispensable role in the de facto ‘retrieval & rerank’ pipeline, but its training still lags behind due to the weak negative mining during contrastive learning. Compared to retrievers boosted by self-adversarial (i.e., in-distribution) negative mining, the ranker’s heavy structure suffers from query-document combinatorial explosions, so it can only resort to the negative sampled by the fast yet out-of-distribution retriever. Thereby, the moderate negatives compose... | Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Binxing Jiao, Daxin Jiang |  |
| 675 |  |  [Semi-Supervised Domain Adaptation for Emotion-Related Tasks](https://doi.org/10.18653/v1/2023.findings-acl.333) |  | 0 | Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often fails to effectively utilize the target supervision and cannot address distribution shifts across different domains due to the training data bias toward the source-labeled samples. In this paper,... | Mahshid Hosseini, Cornelia Caragea |  |
| 676 |  |  [Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy](https://doi.org/10.18653/v1/2023.findings-acl.334) |  | 0 | AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted... | Anuradha Welivita, Pearl Pu |  |
| 677 |  |  [ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language Representations](https://doi.org/10.18653/v1/2023.findings-acl.335) |  | 0 | Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity... | Zhen Han, Ruotong Liao, Jindong Gu, Yao Zhang, Zifeng Ding, Yujia Gu, Heinz Koeppl, Hinrich Schütze, Volker Tresp |  |
| 678 |  |  [Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.336) |  | 0 | Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs’ performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks’ datasets. For this aim,... | Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, Hamed Khanpour |  |
| 679 |  |  [TextObfuscator: Making Pre-trained Language Model a Privacy Protector via Obfuscating Word Representations](https://doi.org/10.18653/v1/2023.findings-acl.337) |  | 0 | In real-world applications, pre-trained language models are typically deployed on the cloud, allowing clients to upload data and perform compute-intensive inference remotely. To avoid sharing sensitive data directly with service providers, clients can upload numerical representations rather than plain text to the cloud. However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains. In... | Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Yuran Wang, Yong Ding, Yibo Zhang, Qi Zhang, Xuanjing Huang |  |
| 680 |  |  [Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training](https://doi.org/10.18653/v1/2023.findings-acl.338) |  | 0 | Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large... | Kelly Marchisio, Patrick S. H. Lewis, Yihong Chen, Mikel Artetxe |  |
| 681 |  |  [DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.339) |  | 0 | Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios. Typically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.However, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed. In this work, we propose a novel... | Bo Lv, Xin Liu, Shaojie Dai, Nayu Liu, Fan Yang, Ping Luo, Yue Yu |  |
| 682 |  |  [Exploring Robust Overfitting for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.340) |  | 0 | We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial training data. We attempt to mitigate robust overfitting by combining regularization methods with adversarial training. Following the philosophy that prevents the model from memorizing the adversarial data,... | Bin Zhu, Yanghui Rao |  |
| 683 |  |  [Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations](https://doi.org/10.18653/v1/2023.findings-acl.341) |  | 0 | There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model’s ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In... | Jifan Chen, Yuhao Zhang, Lan Liu, Rui Dong, Xinchi Chen, Patrick Ng, William Yang Wang, Zhiheng Huang |  |
| 684 |  |  [D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias](https://doi.org/10.18653/v1/2023.findings-acl.342) |  | 0 | Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. While active learning (AL) has shown promise in training models with a small amount of annotated data, AL’s reliance on the model’s behavior for selective sampling can lead to... | Sabit Hassan, Malihe Alikhani |  |
| 685 |  |  [Language Anisotropic Cross-Lingual Model Editing](https://doi.org/10.18653/v1/2023.findings-acl.343) |  | 0 | Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model’s raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work,... | Yang Xu, Yutai Hou, Wanxiang Che, Min Zhang |  |
| 686 |  |  [Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.344) |  | 0 | There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting. We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST.First, we formulate DST as a Python... | Brendan King, Jeffrey Flanigan |  |
| 687 |  |  [Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation](https://doi.org/10.18653/v1/2023.findings-acl.345) |  | 0 | Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between... | Chunliu Wang, Huiyuan Lai, Malvina Nissim, Johan Bos |  |
| 688 |  |  [Multi-modal Sarcasm Generation: Dataset and Solution](https://doi.org/10.18653/v1/2023.findings-acl.346) |  | 0 | As an interesting and challenging task, sarcasm generation has attracted widespread attention. Although very recent studies have made promising progress, none of them considers generating a sarcastic description for a given image - as what people are doing on Twitter. In this paper, we present a Multi-modal Sarcasm Generation (MSG) task: Given an image with hashtags that provide the sarcastic target, MSG aims to generate sarcastic descriptions like humans. Different from textual sarcasm... | Wenye Zhao, Qingbao Huang, Dongsheng Xu, Peizhi Zhao |  |
| 689 |  |  [Rethinking Semi-supervised Learning with Language Models](https://doi.org/10.18653/v1/2023.findings-acl.347) |  | 0 | Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge,... | Zhengxiang Shi, Francesco Tonolini, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai, Yunlong Jiao |  |
| 690 |  |  [Retrieval-Based Transformer for Table Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.348) |  | 0 | Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources, such as data lakes. In this paper, we introduce a novel approach toward automatic data wrangling in an attempt to alleviate the effort of end-users, e.g. data analysts, in structuring dynamic views... | Michael R. Glass, Xueqing Wu, Ankita Rajaram Naik, Gaetano Rossiello, Alfio Gliozzo |  |
| 691 |  |  [ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER](https://doi.org/10.18653/v1/2023.findings-acl.349) |  | 0 | Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose ECG-QALM, a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled... | Karan Aggarwal, Henry Jin, Aitzaz Ahmad |  |
| 692 |  |  [Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages](https://doi.org/10.18653/v1/2023.findings-acl.350) |  | 0 | Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual... | Tomasz Limisiewicz, Jirí Balhar, David Marecek |  |
| 693 |  |  [The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding](https://doi.org/10.18653/v1/2023.findings-acl.351) |  | 0 | In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation. We describe a hybrid architecture for dialogue response generation that combines the strengths of both paradigms. The first component of this architecture is a rule-based content selection... | Hao Fang, Anusha Balakrishnan, Harsh Jhamtani, John Bufe, Jean Crawford, Jayant Krishnamurthy, Adam Pauls, Jason Eisner, Jacob Andreas, Dan Klein |  |
| 694 |  |  [Know What I don't Know: Handling Ambiguous and Unknown Questions for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.352) |  | 0 | The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a plausible SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions. To formalize this problem, we conduct a preliminary study on the observed ambiguous and unanswerable cases in text-to-SQL and summarize them into 6 feature categories. Correspondingly, we identify the... | Bing Wang, Yan Gao, Zhoujun Li, JianGuang Lou |  |
| 695 |  |  [Rethinking Document-Level Relation Extraction: A Reality Check](https://doi.org/10.18653/v1/2023.findings-acl.353) |  | 0 | Recently, numerous efforts have continued to push up performance boundaries of document-level relation extraction (DocRE) and have claimed significant progress in DocRE. In this paper, we do not aim at proposing a novel model for DocRE. Instead, we take a closer look at the field to see if these performance gains are actually true. By taking a comprehensive literature review and a thorough examination of popular DocRE datasets, we find that these performance gains are achieved upon a strong or... | Jing Li, Yequan Wang, Shuai Zhang, Min Zhang |  |
| 696 |  |  [Optimizing Test-Time Query Representations for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.354) |  | 0 | Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient... | Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, Jinhyuk Lee |  |
| 697 |  |  [A Customized Text Sanitization Mechanism with Differential Privacy](https://doi.org/10.18653/v1/2023.findings-acl.355) |  | 0 | As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on a relaxed notion of metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good privacy-utility trade-offs. To address these limitations, we propose a novel Customized Text... | Sai Chen, Fengran Mo, Yanhao Wang, Cen Chen, JianYun Nie, Chengyu Wang, Jamie Cui |  |
| 698 |  |  [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://doi.org/10.18653/v1/2023.findings-acl.356) |  | 0 | Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is... | Peng Lu, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Philippe Langlais |  |
| 699 |  |  [Frustratingly Easy Label Projection for Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.findings-acl.357) |  | 0 | Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the... | Yang Chen, Chao Jiang, Alan Ritter, Wei Xu |  |
| 700 |  |  [Enhancing Hierarchical Text Classification through Knowledge Graph Integration](https://doi.org/10.18653/v1/2023.findings-acl.358) |  | 0 | Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the... | Ye Liu, Kai Zhang, Zhenya Huang, Kehang Wang, Yanghai Zhang, Qi Liu, Enhong Chen |  |
| 701 |  |  [How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.359) |  | 0 | The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this phenomenon arises and how to better address it. In this work, we design a taxonomy to categorize commonly-seen multi-answer MRC instances, with which we inspect three multi-answer datasets and analyze... | Chen Zhang, Jiuheng Lin, Xiao Liu, Yuxuan Lai, Yansong Feng, Dongyan Zhao |  |
| 702 |  |  [An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text](https://doi.org/10.18653/v1/2023.findings-acl.360) |  | 0 | Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets—two in the legal domain and two in the biomedical domain, each with two levels of label granularity— and always depart... | Yova Kementchedjhieva, Ilias Chalkidis |  |
| 703 |  |  [Domain Incremental Lifelong Learning in an Open World](https://doi.org/10.18653/v1/2023.findings-acl.361) |  | 0 | Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong learning model that tries to learn... | Yi Dai, Hao Lang, Yinhe Zheng, Bowen Yu, Fei Huang, Yongbin Li |  |
| 704 |  |  [Improving Knowledge Graph Completion with Generative Hard Negative Mining](https://doi.org/10.18653/v1/2023.findings-acl.362) |  | 0 | Contrastive learning has recently shown great potential to improve text-based knowledge graph completion (KGC). In this paper, we propose to learn a more semantically structured entity representation space in text-based KGC via hard negatives mining. Specifically, we novelly leverage a sequence-to-sequence architecture to generate high-quality hard negatives. These negatives are sampled from the same decoding distributions as the anchor (or correct entity), inherently being semantically close... | Zile Qiao, Wei Ye, Dingyao Yu, Tong Mo, Weiping Li, Shikun Zhang |  |
| 705 |  |  [Visually-Enhanced Phrase Understanding](https://doi.org/10.18653/v1/2023.findings-acl.363) |  | 0 | Large-scale vision-language pre-training has exhibited strong performance in various visual and textual understanding tasks. Recently, the textual encoders of multi-modal pre-trained models have been shown to generate high-quality textual representations, which often outperform models that are purely text-based, such as BERT. In this study, our objective is to utilize both textual and visual encoders of multi-modal pre-trained models to enhance language understanding tasks. We achieve this by... | TsuYuan Hsu, ChenAn Li, ChaoWei Huang, YunNung Chen |  |
| 706 |  |  [Reasoning in Large Language Models Through Symbolic Math Word Problems](https://doi.org/10.18653/v1/2023.findings-acl.364) |  | 0 | Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a “concise explanation” of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3’s... | Vedant Gaur, Nikunj Saunshi |  |
| 707 |  |  [It's not Sexually Suggestive; It's Educative \| Separating Sex Education from Suggestive Content on TikTok videos](https://doi.org/10.18653/v1/2023.findings-acl.365) |  | 0 | We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator’s point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children’s exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education,... | Enfa George, Mihai Surdeanu |  |
| 708 |  |  [Dynamic Structured Neural Topic Model with Self-Attention Mechanism](https://doi.org/10.18653/v1/2023.findings-acl.366) |  | 0 | This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies. Our model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism. Additionally, we introduce citation regularization, which induces attention weights to represent citation relations by modeling text and citations jointly. Our model outperforms a prior dynamic embedded topic model... | Nozomu Miyamoto, Masaru Isonuma, Sho Takase, Junichiro Mori, Ichiro Sakata |  |
| 709 |  |  [Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.367) |  | 0 | This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT.... | Qiang Wang, Xinhui Hu, Ming Chen |  |
| 710 |  |  [Commonsense Knowledge Transfer for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.368) |  | 0 | Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text. In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a... | Wangchunshu Zhou, Ronan Le Bras, Yejin Choi |  |
| 711 |  |  [Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection](https://doi.org/10.18653/v1/2023.findings-acl.369) |  | 0 | Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model’s representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training... | Shadi Iskander, Kira Radinsky, Yonatan Belinkov |  |
| 712 |  |  [Focal Training and Tagger Decouple for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.370) |  | 0 | In this paper, we investigate how to improve tagging-based Grammatical Error Correction models. We address two issues of current tagging-based approaches, label imbalance issue, and tagging entanglement issue. Then we propose to down-weight the loss of well-classified labels using Focal Loss and decouple the error detection layer from the label tagging layer through an extra self-attention-based matching module. Experiments over three latest Chinese Grammatical Error Correction datasets show... | Minghuan Tan, Min Yang, Ruifeng Xu |  |
| 713 |  |  [LET: Leveraging Error Type Information for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.371) |  | 0 | Grammatical error correction (GEC) aims to correct errors in given sentences and is significant to many downstream natural language understanding tasks. Recent work introduces the idea of grammatical error detection (GED) to improve the GEC task performance. In contrast, these explicit multi-stage works propagate and amplify the problem of misclassification of the GED module. To introduce more convincing error type information, we propose an end-to-end framework in this paper, which Leverages... | Lingyu Yang, Hongjia Li, Lei Li, Chengyin Xu, Shutao Xia, Chun Yuan |  |
| 714 |  |  [On the Role of Parallel Data in Cross-lingual Transfer Learning](https://doi.org/10.18653/v1/2023.findings-acl.372) |  | 0 | While prior work has established that the use of parallel data is conducive for cross-lingual learning, it is unclear if the improvements come from the data itself, or if it is the modeling of parallel interactions that matters. Exploring this, we examine the usage of unsupervised machine translation to generate synthetic parallel data, and compare it to supervised machine translation and gold parallel data. We find that even model generated parallel data can be useful for downstream tasks, in... | Machel Reid, Mikel Artetxe |  |
| 715 |  |  [CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-acl.373) |  | 0 | Attribute Value Extraction (AVE) aims to automatically obtain attribute value pairs from product descriptions to aid e-commerce. Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at different scales simultaneously; 2) easy confusion by some highly similar fine-grained attributes. This paper proposes a pre-training technique for AVE to address these issues. In particular, we first improve... | Xinnan Guo, Wentao Deng, Yongrui Chen, Yang Li, Mengdi Zhou, Guilin Qi, Tianxing Wu, Dong Yang, Liubin Wang, Yong Pan |  |
| 716 |  |  [Phrase Retrieval for Open Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.374) |  | 0 | Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this... | Soyeong Jeong, Jinheon Baek, Sung Ju Hwang, Jong Park |  |
| 717 |  |  [Unlearning Bias in Language Models by Partitioning Gradients](https://doi.org/10.18653/v1/2023.findings-acl.375) |  | 0 | Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms... | Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, Heng Ji |  |
| 718 |  |  [Meta-training with Demonstration Retrieval for Efficient Few-shot Learning](https://doi.org/10.18653/v1/2023.findings-acl.376) |  | 0 | Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Meta-training allows one to leverage smaller models for few-shot generalization in a domain-general and task-agnostic manner; however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose meta-training with demonstration retrieval, where we use... | Aaron Mueller, Kanika Narang, Lambert Mathias, Qifan Wang, Hamed Firooz |  |
| 719 |  |  [VCSUM: A Versatile Chinese Meeting Summarization Dataset](https://doi.org/10.18653/v1/2023.findings-acl.377) |  | 0 | Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript.... | Han Wu, Mingjie Zhan, Haochen Tan, Zhaohui Hou, Ding Liang, Linqi Song |  |
| 720 |  |  [LEDA: a Large-Organization Email-Based Decision-Dialogue-Act Analysis Dataset](https://doi.org/10.18653/v1/2023.findings-acl.378) |  | 0 | Collaboration increasingly happens online. This is especially true for large groups working on global tasks, with collaborators all around the globe. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of decision-making mechanisms in such groups, and provides a new annotated dataset based on real-world data from the public mail-archives of one such organisation – the Internet Engineering Task Force (IETF). We... | Mladen Karan, Prashant Khare, Ravi Shekhar, Stephen McQuistin, Ignacio Castro, Gareth Tyson, Colin Perkins, Patrick Healey, Matthew Purver |  |
| 721 |  |  [Negation Scope Refinement via Boundary Shift Loss](https://doi.org/10.18653/v1/2023.findings-acl.379) |  | 0 | Negation in natural language may affect many NLP applications, e.g., information extraction and sentiment analysis. The key sub-task of negation detection is negation scope resolution which aims to extract the portion of a sentence that is being negated by a negation cue (e.g., keyword “not” and never”) in the sentence. Due to the long spans, existing methods tend to make wrong predictions around the scope boundaries. In this paper, we propose a simple yet effective model named R-BSL which... | Yin Wu, Aixin Sun |  |
| 722 |  |  [Towards Diverse and Effective Question-Answer Pair Generation from Children Storybooks](https://doi.org/10.18653/v1/2023.findings-acl.380) |  | 0 | Recent advances in QA pair generation (QAG) have raised interest in applying this technique to the educational field. However, the diversity of QA types remains a challenge despite its contributions to comprehensive learning and assessment of children. In this paper, we propose a QAG framework that enhances QA type diversity by producing different interrogative sentences and implicit/explicit answers. Our framework comprises a QFS-based answer generator, an iterative QA generator, and a... | Sugyeong Eo, Hyeonseok Moon, Jinsung Kim, Yuna Hur, Jeongwook Kim, Songeun Lee, Changwoo Chun, Sungsoo Park, Heuiseok Lim |  |
| 723 |  |  [Pulling Out All The Full Stops: Punctuation Sensitivity in Neural Machine Translation and Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.381) |  | 0 | Much of the work testing machine translation systems for robustness and sensitivity has been adversarial or tended towards testing noisy input such as spelling errors, or non-standard input such as dialects. In this work, we take a step back to investigate a sensitivity problem that can seem trivial and is often overlooked: punctuation. We perform basic sentence-final insertion and deletion perturbation tests with full stops, exclamation and questions marks across source languages and... | Prathyusha Jwalapuram |  |
| 724 |  |  [Reimagining Retrieval Augmented Language Models for Answering Queries](https://doi.org/10.18653/v1/2023.findings-acl.382) |  | 0 | We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make... | WangChiew Tan, Yuliang Li, Pedro Rodriguez, Richard James, Xi Victoria Lin, Alon Y. Halevy, Wentau Yih |  |
| 725 |  |  [Numeric Magnitude Comparison Effects in Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.383) |  | 0 | Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that 4<5) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard... | Raj Sanjay Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, Sashank Varma |  |
| 726 |  |  [Multi-Relational Probabilistic Event Representation Learning via Projected Gaussian Embedding](https://doi.org/10.18653/v1/2023.findings-acl.384) |  | 0 | Event representation learning has been shown beneficial in various downstream tasks. Current event representation learning methods, which mainly focus on capturing the semantics of events via deterministic vector embeddings, have made notable progress. However, they ignore two important properties: the multiple relations between events and the uncertainty within events. In this paper, we propose a novel approach to learning multi-relational probabilistic event embeddings based on contrastive... | Linhai Zhang, Congzhi Zhang, Deyu Zhou |  |
| 727 |  |  [PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.385) |  | 0 | Pragmatic reasoning about another speaker’s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks “do you have a minute?”, instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question... | Peng Qi, Nina Du, Christopher D. Manning, Jing Huang |  |
| 728 |  |  [Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks](https://doi.org/10.18653/v1/2023.findings-acl.386) |  | 0 | Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To... | Lukas Hauzenberger, Shahed Masoudian, Deepak Kumar, Markus Schedl, Navid Rekabsaz |  |
| 729 |  |  [Scientific Fact-Checking: A Survey of Resources and Approaches](https://doi.org/10.18653/v1/2023.findings-acl.387) |  | 0 | The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation,... | Juraj Vladika, Florian Matthes |  |
| 730 |  |  [Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-acl.388) |  | 0 | Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each... | Chiyu Song, Hongliang He, Haofei Yu, Pengfei Fang, Leyang Cui, Zhenzhong Lan |  |
| 731 |  |  [DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.389) |  | 0 | A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our... | Amr Keleg, Walid Magdy |  |
| 732 |  |  [Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.390) |  | 0 | The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, i.e., long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for... | Haozhe Yang, Xianqiang Gao, Jianlong Wu, Tian Gan, Ning Ding, Feijun Jiang, Liqiang Nie |  |
| 733 |  |  [Structure-Discourse Hierarchical Graph for Conditional Question Answering on Long Documents](https://doi.org/10.18653/v1/2023.findings-acl.391) |  | 0 | Conditional question answering on long documents aims to find probable answers and identify conditions that need to be satisfied to make the answers correct over long documents. Existing approaches solve this task by segmenting long documents into multiple sections, and attending information at global and local tokens to predict the answers and corresponding conditions. However, the natural structure of the document and discourse relations between sentences in each document section are ignored,... | Haowei Du, Yansong Feng, Chen Li, Yang Li, Yunshi Lan, Dongyan Zhao |  |
| 734 |  |  [COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements](https://doi.org/10.18653/v1/2023.findings-acl.392) |  | 0 | Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance “your English is very good” may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely... | Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap |  |
| 735 |  |  [Distilling Calibrated Knowledge for Stance Detection](https://doi.org/10.18653/v1/2023.findings-acl.393) |  | 0 | Stance detection aims to determine the position of an author toward a target and provides insights into people’s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which ignores meaningful signals among categories offered by soft labels. In this work, we explore knowledge distillation for stance detection and present a comprehensive analysis. Our contributions are: 1) we... | Yingjie Li, Cornelia Caragea |  |
| 736 |  |  [PTCSpell: Pre-trained Corrector Based on Character Shape and Pinyin for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.findings-acl.394) |  | 0 | Chinese spelling correction (CSC) is a challenging task with the goal of correcting each wrong character in Chinese texts. Incorrect characters in a Chinese text are mainly due to the similar shape and similar pronunciation of Chinese characters. Recently, the paradigm of pre-training and fine-tuning has achieved remarkable success in natural language processing. However, the pre-training objectives in existing methods are not tailored for the CSC task since they neglect the visual and phonetic... | Xiao Wei, Jianbao Huang, Hang Yu, Qian Liu |  |
| 737 |  |  [Disentangling Text Representation With Counter-Template For Unsupervised Opinion Summarization](https://doi.org/10.18653/v1/2023.findings-acl.395) |  | 0 | Approaches for unsupervised opinion summarization are generally based on the reconstruction model and generate a summary by decoding the aggregated representation of inputs. Recent work has shown that aggregating via simple average leads to vector degeneration, generating the generic summary. To tackle the challenge, some approaches select the inputs before aggregating. However, we argue that the selection is too coarse as not all information in each input is equally essential for the summary.... | Yanyue Zhang, Deyu Zhou |  |
| 738 |  |  [Evaluation of Question Generation Needs More References](https://doi.org/10.18653/v1/2023.findings-acl.396) |  | 0 | Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential... | Shinhyeok Oh, Hyojun Go, Hyeongdon Moon, Yunsung Lee, Myeongho Jeong, Hyun Seung Lee, Seungtaek Choi |  |
| 739 |  |  [XtremeCLIP: Extremely Parameter-efficient Tuning for Low-resource Vision Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.397) |  | 0 | Recently, Contrastive Visual-Language Pre-training (CLIP) has demonstrated remarkable capability in various Visual Language Understanding (VLU) tasks. Yet, most CLIP-based methods require tasks-specific designs and sufficient training data. In this paper, we introduce a simple yet efficient paradigm for low-resource VLU named XtremeCLIP, which involves very few trainable parameters to improve the generalization ability of the trained models. In our XtremeCLIP framework, we reformulate a series... | Moming Tang, Chengyu Wang, Jianing Wang, Chuanqi Tan, Songfang Huang, Cen Chen, Weining Qian |  |
| 740 |  |  [FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing](https://doi.org/10.18653/v1/2023.findings-acl.398) |  | 0 | Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generated scene graphs fail to capture the true semantics of the captions or the corresponding images, resulting in a lack of faithfulness. Second, the generated scene graphs have high inconsistency, with... | Zhuang Li, Yuyang Chai, Terry Yue Zhuo, Lizhen Qu, Gholamreza Haffari, Fei Li, Donghong Ji, Quan Hung Tran |  |
| 741 |  |  [Target-Oriented Relation Alignment for Cross-Lingual Stance Detection](https://doi.org/10.18653/v1/2023.findings-acl.399) |  | 0 | Stance detection is an important task in text mining and social media analytics, aiming to automatically identify the user’s attitude toward a specific target from text, and has wide applications in a variety of domains. Previous work on stance detection has mainly focused on monolingual setting. To address the problem of imbalanced language resources, cross-lingual stance detection is proposed to transfer the knowledge learned from a high-resource (source) language (typically English) to... | Ruike Zhang, Nan Xu, Hanxuan Yang, Yuan Tian, Wenji Mao |  |
| 742 |  |  [NonFactS: NonFactual Summary Generation for Factuality Evaluation in Document Summarization](https://doi.org/10.18653/v1/2023.findings-acl.400) |  | 0 | Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document summarization, a document-level Natural Language Inference (NLI) classifier can be used. However, training such a classifier requires large-scale high-quality factual and nonfactual samples. To that... | Amir Soleimani, Christof Monz, Marcel Worring |  |
| 743 |  |  [When to Read Documents or QA History: On Unified and Selective Open-domain QA](https://doi.org/10.18653/v1/2023.findings-acl.401) |  | 0 | This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question q was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up... | Kyungjae Lee, Sangeun Han, Seungwon Hwang, Moontae Lee |  |
| 744 |  |  [Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization](https://doi.org/10.18653/v1/2023.findings-acl.402) |  | 0 | Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly... | Hou Pong Chan, Qi Zeng, Heng Ji |  |
| 745 |  |  [A Multi-dimensional study on Bias in Vision-Language models](https://doi.org/10.18653/v1/2023.findings-acl.403) |  | 0 | In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities. This paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions. When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with... | Gabriele Ruggeri, Debora Nozza |  |
| 746 |  |  [Correction of Errors in Preference Ratings from Automated Metrics for Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.404) |  | 0 | A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreements with human judgments. In this paper, we propose to apply automated metrics for Text Generation in a preference-based evaluation protocol. The protocol features a statistical model that incorporates various levels of uncertainty to account for the error-proneness of the metrics. We show that existing metrics are generally... | Jan Deriu, Pius von Däniken, Don Tuggener, Mark Cieliebak |  |
| 747 |  |  [PEER: Pre-training ELECTRA Extended by Ranking](https://doi.org/10.18653/v1/2023.findings-acl.405) |  | 0 | The BERT model and its variants have made great achievements in many downstream natural language processing tasks. The achievements of these models, however, demand highly expensive pre-training computation cost. To address this pre-training efficiency issue, the ELECTRA model is proposed to use a discriminator to perform replaced token detection (RTD) task, that is, to classify whether each input token is original or replaced by a generator. The RTD task performed by the ELECTRA accelerates... | Ru He, Wei Wang, Songfang Huang, Fei Huang |  |
| 748 |  |  [ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.406) |  | 0 | Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are... | Xuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu, Hongxiang Li, Yuexian Zou |  |
| 749 |  |  [Guiding Dialogue Agents to Complex Semantic Targets by Dynamically Completing Knowledge Graph](https://doi.org/10.18653/v1/2023.findings-acl.407) |  | 0 | In the target-oriented dialogue, the representation and achievement of targets are two interrelated essential issues. In current approaches, the target is typically supposed to be a single object represented as a word, which makes it relatively easy to achieve the target through dialogue with the help of a knowledge graph (KG). However, when the target has complex semantics, the existing knowledge graph is often incomplete in tracking complex semantic relations. This paper studies... | Yue Tan, Bo Wang, Anqi Liu, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou |  |
| 750 |  |  [Chain of Thought Prompting Elicits Knowledge Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.408) |  | 0 | The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA... | Dingjun Wu, Jing Zhang, Xinmei Huang |  |
| 751 |  |  [TACR: A Table Alignment-based Cell Selection Method for HybridQA](https://doi.org/10.18653/v1/2023.findings-acl.409) |  | 0 | Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns, and linked passages to retrieve evidence. Such a challenge made it difficult for previous studies to show their reasoning ability in retrieving answers. To bridge this gap, we propose a novel... | Jian Wu, Yicheng Xu, Yan Gao, JianGuang Lou, Börje Karlsson, Manabu Okumura |  |
| 752 |  |  [Modeling Cross-Cultural Pragmatic Inference with Codenames Duet](https://doi.org/10.18653/v1/2023.findings-acl.410) |  | 0 | Pragmatic reference enables efficient interpersonal communication. Prior work uses simple reference games to test models of pragmatic reasoning, often with unidentified speakers and listeners. In practice, however, speakers’ sociocultural background shapes their pragmatic assumptions. For example, readers of this paper assume NLP refers to Natural Language Processing, and not “Neuro-linguistic Programming.” This work introduces the Cultural Codes dataset, which operationalizes sociocultural... | Omar Shaikh, Caleb Ziems, William Held, Aryan J. Pariani, Fred Morstatter, Diyi Yang |  |
| 753 |  |  [Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games](https://doi.org/10.18653/v1/2023.findings-acl.411) |  | 0 | Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level... | Bolin Lai, Hongxin Zhang, Miao Liu, Aryan J. Pariani, Fiona Ryan, Wenqi Jia, Shirley Anugrah Hayati, James M. Rehg, Diyi Yang |  |
| 754 |  |  [Long to reign over us: A Case Study of Machine Translation and a New Monarch](https://doi.org/10.18653/v1/2023.findings-acl.412) |  | 0 | Novel terminology and changes in terminology are often a challenge for machine translation systems. The passing of Queen Elizabeth II and the accession of King Charles III provide a striking example of translation shift in the real world, particularly in translation contexts that have ambiguity. Examining translation between French and English, we present a focused case-study of translations about King Charles III as produced both by publicly-available MT systems and by a neural machine... | Rebecca Knowles, Samuel Larkin |  |
| 755 |  |  [A Unified Generative Approach to Product Attribute-Value Identification](https://doi.org/10.18653/v1/2023.findings-acl.413) |  | 0 | Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., ⟨Material, Cotton⟩) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values, multi-attribute values, and canonicalized values, which are only partly addressed in existing extraction- and classification-based approaches. Motivated by this, we explore a generative approach to the PAVI task.... | Keiji Shinzato, Naoki Yoshinaga, Yandi Xia, WeiTe Chen |  |
| 756 |  |  [K-UniMorph: Korean Universal Morphology and its Feature Schema](https://doi.org/10.18653/v1/2023.findings-acl.414) |  | 0 | We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how... | Eunkyul Leah Jo, Kyuwon Kim, Xihan Wu, Kyungtae Lim, Jungyeul Park, Chulwoo Park |  |
| 757 |  |  [How does the brain process syntactic structure while listening?](https://doi.org/10.18653/v1/2023.findings-acl.415) |  | 0 | Syntactic parsing is the task of assigning a syntactic structure to a sentence. There are two popular syntactic parsing methods: constituency and dependency parsing. Recent works have used syntactic embeddings based on constituency trees, incremental top-down parsing, and other word syntactic features for brain activity prediction given the text stimuli to study how the syntax structure is represented in the brain’s language network. However, the effectiveness of dependency parse trees or the... | Subba Reddy Oota, Mounika Marreddy, Manish Gupta, Raju S. Bapi |  |
| 758 |  |  [Towards Imperceptible Document Manipulations against Neural Ranking Models](https://doi.org/10.18653/v1/2023.findings-acl.416) |  | 0 | Adversarial attacks have gained traction in order to identify vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce noticeable errors. Moreover, current methods rely heavily on using a well-imitated surrogate NRM to guarantee the attack effect, making them difficult to use in practice. This paper proposes a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans.... | Xuanang Chen, Ben He, Zheng Ye, Le Sun, Yingfei Sun |  |
| 759 |  |  [Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models](https://doi.org/10.18653/v1/2023.findings-acl.417) |  | 0 | Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the “Ask an Expert” framework in which the model is trained with access to an “expert” which it can consult at each turn. Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history. In this work... | Qiang Zhang, Jason Naradowsky, Yusuke Miyao |  |
| 760 |  |  [SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation](https://doi.org/10.18653/v1/2023.findings-acl.418) |  | 0 | Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting of over 10,000 literature reviews and 690,000 papers cited in the reviews. Based on the dataset, we evaluate recent transformer-based summarization models on the literature review generation task,... | Tetsu Kasanishi, Masaru Isonuma, Junichiro Mori, Ichiro Sakata |  |
| 761 |  |  [Revisiting Sample Size Determination in Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.419) |  | 0 | Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation. It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource scenarios. Nevertheless, it remains a largely under-explored area of research in NLP. We therefore explored various techniques for estimating the training sample size necessary to achieve a targeted... | Ernie Chang, Muhammad Hassan Rashid, PinJie Lin, Changsheng Zhao, Vera Demberg, Yangyang Shi, Vikas Chandra |  |
| 762 |  |  [TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition](https://doi.org/10.18653/v1/2023.findings-acl.420) |  | 0 | Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue turn. To solve this problem, we propose to take into account turn-level state Transitions of ESC (TransESC) from three perspectives, including semantics transition, strategy transition and emotion... | Weixiang Zhao, Yanyan Zhao, Shilong Wang, Bing Qin |  |
| 763 |  |  [Residual Prompt Tuning: improving prompt tuning with residual reparameterization](https://doi.org/10.18653/v1/2023.findings-acl.421) |  | 0 | Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We... | Anastasia Razdaibiedina, Yuning Mao, Madian Khabsa, Mike Lewis, Rui Hou, Jimmy Ba, Amjad Almahairi |  |
| 764 |  |  [Attend, Select and Eliminate: Accelerating Multi-turn Response Selection with Dual-attention-based Content Elimination](https://doi.org/10.18653/v1/2023.findings-acl.422) |  | 0 | Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs. To alleviate this problem and make the PLM-based response selection model both effective and efficient, we propose an inference framework together with a post-training strategy that builds upon any pre-trained transformer-based response selection models to accelerate inference by progressively selecting... | Jianxin Liang, Chang Liu, Chongyang Tao, Jiazhan Feng, Dongyan Zhao |  |
| 765 |  |  [Medical Dialogue Generation via Dual Flow Modeling](https://doi.org/10.18653/v1/2023.findings-acl.423) |  | 0 | Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical entities as critical dialogue history information. In this work, we argue that it is also essential to capture the transitions of the medical entities and the doctor’s dialogue acts in each turn, as... | Kaishuai Xu, Wenjun Hou, Yi Cheng, Jian Wang, Wenjie Li |  |
| 766 |  |  [Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition](https://doi.org/10.18653/v1/2023.findings-acl.424) |  | 0 | Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We propose speech2sign-U, a neural network-based approach capable of both character-level and word-level SSR-U. Our approach significantly outperforms baselines directly adapted from unsupervised speech... | Liming Wang, Junrui Ni, Heting Gao, Jialu Li, Kai Chieh Chang, Xulin Fan, Junkai Wu, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 767 |  |  [Distinguishing Address vs. Reference Mentions of Personal Names in Text](https://doi.org/10.18653/v1/2023.findings-acl.425) |  | 0 | Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off. While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context.... | Vinodkumar Prabhakaran, Aida Mostafazadeh Davani, Melissa Ferguson, Stav Atir |  |
| 768 |  |  ["Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors](https://doi.org/10.18653/v1/2023.findings-acl.426) |  | 0 | Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip... | Zhiying Jiang, Matthew Y. R. Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, Jimmy Lin |  |
| 769 |  |  [LR-Sum: Summarization for Less-Resourced Languages](https://doi.org/10.18653/v1/2023.findings-acl.427) |  | 0 | We introduce LR-Sum, a new permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages.LR-Sum contains human-written summaries for 40 languages, many of which are less-resourced. We describe our process for extracting and filtering the dataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022).The source data is public domain newswire collected from from Voice of America websites, and LR-Sum is released... | Chester PalenMichel, Constantine Lignos |  |
| 770 |  |  [RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question](https://doi.org/10.18653/v1/2023.findings-acl.428) |  | 0 | Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic... | Alireza Mohammadshahi, Thomas Scialom, Majid Yazdani, Pouya Yanki, Angela Fan, James Henderson, Marzieh Saeidi |  |
| 771 |  |  [Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.429) |  | 0 | Languages are dynamic entities, where the meanings associated with words constantly change with time. Detecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions. Existing work on semantic variation prediction have predominantly focused on comparing some form of an averaged contextualised representation of a target word computed from a given corpus. However, some of the previously associated meanings of a target word can... | Taichi Aida, Danushka Bollegala |  |
| 772 |  |  [TranSFormer: Slow-Fast Transformer for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.430) |  | 0 | Learning multiscale Transformer models has been evidenced as a viable approach to augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into multiscale Transformer has not yet been explored. In this work, we present a Slow-Fast two-stream learning model, referred to as TranSFormer, which utilizes a “slow” branch to deal with subword sequences... | Bei Li, Yi Jing, Xu Tan, Zhen Xing, Tong Xiao, Jingbo Zhu |  |
| 773 |  |  [Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation](https://doi.org/10.18653/v1/2023.findings-acl.431) |  | 0 | Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly... | Jian Guan, Minlie Huang |  |
| 774 |  |  [Digging out Discrimination Information from Generated Samples for Robust Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.432) |  | 0 | Visual Question Answering (VQA) aims to answer a textual question based on a given image. Nevertheless, recent studies have shown that VQA models tend to capture the biases to answer the question, instead of using the reasoning ability, resulting in poor generalisation ability. To alleviate the issue, some existing methods consider the natural distribution of the data, and construct samples to balance the dataset, achieving remarkable performance. However, these methods may encounter some... | Zhiquan Wen, Yaowei Wang, Mingkui Tan, Qingyao Wu, Qi Wu |  |
| 775 |  |  [Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications](https://doi.org/10.18653/v1/2023.findings-acl.433) |  | 0 | Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups. In this work, we develop and validate an interpretable approach for measuring scholarly jargon from text. Expanding the scope of prior work which focuses on word types, we use word sense induction to also identify words that are widespread but overloaded with different meanings across fields. We then estimate the... | Li Lucy, Jesse Dodge, David Bamman, Katherine A. Keith |  |
| 776 |  |  [Trade-Offs Between Fairness and Privacy in Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.434) |  | 0 | Protecting privacy in contemporary NLP models is gaining in importance. So does the need to mitigate social biases of such models. But can we have both at the same time? Existing research suggests that privacy preservation comes at the price of worsening biases in classification tasks. In this paper, we explore the extent to which this tradeoff really holds when we incorporate both privacy preservation and de-biasing techniques into training text generation models. How does improving the model... | Cleo Matzken, Steffen Eger, Ivan Habernal |  |
| 777 |  |  [CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset](https://doi.org/10.18653/v1/2023.findings-acl.435) |  | 0 | The cross-domain text-to-SQL task aims to build a system that can parse user questions into SQL on complete unseen databases, and the single-domain text-to-SQL task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications. To this end, we introduce the cross-schema text-to-SQL task, where the databases of evaluation data are different from that in the training data but come from the same domain. Furthermore, we present... | Hanchong Zhang, Jieyu Li, Lu Chen, Ruisheng Cao, Yunyan Zhang, Yu Huang, Yefeng Zheng, Kai Yu |  |
| 778 |  |  [Silver Syntax Pre-training for Cross-Domain Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.436) |  | 0 | Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically be repeated for each new domain. An intermediate training step on data from related tasks has shown to be beneficial across many NLP tasks. However, this setup still requires supplementary annotated... | Elisa Bassignana, Filip Ginter, Sampo Pyysalo, Rob van der Goot, Barbara Plank |  |
| 779 |  |  [FastDiff 2: Revisiting and Incorporating GANs and Diffusion Models in High-Fidelity Speech Synthesis](https://doi.org/10.18653/v1/2023.findings-acl.437) |  | 0 | Generative adversarial networks (GANs) and denoising diffusion probabilistic models (DDPMs) have recently achieved impressive performances in image and audio synthesis. After revisiting their success in conditional speech synthesis, we find that 1) GANs sacrifice sample diversity for quality and speed, 2) diffusion models exhibit outperformed sample quality and diversity at a high computational cost, where achieving high-quality, fast, and diverse speech synthesis challenges all neural... | Rongjie Huang, Yi Ren, Ziyue Jiang, Chenye Cui, Jinglin Liu, Zhou Zhao |  |
| 780 |  |  [Uncovering Hidden Consequences of Pre-training Objectives in Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2023.findings-acl.438) |  | 0 | Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance. Yet the design of these pre-training objectives leads to behavioural differences that can be uncovered with specific manipulations. We reproduce a recently proposed zero-shot control method and find that it is only successful on a subset of models. To understand what causes the difference in its effectiveness, we perform... | Tannon Kew, Rico Sennrich |  |
| 781 |  |  [Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity](https://doi.org/10.18653/v1/2023.findings-acl.439) |  | 0 | Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple... | Katharina Hämmerl, Alina Fastowski, Jindrich Libovický, Alexander Fraser |  |
| 782 |  |  [Revisiting Sentence Union Generation as a Testbed for Text Consolidation](https://doi.org/10.18653/v1/2023.findings-acl.440) |  | 0 | Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information. However, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models’ consolidation capabilities. In this paper, we suggest... | Eran Hirsch, Valentina Pyatkin, Ruben Wolhandler, Avi Caciularu, Asi Shefer, Ido Dagan |  |
| 783 |  |  [Distilling Reasoning Capabilities into Smaller Language Models](https://doi.org/10.18653/v1/2023.findings-acl.441) |  | 0 | Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In... | Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan |  |
| 784 |  |  [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment](https://doi.org/10.18653/v1/2023.findings-acl.442) |  | 0 | The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing... | Ruiqi Li, Rongjie Huang, Lichao Zhang, Jinglin Liu, Zhou Zhao |  |
| 785 |  |  [A New Task and Dataset on Detecting Attacks on Human Rights Defenders](https://doi.org/10.18653/v1/2023.findings-acl.443) |  | 0 | The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new... | Shihao Ran, Di Lu, Aoife Cahill, Joel R. Tetreault, Alejandro Jaimes |  |
| 786 |  |  [Improving Language Model Integration for Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.444) |  | 0 | The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech... | Christian Herold, Yingbo Gao, Mohammad Zeineldeen, Hermann Ney |  |
| 787 |  |  [Type Enhanced BERT for Correcting NER Errors](https://doi.org/10.18653/v1/2023.findings-acl.445) |  | 0 | We introduce the task of correcting named entity recognition (NER) errors without re-training model. After an NER model is trained and deployed in production,it makes prediction errors, which usually need to be fixed quickly. To address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types. And then, we propose type enhanced BERT (TyBERT),a method that integrates the named entity’s type information into BERT by an adapter layer. When... | Kuai Li, Chen Chen, Tao Yang, Tianming Du, Peijie Yu, Dong Du, Feng Zhang |  |
| 788 |  |  [Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework](https://doi.org/10.18653/v1/2023.findings-acl.446) |  | 0 | Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision... | Lifan Yuan, Yichi Zhang, Yangyi Chen, Wei Wei |  |
| 789 |  |  [DUB: Discrete Unit Back-translation for Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.447) |  | 0 | How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous... | Dong Zhang, Rong Ye, Tom Ko, Mingxuan Wang, Yaqian Zhou |  |
| 790 |  |  [Knowledge Graph Embeddings using Neural Ito Process: From Multiple Walks to Stochastic Trajectories](https://doi.org/10.18653/v1/2023.findings-acl.448) |  | 0 | Knowledge graphs mostly exhibit a mixture of branching relations, e.g., hasFriend, and complex structures, e.g., hierarchy and loop. Most knowledge graph embeddings have problems expressing them, because they model a specific relation r from a head h to tails by starting at the node embedding of h and transitioning deterministically to exactly one other point in the embedding space. We overcome this issue in our novel framework ItCAREToE by modeling relations between nodes by relation-specific,... | Mojtaba Nayyeri, Bo Xiong, Majid Mohammadi, Mst. Mahfuja Akter, Mirza Mohtashim Alam, Jens Lehmann, Steffen Staab |  |
| 791 |  |  [Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.449) |  | 0 | Grammatical Error Correction (GEC) is the task of correcting errorful sentences into grammatically correct, semantically consistent, and coherent sentences. Popular GEC models either use large-scale synthetic corpora or use a large number of human-designed rules. The former is costly to train, while the latter requires quite a lot of human expertise. In recent years, AMR, a semantic representation framework, has been widely used by many natural language tasks due to its completeness and... | Hejing Cao, Dongyan Zhao |  |
| 792 |  |  [Prediction and Calibration: Complex Reasoning over Knowledge Graph with Bi-directional Directed Acyclic Graph Neural Network](https://doi.org/10.18653/v1/2023.findings-acl.450) |  | 0 | Answering complex logical queries is a challenging task for knowledge graph (KG) reasoning. Recently, query embedding (QE) has been proposed to encode queries and entities into the same vector space, and obtain answers based on numerical computation. However, such models obtain the node representations of a query only based on its predecessor nodes, which ignore the information contained in successor nodes. In this paper, we proposed a Bi-directional Directed Acyclic Graph neural network... | Yao Xu, Shizhu He, Li Cai, Kang Liu, Jun Zhao |  |
| 793 |  |  [Prompt-Based Metric Learning for Few-Shot NER](https://doi.org/10.18653/v1/2023.findings-acl.451) |  | 0 | Few-shot named entity recognition (NER) targets generalizing to unseen labels and/or domains with few labeled examples. Existing metric learning methods compute token-level similarities between query and support sets, but are not able to fully incorporate label semantics into modeling. To address this issue, we propose a simple method to largely improve metric learning for NER: 1) multiple prompt schemas are designed to enhance label semantics; 2) we propose a novel architecture to effectively... | Yanru Chen, Yanan Zheng, Zhilin Yang |  |
| 794 |  |  [OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.452) |  | 0 | Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively,... | Xueqing Wu, Sha Li, Heng Ji |  |
| 795 |  |  [I run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets](https://doi.org/10.18653/v1/2023.findings-acl.453) |  | 0 | A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as “like” or “as”. The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually... | Longxuan Ma, Weinan Zhang, Shuhan Zhou, Churui Sun, Changxin Ke, Ting Liu |  |
| 796 |  |  [Controllable Conversation Generation with Conversation Structures via Diffusion Models](https://doi.org/10.18653/v1/2023.findings-acl.454) |  | 0 | Generating coherent conversation is an important and challenging long text generation task, as it has various applications such as daily entertainment, children education or building conversational AI to facilitate human-computer interaction. However, current generation models often fail to effectively utilize rich linguistic and world knowledge to generate conversations just like human. In this work, we introduce a novel conversation generation framework to effectively incorporate human... | Jiaao Chen, Diyi Yang |  |
| 797 |  |  [Few-shot Low-resource Knowledge Graph Completion with Reinforced Task Generation](https://doi.org/10.18653/v1/2023.findings-acl.455) |  | 0 | Despite becoming a prevailing paradigm for organizing knowledge, most knowledge graphs (KGs) suffer from the low-resource issue due to the deficiency of data sources. The enrichment of KGs by automatic knowledge graph completion is impeded by the intrinsic long-tail property of KGs. In spite of their prosperity, existing few-shot learning-based models have difficulty alleviating the impact of the long-tail issue on low-resource KGs because of the lack of training tasks. To tackle the... | Shichao Pei, Qiannan Zhang, Xiangliang Zhang |  |
| 798 |  |  [Incomplete Utterance Rewriting as Sequential Greedy Tagging](https://doi.org/10.18653/v1/2023.findings-acl.456) |  | 0 | The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all... | Yunshan Chen |  |
| 799 |  |  [Exploiting Commonsense Knowledge about Objects for Visual Activity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.457) |  | 0 | Situation recognition is the task of recognizing the activity depictedin an image, including the people and objects involved. Previousmodels for this task typically train a classifier to identify theactivity using a backbone image feature extractor. We propose thatcommonsense knowledge about the objects depicted in an image can alsobe a valuable source of information for activity identification. Previous NLP research has argued that knowledge about the prototypicalfunctions of physical objects... | Tianyu Jiang, Ellen Riloff |  |
| 800 |  |  [Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.458) |  | 0 | Temporal Knowledge Graph Completion aims to complete missing entities or relations under temporal constraints. Previous tensor decomposition-based models for TKGC only independently consider the combination of one single relation with one single timestamp, ignoring the global nature of the embedding. We propose a Frequency Attention (FA) model to capture the global temporal dependencies between one relation and the entire timestamp. Specifically, we use Discrete Cosine Transform (DCT) to... | Likang Xiao, Richong Zhang, Zijie Chen, Junfan Chen |  |
| 801 |  |  [Another Dead End for Morphological Tags? Perturbed Inputs and Parsing](https://doi.org/10.18653/v1/2023.findings-acl.459) |  | 0 | The usefulness of part-of-speech tags for parsing has been heavily questioned due to the success of word-contextualized parsers. Yet, most studies are limited to coarse-grained tags and high quality written content; while we know little about their influence when it comes to models in production that face lexical errors. We expand these setups and design an adversarial attack to verify if the use of morphological information by parsers: (i) contributes to error propagation or (ii) if on the... | Alberto MuñozOrtiz, David Vilares |  |
| 802 |  |  [HeGeL: A Novel Dataset for Geo-Location from Hebrew Text](https://doi.org/10.18653/v1/2023.findings-acl.460) |  | 0 | The task of textual geolocation — retrieving the coordinates of a place based on a free-form language description — calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are... | Tzuf PazArgaman, Tal Bauman, Itai Mondshine, Itzhak Omer, Sagi Dalyot, Reut Tsarfaty |  |
| 803 |  |  [Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making](https://doi.org/10.18653/v1/2023.findings-acl.461) |  | 0 | Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step. In this paper, we formally model the adversarial attack task on PLMs as a sequential decision-making problem, where the whole attack process is sequential with two... | Xuanjie Fang, Sijie Cheng, Yang Liu, Wei Wang |  |
| 804 |  |  [Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization](https://doi.org/10.18653/v1/2023.findings-acl.462) |  | 0 | Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the... | Liang Chen, Hongru Wang, Yang Deng, WaiChung Kwan, Zezhong Wang, KamFai Wong |  |
| 805 |  |  [Cost-effective Distillation of Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.463) |  | 0 | Knowledge distillation (KD) involves training a small “student” model to replicate the strong performance of a high-capacity “teacher” model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD... | Sayantan Dasgupta, Trevor Cohn, Timothy Baldwin |  |
| 806 |  |  [Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.464) |  | 0 | Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune... | Namo Bang, Jeehyun Lee, MyoungWan Koo |  |
| 807 |  |  [I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors](https://doi.org/10.18653/v1/2023.findings-acl.465) |  | 0 | Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL⋅E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the... | Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan |  |
| 808 |  |  [Text Augmentation Using Dataset Reconstruction for Low-Resource Classification](https://doi.org/10.18653/v1/2023.findings-acl.466) |  | 0 | In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification.... | Adir Rahamim, Guy Uziel, Esther Goldbraich, Ateret AnabyTavor |  |
| 809 |  |  [LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning](https://doi.org/10.18653/v1/2023.findings-acl.467) |  | 0 | A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as ‘always’ or ‘rarely’) and mimicking humans in compositionally learning complex tasks. Here, we present LaSQuE, a method that can learn zero-shot classifiers from language explanations by using three new strategies -... | Sayan Ghosh, Rakesh R. Menon, Shashank Srivastava |  |
| 810 |  |  [Learned Adapters Are Better Than Manually Designed Adapters](https://doi.org/10.18653/v1/2023.findings-acl.468) |  | 0 | Recently, a series of works have looked into further improving the adapter-based tuning by manually designing better adapter architectures. Understandably, these manually designed solutions are sub-optimal. In this work, we propose the Learned Adapter framework to automatically learn the optimal adapter architectures for better task adaptation of pre-trained models (PTMs). First, we construct a unified search space for adapter architecture designs. In terms of the optimization method on the... | Yuming Zhang, Peng Wang, Ming Tan, Wei Zhu |  |
| 811 |  |  [Automatic Identification of Code-Switching Functions in Speech Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.469) |  | 0 | Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications. Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new topic, etc. The function of code-switching may be quite useful for the analysis of linguists, cognitive scientists, speech therapists, and others, but is not readily apparent. To remedy this situation,... | Ritu Belani, Jeffrey Flanigan |  |
| 812 |  |  [Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets](https://doi.org/10.18653/v1/2023.findings-acl.470) |  | 0 | Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of multiple domains. In addition, we consider a practical and challenging scenario, where NER datasets of different platforms of federated learning are annotated with heterogeneous tag sets, i.e., different... | Rui Wang, Tong Yu, Junda Wu, Handong Zhao, Sungchul Kim, Ruiyi Zhang, Subrata Mitra, Ricardo Henao |  |
| 813 |  |  [Interpreting Sentiment Composition with Latent Semantic Tree](https://doi.org/10.18653/v1/2023.findings-acl.471) |  | 0 | As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and sentiment ones, which are intrinsically suboptimal in our view. To address this, we propose semantic tree, a new tree form capable of interpreting the sentiment composition in a principled way.... | Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jiansong Chen, Jun Zhao, Kang Liu |  |
| 814 |  |  [Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.472) |  | 0 | Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model... | Yuhui Zhang, Michihiro Yasunaga, Zhengping Zhou, Jeff Z. HaoChen, James Zou, Percy Liang, Serena Yeung |  |
| 815 |  |  [Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents](https://doi.org/10.18653/v1/2023.findings-acl.473) |  | 0 | We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new classification categories could potentially emerge. We focus exclusively... | Muhammad Khalifa, Yogarshi Vyas, Shuai Wang, Graham Horwood, Sunil Mallya, Miguel Ballesteros |  |
| 816 |  |  [Extracting Shopping Interest-Related Product Types from the Web](https://doi.org/10.18653/v1/2023.findings-acl.474) |  | 0 | Recommending a diversity of product types (PTs) is important for a good shopping experience when customers are looking for products around their high-level shopping interests (SIs) such as hiking. However, the SI-PT connection is typically absent in e-commerce product catalogs and expensive to construct manually due to the volume of potential SIs, which prevents us from establishing a recommender with easily accessible knowledge systems. To establish such connections, we propose to extract PTs... | Yinghao Li, Colin Lockard, Prashant Shiralkar, Chao Zhang |  |
| 817 |  |  [Multilingual Pre-training with Self-supervision from Global Co-occurrence Information](https://doi.org/10.18653/v1/2023.findings-acl.475) |  | 0 | Global co-occurrence information is the primary source of structural information on multilingual corpora, and we find that analogical/parallel compound words across languages have similar co-occurrence counts/frequencies (normalized) giving weak but stable self-supervision for cross-lingual transfer. Following the observation, we aim at associating contextualized representations with relevant (contextualized) representations across languages with the help of co-occurrence counts. The result is... | Xi Ai, Bin Fang |  |
| 818 |  |  [Low-Rank Updates of pre-trained Weights for Multi-Task Learning](https://doi.org/10.18653/v1/2023.findings-acl.476) |  | 0 | Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor. We show that low-rank updates in the canonical polyadic tensor decomposition of... | Alexandre Audibert, MassihReza Amini, Konstantin Usevich, Marianne Clausel |  |
| 819 |  |  [Sequential Integrated Gradients: a simple but effective method for explaining language models](https://doi.org/10.18653/v1/2023.findings-acl.477) |  | 0 | Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the... | Joseph Enguehard |  |
| 820 |  |  [DiffuDetox: A Mixed Diffusion Model for Text Detoxification](https://doi.org/10.18653/v1/2023.findings-acl.478) |  | 0 | Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative... | Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner |  |
| 821 |  |  [Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.479) |  | 0 | Extractive summarization aims to select a set of salient sentences from the source document to form a summary. Context information has been considered one of the key factors for this task. Meanwhile, there also exist other pattern factors that can identify sentence importance, such as sentence position or certain n-gram tokens. However, such pattern information is only effective in specific datasets or domains and can not be generalized like the context information when there only exists... | Ruifeng Yuan, Shichao Sun, Zili Wang, Ziqiang Cao, Wenjie Li |  |
| 822 |  |  [Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers](https://doi.org/10.18653/v1/2023.findings-acl.480) |  | 0 | This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled thinking) are decoupled to capture different levels of cognition. Upon the top of the representation module, the pre-trained reasoning modules are modular and professional in specific and fundamental... | Wanjun Zhong, Tingting Ma, Jiahai Wang, Jian Yin, Tiejun Zhao, ChinYew Lin, Nan Duan |  |
| 823 |  |  [Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking](https://doi.org/10.18653/v1/2023.findings-acl.481) |  | 0 | We propose a simple approach for the abstractive summarization of long legal opinions that takes into account the argument structure of the document. Legal opinions often contain complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main points of the legal opinion. Our approach involves using argument role information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document’s... | Mohamed Elaraby, Yang Zhong, Diane J. Litman |  |
| 824 |  |  [Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation](https://doi.org/10.18653/v1/2023.findings-acl.482) |  | 0 | Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When used as an encoder, a transformer produces contextual representation of words in the input sentence. In this work, we propose a new model of contextual word representation, not from a neural... | Haoyi Wu, Kewei Tu |  |
| 825 |  |  [Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data](https://doi.org/10.18653/v1/2023.findings-acl.483) |  | 0 | Self-training has been shown to be helpful in addressing data scarcity for many domains, including vision, speech, and language. Specifically, self-training, or pseudo-labeling, labels unsupervised data and adds that to the training pool. In this work, we investigate and use pseudo-labeling for a recently proposed novel setup: joint transcription and translation of speech, which suffers from an absence of sufficient parallel data resources. We show that under such data-deficient circumstances,... | Mozhdeh Gheini, Tatiana Likhomanenko, Matthias Sperber, Hendra Setiawan |  |
| 826 |  |  [Word-level Prefix/Suffix Sense Detection: A Case Study on Negation Sense with Few-shot Learning](https://doi.org/10.18653/v1/2023.findings-acl.484) |  | 0 | Morphological analysis is an important research issue in the field of natural language processing. In this study, we propose a context-free morphological analysis task, namely word-level prefix/suffix sense detection, which deals with the ambiguity of sense expressed by prefix/suffix. To research this novel task, we first annotate a corpus with prefixes/suffixes expressing negation (e.g., il-, un-, -less) and then propose a novel few-shot learning approach that applies an input-augmentation... | Yameng Li, Zicheng Li, Ying Chen, Shoushan Li |  |
| 827 |  |  [End-to-End Simultaneous Speech Translation with Differentiable Segmentation](https://doi.org/10.18653/v1/2023.findings-acl.485) |  | 0 | End-to-end simultaneous speech translation (SimulST) outputs translation while receiving the streaming speech inputs (a.k.a. streaming speech translation), and hence needs to segment the speech inputs and then translate based on the current received speech. However, segmenting the speech inputs at unfavorable moments can disrupt the acoustic integrity and adversely affect the performance of the translation model. Therefore, learning to segment the speech inputs at those moments that are... | Shaolei Zhang, Yang Feng |  |
| 828 |  |  [Joint Generator-Ranker Learning for Natural Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.486) |  | 0 | Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR... | Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, Weizhu Chen |  |
| 829 |  |  [Multilingual Sequence-to-Sequence Models for Hebrew NLP](https://doi.org/10.18653/v1/2023.findings-acl.487) |  | 0 | Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as... | Matan Eyal, Hila Noga, Roee Aharoni, Idan Szpektor, Reut Tsarfaty |  |
| 830 |  |  [Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints](https://doi.org/10.18653/v1/2023.findings-acl.488) |  | 0 | Multilingual Knowledge Graph Completion (mKGC) aim at solving queries in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs... | Ran Song, Shizhu He, Shengxiang Gao, Li Cai, Kang Liu, Zhengtao Yu, Jun Zhao |  |
| 831 |  |  [Towards Better Hierarchical Text Classification with Data Generation](https://doi.org/10.18653/v1/2023.findings-acl.489) |  | 0 | Hierarchical text classification (HTC) focuses on classifying one text into multiple labels, which are organized as a hierarchical taxonomy. Due to its wide involution in realistic scenarios, HTC attracts long-term attention from both industry and academia. However, the high cost of hierarchical multi-label annotation makes HTC suffer from the data scarcity problem. In view of the difficulty in balancing the controllability of multiple structural labels and text diversity, automatically... | Yue Wang, Dan Qiao, Juntao Li, Jinxiong Chang, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Min Zhang |  |
| 832 |  |  [History repeats: Overcoming catastrophic forgetting for event-centric temporal knowledge graph completion](https://doi.org/10.18653/v1/2023.findings-acl.490) |  | 0 | Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could incorporate fine-tuning to existing methods to allow them to adapt to evolving TKG data, this can lead to forgetting previously learned patterns. Alternatively, retraining the model with the entire updated... | Mehrnoosh Mirtaheri, Mohammad Rostami, Aram Galstyan |  |
| 833 |  |  [Multi-Agent Language Learning: Symbolic Mapping](https://doi.org/10.18653/v1/2023.findings-acl.491) |  | 0 | The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a ‘natural’ way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a curriculum learning... | Yicheng Feng, Zongqing Lu |  |
| 834 |  |  [Scaling Laws for BERT in Low-Resource Settings](https://doi.org/10.18653/v1/2023.findings-acl.492) |  | 0 | Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, dataset size, and computation costs focused on the large scale. In contrast, we analyze the effect those variables have on the performance of language models in constrained settings, by building three... | Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi, Rodrigo Agerri, Aitor Soroa |  |
| 835 |  |  [Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.493) |  | 0 | Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language... | Wenjie Xu, Ben Liu, Miao Peng, Xu Jia, Min Peng |  |
| 836 |  |  [Is Continuous Prompt a Combination of Discrete Prompts? Towards a Novel View for Interpreting Continuous Prompts](https://doi.org/10.18653/v1/2023.findings-acl.494) |  | 0 | The broad adoption of continuous prompts has brought state-of-the-art results on a diverse array of downstream natural language processing (NLP) tasks. Nonetheless, little attention has been paid to the interpretability and transferability of continuous prompts. Faced with the challenges, we investigate the feasibility of interpreting continuous prompts as the weighting of discrete prompts by jointly optimizing prompt fidelity and downstream fidelity. Our experiments show that: (1) one can... | Tianjie Ju, Yubin Zheng, Hanyi Wang, Haodong Zhao, Gongshen Liu |  |
| 837 |  |  [Putting Natural in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.495) |  | 0 | Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent... | Grzegorz Chrupala |  |
| 838 |  |  [Impact of Adversarial Training on Robustness and Generalizability of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.496) |  | 0 | Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding... | Enes Altinisik, Hassan Sajjad, Husrev T. Sencar, Safa Messaoud, Sanjay Chawla |  |
| 839 |  |  [Benchmarking Diverse-Modal Entity Linking with Generative Models](https://doi.org/10.18653/v1/2023.findings-acl.497) |  | 0 | Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three... | Sijia Wang, Alexander Hanbo Li, Henghui Zhu, Sheng Zhang, Pramuditha Perera, ChungWei Hang, Jie Ma, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Bing Xiang, Patrick Ng |  |
| 840 |  |  [Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-acl.498) |  | 0 | In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker’s emotion. Besides, external commonsense knowledge has been applied to enhance the system’s understandings of the speaker’s situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion,... | Hua Cai, Xuli Shen, Qing Xu, Weilin Shen, Xiaomei Wang, Weifeng Ge, Xiaoqing Zheng, Xiangyang Xue |  |
| 841 |  |  [Additive manifesto decomposition: A policy domain aware method for understanding party positioning](https://doi.org/10.18653/v1/2023.findings-acl.499) |  | 0 | Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense the relationship between a pair of parties into a single figure, their similarity. In aggregating over all policy domains (e.g., health or foreign policy), they do not provide any qualitative insights... | Tanise Ceron, Dmitry Nikolaev, Sebastian Padó |  |
| 842 |  |  [Similarizing the Influence of Words with Contrastive Learning to Defend Word-level Adversarial Text Attack](https://doi.org/10.18653/v1/2023.findings-acl.500) |  | 0 | Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words. Previous search methods for word-level attacks assume that the information in the important words is more influential on prediction than unimportant words. In this paper, motivated by this assumption, we propose a self-supervised regularization method for Similarizing the Influence of Words with Contrastive Learning (SIWCon) that... | Pengwei Zhan, Jing Yang, He Wang, Chao Zheng, Xiao Huang, Liming Wang |  |
| 843 |  |  [Responsibility Perspective Transfer for Italian Femicide News](https://doi.org/10.18653/v1/2023.findings-acl.501) |  | 0 | Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader’s perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the... | Gosse Minnema, Huiyuan Lai, Benedetta Muscato, Malvina Nissim |  |
| 844 |  |  [Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models](https://doi.org/10.18653/v1/2023.findings-acl.502) |  | 0 | Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this work, we are the first to investigate how multimodal models handle diverse gender identities. Concretely, we conduct a thorough analysis in which we compare the output of three image generation... | Eddie L. Ungless, Björn Ross, Anne Lauscher |  |
| 845 |  |  [Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding](https://doi.org/10.18653/v1/2023.findings-acl.503) |  | 0 | The Wav2Vec and its variants have achieved unprecedented success in computational auditory and speech processing. Meanwhile, neural encoding studies that integrate the superb representation capability of Wav2Vec and link those representations to brain activities have provided novel insights into a fundamental question of how auditory and speech processing unfold in the human brain. Without an explicit definition, most existing studies treat each transformer encoding layer in Wav2Vec as a single... | Mengyue Zhou, Xu Liu, David Liu, Zihao Wu, Zhengliang Liu, Lin Zhao, Dajiang Zhu, Lei Guo, Junwei Han, Tianming Liu, Xintao Hu |  |
| 846 |  |  [Deeply Coupled Cross-Modal Prompt Learning](https://doi.org/10.18653/v1/2023.findings-acl.504) |  | 0 | Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method... | Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo, Fei Tan |  |
| 847 |  |  [Opinion Tree Parsing for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.505) |  | 0 | Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. These models avoid explicit modeling of structure between sentiment elements, which are succinct yet lack desirable properties such as structure well-formedness guarantees or built-in elements alignments. In this study, we propose an opinion tree parsing model, aiming to parse all the sentiment elements from an opinion tree, which can explicitly... | Xiaoyi Bao, Xiaotong Jiang, Zhongqing Wang, Yue Zhang, Guodong Zhou |  |
| 848 |  |  [CoMix: Guide Transformers to Code-Mix using POS structure and Phonetics](https://doi.org/10.18653/v1/2023.findings-acl.506) |  | 0 | Code-mixing is ubiquitous in multilingual societies, which makes it vital to build models for code-mixed data to power human language interfaces. Existing multilingual transformer models trained on pure corpora lack the ability to intermix words of one language into the structure of another. These models are also not robust to orthographic variations. We propose CoMixCoMix is not a trademark and only used to refer to our models for code-mixed data for presentational brevity., a pretraining... | Gaurav Arora, Srujana Merugu, Vivek Sembium |  |
| 849 |  |  [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://doi.org/10.18653/v1/2023.findings-acl.507) |  | 0 | Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that... | ChengYu Hsieh, ChunLiang Li, ChihKuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, ChenYu Lee, Tomas Pfister |  |
| 850 |  |  [Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech](https://doi.org/10.18653/v1/2023.findings-acl.508) |  | 0 | Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances prosody modeling and... | Rongjie Huang, Chunlei Zhang, Yi Ren, Zhou Zhao, Dong Yu |  |
| 851 |  |  [Duplex Diffusion Models Improve Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.509) |  | 0 | Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a multitask-learned model with low efficiency and inferior performance. In this paper, we propose a duplex diffusion model that applies diffusion probabilistic models to both sides of a reversible duplex Conformer, so... | Xianchao Wu |  |
| 852 |  |  [Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.510) |  | 0 | Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all... | Yuxin Jiang, Linhan Zhang, Wei Wang |  |
| 853 |  |  [PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.511) |  | 0 | While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of... | Zhuocheng Gong, Jiahao Liu, Qifan Wang, Yang Yang, Jingang Wang, Wei Wu, Yunsen Xian, Dongyan Zhao, Rui Yan |  |
| 854 |  |  [Synthetic Pre-Training Tasks for Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.512) |  | 0 | Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation.... | Zexue He, Graeme Blackwood, Rameswar Panda, Julian J. McAuley, Rogério Feris |  |
| 855 |  |  [IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.513) |  | 0 | In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further... | Zihang Xu, Ziqing Yang, Yiming Cui, Shijin Wang |  |
| 856 |  |  [Adversarial Training for Low-Resource Disfluency Correction](https://doi.org/10.18653/v1/2023.findings-acl.514) |  | 0 | Disfluencies commonly occur in conversational speech. Speech with disfluencies can result in noisy Automatic Speech Recognition (ASR) transcripts, which affects downstream tasks like machine translation. In this paper, we propose an adversarially-trained sequence-tagging model for Disfluency Correction (DC) that utilizes a small amount of labeled real disfluent data in conjunction with a large amount of unlabeled data. We show the benefit of our proposed technique, which crucially depends on... | Vineet Bhat, Preethi Jyothi, Pushpak Bhattacharyya |  |
| 857 |  |  [Computer says "No": The Case Against Empathetic Conversational AI](https://doi.org/10.18653/v1/2023.findings-acl.515) |  | 0 | Emotions are an integral part of human cognition and they guide not only our understanding of the world but also our actions within it. As such, whether we soothe or flame an emotion is not inconsequential. Recent work in conversational AI has focused on responding empathetically to users, validating and soothing their emotions without a real basis. This AI-aided emotional regulation can have negative consequences for users and society, tending towards a one-noted happiness defined as only the... | Alba Curry, Amanda Cercas Curry |  |
| 858 |  |  [Stubborn Lexical Bias in Data and Models](https://doi.org/10.18653/v1/2023.findings-acl.516) |  | 0 | In NLP, recent work has seen increased focus on spurious correlations between various features and labels in training data, and how these influence model behavior. However, the presence and effect of such correlations are typically examined feature by feature. We investigate the cumulative impact on a model of many such intersecting features. Using a new statistical method, we examine whether such spurious patterns in data appear in models trained on the data. We select two tasks— natural... | Sofia Serrano, Jesse Dodge, Noah A. Smith |  |
| 859 |  |  [Distilling Efficient Language-Specific Models for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-acl.517) |  | 0 | Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs’ language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which... | Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, Ivan Vulic |  |
| 860 |  |  [An Extensive Exploration of Back-Translation in 60 Languages](https://doi.org/10.18653/v1/2023.findings-acl.518) |  | 0 | Back-translation is a data augmentation technique that has been shown to improve model quality through the creation of synthetic training bitext. Early studies showed the promise of the technique and follow on studies have produced additional refinements. We have undertaken a broad investigation using back-translation to train models from 60 languages into English; the majority of these languages are considered moderate- or low-resource languages. We observed consistent gains, though compared... | Paul McNamee, Kevin Duh |  |
| 861 |  |  [AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.519) |  | 0 | Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be... | Ru Zhou, Wenya Guo, Xumeng Liu, Shenglong Yu, Ying Zhang, Xiaojie Yuan |  |
| 862 |  |  [Forecasting Earnings Surprises from Conference Call Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.520) |  | 0 | There is a multitude of textual data relevant to the financial markets, spanning genres such as financial news, earnings conference calls, and social media posts. Earnings conference calls are one of the most important to information flow as they reflect a direct communication between company executives, financial analysts, and large shareholders. Since these calls contain content that is forward-looking in nature, they can be used to forecast the future performance of the company relative to... | Ross Koval, Nicholas Andrews, Xifeng Yan |  |
| 863 |  |  [MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.521) |  | 0 | Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker’s gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an... | Sebastian T. Vincent, Robert Flynn, Carolina Scarton |  |
| 864 |  |  [Evaluation for Change](https://doi.org/10.18653/v1/2023.findings-acl.522) |  | 0 | Evaluation is the central means for assessing, understanding, and communicating about NLP models. In this position paper, we argue evaluation should be more than that: it is a force for driving change, carrying a sociological and political character beyond its technical dimensions. As a force, evaluation’s power arises from its adoption: under our view, evaluation succeeds when it achieves the desired change in the field. Further, by framing evaluation as a force, we consider how it competes... | Rishi Bommasani |  |
| 865 |  |  [Reconstruction Probing](https://doi.org/10.18653/v1/2023.findings-acl.523) |  | 0 | We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decontextualized lexical prior of the model. This comparison can be understood as quantifying the contribution of... | Najoung Kim, Jatin Khilnani, Alex Warstadt, Abdelrahim Qaddoumi |  |
| 866 |  |  [Towards Distribution-shift Robust Text Classification of Emotional Content](https://doi.org/10.18653/v1/2023.findings-acl.524) |  | 0 | Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are usually created ad-hoc and whose data distribution often differs from the one in real applications, showing significant performance degradation in real-world scenarios. We perform an extensive... | Luana Bulla, Aldo Gangemi, Misael Mongiovì |  |
| 867 |  |  [Multi-lingual and Multi-cultural Figurative Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.525) |  | 0 | Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, {pasted... | Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Aremu Anuoluwapo, Perez Ogayo, Graham Neubig |  |
| 868 |  |  [Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table](https://doi.org/10.18653/v1/2023.findings-acl.526) |  | 0 | Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring over multiple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon... | Sunjun Kweon, Yeonsu Kwon, Seonhee Cho, Yohan Jo, Edward Choi |  |
| 869 |  |  [What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](https://doi.org/10.18653/v1/2023.findings-acl.527) |  | 0 | Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations – even... | Jane Pan, Tianyu Gao, Howard Chen, Danqi Chen |  |
| 870 |  |  [Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.528) |  | 0 | Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with... | Ercong Nie, Sheng Liang, Helmut Schmid, Hinrich Schütze |  |
| 871 |  |  [Unsupervised Summarization Re-ranking](https://doi.org/10.18653/v1/2023.findings-acl.529) |  | 0 | With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to... | Mathieu Ravaut, Shafiq R. Joty, Nancy F. Chen |  |
| 872 |  |  [GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.530) |  | 0 | Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation,... | Zhihua Wen, Zhiliang Tian, Zhen Huang, Yuxin Yang, Zexin Jian, Changjian Wang, Dongsheng Li |  |
| 873 |  |  [So many design choices: Improving and interpreting neural agent communication in signaling games](https://doi.org/10.18653/v1/2023.findings-acl.531) |  | 0 | Emergent language games are experimental protocols designed to model how communication may arise among a group of agents. In this paper, we focus on how to improve performances of neural agents playing a signaling game: a sender is exposed to an image and generates a sequence of symbols that is transmitted to a receiver, which uses it to distinguish between two images, one that is semantically related to the original image, and one that is not. We consider multiple design choices, such as... | Timothée Bernard, Timothee Mickus |  |
| 874 |  |  [Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.532) |  | 0 | As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing... | Fanyu Wang, Zhenping Xie |  |
| 875 |  |  [Fixed Input Parameterization for Efficient Prompting](https://doi.org/10.18653/v1/2023.findings-acl.533) |  | 0 | Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We formally define... | Eunbi Choi, Yongrae Jo, Joel Jang, Joonwon Jang, Minjoon Seo |  |
| 876 |  |  [Data Augmentation for Low-Resource Keyphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.534) |  | 0 | Keyphrase generation is the task of summarizing the contents of any given article into a few salient phrases (or keyphrases). Existing works for the task mostly rely on large-scale annotated datasets, which are not easy to acquire. Very few works address the problem of keyphrase generation in low-resource settings, but they still rely on a lot of additional unlabeled data for pretraining and on automatic methods for pseudo-annotations. In this paper, we present data augmentation strategies... | Krishna Garg, Jishnu Ray Chowdhury, Cornelia Caragea |  |
| 877 |  |  [BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.535) |  | 0 | We present a large-scale video subtitle translation dataset, \*BigVideo\*, to facilitate the study of multi-modality machine translation. Compared with the widely used \*How2\* and \*VaTeX\* datasets, \*BigVideo\* is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: \*Ambiguous\* with the presence of ambiguous words, and \*Unambiguous\* in which the... | Liyan Kang, Luyang Huang, Ningxin Peng, Peihao Zhu, Zewei Sun, Shanbo Cheng, Mingxuan Wang, Degen Huang, Jinsong Su |  |
| 878 |  |  [Constructing Procedural Graphs with Multiple Dependency Relations: A New Dataset and Baseline](https://doi.org/10.18653/v1/2023.findings-acl.536) |  | 0 | Current structured and semi-structured knowledge bases mainly focus on representing descriptive knowledge but ignore another commonsense knowledge (Procedural Knowledge). To structure the procedural knowledge, existing methods are proposed to automatically generate flow graphs from procedural documents. They focus on extracting sequential dependency between sentences but neglect another two important dependencies (i.e., inclusion dependency and constraint dependency) in procedural documents. In... | Haopeng Ren, Yushi Zeng, Yi Cai, Bihan Zhou, Zetao Lian |  |
| 879 |  |  [Multi-Dimensional Evaluation of Text Summarization with In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.537) |  | 0 | Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our... | Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou |  |
| 880 |  |  [Learning to Rank Utterances for Query-Focused Meeting Summarization](https://doi.org/10.18653/v1/2023.findings-acl.538) |  | 0 | Query-focused meeting summarization(QFMS) aims to generate a specific summary for the given query according to the meeting transcripts. Due to the conflict between long meetings and limited input size, previous works mainly adopt extract-then-summarize methods, which use extractors to simulate binary labels or ROUGE scores to extract utterances related to the query and then generate a summary. However, the previous approach fails to fully use the comparison between utterances. To the extractor,... | Xingxian Liu, Yajing Xu |  |
| 881 |  |  [Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.539) |  | 0 | Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET... | Neal Lawton, Anoop Kumar, Govind Thattai, Aram Galstyan, Greg Ver Steeg |  |
| 882 |  |  [Aligning Offline Metrics and Human Judgments of Value for Code Generation Models](https://doi.org/10.18653/v1/2023.findings-acl.540) |  | 0 | Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code are most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N=49 experienced programmers, we show that... | Victor Dibia, Adam Fourney, Gagan Bansal, Forough PoursabziSangdeh, Han Liu, Saleema Amershi |  |
| 883 |  |  [Do transformer models do phonology like a linguist?](https://doi.org/10.18653/v1/2023.findings-acl.541) |  | 0 | Neural sequence-to-sequence models have been very successful at tasks in phonology and morphology that seemingly require a capacity for intricate linguistic generalisations. In this paper, we perform a detailed breakdown of the power of such models to capture various phonological generalisations and to benefit from exposure to one phonological rule to infer the behaviour of another similar rule. We present two types of experiments, one of which establishes the efficacy of the transformer model... | Saliha Muradoglu, Mans Hulden |  |
| 884 |  |  [DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.542) |  | 0 | The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher.... | Sajad Norouzi, Rasa Hosseinzadeh, Felipe Pérez, Maksims Volkovs |  |
| 885 |  |  [Retrieval-augmented Video Encoding for Instructional Captioning](https://doi.org/10.18653/v1/2023.findings-acl.543) |  | 0 | Instructional videos make learning knowledge more efficient, by providing a detailed multimodal context of each procedure in instruction.A unique challenge posed by instructional videos is key-object degeneracy, where any single modality fails to sufficiently capture the key objects referred to in the procedure. For machine systems, such degeneracy can disturb the performance of a downstream task such as dense video captioning, leading to the generation of incorrect captions omitting key... | YeonJoon Jung, Minsoo Kim, Seungtaek Choi, Jihyuk Kim, Minji Seo, Seungwon Hwang |  |
| 886 |  |  [Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training](https://doi.org/10.18653/v1/2023.findings-acl.544) |  | 0 | Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities. Previous work addresses this issue by approaches such as tunable parameters reduction or data augmentation. However, they either limit the trained models’ expressiveness or rely on task-independent knowledge. In this paper, we propose the Bi-level Finetuning with Task-dependent Similarity Structure framework where all parameters, including the... | Sai Ashish Somayajula, Lifeng Jin, Linfeng Song, Haitao Mi, Dong Yu |  |
| 887 |  |  [Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models](https://doi.org/10.18653/v1/2023.findings-acl.545) |  | 0 | Recent studies in natural language processing (NLP) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical Chinese first came to Japan approximately 2,000 years ago. It was gradually adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading and translating methods, which has significantly impacted Japanese literature. However, compared to the rich resources... | Hao Wang, Hirofumi Shimizu, Daisuke Kawahara |  |
| 888 |  |  [Adaptive Attention for Sparse-based Long-sequence Transformer](https://doi.org/10.18653/v1/2023.findings-acl.546) |  | 0 | Recently, Transformers have been widely used in various fields and have achieved remarkable results. But it is still difficult for Transformer-based models to process longer sequences because self-attention in them scales quadratically with the sequence length. Although some models attempt to use sparse attention to reduce computational complexity, hand-crafted attention patterns are unable to select useful tokens adaptively according to the context. Thus, in this paper, we propose a novel... | Xuanyu Zhang, Zhepeng Lv, Qing Yang |  |
| 889 |  |  [Sentiment Analysis using the Relationship between Users and Products](https://doi.org/10.18653/v1/2023.findings-acl.547) |  | 0 | In product reviews, user and product aspects are useful in sentiment analysis. Nevertheless, previous studies mainly focus on modeling user and product aspects without considering the relationship between users and products. The relationship between users and products is typically helpful in estimating the bias of a user toward a product. In this paper, we, therefore, introduce the Graph Neural Network-based model with the pre-trained Language Model (GNNLM), where the relationship between users... | Natthawut Kertkeidkachorn, Kiyoaki Shirai |  |
| 890 |  |  [Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks](https://doi.org/10.18653/v1/2023.findings-acl.548) |  | 0 | Multilingual language models (MLLMs) like mBERTpromise to extend the benefits of NLP research to low-resource languages (LRLs). However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy. (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources. In... | Arijit Nag, Bidisha Samanta, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti |  |
| 891 |  |  [Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data](https://doi.org/10.18653/v1/2023.findings-acl.549) |  | 0 | Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as ‘no_relation’. Models trained with such data inevitably make similar mistakes during the inference stage. Self-training has been proven effective in alleviating... | Qingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng |  |
| 892 |  |  [Solving Cosine Similarity Underestimation between High Frequency Words by \ell₂ Norm Discounting](https://doi.org/10.18653/v1/2023.findings-acl.550) |  | 0 | Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words CITATION.This similarity underestimation problem is particularly severe for high frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the ℓ2 norm of contextualised embeddings of a word correlates with its... | Saeth Wannasuphoprasit, Yi Zhou, Danushka Bollegala |  |
| 893 |  |  [Do Large Language Models Know What They Don't Know?](https://doi.org/10.18653/v1/2023.findings-acl.551) |  | 0 | Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to... | Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang |  |
| 894 |  |  [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://doi.org/10.18653/v1/2023.findings-acl.552) |  | 0 | CLIP (Contrastive Language–Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs. In this work, we present AltCLIP, a simple and low-resource method to build a strong multilingual... | Zhongzhi Chen, Guang Liu, BoWen Zhang, Qinghong Yang, Ledell Wu |  |
| 895 |  |  [RHGN: Relation-gated Heterogeneous Graph Network for Entity Alignment in Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-acl.553) |  | 0 | Entity Alignment, which aims to identify equivalent entities from various Knowledge Graphs (KGs), is a fundamental and crucial task in knowledge graph fusion. Existing methods typically use triple or neighbor information to represent entities, and then align those entities using similarity matching. Most of them, however, fail to account for the heterogeneity among KGs and the distinction between KG entities and relations. To better solve these problems, we propose a Relation-gated... | Xukai Liu, Kai Zhang, Ye Liu, Enhong Chen, Zhenya Huang, Linan Yue, Jiaxian Yan |  |
| 896 |  |  [Feature Interactions Reveal Linguistic Structure in Language Models](https://doi.org/10.18653/v1/2023.findings-acl.554) |  | 0 | We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language... | Jaap Jumelet, Willem H. Zuidema |  |
| 897 |  |  [Clustering-Aware Negative Sampling for Unsupervised Sentence Representation](https://doi.org/10.18653/v1/2023.findings-acl.555) |  | 0 | Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel... | Jinghao Deng, Fanqi Wan, Tao Yang, Xiaojun Quan, Rui Wang |  |
| 898 |  |  [An Effective Deployment of Contrastive Learning in Multi-label Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.556) |  | 0 | The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive... | Nankai Lin, Guanqiu Qin, Gang Wang, Dong Zhou, Aimin Yang |  |
| 899 |  |  [Segment-Level and Category-Oriented Network for Knowledge-Based Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.557) |  | 0 | Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant information in unstructured knowledge sentences. To address these limitations, we propose a segment-level and category-oriented network (SLCO). Our approach includes a segment-level and prompt-based... | Yuqi Bu, Xin Wu, Liuwu Li, Yi Cai, Qiong Liu, Qingbao Huang |  |
| 900 |  |  [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.558) |  | 0 | Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. “supervised pre-training”) showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised... | Tianyi Tang, Junyi Li, Wayne Xin Zhao, JiRong Wen |  |
| 901 |  |  [From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment](https://doi.org/10.18653/v1/2023.findings-acl.559) |  | 0 | Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples... | Yu Zhao, Yike Wu, Xiangrui Cai, Ying Zhang, Haiwei Zhang, Xiaojie Yuan |  |
| 902 |  |  [It is a Bird Therefore it is a Robin: On BERT's Internal Consistency Between Hypernym Knowledge and Logical Words](https://doi.org/10.18653/v1/2023.findings-acl.560) |  | 0 | The lexical knowledge of NLP systems shouldbe tested (i) for their internal consistency(avoiding groundedness issues) and (ii) bothfor content words and logical words. In thispaper we propose a new method to test the understandingof the hypernymy relationship bymeasuring its antisymmetry according to themodels. Previous studies often rely only on thedirect question (e.g., A robin is a ...), where weargue a correct answer could only rely on collocationalcues, rather than hierarchical cues. We... | Nicolas Guérin, Emmanuel Chemla |  |
| 903 |  |  [Defending against Insertion-based Textual Backdoor Attacks via Attribution](https://doi.org/10.18653/v1/2023.findings-acl.561) |  | 0 | Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the... | Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V. G. Vinod Vydiswaran |  |
| 904 |  |  [ActiveAED: A Human in the Loop Improves Annotation Error Detection](https://doi.org/10.18653/v1/2023.findings-acl.562) |  | 0 | Manually annotated datasets are crucial for training and evaluating Natural Language Processing models. However, recent work has discovered that even widely-used benchmark datasets contain a substantial number of erroneous annotations. This problem has been addressed with Annotation Error Detection (AED) models, which can flag such errors for human re-annotation. However, even though many of these AED methods assume a final curation step in which a human annotator decides whether the annotation... | Leon Weber, Barbara Plank |  |
| 905 |  |  [Assessing Word Importance Using Models Trained for Semantic Tasks](https://doi.org/10.18653/v1/2023.findings-acl.563) |  | 0 | Many NLP tasks require to automatically identify the most significant words in a text. In this work, we derive word significance from models trained to solve semantic task: Natural Language Inference and Paraphrase Identification. Using an attribution method aimed to explain the predictions of these models, we derive importance scores for each input token. We evaluate their relevance using a so-called cross-task evaluation: Analyzing the performance of one model on an input masked according to... | Dávid Javorský, Ondrej Bojar, François Yvon |  |
| 906 |  |  [In-context Examples Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.564) |  | 0 | Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality. In... | Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, Marjan Ghazvininejad |  |
| 907 |  |  [PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition](https://doi.org/10.18653/v1/2023.findings-acl.565) |  | 0 | The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions... | Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan Roth, Tal Schuster |  |
| 908 |  |  [CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.566) |  | 0 | Speech or text representation generated by pre-trained models contains modal-specific information that could be combined for benefiting spoken language understanding (SLU) tasks. In this work, we propose a novel pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training (CIF-PT). It relies on a simple but effective frame-to-token alignment: continuous integrate-and-fire (CIF) to bridge the representations between speech and text. It jointly performs speech-to-text training and... | Linhao Dong, Zhecheng An, Peihao Wu, Jun Zhang, Lu Lu, Zejun Ma |  |
| 909 |  |  [Improving Diachronic Word Sense Induction with a Nonparametric Bayesian method](https://doi.org/10.18653/v1/2023.findings-acl.567) |  | 0 | Diachronic Word Sense Induction (DWSI) is the task of inducing the temporal representations of a word meaning from the context, as a set of senses and their prevalence over time. We introduce two new models for DWSI, based on topic modelling techniques: one is based on Hierarchical Dirichlet Processes (HDP), a nonparametric model; the other is based on the Dynamic Embedded Topic Model (DETM), a recent dynamic neural model. We evaluate these models against two state of the art DWSI models, using... | Ashjan Alsulaimani, Erwan Moreau |  |
| 910 |  |  [What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection](https://doi.org/10.18653/v1/2023.findings-acl.568) |  | 0 | Mental health disorders (MHD) are increasingly prevalent worldwide and constitute one of the greatest challenges facing our healthcare systems and modern societies in general. In response to this societal challenge, there has been a surge in digital mental health research geared towards the development of new techniques for unobtrusive and efficient automatic detection of MHD. Within this area of research, natural language processing techniques are playing an increasingly important role,... | Sourabh Zanwar, Xiaofei Li, Daniel Wiechmann, Yu Qiao, Elma Kerz |  |
| 911 |  |  [Adaptive Contrastive Knowledge Distillation for BERT Compression](https://doi.org/10.18653/v1/2023.findings-acl.569) |  | 0 | In this paper, we propose a new knowledge distillation approach called adaptive contrastive knowledge distillation (ACKD) for BERT compression. Different from existing knowledge distillation methods for BERT that implicitly learn discriminative student features by mimicking the teacher features, we first introduce a novel contrastive distillation loss (CDL) based on hidden state features in BERT as the explicit supervision to learn discriminative student features. We further observe sentences... | Jinyang Guo, Jiaheng Liu, Zining Wang, Yuqing Ma, Ruihao Gong, Ke Xu, Xianglong Liu |  |
| 912 |  |  [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://doi.org/10.18653/v1/2023.findings-acl.570) |  | 0 | The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer’s inefficiency has been... | Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin |  |
| 913 |  |  [Zero-Shot Classification by Logical Reasoning on Natural Language Explanations](https://doi.org/10.18653/v1/2023.findings-acl.571) |  | 0 | Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as “it has a slim straight relatively short bill, yellow eyes and a long tail”, so that others can use their knowledge of attributes “slim straight relatively short bill”, “yellow eyes” and “long tail” to recognize a sage... | Chi Han, Hengzhi Pei, Xinya Du, Heng Ji |  |
| 914 |  |  [Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.572) |  | 0 | Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful visual information and thus limiting relation extraction performance. In this paper, we propose a novel MMRE framework to better capture the deeper correlations of text, entity pair, and... | Qian Li, Shu Guo, Cheng Ji, Xutan Peng, Shiyao Cui, Jianxin Li, Lihong Wang |  |
| 915 |  |  [Pruning Pre-trained Language Models with Principled Importance and Self-regularization](https://doi.org/10.18653/v1/2023.findings-acl.573) |  | 0 | Iterative pruning is one of the most effective compression methods for pre-trained language models. We discovered that finding the optimal pruning decision is an equality-constrained 0-1 Integer Linear Programming problem. The solution to this optimization problem leads to a principled importance criterion which we use to rank parameters during iterative model pruning. To mitigate the poor generalization at high sparsity levels, we propose a self-regularization scheme where model prediction is... | Siyu Ren, Kenny Q. Zhu |  |
| 916 |  |  [The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code](https://doi.org/10.18653/v1/2023.findings-acl.574) |  | 0 | Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like “if“, we want to explore whether Code-LLMs acquire better causal reasoning... | Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao |  |
| 917 |  |  [Learning to Leverage High-Order Medical Knowledge Graph for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.575) |  | 0 | Automatic medical entity and relation extraction is essential for daily electronic medical record (EMR) analysis, and has attracted a lot of academic attention. Tremendous progress has been made in recent years. However, medical terms are difficult to understand, and their relations are more complicated than general ones. Based on this situation, domain knowledge gives better background and contexts for medical terms. Despite the benefits of medical domain knowledge, the utilization way of it... | Zhe Yang, Yi Huang, Junlan Feng |  |
| 918 |  |  [Data-Efficient Finetuning Using Cross-Task Nearest Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.576) |  | 0 | Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current... | Hamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi |  |
| 919 |  |  [CoAug: Combining Augmentation of Labels and Labelling Rules](https://doi.org/10.18653/v1/2023.findings-acl.577) |  | 0 | Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well. In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models... | Rakesh R. Menon, Bingqing Wang, Jun Araki, Zhengyu Zhou, Zhe Feng, Liu Ren |  |
| 920 |  |  [Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks](https://doi.org/10.18653/v1/2023.findings-acl.578) |  | 0 | Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel... | Xuming Hu, Yong Jiang, Aiwei Liu, Zhongqiang Huang, Pengjun Xie, Fei Huang, Lijie Wen, Philip S. Yu |  |
| 921 |  |  [World Models for Math Story Problems](https://doi.org/10.18653/v1/2023.findings-acl.579) |  | 0 | Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and... | Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan |  |
| 922 |  |  [AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.580) |  | 0 | Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE – a framework for designing heterogeneous MoE’s under computational... | Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad AbdulMageed, Laks V. S. Lakshmanan, Ahmed Hassan Awadallah, Sébastien Bubeck, Jianfeng Gao |  |
| 923 |  |  [Language Agnostic Multilingual Information Retrieval with Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.581) |  | 0 | Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability. We design a semantic contrastive loss to align representations of... | Xiyang Hu, Xinchi Chen, Peng Qi, Deguang Kong, Kunlun Liu, William Yang Wang, Zhiheng Huang |  |
| 924 |  |  [Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods](https://doi.org/10.18653/v1/2023.findings-acl.582) |  | 0 | A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement – if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated... | Josip Jukic, Martin Tutek, Jan Snajder |  |
| 925 |  |  [Enhancing Cross-lingual Transfer via Phonemic Transcription Integration](https://doi.org/10.18653/v1/2023.findings-acl.583) |  | 0 | Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer.... | Hoang Nguyen, Chenwei Zhang, Tao Zhang, Eugene Rohrbaugh, Philip S. Yu |  |
| 926 |  |  [Human-in-the-loop Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-acl.584) |  | 0 | Abstractive dialogue summarization has received increasing attention recently. Despite the fact that most of the current dialogue summarization systems are trained to maximize the likelihood of human-written summaries and have achieved significant results, there is still a huge gap in generating high-quality summaries as determined by humans, such as coherence and faithfulness, partly due to the misalignment in maximizing a single human-written summary. To this end, we propose to incorporate... | Jiaao Chen, Mohan Dodda, Diyi Yang |  |
| 927 |  |  [A Multi-task Learning Framework for Quality Estimation](https://doi.org/10.18653/v1/2023.findings-acl.585) |  | 0 | Quality Estimation (QE) is the task of evaluating machine translation output in the absence of reference translation. Conventional approaches to QE involve training separate models at different levels of granularity viz., word-level, sentence-level, and document-level, which sometimes lead to inconsistent predictions for the same input. To overcome this limitation, we focus on jointly training a single model for sentence-level and word-level QE tasks in a multi-task learning framework. Using... | Sourabh Dattatray Deoghare, Paramveer Choudhary, Diptesh Kanojia, Tharindu Ranasinghe, Pushpak Bhattacharyya, Constantin Orasan |  |
| 928 |  |  [The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.586) |  | 0 | Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different... | Hao Peng, Xiaozhi Wang, Feng Yao, Kaisheng Zeng, Lei Hou, Juanzi Li, Zhiyuan Liu, Weixing Shen |  |
| 929 |  |  [Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers](https://doi.org/10.18653/v1/2023.findings-acl.587) |  | 0 | The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a referential language game as a controllable example of a task-oriented collaborative joint activity. A teacher utters a referring expression generated by a well-known symbolic algorithm (the “Incremental... | Philipp Sadler, Sherzod Hakimov, David Schlangen |  |
| 930 |  |  [Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.588) |  | 0 | Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to iteratively perform 1-step neural inferences and chain together the results to generate a multi-step reasoning trace. Several works have adapted variants of this central idea and reported significantly high... | Kanagasabai Rajaraman, Saravanan Rajamanickam, Wei Shi |  |
| 931 |  |  [Multilingual Multi-Figurative Language Detection](https://doi.org/10.18653/v1/2023.findings-acl.589) |  | 0 | Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it’s highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling,... | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 932 |  |  [Zero-shot Visual Question Answering with Language Model Feedback](https://doi.org/10.18653/v1/2023.findings-acl.590) |  | 0 | In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task... | Yifan Du, Junyi Li, Tianyi Tang, Wayne Xin Zhao, JiRong Wen |  |
| 933 |  |  [Prompted Opinion Summarization with GPT-3.5](https://doi.org/10.18653/v1/2023.findings-acl.591) |  | 0 | Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised... | Adithya Bhaskar, Alexander R. Fabbri, Greg Durrett |  |
| 934 |  |  [Sentence Ordering with a Coherence Verifier](https://doi.org/10.18653/v1/2023.findings-acl.592) |  | 0 | This paper presents a novel sentence ordering method by plugging a coherence verifier (CoVer) into pair-wise ranking-based and sequence generation-based methods. It does not change the model parameters of the baseline, and only verifies the coherence of candidate (partial) orders produced by the baseline and reranks them in beam search. We also propose a coherence model as CoVer with a novel graph formulation and a novel data construction strategy for contrastive pre-training independently of... | Sainan Jia, Wei Song, Jiefu Gong, Shijin Wang, Ting Liu |  |
| 935 |  |  [GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.593) |  | 0 | Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023’s ‘Reality Check’ theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and... | Yang Janet Liu, Amir Zeldes |  |
| 936 |  |  [Improving Grammatical Error Correction with Multimodal Feature Integration](https://doi.org/10.18653/v1/2023.findings-acl.594) |  | 0 | Grammatical error correction (GEC) is a promising task aimed at correcting errors in a text. Many methods have been proposed to facilitate this task with remarkable results. However, most of them only focus on enhancing textual feature extraction without exploring the usage of other modalities’ information (e.g., speech), which can also provide valuable knowledge to help the model detect grammatical errors. To shore up this deficiency, we propose a novel framework that integrates both speech... | Tao Fang, Jinpeng Hu, Derek F. Wong, Xiang Wan, Lidia S. Chao, TsungHui Chang |  |
| 937 |  |  [Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification](https://doi.org/10.18653/v1/2023.findings-acl.595) |  | 0 | Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical... | Renliang Sun, Wei Xu, Xiaojun Wan |  |
| 938 |  |  [Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction](https://doi.org/10.18653/v1/2023.findings-acl.596) |  | 0 | The semantic frame induction tasks are defined as a clustering of words into the frames that they evoke, and a clustering of their arguments according to the frame element roles that they should fill. In this paper, we address the latter task of argument clustering, which aims to acquire frame element knowledge, and propose a method that applies deep metric learning. In this method, a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use... | Kosuke Yamada, Ryohei Sasano, Koichi Takeda |  |
| 939 |  |  [Leveraging Synthetic Targets for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.597) |  | 0 | In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the... | Sarthak Mittal, Oleksii Hrinchuk, Oleksii Kuchaiev |  |
| 940 |  |  [Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models](https://doi.org/10.18653/v1/2023.findings-acl.598) |  | 0 | Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a seq2seq model, we show it under-performs a Masked Language Modeling (MLM) encoder, particularly on sequence labeling tasks. Variations of masking during seq2seq training, reducing the decoder size, and... | Saleh Soltan, Andy Rosenbaum, Tobias Falke, Qin Lu, Anna Rumshisky, Wael Hamza |  |
| 941 |  |  [Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.599) |  | 0 | Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD- based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to... | Hao Fei, Meishan Zhang, Min Zhang, TatSeng Chua |  |
| 942 |  |  [Spontaneous gestures encoded by hand positions improve language models: An Information-Theoretic motivated study](https://doi.org/10.18653/v1/2023.findings-acl.600) |  | 0 | The multi-modality nature of human communication has been utilized to enhance the performance of language modeling-related tasks. Driven by the development of large-scale end-to-end learning techniques and the availability of multi-modal data, it becomes possible to represent non-verbal communication behaviors through joint-learning, and directly study their interaction with verbal communication. However, there is still gaps in existing studies to better address the underlying mechanism of how... | Yang Xu, Yang Cheng |  |
| 943 |  |  [Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences](https://doi.org/10.18653/v1/2023.findings-acl.601) |  | 0 | Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the “source-like” structure to the “target-like” structure. Such intermediate sequences introduce an inductive bias that reflects a... | Chaojun Wang, Yang Liu, Wai Lam |  |
| 944 |  |  [Controlled Text Generation with Hidden Representation Transformations](https://doi.org/10.18653/v1/2023.findings-acl.602) |  | 0 | We propose CHRT (Control HiddenRepresentation Transformation) – a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model through learned transformations. We employ a contrastive-learning frameworkto learn these transformations that can becombined to gain multi-attribute control. Theeffectiveness of CHRT is experimentallyshown... | Vaibhav Kumar, Hana Koorehdavoudi, Masud Moshtaghi, Amita Misra, Ankit Chadha, Emilio Ferrara |  |
| 945 |  |  [Visual Coherence Loss for Coherent and Visually Grounded Story Generation](https://doi.org/10.18653/v1/2023.findings-acl.603) |  | 0 | Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) character representations should enable us to find instances of the same characters and distinguish different characters. In this paper, we propose a loss function inspired by a linguistic theory of... | Xudong Hong, Vera Demberg, Asad B. Sayeed, Qiankun Zheng, Bernt Schiele |  |
| 946 |  |  [AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks](https://doi.org/10.18653/v1/2023.findings-acl.604) |  | 0 | Tabular data analysis is performed everyday across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived supervision labels for four types of commonly used field metadata: measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. We evaluate a wide range of... | Xinyi He, Mengyu Zhou, Mingjie Zhou, Jialiang Xu, Xiao Lv, Tianle Li, Yijia Shao, Shi Han, Zejian Yuan, Dongmei Zhang |  |
| 947 |  |  [Large Language Models Are Partially Primed in Pronoun Interpretation](https://doi.org/10.18653/v1/2023.findings-acl.605) |  | 0 | While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns;... | SuetYing Lam, Qingcheng Zeng, Kexun Zhang, Chenyu You, Rob Voigt |  |
| 948 |  |  [Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors](https://doi.org/10.18653/v1/2023.findings-acl.606) |  | 0 | In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary... | George Filandrianos, Edmund Dervakos, Orfeas MenisMastromichalakis, Chrysoula Zerva, Giorgos Stamou |  |
| 949 |  |  [A Pilot Study on Dialogue-Level Dependency Parsing for Chinese](https://doi.org/10.18653/v1/2023.findings-acl.607) |  | 0 | Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen... | Gongyao Jiang, Shuang Liu, Meishan Zhang, Min Zhang |  |
| 950 |  |  [On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.608) |  | 0 | While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages’ vocabularies is related with a higher off-target rate. We also find that solely isolating... | Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, Baobao Chang |  |
| 951 |  |  [ORCA: A Challenging Benchmark for Arabic Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.609) |  | 0 | Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but... | AbdelRahim A. Elmadany, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 952 |  |  [Delving into the Openness of CLIP](https://doi.org/10.18653/v1/2023.findings-acl.610) |  | 0 | Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary... | Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, Xu Sun |  |
| 953 |  |  [From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework](https://doi.org/10.18653/v1/2023.findings-acl.611) |  | 0 | Textual adversarial attacks can discover models’ weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this... | Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, Zhiyuan Liu, Maosong Sun, Heng Ji |  |
| 954 |  |  [An Empirical Study of Sentiment-Enhanced Pre-Training for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.612) |  | 0 | Aspect-Based Sentiment Analysis (ABSA) aims to recognize fine-grained opinions and sentiments of users, which is an important problem in sentiment analysis. Recent work has shown that Sentiment-enhanced Pre-Training (SPT) can substantially improve the performance of various ABSA tasks. However, there is currently a lack of comprehensive evaluation and fair comparison of existing SPT approaches. Therefore, this paper performs an empirical study to investigate the effectiveness of different SPT... | Yice Zhang, Yifan Yang, Bin Liang, Shiwei Chen, Bing Qin, Ruifeng Xu |  |
| 955 |  |  [NatCS: Eliciting Natural Customer Support Dialogues](https://doi.org/10.18653/v1/2023.findings-acl.613) |  | 0 | Despite growing interest in applications based on natural customer support conversations,there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark dialogue systems mainly in written human-to-bot settings, are not representative of real customer support conversations and do not provide realistic benchmarks for systems that are applied to natural... | James Gung, Emily Moeng, Wesley Rose, Arshit Gupta, Yi Zhang, Saab Mansour |  |
| 956 |  |  [Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method](https://doi.org/10.18653/v1/2023.findings-acl.614) |  | 0 | The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the model’s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively. Moreover, the parameters of vocabulary are usually neglected in... | Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu Zhao, Peng Zhang, Jie Tang |  |
| 957 |  |  [Diable: Efficient Dialogue State Tracking as Operations on Tables](https://doi.org/10.18653/v1/2023.findings-acl.615) |  | 0 | Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and... | Pietro Lesci, Yoshinari Fujinuma, Momchil Hardalov, Chao Shang, Lluís Màrquez |  |
| 958 |  |  [Neural Topic Modeling based on Cycle Adversarial Training and Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.616) |  | 0 | Neural topic models have been widely used to extract common topics across documents. Recently, contrastive learning has been applied to variational autoencoder-based neural topic models, achieving promising results. However, due to the limitation of the unidirectional structure of the variational autoencoder, the encoder is enhanced with the contrastive loss instead of the decoder, leading to a gap between model training and evaluation. To address the limitation, we propose a novel neural topic... | Boyu Wang, Linhai Zhang, Deyu Zhou, Yi Cao, Jiandong Ding |  |
| 959 |  |  [Alleviating Exposure Bias via Multi-level Contrastive Learning and Deviation Simulation in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.617) |  | 0 | Most Transformer based abstractive summarization systems have a severe mismatch between training and inference, i.e., exposure bias. From diverse perspectives, we introduce a simple multi-level contrastive learning framework for abstractive summarization (SimMCS) and a tailored sparse decoder self-attention pattern (SDSA) to bridge the gap between training and inference to improve model performance. Compared with previous contrastive objectives focusing only on the relative order of probability... | Jiawen Xie, Qi Su, Shaoting Zhang, Xiaofan Zhang |  |
| 960 |  |  [Mapping Brains with Language Models: A Survey](https://doi.org/10.18653/v1/2023.findings-acl.618) |  | 0 | Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before... | Antonia Karamolegkou, Mostafa Abdou, Anders Søgaard |  |
| 961 |  |  [Parameter-Efficient Finetuning for Robust Continual Multilingual Learning](https://doi.org/10.18653/v1/2023.findings-acl.619) |  | 0 | We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance only on the languages included in the latest update (and a few closely related languages) while its performance on all the remaining languages degrade significantly. We address this challenge by... | Kartikeya Badola, Shachi Dave, Partha Talukdar |  |
| 962 |  |  [Interpretable Multimodal Misinformation Detection with Logic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.620) |  | 0 | Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems’ reliability and practical deployment. Inspired by Neural-Symbolic AI which combines the learning ability of neural networks with the explainability of... | Hui Liu, Wenya Wang, Haoliang Li |  |
| 963 |  |  [Semantic-conditioned Dual Adaptation for Cross-domain Query-based Visual Segmentation](https://doi.org/10.18653/v1/2023.findings-acl.621) |  | 0 | Visual segmentation from language queries has attracted significant research interest. Despite the effectiveness, existing works require expensive labeling and suffer severe degradation when deployed to an unseen domain. In this paper, we investigate a novel task Cross-domain Query-based Visual Segmentation (CQVS), aiming to adapt the segmentation model from a labeled domain to a new unlabeled domain. The challenges of CQVS stem from three domain discrepancies: (1) multi-modal content shift,... | Ye Wang, Tao Jin, Wang Lin, Xize Cheng, Linjun Li, Zhou Zhao |  |
| 964 |  |  [Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans](https://doi.org/10.18653/v1/2023.findings-acl.622) |  | 0 | Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to... | Hyewon Jang, Qi Yu, Diego Frassinelli |  |
| 965 |  |  [Taxonomy of Problems in Lexical Semantics](https://doi.org/10.18653/v1/2023.findings-acl.623) |  | 0 | Semantic tasks are rarely formally defined, and the exact relationship between them is an open question. We introduce a taxonomy that elucidates the connection between several problems in lexical semantics, including monolingual and cross-lingual variants. Our theoretical framework is based on the hypothesis of the equivalence of concept and meaning distinctions. Using algorithmic problem reductions, we demonstrate that all problems in the taxonomy can be reduced to word sense disambiguation... | Bradley Hauer, Grzegorz Kondrak |  |
| 966 |  |  [Making Pre-trained Language Models both Task-solvers and Self-calibrators](https://doi.org/10.18653/v1/2023.findings-acl.624) |  | 0 | Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it’s equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves... | Yangyi Chen, Xingyao Wang, Heng Ji |  |
| 967 |  |  [EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding](https://doi.org/10.18653/v1/2023.findings-acl.625) |  | 0 | The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures... | Dae Yon Hwang, Bilal Taha, Yaroslav Nechaev |  |
| 968 |  |  [Denoising Enhanced Distantly Supervised Ultrafine Entity Typing](https://doi.org/10.18653/v1/2023.findings-acl.626) |  | 0 | Recently, the task of distantly supervised (DS) ultra-fine entity typing has received significant attention. However, DS data is noisy and often suffers from missing or wrong labeling issues resulting in low precision and low recall. This paper proposes a novel ultra-fine entity typing model with denoising capability. Specifically, we build a noise model to estimate the unknown labeling noise distribution over input contexts and noisy type labels. With the noise model, more trustworthy labels... | Yue Zhang, Hongliang Fei, Ping Li |  |
| 969 |  |  [INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition](https://doi.org/10.18653/v1/2023.findings-acl.627) |  | 0 | Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning. However, these pre-trained speech models suffer from representational bias as they tend to better represent those prominent accents (i.e., native (L1) English accent) in the pre-training speech corpus than less represented accents, resulting in a deteriorated performance for non-native (L2) English accents. Although... | Eunseop Yoon, Hee Suk Yoon, John B. Harvill, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 970 |  |  [Local Temperature Beam Search: Avoid Neural Text DeGeneration via Enhanced Calibration](https://doi.org/10.18653/v1/2023.findings-acl.628) |  | 0 | Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the “self-loop”. However, the remedy comes at the cost of sacrificing output quality due to the randomness involved. In this work, we introduce a deterministic decoding scheme, local temperature beam search. This inference... | Dongkyu Lee, Gyeonghun Kim, Janghoon Han, Taesuk Hong, Yireun Kim, Stanley Jungkyu Choi, Nevin L. Zhang |  |
| 971 |  |  [Explanation Graph Generation via Generative Pre-training over Synthetic Graphs](https://doi.org/10.18653/v1/2023.findings-acl.629) |  | 0 | The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy be- tween unstructured user queries and structured explanation graphs. Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available... | Han Cui, Shangzhan Li, Yu Zhang, Qi Shi |  |
| 972 |  |  [NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts](https://doi.org/10.18653/v1/2023.findings-acl.630) |  | 0 | We introduce NaSGEC, a new dataset to facilitate research on Chinese grammatical error correction (CGEC) for native speaker texts from multiple domains. Previous CGEC research primarily focuses on correcting texts from a single domain, especially learner essays. To broaden the target domain, we annotate multiple references for 12,500 sentences from three native domains, i.e., social media, scientific writing, and examination. We provide solid benchmark results for NaSGEC by employing... | Yue Zhang, Bo Zhang, Haochen Jiang, Zhenghua Li, Chen Li, Fei Huang, Min Zhang |  |
| 973 |  |  [FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models](https://doi.org/10.18653/v1/2023.findings-acl.631) |  | 0 | It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high... | Shramay Palta, Rachel Rudinger |  |
| 974 |  |  [FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.632) |  | 0 | With increasing concerns about data privacy, there is an increasing necessity of fine-tuning pre-trained language models (PLMs) for adapting to downstream tasks located in end-user devices or local clients without transmitting data to the central server. This urgent necessity therefore calls the research of investigating federated learning (FL) for PLMs. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we... | Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, Zenglin Xu |  |
| 975 |  |  [MixPAVE: Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-acl.633) |  | 0 | The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus on extracting values for a set of known attributes with sufficient training data. However, with the emerging nature of e-commerce, new products with their unique set of new attributes are constantly... | Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu, Dongfang Liu |  |
| 976 |  |  [SlowBERT: Slow-down Attacks on Input-adaptive Multi-exit BERT](https://doi.org/10.18653/v1/2023.findings-acl.634) |  | 0 | For pretrained language models such as Google’s BERT, recent research designs several input-adaptive inference mechanisms to improve the efficiency on cloud and edge devices. In this paper, we reveal a new attack surface on input-adaptive multi-exit BERT, where the adversary imperceptibly modifies the input texts to drastically increase the average inference cost. Our proposed slow-down attack called SlowBERT integrates a new rank-and-substitute adversarial text generation algorithm to... | Shengyao Zhang, Xudong Pan, Mi Zhang, Min Yang |  |
| 977 |  |  [Compositional Mathematical Encoding for Math Word Problems](https://doi.org/10.18653/v1/2023.findings-acl.635) |  | 0 | Solving math word problem (MWP) remains a challenging task, as it requires to understand both the semantic meanings of the text and the mathematical logic among quantities, i.e., for both semantics modal and quantity modal learning. Current MWP encoders work in a uni-modal setting and map the given problem description to a latent representation, then for decoding. The generalizability of these MWP encoders is thus limited because some problems are semantics-demanding and others are... | Zhenwen Liang, Jipeng Zhang, Kehan Guo, Xiaodong Wu, Jie Shao, Xiangliang Zhang |  |
| 978 |  |  [PREADD: Prefix-Adaptive Decoding for Controlled Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.636) |  | 0 | We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any... | Jonathan Pei, Kevin Yang, Dan Klein |  |
| 979 |  |  [EventOA: An Event Ontology Alignment Benchmark Based on FrameNet and Wikidata](https://doi.org/10.18653/v1/2023.findings-acl.637) |  | 0 | Event ontology provides a shared and formal specification about what happens in the real world and can benefit many natural language understanding tasks. However, the independent development of event ontologies often results in heterogeneous representations that raise the need for establishing alignments between semantically related events. There exists a series of works about ontology alignment (OA), but they only focus on the entity-based OA, and neglect the event-based OA. To fill the gap,... | Shaoru Guo, Chenhao Wang, Yubo Chen, Kang Liu, Ru Li, Jun Zhao |  |
| 980 |  |  [Enhancing Continual Relation Extraction via Classifier Decomposition](https://doi.org/10.18653/v1/2023.findings-acl.638) |  | 0 | Continual relation extraction (CRE) models aim at handling emerging new relations while avoiding catastrophically forgetting old ones in the streaming data. Though improvements have been shown by previous CRE studies, most of them only adopt a vanilla strategy when models first learn representations of new relations. In this work, we point out that there exist two typical biases after training of this vanilla strategy: classifier bias and representation bias, which causes the previous knowledge... | Heming Xia, Peiyi Wang, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 981 |  |  [A Comparative Analysis of the Effectiveness of Rare Tokens on Creative Expression using ramBERT](https://doi.org/10.18653/v1/2023.findings-acl.639) |  | 0 | Until now, few studies have been explored on Automated Creative Essay Scoring (ACES), in which a pre-trained model automatically labels an essay as a creative or a non-creative. Since the creativity evaluation of essays is very subjective, each evaluator often has his or her own criteria for creativity. For this reason, quantifying creativity in essays is very challenging. In this work, as one of preliminary studies in developing a novel model for ACES, we deeply investigate the correlation... | Youbin Lee, Deokgi Kim, ByungWon On, Ingyu Lee |  |
| 982 |  |  [MTR: A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.640) |  | 0 | A long-standing difficulty in AI is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets have been presented. However, these datasets mainly focus on a single type of reasoning. There are still significant gaps in the studies when compared to the complex reasoning used in daily life. In... | Yitian Li, Jidong Tian, Caoyun Fan, Wenqing Chen, Hao He, Yaohui Jin |  |
| 983 |  |  [NewsMet : A 'do it all' Dataset of Contemporary Metaphors in News Headlines](https://doi.org/10.18653/v1/2023.findings-acl.641) |  | 0 | Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated with metaphorical verbs. The dataset comprises headlines from various sources including political, satirical, reliable and fake. Our dataset serves the purpose of evaluation for the tasks of metaphor... | Rohan Joseph, Timothy Liu, Aik Beng Ng, Simon See, Sunny Rai |  |
| 984 |  |  [Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-acl.642) |  | 0 | Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic... | Zijie Huang, Daheng Wang, Binxuan Huang, Chenwei Zhang, Jingbo Shang, Yan Liang, Zhengyang Wang, Xian Li, Christos Faloutsos, Yizhou Sun, Wei Wang |  |
| 985 |  |  [Noise-Robust Training with Dynamic Loss and Contrastive Learning for Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.643) |  | 0 | Distantly-supervised named entity recognition (NER) aims at training networks with distantly-labeled data, which is automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. Distant supervision may induce incomplete and noisy labels, so recent state-of-the-art methods employ sample selection mechanism to separate clean data from noisy data based on the model’s prediction scores. However, they ignore the noise distribution change caused by data... | Zhiyuan Ma, Jintao Du, Shuheng Zhou |  |
| 986 |  |  [Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation](https://doi.org/10.18653/v1/2023.findings-acl.644) |  | 0 | Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically — Human often decompose a complex task into subgoals, where each subgoal can be further decomposed into steps. To establish the benchmark, we contribute a new dataset, propose several baseline methods, and set up... | Xinze Li, Yixin Cao, Muhao Chen, Aixin Sun |  |
| 987 |  |  [End-to-End Task-Oriented Dialogue Systems Based on Schema](https://doi.org/10.18653/v1/2023.findings-acl.645) |  | 0 | This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains, we design our approach to support a domain covering multiple services where diverse schemas are available. To enable better generalizability among services and domains with different schemas, we... | Wiradee Imrattanatrai, Ken Fukuda |  |
| 988 |  |  [HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language](https://doi.org/10.18653/v1/2023.findings-acl.646) |  | 0 | This paper presents “HaVQA”, the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We... | Shantipriya Parida, Idris Abdulmumin, Shamsuddeen Hassan Muhammad, Aneesh Bose, Guneet Singh Kohli, Ibrahim Said Ahmad, Ketan Kotwal, Sayan Deb Sarkar, Ondrej Bojar, Habeebah A. Kakudi |  |
| 989 |  |  [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction](https://doi.org/10.18653/v1/2023.findings-acl.647) |  | 0 | We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidence jointly learns to identify: (i) the relevant evidences to the given claim (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way — the final veracity probability is proportional to a linear ensemble of per-evidence relevance... | Martin Fajcik, Petr Motlícek, Pavel Smrz |  |
| 990 |  |  [StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints](https://doi.org/10.18653/v1/2023.findings-acl.648) |  | 0 | We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism: hierarchical structure information strengthening and a final specific task. The model used is better than existing ones at learning the contextual representations of utterances embedded within its hierarchical semantic structure and thereby... | Truong Do, Phuong Nguyen, LeMinh Nguyen |  |
| 991 |  |  [GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks](https://doi.org/10.18653/v1/2023.findings-acl.649) |  | 0 | Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to generate pseudo-annotated sentences beyond limited annotations. These techniques neither preserve the semantic consistency of the original sentences when rule-based augmentations are adopted, nor... | Xuming Hu, Aiwei Liu, Zeqi Tan, Xin Zhang, Chenwei Zhang, Irwin King, Philip S. Yu |  |
| 992 |  |  [WebDP: Understanding Discourse Structures in Semi-Structured Web Documents](https://doi.org/10.18653/v1/2023.findings-acl.650) |  | 0 | Web documents have become rich data resources in current era, and understanding their discourse structure will potentially benefit various downstream document processing applications. Unfortunately, current discourse analysis and document intelligence research mostly focus on either discourse structure of plain text or superficial visual structures in document, which cannot accurately describe discourse structure of highly free-styled and semi-structured web documents. To promote discourse... | Peilin Liu, Hongyu Lin, Meng Liao, Hao Xiang, Xianpei Han, Le Sun |  |
| 993 |  |  [Tab-CoT: Zero-shot Tabular Chain of Thought](https://doi.org/10.18653/v1/2023.findings-acl.651) |  | 0 | The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit highly structured steps. Recent efforts also started investigating methods to encourage more structured reasoning procedures to be captured (cite least to most).In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex... | Ziqi Jin, Wei Lu |  |
| 994 |  |  [KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition](https://doi.org/10.18653/v1/2023.findings-acl.652) |  | 0 | Symptom diagnosis in medical conversations aims to correctly extract both symptom entities and their status from the doctor-patient dialogue. In this paper, we propose a novel framework called KNSE for symptom status recognition (SSR), where the SSR is formulated as a natural language inference (NLI) task. For each mentioned symptom in a dialogue window, we first generate knowledge about the symptom and hypothesis about status of the symptom, to form a (premise, knowledge, hypothesis) triplet.... | Wei Chen, Shiqi Wei, Zhongyu Wei, Xuanjing Huang |  |
| 995 |  |  [Augmenting Large Language Model Translators via Translation Memories](https://doi.org/10.18653/v1/2023.findings-acl.653) |  | 0 | Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to “understand” prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are... | Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu |  |
| 996 |  |  [Character Coreference Resolution in Movie Screenplays](https://doi.org/10.18653/v1/2023.findings-acl.654) |  | 0 | Movie screenplays have a distinct narrative structure. It segments the story into scenes containing interleaving descriptions of actions, locations, and character dialogues.A typical screenplay spans several scenes and can include long-range dependencies between characters and events.A holistic document-level understanding of the screenplay requires several natural language processing capabilities, such as parsing, character identification, coreference resolution, action recognition,... | Sabyasachee Baruah, Shrikanth Narayanan |  |
| 997 |  |  [Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph](https://doi.org/10.18653/v1/2023.findings-acl.655) |  | 0 | Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and the event pair interaction information. In particular, 1) we design an event-causal-label-aware module to model the event causal label information, in which we design the event causal label prediction... | Ruili Pu, Yang Li, Suge Wang, Deyu Li, Jianxing Zheng, Jian Liao |  |
| 998 |  |  [LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing](https://doi.org/10.18653/v1/2023.findings-acl.656) |  | 0 | Transformer has become an important technique for natural language processing tasks with great success. However, it usually requires huge storage space and computational cost, making it difficult to be deployed on resource-constrained edge devices. To compress and accelerate Transformer, we propose LightFormer, which adopts a low-rank factorization initialized by SVD-based weight transfer and parameter sharing. The SVD-based weight transfer can effectively utilize the well-trained Transformer... | Xiuqing Lv, Peng Zhang, Sunzhu Li, Guobing Gan, Yueheng Sun |  |
| 999 |  |  [Multi-hop Evidence Retrieval for Cross-document Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.657) |  | 0 | Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations,along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose Mr.Cod (Multi-hop evidence retrieval for Cross-document relation... | Keming Lu, IHung Hsu, Wenxuan Zhou, Mingyu Derek Ma, Muhao Chen |  |
| 1000 |  |  [Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree](https://doi.org/10.18653/v1/2023.findings-acl.658) |  | 0 | Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However, capturing disagreement can increase annotation time and expense. Fortunately, for many tasks, not all examples are equally controversial; we develop an active learning approach, Disagreement Aware... | Connor Baumler, Anna Sotnikova, Hal Daumé III |  |
| 1001 |  |  [PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.659) |  | 0 | Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled... | Yixin Wan, KuanHao Huang, KaiWei Chang |  |
| 1002 |  |  [DePlot: One-shot visual language reasoning by plot-to-table translation](https://doi.org/10.18653/v1/2023.findings-acl.660) |  | 0 | Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1)... | Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun |  |
| 1003 |  |  [Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.findings-acl.661) |  | 0 | Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only... | Weize Chen, Xu Han, Yankai Lin, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 1004 |  |  [Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation](https://doi.org/10.18653/v1/2023.findings-acl.662) |  | 0 | Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to... | Zhiyang Xu, Jay Yoon Lee, Lifu Huang |  |
| 1005 |  |  [How Well Do Large Language Models Perform on Faux Pas Tests?](https://doi.org/10.18653/v1/2023.findings-acl.663) |  | 0 | Motivated by the question of the extent to which large language models “understand” social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses,... | Natalie Shapira, Guy Zwirn, Yoav Goldberg |  |
| 1006 |  |  [Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference](https://doi.org/10.18653/v1/2023.findings-acl.664) |  | 0 | Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible... | Wangchunshu Zhou, Ronan Le Bras, Yejin Choi |  |
| 1007 |  |  [ISLTranslate: Dataset for Translating Indian Sign Language](https://doi.org/10.18653/v1/2023.findings-acl.665) |  | 0 | Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset... | Abhinav Joshi, Susmit Agrawal, Ashutosh Modi |  |
| 1008 |  |  [LMentry: A Language Model Benchmark of Elementary Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.666) |  | 0 | As the performance of large language models rapidly improves, benchmarks are getting larger and more complex as well. We present LMentry, a benchmark that avoids this “arms race” by focusing on a compact set of tasks that are trivial to humans, e.g. writing a sentence containing a specific word, identifying which words in a list belong to a specific category, or choosing which of two words is longer.LMentry is specifically designed to provide quick and interpretable insights into the... | Avia Efrat, Or Honovich, Omer Levy |  |
| 1009 |  |  [Differentiable Instruction Optimization for Cross-Task Generalization](https://doi.org/10.18653/v1/2023.findings-acl.667) |  | 0 | Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks. Although various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalization ability. This work presents instruction optimization, which optimizes training instructions with respect to generalization ability. Rather than manually tuning instructions, we introduce learnable... | Masaru Isonuma, Junichiro Mori, Ichiro Sakata |  |
| 1010 |  |  [Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.668) |  | 0 | Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging CITATION, especially in the context of math word problem solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these... | Zhanming Jie, Wei Lu |  |
| 1011 |  |  [How does the task complexity of masked pretraining objectives affect downstream performance?](https://doi.org/10.18653/v1/2023.findings-acl.669) |  | 0 | Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays... | Atsuki Yamaguchi, Hiroaki Ozaki, Terufumi Morishita, Gaku Morio, Yasuhiro Sogawa |  |
| 1012 |  |  [AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets](https://doi.org/10.18653/v1/2023.findings-acl.670) |  | 0 | High-quality data is essential for conversational recommendation systems and serves as the cornerstone of the network architecture development and training strategy design. Existing works contribute heavy human efforts to manually labeling or designing and extending recommender dialogue templates. However, they suffer from: (i) the limited number of human annotators results in datasets can hardly capture rich and large-scale cases in the real world, (ii) the limited experience and knowledge of... | Yu Lu, Junwei Bao, Zichen Ma, Xiaoguang Han, Youzheng Wu, Shuguang Cui, Xiaodong He |  |
| 1013 |  |  [Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals](https://doi.org/10.18653/v1/2023.findings-acl.671) |  | 0 | The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how & knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the sTeps and fActs in a gRAh (TARA), which supports a unified inference of various questions. Towards a systematical... | Hongru Liang, Jia Liu, Weihong Du, Dingnan Jin, Wenqiang Lei, Zujie Wen, Jiancheng Lv |  |
| 1014 |  |  [Deep Span Representations for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.672) |  | 0 | Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a... | Enwei Zhu, Yiyang Liu, Jinpeng Li |  |
| 1015 |  |  [Disambiguated Lexically Constrained Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.673) |  | 0 | Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This assumption limits their application to real-world scenarios where a source lexicon may have multiple target constraints, and disambiguation is needed to select the most suitable one. In this paper,... | Jinpeng Zhang, Nini Xiao, Ke Wang, Chuanqi Dong, Xiangyu Duan, Yuqi Zhang, Min Zhang |  |
| 1016 |  |  [Curating Datasets for Better Performance with Example Training Dynamics](https://doi.org/10.18653/v1/2023.findings-acl.674) |  | 0 | The landscape of NLP research is dominated by large-scale models training on colossal datasets, relying on data quantity rather than quality. As an alternative to this landscape, we propose a method for weighing the relative importance of examples in a dataset based on their Example Training dynamics (swayamdipta et al., 2020) — a set of metrics computed during training. We propose a new way of computing the ETD of a dataset, and show that they can be used to improve performance in both... | Aviad SarShalom, Roy Schwartz |  |
| 1017 |  |  [Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking](https://doi.org/10.18653/v1/2023.findings-acl.675) |  | 0 | We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.We propose a multi-armed bandit framework for the sequential selection of pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson... | Iñigo Urteaga, MoulayZaïdane Draïdia, Tomer Lancewicki, Shahram Khadivi |  |
| 1018 |  |  [ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages](https://doi.org/10.18653/v1/2023.findings-acl.676) |  | 0 | Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained... | Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu |  |
| 1019 |  |  [PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts](https://doi.org/10.18653/v1/2023.findings-acl.677) |  | 0 | A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user’s goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target... | Xiangjue Dong, Yun He, Ziwei Zhu, James Caverlee |  |
| 1020 |  |  [Understanding Programs by Exploiting (Fuzzing) Test Cases](https://doi.org/10.18653/v1/2023.findings-acl.678) |  | 0 | Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their... | Jianyu Zhao, Yuyang Rong, Yiwen Guo, Yifeng He, Hao Chen |  |
| 1021 |  |  [Hybrid Hierarchical Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.679) |  | 0 | Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems. Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform the single stage dense passage retriever (DPR). While effective, this approach requires document structure information to learn document representation and is hard to adopt to other domains without... | Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, Zhiheng Huang |  |
| 1022 |  |  [Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages](https://doi.org/10.18653/v1/2023.findings-acl.680) |  | 0 | In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages. We employ a consistent evaluation framework to compare the performance of monolingual and multilingual models in both... | Dominique Brunato, Felice Dell'Orletta, Irene Dini, Andrea Amelio Ravelli |  |
| 1023 |  |  [Understanding Differential Search Index for Text Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.681) |  | 0 | The Differentiable Search Index (DSI) is a novel information retrieval (IR) framework that utilizes a differentiable function to generate a sorted list of document identifiers in response to a given query. However, due to the black-box nature of the end-to-end neural architecture, it remains to be understood to what extent DSI possesses the basic indexing and retrieval abilities. To mitigate this gap, in this study, we define and examine three important abilities that a functioning IR framework... | Xiaoyang Chen, Yanjiang Liu, Ben He, Le Sun, Yingfei Sun |  |
| 1024 |  |  [Masked Audio Text Encoders are Effective Multi-Modal Rescorers](https://doi.org/10.18653/v1/2023.findings-acl.682) |  | 0 | Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of... | Jinglun Cai, Monica Sunkara, Xilai Li, Anshu Bhatia, Xiao Pan, Sravan Bodapati |  |
| 1025 |  |  [Replace and Report: NLP Assisted Radiology Report Generation](https://doi.org/10.18653/v1/2023.findings-acl.683) |  | 0 | Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to... | Kaveri Kale, Pushpak Bhattacharyya, Kshitij S. Jadhav |  |
| 1026 |  |  [Pre-trained Personalized Review Summarization with Effective Salience Estimation](https://doi.org/10.18653/v1/2023.findings-acl.684) |  | 0 | Personalized review summarization in recommender systems is a challenging task of generating condensed summaries for product reviews while preserving the salient content of reviews. Recently, Pretrained Language Models (PLMs) have become a new paradigm in text generation for the strong ability of natural language comprehension. However, it is nontrivial to apply PLMs in personalized review summarization directly since there are rich personalized information (e.g., user preferences and product... | Hongyan Xu, Hongtao Liu, Zhepeng Lv, Qing Yang, Wenjun Wang |  |
| 1027 |  |  [CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.685) |  | 0 | Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on factual errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more effectively, utilizing variations in noise in training samples to reduce hallucination. Starting with a base model fine-tuned on an entire dataset, we additionally train expert and anti-expert models on... | Prafulla Kumar Choubey, Alexander R. Fabbri, Jesse Vig, ChienSheng Wu, Wenhao Liu, Nazneen Rajani |  |
| 1028 |  |  [OpineSum: Entailment-based self-training for abstractive opinion summarization](https://doi.org/10.18653/v1/2023.findings-acl.686) |  | 0 | A typical product or place often has hundreds of reviews, and summarization of these texts is an important and challenging problem. Recent progress on abstractive summarization in domains such as news has been driven by supervised systems trained on hundreds of thousands of news articles paired with human-written summaries. However for opinion texts, such large scale datasets are rarely available. Unsupervised methods, self-training, and few-shot learning approaches bridge that gap. In this... | Annie Louis, Joshua Maynez |  |
| 1029 |  |  [A Call for Standardization and Validation of Text Style Transfer Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.687) |  | 0 | Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and automated evaluation. In addition, we also find a validation gap: only few automated metrics have been validated using human experiments. To this end, we thoroughly scrutinize both the standardization... | Phil Ostheimer, Mayank Kumar Nagda, Marius Kloft, Sophie Fellenz |  |
| 1030 |  |  [Bridging the Granularity Gap for Acoustic Modeling](https://doi.org/10.18653/v1/2023.findings-acl.688) |  | 0 | While Transformer has become the de-facto standard for speech, modeling upon the fine-grained frame-level features remains an open challenge of capturing long-distance dependencies and distributing the attention weights. We propose Progressive Down-Sampling (PDS) which gradually compresses the acoustic features into coarser-grained units containing more complete semantic information, like text-level representation. In addition, we develop a representation fusion method to alleviate information... | Chen Xu, Yuhao Zhang, Chengbo Jiao, Xiaoqian Liu, Chi Hu, Xin Zeng, Tong Xiao, Anxiang Ma, Huizhen Wang, Jingbo Zhu |  |
| 1031 |  |  [MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System](https://doi.org/10.18653/v1/2023.findings-acl.689) |  | 0 | Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The negative samples in MMSD are not always reasonable. To solve the aforementioned issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings of MMSD, by removing the spurious cues and... | Libo Qin, Shijue Huang, Qiguang Chen, Chenran Cai, Yudi Zhang, Bin Liang, Wanxiang Che, Ruifeng Xu |  |
| 1032 |  |  [Learn to Not Link: Exploring NIL Prediction in Entity Linking](https://doi.org/10.18653/v1/2023.findings-acl.690) |  | 0 | Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem.NEL takes ambiguous entities as seeds,... | Fangwei Zhu, Jifan Yu, Hailong Jin, Lei Hou, Juanzi Li, Zhifang Sui |  |
| 1033 |  |  [On Text-based Personality Computing: Challenges and Future Directions](https://doi.org/10.18653/v1/2023.findings-acl.691) |  | 0 | Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the NLP research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, datasets, performance evaluation, modelling choices, as well as ethics and fairness. When addressing each challenge, not only do we combine perspectives from both NLP and social sciences, but also offer... | Qixiang Fang, Anastasia Giachanou, Ayoub Bagheri, Laura Boeschoten, ErikJan van Kesteren, Mahdi Shafiee Kamalabad, Daniel L. Oberski |  |
| 1034 |  |  [Structured Pruning for Efficient Generative Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.692) |  | 0 | The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deploymentin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we find that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. This study... | Chaofan Tao, Lu Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong |  |
| 1035 |  |  [Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks](https://doi.org/10.18653/v1/2023.findings-acl.693) |  | 0 | Retrieval-augmented methods have received increasing attention to support downstream tasks by leveraging useful information from external resources. Recent studies mainly focus on exploring retrieval to solve knowledge-intensive (KI) tasks. However, the potential of retrieval for most non-knowledge-intensive (NKI) tasks remains under-explored. There are two main challenges to leveraging retrieval-augmented methods for NKI tasks: 1) the demand for diverse relevance score functions and 2) the... | Zhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, Yang Liu |  |
| 1036 |  |  [Contextualized Semantic Distance between Highly Overlapped Texts](https://doi.org/10.18653/v1/2023.findings-acl.694) |  | 0 | Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system’s understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a... | Letian Peng, Zuchao Li, Hai Zhao |  |
| 1037 |  |  [Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.695) |  | 0 | Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to... | Yibin Lei, Liang Ding, Yu Cao, Changtong Zan, Andrew Yates, Dacheng Tao |  |
| 1038 |  |  [Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS](https://doi.org/10.18653/v1/2023.findings-acl.696) |  | 0 | Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use. While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators. This norm assumes that multiple annotators are available, which is not the case for highly specialized tasks or low-resource languages. In this paper, we ask: Can we evaluate the quality of a dataset constructed by a single human annotator? To address... | Maitrey Mehta, Vivek Srikumar |  |
| 1039 |  |  [Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.697) |  | 0 | Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies are often only tested on general knowledge domains and tend to use information sources that are not consistent across tasks. We propose Reinforced Active Learning (RAL) to train a Reinforcement Learning... | Lukas Wertz, Jasmina Bogojeska, Katsiaryna Mirylenka, Jonas Kuhn |  |
| 1040 |  |  [Improving Classroom Dialogue Act Recognition from Limited Labeled Data with Self-Supervised Contrastive Learning Classifiers](https://doi.org/10.18653/v1/2023.findings-acl.698) |  | 0 | Recognizing classroom dialogue acts has significant promise for yielding insight into teaching, student learning, and classroom dynamics. However, obtaining K-12 classroom dialogue data with labels is a significant challenge, and therefore, developing data-efficient methods for classroom dialogue act recognition is essential. This work addresses the challenge of classroom dialogue act recognition from limited labeled data using a contrastive learning-based self-supervised approach (SSCon).... | Vikram Kumaran, Jonathan P. Rowe, Bradford W. Mott, Snigdha Chaturvedi, James C. Lester |  |
| 1041 |  |  [Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation](https://doi.org/10.18653/v1/2023.findings-acl.699) |  | 0 | Visual temporal-aligned translation aims to transform the visual sequence into natural words, including important applicable tasks such as lipreading and fingerspelling recognition. However, various performance habits of specific words by different speakers or signers can lead to visual ambiguity, which has become a major obstacle to the development of current methods. Considering the constraints above, the generalization ability of the translation system is supposed to be further explored... | Linjun Li, Tao Jin, Xize Cheng, Ye Wang, Wang Lin, Rongjie Huang, Zhou Zhao |  |
| 1042 |  |  [Enhancing Cross-lingual Prompting with Dual Prompt Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.700) |  | 0 | Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. hao and Schütze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive Universal Prompting, which helps alleviate the discrepancies between... | Meng Zhou, Xin Li, Yue Jiang, Lidong Bing |  |
| 1043 |  |  [Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI](https://doi.org/10.18653/v1/2023.findings-acl.701) |  | 0 | Users’ physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM... | Alex Mei, Sharon Levy, William Yang Wang |  |
| 1044 |  |  [Multijugate Dual Learning for Low-Resource Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.702) |  | 0 | Dialogue data in real scenarios tend to be sparsely available, rendering data-starved end-to-end dialogue systems trained inadequately. We discover that data utilization efficiency in low-resource scenarios can be enhanced by mining alignment information uncertain utterance and deterministic dialogue state. Therefore, we innovatively implement dual learning in task-oriented dialogues to exploit the correlation of heterogeneous data. In addition, the one-to-one duality is converted into a... | Shimin Li, Xiaotian Zhang, Yanjun Zheng, Linyang Li, Xipeng Qiu |  |
| 1045 |  |  [A Class-Rebalancing Self-Training Framework for Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.703) |  | 0 | Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-training is dominated by the high-performance class in selecting candidates, and deteriorates the low-performance class with the bias of generated pseudo label. To address the class-level imbalance... | Qi Li, Tingyu Xie, Peng Peng, Hongwei Wang, Gaoang Wang |  |
| 1046 |  |  [MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.704) |  | 0 | Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using:... | Swarnadeep Saha, Xinyan Yu, Mohit Bansal, Ramakanth Pasunuru, Asli Celikyilmaz |  |
| 1047 |  |  [Learning by Analogy: Diverse Questions Generation in Math Word Problem](https://doi.org/10.18653/v1/2023.findings-acl.705) |  | 0 | Solving math word problem (MWP) with AI techniques has recently made great progress with the success of deep neural networks (DNN), but it is far from being solved. We argue that the ability of learning by analogy is essential for an MWP solver to better understand same problems which may typically be formulated in diverse ways. However most existing works exploit the shortcut learning to train MWP solvers simply based on samples with a single question. In lack of diverse questions, these... | Zihao Zhou, Maizhen Ning, Qiufeng Wang, Jie Yao, Wei Wang, Xiaowei Huang, Kaizhu Huang |  |
| 1048 |  |  [Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.706) |  | 0 | We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance... | Haode Zhang, Haowen Liang, LiMing Zhan, XiaoMing Wu, Albert Y. S. Lam |  |
| 1049 |  |  [Improving Contrastive Learning of Sentence Embeddings from AI Feedback](https://doi.org/10.18653/v1/2023.findings-acl.707) |  | 0 | Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve... | Qinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang Li, Xipeng Qiu |  |
| 1050 |  |  [Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog](https://doi.org/10.18653/v1/2023.findings-acl.708) |  | 0 | Traditional end-to-end task-oriented dialog systems first convert dialog context into belief state and action state before generating the system response. The system response performance is significantly affected by the quality of the belief state and action state. We first explore what dialog context representation is beneficial to improving the quality of the belief state and action state, which further enhances the generated response quality. To tackle our exploration, we propose Mars, an... | Haipeng Sun, Junwei Bao, Youzheng Wu, Xiaodong He |  |
| 1051 |  |  [Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.709) |  | 0 | The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information... | Pengcheng Jiang, Shivam Agarwal, Bowen Jin, Xuan Wang, Jimeng Sun, Jiawei Han |  |
| 1052 |  |  [Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion](https://doi.org/10.18653/v1/2023.findings-acl.710) |  | 0 | Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships... | WeiJen Ko, Yating Wu, Cutter Dalton, Dananjay Srinivas, Greg Durrett, Junyi Jessy Li |  |
| 1053 |  |  [An Integrated Approach for Political Bias Prediction and Explanation Based on Discursive Structure](https://doi.org/10.18653/v1/2023.findings-acl.711) |  | 0 | One crucial aspect of democracy is fair information sharing. While it is hard to prevent biases in news, they should be identified for better transparency. We propose an approach to automatically characterize biases that takes into account structural differences and that is efficient for long texts. This yields new ways to provide explanations for a textual classifier, going beyond mere lexical cues. We show that: (i) the use of discourse-based structure-aware document representations compare... | Nicolas Devatine, Philippe Muller, Chloé Braud |  |
| 1054 |  |  [Smart Word Suggestions for Writing Assistance](https://doi.org/10.18653/v1/2023.findings-acl.712) |  | 0 | Enhancing word usage is a desired feature for writing assistance. To further advance research in this area, this paper introduces “Smart Word Suggestions” (SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario. This task involves identifying words or phrases that require improvement and providing substitution suggestions. The benchmark includes human-labeled data for testing, a large distantly supervised... | Chenshuo Wang, Shaoguang Mao, Tao Ge, Wenshan Wu, Xun Wang, Yan Xia, Jonathan Tien, Dongyan Zhao |  |
| 1055 |  |  [JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions](https://doi.org/10.18653/v1/2023.findings-acl.713) |  | 0 | Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We proposea new commonsense reasoning dataset based on human’s Interactive Fiction (IF) gameplaywalkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hopreasoning. Moreover, the IF game-based construction... | Mo Yu, Yi Gu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael A. Greenspan, Murray Campbell, Chuang Gan |  |
| 1056 |  |  [A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.714) |  | 0 | Distillation from Weak Teacher (DWT) is a method of transferring knowledge from a smaller, weaker teacher model to a larger student model to improve its performance. Previous studies have shown that DWT can be effective in the vision domain and natural language processing (NLP) pre-training stage. Specifically, DWT shows promise in practical scenarios, such as enhancing new generation or larger models using pre-trained yet older or smaller models and lacking a resource budget. However, the... | Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Sung Ju Hwang, Alexander Min |  |
| 1057 |  |  [SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation](https://doi.org/10.18653/v1/2023.findings-acl.715) |  | 0 | Logical data-to-text generation is a representative task in measuring the capabilities of both language generation and complex reasoning. Despite the introduction of reasoning skills in generation, existing works still rely on neural language models to output the final table description. However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions. To... | Xueliang Zhao, Tingchen Fu, Lemao Liu, Lingpeng Kong, Shuming Shi, Rui Yan |  |
| 1058 |  |  [Boosting Event Extraction with Denoised Structure-to-Text Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.716) |  | 0 | Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting,... | Bo Wang, Heyan Huang, Xiaochi Wei, Ge Shi, Xiao Liu, Chong Feng, Tong Zhou, Shuaiqiang Wang, Dawei Yin |  |
| 1059 |  |  [Detecting Adversarial Samples through Sharpness of Loss Landscape](https://doi.org/10.18653/v1/2023.findings-acl.717) |  | 0 | Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep and narrow local minima of the loss landscape (high sharpness) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter (low sharpness).om... | Rui Zheng, Shihan Dou, Yuhao Zhou, Qin Liu, Tao Gui, Qi Zhang, Zhongyu Wei, Xuanjing Huang, Menghan Zhang |  |
| 1060 |  |  [A Simple, Yet Effective Approach to Finding Biases in Code Generation](https://doi.org/10.18653/v1/2023.findings-acl.718) |  | 0 | Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances. To investigate the effect, we propose the “block of influence” concept, which... | Spyridon Mouselinos, Mateusz Malinowski, Henryk Michalewski |  |
| 1061 |  |  [Membership Inference Attacks against Language Models via Neighbourhood Comparison](https://doi.org/10.18653/v1/2023.findings-acl.719) |  | 0 | Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend toassign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the... | Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan, Taylor BergKirkpatrick |  |
| 1062 |  |  [CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation](https://doi.org/10.18653/v1/2023.findings-acl.720) |  | 0 | We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared... | Rahul Madhavan, Rishabh Garg, Kahini Wadhawan, Sameep Mehta |  |
| 1063 |  |  [Can Diffusion Model Achieve Better Performance in Text Generation ? Bridging the Gap between Training and Inference !](https://doi.org/10.18653/v1/2023.findings-acl.721) |  | 0 | Diffusion models have been successfully adapted to text generation tasks by mapping the discrete text into the continuous space. However, there exist nonnegligible gaps between training and inference, owing to the absence of the forward process during inference. Thus, the model only predicts based on the previously generated reverse noise rather than the noise computed by the forward process. Besides, the widely-used downsampling strategy in speeding up the inference will cause the mismatch of... | Zecheng Tang, Pinzheng Wang, Keyan Zhou, Juntao Li, Ziqiang Cao, Min Zhang |  |
| 1064 |  |  [Topic-Guided Self-Introduction Generation for Social Media Users](https://doi.org/10.18653/v1/2023.findings-acl.722) |  | 0 | Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user’s personal interests. While most prior work profiling users with tags (e.g., ages), we investigate sentence-level self-introductions to provide a more natural and engaging way for users to know each other. Here we exploit a user’s tweeting history to generate their... | Chunpu Xu, Jing Li, Piji Li, Min Yang |  |
| 1065 |  |  [Recyclable Tuning for Continual Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.723) |  | 0 | Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper... | Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 1066 |  |  [BLOCSUM: Block Scope-based Source Code Summarization via Shared Block Representation](https://doi.org/10.18653/v1/2023.findings-acl.724) |  | 0 | Code summarization, which aims to automatically generate natural language descriptions from source code, has become an essential task in software development for better program understanding. Abstract Syntax Tree (AST), which represents the syntax structure of the source code, is helpful when utilized together with the sequence of code tokens to improve the quality of code summaries. Recent works on code summarization attempted to capture the sequential and structural information of the source... | YunSeok Choi, Hyojun Kim, JeeHyong Lee |  |
| 1067 |  |  [HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.725) |  | 0 | With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a... | Zhengkun Zhang, Wenya Guo, Xiaojun Meng, Yasheng Wang, Yadao Wang, Xin Jiang, Qun Liu, Zhenglu Yang |  |
| 1068 |  |  [Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations](https://doi.org/10.18653/v1/2023.findings-acl.726) |  | 0 | We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sentences into fragments, clustering the fragments to abstract away syntactic variations of the same meaning, and predicting predicate-argument relations between the fragments. To better model the... | Zixuan Ling, Xiaoqing Zheng, Jianhan Xu, Jinshu Lin, KaiWei Chang, ChoJui Hsieh, Xuanjing Huang |  |
| 1069 |  |  [Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training](https://doi.org/10.18653/v1/2023.findings-acl.727) |  | 0 | Relation Extraction (RE) is the task of identifying semantic relation between real-world entities mentioned in text. Despite significant progress in RE research, a remaining challenge for RE concerns the lack of training data for data-hungry deep learning models. Cost of annotation and difficulty of the task are among hindrance to collect a large-scale RE dataset in different domains. To address this limitation, we propose a novel framework to automatically generate labeled data for RE. Our... | Amir Pouran Ben Veyseh, Franck Dernoncourt, Bonan Min, Thien Huu Nguyen |  |
| 1070 |  |  [Disfluency Generation for More Robust Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-acl.728) |  | 0 | Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only contain a marginal number of disfluent utterances. Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances.... | Benjamin Marie |  |
| 1071 |  |  [Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting](https://doi.org/10.18653/v1/2023.findings-acl.729) |  | 0 | Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural... | Chen Chen, Yufei Wang, Aixin Sun, Bing Li, KwokYan Lam |  |
| 1072 |  |  [Revisiting Pathologies of Neural Models under Input Reduction](https://doi.org/10.18653/v1/2023.findings-acl.730) |  | 0 | We revisit the question of why neural models tend to produce high-confidence predictions on inputs that appear nonsensical to humans. Previous work has suggested that the models fail to assign low probabilities to such inputs due to model overconfidence. We evaluate various regularization methods on fact verification benchmarks and find that this problem persists even with well-calibrated or underconfident models, suggesting that overconfidence is not the only underlying cause. We also find... | Canasai Kruengkrai, Junichi Yamagishi |  |
| 1073 |  |  [Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.731) |  | 0 | Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of... | Fei Yuan, Yinquan Lu, Wenhao Zhu, Lingpeng Kong, Lei Li, Yu Qiao, Jingjing Xu |  |
| 1074 |  |  [FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference](https://doi.org/10.18653/v1/2023.findings-acl.732) |  | 0 | Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose... | Michiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit Sanghai, Fei Sha, William W. Cohen |  |
| 1075 |  |  [Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark](https://doi.org/10.18653/v1/2023.findings-acl.733) |  | 0 | Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use... | Jason HoelscherObermaier, Julia Persson, Esben Kran, Ioannis Konstas, Fazl Barez |  |
| 1076 |  |  [Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data](https://doi.org/10.18653/v1/2023.findings-acl.734) |  | 0 | This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains... | Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu |  |
| 1077 |  |  [Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt](https://doi.org/10.18653/v1/2023.findings-acl.735) |  | 0 | We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot... | Xiaocui Yang, Shi Feng, Daling Wang, Qi Sun, Wenfang Wu, Yifei Zhang, Pengfei Hong, Soujanya Poria |  |
| 1078 |  |  [Predicting Human Translation Difficulty Using Automatic Word Alignment](https://doi.org/10.18653/v1/2023.findings-acl.736) |  | 0 | Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these behavioral measures are costly and do not scale. In this work, we use word alignments computed over large scale bilingual corpora to develop predictors of lexical translation difficulty. We evaluate our... | Zheng Wei Lim, Trevor Cohn, Charles Kemp, Ekaterina Vylomova |  |
| 1079 |  |  [Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning](https://doi.org/10.18653/v1/2023.findings-acl.737) |  | 0 | A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on if and how such knowledge of the downstream fine-tuning approach calls for complementary measures after pre-training and before fine-tuning. In this work, we show that taking the ultimate choice of... | Mozhdeh Gheini, Xuezhe Ma, Jonathan May |  |
| 1080 |  |  [Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking](https://doi.org/10.18653/v1/2023.findings-acl.738) |  | 0 | Neural information retrieval (IR) systems have progressed rapidly in recent years, in large part due to the release of publicly available benchmarking tasks. Unfortunately, some dimensions of this progress are illusory: the majority of the popular IR benchmarks today focus exclusively on downstream task accuracy and thus conceal the costs incurred by systems that trade away efficiency for quality. Latency, hardware cost, and other efficiency considerations are paramount to the deployment of IR... | Keshav Santhanam, Jon SaadFalcon, Martin Franz, Omar Khattab, Avi Sil, Radu Florian, Md. Arafat Sultan, Salim Roukos, Matei Zaharia, Christopher Potts |  |
| 1081 |  |  [AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese](https://doi.org/10.18653/v1/2023.findings-acl.739) |  | 0 | Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce... | Abhijnan Nath, Sheikh Mannan, Nikhil Krishnaswamy |  |
| 1082 |  |  [An Exploratory Study on Model Compression for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.740) |  | 0 | Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence... | Shuo Sun, Yuze Gao, Yuchen Zhang, Jian Su, Bin Chen, Yingzhan Lin, Shuqi Sun |  |
| 1083 |  |  [FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models](https://doi.org/10.18653/v1/2023.findings-acl.741) |  | 0 | Stutter removal is an essential scenario in the field of speech editing. However, when the speech recording contains stutters, the existing text-based speech editing approaches still suffer from: 1) the over-smoothing problem in the edited speech; 2) lack of robustness due to the noise introduced by stutter; 3) to remove the stutters, users are required to determine the edited region manually. To tackle the challenges in stutter removal, we propose FluentSpeech, a stutter-oriented automatic... | Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, Yi Ren, Zhou Zhao |  |
| 1084 |  |  [HyHTM: Hyperbolic Geometry-based Hierarchical Topic Model](https://doi.org/10.18653/v1/2023.findings-acl.742) |  | 0 | Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry-based Hierarchical Topic Model - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to... | Simra Shahid, Tanay Anand, Nikitha Srikanth, Sumit Bhatia, Balaji Krishnamurthy, Nikaash Puri |  |
| 1085 |  |  [KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding](https://doi.org/10.18653/v1/2023.findings-acl.743) |  | 0 | Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these... | Zijun Yao, Yantao Liu, Xin Lv, Shulin Cao, Jifan Yu, Juanzi Li, Lei Hou |  |
| 1086 |  |  [DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies](https://doi.org/10.18653/v1/2023.findings-acl.744) |  | 0 | Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, in real-world scenarios, only the latest KB snapshot is available during training and as a result, the train dialogs may contain facts conflicting with the latest KB. These dialog-KB inconsistencies... | Vishal Vivek Saley, Rocktim Jyoti Das, Dinesh Raghu, Mausam |  |
| 1087 |  |  [Scale-Invariant Infinite Hierarchical Topic Model](https://doi.org/10.18653/v1/2023.findings-acl.745) |  | 0 | Hierarchical topic models have been employed to organize a large number of diverse topics from corpora into a latent tree structure. However, existing models yield fragmented topics with overlapping themes whose expected probability becomes exponentially smaller along the depth of the tree. To solve this intrinsic problem, we propose a scale-invariant infinite hierarchical topic model (ihLDA). The ihLDA adaptively adjusts the topic creation to make the expected topic probability decay... | Shusei Eshima, Daichi Mochihashi |  |
| 1088 |  |  [RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.746) |  | 0 | Multilingual vision-language (V&L) pre-training has achieved remarkable progress in learning universal representations across different modalities and languages. In spite of recent success, there still remain challenges limiting further improvements of V&L pre-trained models in multilingual settings. Particularly, current V&L pre-training methods rely heavily on strictly-aligned multilingual image-text pairs generated from English-centric datasets through machine translation. However, the cost... | Chulun Zhou, Yunlong Liang, Fandong Meng, Jinan Xu, Jinsong Su, Jie Zhou |  |
| 1089 |  |  [Deep Equilibrium Non-Autoregressive Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.747) |  | 0 | In this work, we argue that non-autoregressive (NAR) sequence generative models can equivalently be regarded as an iterative refinement process towards the target sequence, implying an underlying dynamical system of NAR model: z = f (z, x) → y. In such a way, the optimal prediction of a NAR model should be the equilibrium state of its dynamics if given infinitely many iterations. However, this is infeasible in practice due to limited computational and memory budgets. To this end, we propose... | Zaixiang Zheng, Yi Zhou, Hao Zhou |  |
| 1090 |  |  [ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.748) |  | 0 | With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using... | Yue Yu, Yuchen Zhuang, Rongzhi Zhang, Yu Meng, Jiaming Shen, Chao Zhang |  |
| 1091 |  |  [Race, Gender, and Age Biases in Biomedical Masked Language Models](https://doi.org/10.18653/v1/2023.findings-acl.749) |  | 0 | Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora. To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models. We curate prompts based on evidence-based practice and compare generated diagnoses... | Michelle Kim, Junghwan Kim, Kristen Marie Johnson |  |
| 1092 |  |  [Neighboring Words Affect Human Interpretation of Saliency Explanations](https://doi.org/10.18653/v1/2023.findings-acl.750) |  | 0 | Word-level saliency explanations (“heat maps over words”) are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to investigate how the marking of a word’s \*neighboring words\* affect the explainee’s perception of the word’s importance in the context of a saliency explanation. We find that neighboring words have... | Alon Jacovi, Hendrik Schuff, Heike Adel, Ngoc Thang Vu, Yoav Goldberg |  |
| 1093 |  |  [HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models](https://doi.org/10.18653/v1/2023.findings-acl.751) |  | 0 | Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper,... | Swaroop Mishra, Elnaz Nouri |  |
| 1094 |  |  [Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification](https://doi.org/10.18653/v1/2023.findings-acl.752) |  | 0 | Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies primarily rest on grasping either unstructured evidence or potential reasoning paths from structured knowledge bases, yet failing to exploit the benefits of heterogeneous knowledge simultaneously. In... | Anni Zou, Zhuosheng Zhang, Hai Zhao |  |
| 1095 |  |  [DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect](https://doi.org/10.18653/v1/2023.findings-acl.753) |  | 0 | Recently, binaural audio synthesis (BAS) has emerged as a promising research field for its applications in augmented and virtual realities. Binaural audio helps ususers orient themselves and establish immersion by providing the brain with interaural time differences reflecting spatial information. However, existing BAS methods are limited in terms of phase estimation, which is crucial for spatial hearing. In this paper, we propose the DopplerBAS method to explicitly address the Doppler effect... | Jinglin Liu, Zhenhui Ye, Qian Chen, Siqi Zheng, Wen Wang, Qinglin Zhang, Zhou Zhao |  |
| 1096 |  |  [Easy-to-Hard Learning for Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.754) |  | 0 | Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human... | Chang Gao, Wenxuan Zhang, Wai Lam, Lidong Bing |  |
| 1097 |  |  [SConE: Simplified Cone Embeddings with Symbolic Operators for Complex Logical Queries](https://doi.org/10.18653/v1/2023.findings-acl.755) |  | 0 | Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (FOL) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods depend on the neural approach to model FOL operators (conjunction, disjunction and negation), which are not easily explainable with considerable computation cost. We overcome this challenge by... | Chau D. M. Nguyen, Tim French, Wei Liu, Michael Stewart |  |
| 1098 |  |  [Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.756) |  | 0 | The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually with multimodal information. Nevertheless, news videos from different perspectives regarding the same event are commonly posted together, which contain complementary or contradictory information and... | Peng Qi, Yuyang Zhao, Yufeng Shen, Wei Ji, Juan Cao, TatSeng Chua |  |
| 1099 |  |  [An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts](https://doi.org/10.18653/v1/2023.findings-acl.757) |  | 0 | With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings. Motivated with limited... | Muskan Garg, Amirmohammad Shahbandegan, Amrit Chadha, Vijay Mago |  |
| 1100 |  |  [Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control](https://doi.org/10.18653/v1/2023.findings-acl.758) |  | 0 | Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions.... | Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 1101 |  |  [Connectivity Patterns are Task Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.759) |  | 0 | Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological... | Zhiheng Xi, Rui Zheng, Yuansen Zhang, Xuanjing Huang, Zhongyu Wei, Minlong Peng, Mingming Sun, Qi Zhang, Tao Gui |  |
| 1102 |  |  [Improving Autoregressive Grammatical Error Correction with Non-autoregressive Models](https://doi.org/10.18653/v1/2023.findings-acl.760) |  | 0 | Grammatical Error Correction (GEC) aims to correct grammatical errors in sentences. We find that autoregressive models tend to assign low probabilities to tokens that need corrections. Here we introduce additional signals to the training of GEC models so that these systems can learn to better predict at ambiguous positions. To do this, we use a non-autoregressive model as an auxiliary model, and develop a new regularization term of training by considering the difference in predictions between... | Hang Cao, Zhiquan Cao, Chi Hu, Baoyu Hou, Tong Xiao, Jingbo Zhu |  |
| 1103 |  |  [SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives](https://doi.org/10.18653/v1/2023.findings-acl.761) |  | 0 | Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from the same encoder towers to the negatives, for which we name it as “contrastive loss with SAMe TOwer NEgatives” (SamToNe). By evaluating on question answering retrieval benchmarks from MS MARCO and MultiReQA,... | Fedor Moiseev, Gustavo Hernández Abrego, Péter Dornbach, Imed Zitouni, Enrique Alfonseca, Zhe Dong |  |
| 1104 |  |  [On the Strength of Sequence Labeling and Generative Models for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-acl.762) |  | 0 | Generative models have achieved great success in aspect sentiment triplet extraction tasks. However, existing methods ignore the mutual informative clues between aspect and opinion terms and may generate false paired triplets. Furthermore, the inherent limitations of generative models, i.e., the token-by-token decoding and the simple structured prompt, prevent models from handling complex structures especially multi-word terms and multi-triplet sentences. To address these issues, we propose a... | Shen Zhou, Tieyun Qian |  |
| 1105 |  |  [Revisiting Non-Autoregressive Translation at Scale](https://doi.org/10.18653/v1/2023.findings-acl.763) |  | 0 | In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance.... | Zhihao Wang, Longyue Wang, Jinsong Su, Junfeng Yao, Zhaopeng Tu |  |
| 1106 |  |  [Improving Radiology Summarization with Radiograph and Anatomy Prompts](https://doi.org/10.18653/v1/2023.findings-acl.764) |  | 0 | The impression is crucial for the referring physicians to grasp key information since it is concluded from the findings and reasoning of radiologists. To alleviate the workload of radiologists and reduce repetitive human labor in impression writing, many researchers have focused on automatic impression generation. However, recent works on this task mainly summarize the corresponding findings and pay less attention to the radiology images. In clinical, radiographs can provide more detailed... | Jinpeng Hu, Zhihong Chen, Yang Liu, Xiang Wan, TsungHui Chang |  |
| 1107 |  |  [Explanation Regeneration via Information Bottleneck](https://doi.org/10.18653/v1/2023.findings-acl.765) |  | 0 | Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated... | Qintong Li, Zhiyong Wu, Lingpeng Kong, Wei Bi |  |
| 1108 |  |  [Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization](https://doi.org/10.18653/v1/2023.findings-acl.766) |  | 0 | The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among... | Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 1109 |  |  [ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision](https://doi.org/10.18653/v1/2023.findings-acl.767) |  | 0 | Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we... | Ming Zhong, Siru Ouyang, Minhao Jiang, Vivian Hu, Yizhu Jiao, Xuan Wang, Jiawei Han |  |
| 1110 |  |  [Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.768) |  | 0 | We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when... | YungSung Chuang, Wei Fang, ShangWen Li, Wentau Yih, James R. Glass |  |
| 1111 |  |  [Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets](https://doi.org/10.18653/v1/2023.findings-acl.769) |  | 0 | We propose a semi-supervised text classifier based on self-training using one positive and one negative property of neural networks. One of the weaknesses of self-training is the semantic drift problem, where noisy pseudo-labels accumulate over iterations and consequently the error rate soars. In order to tackle this challenge, we reshape the role of pseudo-labels and create a hierarchical order of information. In addition, a crucial step in self-training is to use the classifier confidence... | Payam Karisani |  |
| 1112 |  |  [Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training](https://doi.org/10.18653/v1/2023.findings-acl.770) |  | 0 | Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of... | Jing Huang, Zhengxuan Wu, Kyle Mahowald, Christopher Potts |  |
| 1113 |  |  [Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning](https://doi.org/10.18653/v1/2023.findings-acl.771) |  | 0 | Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a... | Daniel Saggau, Mina Rezaei, Bernd Bischl, Ilias Chalkidis |  |
| 1114 |  |  [QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.772) |  | 0 | Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (i.e., textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent... | Ziming Li, Yan Zhou, Yaxin Liu, Fuqing Zhu, Chuanpeng Yang, Songlin Hu |  |
| 1115 |  |  [Language acquisition: do children and language models follow similar learning stages?](https://doi.org/10.18653/v1/2023.findings-acl.773) |  | 0 | During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of human children. Specifically, we test whether, during its training,... | Linnea Evanson, Yair Lakretz, JeanRémi King |  |
| 1116 |  |  [The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.774) |  | 0 | In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query language. We observe that the query vocabulary is distinct from human vocabulary. Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced... | Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, Chris Biemann |  |
| 1117 |  |  [UniCOQE: Unified Comparative Opinion Quintuple Extraction As A Set](https://doi.org/10.18653/v1/2023.findings-acl.775) |  | 0 | Comparative Opinion Quintuple Extraction (COQE) aims to identify comparative opinion sentences in product reviews, extract comparative opinion elements in the sentences, and then incorporate them into quintuples. Existing methods decompose the COQE task into multiple primary subtasks and then solve them in a pipeline manner. However, these approaches ignore the intrinsic connection between subtasks and the error propagation among stages. This paper proposes a unified generative model, UniCOQE,... | Zinong Yang, Feng Xu, Jianfei Yu, Rui Xia |  |
| 1118 |  |  [Response-conditioned Turn-taking Prediction](https://doi.org/10.18653/v1/2023.findings-acl.776) |  | 0 | Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both... | Bing'er Jiang, Erik Ekstedt, Gabriel Skantze |  |
| 1119 |  |  [A Unified One-Step Solution for Aspect Sentiment Quad Prediction](https://doi.org/10.18653/v1/2023.findings-acl.777) |  | 0 | Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings... | Junxian Zhou, Haiqin Yang, Yuxuan He, Hao Mou, Junbo Yang |  |
| 1120 |  |  [On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning](https://doi.org/10.18653/v1/2023.findings-acl.778) |  | 0 | Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes... | Chenghao Xiao, Yang Long, Noura Al Moubayed |  |
| 1121 |  |  [Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.779) |  | 0 | Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the... | Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar |  |
| 1122 |  |  [Common Law Annotations: Investigating the Stability of Dialog System Output Annotations](https://doi.org/10.18653/v1/2023.findings-acl.780) |  | 0 | Metrics for Inter-Annotator Agreement (IAA), like Cohen’s Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure or reproducibility. While researchers are encouraged to increase annotator agreement, this can lead to specific and tailored annotation guidelines. We hypothesize that this may result in diverging annotations from different groups. To study this, we first propose the... | Seunggun Lee, Alexandra DeLucia, Nikita Nangia, Praneeth Ganedi, Ryan Guan, Rubing Li, Britney Ngaw, Aditya Singhal, Shalaka Vaidya, Zijun Yuan, Lining Zhang, João Sedoc |  |
| 1123 |  |  [HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models](https://doi.org/10.18653/v1/2023.findings-acl.781) |  | 0 | Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating shortcut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to... | Yuqi Ren, Deyi Xiong |  |
| 1124 |  |  [PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data](https://doi.org/10.18653/v1/2023.findings-acl.782) |  | 0 | Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs. Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data. In this work, we propose PMI-Align which computes and uses the point-wise mutual information between source and target tokens to extract... | Fatemeh Azadi, Heshaam Faili, Mohammad Javad Dousti |  |
| 1125 |  |  [Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities](https://doi.org/10.18653/v1/2023.findings-acl.783) |  | 0 | Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs. Conversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives. However, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings – newspaper headlines, dialogues, and tweets, among others. In this... | Riccardo Orlando, Simone Conia, Roberto Navigli |  |
| 1126 |  |  [DSPM-NLG: A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.784) |  | 0 | In few-shot settings, fully conveying the semantic information of the dialogue act is a crucial challenge for Natural Language Generation (NLG) in the task-oriented dialogue system. An interesting fact is that NLG and Spoken Language Understanding (SLU) are a natural dual problem pair. Suppose the response generated by the NLG module can be restored to the corresponding dialogue act by the SLU module, which reflects that the generated response fully conveys the semantic information of the... | Yufan Wang, Bowei Zou, Rui Fan, Ai Ti Aw, Tingting He |  |
| 1127 |  |  [TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.785) |  | 0 | Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt (Xiang et al., 2022) has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an... | Wei Xiang, Chao Liang, Bang Wang |  |
| 1128 |  |  [Evaluating Factuality in Cross-lingual Summarization](https://doi.org/10.18653/v1/2023.findings-acl.786) |  | 0 | Cross-lingual summarization aims to help people efficiently grasp the core idea of the document written in a foreign language. Modern text summarization models generate highly fluent but often factually inconsistent outputs, which has received heightened attention in recent research. However, the factual consistency of cross-lingual summarization has not been investigated yet. In this paper, we propose a cross-lingual factuality dataset by collecting human annotations of reference summaries as... | Mingqi Gao, Wenqing Wang, Xiaojun Wan, Yuemei Xu |  |
| 1129 |  |  [On the Correspondence between Compositionality and Imitation in Emergent Neural Communication](https://doi.org/10.18653/v1/2023.findings-acl.787) |  | 0 | Compositionality is a hallmark of human language that not only enables linguistic generalization, but also potentially facilitates acquisition. When simulating language emergence with neural networks, compositionality has been shown to improve communication performance; however, its impact on imitation learning has yet to be investigated. Our work explores the link between compositionality and imitation in a Lewis game played by deep neural agents. Our contributions are twofold: first, we show... | Emily Cheng, Mathieu Rita, Thierry Poibeau |  |
| 1130 |  |  [The Coreference under Transformation Labeling Dataset: Entity Tracking in Procedural Texts Using Event Models](https://doi.org/10.18653/v1/2023.findings-acl.788) |  | 0 | We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accurately link entities in anaphoric and coreference relations without an understanding of the transformations those entities undergo. We show how adding event semantics helps to better model entity... | Kyeongmin Rim, Jingxuan Tu, Bingyang Ye, Marc Verhagen, Eben Holderness, James Pustejovsky |  |
| 1131 |  |  [Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution](https://doi.org/10.18653/v1/2023.findings-acl.789) |  | 0 | Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for... | Tianjian Li, Kenton Murray |  |
| 1132 |  |  [Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation](https://doi.org/10.18653/v1/2023.findings-acl.790) |  | 0 | In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for regulating the generation to consider the item discrimination index in education evaluation. Third, we explore the candidate augmentation strategy and multi-tasking training with cloze-related tasks to... | HuiJuan Wang, KaiYu Hsieh, HanCheng Yu, JuiChing Tsou, YuAn Shih, ChenHua Huang, YaoChung Fan |  |
| 1133 |  |  [Lexical Translation Inconsistency-Aware Document-Level Translation Repair](https://doi.org/10.18653/v1/2023.findings-acl.791) |  | 0 | Following the idea of “one translation per discourse”, in this paper we aim to improve translation consistency via document-level translation repair (DocRepair), i.e., automatic post-editing on translations of documents. To this end, we propose a lexical translation inconsistency-aware DocRepair to explicitly model translation inconsistency. First we locate the inconsistency in automatic translation. Then we provide translation candidates for those inconsistency. Finally, we propose... | Zhen Zhang, Junhui Li, Shimin Tao, Hao Yang |  |
| 1134 |  |  [CausalDialogue: Modeling Utterance-level Causality in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.792) |  | 0 | Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explore this concept, we have compiled and expanded upon a new dataset called CausalDialogue through crowd-sourcing. This dataset includes multiple cause-effect pairs within a directed acyclic graph (DAG)... | YiLin Tuan, Alon Albalak, Wenda Xu, Michael Saxon, Connor Pryor, Lise Getoor, William Yang Wang |  |
| 1135 |  |  [Towards Unified Spoken Language Understanding Decoding via Label-aware Compact Linguistics Representations](https://doi.org/10.18653/v1/2023.findings-acl.793) |  | 0 | Joint intent detection and slot filling models have shown promising success in recent years due to the high correlations between the two tasks. However, previous works independently decode the two tasks, which could result in misaligned predictions for both tasks. To address this shortcoming, we propose a novel method named Label-aware Compact Linguistics Representation (LCLR), which leverages label embeddings to jointly guide the decoding process. Concretely, LCLR projects both task-specific... | Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen, Yuexian Zou |  |
| 1136 |  |  [Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses](https://doi.org/10.18653/v1/2023.findings-acl.794) |  | 0 | A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, “less likely brainstorming,” that... | Liyan Tang, Yifan Peng, Yanshan Wang, Ying Ding, Greg Durrett, Justin F. Rousseau |  |
| 1137 |  |  [Language Modeling with Latent Situations](https://doi.org/10.18653/v1/2023.findings-acl.795) |  | 0 | Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SITUATIONSUPERVISION has two components: an \*auxiliary situation modeling\* task that trains models to predict entity state representations... | Belinda Z. Li, Maxwell I. Nye, Jacob Andreas |  |
| 1138 |  |  [Can Cross-Lingual Transferability of Multilingual Transformers Be Activated Without End-Task Data?](https://doi.org/10.18653/v1/2023.findings-acl.796) |  | 0 | Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when end-task data are unavailable. In this work, we explore whether the cross-lingual transferability can be activated without end-task data. We propose a cross-lingual transfer method, named PlugIn-X.... | Zewen Chi, Heyan Huang, XianLing Mao |  |
| 1139 |  |  [Focus-aware Response Generation in Inquiry Conversation](https://doi.org/10.18653/v1/2023.findings-acl.797) |  | 0 | Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in... | Yiquan Wu, Weiming Lu, Yating Zhang, Adam Jatowt, Jun Feng, Changlong Sun, Fei Wu, Kun Kuang |  |
| 1140 |  |  [A Hierarchical Explanation Generation Method Based on Feature Interaction Detection](https://doi.org/10.18653/v1/2023.findings-acl.798) |  | 0 | The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical attributions tends to limit the text groups to a continuous text span, which we call the connecting rule. While easy for humans to read, limiting the attribution unit to a continuous span might lose... | Yiming Ju, Yuanzhe Zhang, Kang Liu, Jun Zhao |  |
| 1141 |  |  [Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning](https://doi.org/10.18653/v1/2023.findings-acl.799) |  | 0 | Efficient finetuning of pretrained language transformers is becoming increasingly prevalent for solving natural language processing tasks. While effective, it can still require a large number of tunable parameters. This can be a drawback for low-resource applications and training with differential-privacy constraints, where excessive noise may be introduced during finetuning. To this end, we propose a novel language transformer finetuning strategy that introduces task-specific parameters in... | Umang Gupta, Aram Galstyan, Greg Ver Steeg |  |
| 1142 |  |  [A Diffusion Model for Event Skeleton Generation](https://doi.org/10.18653/v1/2023.findings-acl.800) |  | 0 | Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a graph generation perspective but suffer from noise-sensitive and error accumulation, e.g., the inability to correct errors while generating schema. We, therefore, propose a novel Diffusion Event Graph... | Fangqi Zhu, Lin Zhang, Jun Gao, Bing Qin, Ruifeng Xu, Haiqin Yang |  |
| 1143 |  |  [Nonparametric Decoding for Generative Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.801) |  | 0 | The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab... | Hyunji Lee, Jaeyoung Kim, Hoyeon Chang, Hanseok Oh, Sohee Yang, Vladimir Karpukhin, Yi Lu, Minjoon Seo |  |
| 1144 |  |  [Aspect-aware Unsupervised Extractive Opinion Summarization](https://doi.org/10.18653/v1/2023.findings-acl.802) |  | 0 | Extractive opinion summarization extracts sentences from users’ reviews to represent the prevalent opinions about a product or service. However, the extracted sentences can be redundant and may miss some important aspects, especially for centroid-based extractive summarization models (Radev et al., 2004). To alleviate these issues, we introduce TokenCluster– a method for unsupervised extractive opinion summarization that automatically identifies the aspects described in the review sentences and... | Haoyuan Li, Somnath Basu Roy Chowdhury, Snigdha Chaturvedi |  |
| 1145 |  |  [GNN-SL: Sequence Labeling Based on Nearest Examples via GNN](https://doi.org/10.18653/v1/2023.findings-acl.803) |  | 0 | To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word... | Shuhe Wang, Yuxian Meng, Rongbin Ouyang, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Guoyin Wang |  |
| 1146 |  |  [Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.804) |  | 0 | Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with... | Xinyi Wang, Zitao Wang, Wei Hu |  |
| 1147 |  |  [Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond](https://doi.org/10.18653/v1/2023.findings-acl.805) |  | 0 | Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently.... | HawShiuan Chang, Zonghai Yao, Alolika Gon, Hong Yu, Andrew McCallum |  |
| 1148 |  |  [GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective](https://doi.org/10.18653/v1/2023.findings-acl.806) |  | 0 | Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the... | Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, Yue Zhang |  |
| 1149 |  |  [Investigating the Saliency of Sentiment Expressions in Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.807) |  | 0 | We examine the behaviour of an aspect-based sentiment classifier built by fine-tuning the BERT BASE model on the SemEval 2016 English dataset. In a set of masking experiments, we examine the extent to which the tokens identified as salient by LIME and a gradient-based method are being used by the classifier. We find that both methods are able to produce faithful rationales, with LIME outperforming the gradient-based method. We also identify a set of manually annotated sentiment expressions for... | Joachim Wagner, Jennifer Foster |  |
| 1150 |  |  [DMLM: Descriptive Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.808) |  | 0 | Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word... | Edoardo Barba, Niccolò Campolungo, Roberto Navigli |  |
| 1151 |  |  [Reproducibility in NLP: What Have We Learned from the Checklist?](https://doi.org/10.18653/v1/2023.findings-acl.809) |  | 0 | Scientific progress in NLP rests on the reproducibility of researchers’ claims. The \*CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist’s... | Ian Magnusson, Noah A. Smith, Jesse Dodge |  |
| 1152 |  |  [Domain Generalization via Switch Knowledge Distillation for Robust Review Representation](https://doi.org/10.18653/v1/2023.findings-acl.810) |  | 0 | Applying neural models injected with in-domain user and product information to learn review representations of unseen or anonymous users incurs an obvious obstacle in content-based recommender systems. For the generalization of the in-domain classifier, most existing models train an extra plain-text model for the unseen domain. Without incorporating historical user and product information, such a schema makes unseen and anonymous users dissociate from the recommender system. To simultaneously... | You Zhang, Jin Wang, LiangChih Yu, Dan Xu, Xuejie Zhang |  |
| 1153 |  |  [On Search Strategies for Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.811) |  | 0 | Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes... | Christian Herold, Hermann Ney |  |
| 1154 |  |  [Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.812) |  | 0 | Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability. MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias. In this paper, we propose a novel Causal Interventional paradigm for MRC (CI4MRC) to... | Jiazheng Zhu, Shaojuan Wu, Xiaowang Zhang, Yuexian Hou, Zhiyong Feng |  |
| 1155 |  |  [Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias](https://doi.org/10.18653/v1/2023.findings-acl.813) |  | 0 | While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts — thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and... | Venkata Subrahmanyan Govindarajan, David Beaver, Kyle Mahowald, Junyi Jessy Li |  |
| 1156 |  |  [SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme](https://doi.org/10.18653/v1/2023.findings-acl.814) |  | 0 | Although lyrics generation has achieved significant progress in recent years, it has limited practical applications because the generated lyrics cannot be performed without composing compatible melodies. In this work, we bridge this practical gap by proposing a song rewriting system which rewrites the lyrics of an existing song such that the generated lyrics are compatible with the rhythm of the existing melody and thus singable. In particular, we propose SongRewriter, a controllable Chinese... | Yusen Sun, Liangyou Li, Qun Liu, DitYan Yeung |  |
| 1157 |  |  [Triplet-Free Knowledge-Guided Response Generation](https://doi.org/10.18653/v1/2023.findings-acl.815) |  | 0 | Generating vivid and informative responses (e.g., comments for social posts and utterances for dialogues) is challenging without giving relevant knowledge. Prior works focus on constructing the ”latent” knowledge first and then learning how to ”ground” it based on pseudo (context, knowledge, response) triplets. However, the retrieval between real responses and their latent knowledge is difficult in nature. In this paper, instead of focusing on how to ground knowledge given the responses, we... | Dongming Li, Jianfeng Liu, Baoyuan Wang |  |
| 1158 |  |  [Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.816) |  | 0 | Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations... | Matthew Raffel, Lizhong Chen |  |
| 1159 |  |  [Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance](https://doi.org/10.18653/v1/2023.findings-acl.817) |  | 0 | Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which... | Wanlong Liu, Shaohuan Cheng, Dingyi Zeng, Hong Qu |  |
| 1160 |  |  [Exploring the Impact of Vision Features in News Image Captioning](https://doi.org/10.18653/v1/2023.findings-acl.818) |  | 0 | The task of news image captioning aims to generate a detailed caption which describes the specific information of an image in a news article. However, we find that recent state-of-art models can achieve competitive performance even without vision features. To resolve the impact of vision features in the news image captioning task, we conduct extensive experiments with mainstream models based on encoder-decoder framework. From our exploration, we find 1) vision features do contribute to the... | Junzhe Zhang, Xiaojun Wan |  |
| 1161 |  |  [Using Collostructional Analysis to evaluate BERT's representation of linguistic constructions](https://doi.org/10.18653/v1/2023.findings-acl.819) |  | 0 | Collostructional analysis is a technique devised to find correlations between particular words and linguistic constructions in order to analyse meaning associations of these constructions. Contrasting collostructional analysis results with output from BERT might provide insights into the way BERT represents the meaning of linguistic constructions. This study tests to what extent English BERT’s meaning representations correspond to known constructions from the linguistics literature by means of... | Tim Veenboer, Jelke Bloem |  |
| 1162 |  |  [Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation](https://doi.org/10.18653/v1/2023.findings-acl.820) |  | 0 | Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches, namely round-trip and prompt-based... | Xingdi Yuan, Tong Wang, YenHsiang Wang, Emery Fine, Rania Abdelghani, Hélène Sauzéon, PierreYves Oudeyer |  |
| 1163 |  |  [Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.821) |  | 0 | Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment... | Fan Qian, Jiqing Han, Yongjun He, Tieran Zheng, Guibin Zheng |  |
| 1164 |  |  [Theory of Mind in Freely-Told Children's Narratives: A Classification Approach](https://doi.org/10.18653/v1/2023.findings-acl.822) |  | 0 | Children are the focal point for studying the link between language and Theory of Mind (ToM) competence. Language and ToM are often studied with younger children and standardized tests, but as both are social competences, data and methods with higher ecological validity are critical. We leverage a corpus of 442 freely-told stories by Dutch children aged 4-12, recorded in their everyday classroom environments, to study language and ToM with NLP-tools. We labelled stories according to the mental... | Bram van Dijk, Marco Spruit, Max J. van Duijn |  |
| 1165 |  |  [Better Language Models of Code through Self-Improvement](https://doi.org/10.18653/v1/2023.findings-acl.823) |  | 0 | Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which... | Hung Quoc To, Nghi D. Q. Bui, Jin L. C. Guo, Tien N. Nguyen |  |
| 1166 |  |  [Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://doi.org/10.18653/v1/2023.findings-acl.824) |  | 0 | BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by... | Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, Jason Wei |  |
| 1167 |  |  [Score It All Together: A Multi-Task Learning Study on Automatic Scoring of Argumentative Essays](https://doi.org/10.18653/v1/2023.findings-acl.825) |  | 0 | When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. On the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from additional information about context, writing prompt and argument type. We then explore the different combinations of three tasks: automated span detection, type and quality prediction. Results show... | Yuning Ding, Marie Bexte, Andrea Horbach |  |
| 1168 |  |  [Data Sampling and (In)stability in Machine Translation Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.826) |  | 0 | We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling procedure as separate from questions about human annotator consistency. We provide evidence that the latest data sampling approach used at WMT skews the annotated data toward shorter documents, not... | Chikiu Lo, Rebecca Knowles |  |
| 1169 |  |  [Probing Graph Decomposition for Argument Pair Extraction](https://doi.org/10.18653/v1/2023.findings-acl.827) |  | 0 | Argument pair extraction (APE) aims to extract interactive argument pairs from two passages within a discussion. The key challenge of APE is to effectively capture the complex context-aware interactive relations of arguments between the two passages. In this paper, we elicit relational semantic knowledge from large-scale pre-trained language models (PLMs) via a probing technique. The induced sentence-level relational probing graph can help capture rich explicit interactive relations between... | Yang Sun, Bin Liang, Jianzhu Bao, Yice Zhang, Geng Tu, Min Yang, Ruifeng Xu |  |
| 1170 |  |  [DiffuSum: Generation Enhanced Extractive Summarization with Diffusion](https://doi.org/10.18653/v1/2023.findings-acl.828) |  | 0 | Extractive summarization aims to form a summary by directly extracting sentences from the source document. Existing works mostly formulate it as a sequence labeling problem by making individual sentence label predictions. This paper proposes DiffuSum, a novel paradigm for extractive summarization, by directly generating the desired summary sentence representations with diffusion models and extracting sentences based on sentence representation matching. In addition, DiffuSum jointly optimizes a... | Haopeng Zhang, Xiao Liu, Jiawei Zhang |  |
| 1171 |  |  [Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding](https://doi.org/10.18653/v1/2023.findings-acl.829) |  | 0 | This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. Recent works tackled this task by improving query inputs with large pre-trained language models (PLM), at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual... | Erica Kido Shimomoto, Edison MarreseTaylor, Hiroya Takamura, Ichiro Kobayashi, Hideki Nakayama, Yusuke Miyao |  |
| 1172 |  |  [A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information](https://doi.org/10.18653/v1/2023.findings-acl.830) |  | 0 | Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms... | Vladimir Araujo, Alvaro Soto, MarieFrancine Moens |  |
| 1173 |  |  [Pay Attention to Implicit Attribute Values: A Multi-modal Generative Framework for AVE Task](https://doi.org/10.18653/v1/2023.findings-acl.831) |  | 0 | Attribute Value Extraction (AVE) boosts many e-commerce platform services such as targeted recommendation, product retrieval and question answering. Most previous studies adopt an extractive framework such as named entity recognition (NER) to capture subtokens in the product descriptions as the corresponding values of target attributes. However, in the real world scenario, there also exist implicit attribute values that are not mentioned explicitly but embedded in the image information and... | Yupeng Zhang, Shensi Wang, Peiguang Li, Guanting Dong, Sirui Wang, Yunsen Xian, Zhoujun Li, Hongzhi Zhang |  |
| 1174 |  |  [CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding](https://doi.org/10.18653/v1/2023.findings-acl.832) |  | 0 | Story generation and understanding—as with all NLG/NLU tasks—has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as... | Yijiang River Dong, Lara J. Martin, Chris CallisonBurch |  |
| 1175 |  |  [Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases](https://doi.org/10.18653/v1/2023.findings-acl.833) |  | 0 | NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle... | Yuval Reif, Roy Schwartz |  |
| 1176 |  |  [Context-Aware Document Simplification](https://doi.org/10.18653/v1/2023.findings-acl.834) |  | 0 | To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of... | Liam Cripwell, Joël Legrand, Claire Gardent |  |
| 1177 |  |  [Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.835) |  | 0 | Existing knowledge-enhanced methods have achieved remarkable results in certain Q&A tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into the... | Qianglong Chen, Guohai Xu, Ming Yan, Ji Zhang, Fei Huang, Luo Si, Yin Zhang |  |
| 1178 |  |  [Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games](https://doi.org/10.18653/v1/2023.findings-acl.836) |  | 0 | Text-based games present an exciting test-bed for reinforcement learning algorithms in the natural language environment. In these adventure games, an agent must learn to interact with the environment through text in order to accomplish tasks, facing large and combinational action space as well as partial observability issues. However, existing solutions fail to decompose the task and abstract the action autonomously, which either pre-specify the subtasks or pre-train on the human gameplay... | Anjie Zhu, PengFei Zhang, Yi Zhang, Zi Huang, Jie Shao |  |
| 1179 |  |  [SSP: Self-Supervised Post-training for Conversational Search](https://doi.org/10.18653/v1/2023.findings-acl.837) |  | 0 | Conversational search has been regarded as the next-generation search paradigm. Constrained by data scarcity, most existing methods distill the well-trained ad-hoc retriever to the conversational retriever. However, these methods, which usually initialize parameters by query reformulation to discover contextualized dependency, have trouble in understanding the dialogue structure information and struggle with contextual semantic vanishing. In this paper, we propose {pasted macro ‘FULLMODEL’}... | Quan Tu, Shen Gao, Xiaolong Wu, Zhao Cao, JiRong Wen, Rui Yan |  |
| 1180 |  |  [Towards Reference-free Text Simplification Evaluation with a BERT Siamese Network Architecture](https://doi.org/10.18653/v1/2023.findings-acl.838) |  | 0 | Text simplification (TS) aims to modify sentences to make their both content and structure easier to understand. Traditional n-gram matching-based TS evaluation metrics heavily rely on the exact token match and human-annotated simplified sentences. In this paper, we present a novel neural-network-based reference-free TS metric BETS that leverages pre-trained contextualized language representation models and large-scale paraphrasing datasets to evaluate simplicity and meaning preservation. We... | Xinran Zhao, Esin Durmus, DitYan Yeung |  |
| 1181 |  |  [Causal interventions expose implicit situation models for commonsense language understanding](https://doi.org/10.18653/v1/2023.findings-acl.839) |  | 0 | Accounts of human language processing have long appealed to implicit “situation models” that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word... | Takateru Yamakoshi, James L. McClelland, Adele Goldberg, Robert D. Hawkins |  |
| 1182 |  |  [Iterative Nearest Neighbour Machine Translation for Unsupervised Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.840) |  | 0 | Unsupervised domain adaptation of machine translation, which adapts a pre-trained translation model to a specific domain without in-domain parallel data, has drawn extensive attention in recent years. However, most existing methods focus on the fine-tuning based techniques, which is non-extensible. In this paper, we propose a new method to perform unsupervised domain adaptation in a non-parametric manner. Our method only resorts to in-domain monolingual data, and we jointly perform nearest... | Hui Huang, Shuangzhi Wu, Xinnian Liang, Zefan Zhou, Muyun Yang, Tiejun Zhao |  |
| 1183 |  |  [PruMUX: Augmenting Data Multiplexing with Model Compression](https://doi.org/10.18653/v1/2023.findings-acl.841) |  | 0 | As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods – structured pruning and data multiplexing – to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to... | Yushan Su, Vishvak Murahari, Karthik Narasimhan, Kai Li |  |
| 1184 |  |  [With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.842) |  | 0 | In this paper, we describe our work on social bias detection in a low-resource multilingual setting in which the languages are from two very divergent families- Indo-European (English, Hindi, and Italian) and Altaic (Korean). Currently, the majority of the social bias datasets available are in English and this inhibits progress on social bias detection in low-resource languages. To address this problem, we introduce a new dataset for social bias detection in Hindi and investigate multilingual... | Nihar Sahoo, Niteesh Mallela, Pushpak Bhattacharyya |  |
| 1185 |  |  [Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness](https://doi.org/10.18653/v1/2023.findings-acl.843) |  | 0 | As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings and thoughts of others via other-awareness. However, they ignore to include self-awareness to consider the own views of the self in their responses, which is a crucial process to achieve the empathy.... | Weixiang Zhao, Yanyan Zhao, Xin Lu, Bing Qin |  |
| 1186 |  |  [Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents](https://doi.org/10.18653/v1/2023.findings-acl.844) |  | 0 | Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether... | Catherine Chen, Zejiang Shen, Dan Klein, Gabriel Stanovsky, Doug Downey, Kyle Lo |  |
| 1187 |  |  [Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words](https://doi.org/10.18653/v1/2023.findings-acl.845) |  | 0 | Efforts have been made to apply topic seed words to improve the topic interpretability of topic models. However, due to the semantic diversity of natural language, supervisions from seed words could be ambiguous, making it hard to be incorporated into the current neural topic models. In this paper, we propose SeededNTM, a neural topic model enhanced with supervisions from seed words on both word and document levels. We introduce a context-dependency assumption to alleviate the ambiguities with... | Yang Lin, Xin Gao, Xu Chu, Yasha Wang, Junfeng Zhao, Chao Chen |  |
| 1188 |  |  [Learning from Children: Improving Image-Caption Pretraining via Curriculum](https://doi.org/10.18653/v1/2023.findings-acl.846) |  | 0 | Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem – it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots – the best learner, children. We take inspiration from cognitive science studies dealing with children’s language learning to propose a curriculum... | Hammad A. Ayyubi, Rahul Lokesh, Alireza Zareian, Bo Wu, ShihFu Chang |  |
| 1189 |  |  [Discovering Language Model Behaviors with Model-Written Evaluations](https://doi.org/10.18653/v1/2023.findings-acl.847) |  | 0 | As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of... | Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli TranJohnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy TelleenLawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac HatfieldDodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger B. Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, Jared Kaplan |  |
| 1190 |  |  [Cross-Domain Argument Quality Estimation](https://doi.org/10.18653/v1/2023.findings-acl.848) |  | 0 | Argumentation is one of society’s foundational pillars, and, sparked by advances in NLP, and the vast availability of text data, automated mining of arguments receives increasing attention. A decisive property of arguments is their strength or quality. While there are works on the automated estimation of argument strength, their scope is narrow:They focus on isolated datasets and neglect the interactions with related argument-mining tasks, such as argument identification and evidence detection.... | Michael Fromm, Max Berrendorf, Evgeniy Faerman, Thomas Seidl |  |
| 1191 |  |  [DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis](https://doi.org/10.18653/v1/2023.findings-acl.849) |  | 0 | The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinion mining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely DiaASQ, aiming to... | Bobo Li, Hao Fei, Fei Li, Yuhan Wu, Jinsong Zhang, Shengqiong Wu, Jingye Li, Yijiang Liu, Lizi Liao, TatSeng Chua, Donghong Ji |  |
| 1192 |  |  [GeoDRL: A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.850) |  | 0 | Ensuring both interpretability and correctness is a great challenge in automated geometry problem solving (GPS), and the scarcity of labeled data hinders learning mathematical reasoning from samples. Therefore, we present GeoDRL, a self-learning geometry problem solving framework that integrates logic graph deduction and Deep Reinforcement Learning (DRL) to optimize geometry reasoning as a Markov Decision Process. GeoDRL employs a Graph Neural Network on a Geometry Logic Graph, updating the... | Shuai Peng, Di Fu, Yijun Liang, Liangcai Gao, Zhi Tang |  |
| 1193 |  |  [Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction](https://doi.org/10.18653/v1/2023.findings-acl.851) |  | 0 | Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to... | Mengting Hu, Yinhao Bai, Yike Wu, Zhen Zhang, Liqi Zhang, Hang Gao, Shiwan Zhao, Minlie Huang |  |
| 1194 |  |  [Adversarial Knowledge Stimulated Contrastive Prompting for Few-shot Language Learners](https://doi.org/10.18653/v1/2023.findings-acl.852) |  | 0 | Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models(PLMs) on few-shot Natural Language Understanding (NLU) tasks by employing task-specific prompts. Yet, PLMsare unfamiliar with prompt-style expressionsduring pre-training, which limits the few-shotlearning performance on downstream tasks. It would be desirable if the models can stimulate prompting knowledge while adaptation to specific NLU tasks. We present the Adversarial Knowledge Stimulated Contrastive... | Kai Zheng, Qingfeng Sun, Yaming Yang, Tengchao Lv, Yeyong Pi, Changlin Zhao, Fei Xu, Qi Zhang |  |
| 1195 |  |  [Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios](https://doi.org/10.18653/v1/2023.findings-acl.853) |  | 0 | Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a... | Yufan Wang, Jie Mei, Bowei Zou, Rui Fan, Tingting He, Ai Ti Aw |  |
| 1196 |  |  [Typology Guided Multilingual Position Representations: Case on Dependency Parsing](https://doi.org/10.18653/v1/2023.findings-acl.854) |  | 0 | Recent multilingual models benefit from strong unified semantic representation models. However, due to conflict linguistic regularities, ignoring language-specific features during multilingual learning may suffer from negative transfer. In this work, we analyze the relationbetween a language’s position space and its typological characterization, and suggest deploying different position spaces for different languages. We develop a position generation network which combines prior knowledge from... | Tao Ji, Yuanbin Wu, Xiaoling Wang |  |
| 1197 |  |  [Learning Event-aware Measures for Event Coreference Resolution](https://doi.org/10.18653/v1/2023.findings-acl.855) |  | 0 | Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event but not entity as before, our model learns and integrates multiple representations from both event alone and event pair. For the former, we introduce multiple linguistics-motivated event alone... | Yao Yao, Zuchao Li, Hai Zhao |  |
| 1198 |  |  [Second Language Acquisition of Neural Language Models](https://doi.org/10.18653/v1/2023.findings-acl.856) |  | 0 | With the success of neural language models (LMs), their language acquisition has gained much attention. This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition. Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives. Our exploratory experiments demonstrated that the L1 pretraining accelerated their... | Miyu Oba, Tatsuki Kuribayashi, Hiroki Ouchi, Taro Watanabe |  |
| 1199 |  |  [On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection](https://doi.org/10.18653/v1/2023.findings-acl.857) |  | 0 | Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal... | Songyang Gao, Shihan Dou, Qi Zhang, Xuanjing Huang, Jin Ma, Ying Shan |  |
| 1200 |  |  [Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks](https://doi.org/10.18653/v1/2023.findings-acl.858) |  | 0 | The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering. Recent studies have demonstrated that Chain-of-Thought (CoT) prompts can lead to significant improvements in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar... | Fangyi Yu, Lee Quartey, Frank Schilder |  |
| 1201 |  |  [End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar](https://doi.org/10.18653/v1/2023.findings-acl.859) |  | 0 | End-to-end Aspect-based Sentiment Analysis (EASA) is a natural language processing (NLP) task that involves extracting aspect terms and identifying the sentiments for them, which provides a fine-grained level of text analysis and thus requires a deep understanding of the running text. Many previous studies leverage advanced text encoders to extract context information and use syntactic information, e.g., the dependency structure of the input sentence, to improve the model performance. However,... | Yuanhe Tian, Weidong Chen, Bo Hu, Yan Song, Fei Xia |  |
| 1202 |  |  [ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.860) |  | 0 | Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with... | Yakun Yu, Mingjun Zhao, Shiang Qi, Feiran Sun, Baoxun Wang, Weidong Guo, Xiaoli Wang, Lei Yang, Di Niu |  |
| 1203 |  |  [On Degrees of Freedom in Defining and Testing Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.861) |  | 0 | Natural language understanding (NLU) studies often exaggerate or underestimate the capabilities of systems, thereby limiting the reproducibility of their findings. These erroneous evaluations can be attributed to the difficulty of defining and testing NLU adequately. In this position paper, we reconsider this challenge by identifying two types of researcher degrees of freedom. We revisit Turing’s original interpretation of the Turing test and reveal that an effective test of NLU does not... | Saku Sugawara, Shun Tsugita |  |
| 1204 |  |  [AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking](https://doi.org/10.18653/v1/2023.findings-acl.862) |  | 0 | Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ULQA), aiming to generate high-quality long-document QA... | Yuxiang Nie, Heyan Huang, Wei Wei, XianLing Mao |  |
| 1205 |  |  [Adaptive Ordered Information Extraction with Deep Reinforcement Learning](https://doi.org/10.18653/v1/2023.findings-acl.863) |  | 0 | Information extraction (IE) has been studied extensively. The existing methods always follow a fixed extraction order for complex IE tasks with multiple elements to be extracted in one instance such as event extraction. However, we conduct experiments on several complex IE datasets and observe that different extraction orders can significantly affect the extraction results for a great portion of instances, and the ratio of sentences that are sensitive to extraction orders increases dramatically... | Wenhao Huang, Jiaqing Liang, Zhixu Li, Yanghua Xiao, Chuanjun Ji |  |
| 1206 |  |  [Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport](https://doi.org/10.18653/v1/2023.findings-acl.864) |  | 0 | Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learningbased models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scorning functions motivated by local comparison or global transport, this work investigates the local and global... | Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 1207 |  |  [RISE: Leveraging Retrieval Techniques for Summarization Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.865) |  | 0 | Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained as a retrieval task using a dual-encoder retrieval setup, and can then be subsequently utilized for evaluating a generated summary given an input document, without gold reference summaries. RISE is... | David C. Uthus, Jianmo Ni |  |
| 1208 |  |  [On the Difference of BERT-style and CLIP-style Text Encoders](https://doi.org/10.18653/v1/2023.findings-acl.866) |  | 0 | Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing, e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent performance on a broad range of vision tasks. However, few studies are dedicated to studying the text encoders learned by CLIP. In this paper, we analyze the difference between BERT-style and... | Zhihong Chen, Guiming Chen, Shizhe Diao, Xiang Wan, Benyou Wang |  |
| 1209 |  |  [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://doi.org/10.18653/v1/2023.findings-acl.867) |  | 0 | Concurrent with the rapid progress in neural network-based models in NLP, the need for creating explanations for the predictions of these black-box models has risen steadily. Yet, especially for complex inputs like texts or images, existing interpretability methods still struggle with deriving easily interpretable explanations that also accurately represent the basis for the model’s decision. To this end, we propose a new, model-agnostic method to generate extractive explanations for... | Marc Felix Brinner, Sina Zarrieß |  |
| 1210 |  |  [NusaCrowd: Open Source Initiative for Indonesian NLP Resources](https://doi.org/10.18653/v1/2023.findings-acl.868) |  | 0 | We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments.NusaCrowd’s data collection enables the creation of the first zero-shot benchmarks for... | Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Fajri Koto, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Muhammad Satrio Wicaksono, Ivan Halim Parmonangan, Ika Alfina, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Akbar Septiandri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Ryan Hadiwijaya, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Haryo Akbarianto Wibowo, Cuk Tho, Ichwanul Muslim Karo Karo, Tirana Fatyanosa, Ziwei Ji, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Pascale Fung, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti |  |
| 1211 |  |  [Transcribing Vocal Communications of Domestic Shiba lnu Dogs](https://doi.org/10.18653/v1/2023.findings-acl.869) |  | 0 | How animals communicate and whether they have languages is a persistent curiosity of human beings. However, the study of animal communications has been largely restricted to data from field recordings or in a controlled environment, which is expensive and limited in scale and variety. In this paper, we take domestic Shiba Inu dogs as an example, and extract their vocal communications from large amount of YouTube videos of Shiba Inu dogs. We classify these clips into different scenarios and... | Jieyi Huang, Chunhao Zhang, Mengyue Wu, Kenny Q. Zhu |  |
| 1212 |  |  [SkillQG: Learning to Generate Question for Reading Comprehension Assessment](https://doi.org/10.18653/v1/2023.findings-acl.870) |  | 0 | We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. Existing question generation systems widely differentiate questions by literal information such as question words and answer types to generate semantically relevant questions for a given context. However, they rarely consider the comprehension nature of questions, i.e., the different comprehension capabilities embodied by different... | Xiaoqiang Wang, Bang Liu, Siliang Tang, Lingfei Wu |  |
| 1213 |  |  [Improving Long Dialogue Summarization with Semantic Graph Representation](https://doi.org/10.18653/v1/2023.findings-acl.871) |  | 0 | Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into topic-segment-level Abstract Meaning Representation (AMR) graphs, which explicitly capture the dialogue structure, highlight salient semantics, and preserve high-level information. We also develop a new text-graph... | Bobby Yilun Hua, Zhaoyuan Deng, Kathleen R. McKeown |  |
| 1214 |  |  [Model Intrinsic Features of Fine-tuning based Text Summarization Models for Factual Consistency](https://doi.org/10.18653/v1/2023.findings-acl.872) |  | 0 | In this study, we analyze the model intrinsic features of a summarization model by varying the fine-tuning objectives and datasets. We fine-tune BART models combining three fine-tuning objectives (negative log-likelihood, unlikelihood, and contrastive loss) and two datasets (CNN/DailyMail and XSum) and provide shuffled or aligned documents to observe changes in the model predictions and intrinsic features. We find that (i) the inductive bias for factual consistency during the fine-tuning... | Jongyoon Song, Nohil Park, Bongkyu Hwang, Jaewoong Yun, Seongho Joe, Youngjune Gwon, Sungroh Yoon |  |
| 1215 |  |  [EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning](https://doi.org/10.18653/v1/2023.findings-acl.873) |  | 0 | Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size ofa... | Tiannan Wang, Wangchunshu Zhou, Yan Zeng, Xinsong Zhang |  |
| 1216 |  |  [DP-BART for Privatized Text Rewriting under Local Differential Privacy](https://doi.org/10.18653/v1/2023.findings-acl.874) |  | 0 | Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privacy guarantees, privatization of only individual words, as well as a lack of transparency and reproducibility. In this paper, we propose a new system ‘DP-BART’ that largely outperforms existing LDP... | Timour Igamberdiev, Ivan Habernal |  |
| 1217 |  |  [Robustness of Learning from Task Instructions](https://doi.org/10.18653/v1/2023.findings-acl.875) |  | 0 | Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output... | Jiasheng Gu, Hongyu Zhao, Hanzi Xu, Liangyu Nie, Hongyuan Mei, Wenpeng Yin |  |
| 1218 |  |  [Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.876) |  | 0 | In this paper, we propose an alternative to the classic masked language modeling (MLM) pre-training paradigm, where the objective is altered from the reconstruction of the exact identity of randomly selected masked subwords to the prediction of their latent semantic properties. We coin the proposed pre-training technique masked latent semantic modeling (MLSM for short). In order to make the contextualized determination of the latent semantic properties of the masked subwords possible, we rely... | Gábor Berend |  |
| 1219 |  |  [Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.877) |  | 0 | In text summarization, extractivity is defined as a measurement of the degree of overlap between a source document and its summary. Previous research has shown that the extractivity level of training data can influence both output extractivity and the amount of factual information (i.e. faithfulness) in outputs for abstractive summarization. However, it remains unclear if and how extractivity impacts the performance of abstractive models. In this work, we investigate the relationship between... | Yubin Ge, Sullam Jeoung, Ly Dinh, Jana Diesner |  |
| 1220 |  |  [Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering](https://doi.org/10.18653/v1/2023.findings-acl.878) |  | 0 | The nodes in the commonsense knowledge graph (CSKG) are normally represented by free-form short text (e.g., word or phrase). Different nodes may represent the same concept. This leads to the problems of edge sparsity and node redundancy, which challenges CSKG representation and completion. On the one hand, edge sparsity limits the performance of graph representation learning; On the other hand, node redundancy makes different nodes corresponding to the same concept have inconsistent relations... | Siwei Wu, Xiangqing Shen, Rui Xia |  |
| 1221 |  |  [Incorporating Factuality Inference to Identify Document-level Event Factuality](https://doi.org/10.18653/v1/2023.findings-acl.879) |  | 0 | Document-level Event Factuality Identification (DEFI) refers to identifying the degree of certainty that a specific event occurs in a document. Previous studies on DEFI failed to link the document-level event factuality with various sentence-level factuality values in the same document. In this paper, we innovatively propose an event factuality inference task to bridge the sentence-level and the document-level event factuality semantically. Specifically, we present a Sentence-to-Document... | Heng Zhang, Peifeng Li, Zhong Qian, Xiaoxu Zhu |  |
| 1222 |  |  [Hybrid and Collaborative Passage Reranking](https://doi.org/10.18653/v1/2023.findings-acl.880) |  | 0 | In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of... | Zongmeng Zhang, Wengang Zhou, Jiaxin Shi, Houqiang Li |  |
| 1223 |  |  [Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](https://doi.org/10.18653/v1/2023.findings-acl.881) |  | 0 | Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion... | Haoran Li, Mingshi Xu, Yangqiu Song |  |
| 1224 |  |  [Learning Query Adaptive Anchor Representation for Inductive Relation Prediction](https://doi.org/10.18653/v1/2023.findings-acl.882) |  | 0 | Relation prediction on knowledge graphs (KGs) attempts to infer the missing links between entities. Most previous studies are limited to the transductive setting where all entities must be seen during the training, making them unable to perform reasoning on emerging entities. Recently, the inductive setting is proposed to handle the entities in the test phase to be unseen during training, However, it suffers from the inefficient reasoning under the enclosing subgraph extraction issue and the... | Zhiwen Xie, Yi Zhang, Jin Liu, Guangyou Zhou, Jimmy Xiangji Huang |  |
| 1225 |  |  [Context or Knowledge is Not Always Necessary: A Contrastive Learning Framework for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.883) |  | 0 | Emotion recognition in conversations (ERC) aims to detect the emotion of utterances in conversations. Existing efforts generally focus on modeling context- and knowledge-sensitive dependencies. However, it is observed that the emotions of many utterances can be correctly detected without context or external knowledge. In such cases, blindly leveraging the context and external knowledge may impede model training. Based on this, we propose a novel framework based on contrastive learning (CL),... | Geng Tu, Bin Liang, Ruibin Mao, Min Yang, Ruifeng Xu |  |
| 1226 |  |  [Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization](https://doi.org/10.18653/v1/2023.findings-acl.884) |  | 0 | Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic environment. In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization. We... | Luyao Cheng, Siqi Zheng, Qinglin Zhang, Hui Wang, Yafeng Chen, Qian Chen |  |
| 1227 |  |  [Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.885) |  | 0 | While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to train AS2 models for low-resource languages in the tasks without the need of labeled data for the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a translation-based WikiQA dataset... | Shivanshu Gupta, Yoshitomo Matsubara, Ankit Chadha, Alessandro Moschitti |  |
| 1228 |  |  [Run Like a Girl! Sport-Related Gender Bias in Language and Vision](https://doi.org/10.18653/v1/2023.findings-acl.886) |  | 0 | Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. We analyze gender bias in two Language and Vision datasets. Consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. Moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. “tennis player” or “surfer”) more often when... | Sophia Harrison, Eleonora Gualdoni, Gemma Boleda |  |
| 1229 |  |  [People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts](https://doi.org/10.18653/v1/2023.findings-acl.887) |  | 0 | Although pre-trained named entity recognition (NER) models are highly accurate on modern corpora, they underperform on historical texts due to differences in language OCR errors. In this work, we develop a new NER corpus of 3.6M sentences from late medieval charters written mainly in Czech, Latin, and German.We show that we can start with a list of known historical figures and locations and an unannotated corpus of historical texts, and use information retrieval techniques to automatically... | Vit Novotny, Kristina Luger, Michal Stefánik, Tereza Vrabcová, Ales Horák |  |
| 1230 |  |  [Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence](https://doi.org/10.18653/v1/2023.findings-acl.888) |  | 0 | We present a new fact-checking benchmark, Check-COVID, that requires systems to verify claims about COVID-19 from news using evidence from scientific articles. This approach to fact-checking is particularly challenging as it requires checking internet text written in everyday language against evidence from journal articles written in formal academic language. Check-COVID contains 1, 504 expert-annotated news claims about the coronavirus paired with sentence-level evidence from scientific... | Gengyu Wang, Kate Harwood, Lawrence Chillrud, Amith Ananthram, Melanie Subbiah, Kathleen R. McKeown |  |
| 1231 |  |  [Early Exit with Disentangled Representation and Equiangular Tight Frame](https://doi.org/10.18653/v1/2023.findings-acl.889) |  | 0 | Dynamic early exit has demonstrated great potential in coping with the sharply increasing number of pre-trained language model parameters, which can achieve a good trade-off between performance and efficiency. The existing early exit paradigm relies on training parametrical internal classifiers at each intermediate layer to complete specific tasks. Based on the predictions of these internal classifiers, different methods are designed to decide when to exit. Under this circumstance, each... | Yixin Ji, Jikai Wang, Juntao Li, Qiang Chen, Wenliang Chen, Min Zhang |  |
| 1232 |  |  [Tokenization with Factorized Subword Encoding](https://doi.org/10.18653/v1/2023.findings-acl.890) |  | 0 | In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages.... | David Samuel, Lilja Øvrelid |  |
| 1233 |  |  [Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers](https://doi.org/10.18653/v1/2023.findings-acl.891) |  | 0 | How well do language models deal with quantification? In this study, we focus on ‘few’-type quantifiers, as in ‘few children like toys’, which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and ‘few’-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly... | James A. Michaelov, Benjamin K. Bergen |  |
| 1234 |  |  ["A Little is Enough": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.892) |  | 0 | Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of QE framework to extracting quality... | Akshay Batheja, Pushpak Bhattacharyya |  |
| 1235 |  |  [How effective is machine translation on low-resource code-switching? A case study comparing human and automatic metrics](https://doi.org/10.18653/v1/2023.findings-acl.893) |  | 0 | This paper presents an investigation into the differences between processing monolingual input and code-switching (CSW) input in the context of machine translation (MT). Specifically, we compare the performance of three MT systems (Google, mBART-50 and M2M-100-big) in terms of their ability to translate monolingual Vietnamese, a low-resource language, and Vietnamese-English CSW respectively. To our knowledge, this is the first study to systematically analyse what might happen when multilingual... | Li Nguyen, Christopher Bryant, Oliver Mayeux, Zheng Yuan |  |
| 1236 |  |  [Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.894) |  | 0 | Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input – but also, as we argue, often require a strong reasoning component. Similar to some recent... | Sherzod Hakimov, David Schlangen |  |
| 1237 |  |  [On the Expressivity Role of LayerNorm in Transformers' Attention](https://doi.org/10.18653/v1/2023.findings-acl.895) |  | 0 | Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm’s only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the... | Shaked Brody, Uri Alon, Eran Yahav |  |
| 1238 |  |  [DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation](https://doi.org/10.18653/v1/2023.findings-acl.896) |  | 0 | Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DENSITY, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier.... | ChaeHun Park, Seungil Chad Lee, Daniel Rim, Jaegul Choo |  |
| 1239 |  |  [Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.897) |  | 0 | Sparsely gated Mixture of Experts (MoE) models have been shown to be a compute-efficient method to scale model capacity for multilingual machine translation. However, for low-resource tasks, MoE models severely over-fit. We show effective regularization strategies, namely dropout techniques for MoE layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods that prevent over-fitting and improve the performance of MoE models on low-resource tasks without adversely affecting... | Maha Elbayad, Anna Y. Sun, Shruti Bhosale |  |
| 1240 |  |  [Intent Discovery with Frame-guided Semantic Regularization and Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.898) |  | 0 | Most existing intent discovery methods leverage representation learning and clustering to transfer the prior knowledge of known intents to unknown ones. The learned representations are limited to the syntactic forms of sentences, therefore, fall short of recognizing adequate variations under the same meaning of unknown intents. This paper proposes an approach utilizing frame knowledge as conceptual semantic guidance to bridge the gap between known intents representation learning and unknown... | Yajing Sun, Rui Zhang, Jingyuan Yang, Wei Peng |  |
| 1241 |  |  [An Empirical Comparison of LM-based Question and Answer Generation Methods](https://doi.org/10.18653/v1/2023.findings-acl.899) |  | 0 | Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g. a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish baselines with three different QAG methodologies that leverage sequence-to-sequence language model (LM) fine-tuning. Experiments show that an end-to-end QAG model, which is computationally light at both... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 1242 |  |  [Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding](https://doi.org/10.18653/v1/2023.findings-acl.900) |  | 0 | With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in inductive KGs are limited by sparsity and implicit transfer. In this paper, we propose VMCL, a Contrastive Learning (CL) framework with graph guided Variational autoencoder on Meta-KGs in the inductive... | Qian Li, Shafiq Joty, Daling Wang, Shi Feng, Yifei Zhang, Chengwei Qin |  |
| 1243 |  |  [Decouple knowledge from paramters for plug-and-play language modeling](https://doi.org/10.18653/v1/2023.findings-acl.901) |  | 0 | Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is... | Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, Rui Yan |  |
| 1244 |  |  [One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.1) |  | 0 | User simulators are agents designed to imitate human users; recent advances have found that Task-oriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a sub-optimal ToD system if it is tailored to only one ad hoc user simulator, since human users can behave differently. In this paper, we propose a framework called MUST to optimize ToD systems via leveraging Multiple User SimulaTors. The main challenges of... | Yajiao Liu, Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, Benyou Wang |  |
| 1245 |  |  [SafeConv: Explaining and Correcting Conversational Unsafe Behavior](https://doi.org/10.18653/v1/2023.acl-long.2) |  | 0 | One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information... | Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi, Wenliang Chen, Dong Yu |  |
| 1246 |  |  [Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better](https://doi.org/10.18653/v1/2023.acl-long.3) |  | 0 | While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we... | David Dale, Elena Voita, Loïc Barrault, Marta R. Costajussà |  |
| 1247 |  |  [Explainable Recommendation with Personalized Review Retrieval and Aspect Learning](https://doi.org/10.18653/v1/2023.acl-long.4) |  | 0 | Explainable recommendation is a technique that combines prediction and generation tasks to produce more persuasive results. Among these tasks, textual generation demands large amounts of data to achieve satisfactory accuracy. However, historical user reviews of items are often insufficient, making it challenging to ensure the precision of generated explanation text. To address this issue, we propose a novel model, ERRA (Explainable Recommendation by personalized Review retrieval and Aspect... | Hao Cheng, Shuo Wang, Wensheng Lu, Wei Zhang, Mingyang Zhou, Kezhong Lu, Hao Liao |  |
| 1248 |  |  [Binary and Ternary Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.5) |  | 0 | Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the... | Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, Raghuraman Krishnamoorthi |  |
| 1249 |  |  [Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking](https://doi.org/10.18653/v1/2023.acl-long.6) |  | 0 | In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves... | Björn Bebensee, Haejun Lee |  |
| 1250 |  |  [EM Pre-training for Multi-party Dialogue Response Generation](https://doi.org/10.18653/v1/2023.acl-long.7) |  | 0 | Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party... | Yiyang Li, Hai Zhao |  |
| 1251 |  |  [ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER](https://doi.org/10.18653/v1/2023.acl-long.8) |  | 0 | Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often... | Sreyan Ghosh, Utkarsh Tyagi, Manan Suri, Sonal Kumar, Ramaneswaran S., Dinesh Manocha |  |
| 1252 |  |  [Natural Language to Code Generation in Interactive Data Science Notebooks](https://doi.org/10.18653/v1/2023.acl-long.9) |  | 0 | Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of... | Pengcheng Yin, WenDing Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Oleksandr Polozov, Charles Sutton |  |
| 1253 |  |  [Subset Retrieval Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.10) |  | 0 | k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose “Subset kNN-MT”, which improves the decoding speed... | Hiroyuki Deguchi, Taro Watanabe, Yusuke Matsui, Masao Utiyama, Hideki Tanaka, Eiichiro Sumita |  |
| 1254 |  |  [MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning](https://doi.org/10.18653/v1/2023.acl-long.11) |  | 0 | Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a... | Xu Zhang, Xiaojun Wan |  |
| 1255 |  |  [Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies](https://doi.org/10.18653/v1/2023.acl-long.12) |  | 0 | Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show... | Iria deDiosFlores, Juan Garcia Amboage, Marcos García |  |
| 1256 |  |  [Open-ended Long Text Generation via Masked Language Modeling](https://doi.org/10.18653/v1/2023.acl-long.13) |  | 0 | Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our... | Xiaobo Liang, Zecheng Tang, Juntao Li, Min Zhang |  |
| 1257 |  |  [A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces](https://doi.org/10.18653/v1/2023.acl-long.14) |  | 0 | We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., ‘a... | Gabriella Chronis, Kyle Mahowald, Katrin Erk |  |
| 1258 |  |  [Holographic CCG Parsing](https://doi.org/10.18653/v1/2023.acl-long.15) |  | 0 | We propose a method for formulating CCG as a recursive composition in a continuous vector space. Recent CCG supertagging and parsing models generally demonstrate high performance, yet rely on black-box neural architectures to implicitly model phrase structure dependencies. Instead, we leverage the method of holographic embeddings as a compositional operator to explicitly model the dependencies between words and phrase structures in the embedding space. Experimental results revealed that... | Ryosuke Yamaki, Tadahiro Taniguchi, Daichi Mochihashi |  |
| 1259 |  |  [Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning](https://doi.org/10.18653/v1/2023.acl-long.16) |  | 0 | Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. Nonetheless, in real-world scenarios of UIE applications, new data of different IE tasks and domains usually come in a stream over time. A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing... | Zujie Liang, Feng Wei, Yin Jie, Yuxi Qian, Zhenghong Hao, Bing Han |  |
| 1260 |  |  [Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation](https://doi.org/10.18653/v1/2023.acl-long.17) |  | 0 | Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label... | Yubing Ren, Yanan Cao, Ping Guo, Fang Fang, Wei Ma, Zheng Lin |  |
| 1261 |  |  [WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.18) |  | 0 | A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs. Due to lack of annotated data, existing factual consistency metrics usually train evaluation models on synthetic texts or directly transfer from other related tasks, such as question answering (QA) and natural language inference (NLI).Bias in synthetic text or upstream tasks makes them perform poorly on text actually generated by language models,... | Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lyu |  |
| 1262 |  |  [AMR-based Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.19) |  | 0 | Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure... | Fukun Ma, Xuming Hu, Aiwei Liu, Yawen Yang, Shuang Li, Philip S. Yu, Lijie Wen |  |
| 1263 |  |  [Text Adversarial Purification as Defense against Adversarial Attacks](https://doi.org/10.18653/v1/2023.acl-long.20) |  | 0 | Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models,using purification as a... | Linyang Li, Demin Song, Xipeng Qiu |  |
| 1264 |  |  [SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres](https://doi.org/10.18653/v1/2023.acl-long.21) |  | 0 | Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but... | Shumin Deng, Shengyu Mao, Ningyu Zhang, Bryan Hooi |  |
| 1265 |  |  [Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection](https://doi.org/10.18653/v1/2023.acl-long.22) |  | 0 | Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved... | Christopher Clarke, Matthew Hall, Gaurav Mittal, Ye Yu, Sandra Sajeev, Jason Mars, Mei Chen |  |
| 1266 |  |  [What about "em"? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns](https://doi.org/10.18653/v1/2023.acl-long.23) |  | 0 | As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns, we need more research on identity-inclusive NLP. Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021). In this “reality check”, we study how three commercial MT systems translate 3rd-person pronouns. Concretely, we compare the translations of... | Anne Lauscher, Debora Nozza, Ehm Miltersen, Archie Crowley, Dirk Hovy |  |
| 1267 |  |  [What Is Overlap Knowledge in Event Argument Extraction? APE: A Cross-datasets Transfer Learning Model for EAE](https://doi.org/10.18653/v1/2023.acl-long.24) |  | 0 | The EAE task extracts a structured event record from an event text. Most existing approaches train the EAE model on each dataset independently and ignore the overlap knowledge across datasets. However, insufficient event records in a single dataset often prevent the existing model from achieving better performance. In this paper, we clearly define the overlap knowledge across datasets and split the knowledge of the EAE task into overlap knowledge across datasets and specific knowledge of the... | Kaihang Zhang, Kai Shuang, Xinyue Yang, Xuyang Yao, Jinyu Guo |  |
| 1268 |  |  [Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation](https://doi.org/10.18653/v1/2023.acl-long.25) |  | 0 | Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address these concerns, we explore attribute-based CTG in a parameter-efficient manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector i.e., single-attribute prompt),... | Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Mingfeng Xue, Boxing Chen, Jun Xie |  |
| 1269 |  |  [Knowledge of cultural moral norms in large language models](https://doi.org/10.18653/v1/2023.acl-long.26) |  | 0 | Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics... | Aida Ramezani, Yang Xu |  |
| 1270 |  |  [Songs Across Borders: Singable and Controllable Neural Lyric Translation](https://doi.org/10.18653/v1/2023.acl-long.27) |  | 0 | The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to prompt-driven NMT approaches, exploring... | Longshen Ou, Xichu Ma, MinYen Kan, Ye Wang |  |
| 1271 |  |  [Fantastic Expressions and Where to Find Them: Chinese Simile Generation with Multiple Constraints](https://doi.org/10.18653/v1/2023.acl-long.28) |  | 0 | Similes occur in the creative context of describing a concept (i.e., tenor) by making a literally false yet figuratively meaningful comparison to another (i.e., vehicle). Previous efforts form simile generation as a context-free generation task, focusing on simile-style transfer or writing a simile from a given prefix. However, generated texts under such settings might be undesirable, such as hardly meeting the simile definition (e.g., missing vehicle) or difficult to address certain... | Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Xiangpeng Wei, Zhengyuan Liu, Jun Xie |  |
| 1272 |  |  [Revealing Single Frame Bias for Video-and-Language Learning](https://doi.org/10.18653/v1/2023.acl-long.29) |  | 0 | Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video... | Jie Lei, Tamara L. Berg, Mohit Bansal |  |
| 1273 |  |  [Learning with Partial Annotations for Event Detection](https://doi.org/10.18653/v1/2023.acl-long.30) |  | 0 | Event detection (ED) seeks to discover and classify event instances in plain texts. Previous methods for ED typically adopt supervised learning, requiring fully labeled and high-quality training data. However, in a real-world application, we may not obtain clean training data but only partially labeled one, which could substantially impede the learning process. In this work, we conduct a seminal study for learning with partial annotations for ED.We propose a new trigger localization formulation... | Jian Liu, Dianbo Sui, Kang Liu, Haoyan Liu, Zhe Zhao |  |
| 1274 |  |  [World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-long.31) |  | 0 | The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine... | Ziqiao Ma, Jiayi Pan, Joyce Chai |  |
| 1275 |  |  [A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models](https://doi.org/10.18653/v1/2023.acl-long.32) |  | 0 | We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the... | Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, Mrinmaya Sachan |  |
| 1276 |  |  [Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information](https://doi.org/10.18653/v1/2023.acl-long.33) |  | 0 | The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based automatic evaluation metric (CMN), which can robustly evaluate open-domain dialogues by augmenting Conditional Variational Autoencoders (CVAEs) with a Next Sentence Prediction (NSP) objective and... | Kun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio, Xiaohui Cui |  |
| 1277 |  |  [Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions](https://doi.org/10.18653/v1/2023.acl-long.34) |  | 0 | Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature... | John Joon Young Chung, Ece Kamar, Saleema Amershi |  |
| 1278 |  |  [Pruning Pre-trained Language Models Without Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-long.35) |  | 0 | To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order... | Ting Jiang, Deqing Wang, Fuzhen Zhuang, Ruobing Xie, Feng Xia |  |
| 1279 |  |  [When Does Translation Require Context? A Data-driven, Multilingual Exploration](https://doi.org/10.18653/v1/2023.acl-long.36) |  | 0 | Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse... | Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, Graham Neubig |  |
| 1280 |  |  [Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection](https://doi.org/10.18653/v1/2023.acl-long.37) |  | 0 | Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only... | Ziwei Chen, Linmei Hu, Weixin Li, Yingxia Shao, Liqiang Nie |  |
| 1281 |  |  [LexSym: Compositionality as Lexical Symmetry](https://doi.org/10.18653/v1/2023.acl-long.38) |  | 0 | In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever... | Ekin Akyürek, Jacob Andreas |  |
| 1282 |  |  [Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition](https://doi.org/10.18653/v1/2023.acl-long.39) |  | 0 | Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new... | Jun Sun, Shoukang Han, YuPing Ruan, Xiaoning Zhang, ShuKai Zheng, Yulong Liu, Yuxin Huang, Taihao Li |  |
| 1283 |  |  [CASN: Class-Aware Score Network for Textual Adversarial Detection](https://doi.org/10.18653/v1/2023.acl-long.40) |  | 0 | Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data and identifying adversarial ones as outliers. However, these methods suffer from significant performance degradation when the adversarial samples lie close to the non-adversarial data manifold. To... | Rong Bao, Rui Zheng, Liang Ding, Qi Zhang, Dacheng Tao |  |
| 1284 |  |  [Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest](https://doi.org/10.18653/v1/2023.acl-long.41) |  | 0 | Large neural networks can now generate jokes, but do they really “understand” humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of “understanding” a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of... | Jack Hessel, Ana Marasovic, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi |  |
| 1285 |  |  [Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation](https://doi.org/10.18653/v1/2023.acl-long.42) |  | 0 | The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse... | Martijn Bartelds, Nay San, Bradley McDonnell, Dan Jurafsky, Martijn Wieling |  |
| 1286 |  |  [CLCL: Non-compositional Expression Detection with Contrastive Learning and Curriculum Learning](https://doi.org/10.18653/v1/2023.acl-long.43) |  | 0 | Non-compositional expressions present a substantial challenge for natural language processing (NLP) systems, necessitating more intricate processing compared to general language tasks, even with large pre-trained language models. Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations. This paper addresses both of these challenges. By leveraging contrastive learning techniques to build improved... | Jianing Zhou, Ziheng Zeng, Suma Bhat |  |
| 1287 |  |  [Multi-VALUE: A Framework for Cross-Dialectal English NLP](https://doi.org/10.18653/v1/2023.acl-long.44) |  | 0 | Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving... | Caleb Ziems, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, Diyi Yang |  |
| 1288 |  |  [Self-Edit: Fault-Aware Code Editor for Code Generation](https://doi.org/10.18653/v1/2023.acl-long.45) |  | 0 | Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the... | Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin |  |
| 1289 |  |  [ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning](https://doi.org/10.18653/v1/2023.acl-long.46) |  | 0 | Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but... | Shachar DonYehiya, Elad Venezian, Colin Raffel, Noam Slonim, Leshem Choshen |  |
| 1290 |  |  [Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization](https://doi.org/10.18653/v1/2023.acl-long.47) |  | 0 | The neural metrics recently received considerable attention from the research community in the automatic evaluation of machine translation. Unlike text-based metrics that have interpretable and consistent evaluation mechanisms for various data sources, the reliability of neural metrics in assessing out-of-distribution data remains a concern due to the disparity between training data and real-world data. This paper aims to address the inference bias of neural metrics through uncertainty... | Runzhe Zhan, Xuebo Liu, Derek F. Wong, Cuilian Zhang, Lidia S. Chao, Min Zhang |  |
| 1291 |  |  [Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling](https://doi.org/10.18653/v1/2023.acl-long.48) |  | 0 | Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need... | HawShiuan Chang, RueiYao Sun, Kathryn Ricci, Andrew McCallum |  |
| 1292 |  |  [On-the-fly Cross-lingual Masking for Multilingual Pre-training](https://doi.org/10.18653/v1/2023.acl-long.49) |  | 0 | In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token [𝒞]x to replace a random token x in the... | Xi Ai, Bin Fang |  |
| 1293 |  |  [How About Kind of Generating Hedges using End-to-End Neural Models?](https://doi.org/10.18653/v1/2023.acl-long.50) |  | 0 | Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, “face threat”) to one’s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best... | Alafate Abulimiti, Chloé Clavel, Justine Cassell |  |
| 1294 |  |  [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models](https://doi.org/10.18653/v1/2023.acl-long.51) |  | 0 | With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable... | Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau |  |
| 1295 |  |  [From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization](https://doi.org/10.18653/v1/2023.acl-long.52) |  | 0 | Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments. KPA extracts the main points in the data as a list of concise sentences or phrases, termed Key Points, and quantifies their prevalence. While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging. To address this... | Arie Cattan, Lilach Eden, Yoav Kantor, Roy BarHaim |  |
| 1296 |  |  [When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications](https://doi.org/10.18653/v1/2023.acl-long.53) |  | 0 | Open Information Extraction (OpenIE) has been used in the pipelines of various NLP tasks. Unfortunately, there is no clear consensus on which models to use in which tasks. Muddying things further is the lack of comparisons that take differing training sets into account. In this paper, we present an application-focused empirical survey of neural OpenIE models, training sets, and benchmarks in an effort to help users choose the most suitable OpenIE systems for their applications. We find that the... | Kevin Pei, Ishan Jindal, Kevin ChenChuan Chang, ChengXiang Zhai, Yunyao Li |  |
| 1297 |  |  [Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning](https://doi.org/10.18653/v1/2023.acl-long.54) |  | 0 | Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree. Particularly when minority views are disregarded,... | Tharindu Cyril Weerasooriya, Sarah Luger, Saloni Poddar, Ashiqur R. KhudaBukhsh, Christopher Homan |  |
| 1298 |  |  [Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA](https://doi.org/10.18653/v1/2023.acl-long.55) |  | 0 | Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. ‘Selective prediction’ partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question ‘what to do... | Neeraj Varshney, Chitta Baral |  |
| 1299 |  |  [UniLG: A Unified Structure-aware Framework for Lyrics Generation](https://doi.org/10.18653/v1/2023.acl-long.56) |  | 0 | As a special task of natural language generation, conditional lyrics generation needs to consider the structure of generated lyrics and the relationship between lyrics and music. Due to various forms of conditions, a lyrics generation system is expected to generate lyrics conditioned on different signals, such as music scores, music audio, or partially-finished lyrics, etc. However, most of the previous works have ignored the musical attributes hidden behind the lyrics and the structure of the... | Tao Qian, Fan Lou, Jiatong Shi, Yuning Wu, Shuai Guo, Xiang Yin, Qin Jin |  |
| 1300 |  |  [FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.57) |  | 0 | The generalization problem on KBQA has drawn considerable attention. Existing research suffers from the generalization issue brought by the entanglement in the coarse-grained modeling of the logical expression, or inexecutability issues due to the fine-grained modeling of disconnected classes and relations in real KBs. We propose a Fine-to-Coarse Composition framework for KBQA (FC-KBQA) to both ensure the generalization ability and executability of the logical expression. The main idea of... | Lingxi Zhang, Jing Zhang, Yanling Wang, Shulin Cao, Xinmei Huang, Cuiping Li, Hong Chen, Juanzi Li |  |
| 1301 |  |  [Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models](https://doi.org/10.18653/v1/2023.acl-long.58) |  | 0 | Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor’s... | Lennart Wachowiak, Dagmar Gromann |  |
| 1302 |  |  [Being Right for Whose Right Reasons?](https://doi.org/10.18653/v1/2023.acl-long.59) |  | 0 | Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are ‘right for the right reasons’. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and... | Terne Sasha Thorn Jakobsen, Laura Cabello, Anders Søgaard |  |
| 1303 |  |  [ALERT: Adapt Language Models to Reasoning Tasks](https://doi.org/10.18653/v1/2023.acl-long.60) |  | 0 | Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses... | Ping Yu, Tianlu Wang, Olga Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona T. Diab, Asli Celikyilmaz |  |
| 1304 |  |  [Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages](https://doi.org/10.18653/v1/2023.acl-long.61) |  | 0 | The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages.... | Ayyoob Imani, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André F. T. Martins, François Yvon, Hinrich Schütze |  |
| 1305 |  |  [Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2023.acl-long.62) |  | 0 | Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using... | Huawen Feng, Junlong Liu, Junhao Zheng, Haibin Chen, Xichen Shang, Qianli Ma |  |
| 1306 |  |  [Pretrained Bidirectional Distillation for Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.63) |  | 0 | Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained... | Yimeng Zhuang, Mei Tu |  |
| 1307 |  |  [Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.64) |  | 0 | Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender... | Kyuyong Shin, Hanock Kwak, Wonjae Kim, Jisu Jeong, Seungjae Jung, Kyungmin Kim, JungWoo Ha, SangWoo Lee |  |
| 1308 |  |  [Improving Continual Relation Extraction by Distinguishing Analogous Semantics](https://doi.org/10.18653/v1/2023.acl-long.65) |  | 0 | Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual... | Wenzheng Zhao, Yuanning Cui, Wei Hu |  |
| 1309 |  |  [Improving Pretraining Techniques for Code-Switched NLP](https://doi.org/10.18653/v1/2023.acl-long.66) |  | 0 | Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world’s languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages interspersed within a sentence) is much more scarce. Given this resource constraint, the question of how pretraining using limited amounts of code-switched text could be altered to improve performance... | Richeek Das, Sahasra Ranjan, Shreya Pathak, Preethi Jyothi |  |
| 1310 |  |  [A Theory of Unsupervised Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.67) |  | 0 | Unsupervised speech recognition ({pasted macro ‘ASRU’}/) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and address such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of {pasted macro ‘ASRU’}/ systems based... | Liming Wang, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 1311 |  |  [ThinkSum: Probabilistic reasoning over sets using large language models](https://doi.org/10.18653/v1/2023.acl-long.68) |  | 0 | Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which... | Batu Ozturkler, Nikolay Malkin, Zhen Wang, Nebojsa Jojic |  |
| 1312 |  |  [NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist](https://doi.org/10.18653/v1/2023.acl-long.69) |  | 0 | In this study, we analyze automatic evaluation metrics for Natural Language Generation (NLG), specifically task-agnostic metrics and human-aligned metrics. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at... | Iftitahu Ni'mah, Meng Fang, Vlado Menkovski, Mykola Pechenizkiy |  |
| 1313 |  |  [DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations](https://doi.org/10.18653/v1/2023.acl-long.70) |  | 0 | In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to... | Ang Lv, Jinpeng Li, Yuhan Chen, Gao Xing, Ji Zhang, Rui Yan |  |
| 1314 |  |  [TECHS: Temporal Logical Graph Networks for Explainable Extrapolation Reasoning](https://doi.org/10.18653/v1/2023.acl-long.71) |  | 0 | Extrapolation reasoning on temporal knowledge graphs (TKGs) aims to forecast future facts based on past counterparts. There are two main challenges: (1) incorporating the complex information, including structural dependencies, temporal dynamics, and hidden logical rules; (2) implementing differentiable logical rule learning and reasoning for explainability. To this end, we propose an explainable extrapolation reasoning framework TEemporal logiCal grapH networkS (TECHS), which mainly contains a... | Qika Lin, Jun Liu, Rui Mao, Fangzhi Xu, Erik Cambria |  |
| 1315 |  |  [Consistency Regularization Training for Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.72) |  | 0 | Existing neural models have difficulty generalizing to unseen combinations of seen components. To achieve compositional generalization, models are required to consistently interpret (sub)expressions across contexts. Without modifying model architectures, we improve the capability of Transformer on compositional generalization through consistency regularization training, which promotes representation consistency across samples and prediction consistency for a single sample. Experimental results... | Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Jie Zhou, Yue Zhang |  |
| 1316 |  |  [NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](https://doi.org/10.18653/v1/2023.acl-long.73) |  | 0 | In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a “coarse-to-fine” process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to... | Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Ming Gong, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan |  |
| 1317 |  |  [Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe](https://doi.org/10.18653/v1/2023.acl-long.74) |  | 0 | Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe... | Xiang Yue, Huseyin A. Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, Robert Sim |  |
| 1318 |  |  [A Close Look into the Calibration of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.75) |  | 0 | Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs’ calibration performance in training. We consider six factors as control variables, including dataset... | Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, Heng Ji |  |
| 1319 |  |  [DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.76) |  | 0 | Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing... | Yu Li, Baolin Peng, Pengcheng He, Michel Galley, Zhou Yu, Jianfeng Gao |  |
| 1320 |  |  [MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction](https://doi.org/10.18653/v1/2023.acl-long.77) |  | 0 | Given a text query, the task of Natural Language Video Localization (NLVL) is to localize a temporal moment in an untrimmed video that semantically matches the query. In this paper, we adopt a proposal-based solution that generates proposals (i.e. candidate moments) and then select the best matching proposal. On top of modeling the cross-modal interaction between candidate moments and the query, our proposed Moment Sampling DETR (MS-DETR) enables efficient moment-moment relation modeling. The... | Jing Wang, Aixin Sun, Hao Zhang, Xiaoli Li |  |
| 1321 |  |  [Diverse Demonstrations Improve In-context Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.78) |  | 0 | In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the... | Itay Levy, Ben Bogin, Jonathan Berant |  |
| 1322 |  |  [Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering](https://doi.org/10.18653/v1/2023.acl-long.79) |  | 0 | Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we... | Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong |  |
| 1323 |  |  [On the Efficacy of Sampling Adapters](https://doi.org/10.18653/v1/2023.acl-long.80) |  | 0 | Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model’s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling... | Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan Wilcox, Ryan Cotterell |  |
| 1324 |  |  [Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.81) |  | 0 | Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving... | Jianfei Yu, Qiankun Zhao, Rui Xia |  |
| 1325 |  |  [Compositional Data Augmentation for Abstractive Conversation Summarization](https://doi.org/10.18653/v1/2023.acl-long.82) |  | 0 | Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and... | Siru Ouyang, Jiaao Chen, Jiawei Han, Diyi Yang |  |
| 1326 |  |  [PMAES: Prompt-mapping Contrastive Learning for Cross-prompt Automated Essay Scoring](https://doi.org/10.18653/v1/2023.acl-long.83) |  | 0 | Current cross-prompt automated essay scoring (AES) is a challenging task due to the large discrepancies between different prompts, such as different genres and expressions. The main goal of current cross-prompt AES systems is to learn enough shared features between the source and target prompts to grade well on the target prompt. However, because the features are captured based on the original prompt representation, they may be limited by being extracted directly between essays. In fact, when... | Yuan Chen, Xia Li |  |
| 1327 |  |  [Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models](https://doi.org/10.18653/v1/2023.acl-long.84) |  | 0 | To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling. Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold:... | Myra Cheng, Esin Durmus, Dan Jurafsky |  |
| 1328 |  |  [On Prefix-tuning for Lightweight Out-of-distribution Detection](https://doi.org/10.18653/v1/2023.acl-long.85) |  | 0 | Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to... | Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, Xinyu Dai |  |
| 1329 |  |  [GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding](https://doi.org/10.18653/v1/2023.acl-long.86) |  | 0 | Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we propose a novel non-autoregressive approach to GEC that decouples the architecture into a permutation network that outputs a self-attention weight matrix that can be used in beam search to find the best... | Konstantin Yakovlev, Alexander Podolskiy, Andrey Bout, Sergey I. Nikolenko, Irina Piontkovskaya |  |
| 1330 |  |  [Measuring Progress in Fine-grained Vision-and-Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.87) |  | 0 | While pretraining on large-scale image–text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack “fine-grained” understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we... | Emanuele Bugliarello, Laurent Sartran, Aishwarya Agrawal, Lisa Anne Hendricks, Aida Nematzadeh |  |
| 1331 |  |  [Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information](https://doi.org/10.18653/v1/2023.acl-long.88) |  | 0 | Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when... | Sunjae Kwon, Rishabh Garodia, Minhwa Lee, Zhichao Yang, Hong Yu |  |
| 1332 |  |  [Chain-of-Skills: A Configurable Model for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.89) |  | 0 | The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the... | Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao |  |
| 1333 |  |  [Elaboration-Generating Commonsense Question Answering at Scale](https://doi.org/10.18653/v1/2023.acl-long.90) |  | 0 | In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models—an elaboration generator and an answer predictor—allowing each to influence the... | Wenya Wang, Vivek Srikumar, Hannaneh Hajishirzi, Noah A. Smith |  |
| 1334 |  |  [Neural Unsupervised Reconstruction of Protolanguage Word Forms](https://doi.org/10.18653/v1/2023.acl-long.91) |  | 0 | We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints... | Andre He, Nicholas Tomlin, Dan Klein |  |
| 1335 |  |  [DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation](https://doi.org/10.18653/v1/2023.acl-long.92) |  | 0 | Self-training emerges as an important research line on domain adaptation. By taking the model’s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model.... | Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu, Dongsheng Li |  |
| 1336 |  |  [On Evaluating Multilingual Compositional Generalization with Translated Datasets](https://doi.org/10.18653/v1/2023.acl-long.93) |  | 0 | Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages? Can models compositionally generalize cross-lingually? As a first step to answering these questions, recent work used neural machine translation to translate datasets for evaluating compositional... | Zi Wang, Daniel Hershcovich |  |
| 1337 |  |  [FAA: Fine-grained Attention Alignment for Cascade Document Ranking](https://doi.org/10.18653/v1/2023.acl-long.94) |  | 0 | Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high query latency. Some recent works further propose cascade document ranking models that extract relevant passages with an efficient selector before ranking, however, their selection and ranking modules... | Zhen Li, Chongyang Tao, Jiazhan Feng, Tao Shen, Dongyan Zhao, Xiubo Geng, Daxin Jiang |  |
| 1338 |  |  [Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.95) |  | 0 | Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic... | Zhong Zhang, Bang Liu, Junming Shao |  |
| 1339 |  |  [Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach](https://doi.org/10.18653/v1/2023.acl-long.96) |  | 0 | Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the... | Jinfeng Zhou, Zhuang Chen, Bo Wang, Minlie Huang |  |
| 1340 |  |  [Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling](https://doi.org/10.18653/v1/2023.acl-long.97) |  | 0 | In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. The joint training solely relies on the dialogue context and target response, getting exempt from... | Mingzhu Cai, Siqi Bao, Xin Tian, Huang He, Fan Wang, Hua Wu |  |
| 1341 |  |  [Why Aren't We NER Yet? Artifacts of ASR Errors in Named Entity Recognition in Spontaneous Speech Transcripts](https://doi.org/10.18653/v1/2023.acl-long.98) |  | 0 | Transcripts of spontaneous human speech present a significant obstacle for traditional NER models. The lack of grammatical structure of spoken utterances and word errors introduced by the ASR make downstream NLP tasks challenging. In this paper, we examine in detail the complex relationship between ASR and NER errors which limit the ability of NER models to recover entity mentions from spontaneous speech transcripts. Using publicly available benchmark datasets (SWNE, Earnings-21, OntoNotes), we... | Piotr Szymanski, Lukasz Augustyniak, Mikolaj Morzy, Adrian Szymczak, Krzysztof Surdyk, Piotr Zelasko |  |
| 1342 |  |  [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://doi.org/10.18653/v1/2023.acl-long.99) |  | 0 | While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate... | Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan |  |
| 1343 |  |  [White-Box Multi-Objective Adversarial Attack on Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.100) |  | 0 | Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for... | Yufei Li, Zexin Li, Yingfan Gao, Cong Liu |  |
| 1344 |  |  [A Cautious Generalization Goes a Long Way: Learning Morphophonological Rules](https://doi.org/10.18653/v1/2023.acl-long.101) |  | 0 | Explicit linguistic knowledge, encoded by resources such as rule-based morphological analyzers, continues to prove useful in downstream NLP tasks, especially for low-resource languages and dialects. Rules are an important asset in descriptive linguistic grammars. However, creating such resources is usually expensive and non-trivial, especially for spoken varieties with no written standard. In this work, we present a novel approach for automatically learning morphophonological rules of Arabic... | Salam Khalifa, Sarah R. B. Payne, Jordan Kodner, Ellen Broselow, Owen Rambow |  |
| 1345 |  |  [Few-shot Adaptation Works with UnpredicTable Data](https://doi.org/10.18653/v1/2023.acl-long.102) |  | 0 | Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of... | Jun Shern Chan, Michael Pieler, Jonathan Jao, Jérémy Scheurer, Ethan Perez |  |
| 1346 |  |  [Cross-lingual Science Journalism: Select, Simplify and Rewrite Summaries for Non-expert Readers](https://doi.org/10.18653/v1/2023.acl-long.103) |  | 0 | Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science journalists’ work. We analyze the performance of possible existing solutions as baselines for the CSJ task. Based on these findings, we propose to combine the three components - SELECT, SIMPLIFY and... | Mehwish Fatima, Michael Strube |  |
| 1347 |  |  [HuCurl: Human-induced Curriculum Discovery](https://doi.org/10.18653/v1/2023.acl-long.104) |  | 0 | We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy... | Mohamed Elgaar, Hadi Amiri |  |
| 1348 |  |  [kNN-TL: k-Nearest-Neighbor Transfer Learning for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.105) |  | 0 | Transfer learning has been shown to be an effective technique for enhancing the performance of low-resource neural machine translation (NMT). This is typically achieved through either fine-tuning a child model with a pre-trained parent model, or by utilizing the out- put of the parent model during the training of the child model. However, these methods do not make use of the parent knowledge during the child inference, which may limit the translation performance. In this paper, we propose a... | Shudong Liu, Xuebo Liu, Derek F. Wong, Zhaocong Li, Wenxiang Jiao, Lidia S. Chao, Min Zhang |  |
| 1349 |  |  [Do language models have coherent mental models of everyday things?](https://doi.org/10.18653/v1/2023.acl-long.106) |  | 0 | When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 “X relation Y?” true/false questions. Using... | Yuling Gu, Bhavana Dalvi Mishra, Peter Clark |  |
| 1350 |  |  [Rogue Scores](https://doi.org/10.18653/v1/2023.acl-long.107) |  | 0 | Correct, comparable, and reproducible model evaluation is essential for progress in machine learning. Over twenty years, thousands of language and vision models have been evaluated with a popular metric called ROUGE. Does this widespread benchmark metric meet these three evaluation criteria? This systematic review of over two thousand publications using ROUGE finds: (A) Critical evaluation decisions and parameters are routinely omitted, making most reported scores irreproducible. (B)... | Max Grusky |  |
| 1351 |  |  [Instruction Induction: From Few Examples to Natural Language Task Descriptions](https://doi.org/10.18653/v1/2023.acl-long.108) |  | 0 | Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing... | Or Honovich, Uri Shaham, Samuel R. Bowman, Omer Levy |  |
| 1352 |  |  [In-Context Analogical Reasoning with Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.109) |  | 0 | Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of... | Xiaoyang Hu, Shane Storks, Richard L. Lewis, Joyce Chai |  |
| 1353 |  |  [Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering](https://doi.org/10.18653/v1/2023.acl-long.110) |  | 0 | The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and... | Avi Caciularu, Matthew E. Peters, Jacob Goldberger, Ido Dagan, Arman Cohan |  |
| 1354 |  |  [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.111) |  | 0 | It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability. In this paper, we propose Learning Good... | Yuxin Ren, Zihan Zhong, Xingjian Shi, Yi Zhu, Chun Yuan, Mu Li |  |
| 1355 |  |  [REV: Information-Theoretic Evaluation of Free-Text Rationales](https://doi.org/10.18653/v1/2023.acl-long.112) |  | 0 | Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional... | Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, Swabha Swayamdipta |  |
| 1356 |  |  [ELQA: A Corpus of Metalinguistic Questions and Answers about English](https://doi.org/10.18653/v1/2023.acl-long.113) |  | 0 | We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic—it consists of language... | Shabnam Behzad, Keisuke Sakaguchi, Nathan Schneider, Amir Zeldes |  |
| 1357 |  |  [Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2023.acl-long.114) |  | 0 | Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively decouple semantics of samples, limiting the zero-shot performance of DST. In this paper, we present a simple and effective “divide, conquer and combine” solution, which explicitly disentangles the... | Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng Lin, Shi Wang, Dacheng Tao, Li Guo |  |
| 1358 |  |  [BIG-C: a Multimodal Multi-Purpose Dataset for Bemba](https://doi.org/10.18653/v1/2023.acl-long.115) |  | 0 | We present BIG-C (Bemba Image Grounded Conversations), a large multimodal dataset for Bemba. While Bemba is the most populous language of Zambia, it exhibits a dearth of resources which render the development of language technologies or language processing research almost impossible. The dataset is comprised of multi-turn dialogues between Bemba speakers based on images, transcribed and translated into English. There are more than 92,000 utterances/sentences, amounting to more than 180 hours of... | Claytone Sikasote, Eunice Mukonde, Md Mahfuz Ibn Alam, Antonios Anastasopoulos |  |
| 1359 |  |  [Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues](https://doi.org/10.18653/v1/2023.acl-long.116) |  | 0 | User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user’s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user’s task goals. Existing studies on USM neglect explicitly modeling the user’s task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided... | Yue Feng, Yunlong Jiao, Animesh Prasad, Nikolaos Aletras, Emine Yilmaz, Gabriella Kazai |  |
| 1360 |  |  [Robust Multi-bit Natural Language Watermarking through Invariant Features](https://doi.org/10.18653/v1/2023.acl-long.117) |  | 0 | Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a... | KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak |  |
| 1361 |  |  [KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.118) |  | 0 | With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts — from... | Shangbin Feng, Zhaoxuan Tan, Wenqian Zhang, Zhenyu Lei, Yulia Tsvetkov |  |
| 1362 |  |  [AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction](https://doi.org/10.18653/v1/2023.acl-long.119) |  | 0 | Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally treated as a classification problem for predicting attribute types or a sequence tagging problem for labeling attribute values, where two paradigms, i.e., closed-world and open-world assumption, are... | Yanzeng Li, Bingcong Xue, Ruoyu Zhang, Lei Zou |  |
| 1363 |  |  [Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.120) |  | 0 | The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference,... | Shiyue Zhang, David Wan, Mohit Bansal |  |
| 1364 |  |  [Improving Translation Quality Estimation with Bias Mitigation](https://doi.org/10.18653/v1/2023.acl-long.121) |  | 0 | State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation performance. Our method is based on the contrastive learning between clean and noisy sentence pairs. We first introduce noise to the target side of the parallel sentence pair, forming the negative samples.... | Hui Huang, Shuangzhi Wu, Kehai Chen, Hui Di, Muyun Yang, Tiejun Zhao |  |
| 1365 |  |  [Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation](https://doi.org/10.18653/v1/2023.acl-long.122) |  | 0 | We propose a genetic algorithm (GA) based method for modifying n-best lists produced by a machine translation (MT) system. Our method offers an innovative approach to improving MT quality and identifying weaknesses in evaluation metrics. Using common GA operations (mutation and crossover) on a list of hypotheses in combination with a fitness function (an arbitrary MT metric), we obtain novel and diverse outputs with high metric scores. With a combination of multiple MT metrics as the fitness... | Josef Jon, Ondrej Bojar |  |
| 1366 |  |  [MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions](https://doi.org/10.18653/v1/2023.acl-long.123) |  | 0 | Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users’ values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design... | Hao Sun, Zhexin Zhang, Fei Mi, Yasheng Wang, Wei Liu, Jianwei Cui, Bin Wang, Qun Liu, Minlie Huang |  |
| 1367 |  |  [Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion](https://doi.org/10.18653/v1/2023.acl-long.124) |  | 0 | Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing... | Shaoxiang Wu, Damai Dai, Ziwei Qin, Tianyu Liu, Binghuai Lin, Yunbo Cao, Zhifang Sui |  |
| 1368 |  |  [SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.acl-long.125) |  | 0 | In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between... | Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei |  |
| 1369 |  |  [From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained](https://doi.org/10.18653/v1/2023.acl-long.126) |  | 0 | For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly annotated training data that contains incorrect labels. However, the performance of models trained solely with such data can be limited by the errors in the automatic annotation. Recently, there are a few... | Hongliang Dai, Ziqian Zeng |  |
| 1370 |  |  [Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers](https://doi.org/10.18653/v1/2023.acl-long.127) |  | 0 | To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model’s predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model’s... | Parikshit Bansal, Amit Sharma |  |
| 1371 |  |  [What Makes Pre-trained Language Models Better Zero-shot Learners?](https://doi.org/10.18653/v1/2023.acl-long.128) |  | 0 | Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language... | Jinghui Lu, Dongsheng Zhu, Weidong Han, Rui Zhao, Brian Mac Namee, Fei Tan |  |
| 1372 |  |  [Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations](https://doi.org/10.18653/v1/2023.acl-long.129) |  | 0 | Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying... | Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 1373 |  |  [Learning Optimal Policy for Simultaneous Machine Translation via Binary Search](https://doi.org/10.18653/v1/2023.acl-long.130) |  | 0 | Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In... | Shoutao Guo, Shaolei Zhang, Yang Feng |  |
| 1374 |  |  [Better Simultaneous Translation with Monotonic Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.131) |  | 0 | Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel... | Shushu Wang, Jing Wu, Kai Fan, Wei Luo, Jun Xiao, Zhongqiang Huang |  |
| 1375 |  |  [StoryARG: a corpus of narratives and personal experiences in argumentative texts](https://doi.org/10.18653/v1/2023.acl-long.132) |  | 0 | Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do – and yet, this phenomenon is largely unexplored in computational argumentation. Which role do stories play in an argument? Do they make the argument more effective? What are their narrative properties? To address these questions, we collected and... | Neele Falk, Gabriella Lapesa |  |
| 1376 |  |  [Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue](https://doi.org/10.18653/v1/2023.acl-long.133) |  | 0 | Factual correctness is often the limiting factor in practical applications of natural language generation in high-stakes domains such as healthcare. An essential requirement for maintaining factuality is the ability to deal with rare tokens. This paper focuses on rare tokens that appear in both the source and the reference sequences, and which, when missed during generation, decrease the factual correctness of the output text. For high-stake domains that are also knowledge-rich, we show how to... | Maksim Eremeev, Ilya Valmianski, Xavier Amatriain, Anitha Kannan |  |
| 1377 |  |  [Sequence Parallelism: Long Sequence Training from System Perspective](https://doi.org/10.18653/v1/2023.acl-long.134) |  | 0 | Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which... | Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, Yang You |  |
| 1378 |  |  [MUSTIE: Multimodal Structural Transformer for Web Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.135) |  | 0 | The task of web information extraction is to extract target fields of an object from web pages, such as extracting the name, genre and actor from a movie page. Recent sequential modeling approaches have achieved state-of-the-art results on web information extraction. However, most of these methods only focus on extracting information from textual sources while ignoring the rich information from other modalities such as image and web layout. In this work, we propose a novel MUltimodal Structural... | Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Zenglin Xu, Shaoliang Nie, Sinong Wang, Madian Khabsa, Hamed Firooz, Dongfang Liu |  |
| 1379 |  |  [Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In](https://doi.org/10.18653/v1/2023.acl-long.136) |  | 0 | Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose... | Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu |  |
| 1380 |  |  [TableVLM: Multi-modal Pre-training for Table Structure Recognition](https://doi.org/10.18653/v1/2023.acl-long.137) |  | 0 | Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal... | Leiyuan Chen, Chengsong Huang, Xiaoqing Zheng, Jinshu Lin, Xuanjing Huang |  |
| 1381 |  |  [Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?](https://doi.org/10.18653/v1/2023.acl-long.138) |  | 0 | Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches.... | Jiashu Xu, Mingyu Derek Ma, Muhao Chen |  |
| 1382 |  |  [Dynamic Routing Transformer Network for Multimodal Sarcasm Detection](https://doi.org/10.18653/v1/2023.acl-long.139) |  | 0 | Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by... | Yuan Tian, Nan Xu, Ruike Zhang, Wenji Mao |  |
| 1383 |  |  [What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary](https://doi.org/10.18653/v1/2023.acl-long.140) |  | 0 | Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and... | Ori Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov, Jonathan Berant, Amir Globerson |  |
| 1384 |  |  [Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach](https://doi.org/10.18653/v1/2023.acl-long.141) |  | 0 | We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start... | Yue Yu, Rongzhi Zhang, Ran Xu, Jieyu Zhang, Jiaming Shen, Chao Zhang |  |
| 1385 |  |  [Training-free Neural Architecture Search for RNNs and Transformers](https://doi.org/10.18653/v1/2023.acl-long.142) |  | 0 | Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS... | Aaron Serianni, Jugal Kalita |  |
| 1386 |  |  [CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1, 500+ Language Pairs](https://doi.org/10.18653/v1/2023.acl-long.143) |  | 0 | We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of... | Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, YuanFang Li, YongBin Kang, Rifat Shahriyar |  |
| 1387 |  |  [Improving Gradient Trade-offs between Tasks in Multi-task Text Classification](https://doi.org/10.18653/v1/2023.acl-long.144) |  | 0 | Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. However, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since different tasks might conflict with each other. Existing MTL methods for alleviating this issue is to leverage heuristics or gradient-based algorithm to achieve an arbitrary Pareto optimal trade-off... | Heyan Chai, Jinhao Cui, Ye Wang, Min Zhang, Binxing Fang, Qing Liao |  |
| 1388 |  |  [Bi-Phone: Modeling Inter Language Phonetic Influences in Text](https://doi.org/10.18653/v1/2023.acl-long.145) |  | 0 | A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing... | Abhirut Gupta, Ananya B. Sai, Richard Sproat, Yuri Vasilevski, James S. Ren, Ambarish Jash, Sukhdeep S. Sodhi, Aravindan Raghuveer |  |
| 1389 |  |  [Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment](https://doi.org/10.18653/v1/2023.acl-long.146) |  | 0 | Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined... | Shengqiong Wu, Hao Fei, Wei Ji, TatSeng Chua |  |
| 1390 |  |  [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.147) |  | 0 | Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt... | Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy KaWei Lee, EePeng Lim |  |
| 1391 |  |  [RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models](https://doi.org/10.18653/v1/2023.acl-long.148) |  | 0 | To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As... | Zheng Liu, Shitao Xiao, Yingxia Shao, Zhao Cao |  |
| 1392 |  |  [DecompX: Explaining Transformers Decisions by Propagating Token Decomposition](https://doi.org/10.18653/v1/2023.acl-long.149) |  | 0 | An emerging solution for explaining Transformer-based models is to use vector-based analysis on how the representations are formed. However, providing a faithful vector-based explanation for a multi-layer model could be challenging in three aspects: (1) Incorporating all components into the analysis, (2) Aggregating the layer dynamics to determine the information flow and mixture throughout the entire model, and (3) Identifying the connection between the vector-based analysis and the model’s... | Ali Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar |  |
| 1393 |  |  [Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step](https://doi.org/10.18653/v1/2023.acl-long.150) |  | 0 | Chain-of-thought prompting (e.g., “Let’s think step-by-ste”) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M—1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method... | Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, KaiWei Chang, Yejin Choi |  |
| 1394 |  |  [Generating EDU Extracts for Plan-Guided Summary Re-Ranking](https://doi.org/10.18653/v1/2023.acl-long.151) |  | 0 | Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues. We ground each candidate abstract on its own unique content... | Griffin Adams, Alexander R. Fabbri, Faisal Ladhak, Noémie Elhadad, Kathleen R. McKeown |  |
| 1395 |  |  [A Survey on Asking Clarification Questions Datasets in Conversational Systems](https://doi.org/10.18653/v1/2023.acl-long.152) |  | 0 | The ability to understand a user’s underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users’ true intent from their queries or utterances arise as an essential task. However, it is noticeable that a key limitation of the existing ACQs studies is their incomparability, from inconsistent use of data, distinct experimental setups and evaluation strategies.... | Hossein A. Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz, Aldo Lipani |  |
| 1396 |  |  [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://doi.org/10.18653/v1/2023.acl-long.153) |  | 0 | Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT... | Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun |  |
| 1397 |  |  [Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.154) |  | 0 | For many languages, machine translation progress is hindered by the lack of reliable training data. Models are trained on whatever pre-existing datasets may be available and then augmented with synthetic data, because it is often not economical to pay for the creation of large-scale datasets. But for the case of low-resource languages, would the creation of a few thousand professionally translated sentence pairs give any benefit? In this paper, we show that it does. We describe a broad data... | Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, Francisco Guzmán |  |
| 1398 |  |  [RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks](https://doi.org/10.18653/v1/2023.acl-long.155) |  | 0 | Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during inference. Towards this overlooked aspect, we propose a defense framework that aims to mitigate attacks by confusing attackers and correcting adversarial contexts that are caused by malicious perturbations.... | Zhaoyang Wang, Zhiyue Liu, Xiaopeng Zheng, Qinliang Su, Jiahai Wang |  |
| 1399 |  |  [Gradient-based Intra-attention Pruning on Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.156) |  | 0 | Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and... | Ziqing Yang, Yiming Cui, Xin Yao, Shijin Wang |  |
| 1400 |  |  [Learning to Substitute Spans towards Improving Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.157) |  | 0 | Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias... | Zhaoyi Li, Ying Wei, Defu Lian |  |
| 1401 |  |  [DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.acl-long.158) |  | 0 | Empathy is a crucial factor in open-domain conversations, which naturally shows one’s caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context... | Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He |  |
| 1402 |  |  [BREAK: Breaking the Dialogue State Tracking Barrier with Beam Search and Re-ranking](https://doi.org/10.18653/v1/2023.acl-long.159) |  | 0 | Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue state. Motivated by this observation, we introduce a novel framework, called BREAK (Beam search and RE-rAnKing), that achieves outstanding performance on DST. BREAK performs DST in two stages: (i)... | Seungpil Won, Heeyoung Kwak, Joongbo Shin, Janghoon Han, Kyomin Jung |  |
| 1403 |  |  [Faithful Low-Resource Data-to-Text Generation through Cycle Training](https://doi.org/10.18653/v1/2023.acl-long.160) |  | 0 | Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency... | Zhuoer Wang, Marcus D. Collins, Nikhita Vedula, Simone Filice, Shervin Malmasi, Oleg Rokhlenko |  |
| 1404 |  |  [Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing](https://doi.org/10.18653/v1/2023.acl-long.161) |  | 0 | Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. Toincrease the performance stability, previous debiasing methods empirically capture bias features from data to prevent the model from corresponding biases. However, our analyses show that the empirical debiasing methods may fail to capture part of the... | Li Du, Xiao Ding, Zhouhao Sun, Ting Liu, Bing Qin, Jingshuo Liu |  |
| 1405 |  |  [Dynamic and Efficient Inference for Text Generation via BERT Family](https://doi.org/10.18653/v1/2023.acl-long.162) |  | 0 | Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight... | Xiaobo Liang, Juntao Li, Lijun Wu, Ziqiang Cao, Min Zhang |  |
| 1406 |  |  [Learning to Generate Equitable Text in Dialogue from Biased Training Data](https://doi.org/10.18653/v1/2023.acl-long.163) |  | 0 | The ingrained principles of fairness in a dialogue system’s decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text... | Anthony Sicilia, Malihe Alikhani |  |
| 1407 |  |  [Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.164) |  | 0 | Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (PLMs), which has exhibited effectiveness in the few-shot flat text classification tasks. However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training... | Ke Ji, Yixin Lian, Jingsheng Gao, Baoyuan Wang |  |
| 1408 |  |  [Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.165) |  | 0 | The goal of multimodal abstractive summarization (MAS) is to produce a concise summary given the multimodal data (text and vision). Existing studies on MAS mainly focus on how to effectively use the extracted visual features, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the quality of the visual features to the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. In this paper, we... | Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, Jie Zhou |  |
| 1409 |  |  [Helping a Friend or Supporting a Cause? Disentangling Active and Passive Cosponsorship in the U.S. Congress](https://doi.org/10.18653/v1/2023.acl-long.166) |  | 0 | In the U.S. Congress, legislators can use active and passive cosponsorship to support bills. We show that these two types of cosponsorship are driven by two different motivations: the backing of political colleagues and the backing of the bill’s content. To this end, we develop an Encoder+RGCN based model that learns legislator representations from bill texts and speech transcripts. These representations predict active and passive cosponsorship with an F1-score of 0.88.Applying our... | Giuseppe Russo, Christoph Gote, Laurence Brandenberger, Sophia Schlosser, Frank Schweitzer |  |
| 1410 |  |  [TREA: Tree-Structure Reasoning Schema for Conversational Recommendation](https://doi.org/10.18653/v1/2023.acl-long.167) |  | 0 | Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they... | Wendi Li, Wei Wei, Xiaoye Qu, XianLing Mao, Ye Yuan, Wenfeng Xie, Dangyang Chen |  |
| 1411 |  |  [CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality](https://doi.org/10.18653/v1/2023.acl-long.168) |  | 0 | There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while leaving other languages underexplored.To alleviate these limitations, in this paper, we present CATS, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality. The dataset aims... | Liang Li, Ruiying Geng, Chengyang Fang, Bing Li, Can Ma, Rongyu Cao, Binhua Li, Fei Huang, Yongbin Li |  |
| 1412 |  |  [Multilingual Multifaceted Understanding of Online News in Terms of Genre, Framing, and Persuasion Techniques](https://doi.org/10.18653/v1/2023.acl-long.169) |  | 0 | We present a new multilingual multifacet dataset of news articles, each annotated for genre (objective news reporting vs. opinion vs. satire), framing (what key aspects are highlighted), and persuasion techniques (logical fallacies, emotional appeals, ad hominem attacks, etc.). The persuasion techniques are annotated at the span level, using a taxonomy of 23 fine-grained techniques grouped into 6 coarse categories. The dataset contains 1,612 news articles covering recent news on current topics... | Jakub Piskorski, Nicolas Stefanovitch, Nikolaos Nikolaidis, Giovanni Da San Martino, Preslav Nakov |  |
| 1413 |  |  [Learning Action Conditions from Instructional Manuals for Instruction Understanding](https://doi.org/10.18653/v1/2023.acl-long.170) |  | 0 | The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, which extracts mentions of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach utilizing automatically constructed... | TeLin Wu, Caiqi Zhang, Qingyuan Hu, Alexander Spangher, Nanyun Peng |  |
| 1414 |  |  [StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation](https://doi.org/10.18653/v1/2023.acl-long.171) |  | 0 | Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora. To address this, we introduce StoryWars, a new dataset of over 40,000 collaborative stories written by 9,400 different authors from an online platform. We design 12 task types, comprising 7... | Yulun Du, Lydia B. Chilton |  |
| 1415 |  |  [Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning](https://doi.org/10.18653/v1/2023.acl-long.172) |  | 0 | Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find... | Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, ChienSheng Wu |  |
| 1416 |  |  [Do PLMs Know and Understand Ontological Knowledge?](https://doi.org/10.18653/v1/2023.acl-long.173) |  | 0 | Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a system- atic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic un- derstanding of the knowledge rather than... | Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie, Kewei Tu |  |
| 1417 |  |  [CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection](https://doi.org/10.18653/v1/2023.acl-long.174) |  | 0 | Establishing retrieval-based dialogue systems that can select appropriate responses from the pre-built index has gained increasing attention. Recent common practice is to construct a two-stage pipeline with a fast retriever (e.g., bi-encoder) for first-stage recall followed by a smart response reranker (e.g., cross-encoder) for precise ranking. However, existing studies either optimize the retriever and reranker in independent ways, or distill the knowledge from a pre-trained reranker into the... | Chongyang Tao, Jiazhan Feng, Tao Shen, Chang Liu, Juntao Li, Xiubo Geng, Daxin Jiang |  |
| 1418 |  |  [Exploring How Generative Adversarial Networks Learn Phonological Representations](https://doi.org/10.18653/v1/2023.acl-long.175) |  | 0 | This paper explores how Generative Adversarial Networks (GANs) learn representations of phonological phenomena. We analyze how GANs encode contrastive and non-contrastive nasality in French and English vowels by applying the ciwGAN architecture (Begus, 2021). Begus claims that ciwGAN encodes linguistically meaningful representations with categorical variables in its latent space and manipulating the latent variables shows an almost one to one corresponding control of the phonological features... | Jingyi Chen, Micha Elsner |  |
| 1419 |  |  [Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis](https://doi.org/10.18653/v1/2023.acl-long.176) |  | 0 | We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can... | Mario Giulianelli, Iris Luden, Raquel Fernández, Andrey Kutuzov |  |
| 1420 |  |  [Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.177) |  | 0 | Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the... | Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wentau Yih, Ziyu Yao |  |
| 1421 |  |  [InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation](https://doi.org/10.18653/v1/2023.acl-long.178) |  | 0 | Automatic image captioning evaluation is critical for benchmarking and promoting advances in image captioning research. Existing metrics only provide a single score to measure caption qualities, which are less explainable and informative. Instead, we humans can easily identify the problems of captions in details, e.g., which words are inaccurate and which salient objects are not described, and then rate the caption quality. To support such informative feedback, we propose an Informative Metric... | Anwen Hu, Shizhe Chen, Liang Zhang, Qin Jin |  |
| 1422 |  |  [An Invariant Learning Characterization of Controlled Text Generation](https://doi.org/10.18653/v1/2023.acl-long.179) |  | 0 | Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper,... | Carolina Zheng, Claudia Shi, Keyon Vafa, Amir Feder, David M. Blei |  |
| 1423 |  |  [HistRED: A Historical Document-Level Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.180) |  | 0 | Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE... | Soyoung Yang, Minseok Choi, Youngwoo Cho, Jaegul Choo |  |
| 1424 |  |  [A Critical Evaluation of Evaluations for Long-form Question Answering](https://doi.org/10.18653/v1/2023.acl-long.181) |  | 0 | Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts’ evaluation, which focuses on new... | Fangyuan Xu, Yixiao Song, Mohit Iyyer, Eunsol Choi |  |
| 1425 |  |  [HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation](https://doi.org/10.18653/v1/2023.acl-long.182) |  | 0 | Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue... | Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang |  |
| 1426 |  |  [Generating User-Engaging News Headlines](https://doi.org/10.18653/v1/2023.acl-long.183) |  | 0 | The potential choices for news article headlines are enormous, and finding the right balance between conveying the essential message and capturing the reader’s attention is key to effective headlining. However, presenting the same news headline to all readers is a suboptimal strategy, because it does not take into account the different preferences and interests of diverse readers, who may be confused about why a particular article has been recommended to them and do not see a clear connection... | Pengshan Cai, Kaiqiang Song, Sangwoo Cho, Hongwei Wang, Xiaoyang Wang, Hong Yu, Fei Liu, Dong Yu |  |
| 1427 |  |  [Word sense extension](https://doi.org/10.18653/v1/2023.acl-long.184) |  | 0 | Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word... | Lei Yu, Yang Xu |  |
| 1428 |  |  [PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism](https://doi.org/10.18653/v1/2023.acl-long.185) |  | 0 | We investigate response generation for multi-turn dialogue in generative chatbots. Existing generative modelsbased on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the history, which makesmodels unable to capture the subtle variability observed in different dialogues and cannot distinguish the differencesbetween dialogues that are similar in composition. In this paper, we propose Pseudo-Variational Gated Recurrent Unit (PVGRU). The key novelty of PVGRU is a... | Yongkang Liu, Shi Feng, Daling Wang, Yifei Zhang, Hinrich Schütze |  |
| 1429 |  |  [Decoding Symbolism in Language Models](https://doi.org/10.18653/v1/2023.acl-long.186) |  | 0 | This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on different types of symbolism and analyze the outcomes along multiple metrics. Our findings suggest that conventional symbols are more reliably elicited from LMs while situated symbols are more challenging.... | Meiqi Guo, Rebecca Hwa, Adriana Kovashka |  |
| 1430 |  |  [A Survey on Zero Pronoun Translation](https://doi.org/10.18653/v1/2023.acl-long.187) |  | 0 | Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural... | Longyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi, Zhaopeng Tu |  |
| 1431 |  |  [We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit](https://doi.org/10.18653/v1/2023.acl-long.188) |  | 0 | Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to... | Davide Testa, Emmanuele Chersoni, Alessandro Lenci |  |
| 1432 |  |  [MPCHAT: Towards Multimodal Persona-Grounded Conversation](https://doi.org/10.18653/v1/2023.acl-long.189) |  | 0 | In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker’s personal characteristics and experiences in episodic memory (Rubin et al., 2003; Conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we... | Jaewoo Ahn, Yeda Song, Sangdoo Yun, Gunhee Kim |  |
| 1433 |  |  [DOC: Improving Long Story Coherence With Detailed Outline Control](https://doi.org/10.18653/v1/2023.acl-long.190) |  | 0 | We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected... | Kevin Yang, Dan Klein, Nanyun Peng, Yuandong Tian |  |
| 1434 |  |  [Dual-Alignment Pre-training for Cross-lingual Sentence Embedding](https://doi.org/10.18653/v1/2023.acl-long.191) |  | 0 | Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment.... | Ziheng Li, Shaohan Huang, Zihan Zhang, ZhiHong Deng, Qiang Lou, Haizhen Huang, Jian Jiao, Furu Wei, Weiwei Deng, Qi Zhang |  |
| 1435 |  |  [Exploring Better Text Image Translation with Multimodal Codebook](https://doi.org/10.18653/v1/2023.acl-long.192) |  | 0 | Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a... | Zhibin Lan, Jiawei Yu, Xiang Li, Wen Zhang, Jian Luan, Bin Wang, Degen Huang, Jinsong Su |  |
| 1436 |  |  [FEDLEGAL: The First Real-World Federated Learning Benchmark for Legal NLP](https://doi.org/10.18653/v1/2023.acl-long.193) |  | 0 | The inevitable private information in legal data necessitates legal artificial intelligence to study privacy-preserving and decentralized learning methods. Federated learning (FL) has merged as a promising technique for multiple participants to collaboratively train a shared model while efficiently protecting the sensitive data of participants. However, to the best of our knowledge, there is no work on applying FL to legal NLP. To fill this gap, this paper presents the first real-world FL... | Zhuo Zhang, Xiangjing Hu, Jingyuan Zhang, Yating Zhang, Hui Wang, Lizhen Qu, Zenglin Xu |  |
| 1437 |  |  [A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.acl-long.194) |  | 0 | Parameter-Efficient Tuning (PET) has shown remarkable performance by fine-tuning only a small number of parameters of the pre-trained language models (PLMs) for the downstream tasks, while it is also possible to construct backdoor attacks due to the vulnerability of pre-trained weights. However, a large reduction in the number of attackable parameters in PET will cause the user’s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting. We find that... | Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, Weiping Wang |  |
| 1438 |  |  [History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling](https://doi.org/10.18653/v1/2023.acl-long.195) |  | 0 | Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models... | Hao Sun, Yang Li, Liwei Deng, Bowen Li, Binyuan Hui, Binhua Li, Yunshi Lan, Yan Zhang, Yongbin Li |  |
| 1439 |  |  [From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation](https://doi.org/10.18653/v1/2023.acl-long.196) |  | 0 | Entailment Graphs (EGs) have been constructed based on extracted corpora as a strong and explainable form to indicate context-independent entailment relation in natural languages. However, EGs built by previous methods often suffer from the severe sparsity issues, due to limited corpora available and the long-tail phenomenon of predicate distributions. In this paper, we propose a multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to tackle this problem. Given several seed... | Zhibin Chen, Yansong Feng, Dongyan Zhao |  |
| 1440 |  |  [Alleviating Over-smoothing for Unsupervised Sentence Representation](https://doi.org/10.18653/v1/2023.acl-long.197) |  | 0 | Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to... | Nuo Chen, Linjun Shou, Jian Pei, Ming Gong, Bowen Cao, Jianhui Chang, Jia Li, Daxin Jiang |  |
| 1441 |  |  [Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model](https://doi.org/10.18653/v1/2023.acl-long.198) |  | 0 | The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80% of experts without further finetuning and with a negligible loss in translation quality, which... | Yeskendir Koishekenov, Alexandre Berard, Vassilina Nikoulina |  |
| 1442 |  |  [DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.acl-long.199) |  | 0 | Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and... | William Held, Christopher Hidey, Fei Liu, Eric Zhu, Rahul Goel, Diyi Yang, Rushin Shah |  |
| 1443 |  |  [From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.200) |  | 0 | Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another... | Li Sun, Florian Luisier, Kayhan Batmanghelich, Dinei A. F. Florêncio, Cha Zhang |  |
| 1444 |  |  [MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling](https://doi.org/10.18653/v1/2023.acl-long.201) |  | 0 | We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures... | Yu Song, Santiago Miret, Bang Liu |  |
| 1445 |  |  [Code4Struct: Code Generation for Few-Shot Event Structure Prediction](https://doi.org/10.18653/v1/2023.acl-long.202) |  | 0 | Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as... | Xingyao Wang, Sha Li, Heng Ji |  |
| 1446 |  |  [GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles](https://doi.org/10.18653/v1/2023.acl-long.203) |  | 0 | Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming... | Tanmay Parekh, IHung Hsu, KuanHao Huang, KaiWei Chang, Nanyun Peng |  |
| 1447 |  |  [Efficient Semiring-Weighted Earley Parsing](https://doi.org/10.18653/v1/2023.acl-long.204) |  | 0 | We present Earley’s (1970) context-free parsing algorithm as a deduction system, incorporating various known and new speed-ups. In particular, our presentation supports a known worst-case runtime improvement from Earley’s (1970) O(N3\|G\|\|R\|), which is unworkable for the large grammars that arise in natural language processing, to O(N3\|G\|), which matches the complexity of CKY on a binarized version of the grammar G. Here N is the length of the sentence, \|R\| is the number of productions in... | Andreas Opedal, Ran Zmigrod, Tim Vieira, Ryan Cotterell, Jason Eisner |  |
| 1448 |  |  [Tree-Based Representation and Generation of Natural and Mathematical Language](https://doi.org/10.18653/v1/2023.acl-long.205) |  | 0 | Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text,... | Alexander Scarlatos, Andrew S. Lan |  |
| 1449 |  |  [ParaLS: Lexical Substitution via Pretrained Paraphraser](https://doi.org/10.18653/v1/2023.acl-long.206) |  | 0 | Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence’s meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the... | Jipeng Qiang, Kang Liu, Yun Li, Yunhao Yuan, Yi Zhu |  |
| 1450 |  |  [Peer-Label Assisted Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.207) |  | 0 | Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in... | Junru Song, Feifei Wang, Yang Yang |  |
| 1451 |  |  [Free Lunch for Efficient Textual Commonsense Integration in Language Models](https://doi.org/10.18653/v1/2023.acl-long.208) |  | 0 | Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its... | Wanyun Cui, Xingran Chen |  |
| 1452 |  |  [A Probabilistic Framework for Discovering New Intents](https://doi.org/10.18653/v1/2023.acl-long.209) |  | 0 | Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not deeply explore the intrinsic structure of unlabeled data, and as a result, cannot seek out the characteristics that define an intent in general. In this paper, starting from the intuition that... | Yunhua Zhou, Guofeng Quan, Xipeng Qiu |  |
| 1453 |  |  [MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.210) |  | 0 | Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We... | Leonhard Hennig, Philippe Thomas, Sebastian Möller |  |
| 1454 |  |  [Towards Higher Pareto Frontier in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.211) |  | 0 | Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontierIn Pareto optimization, Pareto optimal solutions refer to solutions in which... | YiChong Huang, Xiaocheng Feng, Xinwei Geng, Baohang Li, Bing Qin |  |
| 1455 |  |  [Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization](https://doi.org/10.18653/v1/2023.acl-long.212) |  | 0 | By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin. However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems. In this paper, we focus on scaling up the parameters of PLMs only during fine-tuning, to benefit from the over-parameterization, while without increasing... | ZeFeng Gao, Kun Zhou, Peiyu Liu, Wayne Xin Zhao, JiRong Wen |  |
| 1456 |  |  [Entity Tracking in Language Models](https://doi.org/10.18653/v1/2023.acl-long.213) |  | 0 | Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate... | Najoung Kim, Sebastian Schuster |  |
| 1457 |  |  [A Textual Dataset for Situated Proactive Response Selection](https://doi.org/10.18653/v1/2023.acl-long.214) |  | 0 | Recent data-driven conversational models are able to return fluent, consistent, and informative responses to many kinds of requests and utterances in task-oriented scenarios. However, these responses are typically limited to just the immediate local topic instead of being wider-ranging and proactively taking the conversation further, for example making suggestions to help customers achieve their goals. This inadequacy reflects a lack of understanding of the interlocutor’s situation and implicit... | Naoki Otani, Jun Araki, HyeongSik Kim, Eduard H. Hovy |  |
| 1458 |  |  [DiffusionNER: Boundary Diffusion for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.215) |  | 0 | In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then... | Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang |  |
| 1459 |  |  [WACO: Word-Aligned Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.216) |  | 0 | End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea... | Siqi Ouyang, Rong Ye, Lei Li |  |
| 1460 |  |  [Cross-lingual Continual Learning](https://doi.org/10.18653/v1/2023.acl-long.217) |  | 0 | The longstanding goal of multi-lingual learning has been to develop a universal cross-lingual model that can withstand the changes in multi-lingual data distributions. There has been a large amount of work to adapt such multi-lingual models to unseen target languages. However, the majority of work in this direction focuses on the standard one-hop transfer learning pipeline from source to target languages, whereas in realistic scenarios, new languages can be incorporated at any time in a... | Meryem M'hamdi, Xiang Ren, Jonathan May |  |
| 1461 |  |  [Faithful Question Answering with Monte-Carlo Planning](https://doi.org/10.18653/v1/2023.acl-long.218) |  | 0 | Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of... | Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu, Changshui Zhang |  |
| 1462 |  |  [Unbalanced Optimal Transport for Unbalanced Word Alignment](https://doi.org/10.18653/v1/2023.acl-long.219) |  | 0 | Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment,... | Yuki Arase, Han Bao, Sho Yokoi |  |
| 1463 |  |  [Guiding Computational Stance Detection with Expanded Stance Triangle Framework](https://doi.org/10.18653/v1/2023.acl-long.220) |  | 0 | Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational solutions to model semantic features and infer the corresponding implications from a literal statement. Moreover, the limited amount of available training data leads to subpar performance in out-of-domain... | Zhengyuan Liu, Yong Keong Yap, Hai Leong Chieu, Nancy F. Chen |  |
| 1464 |  |  [Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast](https://doi.org/10.18653/v1/2023.acl-long.221) |  | 0 | Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall... | Yiduo Guo, Yaobo Liang, Dongyan Zhao, Bing Liu, Nan Duan |  |
| 1465 |  |  [Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning](https://doi.org/10.18653/v1/2023.acl-long.222) |  | 0 | In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly... | Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Chunyan Miao |  |
| 1466 |  |  [MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks](https://doi.org/10.18653/v1/2023.acl-long.223) |  | 0 | Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we... | Letitia Parcalabescu, Anette Frank |  |
| 1467 |  |  [Towards Boosting the Open-Domain Chatbot with Human Feedback](https://doi.org/10.18653/v1/2023.acl-long.224) |  | 0 | Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses. This phenomenon might mainly result from the deficiency of annotated human-human conversations and the misalignment with human preference. In this paper, we propose a novel and efficient framework Diamante to boost the open-domain chatbot, where two kinds of human feedback (including explicit demonstration and implicit preference) are... | Hua Lu, Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang |  |
| 1468 |  |  [Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations](https://doi.org/10.18653/v1/2023.acl-long.225) |  | 0 | Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that... | Yang Deng, Wenxuan Zhang, Yifei Yuan, Wai Lam |  |
| 1469 |  |  [UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.226) |  | 0 | Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into token-pair classification tasks by using the start and end token of a span to pinpoint... | Hang Yan, Yu Sun, Xiaonan Li, Yunhua Zhou, Xuanjing Huang, Xipeng Qiu |  |
| 1470 |  |  [Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model](https://doi.org/10.18653/v1/2023.acl-long.227) |  | 0 | Existing bias mitigation methods require social-group-specific word pairs (e.g., “man” – “woman”) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) — a theoretical framework developed in social psychology for understanding the content of... | Ali Omrani, Alireza Salkhordeh Ziabari, Charles Yu, Preni Golazizian, Brendan Kennedy, Mohammad Atari, Heng Ji, Morteza Dehghani |  |
| 1471 |  |  [Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation](https://doi.org/10.18653/v1/2023.acl-long.228) |  | 0 | Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units... | Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, ChienSheng Wu, Caiming Xiong, Dragomir Radev |  |
| 1472 |  |  [FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information](https://doi.org/10.18653/v1/2023.acl-long.229) |  | 0 | Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing... | Andrew Zhu, Karmanya Aggarwal, Alexander H. Feng, Lara J. Martin, Chris CallisonBurch |  |
| 1473 |  |  [A fine-grained comparison of pragmatic language understanding in humans and language models](https://doi.org/10.18653/v1/2023.acl-long.230) |  | 0 | Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans... | Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson |  |
| 1474 |  |  [Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning](https://doi.org/10.18653/v1/2023.acl-long.231) |  | 0 | Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred as disconnected reasoning problem. To alleviate this issue, we propose a novel counterfactual multihop QA, a causal-effect approach that enables to reduce the disconnected reasoning. It builds upon explicitly modeling of causality: 1) the direct causal... | Wangzhen Guo, Qinkang Gong, Yanghui Rao, Hanjiang Lai |  |
| 1475 |  |  [Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning](https://doi.org/10.18653/v1/2023.acl-long.232) |  | 0 | Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the... | Fan Zhou, Yuzhou Mao, Liu Yu, Yi Yang, Ting Zhong |  |
| 1476 |  |  [Parameter-Efficient Fine-Tuning without Introducing New Latency](https://doi.org/10.18653/v1/2023.acl-long.233) |  | 0 | Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the... | Baohao Liao, Yan Meng, Christof Monz |  |
| 1477 |  |  [MANNER: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.234) |  | 0 | This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task... | Jinyuan Fang, Xiaobin Wang, Zaiqiao Meng, Pengjun Xie, Fei Huang, Yong Jiang |  |
| 1478 |  |  [MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages](https://doi.org/10.18653/v1/2023.acl-long.235) |  | 0 | We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5,... | Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gökhan Tür, Prem Natarajan |  |
| 1479 |  |  [Distilling Script Knowledge from Large Language Models for Constrained Language Planning](https://doi.org/10.18653/v1/2023.acl-long.236) |  | 0 | In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter... | Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang |  |
| 1480 |  |  [REDFM: a Filtered and Multilingual Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.237) |  | 0 | Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English.In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE... | PereLluís Huguet Cabot, Simone Tedeschi, AxelCyrille Ngonga Ngomo, Roberto Navigli |  |
| 1481 |  |  [Modeling Appropriate Language in Argumentation](https://doi.org/10.18653/v1/2023.acl-long.238) |  | 0 | Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the... | Timon Ziegenbein, Shahbaz Syed, Felix Lange, Martin Potthast, Henning Wachsmuth |  |
| 1482 |  |  [CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels](https://doi.org/10.18653/v1/2023.acl-long.239) |  | 0 | Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle... | Hyunsoo Cho, Youna Kim, Sanggoo Lee |  |
| 1483 |  |  [MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction](https://doi.org/10.18653/v1/2023.acl-long.240) |  | 0 | Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the... | Zhibin Gou, Qingyan Guo, Yujiu Yang |  |
| 1484 |  |  [ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.241) |  | 0 | Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first... | Sarik Ghazarian, Yijia Shao, Rujun Han, Aram Galstyan, Nanyun Peng |  |
| 1485 |  |  [Explanation-based Finetuning Makes Models More Robust to Spurious Cues](https://doi.org/10.18653/v1/2023.acl-long.242) |  | 0 | Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs’ reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer.... | Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, Chris CallisonBurch |  |
| 1486 |  |  [CAME: Confidence-guided Adaptive Memory Efficient Optimization](https://doi.org/10.18653/v1/2023.acl-long.243) |  | 0 | Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper,... | Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You |  |
| 1487 |  |  [On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://doi.org/10.18653/v1/2023.acl-long.244) |  | 0 | Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and... | Omar Shaikh, Hongxin Zhang, William Held, Michael S. Bernstein, Diyi Yang |  |
| 1488 |  |  [Solving Math Word Problems via Cooperative Reasoning induced Language Models](https://doi.org/10.18653/v1/2023.acl-long.245) |  | 0 | Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system... | Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang |  |
| 1489 |  |  [Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model](https://doi.org/10.18653/v1/2023.acl-long.246) |  | 0 | Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data through linguistic rules. However, this approach is not practical for languages with more complex morphology than English. We hypothesise that creating training data in the reverse direction, i.e.... | Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Läubli |  |
| 1490 |  |  [Early Discovery of Disappearing Entities in Microblogs](https://doi.org/10.18653/v1/2023.acl-long.247) |  | 0 | We make decisions by reacting to changes in the real world, particularly the emergence and disappearance of impermanent entities such as restaurants, services, and events. Because we want to avoid missing out on opportunities or making fruitless actions after those entities have disappeared, it is important to know when entities disappear as early as possible. We thus tackle the task of detecting disappearing entities from microblogs where various information is shared timely. The major... | Satoshi Akasaki, Naoki Yoshinaga, Masashi Toyoda |  |
| 1491 |  |  [DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](https://doi.org/10.18653/v1/2023.acl-long.248) |  | 0 | We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used... | Zhengfu He, Tianxiang Sun, Qiong Tang, Kuanning Wang, Xuanjing Huang, Xipeng Qiu |  |
| 1492 |  |  [Lifting the Curse of Capacity Gap in Distilling Language Models](https://doi.org/10.18653/v1/2023.acl-long.249) |  | 0 | Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet... | Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, Dawei Song |  |
| 1493 |  |  [Towards Faithful Dialogues via Focus Learning](https://doi.org/10.18653/v1/2023.acl-long.250) |  | 0 | Maintaining faithfulness between responses and knowledge is an important research topic for building reliable knowledge-grounded dialogue systems. Existing models heavily rely on elaborate data engineering or increasing the model’s parameters ignoring to track the tokens that significantly influence losses, which is decisive for the optimization direction of the model in each iteration. To address this issue, we propose Focus Learning (FocusL), a novel learning approach that adjusts the... | Yifan Deng, Xingsheng Zhang, Heyan Huang, Yue Hu |  |
| 1494 |  |  [Back Translation for Speech-to-text Translation Without Transcripts](https://doi.org/10.18653/v1/2023.acl-long.251) |  | 0 | The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the... | Qingkai Fang, Yang Feng |  |
| 1495 |  |  [Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation](https://doi.org/10.18653/v1/2023.acl-long.252) |  | 0 | A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the... | Ibrahim Taha Aksu, MinYen Kan, Nancy F. Chen |  |
| 1496 |  |  [Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation](https://doi.org/10.18653/v1/2023.acl-long.253) |  | 0 | Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced... | Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin, Frank Guerin |  |
| 1497 |  |  [Multi-modal Action Chain Abductive Reasoning](https://doi.org/10.18653/v1/2023.acl-long.254) |  | 0 | Abductive Reasoning, has long been considered to be at the core ability of humans, which enables us to infer the most plausible explanation of incomplete known phenomena in daily life. However, such critical reasoning capability is rarely investigated for contemporary AI systems under such limited observations. To facilitate this research community, this paper sheds new light on Abductive Reasoning by studying a new vision-language task, Multi-modal Action chain abductive Reasoning (MAR),... | Mengze Li, Tianbao Wang, Jiahe Xu, Kairong Han, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang, Shiliang Pu, Fei Wu |  |
| 1498 |  |  [Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change](https://doi.org/10.18653/v1/2023.acl-long.255) |  | 0 | Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs... | Weinan He, Canming Huang, Zhanhao Xiao, Yongmei Liu |  |
| 1499 |  |  [Unified Demonstration Retriever for In-Context Learning](https://doi.org/10.18653/v1/2023.acl-long.256) |  | 0 | In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for... | Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu |  |
| 1500 |  |  [Movie101: A New Movie Understanding Benchmark](https://doi.org/10.18653/v1/2023.acl-long.257) |  | 0 | To help the visually impaired enjoy movies, automatic movie narrating systems are expected to narrate accurate, coherent, and role-aware plots when there are no speaking lines of actors. Existing works benchmark this challenge as a normal video captioning task via some simplifications, such as removing role names and evaluating narrations with ngram-based metrics, which makes it difficult for automatic systems to meet the needs of real application scenarios. To narrow this gap, we construct a... | Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, Qin Jin |  |
| 1501 |  |  [Enhancing Language Representation with Constructional Information for Natural Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.258) |  | 0 | Natural language understanding (NLU) is an essential branch of natural language processing, which relies on representations generated by pre-trained language models (PLMs). However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions. To address this issue, we introduce construction grammar (CxG), which highlights the pairings of form and meaning, to enrich language representation. We adopt usage-based... | Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai, Tianxiang Wang |  |
| 1502 |  |  [Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.259) |  | 0 | Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for... | Siyuan Wang, Zhongyu Wei, Meng Han, Zhihao Fan, Haijun Shan, Qi Zhang, Xuanjing Huang |  |
| 1503 |  |  [DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships](https://doi.org/10.18653/v1/2023.acl-long.260) |  | 0 | In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that... | Chenzhengyi Liu, Jie Huang, Kerui Zhu, Kevin ChenChuan Chang |  |
| 1504 |  |  [Incorporating Attribution Importance for Improving Faithfulness Metrics](https://doi.org/10.18653/v1/2023.acl-long.261) |  | 0 | Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive... | Zhixue Zhao, Nikolaos Aletras |  |
| 1505 |  |  [Reward Gaming in Conditional Text Generation](https://doi.org/10.18653/v1/2023.acl-long.262) |  | 0 | To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high... | Richard Yuanzhe Pang, Vishakh Padmakumar, Thibault Sellam, Ankur P. Parikh, He He |  |
| 1506 |  |  [Hidden Schema Networks](https://doi.org/10.18653/v1/2023.acl-long.263) |  | 0 | Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased... | Ramsés J. Sánchez, Lukas Conrads, Pascal Welke, Kostadin Cvejoski, César Ojeda Marin |  |
| 1507 |  |  [Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations](https://doi.org/10.18653/v1/2023.acl-long.264) |  | 0 | Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The... | Linlin Liu, Xingxuan Li, Megh Thakkar, Xin Li, Shafiq Joty, Luo Si, Lidong Bing |  |
| 1508 |  |  [An Ordinal Latent Variable Model of Conflict Intensity](https://doi.org/10.18653/v1/2023.acl-long.265) |  | 0 | Measuring the intensity of events is crucial for monitoring and tracking armed conflict. Advances in automated event extraction have yielded massive data sets of “who did what to whom” micro-records that enable data-driven approaches to monitoring conflict. The Goldstein scale is a widely-used expert-based measure that scores events on a conflictual–cooperative scale. It is based only on the action category (“what”) and disregards the subject (“who”) and object (“to whom”) of an event, as well... | Niklas Stoehr, Lucas Torroba Hennigen, Josef Valvoda, Robert West, Ryan Cotterell, Aaron Schein |  |
| 1509 |  |  [Multilingual Conceptual Coverage in Text-to-Image Models](https://doi.org/10.18653/v1/2023.acl-long.266) |  | 0 | We propose “Conceptual Coverage Across Languages” (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess “conceptual coverage” of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under... | Michael Saxon, William Yang Wang |  |
| 1510 |  |  [Pre-Training to Learn in Context](https://doi.org/10.18653/v1/2023.acl-long.267) |  | 0 | In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a... | Yuxian Gu, Li Dong, Furu Wei, Minlie Huang |  |
| 1511 |  |  [Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers](https://doi.org/10.18653/v1/2023.acl-long.268) |  | 0 | In recent years machine translation has become very successful for high-resource language pairs. This has also sparked new interest in research on the automatic translation of low-resource languages, including Indigenous languages. However, the latter are deeply related to the ethnic and cultural groups that speak (or used to speak) them. The data collection, modeling and deploying machine translation systems thus result in new ethical questions that must be addressed. Motivated by this, we... | Manuel Mager, Elisabeth Mager, Katharina Kann, Ngoc Thang Vu |  |
| 1512 |  |  [Revisiting non-English Text Simplification: A Unified Multilingual Benchmark](https://doi.org/10.18653/v1/2023.acl-long.269) |  | 0 | Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This... | Michael J. Ryan, Tarek Naous, Wei Xu |  |
| 1513 |  |  [Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments](https://doi.org/10.18653/v1/2023.acl-long.270) |  | 0 | A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs... | Yu Gu, Xiang Deng, Yu Su |  |
| 1514 |  |  [Privacy-Preserving Domain Adaptation of Semantic Parsers](https://doi.org/10.18653/v1/2023.acl-long.271) |  | 0 | Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this work, we study ways in which realistic user utterances can be generated synthetically, to help increase the linguistic and functional coverage of the system, without compromising the privacy of... | Fatemehsadat Mireshghallah, Yu Su, Tatsunori Hashimoto, Jason Eisner, Richard Shin |  |
| 1515 |  |  [Guide the Many-to-One Assignment: Open Information Extraction via IoU-aware Optimal Transport](https://doi.org/10.18653/v1/2023.acl-long.272) |  | 0 | Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how to assign ground truth labels to the parallelly generated tuple proposals remains under-exploited. The commonly utilized Hungarian algorithm for this procedure is restricted to handling one-to-one... | Kaiwen Wei, Yiran Yang, Li Jin, Xian Sun, Zequn Zhang, Jingyuan Zhang, Xiao Li, Linhao Zhang, Jintao Liu, Zhi Guo |  |
| 1516 |  |  [Actively Supervised Clustering for Open Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.273) |  | 0 | Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline, which simultaneously learns relation representations and assignments in the first stage, then manually labels relation for each cluster. However, unsupervised objectives struggle to explicitly optimize clusters to align with relational semantics, and the number of clusters K has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for... | Jun Zhao, Yongxin Zhang, Qi Zhang, Tao Gui, Zhongyu Wei, Minlong Peng, Mingming Sun |  |
| 1517 |  |  [ConvGQR: Generative Query Reformulation for Conversational Search](https://doi.org/10.18653/v1/2023.acl-long.274) |  | 0 | In conversational search, the user’s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus,... | Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, JianYun Nie |  |
| 1518 |  |  [KILM: Knowledge Injection into Encoder-Decoder Language Models](https://doi.org/10.18653/v1/2023.acl-long.275) |  | 0 | Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of... | Yan Xu, Mahdi Namazifar, Devamanyu Hazarika, Aishwarya Padmakumar, Yang Liu, Dilek HakkaniTür |  |
| 1519 |  |  [VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions](https://doi.org/10.18653/v1/2023.acl-long.276) |  | 0 | Video-grounded dialogue understanding is a challenging problem that requires machine to perceive, parse and reason over situated semantics extracted from weakly aligned video and dialogues. Most existing benchmarks treat both modalities the same as a frame-independent visual understanding task, while neglecting the intrinsic attributes in multimodal dialogues, such as scene and topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe dialogue (VSTAR) dataset, a large scale... | Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, Dongyan Zhao |  |
| 1520 |  |  [NLPeer: A Unified Resource for the Computational Study of Peer Review](https://doi.org/10.18653/v1/2023.acl-long.277) |  | 0 | Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer– the first ethically sourced multidomain corpus of more than 5k papers and... | Nils Dycke, Ilia Kuznetsov, Iryna Gurevych |  |
| 1521 |  |  [IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures](https://doi.org/10.18653/v1/2023.acl-long.278) |  | 0 | Various datasets have been proposed to promote the development of Table Question Answering (TQA) technique. However, the problem setting of existing TQA benchmarks suffers from two limitations. First, they directly provide models with explicit table structures where row headers and column headers of the table are explicitly annotated and treated as model input during inference. Second, they only consider tables of limited types and ignore other tables especially complex tables with flexible... | Mingyu Zheng, Yang Hao, Wenbin Jiang, Zheng Lin, Yajuan Lyu, Qiaoqiao She, Weiping Wang |  |
| 1522 |  |  [Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.279) |  | 0 | This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model’s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace... | Pengcheng He, Baolin Peng, Song Wang, Yang Liu, Ruochen Xu, Hany Hassan, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, Xuedong Huang |  |
| 1523 |  |  [Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories](https://doi.org/10.18653/v1/2023.acl-long.280) |  | 0 | Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture... | Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, Tong Zhang |  |
| 1524 |  |  [Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model](https://doi.org/10.18653/v1/2023.acl-long.281) |  | 0 | Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language... | Yi Xu, Shuqian Sheng, Jiexing Qi, Luoyi Fu, Zhouhan Lin, Xinbing Wang, Chenghu Zhou |  |
| 1525 |  |  [Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications](https://doi.org/10.18653/v1/2023.acl-long.282) |  | 0 | Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a... | Han Cheol Moon, Shafiq R. Joty, Ruochen Zhao, Megh Thakkar, Chi Xu |  |
| 1526 |  |  [SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes](https://doi.org/10.18653/v1/2023.acl-long.283) |  | 0 | Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. We... | Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, William Yang Wang |  |
| 1527 |  |  [Tokenization and the Noiseless Channel](https://doi.org/10.18653/v1/2023.acl-long.284) |  | 0 | Subword tokenization is a key part of most NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.... | Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan, Ryan Cotterell |  |
| 1528 |  |  [Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers](https://doi.org/10.18653/v1/2023.acl-long.285) |  | 0 | Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of perturbations motivated by the... | Jiaxi Li, Wei Lu |  |
| 1529 |  |  [MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning](https://doi.org/10.18653/v1/2023.acl-long.286) |  | 0 | With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose... | Zhenrui Yue, Huimin Zeng, Yang Zhang, Lanyu Shang, Dong Wang |  |
| 1530 |  |  [Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection](https://doi.org/10.18653/v1/2023.acl-long.287) |  | 0 | With the popularity of social media, detecting sentiment from multimodal posts (e.g. image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with inherent disparities may bring three problems: 1) introducing redundant visual features during feature fusion; 2) causing feature shift in the representation space; 3) leading to inconsistent... | Yiwei Wei, Shaozu Yuan, Ruosong Yang, Lei Shen, Zhangmeizhi Li, Longbiao Wang, Meng Chen |  |
| 1531 |  |  [COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective](https://doi.org/10.18653/v1/2023.acl-long.288) |  | 0 | Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between... | Zhaowei Wang, Quyet V. Do, Hongming Zhang, Jiayao Zhang, Weiqi Wang, Tianqing Fang, Yangqiu Song, Ginny Y. Wong, Simon See |  |
| 1532 |  |  [MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization](https://doi.org/10.18653/v1/2023.acl-long.289) |  | 0 | Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme’s context... | Shivam Sharma, Ramaneswaran S., Udit Arora, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 1533 |  |  [WikiHowQA: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering](https://doi.org/10.18653/v1/2023.acl-long.290) |  | 0 | Answering non-factoid questions (NFQA) is a challenging task, requiring passage-level answers that are difficult to construct and evaluate. Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents. Meanwhile, modern models can generate highly coherent and fluent, but often factually incorrect answers that can deceive even non-expert humans. There is a critical need for high-quality resources for multi-document NFQA (MD-NFQA) to... | Valeria BolotovaBaranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, Mark Sanderson |  |
| 1534 |  |  [Making Language Models Better Reasoners with Step-Aware Verifier](https://doi.org/10.18653/v1/2023.acl-long.291) |  | 0 | Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant... | Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, JianGuang Lou, Weizhu Chen |  |
| 1535 |  |  [Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations](https://doi.org/10.18653/v1/2023.acl-long.292) |  | 0 | Discourse analysis is an important task because it models intrinsic semantic structures between sentences in a document. Discourse markers are natural representations of discourse in our daily language. One challenge is that the markers as well as pre-defined and human-labeled discourse relations can be ambiguous when describing the semantics between sentences. We believe that a better approach is to use a contextual-dependent distribution over the markers to express discourse information. In... | Dongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang, Zheng Zhang |  |
| 1536 |  |  [MISGENDERED: Limits of Large Language Models in Understanding Pronouns](https://doi.org/10.18653/v1/2023.acl-long.293) |  | 0 | Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use... | Tamanna Hossain, Sunipa Dev, Sameer Singh |  |
| 1537 |  |  [Reasoning with Language Model Prompting: A Survey](https://doi.org/10.18653/v1/2023.acl-long.294) |  | 0 | Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future... | Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen |  |
| 1538 |  |  [Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation](https://doi.org/10.18653/v1/2023.acl-long.295) |  | 0 | One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural... | Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoît Sagot, Rachel Bawden |  |
| 1539 |  |  [Hybrid Knowledge Transfer for Improved Cross-Lingual Event Detection via Hierarchical Sample Selection](https://doi.org/10.18653/v1/2023.acl-long.296) |  | 0 | In this paper, we address the Event Detection task under a zero-shot cross-lingual setting where a model is trained on a source language but evaluated on a distinct target language for which there is no labeled data available. Most recent efforts in this field follow a direct transfer approach in which the model is trained using language-invariant features and then directly applied to the target language. However, we argue that these methods fail to take advantage of the benefits of the data... | Luis GuzmanNateras, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1540 |  |  [BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training](https://doi.org/10.18653/v1/2023.acl-long.297) |  | 0 | Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and... | Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, Mingxuan Wang |  |
| 1541 |  |  [Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment](https://doi.org/10.18653/v1/2023.acl-long.298) |  | 0 | Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., ‘mug in grass’) with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show... | Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 1542 |  |  [Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona](https://doi.org/10.18653/v1/2023.acl-long.299) |  | 0 | The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we... | Yihong Tang, Bo Wang, Miao Fang, Dongming Zhao, Kun Huang, Ruifang He, Yuexian Hou |  |
| 1543 |  |  [Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge](https://doi.org/10.18653/v1/2023.acl-long.300) |  | 0 | Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs’ abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that... | Yasumasa Onoe, Michael J. Q. Zhang, Shankar Padmanabhan, Greg Durrett, Eunsol Choi |  |
| 1544 |  |  [Explaining How Transformers Use Context to Build Predictions](https://doi.org/10.18653/v1/2023.acl-long.301) |  | 0 | Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model’s prediction, it is still unclear how prior words affect the model’s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the... | Javier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, Marta R. Costajussà |  |
| 1545 |  |  [DISCO: Distilling Counterfactuals with Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.302) |  | 0 | Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data),... | Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson |  |
| 1546 |  |  [Non-Sequential Graph Script Induction via Multimedia Grounding](https://doi.org/10.18653/v1/2023.acl-long.303) |  | 0 | Online resources such as WikiHow compile a wide range of scripts for performing everyday tasks, which can assist models in learning to reason about procedures. However, the scripts are always presented in a linear manner, which does not reflect the flexibility displayed by people executing tasks in real life. For example, in the CrossTask Dataset, 64.5% of consecutive step pairs are also observed in the reverse order, suggesting their ordering is not fixed. In addition, each step has an average... | Yu Zhou, Sha Li, Manling Li, Xudong Lin, ShihFu Chang, Mohit Bansal, Heng Ji |  |
| 1547 |  |  [SCOTT: Self-Consistent Chain-of-Thought Distillation](https://doi.org/10.18653/v1/2023.acl-long.304) |  | 0 | Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge... | Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, Xiang Ren |  |
| 1548 |  |  [Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.305) |  | 0 | Leveraging knowledge from electronic health records (EHRs) to predict a patient’s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize... | Nayeon Kim, Yinhua Piao, Sun Kim |  |
| 1549 |  |  [Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.306) |  | 0 | For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the ‘RSTformer’, a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our... | Dongqi Liu, Yifan Wang, Vera Demberg |  |
| 1550 |  |  [Evaluating Open-Domain Question Answering in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.307) |  | 0 | Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more... | Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei |  |
| 1551 |  |  [No clues good clues: out of context Lexical Relation Classification](https://doi.org/10.18653/v1/2023.acl-long.308) |  | 0 | The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs “well-formed” verbalized text to interact with it, either to fine-tune it or to exploit it. However, there are indications that commonly used PTLMs already encode enough linguistic knowledge to allow the use of minimal (or none) textual context for some... | Lucia Pitarch, Jordi Bernad, Lacramioara Dranca, Carlos Bobed Lisbona, Jorge Gracia |  |
| 1552 |  |  [Won't Get Fooled Again: Answering Questions with False Premises](https://doi.org/10.18653/v1/2023.acl-long.309) |  | 0 | Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them. In this paper, we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge. To systematize this... | Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, Maosong Sun |  |
| 1553 |  |  [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://doi.org/10.18653/v1/2023.acl-long.310) |  | 0 | Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech,... | Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, Ferhan Ture |  |
| 1554 |  |  [Zero-shot Faithful Factual Error Correction](https://doi.org/10.18653/v1/2023.acl-long.311) |  | 0 | Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans’ ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms... | KungHsiang Huang, Hou Pong Chan, Heng Ji |  |
| 1555 |  |  [Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification](https://doi.org/10.18653/v1/2023.acl-long.312) |  | 0 | Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and... | Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris CallisonBurch, Jiawei Han |  |
| 1556 |  |  [Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts](https://doi.org/10.18653/v1/2023.acl-long.313) |  | 0 | Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a... | Mohna Chakraborty, Adithya Kulkarni, Qi Li |  |
| 1557 |  |  [Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging](https://doi.org/10.18653/v1/2023.acl-long.314) |  | 0 | Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target... | Fabian David Schmidt, Ivan Vulic, Goran Glavas |  |
| 1558 |  |  [Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training](https://doi.org/10.18653/v1/2023.acl-long.315) |  | 0 | In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e.,... | Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng, Xinsong Zhang |  |
| 1559 |  |  [Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars](https://doi.org/10.18653/v1/2023.acl-long.316) |  | 0 | We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To reduce the computational complexity of both parsing and parameter estimation, we restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two) and further discard rules that require... | Songlin Yang, Roger Levy, Yoon Kim |  |
| 1560 |  |  [Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions](https://doi.org/10.18653/v1/2023.acl-long.317) |  | 0 | Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased... | Satwik Bhattamishra, Arkil Patel, Varun Kanade, Phil Blunsom |  |
| 1561 |  |  [Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation](https://doi.org/10.18653/v1/2023.acl-long.318) |  | 0 | Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore... | Rishabh Gupta, Shaily Desai, Manvi Goel, Anil Bandhakavi, Tanmoy Chakraborty, Md. Shad Akhtar |  |
| 1562 |  |  [DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation](https://doi.org/10.18653/v1/2023.acl-long.319) |  | 0 | State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring large amounts of labeled speech for specific target accents is challenging. Choosing an informative subset of speech samples that are most representative of the target accents becomes important for... | Suraj Kothawade, Anmol Reddy Mekala, D. Chandra Sekhara Hetha Havya, Mayank Kothyari, Rishabh K. Iyer, Ganesh Ramakrishnan, Preethi Jyothi |  |
| 1563 |  |  [Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework](https://doi.org/10.18653/v1/2023.acl-long.320) |  | 0 | As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from... | Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, Lidong Bing |  |
| 1564 |  |  [Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.321) |  | 0 | k-Nearest neighbor machine translation (kNN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation.However, there often exists a significant gap between upstream and downstream domains, which hurts the datastore retrieval and the... | Zhiwei Cao, Baosong Yang, Huan Lin, Suhang Wu, Xiangpeng Wei, Dayiheng Liu, Jun Xie, Min Zhang, Jinsong Su |  |
| 1565 |  |  [Node Placement in Argument Maps: Modeling Unidirectional Relations in High & Low-Resource Scenarios](https://doi.org/10.18653/v1/2023.acl-long.322) |  | 0 | Argument maps structure discourse into nodes in a tree with each node being an argument that supports or opposes its parent argument. This format is more comprehensible and less redundant compared to an unstructured one. Exploring those maps and maintaining their structure by placing new arguments under suitable parents is more challenging for users with huge maps that are typical in online discussions. To support those users, we introduce the task of node placement: suggesting candidate nodes... | Iman Jundi, Neele Falk, Eva Maria Vecchi, Gabriella Lapesa |  |
| 1566 |  |  [Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review](https://doi.org/10.18653/v1/2023.acl-long.323) |  | 0 | In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform... | Fred Philippy, Siwen Guo, Shohreh Haddadan |  |
| 1567 |  |  [Toward Human-Like Evaluation for Natural Language Generation with Error Analysis](https://doi.org/10.18653/v1/2023.acl-long.324) |  | 0 | The pretrained language model (PLM) based metrics have been successfully used in evaluating language generation tasks. Recent studies of the human evaluation community show that considering both major errors (e.g. mistranslated tokens) and minor errors (e.g. imperfections in fluency) can produce high-quality judgments. This inspires us to approach the final goal of the automatic metrics (human-like evaluations) by fine-grained error analysis. In this paper, we argue that the ability to estimate... | Qingyu Lu, Liang Ding, Liping Xie, Kanjian Zhang, Derek F. Wong, Dacheng Tao |  |
| 1568 |  |  [Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.325) |  | 0 | Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach... | Hongyi Wu, Hao Zhou, Man Lan, Yuanbin Wu, Yadong Zhang |  |
| 1569 |  |  [What is the best recipe for character-level encoder-only modelling?](https://doi.org/10.18653/v1/2023.acl-long.326) |  | 0 | This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations (Clark et al., 2022, Jaegle et al.,... | Kris Cao |  |
| 1570 |  |  [Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training](https://doi.org/10.18653/v1/2023.acl-long.327) |  | 0 | Multilingual Vision-Language Pre-training (VLP) is a promising but challenging topic due to the lack of large-scale multilingual image-text pairs. Existing works address the problem by translating English data into other languages, which is intuitive and the generated data is usually limited in form and scale. In this paper, we explore a more practical and scalable setting: weakly supervised multilingual VLP with only English image-text pairs and multilingual text corpora. We argue that the... | Zejun Li, Zhihao Fan, Jingjing Chen, Qi Zhang, Xuanjing Huang, Zhongyu Wei |  |
| 1571 |  |  [Learning "O" Helps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER](https://doi.org/10.18653/v1/2023.acl-long.328) |  | 0 | As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are regarded as “Non-entity” (or “O”). In this work, we conduct an... | Ruotian Ma, Xuanting Chen, Zhang Lin, Xin Zhou, Junzhe Wang, Tao Gui, Qi Zhang, Xiang Gao, Yun Wen Chen |  |
| 1572 |  |  [Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination](https://doi.org/10.18653/v1/2023.acl-long.329) |  | 0 | In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene... | Hao Fei, Qian Liu, Meishan Zhang, Min Zhang, TatSeng Chua |  |
| 1573 |  |  [CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.330) |  | 0 | Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge distillation on target-language unlabeled data. However, these methods may suffer from label noise due to the automatic labeling process. In this paper, we propose CoLaDa, a Collaborative Label Denoising... | Tingting Ma, Qianhui Wu, Huiqiang Jiang, Börje Karlsson, Tiejun Zhao, ChinYew Lin |  |
| 1574 |  |  [Dialect-robust Evaluation of Generated Text](https://doi.org/10.18653/v1/2023.acl-long.331) |  | 0 | Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over... | Jiao Sun, Thibault Sellam, Elizabeth Clark, Tu Vu, Timothy Dozat, Dan Garrette, Aditya Siddhant, Jacob Eisenstein, Sebastian Gehrmann |  |
| 1575 |  |  [Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.332) |  | 0 | In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. We identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO, are blind to this issue due to oversimplified constraint settings. To solve it, we create a new challenging test set of English-German, increasing the average constraint count per sentence from... | Huaao Zhang, Qiang Wang, Bo Qin, Zelin Shi, Haibo Wang, Ming Chen |  |
| 1576 |  |  [Language model acceptability judgements are not always robust to context](https://doi.org/10.18653/v1/2023.acl-long.333) |  | 0 | Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models’ syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of... | Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams |  |
| 1577 |  |  [RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations](https://doi.org/10.18653/v1/2023.acl-long.334) |  | 0 | Despite significant progress having been made in question answering on tabular data (Table QA), it’s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in... | Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev |  |
| 1578 |  |  [Morphological Inflection: A Reality Check](https://doi.org/10.18653/v1/2023.acl-long.335) |  | 0 | Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high variability; we find several aspects of data set creation and evaluation which systematically inflate performance and obfuscate differences between languages. To improve generalizability and reliability... | Jordan Kodner, Sarah R. B. Payne, Salam Khalifa, Zoey Liu |  |
| 1579 |  |  [TOME: A Two-stage Approach for Model-based Retrieval](https://doi.org/10.18653/v1/2023.acl-long.336) |  | 0 | Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major... | Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, JiRong Wen, Haifeng Wang |  |
| 1580 |  |  [Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner](https://doi.org/10.18653/v1/2023.acl-long.337) |  | 0 | We propose a novel approach to automatically generate distractors for cloze exercises for English language learners, using round-trip neural machine translation. A carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence with its round-trip translation. We make use of 16 linguistically-diverse pivots and generate hundreds of translation hypotheses in each direction. We show that using hundreds of... | Frank Palma Gomez, Subhadarshi Panda, Michael Flor, Alla Rozovskaya |  |
| 1581 |  |  [Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks](https://doi.org/10.18653/v1/2023.acl-long.338) |  | 0 | Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction... | Moritz Plenz, Juri Opitz, Philipp Heinisch, Philipp Cimiano, Anette Frank |  |
| 1582 |  |  [miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.339) |  | 0 | This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the... | Tassilo Klein, Moin Nabi |  |
| 1583 |  |  [Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency](https://doi.org/10.18653/v1/2023.acl-long.340) |  | 0 | The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the... | Mandar Sharma, Nikhil Muralidhar, Naren Ramakrishnan |  |
| 1584 |  |  [Forgotten Knowledge: Examining the Citational Amnesia in NLP](https://doi.org/10.18653/v1/2023.acl-long.341) |  | 0 | Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. This work systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational... | Janvijay Singh, Mukund Rungta, Diyi Yang, Saif M. Mohammad |  |
| 1585 |  |  [Measuring the Instability of Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-long.342) |  | 0 | Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most of these studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability of different... | Yupei Du, Dong Nguyen |  |
| 1586 |  |  [FairPrism: Evaluating Fairness-Related Harms in Text Generation](https://doi.org/10.18653/v1/2023.acl-long.343) |  | 0 | It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer... | Eve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin Blodgett, Hal Daumé III, Alexandra Olteanu, Emily Sheng, Dan Vann, Hanna M. Wallach |  |
| 1587 |  |  [Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback](https://doi.org/10.18653/v1/2023.acl-long.344) |  | 0 | Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free,... | Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Léonard Hussenot, Orgad Keller, Nikola Momchev, Sabela Ramos Garea, Piotr Stanczyk, Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan Hassidim, Olivier Pietquin, Idan Szpektor |  |
| 1588 |  |  [SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams](https://doi.org/10.18653/v1/2023.acl-long.345) |  | 0 | Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities:(1) spatial and temporal understanding of the situated and real-time user scenes,(2) capability of grounding the actively perceived visuals of users to conversation contexts,and (3) conversational reasoning over past utterances to perform just-in-time assistance. However, we currently lack a large-scale benchmark that captures user–assistant... | TeLin Wu, Satwik Kottur, Andrea Madotto, Mahmoud Azab, Pedro Rodríguez, Babak Damavandi, Nanyun Peng, Seungwhan Moon |  |
| 1589 |  |  [Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment](https://doi.org/10.18653/v1/2023.acl-long.346) |  | 0 | In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is... | Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, Tanmoy Chakraborty |  |
| 1590 |  |  [APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.347) |  | 0 | Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining... | Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren |  |
| 1591 |  |  [MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering](https://doi.org/10.18653/v1/2023.acl-long.348) |  | 0 | Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates... | Vaishali Pal, Andrew Yates, Evangelos Kanoulas, Maarten de Rijke |  |
| 1592 |  |  [To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.349) |  | 0 | Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate... | Rui Li, Xu Chen, Chaozhuo Li, Yanming Shen, Jianan Zhao, Yujing Wang, Weihao Han, Hao Sun, Weiwei Deng, Qi Zhang, Xing Xie |  |
| 1593 |  |  [CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation](https://doi.org/10.18653/v1/2023.acl-long.350) |  | 0 | Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused... | Huimin Wang, WaiChung Kwan, KamFai Wong, Yefeng Zheng |  |
| 1594 |  |  [Long-Tailed Question Answering in an Open World](https://doi.org/10.18653/v1/2023.acl-long.351) |  | 0 | Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We... | Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Yongbin Li |  |
| 1595 |  |  [Parallel Context Windows for Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.352) |  | 0 | When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (“windows”), restrict the attention... | Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin LeytonBrown, Yoav Shoham |  |
| 1596 |  |  [Efficient Transformers with Dynamic Token Pooling](https://doi.org/10.18653/v1/2023.acl-long.353) |  | 0 | Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We... | Piotr Nawrot, Jan Chorowski, Adrian Lancucki, Edoardo Maria Ponti |  |
| 1597 |  |  [Did the Models Understand Documents? Benchmarking Models for Language Understanding in Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.354) |  | 0 | Document-level relation extraction (DocRE) attracts more research interest recently. While models achieve consistent performance gains in DocRE, their underlying decision rules are still understudied: Do they make the right predictions according to rationales? In this paper, we take the first step toward answering this question and then introduce a new perspective on comprehensively evaluating a model. Specifically, we first conduct annotations to provide the rationales considered by humans in... | Haotian Chen, Bingsheng Chen, Xiangdong Zhou |  |
| 1598 |  |  [ContraCLM: Contrastive Learning For Causal Language Model](https://doi.org/10.18653/v1/2023.acl-long.355) |  | 0 | Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better... | Nihal Jain, Dejiao Zhang, Wasi Uddin Ahmad, Zijian Wang, Feng Nan, Xiaopeng Li, Ming Tan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Xiaofei Ma, Bing Xiang |  |
| 1599 |  |  [Advancing Multi-Criteria Chinese Word Segmentation Through Criterion Classification and Denoising](https://doi.org/10.18653/v1/2023.acl-long.356) |  | 0 | Recent research on multi-criteria Chinese word segmentation (MCCWS) mainly focuses on building complex private structures, adding more handcrafted features, or introducing complex optimization processes. In this work, we show that through a simple yet elegant input-hint-based MCCWS model, we can achieve state-of-the-art (SoTA) performances on several datasets simultaneously. We further propose a novel criterion-denoising objective that hurts slightly on F1 score but achieves SoTA recall on... | TzuHsuan Chou, ChunYi Lin, HungYu Kao |  |
| 1600 |  |  [Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.acl-long.357) |  | 0 | Multi-level implicit discourse relation recognition (MIDRR) aims at identifying hierarchical discourse relations among arguments. Previous methods achieve the promotion through fine-tuning PLMs. However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately tuned to the task-specific space, which even aggravates the collapse of the vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR makes the conversion much harder. In this... | Haodong Zhao, Ruifang He, Mengnan Xiao, Jing Xu |  |
| 1601 |  |  [Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model](https://doi.org/10.18653/v1/2023.acl-long.358) |  | 0 | Neural language models have achieved superior performance. However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions. In this paper, we explain the model pathology from the view of sentence representation and argue that the counter-intuitive bias degree and direction of the out-of-distribution examples’ representation... | Pengwei Zhan, Jing Yang, Xiao Huang, Chunlei Jing, Jingying Li, Liming Wang |  |
| 1602 |  |  [Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales](https://doi.org/10.18653/v1/2023.acl-long.359) |  | 0 | Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing (NLP) methods that examine social bias in models and data corpora. Our work joins this interdisciplinary effort... | Paulina Toro Isaza, Guangxuan Xu, Toye Oloko, Yufang Hou, Nanyun Peng, Dakuo Wang |  |
| 1603 |  |  [FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.acl-long.360) |  | 0 | Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills... | Weihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jingang Wang, Yunsen Xian, Weiran Xu |  |
| 1604 |  |  [LAMBADA: Backward Chaining for Automated Reasoning in Natural Language](https://doi.org/10.18653/v1/2023.acl-long.361) |  | 0 | Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the... | Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran |  |
| 1605 |  |  [PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives](https://doi.org/10.18653/v1/2023.acl-long.362) |  | 0 | Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understandhow the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story. However, personas are diverse and... | Silin Gao, Beatriz Borges, Soyoung Oh, Deniz Bayazit, Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, Antoine Bosselut |  |
| 1606 |  |  [OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment](https://doi.org/10.18653/v1/2023.acl-long.363) |  | 0 | Speech Recognition builds a bridge between the multimedia streaming (audio-only, visual-only or audio-visual) and the corresponding text transcription. However, when training the specific model of new domain, it often gets stuck in the lack of new-domain utterances, especially the labeled visual utterances. To break through this restriction, we attempt to achieve zero-shot modality transfer by maintaining the multi-modality alignment in phoneme space learned with unlabeled multimedia utterances... | Xize Cheng, Tao Jin, Linjun Li, Wang Lin, Xinyu Duan, Zhou Zhao |  |
| 1607 |  |  [Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models](https://doi.org/10.18653/v1/2023.acl-long.364) |  | 0 | Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline. Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it costly to transfer. To tackle this challenge, this paper proposed a retrieval-free approach, KiDG, by automatically turning knowledge documents into simulated multi-turn dialogues through a Multi-Document... | Rui Wang, Jianzhu Bao, Fei Mi, Yi Chen, Hongru Wang, Yasheng Wang, Yitong Li, Lifeng Shang, KamFai Wong, Ruifeng Xu |  |
| 1608 |  |  [BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval](https://doi.org/10.18653/v1/2023.acl-long.365) |  | 0 | Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal... | Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng |  |
| 1609 |  |  [Multiview Identifiers Enhanced Generative Retrieval](https://doi.org/10.18653/v1/2023.acl-long.366) |  | 0 | Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage’s content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are... | Yongqi Li, Nan Yang, Liang Wang, Furu Wei, Wenjie Li |  |
| 1610 |  |  [Prompting Language Models for Linguistic Structure](https://doi.org/10.18653/v1/2023.acl-long.367) |  | 0 | Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity... | Terra Blevins, Hila Gonen, Luke Zettlemoyer |  |
| 1611 |  |  [Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis](https://doi.org/10.18653/v1/2023.acl-long.368) |  | 0 | Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model... | Agam Shah, Suvan Paturi, Sudheer Chava |  |
| 1612 |  |  [RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.369) |  | 0 | Semantic matching is a mainstream paradigm of zero-shot relation extraction, which matches a given input with a corresponding label description. The entities in the input should exactly match their hypernyms in the description, while the irrelevant contexts should be ignored when matching. However, general matching methods lack explicit modeling of the above matching pattern. In this work, we propose a fine-grained semantic matching method tailored for zero-shot relation extraction. Guided by... | Jun Zhao, WenYu Zhan, Xin Zhao, Qi Zhang, Tao Gui, Zhongyu Wei, Junzhe Wang, Minlong Peng, Mingming Sun |  |
| 1613 |  |  [SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration](https://doi.org/10.18653/v1/2023.acl-long.370) |  | 0 | The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable... | Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Meeyoung Cha, Yejin Choi, Byoung Pil Kim, Gunhee Kim, EunJu Lee, Yong Lim, Alice Oh, Sangchul Park, JungWoo Ha |  |
| 1614 |  |  [Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation](https://doi.org/10.18653/v1/2023.acl-long.371) |  | 0 | Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS... | Soyoung Yoon, Sungjoon Park, Gyuwan Kim, Junhee Cho, Kihyo Park, Gyu Tae Kim, Minjoon Seo, Alice Oh |  |
| 1615 |  |  [FLamE: Few-shot Learning from Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-long.372) |  | 0 | Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural... | Yangqiaoyu Zhou, Yiming Zhang, Chenhao Tan |  |
| 1616 |  |  [Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning](https://doi.org/10.18653/v1/2023.acl-long.373) |  | 0 | Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of... | Subhajit Chaudhury, Sarathkrishna Swaminathan, Daiki Kimura, Prithviraj Sen, Keerthiram Murugesan, Rosario UcedaSosa, Michiaki Tatsubori, Achille Fokoue, Pavan Kapanipathi, Asim Munawar, Alexander Gray |  |
| 1617 |  |  [Counterfactual Debiasing for Fact Verification](https://doi.org/10.18653/v1/2023.acl-long.374) |  | 0 | Fact verification aims to automatically judge the veracity of a claim according to several pieces of evidence. Due to the manual construction of datasets, spurious correlations between claim patterns and its veracity (i.e., biases) inevitably exist. Recent studies show that models usually learn such biases instead of understanding the semantic relationship between the claim and evidence. Existing debiasing works can be roughly divided into data-augmentation-based and weight-regularization-based... | Weizhi Xu, Qiang Liu, Shu Wu, Liang Wang |  |
| 1618 |  |  [What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics](https://doi.org/10.18653/v1/2023.acl-long.375) |  | 0 | Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people’s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender... | Julia Watson, Barend Beekhuizen, Suzanne Stevenson |  |
| 1619 |  |  [Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View](https://doi.org/10.18653/v1/2023.acl-long.376) |  | 0 | We revisit the multimodal entity and relation extraction from a translation point of view. Special attention is paid on the misalignment issue in text-image datasets which may mislead the learning. We are motivated by the fact that the cross-modal misalignment is a similar problem of cross-lingual divergence issue in machine translation. The problem can then be transformed and existing solutions can be borrowed by treating a text and its paired image as the translation to each other. We... | Changmeng Zheng, Junhao Feng, Yi Cai, Xiaoyong Wei, Qing Li |  |
| 1620 |  |  [Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.377) |  | 0 | A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and weevaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results,... | Rongxin Zhu, Jianzhong Qi, Jey Han Lau |  |
| 1621 |  |  [Improving the Robustness of Summarization Systems with Dual Augmentation](https://doi.org/10.18653/v1/2023.acl-long.378) |  | 0 | A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise. To create semantic-consistent substitutes, we propose a SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models. Experimental results show that... | Xiuying Chen, Guodong Long, Chongyang Tao, Mingzhe Li, Xin Gao, Chengqi Zhang, Xiangliang Zhang |  |
| 1622 |  |  [Interpretable Math Word Problem Solution Generation via Step-by-step Planning](https://doi.org/10.18653/v1/2023.acl-long.379) |  | 0 | Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of... | Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, Andrew S. Lan |  |
| 1623 |  |  [TemplateGEC: Improving Grammatical Error Correction with Detection Template](https://doi.org/10.18653/v1/2023.acl-long.380) |  | 0 | Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to... | Yinghao Li, Xuebo Liu, Shuo Wang, Peiyuan Gong, Derek F. Wong, Yang Gao, Heyan Huang, Min Zhang |  |
| 1624 |  |  [Deep Model Compression Also Helps Models Capture Ambiguity](https://doi.org/10.18653/v1/2023.acl-long.381) |  | 0 | Natural language understanding (NLU) tasks face a non-trivial amount of ambiguous samples where veracity of their labels is debatable among annotators. NLU models should thus account for such ambiguity, but they approximate the human opinion distributions quite poorly and tend to produce over-confident predictions. To address this problem, we must consider how to exactly capture the degree of relationship between each sample and its candidate classes. In this work, we propose a novel method... | Hancheol Park, Jong C. Park |  |
| 1625 |  |  [Are Experts Needed? On Human Evaluation of Counselling Reflection Generation](https://doi.org/10.18653/v1/2023.acl-long.382) |  | 0 | Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a... | Zixiu Wu, Simone Balloccu, Ehud Reiter, Rim Helaoui, Diego Reforgiato Recupero, Daniele Riboni |  |
| 1626 |  |  [PairSpanBERT: An Enhanced Language Model for Bridging Resolution](https://doi.org/10.18653/v1/2023.acl-long.383) |  | 0 | We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation... | Hideo Kobayashi, Yufang Hou, Vincent Ng |  |
| 1627 |  |  [Compounding Geometric Operations for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.384) |  | 0 | Geometric transformations including translation, rotation, and scaling are commonly used operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE). Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a composite one, the new model is named CompoundE. By casting CompoundE in the framework of group... | Xiou Ge, YunCheng Wang, Bin Wang, C.C. Jay Kuo |  |
| 1628 |  |  [Few-shot In-context Learning on Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.385) |  | 0 | Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first... | Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, Wenhu Chen |  |
| 1629 |  |  [Fact-Checking Complex Claims with Program-Guided Reasoning](https://doi.org/10.18653/v1/2023.acl-long.386) |  | 0 | Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward,... | Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, MinYen Kan, Preslav Nakov |  |
| 1630 |  |  [Patton: Language Model Pretraining on Text-Rich Networks](https://doi.org/10.18653/v1/2023.acl-long.387) |  | 0 | A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various... | Bowen Jin, Wentao Zhang, Yu Zhang, Yu Meng, Xinyang Zhang, Qi Zhu, Jiawei Han |  |
| 1631 |  |  [Soft Language Clustering for Multilingual Model Pre-training](https://doi.org/10.18653/v1/2023.acl-long.388) |  | 0 | Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of... | Jiali Zeng, Yufan Jiang, Yongjing Yin, Yi Jing, Fandong Meng, Binghuai Lin, Yunbo Cao, Jie Zhou |  |
| 1632 |  |  [Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach](https://doi.org/10.18653/v1/2023.acl-long.389) |  | 0 | A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a... | Nidhi Vakil, Hadi Amiri |  |
| 1633 |  |  [When and how to paraphrase for named entity recognition?](https://doi.org/10.18653/v1/2023.acl-long.390) |  | 0 | While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in... | Saket Sharma, Aviral Joshi, Yiyun Zhao, Namrata Mukhija, Hanoz Bhathena, Prateek Singh, Sashank Santhanam |  |
| 1634 |  |  [UniEvent: Unified Generative Model with Multi-Dimensional Prefix for Zero-Shot Event-Relational Reasoning](https://doi.org/10.18653/v1/2023.acl-long.391) |  | 0 | Reasoning about events and their relations attracts surging research efforts since it is regarded as an indispensable ability to fulfill various event-centric or common-sense reasoning tasks. However, these tasks often suffer from limited data availability due to the labor-intensive nature of their annotations. Consequently, recent studies have explored knowledge transfer approaches within a multi-task learning framework to address this challenge. Although such methods have achieved acceptable... | Zhengwei Tao, Zhi Jin, Haiyan Zhao, Chengfeng Dou, Yongqiang Zhao, Tao Shen, Chongyang Tao |  |
| 1635 |  |  [Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales](https://doi.org/10.18653/v1/2023.acl-long.392) |  | 0 | Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing... | Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, Xiang Ren |  |
| 1636 |  |  [Automatic Annotation of Direct Speech in Written French Narratives](https://doi.org/10.18653/v1/2023.acl-long.393) |  | 0 | The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many works exist. Our goal is to create a unified framework to design and evaluate AADS models in French. For this, we consolidated the largest-to-date French narrative dataset annotated with DS per word; we... | Noé Durandard, VietAnh Tran, Gaspard Michel, Elena V. Epure |  |
| 1637 |  |  [Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations](https://doi.org/10.18653/v1/2023.acl-long.394) |  | 0 | Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a... | Hyunjae Kim, Jaehyo Yoo, Seunghyun Yoon, Jaewoo Kang |  |
| 1638 |  |  [Dynamic Transformers Provide a False Sense of Efficiency](https://doi.org/10.18653/v1/2023.acl-long.395) |  | 0 | Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a trade-off between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from early-exiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting... | Yiming Chen, Simin Chen, Zexin Li, Wei Yang, Cong Liu, Robby T. Tan, Haizhou Li |  |
| 1639 |  |  [Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features](https://doi.org/10.18653/v1/2023.acl-long.396) |  | 0 | A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models’ behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at... | Ester Hlavnova, Sebastian Ruder |  |
| 1640 |  |  [Local Byte Fusion for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.397) |  | 0 | Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus may not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes oversegment low-resource languages, leading to a drop in translation performance. An alternative to subword tokenizers is byte-based tokenization, i.e., tokenization into byte sequences using the UTF-8 encoding... | Makesh Narsimhan Sreedhar, Xiangpeng Wan, Yu Cheng, Junjie Hu |  |
| 1641 |  |  [Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation](https://doi.org/10.18653/v1/2023.acl-long.398) |  | 0 | Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale. In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on... | Benjamin Minixhofer, Jonas Pfeiffer, Ivan Vulic |  |
| 1642 |  |  [Multi-target Backdoor Attacks for Code Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-long.399) |  | 0 | Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning... | Yanzhou Li, Shangqing Liu, Kangjie Chen, Xiaofei Xie, Tianwei Zhang, Yang Liu |  |
| 1643 |  |  [Learning Better Masking for Better Language Model Pre-training](https://doi.org/10.18653/v1/2023.acl-long.400) |  | 0 | Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings... | Dongjie Yang, Zhuosheng Zhang, Hai Zhao |  |
| 1644 |  |  [VisText: A Benchmark for Semantically Rich Chart Captioning](https://doi.org/10.18653/v1/2023.acl-long.401) |  | 0 | Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts’ construction,... | Benny J. Tang, Angie W. Boggust, Arvind Satyanarayan |  |
| 1645 |  |  [Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora](https://doi.org/10.18653/v1/2023.acl-long.402) |  | 0 | Grammatical error correction (GEC) is the task of correcting typos, spelling, punctuation and grammatical issues in text. Approaching the problem as a sequence-to-sequence task, we compare the use of a common subword unit vocabulary and byte-level encoding. Initial synthetic training data is created using an error-generating pipeline, and used for finetuning two subword-level models and one byte-level model. Models are then finetuned further on hand-corrected error corpora, including texts... | Svanhvít Lilja Ingólfsdóttir, Petur Orri Ragnarsson, Haukur Páll Jónsson, Haukur Barri Símonarson, Vilhjalmur Thorsteinsson, Vésteinn Snæbjarnarson |  |
| 1646 |  |  [Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text](https://doi.org/10.18653/v1/2023.acl-long.403) |  | 0 | Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach... | Qianhui Wu, Huiqiang Jiang, Haonan Yin, Börje Karlsson, ChinYew Lin |  |
| 1647 |  |  [Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection](https://doi.org/10.18653/v1/2023.acl-long.404) |  | 0 | Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the interest of the natural language processing (NLP) community. Most of the earlier studies failed to justify their findings, necessitating the adoption of interpretable models that can explain the... | Apoorva Singh, Raghav Jain, Prince Jha, Sriparna Saha |  |
| 1648 |  |  [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://doi.org/10.18653/v1/2023.acl-long.405) |  | 0 | Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it... | Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao, Qingwei Lin |  |
| 1649 |  |  [ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models](https://doi.org/10.18653/v1/2023.acl-long.406) |  | 0 | State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme,... | Jonas Belouadi, Steffen Eger |  |
| 1650 |  |  [Envisioning Future from the Past: Hierarchical Duality Learning for Multi-Turn Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.407) |  | 0 | In this paper, we define a widely neglected property in dialogue text, duality, which is a hierarchical property that is reflected in human behaviours in daily conversations: Based on the logic in a conversation (or a sentence), people can infer follow-up utterances (or tokens) based on the previous text, and vice versa. We propose a hierarchical duality learning for dialogue (HDLD) to simulate this human cognitive ability, for generating high quality responses that connect both previous and... | Ang Lv, Jinpeng Li, Shufang Xie, Rui Yan |  |
| 1651 |  |  [DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.408) |  | 0 | Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT)... | Duzhen Zhang, Feilong Chen, Xiuyi Chen |  |
| 1652 |  |  [Consistent Prototype Learning for Few-Shot Continual Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.409) |  | 0 | Few-shot continual relation extraction aims to continually train a model on incrementally few-shot data to learn new relations while avoiding forgetting old ones. However, current memory-based methods are prone to overfitting memory samples, resulting in insufficient activation of old relations and limited ability to handle the confusion of similar classes. In this paper, we design a new N-way-K-shot Continual Relation Extraction (NK-CRE) task and propose a novel few-shot continual relation... | Xiudi Chen, Hui Wu, Xiaodong Shi |  |
| 1653 |  |  [Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.410) |  | 0 | The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the... | Myles Foley, Ambrish Rawat, Taesung Lee, Yufang Hou, Gabriele Picco, Giulio Zizzo |  |
| 1654 |  |  [Large Language Models Meet NL2Code: A Survey](https://doi.org/10.18653/v1/2023.acl-long.411) |  | 0 | The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and... | Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, JianGuang Lou |  |
| 1655 |  |  [When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP](https://doi.org/10.18653/v1/2023.acl-long.412) |  | 0 | Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work – sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works. Previous studies show that MTL performance can be improved by algorithmic tricks. However, what tasks and skills should be included is less well explored. In this work, we conduct a case study in... | Jingwei Ni, Zhijing Jin, Qian Wang, Mrinmaya Sachan, Markus Leippold |  |
| 1656 |  |  [Enhancing Grammatical Error Correction Systems with Explanations](https://doi.org/10.18653/v1/2023.acl-long.413) |  | 0 | Grammatical error correction systems improve written communication by detecting and correcting language mistakes. To help language learners better understand why the GEC system makes a certain correction, the causes of errors (evidence words) and the corresponding error types are two key factors. To enhance GEC systems with explanations, we introduce EXPECT, a large dataset annotated with evidence words and grammatical error types. We propose several baselines and anlysis to understand this... | Yuejiao Fei, Leyang Cui, Sen Yang, Wai Lam, Zhenzhong Lan, Shuming Shi |  |
| 1657 |  |  [Linguistic representations for fewer-shot relation extraction across domains](https://doi.org/10.18653/v1/2023.acl-long.414) |  | 0 | Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation... | Sireesh Gururaja, Ritam Dutt, Tinglong Liao, Carolyn P. Rosé |  |
| 1658 |  |  [DarkBERT: A Language Model for the Dark Side of the Internet](https://doi.org/10.18653/v1/2023.acl-long.415) |  | 0 | Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical... | Youngjin Jin, Eugene Jang, Jian Cui, JinWoo Chung, Yongjae Lee, Seungwon Shin |  |
| 1659 |  |  [MDACE: MIMIC Documents Annotated with Code Evidence](https://doi.org/10.18653/v1/2023.acl-long.416) |  | 0 | We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply predicting a set of final codes for a patient encounter is insufficient as CAC systems are required to provide supporting textual evidence to justify the billing codes. A model able to produce accurate and... | Hua Cheng, Rana Jafari, April Russell, Russell Klopfer, Edmond Lu, Benjamin Striner, Matthew Gormley |  |
| 1660 |  |  [Towards Zero-Shot Multilingual Transfer for Code-Switched Responses](https://doi.org/10.18653/v1/2023.acl-long.417) |  | 0 | Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we... | TingWei Wu, Changsheng Zhao, Ernie Chang, Yangyang Shi, Pierce Chuang, Vikas Chandra, BiingHwang Juang |  |
| 1661 |  |  [One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning](https://doi.org/10.18653/v1/2023.acl-long.418) |  | 0 | Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage. Parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose ProPETL, a novel method that enables efficient sharing of a single prototype PETL network (e.g. adapter, LoRA, and prefix-tuning) across layers and... | Guangtao Zeng, Peiyuan Zhang, Wei Lu |  |
| 1662 |  |  [Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk](https://doi.org/10.18653/v1/2023.acl-long.419) |  | 0 | Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work,... | Jianquan Li, Xiangbo Wu, Xiaokang Liu, Qianqian Xie, Prayag Tiwari, Benyou Wang |  |
| 1663 |  |  [Convergence and Diversity in the Control Hierarchy](https://doi.org/10.18653/v1/2023.acl-long.420) |  | 0 | Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown... | Alexandra Butoi, Ryan Cotterell, David Chiang |  |
| 1664 |  |  [ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.421) |  | 0 | Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent across modalities as well as modality-specific representations. In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and... | Jiuding Yang, Yakun Yu, Di Niu, Weidong Guo, Yu Xu |  |
| 1665 |  |  [Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic](https://doi.org/10.18653/v1/2023.acl-long.422) |  | 0 | Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit... | Connor Pryor, Quan Yuan, Jeremiah Z. Liu, Mehran Kazemi, Deepak Ramachandran, Tania BedraxWeiss, Lise Getoor |  |
| 1666 |  |  [Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark](https://doi.org/10.18653/v1/2023.acl-long.423) |  | 0 | Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of... | Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie |  |
| 1667 |  |  [Answering Ambiguous Questions via Iterative Prompting](https://doi.org/10.18653/v1/2023.acl-long.424) |  | 0 | In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question,one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the... | Weiwei Sun, Hengyi Cai, Hongshen Chen, Pengjie Ren, Zhumin Chen, Maarten de Rijke, Zhaochun Ren |  |
| 1668 |  |  [A Dataset of Argumentative Dialogues on Scientific Papers](https://doi.org/10.18653/v1/2023.acl-long.425) |  | 0 | With recent advances in question-answering models, various datasets have been collected to improve and study the effectiveness of these models on scientific texts. Questions and answers in these datasets explore a scientific paper by seeking factual information from the paper’s content. However, these datasets do not tackle the argumentative content of scientific papers, which is of huge importance in persuasiveness of a scientific discussion. We introduce ArgSciChat, a dataset of 41... | Federico Ruggeri, Mohsen Mesgar, Iryna Gurevych |  |
| 1669 |  |  [Massively Multilingual Lexical Specialization of Multilingual Transformers](https://doi.org/10.18653/v1/2023.acl-long.426) |  | 0 | While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose... | Tommaso Green, Simone Paolo Ponzetto, Goran Glavas |  |
| 1670 |  |  [RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs](https://doi.org/10.18653/v1/2023.acl-long.427) |  | 0 | Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this... | Afra Feyza Akyürek, Ekin Akyürek, Ashwin Kalyan, Peter Clark, Derry Tanti Wijaya, Niket Tandon |  |
| 1671 |  |  [WebIE: Faithful and Robust Information Extraction on the Web](https://doi.org/10.18653/v1/2023.acl-long.428) |  | 0 | Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset... | Chenxi Whitehouse, Clara Vania, Alham Fikri Aji, Christos Christodoulopoulos, Andrea Pierleoni |  |
| 1672 |  |  [NormBank: A Knowledge Bank of Situational Social Norms](https://doi.org/10.18653/v1/2023.acl-long.429) |  | 0 | We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents’ contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the... | Caleb Ziems, Jane DwivediYu, YiChia Wang, Alon Y. Halevy, Diyi Yang |  |
| 1673 |  |  [DIP: Dead code Insertion based Black-box Attack for Programming Language Model](https://doi.org/10.18653/v1/2023.acl-long.430) |  | 0 | Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable.... | CheolWon Na, YunSeok Choi, JeeHyong Lee |  |
| 1674 |  |  [Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks](https://doi.org/10.18653/v1/2023.acl-long.431) |  | 0 | Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document’s coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where we mine different subgraph patterns. We then construct a... | Wei Liu, Xiyan Fu, Michael Strube |  |
| 1675 |  |  [HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.432) |  | 0 | Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowledge. Under such observation, we tend to investigate the feasibility of a memory-friendly model with strong generalization capability that could boost the performance of HTC without prior statistics... | He Zhu, Chong Zhang, Junjie Huang, Junran Wu, Ke Xu |  |
| 1676 |  |  [Contextual Knowledge Learning for Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.433) |  | 0 | Incorporating conversational context and knowledge into dialogue generation models has been essential for improving the quality of the generated responses. The context, comprising utterances from previous dialogue exchanges, is used as a source of content for response generation and as a means of selecting external knowledge. However, to avoid introducing irrelevant content, it is key to enable fine-grained scoring of context and knowledge. In this paper, we present a novel approach to context... | Wen Zheng, Natasa MilicFrayling, Ke Zhou |  |
| 1677 |  |  [Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.434) |  | 0 | Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human’s edits. Translation Suggestion (TS), as an... | Ke Wang, Xin Ge, Jiayi Wang, Yuqi Zhang, Yu Zhao |  |
| 1678 |  |  [Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated Parallel Corpus of Novels](https://doi.org/10.18653/v1/2023.acl-long.435) |  | 0 | Several recent papers claim to have achieved human parity at sentence-level machine translation (MT)—especially between high-resource language pairs. In response, the MT community has, in part, shifted its focus to document-level translation. Translating documents requires a deeper understanding of the structure and meaning of text, which is often captured by various kinds of discourse phenomena such as consistency, coherence, and cohesion. However, this renders conventional sentence-level MT... | Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Mrinmaya Sachan, Ryan Cotterell |  |
| 1679 |  |  [CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.436) |  | 0 | End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment... | Yan Zhou, Qingkai Fang, Yang Feng |  |
| 1680 |  |  [On the Evaluation of Neural Selective Prediction Methods for Natural Language Processing](https://doi.org/10.18653/v1/2023.acl-long.437) |  | 0 | We provide a survey and empirical comparison of the state-of-the-art in neural selective classification for NLP tasks. We also provide a methodological blueprint, including a novel metric called refinement that provides a calibrated evaluation of confidence functions for selective prediction. Finally, we supply documented, open-source code to support the future development of selective prediction techniques. | Zhengyao Gu, Mark Hopkins |  |
| 1681 |  |  [Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment](https://doi.org/10.18653/v1/2023.acl-long.438) |  | 0 | Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text Pre-training for spoken dialog... | Tianshu Yu, Haoyu Gao, TingEn Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li |  |
| 1682 |  |  [Text Style Transfer with Contrastive Transfer Pattern Mining](https://doi.org/10.18653/v1/2023.acl-long.439) |  | 0 | Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transformation can be actually carried out via different hidden transfer patterns. To address this problem, we propose a novel approach, contrastive transfer pattern mining (CTPM), which automatically mines... | Jingxuan Han, Quan Wang, Licheng Zhang, Weidong Chen, Yan Song, Zhendong Mao |  |
| 1683 |  |  [Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning](https://doi.org/10.18653/v1/2023.acl-long.440) |  | 0 | With emerging online topics as a source for numerous new events, detecting unseen / rare event types presents an elusive challenge for existing event detection methods, where only limited data access is provided for training. To address the data scarcity problem in event detection, we propose MetaEvent, a meta learning-based framework for zero- and few-shot event detection. Specifically, we sample training tasks from existing event types and perform meta training to search for optimal... | Zhenrui Yue, Huimin Zeng, Mengfei Lan, Heng Ji, Dong Wang |  |
| 1684 |  |  [Text Style Transfer Back-Translation](https://doi.org/10.18653/v1/2023.acl-long.441) |  | 0 | Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT),... | Daimeng Wei, Zhanglin Wu, Hengchao Shang, Zongyao Li, Minghan Wang, Jiaxin Guo, Xiaoyu Chen, Zhengzhe Yu, Hao Yang |  |
| 1685 |  |  [Generating Visual Spatial Description via Holistic 3D Scene Understanding](https://doi.org/10.18653/v1/2023.acl-long.442) |  | 0 | Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a... | Yu Zhao, Hao Fei, Wei Ji, Jianguo Wei, Meishan Zhang, Min Zhang, TatSeng Chua |  |
| 1686 |  |  [Continual Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.443) |  | 0 | While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on... | Yuanchi Zhang, Peng Li, Maosong Sun, Yang Liu |  |
| 1687 |  |  [Query Refinement Prompts for Closed-Book Long-Form QA](https://doi.org/10.18653/v1/2023.acl-long.444) |  | 0 | Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once – to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require... | Reinald Kim Amplayo, Kellie Webster, Michael Collins, Dipanjan Das, Shashi Narayan |  |
| 1688 |  |  [CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding](https://doi.org/10.18653/v1/2023.acl-long.445) |  | 0 | This paper tackles an emerging and challenging problem of long video temporal grounding (VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG... | Zhijian Hou, Wanjun Zhong, Lei Ji, Difei Gao, Kun Yan, Wing Kwong Chan, ChongWah Ngo, Mike Zheng Shou, Nan Duan |  |
| 1689 |  |  [Few-Shot Document-Level Event Argument Extraction](https://doi.org/10.18653/v1/2023.acl-long.446) |  | 0 | Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production. To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing... | Xianjun Yang, Yujie Lu, Linda R. Petzold |  |
| 1690 |  |  [ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation](https://doi.org/10.18653/v1/2023.acl-long.447) |  | 0 | Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity – the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a... | KuanHao Huang, Varun Iyer, IHung Hsu, Anoop Kumar, KaiWei Chang, Aram Galstyan |  |
| 1691 |  |  [Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.448) |  | 0 | Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla... | Songming Zhang, Yunlong Liang, Shuaibo Wang, Yufeng Chen, Wenjuan Han, Jian Liu, Jinan Xu |  |
| 1692 |  |  [Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering](https://doi.org/10.18653/v1/2023.acl-long.449) |  | 0 | Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the... | Vishwajeet Kumar, Yash Gupta, Saneem A. Chemmengath, Jaydeep Sen, Soumen Chakrabarti, Samarth Bharadwaj, Feifei Pan |  |
| 1693 |  |  [HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level](https://doi.org/10.18653/v1/2023.acl-long.450) |  | 0 | Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical... | Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin |  |
| 1694 |  |  [ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning](https://doi.org/10.18653/v1/2023.acl-long.451) |  | 0 | This paper explores the task of radiology report generation, which aims at generating free-text descriptions for a set of radiographs. One significant challenge of this task is how to correctly maintain the consistency between the images and the lengthy report. Previous research explored solving this issue through planning-based methods, which generate reports only based on high-level plans. However, these plans usually only contain the major observations from the radiographs (e.g., lung... | Wenjun Hou, Kaishuai Xu, Yi Cheng, Wenjie Li, Jiang Liu |  |
| 1695 |  |  [Data Curation Alone Can Stabilize In-context Learning](https://doi.org/10.18653/v1/2023.acl-long.452) |  | 0 | In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or... | TingYun Chang, Robin Jia |  |
| 1696 |  |  [MidMed: Towards Mixed-Type Dialogues for Medical Consultation](https://doi.org/10.18653/v1/2023.acl-long.453) |  | 0 | Most medical dialogue systems assume that patients have clear goals (seeking a diagnosis, medicine querying, etc.) before medical consultation. However, in many real situations, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. For further study, we create a novel human-to-human... | Xiaoming Shi, Zeming Liu, Chuan Wang, Haitao Leng, Kui Xue, Xiaofan Zhang, Shaoting Zhang |  |
| 1697 |  |  [FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning](https://doi.org/10.18653/v1/2023.acl-long.454) |  | 0 | Large pre-trained models are capable of few-shot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in open-domain QA, we hypothesize that similar techniques can be applied to improve the efficiency... | Qinyuan Ye, Iz Beltagy, Matthew E. Peters, Xiang Ren, Hannaneh Hajishirzi |  |
| 1698 |  |  [S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.455) |  | 0 | Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically... | Benfeng Xu, Quan Wang, Yajuan Lyu, Dai Dai, Yongdong Zhang, Zhendong Mao |  |
| 1699 |  |  [DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.456) |  | 0 | Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more... | Xuxi Chen, Tianlong Chen, Weizhu Chen, Ahmed Hassan Awadallah, Zhangyang Wang, Yu Cheng |  |
| 1700 |  |  [CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.acl-long.457) |  | 0 | Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional... | Jinfeng Zhou, Chujie Zheng, Bo Wang, Zheng Zhang, Minlie Huang |  |
| 1701 |  |  [Comparative evaluation of boundary-relaxed annotation for Entity Linking performance](https://doi.org/10.18653/v1/2023.acl-long.458) |  | 0 | Entity Linking performance has a strong reliance on having a large quantity of high-quality annotated training data available. Yet, manual annotation of named entities, especially their boundaries, is ambiguous, error-prone, and raises many inconsistencies between annotators. While imprecise boundary annotation can degrade a model’s performance, there are applications where accurate extraction of entities’ surface form is not necessary. For those cases, a lenient annotation guideline could... | Gabriel Herman Bernardim Andrade, Shuntaro Yada, Eiji Aramaki |  |
| 1702 |  |  [Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?](https://doi.org/10.18653/v1/2023.acl-long.459) |  | 0 | The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we... | Shuheng Liu, Alan Ritter |  |
| 1703 |  |  [READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises](https://doi.org/10.18653/v1/2023.acl-long.460) |  | 0 | For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we... | Chenglei Si, Zhengyan Zhang, Yingfa Chen, Xiaozhi Wang, Zhiyuan Liu, Maosong Sun |  |
| 1704 |  |  [MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification](https://doi.org/10.18653/v1/2023.acl-long.461) |  | 0 | Target-dependent sentiment classification (TSC) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets. While socially important, TSC in the news domain remains relatively understudied. We introduce MAD-TSC, a new dataset which differs substantially from existing resources. First, it includes aligned examples in eight languages to facilitate a comparison of... | Evan Dufraisse, Adrian Popescu, Julien Tourille, Armelle Brun, Jérôme Deshayes |  |
| 1705 |  |  [A New Dataset and Empirical Study for Sentence Simplification in Chinese](https://doi.org/10.18653/v1/2023.acl-long.462) |  | 0 | Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference... | Shiping Yang, Renliang Sun, Xiaojun Wan |  |
| 1706 |  |  [Factual or Contextual? Disentangling Error Types in Entity Description Generation](https://doi.org/10.18653/v1/2023.acl-long.463) |  | 0 | In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally... | Navita Goyal, Ani Nenkova, Hal Daumé III |  |
| 1707 |  |  [Weakly Supervised Vision-and-Language Pre-training with Relative Representations](https://doi.org/10.18653/v1/2023.acl-long.464) |  | 0 | Weakly supervised vision-and-language pre-training (WVLP), which learns cross-modal representations with limited cross-modal supervision, has been shown to effectively reduce the data cost of pre-training while maintaining decent performance on downstream tasks. However, current WVLP methods use only local descriptions of images, i.e., object tags, as cross-modal anchors to construct weakly-aligned image-text pairs for pre-training. This affects the data quality and thus the effectiveness of... | Chi Chen, Peng Li, Maosong Sun, Yang Liu |  |
| 1708 |  |  [HermEs: Interactive Spreadsheet Formula Prediction via Hierarchical Formulet Expansion](https://doi.org/10.18653/v1/2023.acl-long.465) |  | 0 | We propose HermEs, the first approach for spreadsheet formula prediction via HiEraRchical forMulet ExpanSion, where hierarchical expansion means generating formulas following the underlying parse tree structure, and Formulet refers to commonly-used multi-level patterns mined from real formula parse trees. HermEs improves the formula prediction accuracy by (1) guaranteeing correct grammar by hierarchical generation rather than left-to-right generation and (2) significantly streamlining the... | Wanrong He, Haoyu Dong, Yihuai Gao, Zhichao Fan, Xingzhuo Guo, Zhitao Hou, Xiao Lv, Ran Jia, Shi Han, Dongmei Zhang |  |
| 1709 |  |  [ArgU: A Controllable Factual Argument Generator](https://doi.org/10.18653/v1/2023.acl-long.466) |  | 0 | Effective argumentation is essential towards a purposeful conversation with a satisfactory outcome. For example, persuading someone to reconsider smoking might involve empathetic, well founded arguments based on facts and expert opinions about its ill-effects and the consequences on one’s family. However, the automatic generation of high-quality factual arguments can be challenging. Addressing existing controllability issues can make the recent advances in computational models for argument... | Sougata Saha, Rohini K. Srihari |  |
| 1710 |  |  [Learning Answer Generation using Supervision from Automatic Question Answering Evaluators](https://doi.org/10.18653/v1/2023.acl-long.467) |  | 0 | Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we propose a novel training paradigm for GenQA using supervision from automatic QA evaluation models (GAVA). Specifically, we propose three strategies to transfer knowledge from these QA evaluation models... | Matteo Gabburo, Siddhant Garg, Rik KoncelKedziorski, Alessandro Moschitti |  |
| 1711 |  |  [RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation](https://doi.org/10.18653/v1/2023.acl-long.468) |  | 0 | Endowing chatbots with a consistent persona is essential to an engaging conversation, yet it remains an unresolved challenge. In this work, we propose a new retrieval-enhanced approach for personalized response generation. Specifically, we design a hierarchical transformer retriever trained on dialogue domain data to perform personalized retrieval and a context-aware prefix encoder that fuses the retrieved information to the decoder more effectively. Extensive experiments on a real-world... | Shuai Liu, Hyundong Cho, Marjorie Freedman, Xuezhe Ma, Jonathan May |  |
| 1712 |  |  [Don't Parse, Choose Spans! Continuous and Discontinuous Constituency Parsing via Autoregressive Span Selection](https://doi.org/10.18653/v1/2023.acl-long.469) |  | 0 | We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection. Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and leverage a pointer network to autoregressively select spans by that order. To deal with discontinuous spans, we consecutively select their subspans from left to right, label all but last subspans with... | Songlin Yang, Kewei Tu |  |
| 1713 |  |  [Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.470) |  | 0 | Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their ability to compositionally generalize. In this work, we approach semantic parsing from, quite literally, the opposite direction; that is, we introduce a neural semantic parsing generation method that... | Maxwell Crouse, Pavan Kapanipathi, Subhajit Chaudhury, Tahira Naseem, Ramón Fernandez Astudillo, Achille Fokoue, Tim Klinger |  |
| 1714 |  |  [AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression](https://doi.org/10.18653/v1/2023.acl-long.471) |  | 0 | Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation... | Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, Rui Wang |  |
| 1715 |  |  [(QA)²: Question Answering with Questionable Assumptions](https://doi.org/10.18653/v1/2023.acl-long.472) |  | 0 | Naturally occurring information-seeking questions often contain questionable assumptions—assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for information-seeking questions. For instance, the question “When did Marie Curie discover Uranium?” cannot be answered as a typical “when” question without addressing the false assumption “Marie Curie discovered... | Najoung Kim, Phu Mon Htut, Samuel R. Bowman, Jackson Petty |  |
| 1716 |  |  [Attributable and Scalable Opinion Summarization](https://doi.org/10.18653/v1/2023.acl-long.473) |  | 0 | We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary... | Tom Hosking, Hao Tang, Mirella Lapata |  |
| 1717 |  |  [Targeted Data Generation: Finding and Fixing Model Weaknesses](https://doi.org/10.18653/v1/2023.acl-long.474) |  | 0 | Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those... | Zexue He, Marco Túlio Ribeiro, Fereshte Khani |  |
| 1718 |  |  [HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation](https://doi.org/10.18653/v1/2023.acl-long.475) |  | 0 | To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameter-efficient fine-tuning method HiFi,... | Anchun Gui, Han Xiao |  |
| 1719 |  |  [CFSum Coarse-to-Fine Contribution Network for Multimodal Summarization](https://doi.org/10.18653/v1/2023.acl-long.476) |  | 0 | Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the... | Min Xiao, Junnan Zhu, Haitao Lin, Yu Zhou, Chengqing Zong |  |
| 1720 |  |  [On "Scientific Debt" in NLP: A Case for More Rigour in Language Model Pre-Training Research](https://doi.org/10.18653/v1/2023.acl-long.477) |  | 0 | This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand... | Made Nindyatama Nityasya, Haryo Akbarianto Wibowo, Alham Fikri Aji, Genta Indra Winata, Radityo Eko Prasojo, Phil Blunsom, Adhiguna Kuncoro |  |
| 1721 |  |  [End-to-end Knowledge Retrieval with Multi-modal Queries](https://doi.org/10.18653/v1/2023.acl-long.478) |  | 0 | We investigate knowledge retrieval with multi-modal queries, i.e. queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a system to retrieve knowledge from a large corpus by integrating contents from both text and image queries. We introduce a retriever model “ReViz” that can directly process input text and images... | Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, Chitta Baral |  |
| 1722 |  |  [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.479) |  | 0 | Direct speech-to-speech translation (S2ST) aims to convert speech from one language into another, and has demonstrated significant progress to date. Despite the recent success, current S2ST models still suffer from distinct degradation in noisy environments and fail to translate visual speech (i.e., the movement of lips and teeth). In this work, we present AV-TranSpeech, the first audio-visual speech-to-speech (AV-S2ST) translation model without relying on intermediate text. AV-TranSpeech... | Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, Zhou Zhao |  |
| 1723 |  |  [Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection](https://doi.org/10.18653/v1/2023.acl-long.480) |  | 0 | Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent detection. However, previous studies are confused by the identical representation of the utterance with multiple labels and overlook the intrinsic intra-class and inter-class interactions. To address these... | Feng Zhang, Wei Chen, Fei Ding, Tengjiao Wang |  |
| 1724 |  |  [VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets](https://doi.org/10.18653/v1/2023.acl-long.481) |  | 0 | The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In... | Vageesh Saxena, Nils Rethmeier, Gijs van Dijck, Gerasimos Spanakis |  |
| 1725 |  |  [Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method](https://doi.org/10.18653/v1/2023.acl-long.482) |  | 0 | Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the “Lasswell Communication... | Yiming Wang, Zhuosheng Zhang, Rui Wang |  |
| 1726 |  |  [Efficient Shapley Values Estimation by Amortization for Text Classification](https://doi.org/10.18653/v1/2023.acl-long.483) |  | 0 | Despite the popularity of Shapley Values in explaining neural text classification models, computing them is prohibitive for large pretrained models due to a large number of model evaluations. In practice, Shapley Values are often estimated with a small number of stochastic model evaluations. However, we show that the estimated Shapley Values are sensitive to random seed choices – the top-ranked features often have little overlap across different seeds, especially on examples with longer input... | Chenghao Yang, Fan Yin, He He, KaiWei Chang, Xiaofei Ma, Bing Xiang |  |
| 1727 |  |  [PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks](https://doi.org/10.18653/v1/2023.acl-long.484) |  | 0 | Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach... | Weiwen Xu, Xin Li, Yang Deng, Wai Lam, Lidong Bing |  |
| 1728 |  |  [Dynamic Regularization in UDA for Transformers in Multimodal Classification](https://doi.org/10.18653/v1/2023.acl-long.485) |  | 0 | Multimodal machine learning is a cutting-edge field that explores ways to incorporate information from multiple sources into models. As more multimodal data becomes available, this field has become increasingly relevant. This work focuses on two key challenges in multimodal machine learning. The first is finding efficient ways to combine information from different data types. The second is that often, one modality (e.g., text) is stronger and more relevant, making it difficult to identify... | Ivonne MonterAldana, Adrián Pastor LópezMonroy, Fernando SánchezVega |  |
| 1729 |  |  [Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing](https://doi.org/10.18653/v1/2023.acl-long.486) |  | 0 | Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization of framing from the communication sciences which explicitly captures elements of narratives, including conflict and its resolution, and integrate it with the narrative framing of key entities in the... | Lea Frermann, Jiatong Li, Shima Khanehzar, Gosia Mikolajczak |  |
| 1730 |  |  [bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2023.acl-long.487) |  | 0 | We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic... | Momchil Hardalov, Pepa Atanasova, Todor Mihaylov, Galia Angelova, Kiril Simov, Petya Osenova, Veselin Stoyanov, Ivan Koychev, Preslav Nakov, Dragomir Radev |  |
| 1731 |  |  [DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-long.488) |  | 0 | Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we... | Yuxi Feng, Xiaoyuan Yi, Xiting Wang, Laks V. S. Lakshmanan, Xing Xie |  |
| 1732 |  |  [What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?](https://doi.org/10.18653/v1/2023.acl-long.489) |  | 0 | Humans can effortlessly understand the coordinate structure of sentences such as “Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, \*respectively\*”. In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass... | Ruixiang Cui, Seolhwa Lee, Daniel Hershcovich, Anders Søgaard |  |
| 1733 |  |  [BLIND: Bias Removal With No Demographics](https://doi.org/10.18653/v1/2023.acl-long.490) |  | 0 | Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that... | Hadas Orgad, Yonatan Belinkov |  |
| 1734 |  |  [How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks](https://doi.org/10.18653/v1/2023.acl-long.491) |  | 0 | Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks – malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this... | Salijona Dyrmishi, Salah Ghamizi, Maxime Cordy |  |
| 1735 |  |  [Soft Alignment Objectives for Robust Adaptation of Language Generation](https://doi.org/10.18653/v1/2023.acl-long.492) |  | 0 | Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference. Our results show that (1) avoiding... | Michal Stefánik, Marek Kadlcík, Petr Sojka |  |
| 1736 |  |  [The CRINGE Loss: Learning what language not to model](https://doi.org/10.18653/v1/2023.acl-long.493) |  | 0 | Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We... | Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, Jason Weston |  |
| 1737 |  |  [Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process](https://doi.org/10.18653/v1/2023.acl-long.494) |  | 0 | Dialogue systems have received increasing attention while automatically evaluating their performance remains challenging. User satisfaction estimation (USE) has been proposed as an alternative. It assumes that the performance of a dialogue system can be measured by user satisfaction and uses an estimator to simulate users. The effectiveness of USE depends heavily on the estimator. Existing estimators independently predict user satisfaction at each turn and ignore satisfaction dynamics across... | Fanghua Ye, Zhiyuan Hu, Emine Yilmaz |  |
| 1738 |  |  [Towards Identifying Fine-Grained Depression Symptoms from Memes](https://doi.org/10.18653/v1/2023.acl-long.495) |  | 0 | The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized intervention, it is crucial to identify fine-grained mental health symptoms. To this end, we conduct a focused study on depression disorder and introduce a new task of identifying fine-grained depressive... | Shweta Yadav, Cornelia Caragea, Chenye Zhao, Naincy Kumari, Marvin Solberg, Tanmay Sharma |  |
| 1739 |  |  [SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks](https://doi.org/10.18653/v1/2023.acl-long.496) |  | 0 | Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer... | Suwon Shon, Siddhant Arora, ChyiJiunn Lin, Ankita Pasad, Felix Wu, Roshan S. Sharma, WeiLun Wu, Hungyi Lee, Karen Livescu, Shinji Watanabe |  |
| 1740 |  |  [My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave](https://doi.org/10.18653/v1/2023.acl-long.497) |  | 0 | News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of... | Pavan Holur, David Chong, Timothy R. Tangherlini, Vwani Roychowdhury |  |
| 1741 |  |  [Characterizing and Measuring Linguistic Dataset Drift](https://doi.org/10.18653/v1/2023.acl-long.498) |  | 0 | NLP models often degrade in performance when real world data distributions differ markedly from training data. However, existing dataset drift metrics in NLP have generally not considered specific dimensions of linguistic drift that affect model performance, and they have not been validated in their ability to predict model performance at the individual example level, where such metrics are often used in practice. In this paper, we propose three dimensions of linguistic dataset drift:... | Tyler A. Chang, Kishaloy Halder, Neha Anna John, Yogarshi Vyas, Yassine Benajiba, Miguel Ballesteros, Dan Roth |  |
| 1742 |  |  [WebCPM: Interactive Web Search for Chinese Long-form Question Answering](https://doi.org/10.18653/v1/2023.acl-long.499) |  | 0 | Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web... | Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, Jie Zhou |  |
| 1743 |  |  [Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model](https://doi.org/10.18653/v1/2023.acl-long.500) |  | 0 | Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation (ZeroCQG), which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage... | Hongwei Zeng, Bifan Wei, Jun Liu, Weiping Fu |  |
| 1744 |  |  [FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.501) |  | 0 | The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss.... | ChenYu Lee, ChunLiang Li, Hao Zhang, Timothy Dozat, Vincent Perot, Guolong Su, Xiang Zhang, Kihyuk Sohn, Nikolay Glushnev, Renshen Wang, Joshua Ainslie, Shangbang Long, Siyang Qin, Yasuhisa Fujii, Nan Hua, Tomas Pfister |  |
| 1745 |  |  [MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies](https://doi.org/10.18653/v1/2023.acl-long.502) |  | 0 | Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a... | Shiyue Zhang, Shijie Wu, Ozan Irsoy, Steven Lu, Mohit Bansal, Mark Dredze, David S. Rosenberg |  |
| 1746 |  |  [Knowledgeable Parameter Efficient Tuning Network for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-long.503) |  | 0 | Commonsense question answering is important for making decisions about everyday matters. Although existing commonsense question answering works based on fully fine-tuned PLMs have achieved promising results, they suffer from prohibitive computation costs as well as poor interpretability. Some works improve the PLMs by incorporating knowledge to provide certain evidence, via elaborately designed GNN modules which require expertise. In this paper, we propose a simple knowledgeable parameter... | Ziwang Zhao, Linmei Hu, Hanyu Zhao, Yingxia Shao, Yequan Wang |  |
| 1747 |  |  [BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric](https://doi.org/10.18653/v1/2023.acl-long.504) |  | 0 | End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we propose a text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the dependency on ASR systems. BLASER leverages a multilingual multimodal encoder to directly encode the speech segments... | Mingda Chen, PaulAmbroise Duquenne, Pierre Andrews, Justine Kao, Alexandre Mourachko, Holger Schwenk, Marta R. Costajussà |  |
| 1748 |  |  [NLPositionality: Characterizing Design Biases of Datasets and Models](https://doi.org/10.18653/v1/2023.acl-long.505) |  | 0 | Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our... | Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap |  |
| 1749 |  |  [Backpack Language Models](https://doi.org/10.18653/v1/2023.acl-long.506) |  | 0 | We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its... | John Hewitt, John Thickstun, Christopher D. Manning, Percy Liang |  |
| 1750 |  |  [WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.507) |  | 0 | We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat... | Virginia K. Felkner, HoChun Herbert Chang, Eugene Jang, Jonathan May |  |
| 1751 |  |  [Grounded Multimodal Named Entity Recognition on Social Media](https://doi.org/10.18653/v1/2023.acl-long.508) |  | 0 | In recent years, Multimodal Named Entity Recognition (MNER) on social media has attracted considerable attention. However, existing MNER studies only extract entity-type pairs in text, which is useless for multimodal knowledge graph construction and insufficient for entity disambiguation. To solve these issues, in this work, we introduce a Grounded Multimodal Named Entity Recognition (GMNER) task. Given a text-image social post, GMNER aims to identify the named entities in text, their entity... | Jianfei Yu, Ziyan Li, Jieming Wang, Rui Xia |  |
| 1752 |  |  [Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference](https://doi.org/10.18653/v1/2023.acl-long.509) |  | 0 | Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and... | Junhao Zheng, Qianli Ma, Shengjie Qiu, Yue Wu, Peitian Ma, Junlong Liu, Huawen Feng, Xichen Shang, Haibin Chen |  |
| 1753 |  |  [Translation-Enhanced Multilingual Text-to-Image Generation](https://doi.org/10.18653/v1/2023.acl-long.510) |  | 0 | Research on text-to-image generation (TTI) still predominantly focuses on the English language due to the lack of annotated image-caption data in other languages; in the long run, this might widen inequitable access to TTI technology. In this work, we thus investigate multilingual TTI (termed mTTI) and the current potential of neural machine translation (NMT) to bootstrap mTTI systems. We provide two key contributions. 1) Relying on a multilingual multi-modal encoder, we provide a systematic... | Yaoyiran Li, ChingYun Chang, Stephen Rawls, Ivan Vulic, Anna Korhonen |  |
| 1754 |  |  [Benchmarking Large Language Model Capabilities for Conditional Generation](https://doi.org/10.18653/v1/2023.acl-long.511) |  | 0 | Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like fewshot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is... | Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann |  |
| 1755 |  |  [lilGym: Natural Language Visual Reasoning with Reinforcement Learning](https://doi.org/10.18653/v1/2023.acl-long.512) |  | 0 | We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct... | Anne Wu, Kianté Brantley, Noriyuki Kojima, Yoav Artzi |  |
| 1756 |  |  [Unsupervised Melody-to-Lyrics Generation](https://doi.org/10.18653/v1/2023.acl-long.513) |  | 0 | Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics... | Yufei Tian, Anjali NarayanChen, Shereen Oraby, Alessandra Cervone, Gunnar A. Sigurdsson, Chenyang Tao, Wenbo Zhao, Tagyoung Chung, Jing Huang, Nanyun Peng |  |
| 1757 |  |  [Causality-aware Concept Extraction based on Knowledge-guided Prompting](https://doi.org/10.18653/v1/2023.acl-long.514) |  | 0 | Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations,... | Siyu Yuan, Deqing Yang, Jinxi Liu, Shuyu Tian, Jiaqing Liang, Yanghua Xiao, Rui Xie |  |
| 1758 |  |  [Span-level Aspect-based Sentiment Analysis via Table Filling](https://doi.org/10.18653/v1/2023.acl-long.515) |  | 0 | In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding opinion expressions, in this paper, we propose Table Filling BERT (TF-BERT), which considers the consistency of multi-word opinion expressions at the span-level. Specially, we learn the span... | Mao Zhang, Yongxin Zhu, Zhen Liu, Zhimin Bao, Yunfei Wu, Xing Sun, Linli Xu |  |
| 1759 |  |  [Limitations of Language Models in Arithmetic and Symbolic Induction](https://doi.org/10.18653/v1/2023.acl-long.516) |  | 0 | Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such... | Jing Qian, Hong Wang, Zekun Li, Shiyang Li, Xifeng Yan |  |
| 1760 |  |  [EEL: Efficiently Encoding Lattices for Reranking](https://doi.org/10.18653/v1/2023.acl-long.517) |  | 0 | Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for “downstream” metrics can more closely optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using... | Prasann Singhal, Jiacheng Xu, Xi Ye, Greg Durrett |  |
| 1761 |  |  [CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training](https://doi.org/10.18653/v1/2023.acl-long.518) |  | 0 | Improving text representation has attracted much attention to achieve expressive text-to-speech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech, a cross-modal contrastive pre-training framework that learns from the prosody variance of the same text token under different contexts. Specifically, 1) with the design of a text encoder and a prosody... | Zhenhui Ye, Rongjie Huang, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Xiang Yin, Zhou Zhao |  |
| 1762 |  |  [Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation](https://doi.org/10.18653/v1/2023.acl-long.519) |  | 0 | Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each... | Yulong Chen, Huajian Zhang, Yijie Zhou, Xuefeng Bai, Yueguan Wang, Ming Zhong, Jianhao Yan, Yafu Li, Judy Li, Xianchao Zhu, Yue Zhang |  |
| 1763 |  |  [Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation](https://doi.org/10.18653/v1/2023.acl-long.520) |  | 0 | Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two distinct timestamps T1 and T2, we first propose an unsupervised method to select (a) pivot terms related to both C1 and C2, and (b) anchor terms that are associated with a specific pivot term in each... | Xiaohang Tang, Yi Zhou, Danushka Bollegala |  |
| 1764 |  |  [How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech](https://doi.org/10.18653/v1/2023.acl-long.521) |  | 0 | When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children’s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children’s linguistic input: text from the... | Aditya Yedetore, Tal Linzen, Robert Frank, R. Thomas McCoy |  |
| 1765 |  |  [GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator](https://doi.org/10.18653/v1/2023.acl-long.522) |  | 0 | Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two... | Jian Yang, Shuming Ma, Li Dong, Shaohan Huang, Haoyang Huang, Yuwei Yin, Dongdong Zhang, Liqun Yang, Furu Wei, Zhoujun Li |  |
| 1766 |  |  [Linear Guardedness and its Implications](https://doi.org/10.18653/v1/2023.acl-long.523) |  | 0 | Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary... | Shauli Ravfogel, Yoav Goldberg, Ryan Cotterell |  |
| 1767 |  |  [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://doi.org/10.18653/v1/2023.acl-long.524) |  | 0 | Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism—the unintentional consumption of bilingual signals, including translation examples—in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study.... | Eleftheria Briakou, Colin Cherry, George F. Foster |  |
| 1768 |  |  [Open Set Relation Extraction via Unknown-Aware Training](https://doi.org/10.18653/v1/2023.acl-long.525) |  | 0 | The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, in which the relations remain the same during both training and testing. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training... | Jun Zhao, Xin Zhao, WenYu Zhan, Qi Zhang, Tao Gui, Zhongyu Wei, Yun Wen Chen, Xiang Gao, Xuanjing Huang |  |
| 1769 |  |  [Learning to Imagine: Visually-Augmented Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.526) |  | 0 | People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visually-augmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can... | Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, JiRong Wen |  |
| 1770 |  |  [Generating Hashtags for Short-form Videos with Guided Signals](https://doi.org/10.18653/v1/2023.acl-long.527) |  | 0 | Short-form video hashtag recommendation (SVHR) aims to recommend hashtags to content creators from videos and corresponding descriptions. Most prior studies regard SVHR as a classification or ranking problem and select hashtags from a set of limited candidates. However, in reality, users can create new hashtags, and trending hashtags change rapidly over time on social media. Both of these properties cannot be easily modeled with classification approaches. To bridge this gap, we formulate SVHR... | Tiezheng Yu, Hanchao Yu, Davis Liang, Yuning Mao, Shaoliang Nie, PoYao Huang, Madian Khabsa, Pascale Fung, YiChia Wang |  |
| 1771 |  |  [NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints](https://doi.org/10.18653/v1/2023.acl-long.528) |  | 0 | Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints. We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic... | Mohaddeseh Bastan, Mihai Surdeanu, Niranjan Balasubramanian |  |
| 1772 |  |  [The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning](https://doi.org/10.18653/v1/2023.acl-long.529) |  | 0 | Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues. However, human translations are expensive, while machine translations are cheap but prone to error and bias. In this work, we propose an active learning approach that exploits the strengths of both human... | Zhuang Li, Lizhen Qu, Philip R. Cohen, Raj Tumuluri, Gholamreza Haffari |  |
| 1773 |  |  [Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the "What" and Focus on the "How"!](https://doi.org/10.18653/v1/2023.acl-long.530) |  | 0 | We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant selection bias. We propose a novel statistical model that decomposes the document embeddings into a linear superposition of two vectors; a latent neutral context vector independent of ideology, and a latent... | Chen Chen, Dylan Walker, Venkatesh Saligrama |  |
| 1774 |  |  [Unsupervised Extractive Summarization of Emotion Triggers](https://doi.org/10.18653/v1/2023.acl-long.531) |  | 0 | Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In... | Tiberiu Sosea, Hongli Zhan, Junyi Jessy Li, Cornelia Caragea |  |
| 1775 |  |  [Document-Level Event Argument Extraction With a Chain Reasoning Paradigm](https://doi.org/10.18653/v1/2023.acl-long.532) |  | 0 | Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies. Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning. This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning... | Jian Liu, Chen Liang, Jinan Xu, Haoyan Liu, Zhe Zhao |  |
| 1776 |  |  [Pre-training Multi-party Dialogue Models with Latent Discourse Inference](https://doi.org/10.18653/v1/2023.acl-long.533) |  | 0 | Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that understands the discourse structure of multi-party dialogues, namely, to whom each utterance is replying. However, due to the lack of explicitly annotated discourse labels in multi-party dialogue corpora,... | Yiyang Li, Xinting Huang, Wei Bi, Hai Zhao |  |
| 1777 |  |  [Interpreting Positional Information in Perspective of Word Order](https://doi.org/10.18653/v1/2023.acl-long.534) |  | 0 | The attention mechanism is a powerful and effective method utilized in natural language processing. However, it has been observed that this method is insensitive to positional information. Although several studies have attempted to improve positional encoding and investigate the influence of word order perturbation, it remains unclear how positional encoding impacts NLP models from the perspective of word order. In this paper, we aim to shed light on this problem by analyzing the working... | Xilong Zhang, Ruochen Liu, Jin Liu, Xuefeng Liang |  |
| 1778 |  |  [I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation](https://doi.org/10.18653/v1/2023.acl-long.535) |  | 0 | Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a... | Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui Qin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin Choi |  |
| 1779 |  |  [More than Classification: A Unified Framework for Event Temporal Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.536) |  | 0 | Event temporal relation extraction (ETRE) is usually formulated as a multi-label classification task, where each type of relation is simply treated as a one-hot label. This formulation ignores the meaning of relations and wipes out their intrinsic dependency. After examining the relation definitions in various ETRE tasks, we observe that all relations can be interpreted using the start and end time points of events. For example, relation Includes could be interpreted as event 1 starting no... | Quzhe Huang, Yutong Hu, Shengqi Zhu, Yansong Feng, Chang Liu, Dongyan Zhao |  |
| 1780 |  |  [Multi-Source Test-Time Adaptation as Dueling Bandits for Extractive Question Answering](https://doi.org/10.18653/v1/2023.acl-long.537) |  | 0 | In this work, we study multi-source test-time model adaptation from user feedback, where K distinct models are established for adaptation. To allow efficient adaptation, we cast the problem as a stochastic decision-making process, aiming to determine the best adapted model after adaptation. We discuss two frameworks: multi-armed bandit learning and multi-armed dueling bandits. Compared to multi-armed bandit learning, the dueling framework allows pairwise collaboration among K models, which is... | Hai Ye, Qizhe Xie, Hwee Tou Ng |  |
| 1781 |  |  [Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery](https://doi.org/10.18653/v1/2023.acl-long.538) |  | 0 | Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label disambiguation and representation learning, that is, the reliability of pseudo labels relies on representation learning, and representation learning is restricted by pseudo labels in turn. In this... | Yutao Mou, Xiaoshuai Song, Keqing He, Chen Zeng, Pei Wang, Jingang Wang, Yunsen Xian, Weiran Xu |  |
| 1782 |  |  [DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering](https://doi.org/10.18653/v1/2023.acl-long.539) |  | 0 | Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To... | Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, Minlie Huang |  |
| 1783 |  |  [Backdooring Neural Code Search](https://doi.org/10.18653/v1/2023.acl-long.540) |  | 0 | Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied.... | Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo |  |
| 1784 |  |  [Concise Answers to Complex Questions: Summarization of Long-form Answers](https://doi.org/10.18653/v1/2023.acl-long.541) |  | 0 | Long-form question answering systems provide rich information by presenting paragraph-level answers, often containing optional background or auxiliary information. While such comprehensive answers are helpful, not all information is required to answer the question (e.g. users with domain knowledge do not need an explanation of background). Can we provide a concise version of the answer by summarizing it, while still addressing the question? We conduct a user study on summarized answers... | Abhilash Potluri, Fangyuan Xu, Eunsol Choi |  |
| 1785 |  |  [Towards Better Entity Linking with Multi-View Enhanced Distillation](https://doi.org/10.18653/v1/2023.acl-long.542) |  | 0 | Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes... | Yi Liu, Yuan Tian, Jianxun Lian, Xinlong Wang, Yanan Cao, Fang Fang, Wen Zhang, Haizhen Huang, Weiwei Deng, Qi Zhang |  |
| 1786 |  |  [A Measure-Theoretic Characterization of Tight Language Models](https://doi.org/10.18653/v1/2023.acl-long.543) |  | 0 | Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in... | Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, Ryan Cotterell |  |
| 1787 |  |  [PAED: Zero-Shot Persona Attribute Extraction in Dialogues](https://doi.org/10.18653/v1/2023.acl-long.544) |  | 0 | Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for... | Luyao Zhu, Wei Li, Rui Mao, Vlad Pandelea, Erik Cambria |  |
| 1788 |  |  [PromptRank: Unsupervised Keyphrase Extraction Using Prompt](https://doi.org/10.18653/v1/2023.acl-long.545) |  | 0 | The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to document embeddings. However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning.... | Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xiaoyan Bai |  |
| 1789 |  |  [When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories](https://doi.org/10.18653/v1/2023.acl-long.546) |  | 0 | Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and... | Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi |  |
| 1790 |  |  [infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information](https://doi.org/10.18653/v1/2023.acl-long.547) |  | 0 | The success of NLP systems often relies on the availability of large, high-quality datasets. However, not all samples in these datasets are equally valuable for learning, as some may be redundant or noisy. Several methods for characterizing datasets based on model-driven meta-information (e.g., model’s confidence) have been developed, but the relationship and complementary effects of these methods have received less attention. In this paper, we introduce infoVerse, a universal framework for... | Jaehyung Kim, Yekyung Kim, Karin de Langis, Jinwoo Shin, Dongyeop Kang |  |
| 1791 |  |  [SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models](https://doi.org/10.18653/v1/2023.acl-long.548) |  | 0 | Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as... | Akshita Jha, Aida Mostafazadeh Davani, Chandan K. Reddy, Shachi Dave, Vinodkumar Prabhakaran, Sunipa Dev |  |
| 1792 |  |  [Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations](https://doi.org/10.18653/v1/2023.acl-long.549) |  | 0 | Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics... | Lucy Lu Wang, Yulia Otmakhova, Jay DeYoung, Thinh Hung Truong, Bailey Kuehl, Erin Bransom, Byron C. Wallace |  |
| 1793 |  |  [Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge](https://doi.org/10.18653/v1/2023.acl-long.550) |  | 0 | Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our... | Jiangjie Chen, Wei Shi, Ziquan Fu, Sijie Cheng, Lei Li, Yanghua Xiao |  |
| 1794 |  |  [An Inner Table Retriever for Robust Table Question Answering](https://doi.org/10.18653/v1/2023.acl-long.551) |  | 0 | Recent years have witnessed the thriving of pretrained Transformer-based language models for understanding semi-structured tables, with several applications, such as Table Question Answering (TableQA).These models are typically trained on joint tables and surrounding natural language text, by linearizing table content into sequences comprising special tokens and cell information. This yields very long sequences which increase system inefficiency, and moreover, simply truncating long sequences... | Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert, Gonzalo Iglesias |  |
| 1795 |  |  [SIMSUM: Document-level Text Simplification via Simultaneous Summarization](https://doi.org/10.18653/v1/2023.acl-long.552) |  | 0 | Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Our model is designed with explicit summarization and simplification models and guides the generation using the main keywords of a source text. In order to evaluate our new model, we use two existing... | Sofia Blinova, Xinyu Zhou, Martin Jaggi, Carsten Eickhoff, Seyed Ali Bahrainian |  |
| 1796 |  |  [SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation](https://doi.org/10.18653/v1/2023.acl-long.553) |  | 0 | Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we... | Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng |  |
| 1797 |  |  [NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic](https://doi.org/10.18653/v1/2023.acl-long.554) |  | 0 | Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles’ heel of the victim models. In... | Zi'ou Zheng, Xiaodan Zhu |  |
| 1798 |  |  [Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction](https://doi.org/10.18653/v1/2023.acl-long.555) |  | 0 | A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven... | Ashish Sharma, Kevin Rushton, Inna E. Lin, David Wadden, Khendra G. Lucas, Adam S. Miner, Theresa Nguyen, Tim Althoff |  |
| 1799 |  |  [Dating Greek Papyri with Text Regression](https://doi.org/10.18653/v1/2023.acl-long.556) |  | 0 | Dating Greek papyri accurately is crucial not only to edit their texts but also to understand numerous other aspects of ancient writing, document and book production and circulation, as well as various other aspects of administration, everyday life and intellectual history of antiquity. Although a substantial number of Greek papyri documents bear a date or other conclusive data as to their chronological placement, an even larger number can only be dated tentatively or in approximation, due to... | John Pavlopoulos, Maria Konstantinidou, Isabelle MarthotSantaniello, Holger Essler, Asimina Paparigopoulou |  |
| 1800 |  |  [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://doi.org/10.18653/v1/2023.acl-long.557) |  | 0 | Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step... | Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal |  |
| 1801 |  |  [Direct Fact Retrieval from Knowledge Graphs without Entity Linking](https://doi.org/10.18653/v1/2023.acl-long.558) |  | 0 | There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous... | Jinheon Baek, Alham Fikri Aji, Jens Lehmann, Sung Ju Hwang |  |
| 1802 |  |  [DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering](https://doi.org/10.18653/v1/2023.acl-long.559) |  | 0 | Question answering models commonly have access to two sources of “knowledge” during inference time: (1) parametric knowledge - the factual knowledge encoded in the model weights, and (2) contextual knowledge - external knowledge (e.g., a Wikipedia passage) given to the model to generate a grounded answer. Having these two sources of knowledge entangled together is a core issue for generative QA models as it is unclear whether the answer stems from the given non-parametric knowledge or not. This... | Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, Omri Abend |  |
| 1803 |  |  [A New Direction in Stance Detection: Target-Stance Extraction in the Wild](https://doi.org/10.18653/v1/2023.acl-long.560) |  | 0 | Stance detection aims to detect the stance toward a corresponding target. Existing works use the assumption that the target is known in advance, which is often not the case in the wild. Given a text from social media platforms, the target information is often unknown due to implicit mentions in the source text and it is infeasible to have manual target annotations at a large scale. Therefore, in this paper, we propose a new task Target-Stance Extraction (TSE) that aims to extract the (target,... | Yingjie Li, Krishna Garg, Cornelia Caragea |  |
| 1804 |  |  [Improved Instruction Ordering in Recipe-Grounded Conversation](https://doi.org/10.18653/v1/2023.acl-long.561) |  | 0 | In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model’s lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks,... | Duong Minh Le, Ruohao Guo, Wei Xu, Alan Ritter |  |
| 1805 |  |  [Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions](https://doi.org/10.18653/v1/2023.acl-long.562) |  | 0 | While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have made their behavior somewhat opaque. To mitigate this opacity, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of... | ByungDoh Oh, William Schuler |  |
| 1806 |  |  [Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization](https://doi.org/10.18653/v1/2023.acl-long.563) |  | 0 | Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of inter-dependency of multiple events. Moreover, an event is decoded by iteratively merging its related entities as arguments, which might suffer from error propagation and is computationally inefficient.... | Xinyu Wang, Lin Gui, Yulan He |  |
| 1807 |  |  [Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training](https://doi.org/10.18653/v1/2023.acl-long.564) |  | 0 | Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance. Recently, several work focused on the dialogue-adaptive post-training (DialPost) that further trains PLMs to fit dialogues.... | Zhenyu Zhang, Lei Shen, Yuming Zhao, Meng Chen, Xiaodong He |  |
| 1808 |  |  [Language Detoxification with Attribute-Discriminative Latent Space](https://doi.org/10.18653/v1/2023.acl-long.565) |  | 0 | Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To... | Jin Myung Kwak, Minseon Kim, Sung Ju Hwang |  |
| 1809 |  |  [Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction](https://doi.org/10.18653/v1/2023.acl-long.566) |  | 0 | Sarcasm, as a form of irony conveying mockery and contempt, has been widespread in social media such as Twitter and Weibo, where the sarcastic text is commonly characterized as an incongruity between the surface positive and negative situation. Naturally, it has an urgent demand to automatically identify sarcasm from social media, so as to illustrate people’s real views toward specific targets. In this paper, we develop a novel sarcasm detection method, namely Sarcasm Detector with Augmentation... | Changrong Min, Ximing Li, Liang Yang, Zhilin Wang, Bo Xu, Hongfei Lin |  |
| 1810 |  |  [Adaptive and Personalized Exercise Generation for Online Language Learning](https://doi.org/10.18653/v1/2023.acl-long.567) |  | 0 | Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise generation for online language learning. To this end, we combine a knowledge tracing model that estimates each student’s evolving knowledge states from their learning history and a controlled text generation... | Peng Cui, Mrinmaya Sachan |  |
| 1811 |  |  [NLP Reproducibility For All: Understanding Experiences of Beginners](https://doi.org/10.18653/v1/2023.acl-long.568) |  | 0 | As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and... | Shane Storks, Keunwoo Peter Yu, Ziqiao Ma, Joyce Chai |  |
| 1812 |  |  [Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA](https://doi.org/10.18653/v1/2023.acl-long.569) |  | 0 | Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. Our analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. We then develop an English question-generation model which we demonstrate via... | Elias StengelEskin, Jimena GuallarBlasco, Yi Zhou, Benjamin Van Durme |  |
| 1813 |  |  [UMRSpell: Unifying the Detection and Correction Parts of Pre-trained Models towards Chinese Missing, Redundant, and Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.570) |  | 0 | Chinese Spelling Correction (CSC) is the task of detecting and correcting misspelled charac- ters in Chinese texts. As an important step for various downstream tasks, CSC confronts two challenges: 1) Character-level errors consist not only of spelling errors but also of missing and redundant ones that cause variable length between input and output texts, for which most CSC methods could not handle well because of the consistence length of texts required by their inherent detection-correction... | Zheyu He, Yujin Zhu, Linlin Wang, Liang Xu |  |
| 1814 |  |  [LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction](https://doi.org/10.18653/v1/2023.acl-long.571) |  | 0 | Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until... | Jeremiah Milbauer, Annie Louis, Mohammad Javad Hosseini, Alex Fabrikant, Donald Metzler, Tal Schuster |  |
| 1815 |  |  [Local Interpretation of Transformer Based on Linear Decomposition](https://doi.org/10.18653/v1/2023.acl-long.572) |  | 0 | In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local... | Sen Yang, Shujian Huang, Wei Zou, Jianbing Zhang, Xinyu Dai, Jiajun Chen |  |
| 1816 |  |  [DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions](https://doi.org/10.18653/v1/2023.acl-long.573) |  | 0 | Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help... | Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, Graham Neubig |  |
| 1817 |  |  [Multilingual Event Extraction from Historical Newspaper Adverts](https://doi.org/10.18653/v1/2023.acl-long.574) |  | 0 | NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well... | Nadav Borenstein, Natalia da Silva Perez, Isabelle Augenstein |  |
| 1818 |  |  [BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency](https://doi.org/10.18653/v1/2023.acl-long.575) |  | 0 | Twitter bots are automatic programs operated by malicious actors to manipulate public opinion and spread misinformation. Research efforts have been made to automatically identify bots based on texts and networks on social media. Existing methods only leverage texts or networks alone, and while few works explored the shallow combination of the two modalities, we hypothesize that the interaction and information exchange between texts and graphs could be crucial for holistically evaluating bot... | Zhenyu Lei, Herun Wan, Wenqian Zhang, Shangbin Feng, Zilong Chen, Jundong Li, Qinghua Zheng, Minnan Luo |  |
| 1819 |  |  [Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions](https://doi.org/10.18653/v1/2023.acl-long.576) |  | 0 | When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a... | Mayur Patidar, Prayushi Faldu, Avinash Kumar Singh, Lovekesh Vig, Indrajit Bhattacharya, Mausam |  |
| 1820 |  |  [Understanding Client Reactions in Online Mental Health Counseling](https://doi.org/10.18653/v1/2023.acl-long.577) |  | 0 | Communication success relies heavily on reading participants’ reactions. Such feedback is especially important for mental health counselors, who must carefully consider the client’s progress and adjust their approach accordingly. However, previous NLP research on counseling has mainly focused on studying counselors’ intervention strategies rather than their clients’ reactions to the intervention. This work aims to fill this gap by developing a theoretically grounded annotation framework that... | Anqi Li, Lizhi Ma, Yaling Mei, Hongliang He, Shuai Zhang, Huachuan Qiu, Zhenzhong Lan |  |
| 1821 |  |  [Nonlinear Structural Equation Model Guided Gaussian Mixture Hierarchical Topic Modeling](https://doi.org/10.18653/v1/2023.acl-long.578) |  | 0 | Hierarchical topic models, which can extract semantically meaningful topics from a textcorpus in an unsupervised manner and automatically organise them into a topic hierarchy, have been widely used to discover the underlying semantic structure of documents. However, the existing models often assume in the prior that the topic hierarchy is a tree structure, ignoring symmetrical dependenciesbetween topics at the same level. Moreover, the sparsity of text data often complicate the analysis. To... | Hegang Chen, Pengbo Mao, Yuyin Lu, Yanghui Rao |  |
| 1822 |  |  [Revisiting Token Dropping Strategy in Efficient BERT Pretraining](https://doi.org/10.18653/v1/2023.acl-long.579) |  | 0 | Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective... | Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du, Dacheng Tao |  |
| 1823 |  |  [The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers](https://doi.org/10.18653/v1/2023.acl-long.580) |  | 0 | Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the... | Ariel Gera, Roni Friedman, Ofir Arviv, R. Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, Eyal Shnarch |  |
| 1824 |  |  [FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering](https://doi.org/10.18653/v1/2023.acl-long.581) |  | 0 | Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude claim and conclude whether it’s truthful or a mere masquerade. Popular fact-checking websites follow a common structure for fact categorization such as half true, half false, false, pants on fire, etc.... | Anku Rani, S. M. Towhidul Islam Tonmoy, Dwip Dalal, Shreya Gautam, Megha Chakraborty, Aman Chadha, Amit P. Sheth, Amitava Das |  |
| 1825 |  |  [Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages](https://doi.org/10.18653/v1/2023.acl-long.582) |  | 0 | We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families. The dataset contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person, Location, and, Organization) for 9 out of the 11 languages. The training dataset has been automatically created from the Samanantar parallel corpus by projecting automatically tagged entities from an... | Arnav Mhaske, Harshit Kedia, Sumanth Doddapaneni, Mitesh M. Khapra, Pratyush Kumar, V. Rudra Murthy, Anoop Kunchukuttan |  |
| 1826 |  |  [CREPE: Open-Domain Question Answering with False Presuppositions](https://doi.org/10.18653/v1/2023.acl-long.583) |  | 0 | When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25% of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through... | Xinyan Yu, Sewon Min, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 1827 |  |  [Joint Document-Level Event Extraction via Token-Token Bidirectional Event Completed Graph](https://doi.org/10.18653/v1/2023.acl-long.584) |  | 0 | We solve the challenging document-level event extraction problem by proposing a joint exaction methodology that can avoid inefficiency and error propagation issues in classic pipeline methods. Essentially, we address the three crucial limitations in existing studies. First, the autoregressive strategy of path expansion heavily relies on the orders of argument role. Second, the number of events in documents must be specified in advance. Last, unexpected errors usually exist when decoding events... | Qizhi Wan, Changxuan Wan, Keli Xiao, Dexi Liu, Chenliang Li, Bolong Zheng, Xiping Liu, Rong Hu |  |
| 1828 |  |  [Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering](https://doi.org/10.18653/v1/2023.acl-long.585) |  | 0 | Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust... | Xiaolin Zheng, Mengling Hu, Weiming Liu, Chaochao Chen, Xinting Liao |  |
| 1829 |  |  [Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention](https://doi.org/10.18653/v1/2023.acl-long.586) |  | 0 | Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces additional alignment models and increases the complexity of the whole framework; (b) training inefficiency: the trained model will only be used for the completion of one target KG, although the data... | Rongchuan Tang, Yang Zhao, Chengqing Zong, Yu Zhou |  |
| 1830 |  |  [What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization](https://doi.org/10.18653/v1/2023.acl-long.587) |  | 0 | Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about... | Griffin Adams, Bichlien Nguyen, Jake Smith, Yingce Xia, Shufang Xie, Anna Ostropolets, Budhaditya Deb, YuanJyue Chen, Tristan Naumann, Noémie Elhadad |  |
| 1831 |  |  [Annotating Mentions Alone Enables Efficient Domain Adaptation for Coreference Resolution](https://doi.org/10.18653/v1/2023.acl-long.588) |  | 0 | Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, it remains a challenge to successfully transfer these models to new target domains containing many out-of-vocabulary spans and requiring differing annotation schemes. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. In this work, we show that adapting mention detection is the key component to... | Nupoor Gandhi, Anjalie Field, Emma Strubell |  |
| 1832 |  |  [A Universal Discriminator for Zero-Shot Generalization](https://doi.org/10.18653/v1/2023.acl-long.589) |  | 0 | Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this... | Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, Zhilin Yang |  |
| 1833 |  |  [Syntax and Geometry of Information](https://doi.org/10.18653/v1/2023.acl-long.590) |  | 0 | This paper presents an information-theoretical model of syntactic generalization. We study syntactic generalization from the perspective of the capacity to disentangle semantic and structural information, emulating the human capacity to assign a grammaticality judgment to semantically nonsensical sentences. In order to isolate the structure, we propose to represent the probability distribution behind a corpus as the product of the probability of a semantic context and the probability of a... | Raphaël Bailly, Laurent Leblond, Kata Gábor |  |
| 1834 |  |  [GreenKGC: A Lightweight Knowledge Graph Completion Method](https://doi.org/10.18653/v1/2023.acl-long.591) |  | 0 | Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple score function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution,... | Yuncheng Wang, Xiou Ge, Bin Wang, C.C. Jay Kuo |  |
| 1835 |  |  [Unsupervised Open-domain Keyphrase Generation](https://doi.org/10.18653/v1/2023.acl-long.592) |  | 0 | In this work, we study the problem of unsupervised open-domain keyphrase generation, where the objective is a keyphrase generation model that can be built without using human-labeled data and can perform consistently across domains. To solve this problem, we propose a seq2seq model that consists of two modules, namely phraseness and informativeness module, both of which can be built in an unsupervised and open-domain fashion. The phraseness module generates phrases, while the informativeness... | Lam Do, Pritom Saha Akash, Kevin ChenChuan Chang |  |
| 1836 |  |  [A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment](https://doi.org/10.18653/v1/2023.acl-long.593) |  | 0 | When communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS conversation (CSConv) dataset, which contains about 2.6K groups of dialogues with therapy principles and emotional support strategy labels. Making chit chat while providing emotional support is overlooked by... | Jiyue Jiang, Sheng Wang, Qintong Li, Lingpeng Kong, Chuan Wu |  |
| 1837 |  |  [Plug-and-Play Knowledge Injection for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.594) |  | 0 | Injecting external knowledge can improve the performance of pre-trained language models (PLMs) on various downstream NLP tasks. However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks. In this work, we are the first to study how to improve the flexibility and efficiency of knowledge injection by reusing existing downstream models. To this end, we explore a new paradigm plug-and-play knowledge injection, where knowledge bases are... | Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 1838 |  |  [Two Birds One Stone: Dynamic Ensemble for OOD Intent Classification](https://doi.org/10.18653/v1/2023.acl-long.595) |  | 0 | Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does not know. This paper investigates “overthinking” in the open-world scenario and its impact on OOD intent classification. Inspired by this, we propose a two-birds-one-stone method, which allows the... | Yunhua Zhou, Jianqiang Yang, Pengyu Wang, Xipeng Qiu |  |
| 1839 |  |  [SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages](https://doi.org/10.18653/v1/2023.acl-long.596) |  | 0 | Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the... | Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq Joty, Caiming Xiong, ChienSheng Wu |  |
| 1840 |  |  [Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?](https://doi.org/10.18653/v1/2023.acl-long.597) |  | 0 | Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler... | Juanhui Li, Harry Shomer, Jiayuan Ding, Yiqi Wang, Yao Ma, Neil Shah, Jiliang Tang, Dawei Yin |  |
| 1841 |  |  [A dynamic programming algorithm for span-based nested named-entity recognition in O(n²)](https://doi.org/10.18653/v1/2023.acl-long.598) |  | 0 | Span-based nested named-entity recognition (NER) has a cubic-time complexity using avariant of the CYK algorithm. We show that by adding a supplementary structural constraint on the search space, nested NER has a quadratic-time complexity, that is the same asymptotic complexity than the non-nested case. The proposed algorithm covers a large part of three standard English benchmarks and delivers comparable experimental results. | Caio Corro |  |
| 1842 |  |  [Target-Side Augmentation for Document-Level Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.599) |  | 0 | Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA) model to generate many potential translations for each source document. Learning on these wider range translations, an MT model can learn a smoothed distribution, thereby reducing the risk of data... | Guangsheng Bao, Zhiyang Teng, Yue Zhang |  |
| 1843 |  |  [Rethinking Masked Language Modeling for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.600) |  | 0 | In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a... | Hongqiu Wu, Shaohua Zhang, Yuchen Zhang, Hai Zhao |  |
| 1844 |  |  [A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues](https://doi.org/10.18653/v1/2023.acl-long.601) |  | 0 | Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a... | Yunxin Li, Baotian Hu, Xinyu Chen, Yuxin Ding, Lin Ma, Min Zhang |  |
| 1845 |  |  [Simple and Effective Unsupervised Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.602) |  | 0 | The amount of labeled data to train models for speech tasks is limited for most languages, however, the data scarcity is exacerbated for speech translation which requires labeled data covering two different languages. To address this issue, we study a simple and effective approach to build speech translation systems without labeled data by leveraging recent advances in unsupervised speech recognition, machine translation and speech synthesis, either in a pipeline approach, or to generate... | Changhan Wang, Hirofumi Inaguma, PengJen Chen, Ilia Kulikov, Yun Tang, WeiNing Hsu, Michael Auli, Juan Pino |  |
| 1846 |  |  [Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation](https://doi.org/10.18653/v1/2023.acl-long.603) |  | 0 | Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first... | Xuan Long Do, Bowei Zou, Shafiq R. Joty, Anh Tran Tai, Liangming Pan, Nancy F. Chen, Ai Ti Aw |  |
| 1847 |  |  [CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification](https://doi.org/10.18653/v1/2023.acl-long.604) |  | 0 | Document-level Event Causality Identification (DECI) aims to recognize causal relations between events within a document. Recent studies focus on building a document-level graph for cross-sentence reasoning, but ignore important causal structures — there are one or two “central” events that prevail throughout the document, with most other events serving as either their cause or consequence. In this paper, we manually annotate central events for a systematical investigation and propose a novel... | Meiqi Chen, Yixin Cao, Yan Zhang, Zhiwei Liu |  |
| 1848 |  |  [f-Divergence Minimization for Sequence-Level Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.605) |  | 0 | Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and... | Yuqiao Wen, Zichao Li, Wenyu Du, Lili Mou |  |
| 1849 |  |  [Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.606) |  | 0 | Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level... | Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, Songlin Hu |  |
| 1850 |  |  [A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.607) |  | 0 | Document-level relation extraction (DocRE) aims to extract relations among entities within a document, which is crucial for applications like knowledge graph construction. Existing methods usually assume that entities and their mentions are identified beforehand, which falls short of real-world applications. To overcome this limitation, we propose TaG, a novel table-to-graph generation model for joint extractionof entities and relations at document-level. To enhance the learning of task... | Ruoyu Zhang, Yanzeng Li, Lei Zou |  |
| 1851 |  |  [A Synthetic Data Generation Framework for Grounded Dialogues](https://doi.org/10.18653/v1/2023.acl-long.608) |  | 0 | Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process.... | Jianzhu Bao, Rui Wang, Yasheng Wang, Aixin Sun, Yitong Li, Fei Mi, Ruifeng Xu |  |
| 1852 |  |  [MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages](https://doi.org/10.18653/v1/2023.acl-long.609) |  | 0 | In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS... | Cheikh M. Bamba Dione, David Ifeoluwa Adelani, Peter Nabende, Jesujoba O. Alabi, Thapelo Sindane, Happy Buzaaba, Shamsuddeen Hassan Muhammad, Chris Chinenye Emezue, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jonathan Mukiibi, Blessing K. Sibanda, Bonaventure F. P. Dossou, Andiswa Bukula, Rooweither Mabuya, Allahsera Auguste Tapo, Edwin MunkohBuabeng, Victoire Memdjokam Koagne, Fatoumata Ouoba Kabore, Amelia V. Taylor, Godson Kalipe, Tebogo Macucwa, Vukosi Marivate, Tajuddeen Gwadabe, Elvis Mboning Tchiaze, Ikechukwu E. Onyenwe, Gratien Atindogbe, Tolulope Anu Adelani, Idris Akinade, Samuel Olanrewaju, Marien Nahimana, Théogène Musabeyezu, Emile Niyomutabazi, Ester Chimhenga, Kudzai Gotosa, Patrick Mizha, Apelete Agbolo, Seydou Traore, Chinedu Uchechukwu, Aliyu Yusuf, Muhammad Abdullahi, Dietrich Klakow |  |
| 1853 |  |  [Semantic Structure Enhanced Event Causality Identification](https://doi.org/10.18653/v1/2023.acl-long.610) |  | 0 | Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former... | Zhilei Hu, Zixuan Li, Xiaolong Jin, Long Bai, Saiping Guan, Jiafeng Guo, Xueqi Cheng |  |
| 1854 |  |  [Weakly-Supervised Spoken Video Grounding via Semantic Interaction Learning](https://doi.org/10.18653/v1/2023.acl-long.611) |  | 0 | The task of spoken video grounding aims to localize moments in videos that are relevant to descriptive spoken queries. However, extracting semantic information from speech and modeling the cross-modal correlation pose two critical challenges. Previous studies solve them by representing spoken queries based on the matched video frames, which require tremendous effort for frame-level labeling. In this work, we investigate weakly-supervised spoken video grounding, i.e., learning to localize... | Ye Wang, Wang Lin, Shengyu Zhang, Tao Jin, Linjun Li, Xize Cheng, Zhou Zhao |  |
| 1855 |  |  [Rehearsal-free Continual Language Learning via Efficient Parameter Isolation](https://doi.org/10.18653/v1/2023.acl-long.612) |  | 0 | We study the problem of defying catastrophic forgetting when learning a series of language processing tasks. Compared with previous methods, we emphasize the importance of not caching history tasks’ data, which makes the problem more challenging. Our proposed method applies the parameter isolation strategy. For each task, it allocates a small portion of private parameters and learns them with a shared pre-trained model. To load correct parameters at testing time, we introduce a simple yet... | Zhicheng Wang, Yufang Liu, Tao Ji, Xiaoling Wang, Yuanbin Wu, Congcong Jiang, Ye Chao, Zhencong Han, Ling Wang, Xu Shao, Wenqiu Zeng |  |
| 1856 |  |  [Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification](https://doi.org/10.18653/v1/2023.acl-long.613) |  | 0 | Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate... | ChihYao Chen, TunMin Hung, YiLi Hsu, LunWei Ku |  |
| 1857 |  |  [Combo of Thinking and Observing for Outside-Knowledge VQA](https://doi.org/10.18653/v1/2023.acl-long.614) |  | 0 | Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text which further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we... | Qingyi Si, Yuchen Mo, Zheng Lin, Huishan Ji, Weiping Wang |  |
| 1858 |  |  [AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model](https://doi.org/10.18653/v1/2023.acl-long.615) |  | 0 | Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation... | IHung Hsu, Zhiyu Xie, KuanHao Huang, Prem Natarajan, Nanyun Peng |  |
| 1859 |  |  [Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships](https://doi.org/10.18653/v1/2023.acl-long.616) |  | 0 | Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We... | David Jurgens, Agrima Seth, Jackson Sargent, Athena Aghighi, Michael Geraci |  |
| 1860 |  |  [TART: Improved Few-shot Text Classification Using Task-Adaptive Reference Transformation](https://doi.org/10.18653/v1/2023.acl-long.617) |  | 0 | Meta-learning has emerged as a trending technique to tackle few-shot text classification and achieve state-of-the-art performance. However, the performance of existing approaches heavily depends on the inter-class variance of the support set. As a result, it can perform well on tasks when the semantics of sampled classes are distinct while failing to differentiate classes with similar semantics. In this paper, we propose a novel Task-Adaptive Reference Transformation (TART) network, aiming to... | Shuo Lei, Xuchao Zhang, Jianfeng He, Fanglan Chen, ChangTien Lu |  |
| 1861 |  |  [How Do In-Context Examples Affect Compositional Generalization?](https://doi.org/10.18653/v1/2023.acl-long.618) |  | 0 | Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional... | Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, JianGuang Lou, Dongmei Zhang |  |
| 1862 |  |  [Attractive Storyteller: Stylized Visual Storytelling with Unpaired Text](https://doi.org/10.18653/v1/2023.acl-long.619) |  | 0 | Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases, real-world styles are likely to be implied at the syntactic and discourse levels. In this work, we introduce a new task of Stylized Visual Storytelling (SVST), which aims to describe a photo... | Dingyi Yang, Qin Jin |  |
| 1863 |  |  [Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.acl-long.620) |  | 0 | Many machine learning-based low-code or no-code applications involve generating code that interacts with structured knowledge. For example, one of the most studied tasks in this area is generating SQL code from a natural language statement. Prior work shows that incorporating context information from the database schema, such as table and column names, is beneficial to model performance on this task. In this work we present a large pretraining dataset and strategy for learning representations... | Robert Giaquinto, Dejiao Zhang, Benjamin Kleiner, Yang Li, Ming Tan, Parminder Bhatia, Ramesh Nallapati, Xiaofei Ma |  |
| 1864 |  |  [WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction](https://doi.org/10.18653/v1/2023.acl-long.621) |  | 0 | Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs in this paper. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive... | Qiyu Wu, Masaaki Nagata, Yoshimasa Tsuruoka |  |
| 1865 |  |  [Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models](https://doi.org/10.18653/v1/2023.acl-long.622) |  | 0 | Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner might instead choose to allocate the available budget to hire annotators and manually label additional fine-tuning... | Junmo Kang, Wei Xu, Alan Ritter |  |
| 1866 |  |  [OD-RTE: A One-Stage Object Detection Framework for Relational Triple Extraction](https://doi.org/10.18653/v1/2023.acl-long.623) |  | 0 | The Relational Triple Extraction (RTE) task is a fundamental and essential information extraction task. Recently, the table-filling RTE methods have received lots of attention. Despite their success, they suffer from some inherent problems such as underutilizing regional information of triple. In this work, we treat the RTE task based on table-filling method as an Object Detection task and propose a one-stage Object Detection framework for Relational Triple Extraction (OD-RTE). In this... | Jinzhong Ning, Zhihao Yang, Yuanyuan Sun, Zhizheng Wang, Hongfei Lin |  |
| 1867 |  |  [I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons](https://doi.org/10.18653/v1/2023.acl-long.624) |  | 0 | We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players—students, each with their own personas and abilities—to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1)... | Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris CallisonBurch, Yejin Choi, Prithviraj Ammanabrolu |  |
| 1868 |  |  [Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning](https://doi.org/10.18653/v1/2023.acl-long.625) |  | 0 | Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained... | Tianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, Xuanjing Huang |  |
| 1869 |  |  [Is GPT-3 a Good Data Annotator?](https://doi.org/10.18653/v1/2023.acl-long.626) |  | 0 | Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this... | Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, Lidong Bing |  |
| 1870 |  |  [Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog](https://doi.org/10.18653/v1/2023.acl-long.627) |  | 0 | Retrieving proper domain knowledge from an external database lies at the heart of end-to-end task-oriented dialog systems to generate informative responses. Most existing systems blend knowledge retrieval with response generation and optimize them with direct supervision from reference responses, leading to suboptimal retrieval performance when the knowledge base becomes large-scale. To address this, we propose to decouple knowledge retrieval from response generation and introduce a... | Fanqi Wan, Weizhou Shen, Ke Yang, Xiaojun Quan, Wei Bi |  |
| 1871 |  |  [Few-shot Event Detection: An Empirical Study and a Unified View](https://doi.org/10.18653/v1/2023.acl-long.628) |  | 0 | Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we compare 12 representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis.... | Yubo Ma, Zehao Wang, Yixin Cao, Aixin Sun |  |
| 1872 |  |  [How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases](https://doi.org/10.18653/v1/2023.acl-long.629) |  | 0 | Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth,... | Aaron Mueller, Tal Linzen |  |
| 1873 |  |  [ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations](https://doi.org/10.18653/v1/2023.acl-long.630) |  | 0 | Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life. We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to... | Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula |  |
| 1874 |  |  [HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation](https://doi.org/10.18653/v1/2023.acl-long.631) |  | 0 | Recent NLP models have shown the remarkable ability to effectively generalise ‘zero-shot’ to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into... | Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, Matthew E. Peters |  |
| 1875 |  |  [Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations](https://doi.org/10.18653/v1/2023.acl-long.632) |  | 0 | In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified... | Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, He He |  |
| 1876 |  |  [An Inclusive Notion of Text](https://doi.org/10.18653/v1/2023.acl-long.633) |  | 0 | Natural language processing (NLP) researchers develop models of grammar, meaning and communication based on written text. Due to task and data differences, what is considered text can vary substantially across studies. A conceptual framework for systematically capturing these differences is lacking. We argue that clarity on the notion of text is crucial for reproducible and generalizable NLP. Towards that goal, we propose common terminology to discuss the production and transformation of... | Ilia Kuznetsov, Iryna Gurevych |  |
| 1877 |  |  [AlignScore: Evaluating Factual Consistency with A Unified Alignment Function](https://doi.org/10.18653/v1/2023.acl-long.634) |  | 0 | Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs... | Yuheng Zha, Yichi Yang, Ruichen Li, Zhiting Hu |  |
| 1878 |  |  [Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation](https://doi.org/10.18653/v1/2023.acl-long.635) |  | 0 | Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART backbone, it overlooks the gap between the visual feature space and the decoder semantic space, the object-level metadata of the image, as well as the potential external knowledge. To solve these... | Liqiang Jing, Xuemeng Song, Kun Ouyang, Mengzhao Jia, Liqiang Nie |  |
| 1879 |  |  [Counterfactual Active Learning for Out-of-Distribution Generalization](https://doi.org/10.18653/v1/2023.acl-long.636) |  | 0 | We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples,... | Xun Deng, Wenjie Wang, Fuli Feng, Hanwang Zhang, Xiangnan He, Yong Liao |  |
| 1880 |  |  [Multi-granularity Temporal Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.637) |  | 0 | Recently, question answering over temporal knowledge graphs (i.e., TKGQA) has been introduced and investigated, in quest of reasoning about dynamic factual knowledge. To foster research on TKGQA, a few datasets have been curated (e.g., CronQuestions and Complex-CronQuestions), and various models have been proposed based on these datasets. Nevertheless, existing efforts overlook the fact that real-life applications of TKGQA also tend to be complex in temporal granularity, i.e., the questions may... | Ziyang Chen, Jinzhi Liao, Xiang Zhao |  |
| 1881 |  |  [A New Aligned Simple German Corpus](https://doi.org/10.18653/v1/2023.acl-long.638) |  | 0 | “Leichte Sprache”, the German counterpart to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German – German. It contains multiple document-aligned sources which we have aligned using automatic sentence-alignment methods. We evaluate our alignments based on a manually labelled subset of aligned documents. The quality of our... | Vanessa Toborek, Moritz Busch, Malte Boßert, Christian Bauckhage, Pascal Welke |  |
| 1882 |  |  [Introducing Semantics into Speech Encoders](https://doi.org/10.18653/v1/2023.acl-long.639) |  | 0 | Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a task-agnostic... | Derek Xu, Shuyan Dong, Changhan Wang, Suyoun Kim, Zhaojiang Lin, Bing Liu, Akshat Shrivastava, ShangWen Li, LiangHsuan Tseng, GuanTing Lin, Alexei Baevski, Hungyi Lee, Yizhou Sun, Wei Wang |  |
| 1883 |  |  [Constrained Tuple Extraction with Interaction-Aware Network](https://doi.org/10.18653/v1/2023.acl-long.640) |  | 0 | Tuples extraction is a fundamental task for information extraction and knowledge graph construction. The extracted tuples are usually represented as knowledge triples consisting of subject, relation, and object. In practice, however, the validity of knowledge triples is associated with and changes with the spatial, temporal, or other kinds of constraints. Motivated by this observation, this paper proposes a constrained tuple extraction (CTE) task to guarantee the validity of knowledge tuples.... | Xiaojun Xue, Chunxia Zhang, Tianxiang Xu, Zhendong Niu |  |
| 1884 |  |  [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://doi.org/10.18653/v1/2023.acl-long.641) |  | 0 | Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The... | Zhiyang Xu, Ying Shen, Lifu Huang |  |
| 1885 |  |  [Single Sequence Prediction over Reasoning Graphs for Multi-hop QA](https://doi.org/10.18653/v1/2023.acl-long.642) |  | 0 | Recent generative approaches for multi-hop question answering (QA) utilize the fusion-in-decoder method to generate a single sequence output which includes both a final answer and a reasoning path taken to arrive at that answer, such as passage titles and key facts from those passages. While such models can lead to better interpretability and high quantitative scores, they often have difficulty accurately identifying the passages corresponding to key entities in the context, resulting in... | Gowtham Ramesh, Makesh Narsimhan Sreedhar, Junjie Hu |  |
| 1886 |  |  [Contrastive Error Attribution for Finetuned Language Models](https://doi.org/10.18653/v1/2023.acl-long.643) |  | 0 | Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing,... | Faisal Ladhak, Esin Durmus, Tatsunori Hashimoto |  |
| 1887 |  |  [DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications](https://doi.org/10.18653/v1/2023.acl-long.644) |  | 0 | Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show... | Adam Ivankay, Mattia Rigotti, Pascal Frossard |  |
| 1888 |  |  [Neural Machine Translation for Mathematical Formulae](https://doi.org/10.18653/v1/2023.acl-long.645) |  | 0 | We tackle the problem of neural machine translation of mathematical formulae between ambiguous presentation languages and unambiguous content languages. Compared to neural machine translation on natural language, mathematical formulae have a much smaller vocabulary and much longer sequences of symbols, while their translation requires extreme precision to satisfy mathematical information needs. In this work, we perform the tasks of translating from LaTeX to Mathematica as well as from LaTeX to... | Felix Petersen, Moritz Schubotz, André GreinerPetter, Bela Gipp |  |
| 1889 |  |  [Query-Efficient Black-Box Red Teaming via Bayesian Optimization](https://doi.org/10.18653/v1/2023.acl-long.646) |  | 0 | The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating... | Deokjae Lee, JunYeong Lee, JungWoo Ha, JinHwa Kim, SangWoo Lee, Hwaran Lee, Hyun Oh Song |  |
| 1890 |  |  [SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control](https://doi.org/10.18653/v1/2023.acl-long.647) |  | 0 | Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it... | Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov |  |
| 1891 |  |  [Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing](https://doi.org/10.18653/v1/2023.acl-long.648) |  | 0 | Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g., president, politician) of a given entity mention (e.g., Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance. It brings deeper interaction between the mention and the type to reach better performance but has to perform N (the type... | Chengyue Jiang, Wenyang Hui, Yong Jiang, Xiaobin Wang, Pengjun Xie, Kewei Tu |  |
| 1892 |  |  [MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.649) |  | 0 | Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared... | Yuchen Hu, Chen Chen, Ruizhe Li, Heqing Zou, Eng Siong Chng |  |
| 1893 |  |  [Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors](https://doi.org/10.18653/v1/2023.acl-long.650) |  | 0 | The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems’ outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine... | Liyan Tang, Tanya Goyal, Alexander R. Fabbri, Philippe Laban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin F. Rousseau, Greg Durrett |  |
| 1894 |  |  [GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding](https://doi.org/10.18653/v1/2023.acl-long.651) |  | 0 | Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language... | JiaChen Gu, Zhenhua Ling, Quan Liu, Cong Liu, Guoping Hu |  |
| 1895 |  |  [Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks](https://doi.org/10.18653/v1/2023.acl-long.652) |  | 0 | Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be... | Artem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, Alexander Panchenko, Maxim Panov, Mikhail Burtsev, Artem Shelmanov |  |
| 1896 |  |  [BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting](https://doi.org/10.18653/v1/2023.acl-long.653) |  | 0 | The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language... | Zheng Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M. Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Edward Raff, Dragomir Radev, Vassilina Nikoulina |  |
| 1897 |  |  [Logic-driven Indirect Supervision: An Application to Crisis Counseling](https://doi.org/10.18653/v1/2023.acl-long.654) |  | 0 | Ensuring the effectiveness of text-based crisis counseling requires observing ongoing conversations and providing feedback, both labor-intensive tasks. Automatic analysis of conversations—at the full chat and utterance levels—may help support counselors and provide better care. While some session-level training data (e.g., rating of patient risk) is often available from counselors, labeling utterances requires expensive post hoc annotation. But the latter can not only provide insights about... | Mattia Medina Grespan, Meghan Broadbent, Xinyao Zhang, Katherine Axford, Brent Kious, Zac E. Imel, Vivek Srikumar |  |
| 1898 |  |  [Grounding Characters and Places in Narrative Text](https://doi.org/10.18653/v1/2023.acl-long.655) |  | 0 | Tracking characters and locations throughout a story can help improve the understanding of its plot structure. Prior research has analyzed characters and locations from text independently without grounding characters to their locations in narrative time. Here, we address this gap by proposing a new spatial relationship categorization task. The objective of the task is to assign a spatial relationship category for every character and location co-mention within a window of text, taking into... | Sandeep Soni, Amanpreet Sihra, Elizabeth F. Evans, Matthew Wilkens, David Bamman |  |
| 1899 |  |  [From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models](https://doi.org/10.18653/v1/2023.acl-long.656) |  | 0 | Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of... | Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov |  |
| 1900 |  |  [SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT](https://doi.org/10.18653/v1/2023.acl-long.657) |  | 0 | Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between... | Aditya Yadavalli, Alekhya Yadavalli, Vera Tobin |  |
| 1901 |  |  [Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.658) |  | 0 | In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to... | Albert Xu, Xiang Ren, Robin Jia |  |
| 1902 |  |  [Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?](https://doi.org/10.18653/v1/2023.acl-long.659) |  | 0 | Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt... | Chengwei Qin, Shafiq R. Joty, Qian Li, Ruochen Zhao |  |
| 1903 |  |  [Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale](https://doi.org/10.18653/v1/2023.acl-long.660) |  | 0 | Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: ~70% of the attention heads and ~20%... | Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin Kirchhoff, Dan Roth |  |
| 1904 |  |  [Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya](https://doi.org/10.18653/v1/2023.acl-long.661) |  | 0 | Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on... | Fitsum Gaim, Wonsuk Yang, Hancheol Park, Jong Park |  |
| 1905 |  |  [ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain](https://doi.org/10.18653/v1/2023.acl-long.662) |  | 0 | The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model... | Mike Zhang, Rob van der Goot, Barbara Plank |  |
| 1906 |  |  [CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval](https://doi.org/10.18653/v1/2023.acl-long.663) |  | 0 | Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on various retrieval tasks. These methods, however, are orders of magnitude slower and need much more space to store their indices compared to their single-vector counterparts. In this paper, we unify different multi-vector retrieval models from a token routing viewpoint and propose conditional token interaction via dynamic lexical routing,... | Minghan Li, ShengChieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wentau Yih, Xilun Chen |  |
| 1907 |  |  [MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning](https://doi.org/10.18653/v1/2023.acl-long.664) |  | 0 | Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual... | Bang Yang, Fenglin Liu, Xian Wu, Yaowei Wang, Xu Sun, Yuexian Zou |  |
| 1908 |  |  [Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge](https://doi.org/10.18653/v1/2023.acl-long.665) |  | 0 | While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks – when the class label is very infrequent (e.g. < 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We... | Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann, H. Andrew Schwartz |  |
| 1909 |  |  [In-sample Curriculum Learning by Sequence Completion for Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.666) |  | 0 | Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the “easy-to-hard” intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words,... | Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu |  |
| 1910 |  |  [Product Question Answering in E-Commerce: A Survey](https://doi.org/10.18653/v1/2023.acl-long.667) |  | 0 | Product question answering (PQA), aiming to automatically provide instant responses to customer’s questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of user-generated contents in E-commerce platforms. Therefore, various problem settings and novel methods have been proposed to capture these special characteristics. In this paper, we aim to systematically... | Yang Deng, Wenxuan Zhang, Qian Yu, Wai Lam |  |
| 1911 |  |  [Towards Domain-Agnostic and Domain-Adaptive Dementia Detection from Spoken Language](https://doi.org/10.18653/v1/2023.acl-long.668) |  | 0 | Health-related speech datasets are often small and varied in focus. This makes it difficult to leverage them to effectively support healthcare goals. Robust transfer of linguistic features across different datasets orbiting the same goal carries potential to address this concern. To test this hypothesis, we experiment with domain adaptation (DA) techniques on heterogeneous spoken language data to evaluate generalizability across diverse datasets for a common task: dementia detection. We find... | Shahla Farzana, Natalie Parde |  |
| 1912 |  |  [Generalizing Backpropagation for Gradient-Based Interpretability](https://doi.org/10.18653/v1/2023.acl-long.669) |  | 0 | Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model’s output with respect to its inputs. While these methods can indicate which input features may be important for the model’s prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the... | Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Alex Warstadt, Ryan Cotterell |  |
| 1913 |  |  [UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language](https://doi.org/10.18653/v1/2023.acl-long.670) |  | 0 | Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we... | Xinyi Mou, Zhongyu Wei, Qi Zhang, Xuanjing Huang |  |
| 1914 |  |  [Generic Temporal Reasoning with Differential Analysis and Explanation](https://doi.org/10.18653/v1/2023.acl-long.671) |  | 0 | Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically,... | Yu Feng, Ben Zhou, Haoyu Wang, Helen Jin, Dan Roth |  |
| 1915 |  |  [Model-Based Simulation for Optimising Smart Reply](https://doi.org/10.18653/v1/2023.acl-long.672) |  | 0 | Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of them conveys the user’s desired response. This is a significant challenge, due to the lack of datasets containing sets of responses to learn from. Resultantly, previous work has focused largely on... | Benjamin Towle, Ke Zhou |  |
| 1916 |  |  [Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval](https://doi.org/10.18653/v1/2023.acl-long.673) |  | 0 | Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting,... | John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor BergKirkpatrick |  |
| 1917 |  |  [On the Blind Spots of Model-Based Evaluation Metrics for Text Generation](https://doi.org/10.18653/v1/2023.acl-long.674) |  | 0 | In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal... | Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James R. Glass, Yulia Tsvetkov |  |
| 1918 |  |  [Dealing with Semantic Underspecification in Multimodal NLP](https://doi.org/10.18653/v1/2023.acl-long.675) |  | 0 | Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can... | Sandro Pezzelle |  |
| 1919 |  |  [Trigger Warning Assignment as a Multi-Label Document Classification Problem](https://doi.org/10.18653/v1/2023.acl-long.676) |  | 0 | A trigger warning is used to warn people about potentially disturbing content. We introduce trigger warning assignment as a multi-label classification task, create the Webis Trigger Warning Corpus 2022, and with it the first dataset of 1 million fanfiction works from Archive of our Own with up to 36 different warnings per document. To provide a reliable catalog of trigger warnings, we organized 41 million of free-form tags assigned by fanfiction authors into the first comprehensive taxonomy of... | Matti Wiegmann, Magdalena Wolska, Christopher Schröder, Ole Borchardt, Benno Stein, Martin Potthast |  |
| 1920 |  |  [WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.677) |  | 0 | This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the “pushing” operation is whitening the feature space, which scatters... | Wenjie Zhuo, Yifan Sun, Xiaohan Wang, Linchao Zhu, Yi Yang |  |
| 1921 |  |  [Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms](https://doi.org/10.18653/v1/2023.acl-long.678) |  | 0 | This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as... | Tianshu Zhang, Changchang Liu, WeiHan Lee, Yu Su, Huan Sun |  |
| 1922 |  |  [Causality-Guided Multi-Memory Interaction Network for Multivariate Stock Price Movement Prediction](https://doi.org/10.18653/v1/2023.acl-long.679) |  | 0 | Over the past few years, we’ve witnessed an enormous interest in stock price movement prediction using AI techniques. In recent literature, auxiliary data has been used to improve prediction accuracy, such as textual news. When predicting a particular stock, we assume that information from other stocks should also be utilized as auxiliary data to enhance performance. In this paper, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network... | Di Luo, Weiheng Liao, Shuqi Li, Xin Cheng, Rui Yan |  |
| 1923 |  |  [DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization](https://doi.org/10.18653/v1/2023.acl-long.680) |  | 0 | Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective... | Songyang Gao, Shihan Dou, Yan Liu, Xiao Wang, Qi Zhang, Zhongyu Wei, Jin Ma, Ying Shan |  |
| 1924 |  |  [A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires](https://doi.org/10.18653/v1/2023.acl-long.681) |  | 0 | Social media is one of the most highly sought resources for analyzing characteristics of the language by its users. In particular, many researchers utilized various linguistic features of mental health problems from social media. However, existing approaches to detecting mental disorders face critical challenges, such as the scarcity of high-quality data or the trade-off between addressing the complexity of models and presenting interpretable results grounded in expert domain knowledge. To... | Hoyun Song, Jisu Shin, Huije Lee, Jong Park |  |
| 1925 |  |  [Downstream Datasets Make Surprisingly Good Pretraining Corpora](https://doi.org/10.18653/v1/2023.acl-long.682) |  | 0 | For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gainsare attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both... | Kundan Krishna, Saurabh Garg, Jeffrey P. Bigham, Zachary C. Lipton |  |
| 1926 |  |  [Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach](https://doi.org/10.18653/v1/2023.acl-long.683) |  | 0 | We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed... | Liyan Xu, Chenwei Zhang, Xian Li, Jingbo Shang, Jinho D. Choi |  |
| 1927 |  |  [XDailyDialog: A Multilingual Parallel Dialogue Corpus](https://doi.org/10.18653/v1/2023.acl-long.684) |  | 0 | High-quality datasets are significant to the development of dialogue models. However, most existing datasets for open-domain dialogue modeling are limited to a single language. The absence of multilingual open-domain dialog datasets not only limits the research on multilingual or cross-lingual transfer learning, but also hinders the development of robust open-domain dialog systems that can be deployed in other parts of the world. In this paper, we provide a multilingual parallel open-domain... | Zeming Liu, Ping Nie, Jie Cai, Haifeng Wang, ZhengYu Niu, Peng Zhang, Mrinmaya Sachan, Kaiping Peng |  |
| 1928 |  |  [PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent](https://doi.org/10.18653/v1/2023.acl-long.685) |  | 0 | The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to... | Kshitij Mishra, Priyanshu Priya, Asif Ekbal |  |
| 1929 |  |  [Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.686) |  | 0 | Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a... | Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing |  |
| 1930 |  |  [Contrastive Decoding: Open-ended Text Generation as Optimization](https://doi.org/10.18653/v1/2023.acl-long.687) |  | 0 | Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the... | Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, Mike Lewis |  |
| 1931 |  |  [Resolving Indirect Referring Expressions for Entity Selection](https://doi.org/10.18653/v1/2023.acl-long.688) |  | 0 | Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice ‘Should we make a Simnel cake or a Pandan cake¿ a natural response from a non-expert may be indirect: ‘let’s make the green one‘. Reference resolution has... | Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis |  |
| 1932 |  |  [Accelerating Transformer Inference for Translation via Parallel Decoding](https://doi.org/10.18653/v1/2023.acl-long.689) |  | 0 | Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard... | Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, Emanuele Rodolà |  |
| 1933 |  |  [Hard Sample Aware Prompt-Tuning](https://doi.org/10.18653/v1/2023.acl-long.690) |  | 0 | Prompt-tuning based few-shot learning has garnered increasing attention in recent years due to its efficiency and promising capability. To achieve the best performance for NLP tasks with just a few samples, it is vital to include as many informative samples as possible and to avoid misleading ones. However, there is no work in prompt-tuning literature addressing the problem of differentiating informative hard samples from misleading ones in model training, which is challenging due to the lack... | Yuanjian Xu, Qi An, Jiahuan Zhang, Peng Li, Zaiqing Nie |  |
| 1934 |  |  [WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events](https://doi.org/10.18653/v1/2023.acl-long.691) |  | 0 | Biographical event detection is a relevant task that allows for the exploration and comparison of the ways in which people’s lives are told and represented. This may support several real-life applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which... | Marco Antonio Stranisci, Rossana Damiano, Enrico Mensa, Viviana Patti, Daniele Paolo Radicioni, Tommaso Caselli |  |
| 1935 |  |  [Best-k Search Algorithm for Neural Text Generation](https://doi.org/10.18653/v1/2023.acl-long.692) |  | 0 | Modern natural language generation paradigms require a decoding strategy to obtain quality sequences out of the model. Beam search yields high-quality but low diversity outputs; stochastic approaches suffer from high variance and sometimes low quality. In this work, we propose a deterministic search algorithm balancing both quality and diversity. We first investigate the vanilla best-first search (BFS) algorithm and then propose the best-k search algorithm. Inspired by BFS, we greedily expand... | Jiacheng Xu, Caiming Xiong, Silvio Savarese, Yingbo Zhou |  |
| 1936 |  |  [Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages](https://doi.org/10.18653/v1/2023.acl-long.693) |  | 0 | Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages... | Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, Pratyush Kumar |  |
| 1937 |  |  [Transforming Visual Scene Graphs to Image Captions](https://doi.org/10.18653/v1/2023.acl-long.694) |  | 0 | We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built... | Xu Yang, Jiawei Peng, Zihua Wang, Haiyang Xu, Qinghao Ye, Chenliang Li, Songfang Huang, Fei Huang, Zhangzikang Li, Yu Zhang |  |
| 1938 |  |  [Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks](https://doi.org/10.18653/v1/2023.acl-long.695) |  | 0 | Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED’s strength in non-monotonic sequence to sequence learning while retaining... | Yun Tang, Anna Y. Sun, Hirofumi Inaguma, Xinyue Chen, Ning Dong, Xutai Ma, Paden Tomasello, Juan Pino |  |
| 1939 |  |  [Improving Domain Generalization for Prompt-Aware Essay Scoring via Disentangled Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.696) |  | 0 | Automated Essay Scoring (AES) aims to score essays written in response to specific prompts. Many AES models have been proposed, but most of them are either prompt-specific or prompt-adaptive and cannot generalize well on “unseen” prompts. This work focuses on improving the generalization ability of AES models from the perspective of domain generalization, where the data of target prompts cannot be accessed during training. Specifically, we propose a prompt-aware neural AES model to extract... | Zhiwei Jiang, Tianyi Gao, Yafeng Yin, Meng Liu, Hua Yu, Zifeng Cheng, Qing Gu |  |
| 1940 |  |  [What's the Meaning of Superhuman Performance in Today's NLU?](https://doi.org/10.18653/v1/2023.acl-long.697) |  | 0 | In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain... | Simone Tedeschi, Johan Bos, Thierry Declerck, Jan Hajic, Daniel Hershcovich, Eduard H. Hovy, Alexander Koller, Simon Krek, Steven Schockaert, Rico Sennrich, Ekaterina Shutova, Roberto Navigli |  |
| 1941 |  |  [PromptNER: Prompt Locating and Typing for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.698) |  | 0 | Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost,... | Yongliang Shen, Zeqi Tan, Shuhui Wu, Wenqi Zhang, Rongsheng Zhang, Yadong Xi, Weiming Lu, Yueting Zhuang |  |
| 1942 |  |  [Hints on the data for language modeling of synthetic languages with transformers](https://doi.org/10.18653/v1/2023.acl-long.699) |  | 0 | Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The question we have addressed in this paper is to what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology,... | Rodolfo Zevallos, Núria Bel |  |
| 1943 |  |  [Neural Machine Translation Methods for Translating Text to Sign Language Glosses](https://doi.org/10.18653/v1/2023.acl-long.700) |  | 0 | State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined... | Dele Zhu, Vera Czehmann, Eleftherios Avramidis |  |
| 1944 |  |  [Revisiting Event Argument Extraction: Can EAE Models Learn Better When Being Aware of Event Co-occurrences?](https://doi.org/10.18653/v1/2023.acl-long.701) |  | 0 | Event co-occurrences have been proved effective for event extraction (EE) in previous studies, but have not been considered for event argument extraction (EAE) recently. In this paper, we try to fill this gap between EE research and EAE research, by highlighting the question that “Can EAE models learn better when being aware of event co-occurrences?”. To answer this question, we reformulate EAE as a problem of table generation and extend a SOTA prompt-based EAE model into a non-autoregressive... | Yuxin He, Jingyue Hu, Buzhou Tang |  |
| 1945 |  |  [HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation](https://doi.org/10.18653/v1/2023.acl-long.702) |  | 0 | Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG... | Qianyu He, Yikai Zhang, Jiaqing Liang, Yuncheng Huang, Yanghua Xiao, Yunwen Chen |  |
| 1946 |  |  [Large-scale Lifelong Learning of In-context Instructions and How to Tackle It](https://doi.org/10.18653/v1/2023.acl-long.703) |  | 0 | Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of... | Jisoo Mok, Jaeyoung Do, Sungjin Lee, Tara Taghavi, Seunghak Yu, Sungroh Yoon |  |
| 1947 |  |  [Controllable Text Generation via Probability Density Estimation in the Latent Space](https://doi.org/10.18653/v1/2023.acl-long.704) |  | 0 | Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-specific classifiers or sampling one from relevant discrete samples. However, they cannot effectively model a complex space with diverse attributes, high dimensionality, and asymmetric structure, leaving subsequent controls unsatisfying. In this work, we propose a novel control framework using probability density estimation in the latent space.... | Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, Weihong Zhong, Bing Qin |  |
| 1948 |  |  [Learning Latent Relations for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.acl-long.705) |  | 0 | Temporal Knowledge Graph (TKG) reasoning aims to predict future facts based on historical data. However, due to the limitations in construction tools and data sources, many important associations between entities may be omitted in TKG. We refer to these missing associations as latent relations. Most existing methods have some drawbacks in explicitly capturing intra-time latent relations between co-occurring entities and inter-time latent relations between entities that appear at different... | Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, Liang Wang |  |
| 1949 |  |  [DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function](https://doi.org/10.18653/v1/2023.acl-long.706) |  | 0 | Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide... | Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, Jian Yin, Zhenguo Li, Xiaodan Liang |  |
| 1950 |  |  [Unsupervised Selective Rationalization with Noise Injection](https://doi.org/10.18653/v1/2023.acl-long.707) |  | 0 | A major issue with using deep learning models in sensitive applications is that they provide no explanation for their output. To address this problem, unsupervised selective rationalization produces rationales alongside predictions by chaining two jointly-trained components, a rationale generator and a predictor. Although this architecture guarantees that the prediction relies solely on the rationale, it does not ensure that the rationale contains a plausible explanation for the prediction. We... | Adam Storek, Melanie Subbiah, Kathleen R. McKeown |  |
| 1951 |  |  [Understanding In-Context Learning via Supportive Pretraining Data](https://doi.org/10.18653/v1/2023.acl-long.708) |  | 0 | In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of... | Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, Tianlu Wang |  |
| 1952 |  |  [ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation](https://doi.org/10.18653/v1/2023.acl-long.709) |  | 0 | Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a... | Zhexin Zhang, Jiaxin Wen, Minlie Huang |  |
| 1953 |  |  [Effective Contrastive Weighting for Dense Query Expansion](https://doi.org/10.18653/v1/2023.acl-long.710) |  | 0 | Verbatim queries submitted to search engines often do not sufficiently describe the user’s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query’srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation... | Xiao Wang, Sean MacAvaney, Craig Macdonald, Iadh Ounis |  |
| 1954 |  |  [Improving the Detection of Multilingual Online Attacks with Rich Social Media Data from Singapore](https://doi.org/10.18653/v1/2023.acl-long.711) |  | 0 | Toxic content is a global problem, but most resources for detecting toxic content are in English. When datasets are created in other languages, they often focus exclusively on one language or dialect. In many cultural and geographical settings, however, it is common to code-mix languages, combining and interchanging them throughout conversations. To shine a light on this practice, and enable more research into code-mixed toxic content, we introduce SOA, a new multilingual dataset of online... | Janosch Haber, Bertie Vidgen, Matthew Chapman, Vibhor Agarwal, Roy KaWei Lee, Yong Keong Yap, Paul Röttger |  |
| 1955 |  |  [Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model](https://doi.org/10.18653/v1/2023.acl-long.712) |  | 0 | We use both Bayesian and neural models to dissect a data set of Chinese learners’ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential... | Jakob Prange, Man Ho Ivy Wong |  |
| 1956 |  |  [Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization](https://doi.org/10.18653/v1/2023.acl-long.713) |  | 0 | In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to... | Artidoro Pagnoni, Alexander R. Fabbri, Wojciech Kryscinski, ChienSheng Wu |  |
| 1957 |  |  [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://doi.org/10.18653/v1/2023.acl-long.714) |  | 0 | Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual... | Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos |  |
| 1958 |  |  [MGR: Multi-generator Based Rationalization](https://doi.org/10.18653/v1/2023.acl-long.715) |  | 0 | Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies... | Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, Yuankai Zhang, Yang Qiu |  |
| 1959 |  |  [BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics](https://doi.org/10.18653/v1/2023.acl-long.716) |  | 0 | The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced into a summary, 2) effective on human-written texts, and 3) sensitive to different error types (as summaries can contain... | Liang Ma, Shuyang Cao, Robert L. Logan IV, Di Lu, Shihao Ran, Ke Zhang, Joel R. Tetreault, Alejandro Jaimes |  |
| 1960 |  |  [Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection](https://doi.org/10.18653/v1/2023.acl-long.717) |  | 0 | Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of... | Rheeya Uppaal, Junjie Hu, Yixuan Li |  |
| 1961 |  |  [UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization](https://doi.org/10.18653/v1/2023.acl-long.718) |  | 0 | The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any... | Yulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chenguang Zhu, Michael Zeng, Yue Zhang |  |
| 1962 |  |  [RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue](https://doi.org/10.18653/v1/2023.acl-long.719) |  | 0 | Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other... | Zhengliang Shi, Weiwei Sun, Shuo Zhang, Zhen Zhang, Pengjie Ren, Zhaochun Ren |  |
| 1963 |  |  [An AMR-based Link Prediction Approach for Document-level Event Argument Extraction](https://doi.org/10.18653/v1/2023.acl-long.720) |  | 0 | Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR... | Yuqing Yang, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 1964 |  |  [PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://doi.org/10.18653/v1/2023.acl-long.721) |  | 0 | Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and... | Qingqing Cao, Bhargavi Paranjape, Hannaneh Hajishirzi |  |
| 1965 |  |  [Gloss-Free End-to-End Sign Language Translation](https://doi.org/10.18653/v1/2023.acl-long.722) |  | 0 | In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation datasets, thus handicapping real-world applications. To mitigate this problem, we design the Gloss-Free End-to-end sign language translation framework (GloFE). Our method improves the performance of SLT in... | Kezhou Lin, Xiaohan Wang, Linchao Zhu, Ke Sun, Bang Zhang, Yi Yang |  |
| 1966 |  |  [TAGPRIME: A Unified Framework for Relational Structure Extraction](https://doi.org/10.18653/v1/2023.acl-long.723) |  | 0 | Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address... | IHung Hsu, KuanHao Huang, Shuning Zhang, Wenxin Cheng, Prem Natarajan, KaiWei Chang, Nanyun Peng |  |
| 1967 |  |  [Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers](https://doi.org/10.18653/v1/2023.acl-long.724) |  | 0 | This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned... | Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song |  |
| 1968 |  |  [BITE: Textual Backdoor Attacks with Iterative Trigger Injection](https://doi.org/10.18653/v1/2023.acl-long.725) |  | 0 | Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a “backdoor” into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary’s choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We... | Jun Yan, Vansh Gupta, Xiang Ren |  |
| 1969 |  |  [A Crosslingual Investigation of Conceptualization in 1335 Languages](https://doi.org/10.18653/v1/2023.acl-long.726) |  | 0 | Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all... | Yihong Liu, Haotian Ye, Leonie Weissweiler, Philipp Wicke, Renhao Pei, Robert Zangenfeind, Hinrich Schütze |  |
| 1970 |  |  [Exploring and Verbalizing Academic Ideas by Concept Co-occurrence](https://doi.org/10.18653/v1/2023.acl-long.727) |  | 0 | Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the emergence of a new idea can be regarded as the fusion of two concepts that... | Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou |  |
| 1971 |  |  [mCLIP: Multilingual CLIP via Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.acl-long.728) |  | 0 | Large-scale vision-language pretrained (VLP) models like CLIP have shown remarkable performance on various downstream cross-modal tasks. However, they are usually biased towards English due to the lack of sufficient non-English image-text pairs. Existing multilingual VLP methods often learn retrieval-inefficient single-stream models by translation-augmented non-English image-text pairs. In this paper, we introduce mCLIP, a retrieval-efficient dual-stream multilingual VLP model, trained by... | Guanhua Chen, Lu Hou, Yun Chen, Wenliang Dai, Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan, Wenping Wang |  |
| 1972 |  |  [Distantly Supervised Course Concept Extraction in MOOCs with Academic Discipline](https://doi.org/10.18653/v1/2023.acl-long.729) |  | 0 | With the rapid growth of Massive Open Online Courses (MOOCs), it is expensive and time-consuming to extract high-quality knowledgeable concepts taught in the course by human effort to help learners grasp the essence of the course. In this paper, we propose to automatically extract course concepts using distant supervision to eliminate the heavy work of human annotations, which generates labels by matching them with an easily accessed dictionary. However, this matching process suffers from... | Mengying Lu, Yuquan Wang, Jifan Yu, Yexing Du, Lei Hou, Juanzi Li |  |
| 1973 |  |  [Extrinsic Evaluation of Machine Translation Metrics](https://doi.org/10.18653/v1/2023.acl-long.730) |  | 0 | Automatic machine translation (MT) metrics are widely used to distinguish the quality of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the segment-level quality by correlating metrics with how useful the translations are... | Nikita Moghe, Tom Sherborne, Mark Steedman, Alexandra Birch |  |
| 1974 |  |  [ExplainMeetSum: A Dataset for Explainable Meeting Summarization Aligned with Human Intent](https://doi.org/10.18653/v1/2023.acl-long.731) |  | 0 | To enhance the explainability of meeting summarization, we construct a new dataset called “ExplainMeetSum,” an augmented version of QMSum, by newly annotating evidence sentences that faithfully “explain” a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracles. We further present an explainability-aware task, named... | Hyun Kim, Minsoo Cho, SeungHoon Na |  |
| 1975 |  |  [A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2023.acl-long.732) |  | 0 | Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To... | Xiaoheng Zhang, Yang Li |  |
| 1976 |  |  [CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning](https://doi.org/10.18653/v1/2023.acl-long.733) |  | 0 | Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed” from the existing knowledge that “singing makes people relaxed” by first conceptualizing “singing” as a “relaxing event” and then instantiating that event to “meditation.”This process, known as... | Weiqi Wang, Tianqing Fang, Baixuan Xu, Chun Yi Louis Bo, Yangqiu Song, Lei Chen |  |
| 1977 |  |  [The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research](https://doi.org/10.18653/v1/2023.acl-long.734) |  | 0 | Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP... | Mohamed Abdalla, Jan Philip Wahle, Terry Lima Ruas, Aurélie Névéol, Fanny Ducel, Saif M. Mohammad, Karën Fort |  |
| 1978 |  |  [Language of Bargaining](https://doi.org/10.18653/v1/2023.acl-long.735) |  | 0 | Leveraging an established exercise in negotiation education, we build a novel dataset for studying how the use of language shapes bilateral bargaining. Our dataset extends existing work in two ways: 1) we recruit participants via behavioral labs instead of crowdsourcing platforms and allow participants to negotiate through audio, enabling more naturalistic interactions; 2) we add a control setting where participants negotiate only through alternating, written numeric offers. Despite the two... | Mourad Heddaya, Solomon Dworkin, Chenhao Tan, Rob Voigt, Alexander Zentefis |  |
| 1979 |  |  [Do Question Answering Modeling Improvements Hold Across Benchmarks?](https://doi.org/10.18653/v1/2023.acl-long.736) |  | 0 | Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence—two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high... | Nelson F. Liu, Tony Lee, Robin Jia, Percy Liang |  |
| 1980 |  |  [VLN-Trans: Translator for the Vision and Language Navigation Agent](https://doi.org/10.18653/v1/2023.acl-long.737) |  | 0 | Language understanding is essential for the navigation agent to follow instructions. We observe two kinds of issues in the instructions that can make the navigation task challenging: 1. The mentioned landmarks are not recognizable by the navigation agent due to the different vision abilities of the instructor and the modeled agent. 2. The mentioned landmarks are applicable to multiple targets, thus not distinctive for selecting the target among the candidate viewpoints. To deal with these... | Yue Zhang, Parisa Kordjamshidi |  |
| 1981 |  |  [Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.738) |  | 0 | Conventional knowledge distillation (KD) methods require access to the internal information of teachers, e.g., logits. However, such information may not always be accessible for large pre-trained language models (PLMs). In this work, we focus on decision-based KD for PLMs, where only teacher decisions (i.e., top-1 labels) are accessible. Considering the information gap between logits and decisions, we propose a novel method to estimate logits from the decision distributions. Specifically,... | Qinhong Zhou, Zonghan Yang, Peng Li, Yang Liu |  |
| 1982 |  |  [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.739) |  | 0 | Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity pair embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model... | Wenxuan Zhou, Sheng Zhang, Tristan Naumann, Muhao Chen, Hoifung Poon |  |
| 1983 |  |  [KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment](https://doi.org/10.18653/v1/2023.acl-long.740) |  | 0 | Recent legislation of the “right to be forgotten” has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a... | Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, KamFai Wong, Hongzhi Yin |  |
| 1984 |  |  [UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language](https://doi.org/10.18653/v1/2023.acl-long.741) |  | 0 | Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first open-vocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the... | Nuwa Xi, Sendong Zhao, Haochun Wang, Chi Liu, Bing Qin, Ting Liu |  |
| 1985 |  |  [Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths](https://doi.org/10.18653/v1/2023.acl-long.742) |  | 0 | ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., head event, relation, tail event. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events... | Xiangqing Shen, Siwei Wu, Rui Xia |  |
| 1986 |  |  [Shrinking Embeddings for Hyper-Relational Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.743) |  | 0 | Link prediction on knowledge graphs (KGs) has been extensively studied on binary relational KGs, wherein each fact is represented by a triple. A significant amount of important knowledge, however, is represented by hyper-relational facts where each fact is composed of a primal triple and a set of qualifiers comprising a key-value pair that allows for expressing more complicated semantics. Although some recent works have proposed to embed hyper-relational KGs, these methods fail to capture... | Bo Xiong, Mojtaba Nayyeri, Shirui Pan, Steffen Staab |  |
| 1987 |  |  [CTC-based Non-autoregressive Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.744) |  | 0 | Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for non-autoregressive speech translation (NAST).In particular, we develop a model consisting of two encoders that are guided by CTC to predict the source and target texts, respectively. Introducing CTC into NAST... | Chen Xu, Xiaoqian Liu, Xiaowen Liu, Qingxuan Sun, Yuhao Zhang, Murun Yang, Qianqian Dong, Tom Ko, Mingxuan Wang, Tong Xiao, Anxiang Ma, Jingbo Zhu |  |
| 1988 |  |  [Attention as a Guide for Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.745) |  | 0 | In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDAtt (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns between audio source and target textual translation to guide an offline-trained ST model during simultaneous inference. EDAtt exploits the attention scores modeling the audio-translation relation to... | Sara Papi, Matteo Negri, Marco Turchi |  |
| 1989 |  |  [On Complementarity Objectives for Hybrid Retrieval](https://doi.org/10.18653/v1/2023.acl-long.746) |  | 0 | Dense retrieval has shown promising results in various information retrieval tasks, and hybrid retrieval, combined with the strength of sparse retrieval, has also been actively studied. A key challenge in hybrid retrieval is to make sparse and dense complementary to each other. Existing models have focused on dense models to capture “residual” features neglected in the sparse models. Our key distinction is to show how this notion of residual complementarity is limited, and propose a new... | Dohyeon Lee, Seungwon Hwang, Kyungjae Lee, Seungtaek Choi, Sunghyun Park |  |
| 1990 |  |  [C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2023.acl-long.747) |  | 0 | Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor of, against, or neutral toward a target that is unseen during training. Despite the growing attention on ZSSD, most recent advances in this task are limited to English and do not pay much attention to other languages such as Chinese. To support ZSSD research, in this paper, we present C-STANCE that, to our knowledge, is the first Chinese dataset for zero-shot stance detection. We introduce two... | Chenye Zhao, Yingjie Li, Cornelia Caragea |  |
| 1991 |  |  [Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.748) |  | 0 | Unsupervised pre-training on millions of digital-born or scanned documents has shown promising advances in visual document understanding (VDU). While various vision-language pre-training objectives are studied in existing solutions, the document textline, as an intrinsic granularity in VDU, has seldom been explored so far. A document textline usually contains words that are spatially and semantically correlated, which can be easily obtained from OCR engines. In this paper, we propose... | Haoli Bai, Zhiguang Liu, Xiaojun Meng, Wentao Li, Shuang Liu, Yifeng Luo, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu |  |
| 1992 |  |  [PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts](https://doi.org/10.18653/v1/2023.acl-long.749) |  | 0 | Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new... | Yunshui Li, Binyuan Hui, Zhichao Yin, Min Yang, Fei Huang, Yongbin Li |  |
| 1993 |  |  [MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning](https://doi.org/10.18653/v1/2023.acl-long.750) |  | 0 | Recent advances in pre-trained language models (PLMs) have facilitated the development ofcommonsense reasoning tasks. However, existing methods rely on multi-hop knowledgeretrieval and thus suffer low accuracy due toembedded noise in the acquired knowledge. In addition, these methods often attain highcomputational costs and nontrivial knowledgeloss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poorlocal optimum. In this... | Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, Liwei Wang |  |
| 1994 |  |  [PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation](https://doi.org/10.18653/v1/2023.acl-long.751) |  | 0 | Image translation is a task that translates an image containing text in the source language to the target language. One major challenge with image translation is the modality gap between visual text inputs and textual inputs/outputs of machine translation (MT). In this paper, we propose PEIT, an end-to-end image translation framework that bridges the modality gap with pre-trained models. It is composed of four essential components: a visual encoder, a shared encoder-decoder backbone network, a... | Shaolin Zhu, Shangjie Li, Yikun Lei, Deyi Xiong |  |
| 1995 |  |  [Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection](https://doi.org/10.18653/v1/2023.acl-long.752) |  | 0 | The task of Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a... | Erik Arakelyan, Arnav Arora, Isabelle Augenstein |  |
| 1996 |  |  [DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles](https://doi.org/10.18653/v1/2023.acl-long.753) |  | 0 | A crucial component in the curation of KB for a scientific domain (e.g., materials science, food & nutrition, fuels) is information extraction from tables in the domain’s published research articles. To facilitate research in this direction, we define a novel NLP task of extracting compositions of materials (e.g., glasses) from tables in materials science papers. The task involves solving several challenges in concert, such as tables that mention compositions have highly varying structures;... | Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, N. M. Anoop Krishnan, Mausam |  |
| 1997 |  |  [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://doi.org/10.18653/v1/2023.acl-long.754) |  | 0 | Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their... | Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi |  |
| 1998 |  |  [Disentangled Phonetic Representation for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.755) |  | 0 | Chinese Spelling Correction (CSC) aims to detect and correct erroneous characters in Chinese texts. Although efforts have been made to introduce phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic representations with character representations, which tends to weaken the representation effect of normal texts. In this work, we propose to disentangle the two types of features to allow for direct interaction between textual and phonetic information. To learn useful... | Zihong Liang, Xiaojun Quan, Qifan Wang |  |
| 1999 |  |  [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://doi.org/10.18653/v1/2023.acl-long.756) |  | 0 | Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first... | TaChung Chi, TingHan Fan, Alexander Rudnicky, Peter J. Ramadge |  |
| 2000 |  |  [CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models](https://doi.org/10.18653/v1/2023.acl-long.757) |  | 0 | redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored... | Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, Mykola Pechenizkiy |  |
| 2001 |  |  [Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback](https://doi.org/10.18653/v1/2023.acl-long.758) |  | 0 | Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect... | Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, YLan Boureau, Jason Weston |  |
| 2002 |  |  [Uncovering and Categorizing Social Biases in Text-to-SQL](https://doi.org/10.18653/v1/2023.acl-long.759) |  | 0 | Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social... | Yan Liu, Yan Gao, Zhe Su, Xiaokang Chen, Elliott Ash, JianGuang Lou |  |
| 2003 |  |  [On the Compositional Generalization in Versatile Open-domain Dialogue](https://doi.org/10.18653/v1/2023.acl-long.760) |  | 0 | Previous research has demonstrated the potential of multi-task learning to foster a conversational agent’s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse knowledge and skills learned from other datasets. In contrast to previous works, we develop a sparsely activated modular network: (1) We propose a well-rounded set of operators and instantiate each... | Tingchen Fu, Xueliang Zhao, Lemao Liu, Rui Yan |  |
| 2004 |  |  [What is the Real Intention behind this Question? Dataset Collection and Intention Classification](https://doi.org/10.18653/v1/2023.acl-long.761) |  | 0 | Asking and answering questions are inseparable parts of human social life. The primary purposes of asking questions are to gain knowledge or request help which has been the subject of question-answering studies. However, questions can also reflect negative intentions and include implicit offenses, such as highlighting one’s lack of knowledge or bolstering an alleged superior knowledge, which can lead to conflict in conversations; yet has been scarcely researched. This paper is the first study... | Maryam Sadat Mirzaei, Kourosh Meshgi, Satoshi Sekine |  |
| 2005 |  |  [Conjunct Resolution in the Face of Verbal Omissions](https://doi.org/10.18653/v1/2023.acl-long.762) |  | 0 | Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets,... | Royi Rassin, Yoav Goldberg, Reut Tsarfaty |  |
| 2006 |  |  [Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts](https://doi.org/10.18653/v1/2023.acl-long.763) |  | 0 | Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful... | Mounica Maddela, Megan Ung, Jing Xu, Andrea Madotto, Heather Foran, YLan Boureau |  |
| 2007 |  |  [Learning In-context Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.764) |  | 0 | Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function Lambda_instruction,... | Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han, Le Sun |  |
| 2008 |  |  [Holistic Prediction on a Time-Evolving Attributed Graph](https://doi.org/10.18653/v1/2023.acl-long.765) |  | 0 | Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribute prediction is independent, and fall short of predicting the appearance of new nodes that were not observed in the past. In this paper, we address two interrelated questions; (1) can we exploit task... | Shohei Yamasaki, Yuya Sasaki, Panagiotis Karras, Makoto Onizuka |  |
| 2009 |  |  [Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional Random Field](https://doi.org/10.18653/v1/2023.acl-long.766) |  | 0 | Prior works on joint Information Extraction (IE) typically model instance (e.g., event triggers, entities, roles, relations) interactions by representation enhancement, type dependencies scoring, or global decoding. We find that the previous models generally consider binary type dependency scoring of a pair of instances, and leverage local search such as beam search to approximate global solutions. To better integrate cross-instance interactions, in this work, we introduce a joint IE framework... | Zixia Jia, Zhaohui Yan, Wenjuan Han, Zilong Zheng, Kewei Tu |  |
| 2010 |  |  [Training Trajectories of Language Models Across Scales](https://doi.org/10.18653/v1/2023.acl-long.767) |  | 0 | Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125M to 175B parameters—on next-token prediction, sequence-level generation and... | Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, Veselin Stoyanov |  |
| 2011 |  |  [A Diverse Set of Freely Available Linguistic Resources for Turkish](https://doi.org/10.18653/v1/2023.acl-long.768) |  | 0 | This study presents a diverse set of freely available linguistic resources for Turkish natural language processing, including corpora, pretrained models and education material. Although Turkish is spoken by a sizeable population of over 80 million people, Turkish linguistic resources for natural language processing remain scarce. In this study, we provide corpora to allow practitioners to build their own applications and pretrained models that would assist industry researchers in creating quick... | Duygu Altinok |  |
| 2012 |  |  [Measuring Consistency in Text-based Financial Forecasting Models](https://doi.org/10.18653/v1/2023.acl-long.769) |  | 0 | Financial forecasting has been an important and active area of machine learning research, as even the most modest advantages in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models – their invariance under... | Linyi Yang, Yingpeng Ma, Yue Zhang |  |
| 2013 |  |  [Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.770) |  | 0 | Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning. In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are... | Nuno Miguel Guerreiro, Pierre Colombo, Pablo Piantanida, André F. T. Martins |  |
| 2014 |  |  [RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank](https://doi.org/10.18653/v1/2023.acl-long.771) |  | 0 | Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and pushing dissimilar ones away. However, these methods fail to capture the fine-grained ranking information among the sentences, where each sentence is only treated as either positive or negative. In many... | Jiduan Liu, Jiahao Liu, Qifan Wang, Jingang Wang, Wei Wu, Yunsen Xian, Dongyan Zhao, Kai Chen, Rui Yan |  |
| 2015 |  |  [Entailment as Robust Self-Learner](https://doi.org/10.18653/v1/2023.acl-long.772) |  | 0 | Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can... | Jiaxin Ge, Hongyin Luo, Yoon Kim, James R. Glass |  |
| 2016 |  |  [ReCode: Robustness Evaluation of Code Generation Models](https://doi.org/10.18653/v1/2023.acl-long.773) |  | 0 | Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in... | Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, Bing Xiang |  |
| 2017 |  |  [EPIC: Multi-Perspective Annotation of a Corpus of Irony](https://doi.org/10.18653/v1/2023.acl-long.774) |  | 0 | We present EPIC (English Perspectivist Irony Corpus), the first annotated corpus for irony analysis based on the principles of data perspectivism. The corpus contains short conversations from social media in five regional varieties of English, and it is annotated by contributors from five countries corresponding to those varieties. We analyse the resource along the perspectives induced by the diversity of the annotators, in terms of origin, age, and gender, and the relationship between these... | Simona Frenda, Alessandro Pedrani, Valerio Basile, Soda Marem Lo, Alessandra Teresa Cignarella, Raffaella Panizzon, Cristina Marco, Bianca Scarlini, Viviana Patti, Cristina Bosco, Davide Bernardi |  |
| 2018 |  |  [Dialogue Summarization with Static-Dynamic Structure Fusion Graph](https://doi.org/10.18653/v1/2023.acl-long.775) |  | 0 | Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years. Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web applications such as email thread summarization and meeting minutes draft. Dialogue summarization is a challenging task in that dialogue has dynamic interaction nature and presumably inconsistent... | Shen Gao, Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao, Rui Yan |  |
| 2019 |  |  [Large-Scale Correlation Analysis of Automated Metrics for Topic Models](https://doi.org/10.18653/v1/2023.acl-long.776) |  | 0 | Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling approach to mine topics for the purpose of metric evaluation, and conduct the analysis via three large corpora showing that certain automated coherence metrics are correlated. Moreover, we extend the... | Jia Peng Lim, Hady W. Lauw |  |
| 2020 |  |  [U-CREAT: Unsupervised Case Retrieval using Events extrAcTion](https://doi.org/10.18653/v1/2023.acl-long.777) |  | 0 | The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this... | Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella, Ashutosh Modi |  |
| 2021 |  |  [ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis](https://doi.org/10.18653/v1/2023.acl-long.778) |  | 0 | Argument Quality Detection is an emerging field in NLP which has seen significant recent development. However, existing datasets in this field suffer from a lack of quality, quantity and diversity of topics and arguments, specifically the presence of vague arguments that are not persuasive in nature. In this paper, we leverage a combined experience of 10+ years of Parliamentary Debating to create a dataset that covers significantly more topics and has a wide range of sources to capture more... | Omkar Joshi, Priya Pitre, Yashodhara Haribhakta |  |
| 2022 |  |  [Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework](https://doi.org/10.18653/v1/2023.acl-long.779) |  | 0 | Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance... | Mingqi Gao, Xiaojun Wan, Jia Su, Zhefeng Wang, Baoxing Huai |  |
| 2023 |  |  [Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker](https://doi.org/10.18653/v1/2023.acl-long.780) |  | 0 | Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time... | Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov |  |
| 2024 |  |  [Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text](https://doi.org/10.18653/v1/2023.acl-long.781) |  | 0 | Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment... | Ashim Gupta, Carter Wood Blum, Temma Choji, Yingjie Fei, Shalin Shah, Alakananda Vempala, Vivek Srikumar |  |
| 2025 |  |  [Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring](https://doi.org/10.18653/v1/2023.acl-long.782) |  | 0 | Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the pseudo-groundtruth, and then train a neural AES model by learning from the aggregation of these quality signals. To aggregate these inconsistent quality signals into a unified supervision, we view the AES... | Cong Wang, Zhiwei Jiang, Yafeng Yin, Zifeng Cheng, Shiping Ge, Qing Gu |  |
| 2026 |  |  [Mitigating Label Biases for In-context Learning](https://doi.org/10.18653/v1/2023.acl-long.783) |  | 0 | Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the... | Yu Fei, Yifan Hou, Zeming Chen, Antoine Bosselut |  |
| 2027 |  |  [QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations](https://doi.org/10.18653/v1/2023.acl-long.784) |  | 0 | Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for “shorebirds that are not sandpipers” or “science-fiction films shot in England”. To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The... | Chaitanya Malaviya, Peter Shaw, MingWei Chang, Kenton Lee, Kristina Toutanova |  |
| 2028 |  |  [Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-long.785) |  | 0 | Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and... | Yujie Wang, Hu Zhang, Jiye Liang, Ru Li |  |
| 2029 |  |  [Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation](https://doi.org/10.18653/v1/2023.acl-long.786) |  | 0 | Argument summarisation is a promising but currently under-explored field. Recent work has aimed to provide textual summaries in the form of concise and salient short texts, i.e., key points (KPs), in a task known as Key Point Analysis (KPA). One of the main challenges in KPA is finding high-quality key point candidates from dozens of arguments even in a small corpus. Furthermore, evaluating key points is crucial in ensuring that the automatically generated summaries are useful. Although... | Hao Li, Viktor Schlegel, Riza BatistaNavarro, Goran Nenadic |  |
| 2030 |  |  [Ambiguous Learning from Retrieval: Towards Zero-shot Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.787) |  | 0 | Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. Thus, minimizing the supervision effort is one of the key challenges in semantic parsing. In this paper, we propose the Retrieval as Ambiguous Supervision framework, in which we construct a retrieval system based on pretrained language models to collect high-coverage candidates. Assuming candidates always contain the correct ones, we convert... | Shan Wu, Chunlei Xin, Hongyu Lin, Xianpei Han, Cao Liu, Jiansong Chen, Fan Yang, Guanglu Wan, Le Sun |  |
| 2031 |  |  [Explicit Syntactic Guidance for Neural Text Generation](https://doi.org/10.18653/v1/2023.acl-long.788) |  | 0 | Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2)... | Yafu Li, Leyang Cui, Jianhao Yan, Yongjing Yin, Wei Bi, Shuming Shi, Yue Zhang |  |
| 2032 |  |  [What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric](https://doi.org/10.18653/v1/2023.acl-long.789) |  | 0 | Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier’s representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and... | Enrico Liscio, Oscar Araque, Lorenzo Gatti, Ionut Constantinescu, Catholijn M. Jonker, Kyriaki Kalimeri, Pradeep Kumar Murukannaiah |  |
| 2033 |  |  [Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning](https://doi.org/10.18653/v1/2023.acl-long.790) |  | 0 | The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models. Prior OOV word embedding learning methods failed to model complex word formation well. In this paper, we propose a novel graph-based relation mining method, namely GRM, for OOV word embedding learning. We first build a Word Relationship Graph (WRG) based on word formation and associate OOV words with their semantically relevant words, which can mine the relational... | Ziran Liang, Yuyin Lu, Hegang Chen, Yanghui Rao |  |
| 2034 |  |  [Multimodal Persona Based Generation of Comic Dialogs](https://doi.org/10.18653/v1/2023.acl-long.791) |  | 0 | We focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous works like DialoGPT, PersonaGPT and other dialog generation models encode two-party dialogues and do not account for the visual information. To the best of our knowledge we are the first to propose the... | Harsh Agrawal, Aditya Mishra, Manish Gupta, Mausam |  |
| 2035 |  |  [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://doi.org/10.18653/v1/2023.acl-long.792) |  | 0 | We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input... | Dongfu Jiang, Xiang Ren, Bill Yuchen Lin |  |
| 2036 |  |  [Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.793) |  | 0 | Existing controllable dialogue generation work focuses on the single-attribute control and lacks generalization capability to out-of-distribution multiple attribute combinations. In this paper, we explore the compositional generalization for multi-attribute controllable dialogue generation where a model can learn from seen attribute values and generalize to unseen combinations. We propose a prompt-based disentangled controllable dialogue generation model, DCG. It learns attribute concept... | Weihao Zeng, Lulu Zhao, Keqing He, Ruotong Geng, Jingang Wang, Wei Wu, Weiran Xu |  |
| 2037 |  |  [Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization](https://doi.org/10.18653/v1/2023.acl-long.794) |  | 0 | Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via... | Minghang Zheng, Shaogang Gong, Hailin Jin, Yuxin Peng, Yang Liu |  |
| 2038 |  |  [IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages](https://doi.org/10.18653/v1/2023.acl-long.795) |  | 0 | The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from them, and to date, there are no such systematic studies focused solely on English to Indian language MT. This... | Ananya B. Sai, Tanay Dixit, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre |  |
| 2039 |  |  [Weaker Than You Think: A Critical Look at Weakly Supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.796) |  | 0 | Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these... | Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow |  |
| 2040 |  |  [Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases](https://doi.org/10.18653/v1/2023.acl-long.797) |  | 0 | As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between... | Yingji Li, Mengnan Du, Xin Wang, Ying Wang |  |
| 2041 |  |  [Towards Understanding Omission in Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.798) |  | 0 | Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving... | Yicheng Zou, Kaitao Song, Xu Tan, Zhongkai Fu, Qi Zhang, Dongsheng Li, Tao Gui |  |
| 2042 |  |  [Python Code Generation by Asking Clarification Questions](https://doi.org/10.18653/v1/2023.acl-long.799) |  | 0 | Code generation from text requires understanding the user’s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description... | HaauSing Li, Mohsen Mesgar, André F. T. Martins, Iryna Gurevych |  |
| 2043 |  |  [A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports](https://doi.org/10.18653/v1/2023.acl-long.800) |  | 0 | In this paper, we address the challenge of discovering financial signals in narrative financial reports. As these documents are often lengthy and tend to blend routine information with new information, it is challenging for professionals to discern critical financial signals. To this end, we leverage the inherent nature of the year-to-year structure of reports to define a novel signal-highlighting task; more importantly, we propose a compare-and-contrast multistage pipeline that recognizes... | JiaHuei Ju, YuShiang Huang, ChengWei Lin, Che Lin, ChuanJu Wang |  |
| 2044 |  |  [Improving the robustness of NLI models with minimax training](https://doi.org/10.18653/v1/2023.acl-long.801) |  | 0 | Natural language inference (NLI) models are susceptible to learning shortcuts, i.e. decision rules that spuriously correlate with the label. As a result, they achieve high in-distribution performance, but fail to generalize to out-of-distribution samples where such correlations do not hold. In this paper, we present a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance without assuming prior knowledge of the shortcuts being... | Michalis Korakakis, Andreas Vlachos |  |
| 2045 |  |  [USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.802) |  | 0 | Most previous studies on Structured Sentiment Analysis (SSA) have cast it as a problem of bi-lexical dependency parsing, which cannot address issues of overlap and discontinuity simultaneously. In this paper, we propose a niche-targeting and effective solution. Our approach involves creating a novel bi-lexical dependency parsing graph, which is then converted to a unified 2D table-filling scheme, namely USSA. The proposed scheme resolves the kernel bottleneck of previous SSA methods by... | Zepeng Zhai, Hao Chen, Ruifan Li, Xiaojie Wang |  |
| 2046 |  |  [PAD-Net: An Efficient Framework for Dynamic Networks](https://doi.org/10.18653/v1/2023.acl-long.803) |  | 0 | Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model’s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and... | Shwai He, Liang Ding, Daize Dong, Boan Liu, Fuqiang Yu, Dacheng Tao |  |
| 2047 |  |  [Resolving Ambiguities in Text-to-Image Generative Models](https://doi.org/10.18653/v1/2023.acl-long.804) |  | 0 | Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in... | Ninareh Mehrabi, Palash Goyal, Apurv Verma, Jwala Dhamala, Varun Kumar, Qian Hu, KaiWei Chang, Richard S. Zemel, Aram Galstyan, Rahul Gupta |  |
| 2048 |  |  [Knowledge Unlearning for Mitigating Privacy Risks in Language Models](https://doi.org/10.18653/v1/2023.acl-long.805) |  | 0 | Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on... | Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, Minjoon Seo |  |
| 2049 |  |  [Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor](https://doi.org/10.18653/v1/2023.acl-long.806) |  | 0 | Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and... | Or Honovich, Thomas Scialom, Omer Levy, Timo Schick |  |
| 2050 |  |  [To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.807) |  | 0 | Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only under conservative shifts in data distribution and typically focus on a single component (i.e., retriever or reader) rather than an end-to-end system. This work proposes a more realistic end-to-end... | Dheeru Dua, Emma Strubell, Sameer Singh, Pat Verga |  |
| 2051 |  |  [A Survey for Efficient Open Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.808) |  | 0 | Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a... | Qin Zhang, Shangsi Chen, Dongkuan Xu, Qingqing Cao, Xiaojun Chen, Trevor Cohn, Meng Fang |  |
| 2052 |  |  [Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities](https://doi.org/10.18653/v1/2023.acl-long.809) |  | 0 | The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages. This, however, comes with certain challenges in script normalization, particularly where the speakers of a language in a bilingual community rely on another script or orthography to write their native language. This paper addresses the problem of script normalization for several such languages that are mainly written in a... | Sina Ahmadi, Antonios Anastasopoulos |  |
| 2053 |  |  [Compositional Generalization without Trees using Multiset Tagging and Latent Permutations](https://doi.org/10.18653/v1/2023.acl-long.810) |  | 0 | Seq2seq models have been shown to struggle with compositional generalization in semantic parsing, i.e. generalizing to unseen compositions of phenomena that the model handles correctly in isolation. We phrase semantic parsing as a two-step process: we first tag each input token with a multiset of output tokens. Then we arrange the tokens into an output sequence using a new way of parameterizing and predicting permutations. We formulate predicting a permutation as solving a regularized linear... | Matthias Lindemann, Alexander Koller, Ivan Titov |  |
| 2054 |  |  [ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.811) |  | 0 | Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts... | Xiao Xu, Bei Li, Chenfei Wu, ShaoYen Tseng, Anahita Bhiwandiwalla, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan |  |
| 2055 |  |  [Finding the Pillars of Strength for Multi-Head Attention](https://doi.org/10.18653/v1/2023.acl-long.812) |  | 0 | Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features... | Jinjie Ni, Rui Mao, Zonglin Yang, Han Lei, Erik Cambria |  |
| 2056 |  |  [Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation](https://doi.org/10.18653/v1/2023.acl-long.813) |  | 0 | Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for... | Yandan Zheng, Anran Hao, Anh Tuan Luu |  |
| 2057 |  |  [Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering](https://doi.org/10.18653/v1/2023.acl-long.814) |  | 0 | Explainable question answering (XQA) aims to answer a given question and provide an explanation why the answer is selected. Existing XQA methods focus on reasoning on a single knowledge source, e.g., structured knowledge bases, unstructured corpora, etc. However, integrating information from heterogeneous knowledge sources is essential to answer complex questions. In this paper, we propose to leverage question decomposing for heterogeneous knowledge integration, by breaking down a complex... | Jiajie Zhang, Shulin Cao, Tingjian Zhang, Xin Lv, Juanzi Li, Lei Hou, Jiaxin Shi, Qi Tian |  |
| 2058 |  |  [Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation](https://doi.org/10.18653/v1/2023.acl-long.815) |  | 0 | Despite recent advances in detecting fake news generated by neural models, their results are not readily applicable to effective detection of human-written disinformation. What limits the successful transfer between them is the sizable gap between machine-generated fake news and human-authored ones, including the notable differences in terms of style and underlying intent. With this in mind, we propose a novel framework for generating training examples that are informed by the known styles and... | KungHsiang Huang, Kathleen R. McKeown, Preslav Nakov, Yejin Choi, Heng Ji |  |
| 2059 |  |  [A Length-Extrapolatable Transformer](https://doi.org/10.18653/v1/2023.acl-long.816) |  | 0 | Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We... | Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei |  |
| 2060 |  |  [A Survey of Deep Learning for Mathematical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.817) |  | 0 | Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep... | Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, KaiWei Chang |  |
| 2061 |  |  [A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training](https://doi.org/10.18653/v1/2023.acl-long.818) |  | 0 | Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model... | Nitay Calderon, Subhabrata Mukherjee, Roi Reichart, Amir Kantor |  |
| 2062 |  |  [Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation](https://doi.org/10.18653/v1/2023.acl-long.819) |  | 0 | In this paper, we reconsider the problem of (partial) false negative samples from the Mutual Information (MI) Maximization perspective, the traditional contrastive loss (like InfoNCE loss) will equally push away the anchor of all positive samples and negative samples regardless of their possible semantic similarities. We theoretically show that InfoNCE loss will not only maximize the MI between the anchor and positive samples but minimize the MI between the anchor and false negative samples... | Chaoya Jiang, Wei Ye, Haiyang Xu, Songfang Huang, Fei Huang, Shikun Zhang |  |
| 2063 |  |  [Tell2Design: A Dataset for Language-Guided Floor Plan Generation](https://doi.org/10.18653/v1/2023.acl-long.820) |  | 0 | We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we... | Sicong Leng, Yang Zhou, Mohammed Haroon Dupty, Wee Sun Lee, Sam Joyce, Wei Lu |  |
| 2064 |  |  [Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-long.821) |  | 0 | Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation’s quality? In this paper, we build on the view that the quality of a human-annotated... | Bingsheng Yao, Prithviraj Sen, Lucian Popa, James A. Hendler, Dakuo Wang |  |
| 2065 |  |  [Rethinking Annotation: Can Language Learners Contribute?](https://doi.org/10.18653/v1/2023.acl-long.822) |  | 0 | Researchers have traditionally recruited native speakers to provide annotations for the widely used benchmark datasets. But there are languages for which recruiting native speakers is difficult, and it would help to get learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to the benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional... | Haneul Yoo, Rifki Afina Putri, Changyoon Lee, Youngin Lee, SoYeon Ahn, Dongyeop Kang, Alice Oh |  |
| 2066 |  |  [Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling](https://doi.org/10.18653/v1/2023.acl-long.823) |  | 0 | Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and external-information exploiting. First, we represent the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified... | Shengqiong Wu, Hao Fei, Yixin Cao, Lidong Bing, TatSeng Chua |  |
| 2067 |  |  [MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.824) |  | 0 | Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations... | Tao Shi, ShaoLun Huang |  |
| 2068 |  |  [Learning Language-Specific Layers for Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.825) |  | 0 | Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English).On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower.... | Telmo Pires, Robin M. Schmidt, YiHsiu Liao, Stephan Peitz |  |
| 2069 |  |  [Personality Understanding of Fictional Characters during Book Reading](https://doi.org/10.18653/v1/2023.acl-long.826) |  | 0 | Comprehending characters’ personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present... | Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Xiao Zhou, Fandong Meng, Jie Zhou |  |
| 2070 |  |  [StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing](https://doi.org/10.18653/v1/2023.acl-long.827) |  | 0 | Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an... | Xuekai Zhu, Jian Guan, Minlie Huang, Juan Liu |  |
| 2071 |  |  [Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.828) |  | 0 | Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes... | Qingyu Tan, Hwee Tou Ng, Lidong Bing |  |
| 2072 |  |  [Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings](https://doi.org/10.18653/v1/2023.acl-long.829) |  | 0 | Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit... | Daniel Rotem, Michael Hassid, Jonathan Mamou, Roy Schwartz |  |
| 2073 |  |  [Large Language Models Are Reasoning Teachers](https://doi.org/10.18653/v1/2023.acl-long.830) |  | 0 | Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates... | Namgyu Ho, Laura Schmid, SeYoung Yun |  |
| 2074 |  |  [Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations](https://doi.org/10.18653/v1/2023.acl-long.831) |  | 0 | Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits... | Wenting Zhao, Justin T. Chiu, Claire Cardie, Alexander M. Rush |  |
| 2075 |  |  [PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.acl-long.832) |  | 0 | We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves... | YauShian Wang, TaChung Chi, Ruohong Zhang, Yiming Yang |  |
| 2076 |  |  [Visually-augmented pretrained language models for NLP tasks without images](https://doi.org/10.18653/v1/2023.acl-long.833) |  | 0 | Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel... | Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Qinyu Zhang, JiRong Wen |  |
| 2077 |  |  [Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning](https://doi.org/10.18653/v1/2023.acl-long.834) |  | 0 | In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional... | Armineh Nourbakhsh, Sameena Shah, Carolyn P. Rosé |  |
| 2078 |  |  [A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization](https://doi.org/10.18653/v1/2023.acl-long.835) |  | 0 | To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar... | Lining Zhang, Simon Mille, Yufang Hou, Daniel Deutsch, Elizabeth Clark, Yixin Liu, Saad Mahamood, Sebastian Gehrmann, Miruna Clinciu, Khyathi Raghavi Chandu, João Sedoc |  |
| 2079 |  |  [TAVT: Towards Transferable Audio-Visual Text Generation](https://doi.org/10.18653/v1/2023.acl-long.836) |  | 0 | Audio-visual text generation aims to understand multi-modality contents and translate them into texts. Although various transfer learning techniques of text generation have been proposed, they focused on uni-modal analysis (e.g. text-to-text, visual-to-text) and lack consideration of multi-modal content and cross-modal relation. Motivated by the fact that humans can recognize the timbre of the same low-level concepts (e.g., footstep, rainfall, and laughing), even in different visual conditions,... | Wang Lin, Tao Jin, Wenwen Pan, Linjun Li, Xize Cheng, Ye Wang, Zhou Zhao |  |
| 2080 |  |  [MeetingQA: Extractive Question-Answering on Meeting Transcripts](https://doi.org/10.18653/v1/2023.acl-long.837) |  | 0 | With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long... | Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, Mohit Bansal |  |
| 2081 |  |  [FERMAT: An Alternative to Accuracy for Numerical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.838) |  | 0 | While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and... | Jasivan Alex Sivakumar, Nafise Sadat Moosavi |  |
| 2082 |  |  [Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.839) |  | 0 | Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for open-domain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance... | Sarah E. Finch, James D. Finch, Jinho D. Choi |  |
| 2083 |  |  [Decoder Tuning: Efficient Language Understanding as Decoding](https://doi.org/10.18653/v1/2023.acl-long.840) |  | 0 | With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in... | Ganqu Cui, Wentao Li, Ning Ding, Longtao Huang, Zhiyuan Liu, Maosong Sun |  |
| 2084 |  |  [The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources](https://doi.org/10.18653/v1/2023.acl-long.841) |  | 0 | Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model’s pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple... | Akshatha Arodi, Martin Pömsl, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, Jackie Chi Kit Cheung |  |
| 2085 |  |  [CREST: A Joint Framework for Rationalization and Counterfactual Text Generation](https://doi.org/10.18653/v1/2023.acl-long.842) |  | 0 | Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework... | Marcos V. Treviso, Alexis Ross, Nuno Miguel Guerreiro, André F. T. Martins |  |
| 2086 |  |  [Towards Unifying Multi-Lingual and Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.acl-long.843) |  | 0 | To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate... | Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, Jie Zhou |  |
| 2087 |  |  [On Improving Summarization Factual Consistency from Natural Language Feedback](https://doi.org/10.18653/v1/2023.acl-long.844) |  | 0 | Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto,... | Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Halfaker, Dragomir Radev, Ahmed Hassan Awadallah |  |
| 2088 |  |  [From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models](https://doi.org/10.18653/v1/2023.acl-long.845) |  | 0 | Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation... | Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap |  |
| 2089 |  |  [Exploring Large Language Models for Classical Philology](https://doi.org/10.18653/v1/2023.acl-long.846) |  | 0 | Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii)... | Frederick Riemenschneider, Anette Frank |  |
| 2090 |  |  [LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.847) |  | 0 | Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal... | Yi Tu, Ya Guo, Huan Chen, Jinyang Tang |  |
| 2091 |  |  [Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.848) |  | 0 | Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data... | Yuchen Hu, Ruizhe Li, Chen Chen, Chengwei Qin, QiuShi Zhu, Eng Siong Chng |  |
| 2092 |  |  [An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-long.849) |  | 0 | Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work,... | Xuancheng Huang, Zijun Liu, Peng Li, Tao Li, Maosong Sun, Yang Liu |  |
| 2093 |  |  [Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.850) |  | 0 | Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes, and are not designed well to learn representations of seen entities... | Hongcai Xu, Junpeng Bao, Wenbo Liu |  |
| 2094 |  |  [Dual Cache for Long Document Neural Coreference Resolution](https://doi.org/10.18653/v1/2023.acl-long.851) |  | 0 | Recent works show the effectiveness of cache-based neural coreference resolution models on long documents. These models incrementally process a long document from left to right and extract relations between mentions and entities in a cache, resulting in much lower memory and computation cost compared to computing all mentions in parallel. However, they do not handle cache misses when high-quality entities are purged from the cache, which causes wrong assignments and leads to prediction errors.... | Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang |  |
| 2095 |  |  [Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.852) |  | 0 | In the real-world scenario, a longstanding goal of multilingual neural machine translation (MNMT) is that a single model can incrementally adapt to new language pairs without accessing previous training data. In this scenario, previous studies concentrate on overcoming catastrophic forgetting while lacking encouragement to learn new knowledge from incremental language pairs, especially when the incremental language is not related to the set of original languages. To better acquire new... | Kaiyu Huang, Peng Li, Jin Ma, Ting Yao, Yang Liu |  |
| 2096 |  |  [DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media](https://doi.org/10.18653/v1/2023.acl-long.853) |  | 0 | Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language... | Mario Ezra Aragón, Adrián Pastor LópezMonroy, Luis Gonzalez, David E. Losada, Manuel Montes |  |
| 2097 |  |  [Toward Interactive Dictation](https://doi.org/10.18653/v1/2023.acl-long.854) |  | 0 | Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must... | Belinda Z. Li, Jason Eisner, Adam Pauls, Sam Thomson |  |
| 2098 |  |  [CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors](https://doi.org/10.18653/v1/2023.acl-long.855) |  | 0 | Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this... | Peng Li, Tianxiang Sun, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, Xipeng Qiu |  |
| 2099 |  |  [Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.856) |  | 0 | In this paper, we elaborate upon recipes for building multilingual representation models that are not only competitive with existing state-of-the-art models but are also more parameter efficient, thereby promoting better adoption in resource-constrained scenarios and practical applications. We show that going beyond English-centric bitexts, coupled with a novel sampling strategy aimed at reducing under-utilization of training data, substantially boosts performance across model sizes for both... | Barun Patra, Saksham Singhal, Shaohan Huang, Zewen Chi, Li Dong, Furu Wei, Vishrav Chaudhary, Xia Song |  |
| 2100 |  |  [Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.acl-long.857) |  | 0 | Open-retrieval conversational machine reading comprehension (OCMRC) simulates real-life conversational interaction scenes. Machines are required to make a decision of “Yes/No/Inquire” or generate a follow-up question when the decision is “Inquire” based on retrieved rule texts, user scenario, user question and dialogue history. Recent studies try to reduce the information gap between decision-making and question generation, in order to improve the performance of generation. However, the... | Xiao Zhang, Heyan Huang, Zewen Chi, XianLing Mao |  |
| 2101 |  |  [LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming](https://doi.org/10.18653/v1/2023.acl-long.858) |  | 0 | Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale social media data and large pre-trained models, there is no guarantee these agents could also perform well in fast-growing scenarios, such as live streaming, due to the bounded transferability of pre-trained models and biased distributions of public datasets from Reddit and Weibo, etc. To improve the essential capability of responding and establish a... | Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, Baoyuan Wang |  |
| 2102 |  |  [Prompting PaLM for Translation: Assessing Strategies and Performance](https://doi.org/10.18653/v1/2023.acl-long.859) |  | 0 | Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor.... | David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, George F. Foster |  |
| 2103 |  |  [Exploring Lottery Prompts for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.860) |  | 0 | Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery... | Yulin Chen, Ning Ding, Xiaobin Wang, Shengding Hu, Haitao Zheng, Zhiyuan Liu, Pengjun Xie |  |
| 2104 |  |  [A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations](https://doi.org/10.18653/v1/2023.acl-long.861) |  | 0 | Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous... | Wenjie Zheng, Jianfei Yu, Rui Xia, Shijin Wang |  |
| 2105 |  |  [TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline](https://doi.org/10.18653/v1/2023.acl-long.862) |  | 0 | Temporal knowledge graph embedding (TKGE) models are commonly utilized to infer the missing facts and facilitate reasoning and decision-making in temporal knowledge graph based systems. However, existing methods fuse temporal information into entities, potentially leading to the evolution of entity information and limiting the link prediction performance of TKG. Meanwhile, current TKGE models often lack the ability to simultaneously model important relation patterns and provide... | Jiang Li, Xiangdong Su, Guanglai Gao |  |
| 2106 |  |  [Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition](https://doi.org/10.18653/v1/2023.acl-long.863) |  | 0 | Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared... | Yuwei Bao, Barrett Martin Lattimer, Joyce Chai |  |
| 2107 |  |  [Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination](https://doi.org/10.18653/v1/2023.acl-long.864) |  | 0 | This paper confirms that, in English binary coordinations, left conjuncts tend to be shorter than right conjuncts, regardless of the position of the governor of the coordination. We demonstrate that this tendency becomes stronger when length differences are greater, but only when the governor is on the left or absent, not when it is on the right. We explain this effect via Dependency Length Minimization and we show that this explanation provides support for symmetrical dependency structures of... | Adam Przepiórkowski, Michal Wozniak |  |
| 2108 |  |  [LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development](https://doi.org/10.18653/v1/2023.acl-long.865) |  | 0 | In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models’ size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal... | Ilias Chalkidis, Nicolas Garneau, Catalina Goanta, Daniel Martin Katz, Anders Søgaard |  |
| 2109 |  |  [Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge](https://doi.org/10.18653/v1/2023.acl-long.866) |  | 0 | The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored in terms of effectively training NMT models with high CR abilities and devising accurate automatic evaluation metrics. This paper presents a comprehensive study aimed at expanding the understanding of... | Xuebo Liu, Yutong Wang, Derek F. Wong, Runzhe Zhan, Liangxuan Yu, Min Zhang |  |
| 2110 |  |  [NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models](https://doi.org/10.18653/v1/2023.acl-long.867) |  | 0 | Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream... | Kai Mei, Zheng Li, Zhenting Wang, Yang Zhang, Shiqing Ma |  |
| 2111 |  |  [Revisiting Relation Extraction in the era of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.868) |  | 0 | Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models... | Somin Wadhwa, Silvio Amir, Byron C. Wallace |  |
| 2112 |  |  [Pre-trained Language Models Can be Fully Zero-Shot Learners](https://doi.org/10.18653/v1/2023.acl-long.869) |  | 0 | How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only... | Xuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu, Lei Li |  |
| 2113 |  |  [Can Large Language Models Be an Alternative to Human Evaluations?](https://doi.org/10.18653/v1/2023.acl-long.870) |  | 0 | Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper,... | David ChengHan Chiang, Hungyi Lee |  |
| 2114 |  |  [HyperMixer: An MLP-based Low Cost Alternative to Transformers](https://doi.org/10.18653/v1/2023.acl-long.871) |  | 0 | Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the... | Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, François Marelli, François Fleuret, James Henderson |  |
| 2115 |  |  [UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units](https://doi.org/10.18653/v1/2023.acl-long.872) |  | 0 | Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and... | Hirofumi Inaguma, Sravya Popuri, Ilia Kulikov, PengJen Chen, Changhan Wang, YuAn Chung, Yun Tang, Ann Lee, Shinji Watanabe, Juan Pino |  |
| 2116 |  |  [Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression](https://doi.org/10.18653/v1/2023.acl-long.873) |  | 0 | In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground truth, it ignores the intrinsic uncertainty revealed by the inconsistent labels. This paper proposes a Bayesian approach, deep evidential emotion regression (DEER), to estimate the uncertainty in... | Wen Wu, Chao Zhang, Philip C. Woodland |  |
| 2117 |  |  [Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation](https://doi.org/10.18653/v1/2023.acl-long.874) |  | 0 | Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being... | Wei Liu, Michael Strube |  |
| 2118 |  |  [Plug-and-Play Document Modules for Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-long.875) |  | 0 | Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally inefficient. To this end, we target to decouple document encoding from downstream tasks, and propose to represent each document as a plug-and-play document module, i.e., a document plugin, for PTMs (PlugD). By... | Chaojun Xiao, Zhengyan Zhang, Xu Han, ChiMin Chan, Yankai Lin, Zhiyuan Liu, Xiangyang Li, Zhonghua Li, Zhao Cao, Maosong Sun |  |
| 2119 |  |  [An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.876) |  | 0 | The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to... | Zhongbin Xie, Thomas Lukasiewicz |  |
| 2120 |  |  [Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models](https://doi.org/10.18653/v1/2023.acl-long.877) |  | 0 | The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease. In this work, we first provide a... | Lijing Wang, Yingya Li, Timothy Miller, Steven Bethard, Guergana Savova |  |
| 2121 |  |  [A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models](https://doi.org/10.18653/v1/2023.acl-long.878) |  | 0 | Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language... | Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram |  |
| 2122 |  |  [Ranking-Enhanced Unsupervised Sentence Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.879) |  | 0 | Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that the semantic meaning of a sentence is also determined by nearest-neighbor sentences that are similar to the input sentence. Based on this finding, we propose a novel unsupervised sentence encoder,... | Yeon Seonwoo, Guoyin Wang, Changmin Seo, Sajal Choudhary, Jiwei Li, Xiang Li, Puyang Xu, Sunghyun Park, Alice Oh |  |
| 2123 |  |  [To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support](https://doi.org/10.18653/v1/2023.acl-long.880) |  | 0 | Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying argumentative claims in need of specific revisions. By learning from collaborative editing behaviors in online debates, we seek to capture implicit revision patterns in order to develop approaches... | Gabriella Skitalinskaya, Henning Wachsmuth |  |
| 2124 |  |  [Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments](https://doi.org/10.18653/v1/2023.acl-long.881) |  | 0 | We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on... | Ethan Mendes, Yang Chen, Wei Xu, Alan Ritter |  |
| 2125 |  |  [Composition-contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.882) |  | 0 | Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several... | Sachin Chanchani, Ruihong Huang |  |
| 2126 |  |  [Causes and Cures for Interference in Multilingual Translation](https://doi.org/10.18653/v1/2023.acl-long.883) |  | 0 | Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily... | Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, Shruti Bhosale |  |
| 2127 |  |  [Understanding and Bridging the Modality Gap for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.884) |  | 0 | How to achieve better end-to-end speech translation (ST) by leveraging (text) machine translation (MT) data? Among various existing techniques, multi-task learning is one of the effective ways to share knowledge between ST and MT in which additional MT data can help to learn source-to-target mapping. However, due to the differences between speech and text, there is always a gap between ST and MT. In this paper, we first aim to understand this modality gap from the target-side representation... | Qingkai Fang, Yang Feng |  |
| 2128 |  |  [Few-shot Reranking for Multi-hop QA via Language Model Prompting](https://doi.org/10.18653/v1/2023.acl-long.885) |  | 0 | We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question... | Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Lu Wang |  |
| 2129 |  |  [DICE: Data-Efficient Clinical Event Extraction with Generative Models](https://doi.org/10.18653/v1/2023.acl-long.886) |  | 0 | Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of... | Mingyu Derek Ma, Alexander Taylor, Wei Wang, Nanyun Peng |  |
| 2130 |  |  [XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations](https://doi.org/10.18653/v1/2023.acl-long.887) |  | 0 | Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22... | Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang |  |
| 2131 |  |  [INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.888) |  | 0 | Neural machine translation has achieved promising results on many translation tasks. However, previous studies have shown that neural models induce a non-smooth representation space, which harms its generalization results. Recently, kNN-MT has provided an effective paradigm to smooth the prediction based on neighbor representations during inference. Despite promising results, kNN-MT usually requires large inference overhead. We propose an effective training framework INK to directly smooth the... | Wenhao Zhu, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen |  |
| 2132 |  |  [Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.889) |  | 0 | Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise... | Qi Sun, Kun Huang, Xiaocui Yang, Pengfei Hong, Kun Zhang, Soujanya Poria |  |
| 2133 |  |  [Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning](https://doi.org/10.18653/v1/2023.acl-long.890) |  | 0 | The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the cross-modal information present in multimodal data. Such information from the visual modality, such as color, size, and shape, provides additional attributes that users can include in their inputs. Thus,... | Shivaen Ramshetty, Gaurav Verma, Srijan Kumar |  |
| 2134 |  |  [Crosslingual Generalization through Multitask Finetuning](https://doi.org/10.18653/v1/2023.acl-long.891) |  | 0 | Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the... | Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel |  |
| 2135 |  |  [Evaluate AMR Graph Similarity via Self-supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.892) |  | 0 | In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and inspired by learned similarity evaluation on plain text, we propose AMRSim, an automatic AMR graph similarity evaluation metric. To overcome the high cost of collecting human-annotated data, AMRSim... | Ziyi Shou, Fangzhen Lin |  |
| 2136 |  |  [Analyzing Transformers in Embedding Space](https://doi.org/10.18653/v1/2023.acl-long.893) |  | 0 | Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all... | Guy Dar, Mor Geva, Ankit Gupta, Jonathan Berant |  |
| 2137 |  |  [Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning](https://doi.org/10.18653/v1/2023.acl-long.894) |  | 0 | In this paper, we present a novel approach for data-to-text generation that addresses the limitations of current methods that primarily focus on specific types of structured data. Our proposed method aims to improve performance in multi-task training, zero-shot and few-shot scenarios by providing a unified representation that can handle various forms of structured data such as tables, knowledge graph triples, and meaning representations. We demonstrate that our proposed approach can effectively... | Alexander Hanbo Li, Mingyue Shang, Evangelia Spiliopoulou, Jie Ma, Patrick Ng, Zhiguo Wang, Bonan Min, William Yang Wang, Kathleen R. McKeown, Vittorio Castelli, Dan Roth, Bing Xiang |  |
| 2138 |  |  [FactKG: Fact Verification via Reasoning on Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.895) |  | 0 | In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many... | Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, Edward Choi |  |
| 2139 |  |  [DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains](https://doi.org/10.18653/v1/2023.acl-long.896) |  | 0 | In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare... | Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, PierreAntoine Gourraud |  |
| 2140 |  |  [Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.897) |  | 0 | Document-level Event Causality Identification (DECI) aims to extract causal relations between events in a document. It challenges conventional sentence-level task (SECI) with difficult long-text understanding. In this paper, we propose a novel DECI model (SENDIR) for better document-level reasoning. Different from existing works that build an event graph via linguistic tools, SENDIR does not require any prior knowledge. The basic idea is to discriminate event pairs in the same sentence or span... | Changsen Yuan, Heyan Huang, Yixin Cao, Yonggang Wen |  |
| 2141 |  |  [Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks](https://doi.org/10.18653/v1/2023.acl-long.898) |  | 0 | The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly due to limited datasets. Existing datasets suffer from a lack of fine-grained annotations, such as the toxic type and expressions with indirect toxicity. These fine-grained annotations are crucial factors for accurately detecting the toxicity of posts involved with lexical knowledge, which has been a challenge for researchers.... | Junyu Lu, Bo Xu, Xiaokun Zhang, Changrong Min, Liang Yang, Hongfei Lin |  |
| 2142 |  |  [SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations](https://doi.org/10.18653/v1/2023.acl-long.899) |  | 0 | We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of... | PaulAmbroise Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswami, Changhan Wang, Juan Pino, Benoît Sagot, Holger Schwenk |  |
| 2143 |  |  [Character-Aware Models Improve Visual Text Rendering](https://doi.org/10.18653/v1/2023.acl-long.900) |  | 0 | Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word’s visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a... | Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, Noah Constant |  |
| 2144 |  |  [IDRISI-RA: The First Arabic Location Mention Recognition Dataset of Disaster Tweets](https://doi.org/10.18653/v1/2023.acl-long.901) |  | 0 | Extracting geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, in locating incidents for planning rescue activities, and affected people for evacuation. Nevertheless, geolocation extraction is greatly understudied for the low resource languages such as Arabic. To fill this gap, we introduce IDRISI-RA, the first publicly-available Arabic Location Mention Recognition (LMR) dataset that provides human- and... | Reem Suwaileh, Muhammad Imran, Tamer Elsayed |  |
| 2145 |  |  [FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.902) |  | 0 | Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To... | Tianshuo Peng, Zuchao Li, Lefei Zhang, Bo Du, Hai Zhao |  |
| 2146 |  |  [What Do NLP Researchers Believe? Results of the NLP Community Metasurvey](https://doi.org/10.18653/v1/2023.acl-long.903) |  | 0 | We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In... | Julian Michael, Ari Holtzman, Alicia Parrish, Aaron Mueller, Alex Wang, Angelica Chen, Divyam Madaan, Nikita Nangia, Richard Yuanzhe Pang, Jason Phang, Samuel R. Bowman |  |
| 2147 |  |  [Prototype-Guided Pseudo Labeling for Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2023.acl-long.904) |  | 0 | Semi-supervised text classification (SSTC) aims at text classification with few labeled data and massive unlabeled data. Recent works achieve this task by pseudo-labeling methods, with the belief that the unlabeled and labeled data have identical data distribution, and assign the unlabeled data with pseudo-labels as additional supervision. However, existing pseudo-labeling methods usually suffer from ambiguous categorical boundary issues when training the pseudo-labeling phase, and simply... | Weiyi Yang, Richong Zhang, Junfan Chen, Lihong Wang, Jaein Kim |  |
| 2148 |  |  [LENS: A Learnable Evaluation Metric for Text Simplification](https://doi.org/10.18653/v1/2023.acl-long.905) |  | 0 | Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and... | Mounica Maddela, Yao Dou, David Heineman, Wei Xu |  |
| 2149 |  |  [MeetingBank: A Benchmark Dataset for Meeting Summarization](https://doi.org/10.18653/v1/2023.acl-long.906) |  | 0 | As the number of recorded meetings increases, it becomes increasingly important to utilize summarization technology to create useful summaries of these recordings. However, there is a crucial lack of annotated meeting corpora for developing this technology, as it can be hard to collect meetings, especially when the topics discussed are confidential. Furthermore, meeting summaries written by experienced writers are scarce, making it hard for abstractive summarizers to produce sensible output... | Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, Fei Liu |  |
| 2150 |  |  [UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective](https://doi.org/10.18653/v1/2023.acl-long.907) |  | 0 | We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can... | Yang Ping, Junyu Lu, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Pingjian Zhang, Jiaxing Zhang |  |
| 2151 |  |  [DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification](https://doi.org/10.18653/v1/2023.acl-long.908) |  | 0 | Text simplification is an intralingual translation task in which documents, or sentences of a complex source text are simplified for a target audience. The success of automatic text simplification systems is highly dependent on the quality of parallel data used for training and evaluation. To advance sentence simplification and document simplification in German, this paper presents DEplain, a new dataset of parallel, professionally written and manually aligned simplifications in plain German... | Regina Stodden, Omar Momen, Laura Kallmeyer |  |
| 2152 |  |  [A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text](https://doi.org/10.18653/v1/2023.acl-long.909) |  | 0 | Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer... | Yunxin Li, Baotian Hu, Yuxin Ding, Lin Ma, Min Zhang |  |
| 2153 |  |  [RARR: Researching and Revising What Language Models Say, Using Language Models](https://doi.org/10.18653/v1/2023.acl-long.910) |  | 0 | Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and... | Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, DaCheng Juan, Kelvin Guu |  |
