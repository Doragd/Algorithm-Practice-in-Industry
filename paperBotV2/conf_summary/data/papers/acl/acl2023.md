# ACL2023

## 会议论文列表

本会议共有 2153 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Frontmatter](https://aclanthology.org/2023.acl-demo.0) |  | 0 |  |  |  |
| 2 |  |  [Human-in-the-loop Schema Induction](https://doi.org/10.18653/v1/2023.acl-demo.1) |  | 0 | Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by... | Chris CallisonBurch, Hainiu Xu, Heng Ji, Isaac Tham, Jiaxuan Ren, Lara J. Martin, Leon Zhou, Li Zhang, Martha Palmer, Reece Suchocki, Rotem Dror, Sha Li, Susan Windisch Brown, Tianyi Zhang, Zhaoyi Hou |  |
| 3 |  |  [PersLEARN: Research Training through the Lens of Perspective Cultivation](https://doi.org/10.18653/v1/2023.acl-demo.2) |  | 0 | Scientific research is inherently shaped by its authors’ perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own... | Bingru He, Jiawen Liu, Jinhao Ji, Lecheng Ruan, Lin Qiu, Qiao Xu, Shiqian Li, Shiyu Gu, Sijia Liu, Wenjuan Han, Xinyang Li, Xinyi Niu, Xinyu Zhao, Yidong Lyu, Yifan Xu, Yixin Zhu, YuZhe Shi, Yuxi Ma, Zhen Li, Zijian Zhao |  |
| 4 |  |  [LAVIS: A One-stop Library for Language-Vision Intelligence](https://doi.org/10.18653/v1/2023.acl-demo.3) |  | 0 | We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future... | Dongxu Li, Guangsen Wang, Hung Le, Junnan Li, Silvio Savarese, Steven C. H. Hoi |  |
| 5 |  |  [Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models](https://doi.org/10.18653/v1/2023.acl-demo.4) |  | 0 | Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful... | Bum Chul Kwon, Nandana Mihindukulasooriya |  |
| 6 |  |  [PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development](https://doi.org/10.18653/v1/2023.acl-demo.5) |  | 0 | The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce... | Avi Sil, Bhavani Iyer, J. Scott McCarley, Jaydeep Sen, Jürgen Broß, Kshitij Fadnis, Martin Franz, Md. Arafat Sultan, Mihaela A. Bornea, Radu Florian, Riyaz A. Bhat, Rong Zhang, Salim Roukos, Sara Rosenthal, Vishwajeet Kumar, Yulong Li |  |
| 7 |  |  [Lingxi: A Diversity-aware Chinese Modern Poetry Generation System](https://doi.org/10.18653/v1/2023.acl-demo.6) |  | 0 | Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and... | Jiafeng Liu, Maosong Sun, Xiaobing Li, Xinran Zhang |  |
| 8 |  |  [Autodive: An Integrated Onsite Scientific Literature Annotation Tool](https://doi.org/10.18653/v1/2023.acl-demo.7) |  | 0 | Scientific literature is always available in Adobe’s Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper,... | Dongze Song, Ludi Wang, Mengyi Huang, Wenjuan Cui, Yi Du, Yuanchun Zhou |  |
| 9 |  |  [A Practical Toolkit for Multilingual Question and Answer Generation](https://doi.org/10.18653/v1/2023.acl-demo.8) |  | 0 | Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 10 |  |  [OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-demo.9) |  | 0 | Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible... | Libo Qin, Qiguang Chen, Wanxiang Che, Xiao Xu, Yunlong Feng |  |
| 11 |  |  [SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes](https://doi.org/10.18653/v1/2023.acl-demo.10) |  | 0 | We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems... | Anshul Agarwal, Jivnesh Sandhan, Laxmidhar Behera, Pawan Goyal, Tushar Sandhan |  |
| 12 |  |  [LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models](https://doi.org/10.18653/v1/2023.acl-demo.11) |  | 0 | Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation... | Victor Dibia |  |
| 13 |  |  [MetaPro Online: A Computational Metaphor Processing Online System](https://doi.org/10.18653/v1/2023.acl-demo.12) |  | 0 | Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via... | Erik Cambria, Kai He, Mengshi Ge, Rui Mao, Xiao Li |  |
| 14 |  |  [DIAGRAPH: An Open-Source Graphic Interface for Dialog Flow Design](https://doi.org/10.18653/v1/2023.acl-demo.13) |  | 0 | In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g.,... | Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu |  |
| 15 |  |  [disco: a toolkit for Distributional Control of Generative Models](https://doi.org/10.18653/v1/2023.acl-demo.14) |  | 0 | Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations,... | Germán Kruszewski, Jos Rozen, Marc Dymetman |  |
| 16 |  |  [A Hyperparameter Optimization Toolkit for Neural Machine Translation Research](https://doi.org/10.18653/v1/2023.acl-demo.15) |  | 0 | Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while... | Kevin Duh, Paul McNamee, Xuan Zhang |  |
| 17 |  |  [Japanese-to-English Simultaneous Dubbing Prototype](https://doi.org/10.18653/v1/2023.acl-demo.16) |  | 0 | Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces... | Eiichiro Sumita, Masao Utiyama, Xiaolin Wang |  |
| 18 |  |  [VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-demo.17) |  | 0 | We present Visual Knowledge oriented Programming platform (VisKoP), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language... | Amy Xin, Hailong Jin, Jianjun Xu, Jifan Yu, Juanzi Li, Lei Hou, Peng Zhang, Shulin Cao, Xin Lv, Yuanyong Chen, Zijun Yao |  |
| 19 |  |  [PEEP-Talk: A Situational Dialogue-based Chatbot for English Education](https://doi.org/10.18653/v1/2023.acl-demo.18) |  | 0 | English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a... | Bernardo Yahya, Chanjun Park, Heuiseok Lim, Hyeonseok Moon, Jaehyung Seo, Jungseob Lee, Seounghoon Lee, Seungjun Lee, Sugyeong Eo, Yoonna Jang |  |
| 20 |  |  [OpenTIPE: An Open-source Translation Framework for Interactive Post-Editing Research](https://doi.org/10.18653/v1/2023.acl-demo.19) |  | 0 | Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore,... | Fabian Landwehr, Laura Mascarell, Thomas Steinmann |  |
| 21 |  |  [TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities](https://doi.org/10.18653/v1/2023.acl-demo.20) |  | 0 | Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different... | Chen Chen, Cheng Hou, Feifei Li, Han Guo, Haoyan Liu, Jing Zhao, Kimmo Yan, Linlin Shen, Liqun Liu, Ningyuan Sun, Rong Tian, Shan Huang, Sihong Chen, Taiqiang Wu, Tao Zhu, Weigang Guo, Weijie Liu, Weiquan Mao, Wenhang Shi, Xiaoshuai Chen, Xiaoyong Du, Xingwu Sun, Yiren Chen, Yudong Li, Zhanhui Kang, Zhe Zhao |  |
| 22 |  |  [NeuroX Library for Neuron Analysis of Deep NLP Models](https://doi.org/10.18653/v1/2023.acl-demo.21) |  | 0 | Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural... | Fahim Dalvi, Hassan Sajjad, Nadir Durrani |  |
| 23 |  |  [SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation](https://doi.org/10.18653/v1/2023.acl-demo.22) |  | 0 | Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose... | Nianlong Gu, Richard H. R. Hahnloser |  |
| 24 |  |  [Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search](https://doi.org/10.18653/v1/2023.acl-demo.23) |  | 0 | In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100... | Chris Jenkins, Elizabeth Boschee, Joel Barry, Shantanu Agarwal, Steven Fincke |  |
| 25 |  |  [YANMTT: Yet Another Neural Machine Translation Toolkit](https://doi.org/10.18653/v1/2023.acl-demo.24) |  | 0 | In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning... | Chinmay Sawant, Diptesh Kanojia, Eiichiro Sumita, Raj Dabre |  |
| 26 |  |  [XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models](https://doi.org/10.18653/v1/2023.acl-demo.25) |  | 0 | NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on... | Aaron Chan, Akshen Kadakia, Brihi Joshi, DongHo Lee, Jay Pujara, Kiran Narahari, Ryosuke Mitani, Takashi Shibuya, Toshiyuki Sekiya, Xiang Ren, Ziyi Liu |  |
| 27 |  |  [OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-demo.26) |  | 0 | The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as... | Maosong Sun, Ning Ding, Shengding Hu, Weilin Zhao, Xingtai Lv, Zhen Zhang, Zhiyuan Liu |  |
| 28 |  |  [Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation](https://doi.org/10.18653/v1/2023.acl-demo.27) |  | 0 | Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what’s available, and achance to dive deeper on... | Hillel TaubTabib, Itay Yair, Yoav Goldberg |  |
| 29 |  |  [CARE: Collaborative AI-Assisted Reading Environment](https://doi.org/10.18653/v1/2023.acl-demo.28) |  | 0 | Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and... | Dennis Zyska, Ilia Kuznetsov, Iryna Gurevych, Jan Buchmann, Nils Dycke |  |
| 30 |  |  [The ROOTS Search Tool: Data Transparency for LLMs](https://doi.org/10.18653/v1/2023.acl-demo.29) |  | 0 | ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus... | Aleksandra Piktus, Anna Rogers, Christopher Akiki, Gérard Dupont, Hugo Laurençon, Paulo Villegas, Sasha Luccioni, Yacine Jernite |  |
| 31 |  |  [The OPUS-MT Dashboard - A Toolkit for a Systematic Evaluation of Open Machine Translation Models](https://doi.org/10.18653/v1/2023.acl-demo.30) |  | 0 | The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house... | Jörg Tiedemann, Ona de Gibert |  |
| 32 |  |  [The D-WISE Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis](https://doi.org/10.18653/v1/2023.acl-demo.31) |  | 0 | This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in... | Chris Biemann, Florian Schneider, Fynn PetersenFrey, Gertraud Koch, Isabel Eiser, Tim Fischer |  |
| 33 |  |  [OpenRT: An Open-source Framework for Reasoning Over Tabular Data](https://doi.org/10.18653/v1/2023.acl-demo.32) |  | 0 | There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to... | Arman Cohan, Boyu Mi, Dragomir Radev, Linyong Nan, Minghao Guo, Yilun Zhao, Zhenting Qi |  |
| 34 |  |  [UINAUIL: A Unified Benchmark for Italian Natural Language Understanding](https://doi.org/10.18653/v1/2023.acl-demo.33) |  | 0 | This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes... | Alessio Bosca, Cristina Bosco, Livio Bioglio, Valerio Basile, Viviana Patti |  |
| 35 |  |  [Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction](https://doi.org/10.18653/v1/2023.acl-demo.34) |  | 0 | The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years.... | Alberto Purpura, Gabriele Picco, Leopold Fuchs, Marcos Martínez Galindo, Thanh Lam Hoang, Vanessa López |  |
| 36 |  |  [BiSync: A Bilingual Editor for Synchronized Monolingual Texts](https://doi.org/10.18653/v1/2023.acl-demo.35) |  | 0 | In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies.... | François Yvon, Jitao Xu, Josep Maria Crego |  |
| 37 |  |  [Riveter: Measuring Power and Social Dynamics Between Entities](https://doi.org/10.18653/v1/2023.acl-demo.36) |  | 0 | Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a... | Anjalie Field, Jimin Mun, Lauren F. Klein, Maarten Sap, Maria Antoniak, Melanie Walsh |  |
| 38 |  |  [Fast Whitespace Correction with Encoder-Only Transformers](https://doi.org/10.18653/v1/2023.acl-demo.37) |  | 0 | The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a... | Hannah Bast, Matthias Hertel, Sebastian Walter |  |
| 39 |  |  [ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit](https://doi.org/10.18653/v1/2023.acl-demo.38) |  | 0 | ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech... | Brian Yan, Dan Berrebbi, Hirofumi Inaguma, Jiatong Shi, Juan Pino, Moto Hira, Patrick Fernandes, Peter Polak, Shinji Watanabe, Siddharth Dalmia, Soumi Maiti, Tomoki Hayashi, Xiaohui Zhang, Yifan Peng, Yun Tang, Zhaoheng Ni |  |
| 40 |  |  [CB2: Collaborative Natural Language Interaction Research Platform](https://doi.org/10.18653/v1/2023.acl-demo.39) |  | 0 | CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at... | Jacob Sharf, Mustafa Omer Gul, Yoav Artzi |  |
| 41 |  |  [Inseq: An Interpretability Toolkit for Sequence Generation Models](https://doi.org/10.18653/v1/2023.acl-demo.40) |  | 0 | Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of... | Gabriele Sarti, Ludwig Sickert, Nils Feldhus, Oskar van der Wal |  |
| 42 |  |  [Pipeline for modeling causal beliefs from natural language](https://doi.org/10.18653/v1/2023.acl-demo.41) |  | 0 | We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims... | Fred Morstatter, Ishaan Verma, John Priniski |  |
| 43 |  |  [TabGenie: A Toolkit for Table-to-Text Generation](https://doi.org/10.18653/v1/2023.acl-demo.42) |  | 0 | Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie – a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text... | Ekaterina Garanina, Ondrej Dusek, Ondrej Plátek, Zdenek Kasner |  |
| 44 |  |  [An Efficient Conversational Smart Compose System](https://doi.org/10.18653/v1/2023.acl-demo.43) |  | 0 | Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from... | Bowen Tan, Jindong Chen, Lei Shu, Lijuan Liu, Maria Wang, Ning Ruan, Xiayu Chen, Xinying Song, Yun Zhu |  |
| 45 |  |  [Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals](https://doi.org/10.18653/v1/2023.acl-demo.44) |  | 0 | We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from... | Afra Amini, Mennatallah ElAssady, Robin Chan |  |
| 46 |  |  [LaTeX2Solver: a Hierarchical Semantic Parsing of LaTeX Document into Code for an Assistive Optimization Modeling Application](https://doi.org/10.18653/v1/2023.acl-demo.45) |  | 0 | We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an... | Kun Mao, Linzi Xing, Mahdi Mostajabdaveh, Ren Li, Rindra Ramamonjison, Timothy T. L. Yu, Xiaojin Fu, Xiaorui Li, Xiongwei Han, Yong Zhang, Yuanzhe Chen |  |
| 47 |  |  [Alfred: A System for Prompted Weak Supervision](https://doi.org/10.18653/v1/2023.acl-demo.46) |  | 0 | Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural... | Peilin Yu, Stephen H. Bach |  |
| 48 |  |  [OpenICL: An Open-Source Framework for In-context Learning](https://doi.org/10.18653/v1/2023.acl-demo.47) |  | 0 | In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the... | Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yaoxiang Wang, Yu Qiao, Zhenyu Wu, Zhiyong Wu |  |
| 49 |  |  [Self-Supervised Sentence Polishing by Adding Engaging Modifiers](https://doi.org/10.18653/v1/2023.acl-demo.48) |  | 0 | Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large... | Bo Liu, Jian Guan, Minlie Huang, Xin Cui, Yu Ran, Zhexin Zhang |  |
| 50 |  |  [Effidit: An Assistant for Improving Writing Efficiency](https://doi.org/10.18653/v1/2023.acl-demo.49) |  | 0 | Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and... | Chenyan Huang, Deng Cai, Duyu Tang, Enbo Zhao, Guoping Huang, Haiyun Jiang, Kaiqiang Song, Leyang Cui, Longyue Wang, Piji Li, Shuming Shi, Wei Bi, Xinting Huang, Yan Wang |  |
| 51 |  |  [WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings](https://doi.org/10.18653/v1/2023.acl-demo.50) |  | 0 | Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings... | Duen Horng Chau, Fred Hohman, Zijie J. Wang |  |
| 52 |  |  [A System for Answering Simple Questions in Multiple Languages](https://doi.org/10.18653/v1/2023.acl-demo.51) |  | 0 | Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph... | Alexander Panchenko, Anton Razzhigaev, Mikhail Salnikov, Pavel Braslavski, Valentin Malykh |  |
| 53 |  |  [KWJA: A Unified Japanese Analyzer Based on Foundation Models](https://doi.org/10.18653/v1/2023.acl-demo.52) |  | 0 | We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.KWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS... | Daisuke Kawahara, Hirokazu Kiyomaru, Kazumasa Omura, Nobuhiro Ueda, Sadao Kurohashi, Takashi Kodama, Yugo Murawaki |  |
| 54 |  |  [Disease Network Constructor: a Pathway Extraction and Visualization](https://doi.org/10.18653/v1/2023.acl-demo.53) |  | 0 | We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific... | Goran Topic, Hiroya Takamura, Ikeda Masami, Khoa Duong, Mari Nogami Itoh, Masakata Kuroda, Mohammad Golam Sohrab, Nozomi Nagano, Yayoi NatsumeKitatani |  |
| 55 |  |  [Petals: Collaborative Inference and Fine-tuning of Large Models](https://doi.org/10.18653/v1/2023.acl-demo.54) |  | 0 | Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers.... | Alexander Borzunov, Artem Chumachenko, Colin Raffel, Dmitry Baranchuk, Maksim Riabinin, Pavel Samygin, Tim Dettmers, Younes Belkada |  |
| 56 |  |  [UKP-SQuARE v3: A Platform for Multi-Agent QA Research](https://doi.org/10.18653/v1/2023.acl-demo.55) |  | 0 | The continuous development of Question Answering (QA) datasets has drawn the research community’s attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single... | Haishuo Fang, Hao Zhang, Haritz Puerto, Iryna Gurevych, Kexin Wang, Rachneet Sachdeva, Sewin Tariverdian, Tim Baumgärtner |  |
| 57 |  |  [Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation](https://doi.org/10.18653/v1/2023.acl-demo.56) |  | 0 | In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and... | Mete Sertkan, Sebastian Hofstätter, Sophia Althammer |  |
| 58 |  |  [GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration](https://doi.org/10.18653/v1/2023.acl-demo.57) |  | 0 | Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling... | Akintunde Oladipo, Aleksandra Piktus, Christopher Akiki, Hailey Schoelkopf, Jimmy Lin, Martin Potthast, Odunayo Ogundepo, Stella Biderman, Xinyu Zhang |  |
| 59 |  |  [DeepPavlov Dream: Platform for Building Generative AI Assistants](https://doi.org/10.18653/v1/2023.acl-demo.58) |  | 0 | An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It... | Daniel Kornev, Diliara Zharikova, Dmitry Evseev, Dmitry Karpov, Dmitry Kosenko, Fedor Ignatov, Ksenya Petukhova, Maxim Talimanchuk, Mikhail Burtsev, Veronika Smilga, Yana Shishkina |  |
| 60 |  |  [Frontmatter](https://aclanthology.org/2023.acl-industry.0) |  | 0 |  |  |  |
| 61 |  |  [CWSeg: An Efficient and General Approach to Chinese Word Segmentation](https://doi.org/10.18653/v1/2023.acl-industry.1) |  | 0 | In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in... | Dedong Li, Fei Tan, Rui Zhao |  |
| 62 |  |  ["Knowledge is Power": Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation](https://doi.org/10.18653/v1/2023.acl-industry.2) |  | 0 | In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error.... | Aditya Shetty, Kaveri Kale, Kush Shrivastava, Milind Gune, Pushpak Bhattacharyya, Rustom Lawyer, Spriha Biswas |  |
| 63 |  |  [Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications](https://doi.org/10.18653/v1/2023.acl-industry.3) |  | 0 | Patent applicants write patent specificationsthat describe embodiments of inventions. Some embodiments are claimed for a patent,while others may be unclaimeddue to strategic considerations. Unclaimed embodiments may be extracted byapplicants later and claimed incontinuing applications togain... | Chikara Hashimoto, Gautam Kumar, Jun Suzuki, Shuichiro Hashimoto |  |
| 64 |  |  [MathPrompter: Mathematical Reasoning using Large Language Models](https://doi.org/10.18653/v1/2023.acl-industry.4) |  | 0 | Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs.... | Harsh Shrivastava, Liang Du, Shima Imani |  |
| 65 |  |  [Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems](https://doi.org/10.18653/v1/2023.acl-industry.5) |  | 0 | Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making... | Mohammad Kachuee, Sungjin Lee |  |
| 66 |  |  [pNLP-Mixer: an Efficient all-MLP Architecture for Language](https://doi.org/10.18653/v1/2023.acl-industry.6) |  | 0 | Large pre-trained language models based on transformer architectureƒhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and... | Damian Pascual, Diego Antognini, Francesco Fusco, Peter W. J. Staar |  |
| 67 |  |  [Extracting Text Representations for Terms and Phrases in Technical Domains](https://doi.org/10.18653/v1/2023.acl-industry.7) |  | 0 | Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to... | Diego Antognini, Francesco Fusco |  |
| 68 |  |  [CocaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval](https://doi.org/10.18653/v1/2023.acl-industry.8) |  | 0 | Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial... | Chengyu Wang, Jiapeng Wang, Jun Huang, Lianwen Jin, Xiaodan Wang |  |
| 69 |  |  [KG-FLIP: Knowledge-guided Fashion-domain Language-Image Pre-training for E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.9) |  | 0 | Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without... | Bryan Wang, Daoping Wu, Huidong Liu, Jinmiao Fu, Qinjin Jia, Roland Vollgraf, Shaoyuan Xu, Yang Liu |  |
| 70 |  |  [Domain-specific transformer models for query translation](https://doi.org/10.18653/v1/2023.acl-industry.10) |  | 0 | Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products. Due to a large non-English speaking population in India, we observe... | Anusua Trivedi, Mandar Kulkarni, Nikesh Garera |  |
| 71 |  |  [Label efficient semi-supervised conversational intent classification](https://doi.org/10.18653/v1/2023.acl-industry.11) |  | 0 | To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A... | Anusua Trivedi, Kyung Kim, Mandar Kulkarni, Nikesh Garera |  |
| 72 |  |  [xPQA: Cross-Lingual Product Question Answering in 12 Languages](https://doi.org/10.18653/v1/2023.acl-industry.12) |  | 0 | Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers’ questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product... | Adrià de Gispert, Akari Asai, Bill Byrne, Xiaoyu Shen |  |
| 73 |  |  [Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection](https://doi.org/10.18653/v1/2023.acl-industry.13) |  | 0 | Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on... | Beizhe Hu, Danding Wang, Juan Cao, Qiang Sheng, Yongchun Zhu, Zhengjia Wang, Zhiwei Jin |  |
| 74 |  |  [AVEN-GR: Attribute Value Extraction and Normalization using product GRaphs](https://doi.org/10.18653/v1/2023.acl-industry.14) |  | 0 | Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient... | Donato Crisostomi, Thomas Ricatte |  |
| 75 |  |  [GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model](https://doi.org/10.18653/v1/2023.acl-industry.15) |  | 0 | Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world... | Jie Tang, Peng Zhang, Shicheng Tan, Shu Zhao, Weng Lam Tam, Wenwen Gong, Yuanchun Wang |  |
| 76 |  |  [FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph](https://doi.org/10.18653/v1/2023.acl-industry.16) |  | 0 | Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained... | Ben Chen, Chengyu Wang, Jun Huang, Lei Li, Linbo Jin, Ming Gao, Xiaodan Wang, Yanghua Xiao, Zhixu Li |  |
| 77 |  |  [Entity Contrastive Learning in a Large-Scale Virtual Assistant System](https://doi.org/10.18653/v1/2023.acl-industry.17) |  | 0 | Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of... | George Leung, Jason Crowley, Jonathan Rubin, Maria Minakova, Morteza Ziyadi |  |
| 78 |  |  [Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog](https://doi.org/10.18653/v1/2023.acl-industry.18) |  | 0 | Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text... | Binxuan Huang, Chenwei Zhang, Kewei Cheng, Xian Li, Xin Luna Dong, Yifan Ethan Xu, Yizhou Sun, Zhengyang Wang |  |
| 79 |  |  [Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs](https://doi.org/10.18653/v1/2023.acl-industry.19) |  | 0 | Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately... | Abishek Komma, Angeliki Metallinou, Anuj Goyal, Aram Galstyan, Nagesh Panyam Chandrasekarasastry, Spyros Matsoukas, Timothy Leffel |  |
| 80 |  |  [Tab-CQA: A Tabular Conversational Question Answering Dataset on Financial Reports](https://doi.org/10.18653/v1/2023.acl-industry.20) |  | 0 | Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors... | Chuang Liu, Deyi Xiong, Junzhuo Li |  |
| 81 |  |  [KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications](https://doi.org/10.18653/v1/2023.acl-industry.21) |  | 0 | Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea... | Gunhee Kim, Hwaran Lee, Joonsuk Park, JungWoo Ha, Seokhee Hong, Takyoung Kim |  |
| 82 |  |  [Improving Knowledge Production Efficiency With Question Answering on Conversation](https://doi.org/10.18653/v1/2023.acl-industry.22) |  | 0 | Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question... | Changlin Yang, Jing Zheng, Sen Hu, Siye Liu, Teng Xu, Wangshu Zhang |  |
| 83 |  |  [Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses](https://doi.org/10.18653/v1/2023.acl-industry.23) |  | 0 | Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times. This paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets. Our approach... | Alessandro Manzotti, Alessandro Pedrani, Andrea Caciolai, Davide Bernardi, Donato Crisostomi, Enrico Palumbo, Kay Rottmann |  |
| 84 |  |  [Distilled Language Models are economically efficient for the enterprise. ...mostly](https://doi.org/10.18653/v1/2023.acl-industry.24) |  | 0 | Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context... | Ethan Selfridge, Gitit Kehat, Gwen Christian, Ilana Zimmerman, Jadin Tredup, Joseph Bradley, Julianne Marzulla, Kristen Howell, Leanne Rolston, Pavel Fomitchov |  |
| 85 |  |  [Application-Agnostic Language Modeling for On-Device ASR](https://doi.org/10.18653/v1/2023.acl-industry.25) |  | 0 | On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions... | Lyan Verwimp, Markus NußbaumThom, Youssef Oualil |  |
| 86 |  |  [Building Accurate Low Latency ASR for Streaming Voice Search in E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.26) |  | 0 | Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency... | Abhinav Goyal, Nikesh Garera |  |
| 87 |  |  [PLAtE: A Large-scale Dataset for List Page Web Extraction](https://doi.org/10.18653/v1/2023.acl-industry.27) |  | 0 | Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of... | Aidan San, Colin Lockard, David M. Ciemiewicz, Heba Elfardy, Jan Bakus, Kevin Small, Sandeep Atluri, Yangfeng Ji, Yuan Zhuang |  |
| 88 |  |  [Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed](https://doi.org/10.18653/v1/2023.acl-industry.28) |  | 0 | Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from... | Bingyan Liu, Cen Chen, Chengyu Wang, Jun Huang, Kui Jia, Lianwen Jin, Weifeng Lin, Zhang Zipeng, Zhongjie Duan, Ziheng Wu |  |
| 89 |  |  [Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes](https://doi.org/10.18653/v1/2023.acl-industry.29) |  | 0 | E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don’t label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute... | Anant Khandelwal, Deepak Gupta, Happy Mittal, Shreyas Sunil Kulkarni |  |
| 90 |  |  [Consistent Text Categorization using Data Augmentation in e-Commerce](https://doi.org/10.18653/v1/2023.acl-industry.30) |  | 0 | The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product... | Ariel Raviv, Guy Horowitz, Noa Avigdor, Stav Yanovsky Daye |  |
| 91 |  |  [An efficient method for Natural Language Querying on Structured Data](https://doi.org/10.18653/v1/2023.acl-industry.31) |  | 0 | We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following “bread and butter” NLP tasks: (a) Domain classification, for... | Aviral Joshi, Hanoz Bhathena, Prateek Singh |  |
| 92 |  |  [Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy](https://doi.org/10.18653/v1/2023.acl-industry.32) |  | 0 | Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for... | Kevin Brown, Mariann Micsinai Balan, Zekai Chen |  |
| 93 |  |  [EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only](https://doi.org/10.18653/v1/2023.acl-industry.33) |  | 0 | This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural... | Ahmet Gunduz, Hassan Sawaf, Kamer Ali Yüksel, Mohamed AlBadrashiny |  |
| 94 |  |  [A Static Evaluation of Code Completion by Large Language Models](https://doi.org/10.18653/v1/2023.acl-industry.34) |  | 0 | Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform... | Baishakhi Ray, Bing Xiang, Dan Roth, Hantian Ding, Murali Krishna Ramanathan, Parminder Bhatia, Rob Kwiatkowski, Sudipta Sengupta, Varun Kumar, Xiaopeng Li, Yuchen Tian, Zijian Wang |  |
| 95 |  |  [Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems](https://doi.org/10.18653/v1/2023.acl-industry.35) |  | 0 | Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance... | Fatemeh Sheikholeslami, Jaeyoung Do, Mohammad Kachuee, Sarthak Ahuja, Weiqing Liu |  |
| 96 |  |  [MobileNMT: Enabling Translation in 15MB and 30ms](https://doi.org/10.18653/v1/2023.acl-industry.36) |  | 0 | Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only... | Jingbo Zhu, Mingxuan Wang, Tong Xiao, Xiaohui Wang, Ye Lin, Zhexi Zhang |  |
| 97 |  |  [Multi-doc Hybrid Summarization via Salient Representation Learning](https://doi.org/10.18653/v1/2023.acl-industry.37) |  | 0 | Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and... | Min Xiao |  |
| 98 |  |  [SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels](https://doi.org/10.18653/v1/2023.acl-industry.38) |  | 0 | Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise... | Chao Qu, Xiaoyu Tan, Yinghui Xu, Yuan Qi, Zhenting Qi |  |
| 99 |  |  [Chemical Language Understanding Benchmark](https://doi.org/10.18653/v1/2023.acl-industry.39) |  | 0 | In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of... | Hyuk Ko, Hyun Young Heo, Jane Lee, Jinyoung Yang, KyuHwang Lee, Sungsoo Lee, Yunsoo Kim |  |
| 100 |  |  [HyperT5: Towards Compute-Efficient Korean Language Modeling](https://doi.org/10.18653/v1/2023.acl-industry.40) |  | 0 | Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or... | Dongju Park, Gichang Lee, Jaewook Kang, Kang Min Yoo, Soonwon Ka |  |
| 101 |  |  [Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings](https://doi.org/10.18653/v1/2023.acl-industry.41) |  | 0 | Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design... | HoJin Choi, Jong Myoung Kim, Sangkeun Jung, YoungJun Lee |  |
| 102 |  |  [Reliable and Interpretable Drift Detection in Streams of Short Texts](https://doi.org/10.18653/v1/2023.acl-industry.42) |  | 0 | Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards... | Ateret AnabyTavor, Ella Rabinovich, Matan Vetzler, Samuel Ackerman |  |
| 103 |  |  [Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-industry.43) |  | 0 | Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks. Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems... | Chandana Satya Prakash, Daniil Sorokin, Jin Cao, Jonathan J. Hüser, Judith Gaspers, Nicolas Anastassacos, Quynh Do, Thomas Gueudré, Tobias Falke, Turan Gojayev |  |
| 104 |  |  [Annotating Research Infrastructure in Scientific Papers: An NLP-driven Approach](https://doi.org/10.18653/v1/2023.acl-industry.44) |  | 0 | In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such... | Alberto Zigoni, Georgios Cheirmpos, Georgios Tsatsaronis, Marius A. Doornenbal, Seyed Amin Tabatabaei, Véronique Moore |  |
| 105 |  |  [Event-Centric Query Expansion in Web Search](https://doi.org/10.18653/v1/2023.acl-industry.45) |  | 0 | In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the... | Jin Ma, Tianhua Zhou, Weijie Cui, Xiang Chen, Xiaoling Bai, Yanan Zhang, Yangfan Zhang, Zhe Zhang |  |
| 106 |  |  [Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization](https://doi.org/10.18653/v1/2023.acl-industry.46) |  | 0 | As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. In order to unify the categorization process and ensure... | Fengjiao Chen, Kenny Q. Zhu, Shansan Gong, Shuo Wang, Xiujie Song, Xuezhi Cao, Yunsen Xian, Zelin Zhou |  |
| 107 |  |  [DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction](https://doi.org/10.18653/v1/2023.acl-industry.47) |  | 0 | Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge Base (KB) databases are an effective way of storing and accessing such information to scale. In this work we present a system,... | Annalisa Riccardi, Antonio Valerio Miceli Barone, Paul Darm, Shay B. Cohen |  |
| 108 |  |  [BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting](https://doi.org/10.18653/v1/2023.acl-industry.48) |  | 0 | Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists... | Guotong Xie, Peng Wang, Wei Zhu, Xiaoling Wang, Yuan Ni |  |
| 109 |  |  [K-pop and fake facts: from texts to smart alerting for maritime security](https://doi.org/10.18653/v1/2023.acl-industry.49) |  | 0 | Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard... | Guillaume Gadek, Kilian Vasnier, Maxime Prieur, Souhir Gahbiche, Sylvain Gatepaille, Valerian Justine |  |
| 110 |  |  [Evaluating Embedding APIs for Information Retrieval](https://doi.org/10.18653/v1/2023.acl-industry.50) |  | 0 | The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds... | David AlfonsoHermelo, Ehsan Kamalloo, Jimmy Lin, Mehdi Rezagholizadeh, Nandan Thakur, Odunayo Ogundepo, Xinyu Zhang |  |
| 111 |  |  [Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform](https://doi.org/10.18653/v1/2023.acl-industry.51) |  | 0 | Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic... | Adam Gonczarek, Mateusz Baran, Mateusz Wójcik, Tomasz Kajdanowicz, Witold Kosciukiewicz |  |
| 112 |  |  [Regression-Free Model Updates for Spoken Language Understanding](https://doi.org/10.18653/v1/2023.acl-industry.52) |  | 0 | In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique,... | Alessandro Pedrani, Andrea Caciolai, Davide Bernardi, Tobias Falke, Verena Weber |  |
| 113 |  |  [Reducing cohort bias in natural language understanding systems with targeted self-training scheme](https://doi.org/10.18653/v1/2023.acl-industry.53) |  | 0 | Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is... | Bei Chen, DieuThu Le, Gabriela Hernández, Melanie Bradford |  |
| 114 |  |  [Content Moderation for Evolving Policies using Binary Question Answering](https://doi.org/10.18653/v1/2023.acl-industry.54) |  | 0 | Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of... | Akshat Mathur, Jidnya Shah, Mohan Bhambhani, Sankha Subhra Mullick, Somya Gupta, Suhit Sinha |  |
| 115 |  |  [Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification](https://doi.org/10.18653/v1/2023.acl-industry.55) |  | 0 | Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to... | Jing Gao, Lei Chen, Tianqi Wang, Xiaodan Zhu, Younghun Lee |  |
| 116 |  |  [Towards Building a Robust Toxicity Predictor](https://doi.org/10.18653/v1/2023.acl-industry.56) |  | 0 | Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text... | Dmitriy Bespalov, Liutong Zhou, Sourav Bhabesh, Yanjun Qi, Yi Xiang |  |
| 117 |  |  [AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching](https://doi.org/10.18653/v1/2023.acl-industry.57) |  | 0 | In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for... | Cheng Chen, Mahsa Azizi, Md. Tahmid Rahman Laskar, Shashi Bhushan TN, Simon CorstonOliver, XueYong Fu |  |
| 118 |  |  [Unified Contextual Query Rewriting](https://doi.org/10.18653/v1/2023.acl-industry.58) |  | 0 | Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks... | Eunah Cho, Jie Hao, Kellen Gillespie, Mukund Rungta, Vishal Thanvantri Vasudevan, Xing Fan, Yanbin Lu, Yang Liu, Yingxue Zhou, Zeynab Raeesy |  |
| 119 |  |  [Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites](https://doi.org/10.18653/v1/2023.acl-industry.59) |  | 0 | E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain... | Bing Yin, Chao Zhang, Haoming Jiang, Qingyu Yin, Shaohui Xi, Simiao Zuo, Tuo Zhao |  |
| 120 |  |  [Federated Learning of Gboard Language Models with Differential Privacy](https://doi.org/10.18653/v1/2023.acl-industry.60) |  | 0 | We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To... | Christopher A. ChoquetteChoo, Galen Andrew, H. Brendan McMahan, Jesse Rosenstock, Peter Kairouz, Yanxiang Zhang, Yuanbo Zhang, Zheng Xu |  |
| 121 |  |  [RadLing: Towards Efficient Radiology Report Understanding](https://doi.org/10.18653/v1/2023.acl-industry.61) |  | 0 | Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in... | Larisa Micu, Manuela Daniela Danu, Oladimeji Farri, Ramya Vunikili, Rikhiya Ghosh, Sanjeev Kumar Karn |  |
| 122 |  |  [Predicting Customer Satisfaction with Soft Labels for Ordinal Classification](https://doi.org/10.18653/v1/2023.acl-industry.62) |  | 0 | In a typical call center, only up to 8% of callersleave a Customer Satisfaction (CSAT) surveyresponse at the end of the call, and these tend tobe customers with strongly positive or negativeexperiences. To manage this data sparsity andresponse bias, we outline a predictive CSATdeep learning... | Etienne Manderscheid, Matthias Lee |  |
| 123 |  |  [Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users](https://doi.org/10.18653/v1/2023.acl-industry.63) |  | 0 | Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models. Unfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback... | Alessandro Moschitti, Ivano Lauriola, Liang Wang |  |
| 124 |  |  [SPM: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling](https://doi.org/10.18653/v1/2023.acl-industry.64) |  | 0 | In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label... | Kai Yu, Qingliang Miao, Ruisheng Cao, Sheng Jiang, Su Zhu |  |
| 125 |  |  [NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks](https://doi.org/10.18653/v1/2023.acl-industry.65) |  | 0 | Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering... | Jingfan Zhang, Ming Tan, Wei Zhu, Xinpeng Zhang |  |
| 126 |  |  [Search Query Spell Correction with Weak Supervision in E-commerce](https://doi.org/10.18653/v1/2023.acl-industry.66) |  | 0 | Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless... | Chinmay Sharma, Madhura Pande, Surender Kumar, Vishal Kakkar |  |
| 127 |  |  ["Let's not Quote out of Context": Unified Vision-Language Pretraining for Context Assisted Image Captioning](https://doi.org/10.18653/v1/2023.acl-industry.67) |  | 0 | Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified... | Abisek Rajakumar Kalarani, Niyati Chhaya, Pushpak Bhattacharyya, Sumit Shekhar |  |
| 128 |  |  [What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue](https://doi.org/10.18653/v1/2023.acl-industry.68) |  | 0 | This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed... | Deuk Sin Kwon, Eric Davis, Ki Hyun Kim, Seojin Lee, Sunwoo Lee, Taeyoon Kim |  |
| 129 |  |  [CUPID: Curriculum Learning Based Real-Time Prediction using Distillation](https://doi.org/10.18653/v1/2023.acl-industry.69) |  | 0 | Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model... | Ankit Gandhi, Ankith M. S, Arindam Bhattacharya, Atul Saroop, Rahul Bhagat, Vijay Huddar |  |
| 130 |  |  [Answering Unanswered Questions through Semantic Reformulations in Spoken QA](https://doi.org/10.18653/v1/2023.acl-industry.70) |  | 0 | Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant... | Besnik Fetahu, Oleg Rokhlenko, Pedro Faustini, Shervin Malmasi, Zhiyu Chen |  |
| 131 |  |  [Exploring Zero and Few-shot Techniques for Intent Classification](https://doi.org/10.18653/v1/2023.acl-industry.71) |  | 0 | Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent... | Mitul Tiwari, Prashil Tumbade, Quaizar Vohra, Soham Parikh |  |
| 132 |  |  [Referring to Screen Texts with Voice Assistants](https://doi.org/10.18653/v1/2023.acl-industry.72) |  | 0 | Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their users’ context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to... | Alkesh Patel, Anand Dhoot, Hoang Long Nguyen, Hong Yu, IngMarie Jonsson, Shruti Bhargava, Vincent Renkens |  |
| 133 |  |  [Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search](https://doi.org/10.18653/v1/2023.acl-industry.73) |  | 0 | Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support.... | Besnik Fetahu, Jason Ingyu Choi, Oleg Rokhlenko, Shervin Malmasi, Zhiyu Chen |  |
| 134 |  |  [KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-industry.74) |  | 0 | Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of... | Arjun R. Akula, Garima Pruthi, Hao Su, Pradyumna Narayana, Sugato Basu, Varun Jampani, Zhiwei Jia |  |
| 135 |  |  [Weakly supervised hierarchical multi-task classification of customer questions](https://doi.org/10.18653/v1/2023.acl-industry.75) |  | 0 | Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are... | Chetan Aggarwal, Jitenkumar Rana, Manan Soni, Promod Yenigalla, Rashmi Patange, Sandeep Sricharan Mukku |  |
| 136 |  |  [Automated Digitization of Unstructured Medical Prescriptions](https://doi.org/10.18653/v1/2023.acl-industry.76) |  | 0 | Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to... | Aruna Rajan, Megha Sharma, Srujana Merugu, Tushar Vatsal |  |
| 137 |  |  [Frontmatter](https://aclanthology.org/2023.acl-tutorials.0) |  | 0 |  |  |  |
| 138 |  |  [Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond](https://doi.org/10.18653/v1/2023.acl-tutorials.1) |  | 0 | Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but... | Minlie Huang, TatSeng Chua, Wenqiang Lei, Yang Deng |  |
| 139 |  |  [Complex Reasoning in Natural Languag](https://doi.org/10.18653/v1/2023.acl-tutorials.2) |  | 0 | Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard... | Aman Madaan, Bill Yuchen Lin, Michihiro Yasunaga, Mor Geva, Tao Yu, Wenting Zhao |  |
| 140 |  |  [Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world](https://doi.org/10.18653/v1/2023.acl-tutorials.3) |  | 0 | This tutorial will describe various aspects of scaling up language technologies to many of the world’s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI... | Barun Patra, Kabir Ahuja, Kalika Bali, Monojit Choudhury, Sunayana Sitaram, Vishrav Chaudhary |  |
| 141 |  |  [Generating Text from Language Models](https://doi.org/10.18653/v1/2023.acl-tutorials.4) |  | 0 | An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further,... | Afra Amini, Clara Meister, John Hewitt, Ryan Cotterell, Tiago Pimentel |  |
| 142 |  |  [Indirectly Supervised Natural Language Processing](https://doi.org/10.18653/v1/2023.acl-tutorials.5) |  | 0 | This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a... | Ben Zhou, Dan Roth, KaiWei Chang, Muhao Chen, Qiang Ning, Wenpeng Yin |  |
| 143 |  |  [Retrieval-based Language Models and Applications](https://doi.org/10.18653/v1/2023.acl-tutorials.6) |  | 0 | Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs,... | Akari Asai, Danqi Chen, Sewon Min, Zexuan Zhong |  |
| 144 |  |  [ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer](https://doi.org/10.18653/v1/2023.acl-srw.1) |  | 0 | Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT’s performance in two... | Dongqi Liu, Vera Demberg |  |
| 145 |  |  [Multi-Dialectal Representation Learning of Sinitic Phonology](https://doi.org/10.18653/v1/2023.acl-srw.2) |  | 0 | Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages... | Zhibai Jia |  |
| 146 |  |  [Prompt-based Zero-shot Text Classification with Conceptual Knowledge](https://doi.org/10.18653/v1/2023.acl-srw.4) |  | 0 | In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of... | Anh Nguyen, Kaizhu Huang, Qi Chen, Suparna De, Wei Wang, Yuqi Wang |  |
| 147 |  |  [How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese](https://doi.org/10.18653/v1/2023.acl-srw.5) |  | 0 | This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological... | Atsuki Yamaguchi, Koki Shibata, Takuro Fujii, Terufumi Morishita, Yasuhiro Sogawa |  |
| 148 |  |  [Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering](https://doi.org/10.18653/v1/2023.acl-srw.7) |  | 0 | Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the... | Chenyang Lyu, Jennifer Foster, Tianbo Ji, Yvette Graham |  |
| 149 |  |  [Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models](https://doi.org/10.18653/v1/2023.acl-srw.8) |  | 0 | Natural Language Inference (NLI) tasks involving temporal inference remain challenging for pre-trained language models (LMs). Although various datasets have been created for this task, they primarily focus on English and do not address the need for resources in other languages. It is unclear... | Hitomi Yanaka, Tomoki Sugimoto, Yasumasa Onoe |  |
| 150 |  |  [Constructing Multilingual Code Search Dataset Using Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-srw.10) |  | 0 | Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual... | Hitomi Yanaka, Nan Duan, Ryo Sekizawa, Shuai Lu |  |
| 151 |  |  [Multimodal Neural Machine Translation Using Synthetic Images Transformed by Latent Diffusion Model](https://doi.org/10.18653/v1/2023.acl-srw.12) |  | 0 | This study proposes a new multimodal neural machine translation (MNMT) model using synthetic images transformed by a latent diffusion model. MNMT translates a source language sentence based on its related image, but the image usually contains noisy information that are not relevant to the source... | Akihiro Tamura, Ryoya Yuasa, Takashi Ninomiya, Tomoyuki Kajiwara, Tsuneo Kato |  |
| 152 |  |  [Enhancing Ancient Chinese Understanding with Derived Noisy Syntax Trees](https://doi.org/10.18653/v1/2023.acl-srw.15) |  | 0 | Despite the rapid development of neural-based models, syntax still plays a crucial role in modern natural language processing. However, few studies have incorporated syntactic information into ancient Chinese understanding tasks due to the lack of syntactic annotation. This paper explores the role... | Jingrui Hou, Ping Wang, Shitou Zhang, Zuchao Li |  |
| 153 |  |  [The Turing Quest: Can Transformers Make Good NPCs?](https://doi.org/10.18653/v1/2023.acl-srw.17) |  | 0 | In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In... | Ali Emami, Qi Chen Gao |  |
| 154 |  |  [Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section](https://doi.org/10.18653/v1/2023.acl-srw.18) |  | 0 | Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes... | Eric K. Oermann, Hongyi Zheng, Kyunghyun Cho, Lavender Y. Jiang, Yixin Zhu |  |
| 155 |  |  [Intriguing Effect of the Correlation Prior on ICD-9 Code Assignment](https://doi.org/10.18653/v1/2023.acl-srw.19) |  | 0 | The Ninth Revision of the International Classification of Diseases (ICD-9) is a standardized coding system used to classify health conditions. It is used for billing, tracking individual patient conditions, and for epidemiology. The highly detailed and technical nature of the codes and their... | Chenkang Zhang, Eric K. Oermann, Kyunghyun Cho, Lavender Y. Jiang, Muru Wu, Xujin Liu, Zihao Yang |  |
| 156 |  |  [Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks](https://doi.org/10.18653/v1/2023.acl-srw.20) |  | 0 | State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD... | Adam Gonczarek, Joanna Baran, Maciej Zieba, Mateusz Baran, Mateusz Wójcik |  |
| 157 |  |  [Can LMs Store and Retrieve 1-to-N Relational Knowledge?](https://doi.org/10.18653/v1/2023.acl-srw.22) |  | 0 | It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational... | Benjamin Heinzerling, Haruki Nagasawa, Kazuma Kokuta, Kentaro Inui |  |
| 158 |  |  [Theoretical Linguistics Rivals Embeddings in Language Clustering for Multilingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-srw.24) |  | 0 | While embedding-based methods have been dominant in language clustering for multilingual tasks, clustering based on linguistic features has not yet been explored much, as it remains baselines (Tan et al., 2019; Shaffer, 2021). This study investigates whether and how theoretical linguistics improves... | Daisuke Kawahara, Hiromune Oda, Naho Orita, Sakura Imai |  |
| 159 |  |  [Native Language Prediction from Gaze: a Reproducibility Study](https://doi.org/10.18653/v1/2023.acl-srw.26) |  | 0 | Numerous studies found that the linguistic properties of a person’s native language affect the cognitive processing of other languages. However, only one study has shown that it was possible to identify the native language based on eye-tracking records of natural L2 reading using machine learning.... | Anita Zielinska, Lina Skerath, Maria Barrett, Paulina Toborek, Rob van der Goot |  |
| 160 |  |  [MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries](https://doi.org/10.18653/v1/2023.acl-srw.27) |  | 0 | Discharge summaries are comprehensive medical records that encompass vital information about a patient’s hospital stay. A crucial aspect of discharge summaries is the temporal information of treatments administered throughout the patient’s illness. With an extensive volume of clinical documents,... | Goran Nenadic, Lifeng Han, Yang Cui |  |
| 161 |  |  [Sudden Semantic Shifts in Swedish NATO discourse](https://doi.org/10.18653/v1/2023.acl-srw.28) |  | 0 | In this paper, we investigate a type of semantic shift that occurs when a sudden event radically changes public opinion on a topic. Looking at Sweden’s decision to apply for NATO membership in 2022, we use word embeddings to study how the associations users on Twitter have regarding NATO evolve. We... | Bastiaan Bruinsma, Brian Bonafilia, Denitsa Saynova, Moa Johansson |  |
| 162 |  |  [Building a Buzzer-quiz Answering System](https://doi.org/10.18653/v1/2023.acl-srw.29) |  | 0 | A buzzer quiz is a genre of quiz in which multiple players simultaneously listen to a quiz being read aloud and respond it by buzzing in as soon as they can predict the answer. Because incorrect answers often result in penalties, a buzzer-quiz answering system must not only predict the answer from... | Katsuhiko Toyama, Koichi Takeda, Kosuke Yamada, Naoya Sugiura, Ryohei Sasano |  |
| 163 |  |  [Probing for Hyperbole in Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-srw.30) |  | 0 | Hyperbole is a common figure of speech, which is under-explored in NLP research. In this study, we conduct edge and minimal description length (MDL) probing experiments on three pre-trained language models (PLMs) in an attempt to explore the extent to which hyperbolic information is encoded in... | Bolette S. Pedersen, Daniel Hershcovich, Nina Schneidermann |  |
| 164 |  |  [Towards Efficient Dialogue Processing in the Emergency Response Domain](https://doi.org/10.18653/v1/2023.acl-srw.31) |  | 0 | In this paper we describe the task of adapting NLP models to dialogue processing in the emergency response domain. Our goal is to provide a recipe for building a system that performs dialogue act classification and domain-specific slot tagging while being efficient, flexible and robust. We show... | Tatiana Anikina |  |
| 165 |  |  [I already said that! Degenerating redundant questions in open-domain dialogue systems](https://doi.org/10.18653/v1/2023.acl-srw.33) |  | 0 | Neural text generation models have achieved remarkable success in carrying on short open-domain conversations. However, their performance degrades significantly in the long term, especially in their ability to ask coherent questions. A significant issue is the generation of redundant questions... | Julie CarsonBerndsen, Long Mai |  |
| 166 |  |  [Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation](https://doi.org/10.18653/v1/2023.acl-srw.34) |  | 0 | Currently, most knowledge-grounded dialogue response generation models focus on reflecting given external knowledge. However, even when conveying external knowledge, humans integrate their own knowledge, experiences, and opinions with external knowledge to make their utterances engaging. In this... | Hirokazu Kiyomaru, Sadao Kurohashi, Takashi Kodama, Taro Okahisa, Yin Jou Huang |  |
| 167 |  |  [Choosing What to Mask: More Informed Masking for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.acl-srw.35) |  | 0 | Pre-trained language models have achieved remarkable results on several NLP tasks. Most of them adopt masked language modeling to learn representations by randomly masking tokens and predicting them based on their context. However, this random selection of tokens to be masked is inefficient to... | Helena de Medeiros Caseli, Júlia Sato, Lucia Specia |  |
| 168 |  |  [Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference](https://doi.org/10.18653/v1/2023.acl-srw.36) |  | 0 | Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal–Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best... | Carina Silberer, Chong Shen |  |
| 169 |  |  [Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values](https://doi.org/10.18653/v1/2023.acl-srw.37) |  | 0 | Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley,... | Ritwick Mishra, Stephanie Schoch, Yangfeng Ji |  |
| 170 |  |  [Distractor Generation for Fill-in-the-Blank Exercises by Question Type](https://doi.org/10.18653/v1/2023.acl-srw.38) |  | 0 | This study addresses the automatic generation of distractors for English fill-in-the-blank exercises in the entrance examinations for Japanese universities. While previous studies applied the same method to all questions, actual entrance examinations have multiple question types that reflect the... | Nana Yoshimi, Satoru Uchida, Takashi Ninomiya, Tomoyuki Kajiwara, Yuki Arase |  |
| 171 |  |  [Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://doi.org/10.18653/v1/2023.acl-srw.40) |  | 0 | Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a... | Gabriel Simmons |  |
| 172 |  |  [LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism](https://doi.org/10.18653/v1/2023.acl-srw.43) |  | 0 | Recently, dynamic early exiting has attracted much attention since it can accelerate the inference speed of pre-trained models (PTMs). However, previous work on early exiting has neglected the intermediate exits’ architectural designs. In this work, we propose a novel framework, Learned Exits and... | Jingfan Zhang, Ming Tan, Pengyu Dai, Wei Zhu |  |
| 173 |  |  [Authorship Attribution of Late 19th Century Novels using GAN-BERT](https://doi.org/10.18653/v1/2023.acl-srw.44) |  | 0 | Authorship attribution aims to identify the author of an anonymous text. The task becomes even more worthwhile when it comes to literary works. For example, pen names were commonly used by female authors in the 19th century resulting in some literary works being incorrectly attributed or claimed.... | Burcu Can, Frédéric Blain, Kanishka Silva, Laura Ugolini, Raheem Sarwar, Ruslan Mitkov |  |
| 174 |  |  [How-to Guides for Specific Audiences: A Corpus and Initial Findings](https://doi.org/10.18653/v1/2023.acl-srw.46) |  | 0 | Instructional texts for specific target groups should ideally take into account the prior knowledge and needs of the readers in order to guide them efficiently to their desired goals. However, targeting specific groups also carries the risk of reflecting disparate social norms and subtle... | Agnieszka Falenska, Michael Roth, Nicola Fanton |  |
| 175 |  |  ["When Words Fail, Emojis Prevail": A Novel Architecture for Generating Sarcastic Sentences With Emoji Using Valence Reversal and Semantic Incongruity](https://doi.org/10.18653/v1/2023.acl-srw.47) |  | 0 | Sarcasm is a form of figurative language that serves as a humorous tool for mockery and ridicule. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence in English. We divide the generation task into two sub tasks: one for generating textual sarcasm... | Faria Binte Kader, Hasan Mahmud, Md. Kamrul Hasan, Mohsinul Kabir, Nafisa Hossain Nujat, Tasmia Binte Sogir |  |
| 176 |  |  [Semantic Accuracy in Natural Language Generation: A Thesis Proposal](https://doi.org/10.18653/v1/2023.acl-srw.48) |  | 0 | With the fast-growing popularity of current large pre-trained language models (LLMs), it is necessary to dedicate efforts to making them more reliable. In this thesis proposal, we aim to improve the reliability of natural language generation systems (NLG) by researching the semantic accuracy of... | Patrícia Schmidtová |  |
| 177 |  |  [Should you marginalize over possible tokenizations?](https://doi.org/10.18653/v1/2023.acl-short.1) |  | 0 | Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token... | Germán Kruszewski, Jos Rozen, Marc Dymetman, Nadezhda Chirkova |  |
| 178 |  |  [Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie](https://doi.org/10.18653/v1/2023.acl-short.2) |  | 0 | Accurate neural models are much less efficient than non-neural models and are useless for processing billions of social media posts or handling user queries in real time with a limited budget. This study revisits the fastest pattern-based NLP methods to make them as accurate as possible, thus... | Naoki Yoshinaga |  |
| 179 |  |  [Transformed Protoform Reconstruction](https://doi.org/10.18653/v1/2023.acl-short.3) |  | 0 | Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update... | Chenxuan Cui, David R. Mortensen, Kalvin Chang, Young Min Kim |  |
| 180 |  |  [Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models](https://doi.org/10.18653/v1/2023.acl-short.4) |  | 0 | We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the... | Daniel Hardt |  |
| 181 |  |  [Bootstrapping Neural Relation and Explanation Classifiers](https://doi.org/10.18653/v1/2023.acl-short.5) |  | 0 | We introduce a method that self trains (or bootstraps) neural relation and explanation classifiers. Our work expands the supervised approach of CITATION, which jointly trains a relation classifier with an explanation classifier that identifies context words important for the relation at hand, to... | Mihai Surdeanu, Zheng Tang |  |
| 182 |  |  [A Fast Algorithm for Computing Prefix Probabilities](https://doi.org/10.18653/v1/2023.acl-short.6) |  | 0 | Multiple algorithms are known for efficiently calculating the prefix probability of a string under a probabilistic context-free grammar (PCFG). Good algorithms for the problem have a runtime cubic in the length of the input string. However, some proposed algorithms are suboptimal with respect to... | Franz Nowak, Ryan Cotterell |  |
| 183 |  |  [Analyzing Text Representations by Measuring Task Alignment](https://doi.org/10.18653/v1/2023.acl-short.7) |  | 0 | Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim.... | Ariadna Quattoni, Audi Primadhanty, César GonzálezGutiérrez, Francesco Cazzaro |  |
| 184 |  |  [Tracing Linguistic Markers of Influence in a Large Online Organisation](https://doi.org/10.18653/v1/2023.acl-short.8) |  | 0 | Social science and psycholinguistic research have shown that power and status affect how people use language in a range of domains. Here, we investigate a similar question in a large, distributed, consensus-driven community with little traditional power hierarchy – the Internet Engineering Task... | Colin Perkins, Gareth Tyson, Ignacio Castro, Matthew Purver, Mladen Karan, Patrick Healey, Prashant Khare, Ravi Shekhar, Stephen McQuistin |  |
| 185 |  |  [Metaphor Detection via Explicit Basic Meanings Modelling](https://doi.org/10.18653/v1/2023.acl-short.9) |  | 0 | One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design. While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its contextual meaning... | Chenghua Lin, Frank Guerin, Shun Wang, Yucheng Li |  |
| 186 |  |  [xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages](https://doi.org/10.18653/v1/2023.acl-short.10) |  | 0 | We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which... | Alexandre Mourachko, Holger Schwenk, Kevin Heffernan, Mingda Chen, Onur Çelebi |  |
| 187 |  |  [Graph Propagation based Data Augmentation for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.11) |  | 0 | Data augmentation is an effective solution to improve model performance and robustness for low-resource named entity recognition (NER). However, synthetic data often suffer from poor diversity, which leads to performance limitations. In this paper, we propose a novel Graph Propagated Data... | Jiong Cai, Kewei Tu, Pengjun Xie, Shen Huang, Yong Jiang, Zeqi Tan |  |
| 188 |  |  [Dataset Distillation with Attention Labels for Fine-tuning BERT](https://doi.org/10.18653/v1/2023.acl-short.12) |  | 0 | Dataset distillation aims to create a small dataset of informative synthetic samples to rapidly train neural networks that retain the performance of the original dataset. In this paper, we focus on constructing distilled few-shot datasets for natural language processing (NLP) tasks to fine-tune... | Aru Maekawa, Kotaro Funakoshi, Manabu Okumura, Naoki Kobayashi |  |
| 189 |  |  [Multi-Document Summarization with Centroid-Based Pretraining](https://doi.org/10.18653/v1/2023.acl-short.13) |  | 0 | In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each... | Mark Steedman, Nancy Chen, Parag Jain, Ratish Surendran Puduppully |  |
| 190 |  |  [Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times](https://doi.org/10.18653/v1/2023.acl-short.14) |  | 0 | Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and... | Andrea Gregor de Varda, Marco Marelli |  |
| 191 |  |  [Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques](https://doi.org/10.18653/v1/2023.acl-short.15) |  | 0 | Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM’s generalization in semantic parsing with two simple... | Bailin Wang, Daking Rai, Yilun Zhou, Ziyu Yao |  |
| 192 |  |  [HiPool: Modeling Long Documents Using Graph Neural Networks](https://doi.org/10.18653/v1/2023.acl-short.16) |  | 0 | Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences.... | Aosong Feng, Dragomir Radev, Irene Li, Rex Ying |  |
| 193 |  |  [A Weakly Supervised Classifier and Dataset of White Supremacist Language](https://doi.org/10.18653/v1/2023.acl-short.17) |  | 0 | We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar... | Ahmad Diab, David West Brown, Kathleen M. Carley, Michael Yoder |  |
| 194 |  |  [BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases](https://doi.org/10.18653/v1/2023.acl-short.18) |  | 0 | Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding... | Lu Wang, Muhammad Khalifa, Xin Liu |  |
| 195 |  |  [mOKB6: A Multilingual Open Knowledge Base Completion Benchmark](https://doi.org/10.18653/v1/2023.acl-short.19) |  | 0 | Automated completion of open knowledge bases (Open KBs), which are constructed from triples of the form (subject phrase, relation phrase, object phrase), obtained via open information extraction (Open IE) system, are useful for discovering novel facts that may not be directly present in the text.... | Keshav Kolluru, Mausam, Shubham Mittal, Soumen Chakrabarti |  |
| 196 |  |  [Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment](https://doi.org/10.18653/v1/2023.acl-short.20) |  | 0 | Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the... | Alexandre Djerbetian, Amir Globerson, Gal Elidan, Lidan Hackmon, Reut Tsarfaty, Roee Engelberg, Roni Rabin |  |
| 197 |  |  [Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts](https://doi.org/10.18653/v1/2023.acl-short.21) |  | 0 | Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product... | Alisa Liu, Maarten Sap, Skyler Hallinan, Yejin Choi |  |
| 198 |  |  [A Natural Bias for Language Generation Models](https://doi.org/10.18653/v1/2023.acl-short.22) |  | 0 | After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have... | Adhiguna Kuncoro, Clara Meister, Laura Rimell, Lei Yu, Tiago Pimentel, Wojciech Stokowiec |  |
| 199 |  |  [Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-short.23) |  | 0 | High-quality and high-coverage rule sets are imperative to the success of Neuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form the basis of all symbolic inferences. Recent literature builds neural models for generating rule sets, however, preliminary experiments show that... | Ananjan Nandi, Mausam, Navdeep Kaur, Parag Singla |  |
| 200 |  |  [Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer](https://doi.org/10.18653/v1/2023.acl-short.24) |  | 0 | Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models.... | Maosong Sun, Ning Ding, Xingtai Lv, Yujia Qin, Zhiyuan Liu |  |
| 201 |  |  [Faithfulness Tests for Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-short.25) |  | 0 | Explanations of neural models aim to reveal a model’s decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model’s... | Christina Lioma, Isabelle Augenstein, Jakob Grue Simonsen, OanaMaria Camburu, Pepa Atanasova, Thomas Lukasiewicz |  |
| 202 |  |  [COGEN: Abductive Commonsense Language Generation](https://doi.org/10.18653/v1/2023.acl-short.26) |  | 0 | Reasoning is one of the most important elements in achieving Artificial General Intelligence (AGI), specifically when it comes to Abductive and counterfactual reasoning. In order to introduce these capabilities of reasoning in Natural Language Processing (NLP) models, there have been recent... | Diwanshu Shekhar, Mohammad H. Mahoor, Rohola Zandie |  |
| 203 |  |  [Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis](https://doi.org/10.18653/v1/2023.acl-short.27) |  | 0 | Multimodal relation extraction (MRE) is the task of identifying the semantic relationships between two entities based on the context of the sentence image pair. Existing retrieval-augmented approaches mainly focused on modeling the retrieved textual knowledge, but this may not be able to accurately... | Irwin King, Philip S. Yu, Xuming Hu, Zhijiang Guo, Zhiyang Teng |  |
| 204 |  |  [Characterization of Stigmatizing Language in Medical Records](https://doi.org/10.18653/v1/2023.acl-short.28) |  | 0 | Widespread disparities in clinical outcomes exist between different demographic groups in the United States. A new line of work in medical sociology has demonstrated physicians often use stigmatizing language in electronic medical records within certain groups, such as black patients, which may... | Alya Ahmad, Anne Links, Ayah Zirikly, Brant Chee, Keith Harrigian, Mark Dredze, Mary Catherine Beach, Somnath Saha |  |
| 205 |  |  [Abstractive Summarizers are Excellent Extractive Summarizers](https://doi.org/10.18653/v1/2023.acl-short.29) |  | 0 | Extractive and abstractive summarization designs have historically been fragmented, limiting the benefits that often arise from compatible model architectures. In this paper, we explore the potential synergies of modeling extractive summarization with an abstractive summarization system and propose... | Daniel Varab, Yumo Xu |  |
| 206 |  |  [Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions](https://doi.org/10.18653/v1/2023.acl-short.30) |  | 0 | Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time... | Atishay Jain, Himanshu Thakur, LouisPhilippe Morency, Paul Pu Liang, Praneetha Vaddamanu |  |
| 207 |  |  [PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English](https://doi.org/10.18653/v1/2023.acl-short.31) |  | 0 | Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents.... | Jianfeng Chi, KaiWei Chang, Wasi Uddin Ahmad, Yuan Tian |  |
| 208 |  |  [Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages](https://doi.org/10.18653/v1/2023.acl-short.32) |  | 0 | Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training... | Karl Aberer, Negar Foroutan Eghlidi, Rémi Lebret, Yasmine Karoui |  |
| 209 |  |  [BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-short.33) |  | 0 | Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs),... | Jeff Z. Pan, Jie He, Simon Chi Lok U, Víctor GutiérrezBasulto |  |
| 210 |  |  [Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases](https://doi.org/10.18653/v1/2023.acl-short.34) |  | 0 | Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however,... | Haozhe An, Rachel Rudinger |  |
| 211 |  |  [Improving Syntactic Probing Correctness and Robustness with Control Tasks](https://doi.org/10.18653/v1/2023.acl-short.35) |  | 0 | Syntactic probing methods have been used to examine whether and how pre-trained language models (PLMs) encode syntactic features. However, the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations. This paper... | Brian Wang, Hefan Zhang, Lili Wang, Rolando CotoSolano, Saeed Hassanpour, Soroush Vosoughi, Weicheng Ma |  |
| 212 |  |  [Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications](https://doi.org/10.18653/v1/2023.acl-short.36) |  | 0 | In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as... | Jatin Arora, Youngja Park |  |
| 213 |  |  [Credible without Credit: Domain Experts Assess Generative Language Models](https://doi.org/10.18653/v1/2023.acl-short.37) |  | 0 | Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of... | Brandon M. Stewart, Denis Peskoff |  |
| 214 |  |  [Grokking of Hierarchical Structure in Vanilla Transformers](https://doi.org/10.18653/v1/2023.acl-short.38) |  | 0 | For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel... | Christopher D. Manning, Jacob Andreas, Pratyusha Sharma, Shikhar Murty |  |
| 215 |  |  [Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data](https://doi.org/10.18653/v1/2023.acl-short.39) |  | 0 | Adapters have emerged as a parameter-efficient Transformer-based framework for cross-lingual transfer by inserting lightweight language-specific modules (language adapters) and task-specific modules (task adapters) within pretrained multilingual models. Zero-shot transfer is enabled by pairing the... | Preethi Jyothi, Ridayesh Parab, Ujan Deb |  |
| 216 |  |  [Context-Aware Transformer Pre-Training for Answer Sentence Selection](https://doi.org/10.18653/v1/2023.acl-short.40) |  | 0 | Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large... | Alessandro Moschitti, Luca Di Liello, Siddhant Garg |  |
| 217 |  |  [Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities](https://doi.org/10.18653/v1/2023.acl-short.41) |  | 0 | Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential... | Curtis P. Langlotz, JeanBenoit Delbrouck, Maya Varma, Xiang Wan, Zhihong Chen |  |
| 218 |  |  [Efficient Diagnosis Assignment Using Unstructured Clinical Notes](https://doi.org/10.18653/v1/2023.acl-short.42) |  | 0 | Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However,... | Akshay Chaudhari, Jason A. Fries, Joseph Preston, Louis Blankemeier, Nigam Shah, Robert Tinn |  |
| 219 |  |  [MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-short.43) |  | 0 | Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning.... | KaiWei Chang, Lin Yang, Liunian Harold Li, Masoud Monajatipoor, Mozhdeh Rouhsedaghat |  |
| 220 |  |  [On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach](https://doi.org/10.18653/v1/2023.acl-short.44) |  | 0 | In recent years, word embeddings have been widely used to measure biases in texts. Even if they have proven to be effective in detecting a wide variety of biases, metrics based on word embeddings lack transparency and interpretability. We analyze an alternative PMI-based metric to quantify biases... | Damián E. Blasi, Diego Fernández Slezak, Edgar Altszyler, Francisco Valentini, Germán Rosati |  |
| 221 |  |  [Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models](https://doi.org/10.18653/v1/2023.acl-short.45) |  | 0 | Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query... | Ehsan Doostmohammadi, Marco Kuhlmann, Richard Johansson, Tobias Norlund |  |
| 222 |  |  [MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents](https://doi.org/10.18653/v1/2023.acl-short.46) |  | 0 | Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level... | Aleksander Brechalov, Anastasia Razdaibiedina |  |
| 223 |  |  [KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-short.47) |  | 0 | While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge... | Bodhisattwa Prasad Majumder, Julian J. McAuley, Myeongjun Jang, OanaMaria Camburu, Thomas Lukasiewicz |  |
| 224 |  |  [Measuring the Effect of Influential Messages on Varying Personas](https://doi.org/10.18653/v1/2023.acl-short.48) |  | 0 | Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We... | ChengXiang Zhai, Chenkai Sun, Heng Ji, Hou Pong Chan, Jinning Li |  |
| 225 |  |  [Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity](https://doi.org/10.18653/v1/2023.acl-short.49) |  | 0 | Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix... | Dong Yu, Hongwei Wang |  |
| 226 |  |  [Robust Learning for Multi-party Addressee Recognition with Discrete Addressee Codebook](https://doi.org/10.18653/v1/2023.acl-short.50) |  | 0 | Addressee recognition aims to identify addressees in multi-party conversations. While state-of-the-art addressee recognition models have achieved promising performance, they still suffer from the issue of robustness when applied in real-world scenes. When exposed to a noisy environment, these... | Haiqing Chen, Kuncai Zhang, Pengcheng Zhu, Wei Zhou, Yuankai Ma |  |
| 227 |  |  [TwistList: Resources and Baselines for Tongue Twister Generation](https://doi.org/10.18653/v1/2023.acl-short.51) |  | 0 | Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining... | Chen Tang, Chenghua Lin, Tyler Loakman |  |
| 228 |  |  [Substitution-based Semantic Change Detection using Contextual Embeddings](https://doi.org/10.18653/v1/2023.acl-short.52) |  | 0 | Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of... | Dallas Card |  |
| 229 |  |  [Probing Physical Reasoning with Counter-Commonsense Context](https://doi.org/10.18653/v1/2023.acl-short.53) |  | 0 | In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests... | Akiko Aizawa, Kazushi Kondo, Saku Sugawara |  |
| 230 |  |  [Morphological Inflection with Phonological Features](https://doi.org/10.18653/v1/2023.acl-short.54) |  | 0 | Recent years have brought great advances into solving morphological tasks, mostly due to powerful neural models applied to various tasks as (re)inflection and analysis. Yet, such morphological tasks cannot be considered solved, especially when little training data is available or when generalizing... | David Guriel, Omer Goldman, Reut Tsarfaty |  |
| 231 |  |  [A Holistic Approach to Reference-Free Evaluation of Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.55) |  | 0 | Traditional machine translation evaluation relies on reference written by humans. While reference-free evaluation gets rid of the constraints of labor-intensive annotations, which can pivot easily to new domains and is more scalable. In this paper, we propose a reference-free evaluation approach... | Hanming Wu, Hui Di, Jinan Xu, Wenjuan Han, Yufeng Chen |  |
| 232 |  |  [Balancing Lexical and Semantic Quality in Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-short.56) |  | 0 | An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have... | Jeewoo Sul, Yong Suk Choi |  |
| 233 |  |  [Learning Neuro-Symbolic World Models with Conversational Proprioception](https://doi.org/10.18653/v1/2023.acl-short.57) |  | 0 | The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in... | Alexander Gray, Asim Munawar, Daiki Kimura, Don Joven Agravante, Michiaki Tatsubori |  |
| 234 |  |  [In and Out-of-Domain Text Adversarial Robustness via Label Smoothing](https://doi.org/10.18653/v1/2023.acl-short.58) |  | 0 | Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to... | Dan Roth, Insup Lee, Soham Dan, Yahan Yang |  |
| 235 |  |  [LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-short.59) |  | 0 | In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most... | Amirhossein Abaskohi, Sascha Rothe, Yadollah Yaghoobzadeh |  |
| 236 |  |  [Considerations for meaningful sign language machine translation based on glosses](https://doi.org/10.18653/v1/2023.acl-short.60) |  | 0 | Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss... | Amit Moryossef, Annette Rios, Mathias Müller, Sarah Ebling, Zifan Jiang |  |
| 237 |  |  [Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature](https://doi.org/10.18653/v1/2023.acl-short.61) |  | 0 | The COVID-19 pandemic created a deluge of questionable and contradictory scientific claims about drug efficacy – an “infodemic” with lasting consequences for science and society. In this work, we argue that NLP models can help domain experts distill and understand the literature in this complex,... | Christopher Potts, Daniel N. Sosa, Malavika Suresh, Russ B. Altman |  |
| 238 |  |  [The Role of Global and Local Context in Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.62) |  | 0 | Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an... | Arthur Amalvy, Richard Dufour, Vincent Labatut |  |
| 239 |  |  [Joint End-to-end Semantic Proto-role Labeling](https://doi.org/10.18653/v1/2023.acl-short.63) |  | 0 | Semantic proto-role labeling (SPRL) assigns properties to arguments based on a series of binary labels. While multiple studies have evaluated various approaches to SPRL, it has only been studied in-depth as a standalone task using gold predicate/argument pairs. How do SPRL systems perform as part... | Elizabeth Spaulding, Gary Kazantsev, Mark Dredze |  |
| 240 |  |  [Improving Automatic Quotation Attribution in Literary Novels](https://doi.org/10.18653/v1/2023.acl-short.64) |  | 0 | Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification,... | Adam Hammond, Frank Rudzicz, Graeme Hirst, Krishnapriya Vishnubhotla |  |
| 241 |  |  [Modular Visual Question Answering via Code Generation](https://doi.org/10.18653/v1/2023.acl-short.65) |  | 0 | We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and... | Andy Zeng, Arsha Nagrani, Cordelia Schmid, Dan Klein, Kevin Yang, Kushal Khangaonkar, Medhini Narasimhan, Sanjay Subramanian, Trevor Darrell |  |
| 242 |  |  [Target-Based Offensive Language Identification](https://doi.org/10.18653/v1/2023.acl-short.66) |  | 0 | We present TBO, a new dataset for Target-based Offensive language identification. TBO contains post-level annotations regarding the harmfulness of an offensive post and token-level annotations comprising of the target and the offensive argument expression. Popular offensive language identification... | Austin Simmmons, Kai North, Marcos Zampieri, Paridhi Khandelwal, Preslav Nakov, Sara Rosenthal, Skye Morgan, Tharindu Ranasinghe |  |
| 243 |  |  [Unsupervised Subtitle Segmentation with Masked Language Models](https://doi.org/10.18653/v1/2023.acl-short.67) |  | 0 | We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points. Our approach obtained competitive results in... | David Ponce, Thierry Etchegoyhen, Victor Ruiz Gómez |  |
| 244 |  |  [Exploring Continual Learning for Code Generation Models](https://doi.org/10.18653/v1/2023.acl-short.68) |  | 0 | Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that... | Bing Xiang, Dejiao Zhang, Hantian Ding, Ming Tan, Mohit Bansal, Murali Krishna Ramanathan, Parminder Bhatia, Prateek Yadav, Qing Sun, Ramesh Nallapati, Xiaofei Ma, Xiaopeng Li |  |
| 245 |  |  [Deep Active Learning for Morphophonological Processing](https://doi.org/10.18653/v1/2023.acl-short.69) |  | 0 | Building a system for morphological processing is a challenging task in morphologically complex languages like Arabic. Although there are some deep learning based models that achieve successful results, these models rely on a large amount of annotated data. Building such datasets, specially for... | Owen Rambow, Salam Khalifa, Seyed Abolghasem Mirroshandel, Seyed Morteza Mirbostani, Yasaman Boreshban |  |
| 246 |  |  [Counterfactual reasoning: Testing language models' understanding of hypothetical scenarios](https://doi.org/10.18653/v1/2023.acl-short.70) |  | 0 | Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging... | Allyson Ettinger, Jiaxuan Li, Lang Yu |  |
| 247 |  |  [Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages](https://doi.org/10.18653/v1/2023.acl-short.71) |  | 0 | We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text... | Anoop Kunchukuttan, Mitesh M. Khapra, Yash Madhani |  |
| 248 |  |  [Using contradictions improves question answering systems](https://doi.org/10.18653/v1/2023.acl-short.72) |  | 0 | This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is entailed (supported) by some background context. But is it useful to also determine if an answer... | Domenic Rosati, Etienne FortierDubois |  |
| 249 |  |  [Token-Level Self-Evolution Training for Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2023.acl-short.73) |  | 0 | Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g. word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the... | Dacheng Tao, Keqin Peng, Liang Ding, Qihuang Zhong, Wenge Rong, Yuanxin Ouyang, Zhang Xiong |  |
| 250 |  |  [Gradient Ascent Post-training Enhances Language Model Generalization](https://doi.org/10.18653/v1/2023.acl-short.74) |  | 0 | In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs... | Dongkeun Yoon, Joel Jang, Minjoon Seo, Sungdong Kim |  |
| 251 |  |  [An Open Dataset and Model for Language Identification](https://doi.org/10.18653/v1/2023.acl-short.75) |  | 0 | Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033%... | Alexandra Birch, Kenneth Heafield, Laurie Burchell, Nikolay Bogoychev |  |
| 252 |  |  [Evaluating Paraphrastic Robustness in Textual Entailment Models](https://doi.org/10.18653/v1/2023.acl-short.76) |  | 0 | We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the... | Adam Poliak, Benjamin Van Durme, Dhruv Verma, Shreyashee Sinha, Yash Kumar Lal |  |
| 253 |  |  [Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?](https://doi.org/10.18653/v1/2023.acl-short.77) |  | 0 | Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble... | Chenming Tang, Xiuyu Wu, Yunfang Wu |  |
| 254 |  |  [Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality](https://doi.org/10.18653/v1/2023.acl-short.78) |  | 0 | Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro ‘MODEL’}name (i.e. Effective Factual Summarization),... | Fei Wang, Muhao Chen, Tanay Dixit |  |
| 255 |  |  [With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness](https://doi.org/10.18653/v1/2023.acl-short.79) |  | 0 | Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such... | Anette Frank, Julius Steen, Juri Opitz, Katja Markert |  |
| 256 |  |  [A Better Way to Do Masked Language Model Scoring](https://doi.org/10.18653/v1/2023.acl-short.80) |  | 0 | Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the... | Anna A. Ivanova, Carina Kauf |  |
| 257 |  |  [ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?](https://doi.org/10.18653/v1/2023.acl-short.81) |  | 0 | Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general... | Benjamin Matthias Ruppik, Carel van Niekerk, Christian Geishauser, HsienChin Lin, Michael Heck, Milica Gasic, Nurul Lubis, Renato Vukovic, Shutong Feng |  |
| 258 |  |  [Controllable Mixed-Initiative Dialogue Generation through Prompting](https://doi.org/10.18653/v1/2023.acl-short.82) |  | 0 | Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning... | Maximillian Chen, Urvi Awasthi, Weiyan Shi, Xiao Yu, Zhou Yu |  |
| 259 |  |  [Enhancing Event Causality Identification with Counterfactual Reasoning](https://doi.org/10.18653/v1/2023.acl-short.83) |  | 0 | Existing methods for event causality identification (ECI) focus on mining potential causal signals, i.e., causal context keywords and event pairs. However, causal signals are ambiguous, which may lead to the context-keywords bias and the event-pairs bias. To solve this issue, we propose the... | Feiteng Mu, Wenjie Li |  |
| 260 |  |  [Contrastive Bootstrapping for Label Refinement](https://doi.org/10.18653/v1/2023.acl-short.84) |  | 0 | Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained... | Muhao Chen, Shudi Hou, Sujian Li, Yu Xia |  |
| 261 |  |  [NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification](https://doi.org/10.18653/v1/2023.acl-short.85) |  | 0 | Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other... | Anna Feldman, David Ifeoluwa Adelani, Iyanuoluwa Shode, Jing Peng |  |
| 262 |  |  [Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement](https://doi.org/10.18653/v1/2023.acl-short.86) |  | 0 | State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with... | Kai Sun, Nikolaos Aletras, Samuel Mensah |  |
| 263 |  |  [An (unhelpful) guide to selecting the best ASR architecture for your under-resourced language](https://doi.org/10.18653/v1/2023.acl-short.87) |  | 0 | Advances in deep neural models for automatic speech recognition (ASR) have yielded dramatic improvements in ASR quality for resource-rich languages, with English ASR now achieving word error rates comparable to that of human transcribers. The vast majority of the world’s languages, however, lack... | Emily Prud'hommeaux, Robert Jimerson, Zoey Liu |  |
| 264 |  |  [The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics](https://doi.org/10.18653/v1/2023.acl-short.88) |  | 0 | Many NLP tasks exhibit human label variation, where different annotators give different labels to the same texts. This variation is known to depend, at least in part, on the sociodemographics of annotators. Recent research aims to model individual annotator behaviour rather than predicting... | Dirk Hovy, Matthias Orlikowski, Paul Röttger, Philipp Cimiano |  |
| 265 |  |  [Decomposed scoring of CCG dependencies](https://doi.org/10.18653/v1/2023.acl-short.89) |  | 0 | In statistical parsing with CCG, the standard evaluation method is based on predicate-argument structure and evaluates dependencies labelled in part by lexical categories. When a predicate has multiple argument slots that can be filled, the same lexical category is used for the label of multiple... | Aditya Bhargava, Gerald Penn |  |
| 266 |  |  [Do GPTs Produce Less Literal Translations?](https://doi.org/10.18653/v1/2023.acl-short.90) |  | 0 | Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better... | Arul Menezes, Hany Hassan, Matt Post, Vikas Raunak |  |
| 267 |  |  [Environmental Claim Detection](https://doi.org/10.18653/v1/2023.acl-short.91) |  | 0 | To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable. To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this. Thus, this paper introduces... | Dominik Stammbach, Julia Anna Bingler, Markus Leippold, Mathias Kraus, Nicolas Webersinke |  |
| 268 |  |  [Black-box language model explanation by context length probing](https://doi.org/10.18653/v1/2023.acl-short.92) |  | 0 | The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present \*context length probing\*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of... | Antoine Liutkus, Ondrej Cífka |  |
| 269 |  |  [Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation](https://doi.org/10.18653/v1/2023.acl-short.93) |  | 0 | Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings. Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any... | Hongzhi Zhang, Kaiwen Wei, Sirui Wang, Wei Wu, Yuntao Li |  |
| 270 |  |  [The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics](https://doi.org/10.18653/v1/2023.acl-short.94) |  | 0 | Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, “black boxes” returning a single... | Alon Lavie, André F. T. Martins, Luísa Coheur, Marcos V. Treviso, Nuno Miguel Guerreiro, Ricardo Rei |  |
| 271 |  |  [Typo-Robust Representation Learning for Dense Retrieval](https://doi.org/10.18653/v1/2023.acl-short.95) |  | 0 | Dense retrieval is a basic building block of information retrieval applications. One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words. A popular approach for handling misspelled queries is minimizing the representations... | Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Panuthep Tasawong, Peerat Limkonchotiwat, Sarana Nutanong, Wuttikorn Ponwitayarat |  |
| 272 |  |  [Focused Prefix Tuning for Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-short.96) |  | 0 | In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on... | Congda Ma, Kei Sawada, Makoto Shing, Manabu Okumura, Tianyu Zhao |  |
| 273 |  |  [ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-short.97) |  | 0 | Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate... | Aashiq Muhamed, Aditya Anantharaman, Belinda Zeng, Changyou Chen, Guoyin Wang, Jianyi Zhang, Kai Zhong, Qingjun Cui, Trishul Chilimbi, Yi Xu, Yiran Chen |  |
| 274 |  |  [Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood](https://doi.org/10.18653/v1/2023.acl-short.98) |  | 0 | Recognizing flat, overlapped and discontinuous entities uniformly has been paid increasing attention. Among these works, Seq2Seq formulation prevails for its flexibility and effectiveness. It arranges the output entities into a specific target sequence. However, it introduces bias by assigning all... | Sujian Li, Wenhao Wu, Yongwei Zhao, Yu Xia |  |
| 275 |  |  [Deriving Language Models from Masked Language Models](https://doi.org/10.18653/v1/2023.acl-short.99) |  | 0 | Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions... | Lucas Torroba Hennigen, Yoon Kim |  |
| 276 |  |  [UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation](https://doi.org/10.18653/v1/2023.acl-short.100) |  | 0 | Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations,... | Huimin Wang, KamFai Wong, Yiming Du, Zhiming Mao |  |
| 277 |  |  [Reasoning Implicit Sentiment with Chain-of-Thought Prompting](https://doi.org/10.18653/v1/2023.acl-short.101) |  | 0 | While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense... | Bobo Li, Fei Li, Hao Fei, Lidong Bing, Qian Liu, TatSeng Chua |  |
| 278 |  |  [Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings](https://doi.org/10.18653/v1/2023.acl-short.102) |  | 0 | The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of... | Alexander Rudnicky, LiWei Chen, Peter J. Ramadge, TaChung Chi, TingHan Fan |  |
| 279 |  |  [Is Anisotropy Truly Harmful? A Case Study on Text Clustering](https://doi.org/10.18653/v1/2023.acl-short.103) |  | 0 | In the last few years, several studies have been devoted to dissecting dense text representations in order to understand their effectiveness and further improve their quality. Particularly, the anisotropy of such representations has been observed, which means that the directions of the word vectors... | Mira Ait Saada, Mohamed Nadif |  |
| 280 |  |  [Class based Influence Functions for Error Detection](https://doi.org/10.18653/v1/2023.acl-short.104) |  | 0 | Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are... | Anh T. V. Dau, Hieu Nguyen, Hoang ThanhTung, HuuTien Dang, Nghi D. Q. Bui, Quan Hung Tran, Thang NguyenDuc |  |
| 281 |  |  [Leveraging Prefix Transfer for Multi-Intent Text Revision](https://doi.org/10.18653/v1/2023.acl-short.105) |  | 0 | Text revision is a necessary process to improve text quality. During this process, writers constantly edit texts out of different edit intentions. Identifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts... | Cunliang Kong, Erhong Yang, Hanghang Fan, Liner Yang, Liu Wu, Ruining Chong, Yange Fan, Zhenghao Liu, Ziye Jin |  |
| 282 |  |  [Learning Multi-Step Reasoning by Solving Arithmetic Tasks](https://doi.org/10.18653/v1/2023.acl-short.106) |  | 0 | Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs’ impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions... | Tianduo Wang, Wei Lu |  |
| 283 |  |  [Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning](https://doi.org/10.18653/v1/2023.acl-short.107) |  | 0 | Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on... | Chengyu Wang, Chuanqi Tan, Haiyang Xu, Jun Huang, Songfang Huang, Zhenru Zhang |  |
| 284 |  |  [Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting](https://doi.org/10.18653/v1/2023.acl-short.108) |  | 0 | Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would... | Caiming Xiong, Chen Xing, Wenhao Liu, Zahra Fatemi |  |
| 285 |  |  [Class-Incremental Learning based on Label Generation](https://doi.org/10.18653/v1/2023.acl-short.109) |  | 0 | Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label... | Bing Liu, Dongyan Zhao, Yiduo Guo, Yijia Shao |  |
| 286 |  |  [Evaluating pragmatic abilities of image captioners on A3DS](https://doi.org/10.18653/v1/2023.acl-short.110) |  | 0 | Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans. To enable such evaluation, we present a novel... | Michael Franke, Polina Tsvilodub |  |
| 287 |  |  [The Art of Prompting: Event Detection based on Type Specific Prompts](https://doi.org/10.18653/v1/2023.acl-short.111) |  | 0 | We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zero-shot event detection. The experimental results demonstrate that a well-defined and comprehensive event type prompt can... | Lifu Huang, Mo Yu, Sijia Wang |  |
| 288 |  |  [Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.112) |  | 0 | This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm... | Chenhui Chu, Haiyue Song, Qianying Liu, Raj Dabre, Sadao Kurohashi, Zhuoyuan Mao |  |
| 289 |  |  [Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning](https://doi.org/10.18653/v1/2023.acl-short.113) |  | 0 | Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive... | Nanyun Peng, PoNien Kung |  |
| 290 |  |  [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models](https://doi.org/10.18653/v1/2023.acl-short.114) |  | 0 | We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to... | James O'Neill, Sourav Dutta |  |
| 291 |  |  [Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.acl-short.115) |  | 0 | Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace ”modality gap” between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap... | Chen Xu, Jingbo Zhu, Tong Xiao, Yuchen Han |  |
| 292 |  |  [Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data](https://doi.org/10.18653/v1/2023.acl-short.116) |  | 0 | Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the... | Cong Liu, Haifeng Chen, Xiao Yu, Yanchi Liu, Yufei Li |  |
| 293 |  |  [Text-to-SQL Error Correction with Language Models of Code](https://doi.org/10.18653/v1/2023.acl-short.117) |  | 0 | Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose... | Ali Payani, Huan Sun, Jayanth Srinivasa, Michael White, Raymond J. Mooney, Shijie Chen, Yu Su, Ziru Chen |  |
| 294 |  |  [The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks](https://doi.org/10.18653/v1/2023.acl-short.118) |  | 0 | How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which... | Daniel Khashabi, KaiWei Chang, Nikil Roashan Selvam, Sunipa Dev, Tushar Khot |  |
| 295 |  |  [Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)](https://doi.org/10.18653/v1/2023.acl-short.119) |  | 0 | Large language models, particularly GPT-3, are able to produce high quality summaries ofgeneral domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine. In this paper we enlist domain experts... | Byron C. Wallace, Chantal Shaib, Iain James Marshall, Junyi Jessy Li, Millicent L. Li, Sebastian Joseph |  |
| 296 |  |  [Prefix Propagation: Parameter-Efficient Tuning for Long Sequences](https://doi.org/10.18653/v1/2023.acl-short.120) |  | 0 | Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model’s parameters. Although such models... | Jonathan Li, Rohan Bhambhoria, Will Aitken, Xiaodan Zhu |  |
| 297 |  |  [Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain](https://doi.org/10.18653/v1/2023.acl-short.121) |  | 0 | PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common. It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively.... | Liangze Li, ShihLun Wu, YiHui Chou |  |
| 298 |  |  [Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations](https://doi.org/10.18653/v1/2023.acl-short.122) |  | 0 | Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English–German: the better the given MT is, the harder... | Baikjin Jung, JongHyeok Lee, Myungji Lee, Yunsu Kim |  |
| 299 |  |  [An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-short.123) |  | 0 | Named entity recognition (NER) is the task to detect and classify entity spans in the text. When entity spans overlap between each other, the task is named as nested NER. Span-based methods have been widely used to tackle nested NER. Most of these methods get a score matrix, where each entry... | Hang Yan, Xiaonan Li, Xipeng Qiu, Yu Sun |  |
| 300 |  |  [Hexatagging: Projective Dependency Parsing as Tagging](https://doi.org/10.18653/v1/2023.acl-short.124) |  | 0 | We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the... | Afra Amini, Ryan Cotterell, Tianyu Liu |  |
| 301 |  |  [Understanding Demonstration-based Learning from a Causal Perspective](https://doi.org/10.18653/v1/2023.acl-short.125) |  | 0 | Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal... | Ruiyi Zhang, Tong Yu |  |
| 302 |  |  [RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation](https://doi.org/10.18653/v1/2023.acl-short.126) |  | 0 | Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in... | Anna Currey, Benjamin Hsu, Gabriele Sarti, Georgiana Dinu, Maria Nadejde, Phu Mon Htut, Xing Niu |  |
| 303 |  |  [Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation](https://doi.org/10.18653/v1/2023.acl-short.127) |  | 0 | Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this... | Alexander G. Hauptmann, Haoyang Wen |  |
| 304 |  |  [Discourse-Level Representations can Improve Prediction of Degree of Anxiety](https://doi.org/10.18653/v1/2023.acl-short.128) |  | 0 | Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we... | Adithya V. Ganesan, H. Andrew Schwartz, Johannes C. Eichstaedt, Matthew Matero, Swanie Juhng, Vasudha Varadarajan |  |
| 305 |  |  [Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning](https://doi.org/10.18653/v1/2023.acl-short.129) |  | 0 | Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction... | Charith Peris, Christophe Dupuy, Haidar Khan, Jack FitzGerald, Jimit Majmudar, Mustafa Özdayi, Rahil Parikh, Rahul Gupta |  |
| 306 |  |  [MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting](https://doi.org/10.18653/v1/2023.acl-short.130) |  | 0 | Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge... | Fei Cheng, Hirokazu Kiyomaru, Sadao Kurohashi, Tatsuro Inaba |  |
| 307 |  |  [mPMR: A Multilingual Pre-trained Machine Reader at Scale](https://doi.org/10.18653/v1/2023.acl-short.131) |  | 0 | We present multilingual Pre-trained Machine Reader (mPMR), a novel method for multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims to guide multilingual pre-trained language models (mPLMs) to perform natural language understanding (NLU) including both sequence... | Lidong Bing, Wai Lam, Weiwen Xu, Xin Li |  |
| 308 |  |  [MOSPC: MOS Prediction Based on Pairwise Comparison](https://doi.org/10.18653/v1/2023.acl-short.132) |  | 0 | As a subjective metric to evaluate the quality of synthesized speech, Mean opinion score(MOS) usually requires multiple annotators to score the same speech. Such an annotation approach requires a lot of manpower and is also time-consuming. MOS prediction model for automatic evaluation can... | Kexin Wang, Mingxuan Wang, Qianqian Dong, Tom Ko, Yunlong Zhao |  |
| 309 |  |  [LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering](https://doi.org/10.18653/v1/2023.acl-short.133) |  | 0 | Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each. These fixed vectors can be... | Adrià de Gispert, Bill Byrne, Gonzalo Iglesias, Rexhina Blloshmi, Weizhe Lin |  |
| 310 |  |  [How Well Apply Simple MLP to Incomplete Utterance Rewriting?](https://doi.org/10.18653/v1/2023.acl-short.134) |  | 0 | Incomplete utterance rewriting (IUR) aims to restore the incomplete utterance with sufficient context information for comprehension. This paper introduces a simple yet efficient IUR method. Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic... | Guanglai Gao, Jiang Li, Xiangdong Su, Xinlan Ma |  |
| 311 |  |  [XL-LEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE](https://doi.org/10.18653/v1/2023.acl-short.135) |  | 0 | The recent introduction of large-scale datasets for the WiC (Word in Context) task enables the creation of more reliable and meaningful contextualized word embeddings.However, most of the approaches to the WiC task use cross-encoders, which prevent the possibility of deriving comparable word... | Giovanni Semeraro, Lucia Siciliani, Marco de Gemmis, Pierluigi Cassotti, Pierpaolo Basile |  |
| 312 |  |  [Theory-Grounded Computational Text Analysis](https://doi.org/10.18653/v1/2023.acl-short.136) |  | 0 | In this position paper, we argue that computational text analysis lacks and requires organizing principles. A broad space separates its two constituent disciplines—natural language processing and social science—which has to date been sidestepped rather than filled by applying increasingly complex... | Arya D. McCarthy, Giovanna Maria Dora Dore |  |
| 313 |  |  [AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing](https://doi.org/10.18653/v1/2023.acl-short.137) |  | 0 | In this paper, we examine the current state-of-the-art in AMR parsing, which relies on ensemble strategies by merging multiple graph predictions. Our analysis reveals that the present models often violate AMR structural constraints. To address this issue, we develop a validation method, and show... | Abelardo Carlos Martinez Lorenzo, PereLluís Huguet Cabot, Roberto Navigli |  |
| 314 |  |  [MolXPT: Wrapping Molecules with Text for Generative Pre-training](https://doi.org/10.18653/v1/2023.acl-short.138) |  | 0 | Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified... | Lijun Wu, Ming Zhang, Shufang Xie, Tao Qin, TieYan Liu, Wei Zhang, Yingce Xia, Zequn Liu |  |
| 315 |  |  [A Study on the Efficiency and Generalization of Light Hybrid Retrievers](https://doi.org/10.18653/v1/2023.acl-short.139) |  | 0 | Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study “Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance”? Driven by this question, we... | Anchit Gupta, Arash Einolghozati, Barlas Oguz, Chitta Baral, Debojeet Chatterjee, Man Luo, Peyman Heidari, Shashank Jain, Xilun Chen |  |
| 316 |  |  [The Mechanical Bard: An Interpretable Machine Learning Approach to Shakespearean Sonnet Generation](https://doi.org/10.18653/v1/2023.acl-short.140) |  | 0 | We consider the automated generation of sonnets, a poetic form constrained according to meter, rhyme scheme, and length. Sonnets generally also use rhetorical figures, expressive language, and a consistent theme or narrative. Our constrained decoding approach allows for the generation of sonnets... | Cynthia Rudin, Edwin Agnew, Lily Zhu, Michelle Qiu, Sam Wiseman |  |
| 317 |  |  [When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants](https://doi.org/10.18653/v1/2023.acl-short.141) |  | 0 | We present the first unified study of the efficiency of self-attention-based Transformer variants spanning text, speech and vision. We identify input length thresholds (tipping points) at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency... | Anuj Diwan, David Harwath, Eunsol Choi |  |
| 318 |  |  [Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction (ACE) Annotations](https://doi.org/10.18653/v1/2023.acl-short.142) |  | 0 | Zero-shot event extraction (EE) methods infer richly structured event records from text, based only on a minimal user specification and no training examples, which enables flexibility in exploring and developing applications. Most event extraction research uses the Automatic Content Extraction... | Brendan T. O'Connor, Erica Cai |  |
| 319 |  |  [Event Extraction as Question Generation and Answering](https://doi.org/10.18653/v1/2023.acl-short.143) |  | 0 | Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without... | Alejandro Jaimes, Di Lu, Joel R. Tetreault, Shihao Ran |  |
| 320 |  |  [Are Sample-Efficient NLP Models More Robust?](https://doi.org/10.18653/v1/2023.acl-short.144) |  | 0 | Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-ofdistribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three... | Ananya Kumar, Nelson F. Liu, Percy Liang, Robin Jia |  |
| 321 |  |  [Diversity-Aware Coherence Loss for Improving Neural Topic Models](https://doi.org/10.18653/v1/2023.acl-short.145) |  | 0 | The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents,... | Felipe GonzálezPizarro, Gabriel Murray, Giuseppe Carenini, Linzi Xing, Raymond Li |  |
| 322 |  |  [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://doi.org/10.18653/v1/2023.acl-short.146) |  | 0 | Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that... | Daniel Cheng, Haoxin Li, Jungo Kasai, Noah A. Smith, Phillip Keung |  |
| 323 |  |  [S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering](https://doi.org/10.18653/v1/2023.acl-short.147) |  | 0 | Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of... | Fangyu Lei, Jun Zhao, Kang Liu, Shizhu He, Xiang Li, Yifan Wei, Yiming Huang |  |
| 324 |  |  [Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue](https://doi.org/10.18653/v1/2023.acl-short.148) |  | 0 | Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts. We argue that this problem mainly stems from the polarized... | Bin Sun, Fanhu Bie, Fei Mi, Kan Li, Yitong Li, Yiwei Li |  |
| 325 |  |  [AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models](https://doi.org/10.18653/v1/2023.acl-short.149) |  | 0 | Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation... | Cheng Yang, Lifeng Shang, Qun Liu, Siheng Li, Xin Jiang, Xinyu Zhu, Yichun Yin, Yujiu Yang, Zesen Cheng |  |
| 326 |  |  [STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions](https://doi.org/10.18653/v1/2023.acl-short.150) |  | 0 | We present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The data is collected using a web app in which the speakers are shown Standard German sentences, which they translate to Swiss German and record. We make the corpus publicly available.... | Christian Scheller, Claudio Paonessa, Jan Deriu, Julia Hartmann, Larissa Schmidt, Manfred Vogel, Manuela Hürlimann, Mark Cieliebak, Michel Plüss, Tanja Samardzic, Yanick Schraner |  |
| 327 |  |  [Teaching Small Language Models to Reason](https://doi.org/10.18653/v1/2023.acl-short.151) |  | 0 | Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we... | Aliaksei Severyn, Eric Malmi, Jakub Adámek, Jonathan Mallinson, Lucie Charlotte Magister |  |
| 328 |  |  [A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification](https://doi.org/10.18653/v1/2023.acl-short.152) |  | 0 | In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to... | Lei Chen, Rohan Bhambhoria, Xiaodan Zhu |  |
| 329 |  |  [A Simple Concatenation can Effectively Improve Speech Translation](https://doi.org/10.18653/v1/2023.acl-short.153) |  | 0 | A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to... | Boxing Chen, Kai Fan, Linlin Zhang, Luo Si |  |
| 330 |  |  [ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning](https://doi.org/10.18653/v1/2023.acl-short.154) |  | 0 | A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we... | Atticus Geiger, Christopher Potts, Jingyuan Selena She, Samuel R. Bowman |  |
| 331 |  |  [Revisiting Automated Prompting: Are We Actually Doing Better?](https://doi.org/10.18653/v1/2023.acl-short.155) |  | 0 | Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In... | Ilia Shumailov, Robert D. Mullins, Yarin Gal, Yiren Zhao, Yulin Zhou |  |
| 332 |  |  [Mind the Gap between the Application Track and the Real World](https://doi.org/10.18653/v1/2023.acl-short.156) |  | 0 | Recent advances in NLP have led to a rise in inter-disciplinary and application-oriented research. While this demonstrates the growing real-world impact of the field, research papers frequently feature experiments that do not account for the complexities of realistic data and environments. To... | Ananya Ganesh, E. Margaret Perkoff, Jie Cao, Katharina Kann, Martha Palmer, Rosy Southwell |  |
| 333 |  |  [How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives](https://doi.org/10.18653/v1/2023.acl-short.157) |  | 0 | Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge,... | Barbara Plank, Hinrich Schütze, Leonie Weissweiler, Xinpeng Wang |  |
| 334 |  |  [ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-short.158) |  | 0 | Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds... | Anastasiia Sedova, Benjamin Roth |  |
| 335 |  |  [Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-short.159) |  | 0 | Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but... | Hao Cheng, Hao Fang, Jianfeng Gao, Xiaodong Liu |  |
| 336 |  |  [Linear Classifier: An Often-Forgotten Baseline for Text Classification](https://doi.org/10.18653/v1/2023.acl-short.160) |  | 0 | Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way... | ChihJen Lin, JieJyun Liu, SiAn Chen, YuChen Lin |  |
| 337 |  |  [Randomized Positional Encodings Boost Length Generalization of Transformers](https://doi.org/10.18653/v1/2023.acl-short.161) |  | 0 | Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the... | Anian Ruoss, Grégoire Delétang, Joel Veness, Jordi GrauMoya, Mehdi Bennani, Róbert Csordás, Shane Legg, Tim Genewein |  |
| 338 |  |  [Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models](https://doi.org/10.18653/v1/2023.acl-short.162) |  | 0 | In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V & L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its... | Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe |  |
| 339 |  |  [Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints](https://doi.org/10.18653/v1/2023.acl-short.163) |  | 0 | Neural QCFG is a grammar-based sequence-to-sequence model with strong inductive biases on hierarchical structures. It excels in interpretability and generalization but suffers from expensive inference. In this paper, we study two low-rank variants of Neural QCFG for faster inference with different... | Chao Lou, Kewei Tu |  |
| 340 |  |  [TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation](https://doi.org/10.18653/v1/2023.acl-short.164) |  | 0 | Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model’s mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a... | Kai Yu, Rui Wang, Yiming Ai, Zhiwei He |  |
| 341 |  |  [Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023](https://aclanthology.org/volumes/2023.findings-acl/) |  | 0 |  | Anna Rogers, Jordan L. BoydGraber, Naoaki Okazaki |  |
| 342 |  |  [Frontmatter](https://aclanthology.org/2023.findings-acl.0) |  | 0 |  |  |  |
| 343 |  |  [Investigating Glyph-Phonetic Information for Chinese Spell Checking: What Works and What's Next?](https://doi.org/10.18653/v1/2023.findings-acl.1) |  | 0 | While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability of CSC models to... | Hang Yan, Xiaotian Zhang, Xipeng Qiu, Yanjun Zheng |  |
| 344 |  |  [A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions](https://doi.org/10.18653/v1/2023.findings-acl.2) |  | 0 | We investigate the representation of pretrained language models and humans, using the idea of word definition modeling–how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its... | Hwiyeol Jo |  |
| 345 |  |  [Conformal Nucleus Sampling](https://doi.org/10.18653/v1/2023.findings-acl.3) |  | 0 | Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-p) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. In this work, we assess whether a top-p set is indeed aligned... | Jacob Goldberger, Shauli Ravfogel, Yoav Goldberg |  |
| 346 |  |  [DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.4) |  | 0 | Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation... | Chunkit Chan, Ginny Y. Wong, Jiayang Cheng, Simon See, Xin Liu, Yangqiu Song, Zihan Li |  |
| 347 |  |  [Modularized Zero-shot VQA with Pre-trained Models](https://doi.org/10.18653/v1/2023.findings-acl.5) |  | 0 | Large-scale pre-trained models (PTMs) show great zero-shot capabilities. In this paper, we study how to leverage them for zero-shot visual question answering (VQA).Our approach is motivated by a few observations. First, VQA questions often require multiple steps of reasoning, which is still a... | Jing Jiang, Rui Cao |  |
| 348 |  |  [TimelineQA: A Benchmark for Question Answering over Timelines](https://doi.org/10.18653/v1/2023.findings-acl.6) |  | 0 | Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical... | Alon Y. Halevy, Jane DwivediYu, Jing Nathan Yan, Lambert Mathias, Marzieh Saeidi, WangChiew Tan, Yuliang Li |  |
| 349 |  |  [Abstractive Text Summarization Using the BRIO Training Paradigm](https://doi.org/10.18653/v1/2023.findings-acl.7) |  | 0 | Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model’s dependence on reference summaries, and improve... | Jugal Kalita, Khang Nhut Lam, Khang Thua Pham, Thieu Gia Doan |  |
| 350 |  |  [Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization](https://doi.org/10.18653/v1/2023.findings-acl.8) |  | 0 | Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising,... | Qi Zhang, Rui Zheng, Tao Gui, Ting Wu, Xuanjing Huang |  |
| 351 |  |  [Pre-training Language Model as a Multi-perspective Course Learner](https://doi.org/10.18653/v1/2023.findings-acl.9) |  | 0 | ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked... | Beiduo Chen, Furu Wei, Haizhen Huang, Qi Zhang, Shaohan Huang, Weiwei Deng, Wu Guo, Zhenhua Ling, Zihan Zhang |  |
| 352 |  |  [Layerwise universal adversarial attack on NLP models](https://doi.org/10.18653/v1/2023.findings-acl.10) |  | 0 | In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models... | Andrei Petrovskii, Danil Malaev, Ivan V. Oseledets, Olga Tsymboi |  |
| 353 |  |  [Scene-robust Natural Language Video Localization via Learning Domain-invariant Representations](https://doi.org/10.18653/v1/2023.findings-acl.11) |  | 0 | Natural language video localization(NLVL) task involves the semantic matching of a text query with a moment from an untrimmed video. Previous methods primarily focus on improving performance with the assumption of independently identical data distribution while ignoring the out-of-distribution... | Haifeng Huang, Yan Xia, Yang Zhao, Zehan Wang, Zhou Zhao |  |
| 354 |  |  [Exploiting Pseudo Image Captions for Multimodal Summarization](https://doi.org/10.18653/v1/2023.findings-acl.12) |  | 0 | Multimodal summarization with multimodal output (MSMO) faces a challenging semantic gap between visual and textual modalities due to the lack of reference images for training. Our pilot investigation indicates that image captions, which naturally connect texts and images, can significantly benefit... | Chaoya Jiang, Jinan Sun, Rui Xie, Shikun Zhang, Wei Ye |  |
| 355 |  |  [Cross-Lingual Transfer with Target Language-Ready Task Adapters](https://doi.org/10.18653/v1/2023.findings-acl.13) |  | 0 | Adapters have emerged as a modular and parameter-efficient approach to (zero-shot) cross-lingual transfer. The established MAD-X framework employs separate language and task adapters which can be arbitrarily combined to perform the transfer of any task to any target language. Subsequently, BAD-X,... | Alan Ansell, Anna Korhonen, Ivan Vulic, Marinela Parovic |  |
| 356 |  |  [DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance](https://doi.org/10.18653/v1/2023.findings-acl.14) |  | 0 | Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user’s needs. Further, the... | Diyi Yang, Jiawei Han, Karthik Venkat Ramanan, Nishant Balepur, Shivam Agarwal, Susik Yoon |  |
| 357 |  |  [Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization](https://doi.org/10.18653/v1/2023.findings-acl.15) |  | 0 | Along with the performance improvement in NLP domain, the sizes of transformer-based language models (TLM) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient... | Chong Yu, Tao Chen, Zhongxue Gan |  |
| 358 |  |  [RMSSinger: Realistic-Music-Score based Singing Voice Synthesis](https://doi.org/10.18653/v1/2023.findings-acl.16) |  | 0 | We are interested in a challenging task, Realistic-Music-Score based Singing Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent... | Chenye Cui, Huadai Liu, Jinglin Liu, Jinzheng He, Rongjie Huang, Zhenhui Ye, Zhou Zhao |  |
| 359 |  |  [Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.17) |  | 0 | The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying... | HuiChi Kuo, YunNung Chen |  |
| 360 |  |  [MTGP: Multi-turn Target-oriented Dialogue Guided by Generative Global Path with Flexible Turns](https://doi.org/10.18653/v1/2023.findings-acl.18) |  | 0 | Target-oriented dialogue guides the dialogue to a target quickly and smoothly. The latest approaches focus on global planning, which plans toward the target before the conversation instead of adopting a greedy strategy during the conversation. However, the global plan in existing works is fixed to... | Anqi Liu, Bo Wang, Dongming Zhao, Kun Huang, Ruifang He, Yue Tan, Yuexian Hou |  |
| 361 |  |  [The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python](https://doi.org/10.18653/v1/2023.findings-acl.19) |  | 0 | Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit,... | Antonio Valerio Miceli Barone, Fazl Barez, Ioannis Konstas, Shay B. Cohen |  |
| 362 |  |  [Class Lifelong Learning for Intent Detection via Structure Consolidation Networks](https://doi.org/10.18653/v1/2023.findings-acl.20) |  | 0 | Intent detection, which estimates diverse intents behind user utterances, is an essential component of task-oriented dialogue systems. Previous intent detection models are usually trained offline, which can only handle predefined intent classes. In the real world, new intents may keep challenging... | Bo Li, Dianbo Sui, Jiaoyan Chen, Jun Zhao, Kang Liu, Ningyu Zhang, Qingbin Liu, Shizhu He, Xi Chen, Xiaolong Liu, Yanchao Hao |  |
| 363 |  |  [On Evaluating and Mitigating Gender Biases in Multilingual Settings](https://doi.org/10.18653/v1/2023.findings-acl.21) |  | 0 | While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual... | Aniket Vashishtha, Kabir Ahuja, Sunayana Sitaram |  |
| 364 |  |  [Rethinking Round-Trip Translation for Machine Translation Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.22) |  | 0 | Automatic evaluation methods for translation often require model training, and thus the availability of parallel corpora limits their applicability to low-resource settings. Round-trip translation is a potential workaround, which can reframe bilingual evaluation into a much simpler monolingual... | Qiongkai Xu, Terry Yue Zhuo, Trevor Cohn, Xuanli He |  |
| 365 |  |  [G³R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.findings-acl.23) |  | 0 | We present a framework called G3R for complex and cross-domain Text-to-SQL generation. G3R aims to address two limitations of current approaches: (1) The structure of the abstract syntax tree (AST) is not fully explored during the decoding process which is crucial for complex SQL generation; (2)... | Deyu Zhou, QianWen Zhang, Xu Zhang, Yanzheng Xiang, Yunbo Cao, Zejie Liu |  |
| 366 |  |  [A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks](https://doi.org/10.18653/v1/2023.findings-acl.24) |  | 0 | By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose... | Leye Wang, Ruiqing Ding, Xiao Han |  |
| 367 |  |  [Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.25) |  | 0 | Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored... | Dongding Lin, Jian Wang, Wenjie Li |  |
| 368 |  |  [A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection](https://doi.org/10.18653/v1/2023.findings-acl.26) |  | 0 | Hyperbole and metaphor are common in day-to-day communication (e.g., “I am in deep trouble”: how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these... | Abisek Rajakumar Kalarani, Naveen Badathala, Pushpak Bhattacharyya, Tejpalsingh Siledar |  |
| 369 |  |  [Prompt Tuning for Unified Multimodal Pretrained Models](https://doi.org/10.18653/v1/2023.findings-acl.27) |  | 0 | Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low... | An Yang, Chang Zhou, Hao Yang, Junyang Lin, Peng Wang |  |
| 370 |  |  [Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.28) |  | 0 | Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the... | Dongsheng Li, Linbo Qiao, Yi Han, Yifu Gao, Yongquan He, Zhigang Kan |  |
| 371 |  |  [A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets](https://doi.org/10.18653/v1/2023.findings-acl.29) |  | 0 | The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In... | Jimmy Xiangji Huang, M. Saiful Bari, Md Amran Hossen Bhuiyan, Md. Tahmid Rahman Laskar, Mizanur Rahman, Shafiq Joty |  |
| 372 |  |  [Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference](https://doi.org/10.18653/v1/2023.findings-acl.30) |  | 0 | This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may... | Baoquan Zhao, Jian Yin, Jianxing Yu, Libin Zheng, Qinliang Su, Shiqi Wang, Wei Liu |  |
| 373 |  |  [TADA: Efficient Task-Agnostic Domain Adaptation for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.31) |  | 0 | Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been... | ChiaChien Hung, Jannik Strötgen, Lukas Lange |  |
| 374 |  |  [Robust Natural Language Understanding with Residual Attention Debiasing](https://doi.org/10.18653/v1/2023.findings-acl.32) |  | 0 | Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods... | Fei Wang, James Y. Huang, Muhao Chen, Tianyi Yan, Wenxuan Zhou |  |
| 375 |  |  [MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.33) |  | 0 | Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizes all history information, the dialogue state in the previous turn is typically adopted as the input for predicting the current... | Haipeng Sun, Haoning Zhang, Junwei Bao, Shuguang Cui, Wenye Li, Xiaodong He, Youzheng Wu |  |
| 376 |  |  [PAL: Persona-Augmented Emotional Support Conversation Generation](https://doi.org/10.18653/v1/2023.findings-acl.34) |  | 0 | Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers’ persona... | Hao Sun, Jiale Cheng, Minlie Huang, Sahand Sabour, Zhuang Chen |  |
| 377 |  |  [Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model](https://doi.org/10.18653/v1/2023.findings-acl.35) |  | 0 | Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset... | Jie Zhou, Junzhe Wang, Menghan Zhang, Qi Zhang, Songyang Gao, Tao Gui, Weikang Zhou, Xiang Gao, Xiao Wang, Yunwen Chen |  |
| 378 |  |  [Exclusive Supermask Subnetwork Training for Continual Learning](https://doi.org/10.18653/v1/2023.findings-acl.36) |  | 0 | Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively... | Mohit Bansal, Prateek Yadav |  |
| 379 |  |  [Transferring General Multimodal Pretrained Models to Text Recognition](https://doi.org/10.18653/v1/2023.findings-acl.37) |  | 0 | This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or... | An Yang, Chang Zhou, Gao Liu, Junyang Lin, Peng Wang, Xuancheng Ren, Yichang Zhang |  |
| 380 |  |  [A Formal Perspective on Byte-Pair Encoding](https://doi.org/10.18653/v1/2023.findings-acl.38) |  | 0 | Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE... | Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan, Ryan Cotterell, Tim Vieira, Vilém Zouhar |  |
| 381 |  |  [Automatic Named Entity Obfuscation in Speech](https://doi.org/10.18653/v1/2023.findings-acl.39) |  | 0 | Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator. While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech. This work focuses on identifying,... | Judita Preiss |  |
| 382 |  |  [Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models](https://doi.org/10.18653/v1/2023.findings-acl.40) |  | 0 | Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models’ (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the... | Gunhee Kim, Soochan Lee |  |
| 383 |  |  [UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.41) |  | 0 | Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and... | Chen Chen, Deepu Rajan, Eng Siong Chng, Heqing Zou, Meng Shen, Yuchen Hu |  |
| 384 |  |  [Robustness-Aware Word Embedding Improves Certified Robustness to Adversarial Word Substitutions](https://doi.org/10.18653/v1/2023.findings-acl.42) |  | 0 | Natural Language Processing (NLP) models have gained great success on clean texts, but they are known to be vulnerable to adversarial examples typically crafted by synonym substitutions. In this paper, we target to solve this problem and find that word embedding is important to the certified... | Di He, Kun He, Yibin Wang, Yichen Yang |  |
| 385 |  |  [Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-acl.43) |  | 0 | In the context-dependent Text-to-SQL task, the generated SQL statements are refined iteratively based on the user input utterance from each interaction. The input text from each interaction can be viewed as component modifications to the previous SQL statements, which could be further extracted as... | Aiwei Liu, Fukun Ma, Lijie Wen, Shuang Li, Wei Liu, Xuming Hu, Yawen Yang |  |
| 386 |  |  [Towards Generative Event Factuality Prediction](https://doi.org/10.18653/v1/2023.findings-acl.44) |  | 0 | We present a novel end-to-end generative task and system for predicting event factuality holders, targets, and their associated factuality values. We perform the first experiments using all sources and targets of factuality statements from the FactBank corpus. We perform multi-task learning with... | Amittai Aviram, John Murzaku, Owen Rambow, Tyler Osborne |  |
| 387 |  |  [Can Language Models Be Specific? How?](https://doi.org/10.18653/v1/2023.findings-acl.45) |  | 0 | “He is a person”, “Paris is located on the earth”. Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark... | Jie Huang, Jinjun Xiong, Kevin ChenChuan Chang, WenMei Hwu |  |
| 388 |  |  [The Web Can Be Your Oyster for Improving Language Models](https://doi.org/10.18653/v1/2023.findings-acl.46) |  | 0 | Pretrained language models (PLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of PLMs for knowledge-intensive tasks, we... | JiRong Wen, JianYun Nie, Jingyuan Wang, Junyi Li, Tianyi Tang, Wayne Xin Zhao |  |
| 389 |  |  [Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples](https://doi.org/10.18653/v1/2023.findings-acl.47) |  | 0 | Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent performance in the target language. However, if no labeled data in the target language are available, they... | Hwichan Kim, Mamoru Komachi |  |
| 390 |  |  [Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning](https://doi.org/10.18653/v1/2023.findings-acl.48) |  | 0 | Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further... | Daniel PreotiucPietro, Genta Indra Winata, Karthik Radhakrishnan, Lingjue Xie, Mayank Kulkarni, Pengxiang Cheng, Shijie Wu, Xisen Jin |  |
| 391 |  |  [UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.49) |  | 0 | Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model’s reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in... | Haoxuan You, KaiWei Chang, Noel Codella, Rui Sun, ShihFu Chang, Zhecan Wang |  |
| 392 |  |  [Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors](https://doi.org/10.18653/v1/2023.findings-acl.50) |  | 0 | Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small... | Bernal Jimenez Gutierrez, Kai Zhang, Yu Su |  |
| 393 |  |  [TADA : Task Agnostic Dialect Adapters for English](https://doi.org/10.18653/v1/2023.findings-acl.51) |  | 0 | Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which... | Caleb Ziems, Diyi Yang, William Held |  |
| 394 |  |  [Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting](https://doi.org/10.18653/v1/2023.findings-acl.52) |  | 0 | Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or... | Guanting Dong, Hao Lei, Jiachi Liu, Jinzheng Zhao, Keqing He, Liwen Wang, Weiran Xu, Xuefeng Li |  |
| 395 |  |  [Re-appraising the Schema Linking for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.53) |  | 0 | Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking: aligning the entity references in the question with the schema columns or tables. This... | Matthew Purver, Xinyun Chen, Yujian Gan |  |
| 396 |  |  [Echoes from Alexandria: A Large Resource for Multilingual Book Summarization](https://doi.org/10.18653/v1/2023.findings-acl.54) |  | 0 | In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and... | Alessandro Scirè, Roberto Navigli, Simone Ciciliano, Simone Conia |  |
| 397 |  |  [When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario](https://doi.org/10.18653/v1/2023.findings-acl.55) |  | 0 | Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to... | Chengcheng Han, Jianing Wang, Liqing Cui, Ming Gao, Nuo Chen, Qiushi Sun, Renyu Zhu, Xiang Li |  |
| 398 |  |  [Align-then-Enhance: Multilingual Entailment Graph Enhancement with Soft Predicate Alignment](https://doi.org/10.18653/v1/2023.findings-acl.56) |  | 0 | Entailment graphs (EGs) with predicates as nodes and entailment relations as edges are typically incomplete, while EGs in different languages are often complementary to each other. In this paper, we propose a new task, multilingual entailment graph enhancement, which aims to utilize the entailment... | Dongyan Zhao, Mark Steedman, Tianyi Li, Yansong Feng, Yuting Wu, Yutong Hu |  |
| 399 |  |  [Few-shot Classification with Hypersphere Modeling of Prototypes](https://doi.org/10.18653/v1/2023.findings-acl.57) |  | 0 | Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However,... | Ganqu Cui, Haitao Zheng, Ning Ding, Pengjun Xie, Xiaobin Wang, Yulin Chen, Zhiyuan Liu |  |
| 400 |  |  [Structured Mean-Field Variational Inference for Higher-Order Span-Based Semantic Role](https://doi.org/10.18653/v1/2023.findings-acl.58) |  | 0 | In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling. To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head... | Kewei Tu, Songlin Yang, Wei Liu |  |
| 401 |  |  [AQE: Argument Quadruplet Extraction via a Quad-Tagging Augmented Generative Approach](https://doi.org/10.18653/v1/2023.findings-acl.59) |  | 0 | Argument mining involves multiple sub-tasks that automatically identify argumentative elements, such as claim detection, evidence extraction, stance classification, etc. However, each subtask alone is insufficient for a thorough understanding of the argumentative structure and reasoning process. To... | Jia Guo, Lidong Bing, Liying Cheng, Stanley Kok, Wenxuan Zhang, Xin Li |  |
| 402 |  |  [The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.60) |  | 0 | Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors,... | Arash Eshghi, Dimitris Dimakopoulos, Ioannis Konstas, Ioannis Papaioannou, Marco Antonio Sobrevilla Cabezudo, Sabrina Chiesurin, Verena Rieser |  |
| 403 |  |  [Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker](https://doi.org/10.18653/v1/2023.findings-acl.61) |  | 0 | Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker... | Jeongyeon Seo, Jong C. Park, Soyeong Jeong, Sukmin Cho |  |
| 404 |  |  [Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks](https://doi.org/10.18653/v1/2023.findings-acl.62) |  | 0 | Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks... | Cícero Nogueira dos Santos, Kanishka Misra, Siamak Shakeri |  |
| 405 |  |  [Multimedia Generative Script Learning for Task Planning](https://doi.org/10.18653/v1/2023.findings-acl.63) |  | 0 | Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which... | Girish Chowdhary, Heng Ji, Hou Pong Chan, Julia Hockenmaier, Lifu Huang, Manling Li, Qingyun Wang |  |
| 406 |  |  [Label Agnostic Pre-training for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.64) |  | 0 | Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect... | Christopher Clarke, Jason Mars, Krisztián Flautner, Lingjia Tang, Yiping Kang, Yuzhao Heng |  |
| 407 |  |  [Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.65) |  | 0 | It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Leo for controllable text generation, which needs no modification to the model architecture and... | Chujie Zheng, Minlie Huang, Pei Ke, Zheng Zhang |  |
| 408 |  |  [Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information](https://doi.org/10.18653/v1/2023.findings-acl.66) |  | 0 | Keyphrase extraction aims to extract a set of phrases with the central idea of the source document. In a structured document, there are certain locations (e.g., the title or the first sentence) where a keyphrase is most likely to appear. However, when extracting keyphrases from the document, most... | Huafeng Liu, Liping Jing, Mingyang Song, Yi Feng |  |
| 409 |  |  [Towards Reasoning in Large Language Models: A Survey](https://doi.org/10.18653/v1/2023.findings-acl.67) |  | 0 | Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that... | Jie Huang, Kevin ChenChuan Chang |  |
| 410 |  |  [Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications](https://doi.org/10.18653/v1/2023.findings-acl.68) |  | 0 | Although recent years have been marked by incredible advances in the whole development process of NLP systems, there are still blind spots in characterizing what is still hampering real-world adoption of models in knowledge-intensive settings. In this paper, we illustrate through a real-world... | Aurore Bochnakian, Chyrine Tahri, Patrick Haouat, Xavier Tannier |  |
| 411 |  |  [CLIPText: A New Paradigm for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.69) |  | 0 | While CLIP models are useful for zero-shot vision-and-language (VL) tasks or computer vision tasks, little attention has been paid to the application of CLIP for language tasks. Intuitively, CLIP model have a rich representation pre-trained with natural language supervision, in which we argue that... | Libo Qin, Qiguang Chen, Wanxiang Che, Weiyun Wang |  |
| 412 |  |  [Rethinking Dictionaries and Glyphs for Chinese Language Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.70) |  | 0 | We introduce CDBert, a new learning paradigm that enhances the semantics understanding ability of the Chinese PLMs with dictionary knowledge and structure of Chinese characters. We name the two core modules of CDBert as Shuowen and Jiezi, where Shuowen refers to the process of retrieving the most... | Dongyan Zhao, Jianghui Wang, Yuxuan Wang, Zilong Zheng |  |
| 413 |  |  [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.71) |  | 0 | We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single... | Hongjin Su, Jungo Kasai, Luke Zettlemoyer, Mari Ostendorf, Noah A. Smith, Tao Yu, Weijia Shi, Wentau Yih, Yizhong Wang, Yushi Hu |  |
| 414 |  |  [Towards Speech Dialogue Translation Mediating Speakers of Different Languages](https://doi.org/10.18653/v1/2023.findings-acl.72) |  | 0 | We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of... | Chenhui Chu, Sadao Kurohashi, Sheng Li, Shuichiro Shimizu |  |
| 415 |  |  [Adaptation Approaches for Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2023.findings-acl.73) |  | 0 | Semi-parametric Nearest Neighbor Language Models (kNN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work... | George Polovets, Monica Sunkara, Rishabh Bhardwaj |  |
| 416 |  |  [Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.74) |  | 0 | Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language... | Bartlomiej Jezierski, Georg Groh, Joshua Oehms, Miriam Anschütz, Thomas Wimmer |  |
| 417 |  |  [Client-Customized Adaptation for Parameter-Efficient Federated Learning](https://doi.org/10.18653/v1/2023.findings-acl.75) |  | 0 | Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be... | JunHyung Park, Junho Kim, SangKeun Lee, WingLam Mok, Yeachan Kim |  |
| 418 |  |  [FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery](https://doi.org/10.18653/v1/2023.findings-acl.76) |  | 0 | Understanding users’ intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans’ minds about purchasing items. As commonsense knowledge is usually ineffable and not... | Bing Yin, Changlong Yu, Jiaxin Bai, Tianyu Cao, Weiqi Wang, Xin Liu, Yangqiu Song, Yifan Gao, Zheng Li |  |
| 419 |  |  [I am PsyAM: Modeling Happiness with Cognitive Appraisal Dimensions](https://doi.org/10.18653/v1/2023.findings-acl.77) |  | 0 | This paper proposes and evaluates PsyAM (https://anonymous.4open.science/r/BERT-PsyAM-10B9), a framework that incorporates adaptor modules in a sequential multi-task learning setup to generate high-dimensional feature representations of hedonic well-being (momentary happiness) in terms of its... | Kokil Jaidka, Xuan Liu |  |
| 420 |  |  [Value type: the bridge to a better DST model](https://doi.org/10.18653/v1/2023.findings-acl.78) |  | 0 | Value type of the slots can provide lots of useful information for DST tasks. However, it has been ignored in most previous works. In this paper, we propose a new framework for DST task based on these value types. Firstly, we extract the type of token from each turn. Specifically, we divide the... | Chen Zeng, Mingyang Sun, QiXiang Gao, Weiran Xu, Yutao Mou |  |
| 421 |  |  [Hypothetical Training for Robust Machine Reading Comprehension of Tabular Context](https://doi.org/10.18653/v1/2023.findings-acl.79) |  | 0 | Machine Reading Comprehension (MRC) models easily learn spurious correlations from complex contexts such as tabular data. Counterfactual training—using the factual and counterfactual data by augmentation—has become a promising solution. However, it is costly to construct faithful counterfactual... | Fuli Feng, Hanwang Zhang, Moxin Li, Qifan Wang, TatSeng Chua, Wenjie Wang |  |
| 422 |  |  [BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews](https://doi.org/10.18653/v1/2023.findings-acl.80) |  | 0 | The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due... | Hasan Mahmud, Md. Kamrul Hasan, Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan |  |
| 423 |  |  [Risks and NLP Design: A Case Study on Procedural Document QA](https://doi.org/10.18653/v1/2023.findings-acl.81) |  | 0 | As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications. We argue that clearer assessments... | Alice Gao, Nikita Haduong, Noah A. Smith |  |
| 424 |  |  [The Diminishing Returns of Masked Language Models to Science](https://doi.org/10.18653/v1/2023.findings-acl.82) |  | 0 | Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this... | Aswathy Ajith, Eamon Duede, Ian T. Foster, J. Gregory Pauloski, Kyle Chard, Zhi Hong |  |
| 425 |  |  [Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies](https://doi.org/10.18653/v1/2023.findings-acl.83) |  | 0 | A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year to the next. However, in these settings the content of the manuscript is a confounding variable—each year has a... | Andrew McCallum, Daniel A. McFarland, Daniel Scott Smith, Katherine Keith, Neha Nayak Kennard, Raymond Zhang |  |
| 426 |  |  [Learning to Generalize for Cross-domain QA](https://doi.org/10.18653/v1/2023.findings-acl.84) |  | 0 | There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we... | Linyi Yang, Ruihai Dong, Yingjie Niu, Yue Zhang |  |
| 427 |  |  [Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs](https://doi.org/10.18653/v1/2023.findings-acl.85) |  | 0 | Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart... | Christopher Thomas, Heng Ji, Long Chen, Mingyang Zhou, ShihFu Chang, Yi Ren Fung |  |
| 428 |  |  [Importance of Synthesizing High-quality Data for Text-to-SQL Parsing](https://doi.org/10.18653/v1/2023.findings-acl.86) |  | 0 | There has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented... | Alexander Hanbo Li, Anuj Chauhan, Bing Xiang, ChungWei Hang, Henghui Zhu, Jiang Guo, Jiarong Jiang, Joseph Lilien, Jun Wang, Lin Pan, Mingwen Dong, Patrick Ng, Sheng Zhang, Vittorio Castelli, Wuwei Lan, Yiqun Hu, Yiyun Zhao, Zhiguo Wang |  |
| 429 |  |  [Exploring Schema Generalizability of Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.87) |  | 0 | Exploring the generalizability of a text-to-SQL parser is essential for a system to automatically adapt the real-world databases. Previous investigation works mostly focus on lexical diversity, including the influence of the synonym and perturbations in both natural language questions and... | Hanchong Zhang, Hongshen Xu, Jieyu Li, Kai Yu, Lu Chen, Ruisheng Cao, Su Zhu, Zhi Chen |  |
| 430 |  |  [Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer](https://doi.org/10.18653/v1/2023.findings-acl.88) |  | 0 | Cross-lingual natural language inference is a fundamental problem in cross-lingual language understanding. Many recent works have used prompt learning to address the lack of annotated parallel corpora in XNLI.However, these methods adopt discrete prompting by simply translating the templates to the... | Aiwei Liu, Fukun Ma, Lijie Wen, Philip S. Yu, Shuang Li, Xuming Hu, Yawen Yang |  |
| 431 |  |  [A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.89) |  | 0 | Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity... | Jie Zhou, Jin Ma, Limao Xiong, Qi Zhang, Qunxi Zhu, Tao Gui, Xiao Wang, Xuanjing Huang, Ying Shan, Yuanbin Wu |  |
| 432 |  |  [Towards Zero-Shot Persona Dialogue Generation with In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.90) |  | 0 | Much work has been done to improve persona consistency by finetuning a pretrained dialogue model on high-quality human-annoated persona datasets. However, these methods still face the challenges of high cost and poor scalability. To this end, we propose a simple-yet-effective approach to... | Haifeng Wang, Hua Wu, Wenquan Wu, Xinchao Xu, Zeyang Lei, ZhengYu Niu |  |
| 433 |  |  [Grammar-based Decoding for Improved Compositional Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.91) |  | 0 | Sequence-to-sequence (seq2seq) models have achieved great success in semantic parsing tasks, but they tend to struggle on out-of-distribution (OOD) data. Despite recent progress, robust semantic parsing on large-scale tasks with combined challenges from both compositional generalization and natural... | Jing Zheng, JyhHerng Chow, Peng Xu, Zhongnan Shen |  |
| 434 |  |  [Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.92) |  | 0 | User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those... | Chenyang Lyu, Jennifer Foster, Linyi Yang, Yue Zhang, Yvette Graham |  |
| 435 |  |  [Efficient Out-of-Domain Detection for Sequence to Sequence Models](https://doi.org/10.18653/v1/2023.findings-acl.93) |  | 0 | Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text... | Akim Tsvigun, Alexander Panchenko, Artem Shelmanov, Artem Vazhentsev, Daniil Vasilev, Maxim Panov, Roman Vashurin, Sergey Petrakov |  |
| 436 |  |  [Emotion Cause Extraction on Social Media without Human Annotation](https://doi.org/10.18653/v1/2023.findings-acl.94) |  | 0 | In social media, there is a vast amount of information pertaining to people’s emotions and the corresponding causes. The emotion cause extraction (ECE) from social media data is an important research area that has not been thoroughly explored due to the lack of fine-grained annotations. Early... | Debin Xiao, Jianfei Yu, Rui Xia |  |
| 437 |  |  [Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers](https://doi.org/10.18653/v1/2023.findings-acl.95) |  | 0 | For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular,... | Dongbin Na, Eunbin Park, Jaeyoung Kim, Kyuheon Jung, Sion Jang, Sungchul Choi |  |
| 438 |  |  [Adversarial Multi-task Learning for End-to-end Metaphor Detection](https://doi.org/10.18653/v1/2023.findings-acl.96) |  | 0 | Metaphor detection (MD) suffers from limited training data. In this paper, we started with a linguistic rule called Metaphor Identification Procedure and then proposed a novel multi-task learning framework to transfer knowledge in basic sense discrimination (BSD) to MD. BSD is constructed from word... | Shenglong Zhang, Ying Liu |  |
| 439 |  |  [SERENGETI: Massively Multilingual Language Models for Africa](https://doi.org/10.18653/v1/2023.findings-acl.97) |  | 0 | Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this... | AbdelRahim A. Elmadany, Alcides Alcoba Inciarte, Ife Adebara, Muhammad AbdulMageed |  |
| 440 |  |  [Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring](https://doi.org/10.18653/v1/2023.findings-acl.98) |  | 0 | Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations;... | Gary Geunbae Lee, Heejin Do, Yunsu Kim |  |
| 441 |  |  [AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation](https://doi.org/10.18653/v1/2023.findings-acl.99) |  | 0 | Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task... | Chujie Zheng, Jiaxin Wen, Minlie Huang, Sahand Sabour, Zheng Zhang |  |
| 442 |  |  [2\*n is better than n²: Decomposing Event Coreference Resolution into Two Tractable Problems](https://doi.org/10.18653/v1/2023.findings-acl.100) |  | 0 | Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in... | Abhijnan Nath, James H. Martin, Nikhil Krishnaswamy, Shafiuddin Rehan Ahmed |  |
| 443 |  |  [SCCS: Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment](https://doi.org/10.18653/v1/2023.findings-acl.101) |  | 0 | Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However,... | Bo Li, Ding Zhao, Franck Dernoncourt, Hailin Jin, Jiacheng Zhu, Jielin Qiu, Mengdi Xu, Trung Bui, Zhaowen Wang |  |
| 444 |  |  [General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.102) |  | 0 | Training keyphrase generation (KPG) models require a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG... | Daqing He, Rui Meng, Tong Wang, Xingdi Yuan, Yingbo Zhou |  |
| 445 |  |  [E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.103) |  | 0 | Most named entity recognition (NER) systems focus on improving model performance, ignoring the need to quantify model uncertainty, which is critical to the reliability of NER systems in open environments. Evidential deep learning (EDL) has recently been proposed as a promising solution to... | Bingzhe Wu, Haotian Wang, Lemao Liu, Mengting Hu, Minlie Huang, Shiwan Zhao, Zhe Liu, Zhen Zhang, Zhirui Zhang |  |
| 446 |  |  [LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting](https://doi.org/10.18653/v1/2023.findings-acl.104) |  | 0 | Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works... | Bruno Martins, Desmond Elliott, Rita Ramos |  |
| 447 |  |  [Boosting Text Augmentation via Hybrid Instance Filtering Framework](https://doi.org/10.18653/v1/2023.findings-acl.105) |  | 0 | Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing... | Heng Yang, Ke Li |  |
| 448 |  |  [Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction](https://doi.org/10.18653/v1/2023.findings-acl.106) |  | 0 | Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score... | Anh Tuan Luu, CongDuy Nguyen, Lidong Bing, Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Zhen Hai |  |
| 449 |  |  [Extract and Attend: Improving Entity Translation in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.107) |  | 0 | While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we... | Junliang Guo, Rui Wang, Shufang Xie, Tao Qin, TieYan Liu, Xu Tan, Yichong Leng, Zixin Zeng |  |
| 450 |  |  [Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.108) |  | 0 | Compositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We... | Hao Zheng, Mirella Lapata |  |
| 451 |  |  [Cross-lingual AMR Aligner: Paying Attention to Cross-Attention](https://doi.org/10.18653/v1/2023.findings-acl.109) |  | 0 | This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment... | Abelardo Carlos Martinez Lorenzo, PereLluís Huguet Cabot, Roberto Navigli |  |
| 452 |  |  [Zero-Shot Text Classification via Self-Supervised Tuning](https://doi.org/10.18653/v1/2023.findings-acl.110) |  | 0 | Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised... | Anh Tuan Luu, Chaoqun Liu, ChipHong Chang, Guizhen Chen, Lidong Bing, Wenxuan Zhang, Xiaobao Wu |  |
| 453 |  |  [Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.111) |  | 0 | Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP models’ performance on many important NLU and NLG tasks. Existing... | Aaron Halfaker, Ahmed Hassan Awadallah, Borui Wang, Budhaditya Deb, Daniel McDuff, Dragomir Radev, Jianfeng Gao, Liqun Shao, Qiuyuan Huang |  |
| 454 |  |  [Large Language Models with Controllable Working Memory](https://doi.org/10.18653/v1/2023.findings-acl.112) |  | 0 | Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining. While many downstream applications provide the model with an informational context to aid its underlying... | Andreas Veit, Ankit Singh Rawat, Daliang Li, Felix X. Yu, Manzil Zaheer, Michal Lukasik, Sanjiv Kumar, Xin Wang |  |
| 455 |  |  [A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution](https://doi.org/10.18653/v1/2023.findings-acl.113) |  | 0 | State-of-the-art natural language processing models have been shown to achieve remarkable performance in ‘closed-world’ settings where all the labels in the evaluation set are known at training time. However, in real-world settings, ‘novel’ instances that do not belong to any known class are often... | Bing Liu, Chitta Baral, Eric Robertson, Himanshu Gupta, Neeraj Varshney |  |
| 456 |  |  [CDA: A Contrastive Data Augmentation Method for Alzheimer's Disease Detection](https://doi.org/10.18653/v1/2023.findings-acl.114) |  | 0 | Alzheimer’s Disease (AD) is a neurodegenerative disorder that significantly impacts a patient’s ability to communicate and organize language. Traditional methods for detecting AD, such as physical screening or neurological testing, can be challenging and time-consuming. Recent research has explored... | Fangyuan Wei, Hongdong Li, Jianxin Wang, Jin Liu, Junwen Duan, Tianming Liu |  |
| 457 |  |  [Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions](https://doi.org/10.18653/v1/2023.findings-acl.115) |  | 0 | Mining public opinions about vaccines from social media has been increasingly relevant to analyse trends in public debates and to provide quick insights to policy-makers. However, the application of existing models has been hindered by the wide variety of users’ attitudes and the new aspects... | Gabriele Pergola, Lixing Zhu, Runcong Zhao, Yulan He |  |
| 458 |  |  [Temporal Relation Classification using Boolean Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.116) |  | 0 | Classifying temporal relations between a pair of events is crucial to natural language understanding and a well-known natural language processing task. Given a document and two event mentions, the task is aimed at finding which one started first. We propose an efficient approach for temporal... | Kfir Bar, Omer Cohen |  |
| 459 |  |  [Are Synonym Substitution Attacks Really Synonym Substitution Attacks?](https://doi.org/10.18653/v1/2023.findings-acl.117) |  | 0 | In this paper, we explore the following question: Are synonym substitution attacks really synonym substitution attacks (SSAs)?We approach this question by examining how SSAs replace words in the original sentence and show that there are still unresolved obstacles that make current SSAs generate... | David ChengHan Chiang, Hungyi Lee |  |
| 460 |  |  [DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection](https://doi.org/10.18653/v1/2023.findings-acl.118) |  | 0 | Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source... | Deepak Kumar, Kaushal Maurya, Maunendra Sankar Desarkar, Venkatesh Elangovan |  |
| 461 |  |  [Similarity-Based Content Scoring - A more Classroom-Suitable Alternative to Instance-Based Scoring?](https://doi.org/10.18653/v1/2023.findings-acl.119) |  | 0 | Automatically scoring student answers is an important task that is usually solved using instance-based supervised learning. Recently, similarity-based scoring has been proposed as an alternative approach yielding similar perfor- mance. It has hypothetical advantages such as a lower need for... | Andrea Horbach, Marie Bexte, Torsten Zesch |  |
| 462 |  |  [Pragmatic Inference with a CLIP Listener for Contrastive Captioning](https://doi.org/10.18653/v1/2023.findings-acl.120) |  | 0 | We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game... | Benno Krojer, Daniel Fried, Jiefu Ou |  |
| 463 |  |  [A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus](https://doi.org/10.18653/v1/2023.findings-acl.121) |  | 0 | We present a pipeline for a statistical stylometric exploration of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a... | Axel Bühler, Barak Sober, Eli Piasetzky, Gideon Yoffe, Israel Finkelstein, Nachum Dershowitz, Thomas Römer |  |
| 464 |  |  [A Language-First Approach for Procedure Planning](https://doi.org/10.18653/v1/2023.findings-acl.122) |  | 0 | Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. Encouraged by the recent success of language models (LMs) for zero-shot... | Heng Ji, Jiateng Liu, Manling Li, Sha Li, Zhenhailong Wang |  |
| 465 |  |  [An Empirical Analysis of Leveraging Knowledge for Low-Resource Task-Oriented Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.123) |  | 0 | Task-oriented semantic parsing has drawn a lot of interest from the NLP community, and especially the voice assistant industry as it enables representing the meaning of user requests with arbitrarily nested semantics, including multiple intents and compound entities. SOTA models are large seq2seq... | Aoxiao Zhong, He Xie, Jianhua Lu, Mayank Kulkarni, Mukund Sridhar, Nicolas Guenon des Mesnards, Sahar Movaghati |  |
| 466 |  |  [TempLM: Distilling Language Models into Template-Based Generators](https://doi.org/10.18653/v1/2023.findings-acl.124) |  | 0 | While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content. In contrast, classic template-based systems provide strong guarantees of faithfulness at the cost of fluency. We propose TempLM, which achieves... | Ende Shen, Mina Lee, Tatsunori Hashimoto, Tianyi Zhang, Xiang Lisa Li |  |
| 467 |  |  [Incorporating Graph Information in Transformer-based AMR Parsing](https://doi.org/10.18653/v1/2023.findings-acl.125) |  | 0 | Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version... | Abelardo Carlos Martinez Lorenzo, Pavlo Vasylenko, PereLluís Huguet Cabot, Roberto Navigli |  |
| 468 |  |  [Rethinking the Word-level Quality Estimation for Machine Translation from Human Judgement](https://doi.org/10.18653/v1/2023.findings-acl.126) |  | 0 | Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing... | Fandong Meng, Jie Zhou, Yuanmeng Yan, Zhen Yang |  |
| 469 |  |  [PV2TEA: Patching Visual Modality to Textual-Established Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.127) |  | 0 | Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to... | Carl Yang, Chenwei Zhang, Hejie Cui, Jingbo Shang, Nasser Zalmout, Rongmei Lin, Xian Li |  |
| 470 |  |  [Structural Contrastive Pretraining for Cross-Lingual Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.128) |  | 0 | To present, multilingual language models trained using various pre-training tasks like mask language modeling (MLM) have yielded encouraging results on a wide range of downstream tasks. Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current... | Daxin Jiang, Jia Li, Jian Pei, Jianhui Chang, Linjun Shou, Ming Gong, Nuo Chen, Tengtao Song |  |
| 471 |  |  [Reducing Sensitivity on Speaker Names for Text Generation from Dialogues](https://doi.org/10.18653/v1/2023.findings-acl.129) |  | 0 | Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result... | Haifeng Tang, Kenny Q. Zhu, Qi Jia |  |
| 472 |  |  [Topic and Style-aware Transformer for Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.130) |  | 0 | Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the... | Nitesh Sekhar, Prateek Singhal, Shuwen Qiu |  |
| 473 |  |  [Exploiting Abstract Meaning Representation for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.131) |  | 0 | The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity... | Cunxiang Wang, Qipeng Guo, Xiangkun Hu, Xuefeng Bai, Yue Zhang, Zheng Zhang, Zhikun Xu |  |
| 474 |  |  [Nonparametric Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.132) |  | 0 | Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a... | Hannaneh Hajishirzi, Luke Zettlemoyer, Mike Lewis, Sewon Min, Weijia Shi, Wentau Yih, Xilun Chen |  |
| 475 |  |  [Pay More Attention to Relation Exploration for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.133) |  | 0 | Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations,... | Bin Wang, Daniel Hershcovich, Huiwen Liu, Min Chen, Shuai Chen, Wen Dai, Xianzhi Li, Yong Cao |  |
| 476 |  |  [Speaking Multiple Languages Affects the Moral Bias of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.134) |  | 0 | Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many... | Alexander Fraser, Björn Deiseroth, Constantin A. Rothkopf, Jindrich Libovický, Katharina Hämmerl, Kristian Kersting, Patrick Schramowski |  |
| 477 |  |  [Retrieving Relevant Context to Align Representations for Cross-lingual Event Detection](https://doi.org/10.18653/v1/2023.findings-acl.135) |  | 0 | We study the problem of cross-lingual transfer learning for event detection (ED) where models trained on a source language are expected to perform well on data for a new target language. Among a few recent works for this problem, the main approaches involve representation matching (e.g.,... | Chien Nguyen, Linh Ngo Van, Thien Huu Nguyen |  |
| 478 |  |  [NormNet: Normalize Noun Phrases for More Robust NLP](https://doi.org/10.18653/v1/2023.findings-acl.136) |  | 0 | A critical limitation of deep NLP models is their over-fitting over spurious features. Previous work has proposed several approaches to debunk such features and reduce their impact on the learned models. In this work, a normalization strategy is proposed to eliminate the false features caused by... | Mingming Sun, Minlong Peng |  |
| 479 |  |  [Cross Encoding as Augmentation: Towards Effective Educational Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.137) |  | 0 | Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag... | Christian Wallraven, Hyeongdon Moon, Hyojun Go, Hyun Seung Lee, Myeongho Jeong, Seungtaek Choi, Shinhyeok Oh, Yunsung Lee |  |
| 480 |  |  [Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.138) |  | 0 | State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial... | Gaurav Verma, Srijan Kumar, Subhabrata Mukherjee, Venkata Prabhakara Sarath Nookala |  |
| 481 |  |  [This prompt is measuring \textlessmask\textgreater: evaluating bias evaluation in language models](https://doi.org/10.18653/v1/2023.findings-acl.139) |  | 0 | Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. We analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a... | Eddie Ungless, Esma Balkir, Seraphina GoldfarbTarrant, Su Lin Blodgett |  |
| 482 |  |  [Towards Open Environment Intent Prediction](https://doi.org/10.18653/v1/2023.findings-acl.140) |  | 0 | Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will... | Jiawei Hong, Xipeng Qiu, Yunhua Zhou |  |
| 483 |  |  [Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.141) |  | 0 | Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in... | Lifu Huang, Minqian Liu |  |
| 484 |  |  [C-XNLI: Croatian Extension of XNLI Dataset](https://doi.org/10.18653/v1/2023.findings-acl.142) |  | 0 | Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets... | Andrej Jertec, Branimir Dropuljic, Leo Obadic, Marko Rajnovic |  |
| 485 |  |  [AVATAR: A Parallel Corpus for Java-Python Program Translation](https://doi.org/10.18653/v1/2023.findings-acl.143) |  | 0 | Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software... | KaiWei Chang, Md Golam Rahman Tushar, Saikat Chakraborty, Wasi Uddin Ahmad |  |
| 486 |  |  [On Dataset Transferability in Active Learning for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.144) |  | 0 | Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one... | Fran Jelenic, Jan Snajder, Josip Jukic, Nina Drobac |  |
| 487 |  |  [Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions](https://doi.org/10.18653/v1/2023.findings-acl.145) |  | 0 | We present an annotation approach for capturing structured components and arguments inlegal case solutions of German students. Based on the appraisal style, which dictates the structured way of persuasive writing in German law, we propose an annotation scheme with annotation guidelines that... | Florian Weber, Matthias Söllner, Seyed Parsa Neshaei, Thiemo Wambsganss |  |
| 488 |  |  [Characterizing the Impacts of Instances on Robustness](https://doi.org/10.18653/v1/2023.findings-acl.146) |  | 0 | Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with... | Jin Ma, Qi Zhang, Qin Liu, Rui Zheng, Tao Gui, Weifeng Ge, Wenbin Lai, Xuanjing Huang, Ying Shan, Zhiheng Xi |  |
| 489 |  |  [Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge](https://doi.org/10.18653/v1/2023.findings-acl.147) |  | 0 | The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However,... | Alexander Hanbo Li, Bing Xiang, Dan Roth, Gukyeong Kwon, Henghui Zhu, Patrick Ng, Pramuditha Perera, Sheng Zhang, Vittorio Castelli, William Yang Wang, Xingyu Fu, Yuhao Zhang, Zhiguo Wang |  |
| 490 |  |  [Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.148) |  | 0 | Syllogistic reasoning, a typical form of deductive reasoning, is a critical capability widely required in natural language understanding tasks, such as text entailment and question answering. To better facilitate research on syllogistic reasoning, we develop a benchmark called SylloBase that... | Lei Li, Meng Han, Ruofei Lai, Xiaoguang Li, Xinyu Zhang, Yongkang Wu, Yuanhang Ren, Yutao Zhu, Zhao Cao, Zhicheng Dou |  |
| 491 |  |  [Categorial grammar induction from raw data](https://doi.org/10.18653/v1/2023.findings-acl.149) |  | 0 | Grammar induction, the task of learning a set of grammatical rules from raw or minimally labeled text data, can provide clues about what kinds of syntactic structures are learnable without prior knowledge. Recent work (e.g., Kim et al., 2019; Zhu et al., 2020; Jin et al., 2021a) has achieved... | Christian Clark, William Schuler |  |
| 492 |  |  [Attribute Controlled Dialogue Prompting](https://doi.org/10.18653/v1/2023.findings-acl.150) |  | 0 | Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary... | Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart, Runcheng Liu |  |
| 493 |  |  [Open-World Factually Consistent Question Generation](https://doi.org/10.18653/v1/2023.findings-acl.151) |  | 0 | Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem... | Apoorv Saxena, Himanshu Maheshwari, Niyati Chhaya, Sumit Shekhar |  |
| 494 |  |  [Contrastive Learning of Sociopragmatic Meaning in Social Media](https://doi.org/10.18653/v1/2023.findings-acl.152) |  | 0 | Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations... | Chiyu Zhang, Ganesh Jawahar, Muhammad AbdulMageed |  |
| 495 |  |  [Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.153) |  | 0 | This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative... | Baoyu Li, Dachun Sun, Hanghang Tong, Jinning Li, Ruijie Wang, Shengzhong Liu, Tarek F. Abdelzaher, Yichen Lu, Yuchen Yan |  |
| 496 |  |  [ACROSS: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.findings-acl.154) |  | 0 | This research addresses the challenges of Cross-Lingual Summarization (CLS) in low-resource scenarios and over imbalanced multilingual data. Existing CLS studies mostly resort to pipeline frameworks or multi-task methods in bilingual settings. However, they ignore the data imbalance in multilingual... | Adam Jatowt, Jun Wang, Liang Li, Peiyao Li, Zhengkun Zhang, Zhenglu Yang |  |
| 497 |  |  [RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.155) |  | 0 | Open-Domain Question Answering (ODQA) systems necessitate a reader model capable of generating answers by simultaneously referring to multiple passages. Although representative models like Fusion-in-Decoder (FiD) have been proposed to address this challenge, these systems can inadvertently rely on... | Cunxiang Wang, Haofei Yu, Yue Zhang |  |
| 498 |  |  [Unsupervised Keyphrase Extraction by Learning Neural Keyphrase Set Function](https://doi.org/10.18653/v1/2023.findings-acl.156) |  | 0 | We create a paradigm shift concerning building unsupervised keyphrase extraction systems in this paper. Instead of modeling the relevance between an individual candidate phrase and the document as in the commonly used framework, we formulate the unsupervised keyphrase extraction task as a... | Haiyun Jiang, Lemao Liu, Liping Jing, Mingyang Song, Shuming Shi |  |
| 499 |  |  [Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias](https://doi.org/10.18653/v1/2023.findings-acl.157) |  | 0 | Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying... | Deli Chen, Fandong Meng, Hao Zhou, Jie Zhou, Xu Sun, Zhiyuan Zhang |  |
| 500 |  |  [Retrieving Multimodal Prompts for Generative Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.158) |  | 0 | Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label... | Junjie Hu, Timothy Ossowski |  |
| 501 |  |  [InfoSync: Information Synchronization across Multilingual Semi-structured Tables](https://doi.org/10.18653/v1/2023.findings-acl.159) |  | 0 | Information Synchronization of semi-structured data across languages is challenging. For example, Wikipedia tables in one language need to be synchronized with others. To address this problem, we introduce a new dataset InfoSync and a two-step method for tabular synchronization. InfoSync contains... | Chelsi Jain, Shuo Zhang, Siddharth Khincha, Tushar Kataria, Vivek Gupta |  |
| 502 |  |  [T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation](https://doi.org/10.18653/v1/2023.findings-acl.160) |  | 0 | \*Warning: This paper contains several contents that may be toxic, harmful, or offensive.\*In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid... | Jialu Wang, Xin Eric Wang, Xinyue Liu, Yang Liu, Zonglin Di |  |
| 503 |  |  [An Investigation of Evaluation Methods in Automatic Medical Note Generation](https://doi.org/10.18653/v1/2023.findings-acl.161) |  | 0 | Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations... | Asma Ben Abacha, George Michalopoulos, Thomas Lin, Wenwai Yim |  |
| 504 |  |  [Rethinking Translation Memory Augmented Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.162) |  | 0 | This paper rethinks translation memory augmented neural machine translation (TM-augmented NMT) from two perspectives, i.e., a probabilistic view of retrieval and the variance-bias decomposition principle. The finding demonstrates that TM-augmented NMT is good at the ability of fitting data (i.e.,... | Guoping Huang, Hongkun Hao, Lemao Liu, Rui Wang, Shuming Shi, Zhirui Zhang |  |
| 505 |  |  [Controlling Styles in Neural Machine Translation with Activation Prompt](https://doi.org/10.18653/v1/2023.findings-acl.163) |  | 0 | Controlling styles in neural machine translation (NMT) has attracted wide attention, as it is crucial for enhancing user experience. Earlier studies on this topic typically concentrate on regulating the level of formality and achieve some progress in this area. However, they still encounter two... | Mingxuan Wang, Shanbo Cheng, Weiguo Zheng, Yifan Wang, Zewei Sun |  |
| 506 |  |  [Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.164) |  | 0 | Few-shot named entity recognition (NER), identifying named entities with a small number of labeled data, has attracted much attention. Frequently, entities are nested within each other. However, most of the existing work on few-shot NER addresses flat entities instead of nested entities. To tackle... | Deyu Zhou, Linhai Zhang, Rong Zhou, Tiandeng Wu, Yuanyuan Xu, Zeng Yang |  |
| 507 |  |  [Together We Make Sense-Learning Meta-Sense Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.165) |  | 0 | Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses. For this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning... | Danushka Bollegala, Haochen Luo, Yi Zhou |  |
| 508 |  |  [Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels](https://doi.org/10.18653/v1/2023.findings-acl.166) |  | 0 | Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models.... | Bang Yang, Bing Yin, Chenyu You, Fenglin Liu, Qingyu Yin, Yuexian Zou, Zheng Li |  |
| 509 |  |  [Large Language Models are Built-in Autoregressive Search Engines](https://doi.org/10.18653/v1/2023.findings-acl.167) |  | 0 | Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace... | Meng Jiang, Noah Ziems, Wenhao Yu, Zhihan Zhang |  |
| 510 |  |  [Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.168) |  | 0 | Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems focus on better access and use of visual information and tend to validate their methods on image-related datasets. However, these studies... | Liwei Wu, Luyang Huang, Mingxuan Wang, Shanbo Cheng, Yaoming Zhu, Zewei Sun |  |
| 511 |  |  [From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?](https://doi.org/10.18653/v1/2023.findings-acl.169) |  | 0 | Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task... | Albert Coil, Vered Shwartz |  |
| 512 |  |  [Measuring Intersectional Biases in Historical Documents](https://doi.org/10.18653/v1/2023.findings-acl.170) |  | 0 | Data-driven analyses of biases in historical texts can help illuminate the origin and development of biases prevailing in modern society. However, digitised historical documents pose a challenge for NLP practitioners as these corpora suffer from errors introduced by optical character recognition... | Isabelle Augenstein, Karolina Stanczak, Nadav Borenstein, Natacha Klein Käfer, Natalia da Silva Perez, Thea Rolskov |  |
| 513 |  |  [Incomplete Utterance Rewriting by A Two-Phase Locate-and-Fill Regime](https://doi.org/10.18653/v1/2023.findings-acl.171) |  | 0 | Rewriting incomplete and ambiguous utterances can improve dialogue models’ understanding of the context and help them generate better results. However, the existing end-to-end models will have the problem of too large search space, resulting in poor quality of rewriting results. We propose a... | Haifeng Tang, Jiawei Li, Kenny Q. Zhu, Ruolan Yang, Zitong Li |  |
| 514 |  |  [Exploring Variation of Results from Different Experimental Conditions](https://doi.org/10.18653/v1/2023.findings-acl.172) |  | 0 | It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP... | Anya Belz, Maja Popovic, Mohammad Arvan, Natalie Parde |  |
| 515 |  |  [Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2023.findings-acl.173) |  | 0 | Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge... | Elena Cabrio, Nicolás Benjamín Ocampo, Serena Villata |  |
| 516 |  |  [X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents](https://doi.org/10.18653/v1/2023.findings-acl.174) |  | 0 | Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. To reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by... | Aditya Yadavalli, Anmol Goel, Chaobin You, Deyi Xiong, Gaël de Chalendar, Jiwon Seo, Kalika Bali, Manish Shrivastava, Mehrad Moradshahi, Michael Sun, Monica S. Lam, Monojit Choudhury, Nasredine Semmar, Ponnurangam Kumaraguru, Prashant Kodali, Sina J. Semnani, Sungkyun Kim, Tianhao Shen, Vivek Seshadri |  |
| 517 |  |  [Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation](https://doi.org/10.18653/v1/2023.findings-acl.175) |  | 0 | Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this... | Francois Meyer, Jan Buys |  |
| 518 |  |  [Measuring and Mitigating Local Instability in Deep Neural Networks](https://doi.org/10.18653/v1/2023.findings-acl.176) |  | 0 | Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs... | Anoop Kumar, Aram Galstyan, Arghya Datta, Greg Ver Steeg, He Xie, Jingcheng Xu, Subhrangshu Nandi |  |
| 519 |  |  [What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.177) |  | 0 | kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the... | Jiajun Chen, Shujian Huang, Wenhao Zhu, Xin Zheng, Yunzhe Lv |  |
| 520 |  |  [Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-acl.178) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is... | Fei Zhao, Huiyun Yang, Jiaze Chen, Ting Xu, Xinyu Dai, Zhen Wu |  |
| 521 |  |  [Grounding the Lexical Substitution Task in Entailment](https://doi.org/10.18653/v1/2023.findings-acl.179) |  | 0 | Existing definitions of lexical substitutes are often vague or inconsistent with the gold annotations. We propose a new definition which is grounded in the relation of entailment; namely, that the sentence that results from the substitution should be in the relation of mutual entailment with the... | Grzegorz Kondrak, Talgat Omarov |  |
| 522 |  |  [Operator Selection and Ordering in a Pipeline Approach to Efficiency Optimizations for Transformers](https://doi.org/10.18653/v1/2023.findings-acl.180) |  | 0 | There exists a wide variety of efficiency methods for natural language processing (NLP) tasks, such as pruning, distillation, dynamic inference, quantization, etc. From a different perspective, we can consider an efficiency method as an operator applied on a model. Naturally, we may construct a... | Ji Xin, Jimmy Lin, Raphael Tang, Yaoliang Yu, Zhiying Jiang |  |
| 523 |  |  [AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.181) |  | 0 | Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS... | Abbas Ghaddar, Asaad Alghamdi, Baoxing Huai, Mehdi Rezagholizadeh, Peilun Cheng, Qingrong Xia, Wei Jiang, Xinyu Duan, Yi Zheng, Yimeng Wu, Zhefeng Wang, Zhenhai Wang |  |
| 524 |  |  [Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction](https://doi.org/10.18653/v1/2023.findings-acl.182) |  | 0 | Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of... | Arushi Raghuvanshi, Julia White, Yada Pruksachatkun |  |
| 525 |  |  [Quantifying Train-Evaluation Overlap with Nearest Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.183) |  | 0 | Characterizing benchmark datasets is crucial to interpreting model performance. In this work, we study train-evaluation overlap as a measure of an individual dataset’s adequacy to evaluate model generalization over a wide range of datasets. We quantify the overlap with a simple novel metric based... | Eunsol Choi, Gauri Kambhatla, Thuy Nguyen |  |
| 526 |  |  [Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels](https://doi.org/10.18653/v1/2023.findings-acl.184) |  | 0 | Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling... | Aviv Weinstein, Yoav Goldberg |  |
| 527 |  |  [The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges](https://doi.org/10.18653/v1/2023.findings-acl.185) |  | 0 | Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented... | Alham Fikri Aji, Genta Indra Winata, Thamar Solorio, Zheng Xin Yong |  |
| 528 |  |  [Learning to Predict Persona Information for Dialogue Personalization without Explicit Persona Description](https://doi.org/10.18653/v1/2023.findings-acl.186) |  | 0 | Personalizing dialogue agents is important for dialogue systems to generate more specific,consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we... | Chenle Li, Qifei Li, Wangchunshu Zhou |  |
| 529 |  |  [Automated Refugee Case Analysis: A NLP Pipeline for Supporting Legal Practitioners](https://doi.org/10.18653/v1/2023.findings-acl.187) |  | 0 | In this paper, we introduce an end-to-end pipeline for retrieving, processing, and extracting targeted information from legal cases. We investigate an under-studied legal domain with a case study on refugee law Canada. Searching case law for past similar cases is a key part of legal work for both... | Claire Barale, Michael Rovatsos, Nehal Bhuta |  |
| 530 |  |  [Recurrent Attention Networks for Long-text Modeling](https://doi.org/10.18653/v1/2023.findings-acl.188) |  | 0 | Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention... | Fu Lee Wang, Haoran Xie, Qing Li, Xianming Li, Xiaotian Luo, Xing Lee, Yingbin Zhao, Zongxi Li |  |
| 531 |  |  [Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers](https://doi.org/10.18653/v1/2023.findings-acl.189) |  | 0 | Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated... | Félix Gaschi, Parisa Rastin, Patricio Cerda, Yannick Toussaint |  |
| 532 |  |  [Aerial Vision-and-Dialog Navigation](https://doi.org/10.18653/v1/2023.findings-acl.190) |  | 0 | The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people’s burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities... | Chun Zhou, Tongzhou Jiang, Winson Chen, Xin Wang, Yi Zhang, Yue Fan |  |
| 533 |  |  [Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming](https://doi.org/10.18653/v1/2023.findings-acl.191) |  | 0 | Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained... | Eric P. Xing, Hanlin Zhang, Jiani Huang, Mayur Naik, Ziyang Li |  |
| 534 |  |  [B2T Connection: Serving Stability and Performance in Deep Transformers](https://doi.org/10.18653/v1/2023.findings-acl.192) |  | 0 | In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.Recent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable,... | Jun Suzuki, Sho Takase, Shun Kiyono, Sosuke Kobayashi |  |
| 535 |  |  [Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data](https://doi.org/10.18653/v1/2023.findings-acl.193) |  | 0 | Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in... | Barbara Plank, Ekaterina Artemova, Robert Litschko |  |
| 536 |  |  [Domain-specific Attention with Distributional Signatures for Multi-Domain End-to-end Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.194) |  | 0 | The end-to-end task-oriented dialogue system has achieved great success in recent years. Most of these dialogue systems need to accommodate multi-domain dialogue in real-world scenarios. However, due to the high cost of dialogue data annotation and the scarcity of labeled dialogue data, existing... | Feifei Zhao, Peng Zhang, Xing Ma |  |
| 537 |  |  [CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.195) |  | 0 | Distilling knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation. However, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from... | Deyi Xiong, Haoran Sun, Shaolin Zhu, Xiaodong Lin, Xiaohu Zhao, Yikun Lei, Zhengshan Xue |  |
| 538 |  |  [Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance](https://doi.org/10.18653/v1/2023.findings-acl.196) |  | 0 | Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one... | Chris Welty, Christopher Homan, Lora Aroyo, Shira Wein |  |
| 539 |  |  [Parallel Data Helps Neural Entity Coreference Resolution](https://doi.org/10.18653/v1/2023.findings-acl.197) |  | 0 | Coreference resolution is the task of finding expressions that refer to the same entity in a text. Coreference models are generally trained on monolingual annotated data but annotating coreference is expensive and challenging. Hardmeier et al. (2013) have shown that parallel data contains latent... | Christian Hardmeier, Gongbo Tang |  |
| 540 |  |  [Towards Open-Domain Twitter User Profile Inference](https://doi.org/10.18653/v1/2023.findings-acl.198) |  | 0 | Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users’ privacy. Therefore, it is important for researchers to determine... | Alexander G. Hauptmann, Eduard H. Hovy, Haoyang Wen, Zhenxin Xiao |  |
| 541 |  |  [Eliciting Affective Events from Language Models by Multiple View Co-prompting](https://doi.org/10.18653/v1/2023.findings-acl.199) |  | 0 | Prior research on affective event classification showed that exploiting weakly labeled data for training can improve model performance. In this work, we propose a simpler and more effective approach for generating training data by automatically acquiring and labeling affective events with Multiple... | Ellen Riloff, Yuan Zhuang |  |
| 542 |  |  [ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.200) |  | 0 | Many text classification tasks require handling unseen domains with plenty of unlabeled data, thus giving rise to the self-adaption or the so-called transductive zero-shot learning (TZSL) problem. However, current methods based solely on encoders or decoders overlook the possibility that these two... | Cong Liao, Haipeng Zhang, Hang Yu, Jianguo Li, Kaihao Guo |  |
| 543 |  |  [PRAM: An End-to-end Prototype-based Representation Alignment Model for Zero-resource Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.201) |  | 0 | Zero-resource cross-lingual named entity recognition (ZRCL-NER) aims to leverage rich labeled source language data to address the NER problem in the zero-resource target language. Existing methods are built either based on data transfer or representation transfer. However, the former usually leads... | Chen Li, Jun Lang, Tieliang Gong, Wenqiang Liu, Xianli Zhang, Yucheng Huang |  |
| 544 |  |  [It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance](https://doi.org/10.18653/v1/2023.findings-acl.202) |  | 0 | Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between... | Arjun Subramonian, Hal Daumé III, Su Lin Blodgett, Xingdi Yuan |  |
| 545 |  |  [Task-adaptive Label Dependency Transfer for Few-shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.203) |  | 0 | Named Entity Recognition (NER), as a crucial subtask in natural language processing (NLP), suffers from limited labeled samples (a.k.a. few-shot). Meta-learning methods are widely used for few-shot NER, but these existing methods overlook the importance of label dependency for NER, resulting in... | Bin Cao, Jing Fan, Shan Zhang, Tianming Zhang, Yuqi Liu |  |
| 546 |  |  [WYWEB: A NLP Evaluation Benchmark For Classical Chinese](https://doi.org/10.18653/v1/2023.findings-acl.204) |  | 0 | To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese,... | Bo Zhou, Qianglong Chen, Xiaomi Zhong, Yin Zhang |  |
| 547 |  |  [A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment](https://doi.org/10.18653/v1/2023.findings-acl.205) |  | 0 | Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce... | Jia Li, Jianheng Tang, Kangfei Zhao |  |
| 548 |  |  [Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning](https://doi.org/10.18653/v1/2023.findings-acl.206) |  | 0 | Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is... | ByungGon Chun, Hyeonmin Ha, Jinsol Park, Minjoon Seo, Seungwon Hwang, Soyoung Jung |  |
| 549 |  |  [An Investigation of Noise in Morphological Inflection](https://doi.org/10.18653/v1/2023.findings-acl.207) |  | 0 | With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised... | Adam Wiemerslage, Changbing Yang, Garrett Nicolai, Katharina Kann, Miikka Silfverberg |  |
| 550 |  |  [Graph Reasoning for Question Answering with Triplet Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.208) |  | 0 | Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into... | Bing Yin, Chao Zhang, Haoming Jiang, Qingyu Yin, Shiyang Li, Xifeng Yan, Yifan Gao, Zheng Li |  |
| 551 |  |  [End-to-End Argument Mining over Varying Rhetorical Structures](https://doi.org/10.18653/v1/2023.findings-acl.209) |  | 0 | Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically... | Elena Chistova |  |
| 552 |  |  [Unsupervised Task Graph Generation from Instructional Video Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.210) |  | 0 | This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps... | Honglak Lee, Lajanugen Logeswaran, Moontae Lee, Sungryull Sohn, Yunseok Jang |  |
| 553 |  |  [Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.211) |  | 0 | Chinese Named Entity Recognition (CNER) is a widely used technology in various applications. While recent studies have focused on utilizing additional information of the Chinese language and characters to enhance CNER performance, this paper focuses on a specific aspect of CNER known as... | Di Niu, Jinwen Luo, Jiuding Yang, Weidong Guo, Yu Xu |  |
| 554 |  |  [Adversarial Textual Robustness on Visual Dialog](https://doi.org/10.18653/v1/2023.findings-acl.212) |  | 0 | Adversarial robustness evaluates the worst-case performance scenario of a machine learning model to ensure its safety and reliability. For example, cases where the user input contains a minimal change, e.g. a synonym, which causes the previously correct model to return a wrong answer. Using this... | Lu Yu, Verena Rieser |  |
| 555 |  |  [Language Model Analysis for Ontology Subsumption Inference](https://doi.org/10.18653/v1/2023.findings-acl.213) |  | 0 | Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To... | Ernesto JiménezRuiz, Hang Dong, Ian Horrocks, Jiaoyan Chen, Yuan He |  |
| 556 |  |  [Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.214) |  | 0 | Previous research has demonstrated that natural language explanations provide valuable inductive biases that guide models, thereby improving the generalization ability and data efficiency. In this paper, we undertake a systematic examination of the effectiveness of these explanations. Remarkably,... | Wanyun Cui, Xingran Chen |  |
| 557 |  |  [Varta: A Large-Scale Headline-Generation Dataset for Indic Languages](https://doi.org/10.18653/v1/2023.findings-acl.215) |  | 0 | We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our... | Jackie Chi Kit Cheung, Rahul Aralikatte, Sumanth Doddapaneni, Ziling Cheng |  |
| 558 |  |  [Better Zero-Shot Reasoning with Self-Adaptive Prompting](https://doi.org/10.18653/v1/2023.findings-acl.216) |  | 0 | Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities – they can effectively learn from a handful of handcrafted, completed... | Hanjun Dai, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister, Xingchen Wan |  |
| 559 |  |  [Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark](https://doi.org/10.18653/v1/2023.findings-acl.217) |  | 0 | Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with Subjective Preference), which... | Binyuan Hui, Caixia Yuan, Fei Huang, Xiaojie Wang, Yongbin Li, Yuxing Long |  |
| 560 |  |  [ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.218) |  | 0 | Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on... | Aishwarya Naresh Reganti, Aman Chadha, Amit P. Sheth, Amitava Das, Bimal G. Gajera, Chandan Gupta, Ruwan Wickramarachchi, Shreeyash Mukul Gowaikar, Thilini Wijesiriwardene |  |
| 561 |  |  [Financial Numeric Extreme Labelling: A dataset and benchmarking](https://doi.org/10.18653/v1/2023.findings-acl.219) |  | 0 | The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral... | Afreen Shaikh, Koustuv Dasgupta, Manjunath Hegde, Niloy Ganguly, Pawan Goyal, Soumya Sharma, Subhendu Khatuya |  |
| 562 |  |  [Multilingual Summarization with Factual Consistency Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.220) |  | 0 | Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world... | Elizabeth Clark, Jonathan Herzig, Joshua Maynez, Mirella Lapata, Roee Aharoni, Shashi Narayan |  |
| 563 |  |  [Enhancing Out-of-Vocabulary Estimation with Subword Attention](https://doi.org/10.18653/v1/2023.findings-acl.221) |  | 0 | Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words. However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words. As a result,... | Carlotta Domeniconi, Raj Patel |  |
| 564 |  |  [Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT](https://doi.org/10.18653/v1/2023.findings-acl.222) |  | 0 | Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on... | Hai Zhao, Sufeng Duan |  |
| 565 |  |  [TransGEC: Improving Grammatical Error Correction with Translationese](https://doi.org/10.18653/v1/2023.findings-acl.223) |  | 0 | Data augmentation is an effective way to improve model performance of grammatical error correction (GEC). This paper identifies a critical side-effect of GEC data augmentation, which is due to the style discrepancy between the data used in GEC tasks (i.e., texts produced by non-native speakers) and... | Dacheng Tao, Derek F. Wong, Liang Ding, Lidia S. Chao, Min Zhang, Runzhe Zhan, Tao Fang, Xuebo Liu |  |
| 566 |  |  [NewsDialogues: Towards Proactive News Grounded Conversation](https://doi.org/10.18653/v1/2023.findings-acl.224) |  | 0 | Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can... | Cheng Yang, Lifeng Shang, Qun Liu, Siheng Li, Wangjie Jiang, Xin Jiang, Yichun Yin, Yiwei Li, Yujiu Yang, Zesen Cheng |  |
| 567 |  |  [Task-aware Retrieval with Instructions](https://doi.org/10.18653/v1/2023.findings-acl.225) |  | 0 | We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow... | Akari Asai, Gautier Izacard, Hannaneh Hajishirzi, Patrick Lewis, Sebastian Riedel, Timo Schick, Wentau Yih, Xilun Chen |  |
| 568 |  |  [Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in NLP](https://doi.org/10.18653/v1/2023.findings-acl.226) |  | 0 | Human evaluation is widely regarded as the litmus test of quality in NLP. A basic requirementof all evaluations, but in particular where they are used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually... | Anya Belz, Craig Thomson, Ehud Reiter, Simon Mille |  |
| 569 |  |  [Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models](https://doi.org/10.18653/v1/2023.findings-acl.227) |  | 0 | Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests... | Hal Daumé III, Khanh Nguyen, Lingjun Zhao |  |
| 570 |  |  [Robustness of Multi-Source MT to Transcription Errors](https://doi.org/10.18653/v1/2023.findings-acl.228) |  | 0 | Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve... | Dominik Machácek, Ondrej Bojar, Peter Polak, Raj Dabre |  |
| 571 |  |  [Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings](https://doi.org/10.18653/v1/2023.findings-acl.229) |  | 0 | As the number of vulnerabilities increases day by day, security management requires more and more structured data. In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques. Vulnerability Description... | Jianwei Zhuge, Rui Zhou, Xin Liu, Yuan Tan, Zhenghang Xiao |  |
| 572 |  |  [Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.findings-acl.230) |  | 0 | Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support. These systems are typically constructed for a single domain or language and do not generalise well beyond this. To support work on Natural Language Understanding (NLU)... | Alexandra Birch, Anna Korhonen, Evgeniia Razumovskaia, Ivan Vulic, Liane Guillou, Nikita Moghe |  |
| 573 |  |  [A Robust Information-Masking Approach for Domain Counterfactual Generation](https://doi.org/10.18653/v1/2023.findings-acl.231) |  | 0 | Domain shift is a big challenge in NLP. Many approaches, thus, resort to learning domain-invariant features to mitigate the hurdles of domain shift during inference. Such methods, however, inexorably fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks,... | Navonil Majumder, Pengfei Hong, Rishabh Bhardwaj, Somak Aditya, Soujanya Poria |  |
| 574 |  |  [Misleading Relation Classifiers by Substituting Words in Texts](https://doi.org/10.18653/v1/2023.findings-acl.232) |  | 0 | Relation classification is to determine the semantic relationship between two entities in a given sentence. However, many relation classifiers are vulnerable to adversarial attacks, which is using adversarial examples to lead victim models to output wrong results. In this paper, we propose a simple... | Tian Jiang, Xiaohui Cui, Yan Feng, Yunqi Liu, Yuqing Li |  |
| 575 |  |  [Automatic Table Union Search with Tabular Representation Learning](https://doi.org/10.18653/v1/2023.findings-acl.233) |  | 0 | Given a data lake of tabular data as well as a query table, how can we retrieve all the tables in the data lake that can be unioned with the query table? Table union search constitutes an essential task in data discovery and preparation as it enables data scientists to navigate massive open data... | Asterios Katsifodimos, Christos Faloutsos, Chuan Lei, George Karypis, Lijie Wen, Philip S. Yu, Shen Wang, Xiao Qin, Xuming Hu, Zhengyuan Shen |  |
| 576 |  |  [Bidirectional Transformer Reranker for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.234) |  | 0 | Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability... | Hidetaka Kamigaito, Manabu Okumura, Ying Zhang |  |
| 577 |  |  [Not Enough Data to Pre-train Your Language Model? MT to the Rescue!](https://doi.org/10.18653/v1/2023.findings-acl.235) |  | 0 | In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for... | Ander Corral, Gorka Urbizu, Iñaki San Vicente, Xabier Saralegi |  |
| 578 |  |  [UMSE: Unified Multi-scenario Summarization Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.236) |  | 0 | Summarization quality evaluation is a non-trivial task in text summarization. Contemporary methods can be mainly categorized into two scenarios: (1) reference-based: evaluating with human-labeled reference summary; (2) reference-free: evaluating the summary consistency of the document. Recent... | Chongyang Tao, Pengjie Ren, Shen Gao, Xiuying Chen, Zhaochun Ren, Zhitao Yao, Zhumin Chen |  |
| 579 |  |  [Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.237) |  | 0 | Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples. The stealthiness of backdoor attacks is commonly attained through minimal cross-entropy loss fine-tuning on a union... | Bowen Shen, Fali Wang, Weiping Wang, Zheng Lin, Zhengxiao Liu |  |
| 580 |  |  [Improving Named Entity Recognition via Bridge-based Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.238) |  | 0 | Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to... | Changmeng Zheng, Jingyun Xu, TatSeng Chua, Yi Cai |  |
| 581 |  |  [SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.239) |  | 0 | Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively.... | Baobao Chang, Guoqiang Feng, Jiaxing Lin, Shuang Zeng, Shuzheng Si, Zefan Cai |  |
| 582 |  |  [The State of Profanity Obfuscation in Natural Language Processing Scientific Publications](https://doi.org/10.18653/v1/2023.findings-acl.240) |  | 0 | Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm... | Debora Nozza, Dirk Hovy |  |
| 583 |  |  [Teacher and Student Models of Offensive Language in Social Media](https://doi.org/10.18653/v1/2023.findings-acl.241) |  | 0 | State-of-the-art approaches to identifying offensive language online make use of large pre-trained transformer models. However, the inference time, disk, and memory requirements of these transformer models present challenges for their wide usage in the real world. Even the distilled transformer... | Marcos Zampieri, Tharindu Ranasinghe |  |
| 584 |  |  [A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification](https://doi.org/10.18653/v1/2023.findings-acl.242) |  | 0 | Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance... | Bishwaranjan Bhattacharjee, Masayasu Muraoka, Tetsuya Nasukawa, Yang Zhao |  |
| 585 |  |  [Balancing the Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer](https://doi.org/10.18653/v1/2023.findings-acl.243) |  | 0 | Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the... | David Ma, Debarati Das, Dongyeop Kang |  |
| 586 |  |  [A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches](https://doi.org/10.18653/v1/2023.findings-acl.244) |  | 0 | Extremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1)... | Dheeraj Mekala, Jingbo Shang, Tianle Wang, Zihan Wang |  |
| 587 |  |  [Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation](https://doi.org/10.18653/v1/2023.findings-acl.245) |  | 0 | Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty... | Ying Liu, Zhu Liu |  |
| 588 |  |  [Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks](https://doi.org/10.18653/v1/2023.findings-acl.246) |  | 0 | Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into... | Dian Yu, Dong Yu, Heng Ji, Jianshu Chen, Xiaoman Pan, Zhenhailong Wang |  |
| 589 |  |  [Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers](https://doi.org/10.18653/v1/2023.findings-acl.247) |  | 0 | Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open... | Damai Dai, Furu Wei, Li Dong, Shuming Ma, Yaru Hao, Yutao Sun, Zhifang Sui |  |
| 590 |  |  [Dramatic Conversation Disentanglement](https://doi.org/10.18653/v1/2023.findings-acl.248) |  | 0 | We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in... | Danica Chen, David Bamman, Kent K. Chang |  |
| 591 |  |  [Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation](https://doi.org/10.18653/v1/2023.findings-acl.249) |  | 0 | In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select... | Hyunkyung Bae, Joongbo Shin, Kyomin Jung, Yerin Hwang, Yongil Kim |  |
| 592 |  |  [Emergent Modularity in Pre-trained Transformers](https://doi.org/10.18653/v1/2023.findings-acl.250) |  | 0 | This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate... | Chaojun Xiao, Jie Zhou, Maosong Sun, Ruobing Xie, Xiaozhi Wang, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Zhiyuan Zeng |  |
| 593 |  |  [Universal Information Extraction with Meta-Pretrained Self-Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.251) |  | 0 | Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge... | Bin Wang, Bowen Yu, Fei Huang, Haiyang Yu, Mengcheng Fang, Tingwen Liu, Xin Cong, Yongbin Li, Zhongkai Hu |  |
| 594 |  |  [SETI: Systematicity Evaluation of Textual Inference](https://doi.org/10.18653/v1/2023.findings-acl.252) |  | 0 | We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and... | Anette Frank, Xiyan Fu |  |
| 595 |  |  [Coarse-to-fine Few-shot Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.253) |  | 0 | Recently, Few-shot Named Entity Recognition has received wide attention with the growing need for NER models to learn new classes with minimized annotation costs. However, one common yet understudied situation is to transfer a model trained with coarse-grained classes to recognize fine-grained... | Junzhe Wang, Qi Zhang, Ruotian Ma, Tao Gui, Xiang Gao, Xin Zhou, Xuanting Chen, Yun Wen Chen, Zhang Lin |  |
| 596 |  |  [Self-Evolution Learning for Discriminative Language Model Pretraining](https://doi.org/10.18653/v1/2023.findings-acl.254) |  | 0 | Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted.... | Bo Du, Dacheng Tao, Juhua Liu, Liang Ding, Qihuang Zhong |  |
| 597 |  |  [QueryForm: A Simple Zero-shot Form Entity Query Framework](https://doi.org/10.18653/v1/2023.findings-acl.255) |  | 0 | Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot... | ChenYu Lee, Guolong Su, Hao Zhang, Jacob Devlin, Jennifer G. Dy, Tomas Pfister, Vincent Perot, Zifeng Wang, Zizhao Zhang |  |
| 598 |  |  [Search-Oriented Conversational Query Editing](https://doi.org/10.18653/v1/2023.findings-acl.256) |  | 0 | Conversational query rewriting (CQR) realizes conversational search by reformulating the search dialogue into a standalone rewrite. However, existing CQR models either are not learned toward improving the downstream search performance or inefficiently generate the rewrite token-by-token from... | Bang Liu, Fengran Mo, Hongjin Qian, Kelong Mao, Xiangli Wu, Xiaohua Cheng, Zhao Cao, Zhicheng Dou |  |
| 599 |  |  [TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model](https://doi.org/10.18653/v1/2023.findings-acl.257) |  | 0 | Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing... | Brielen Madureira, David Schlangen, Patrick Kahardipraja |  |
| 600 |  |  [Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind](https://doi.org/10.18653/v1/2023.findings-acl.258) |  | 0 | Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this... | Ece Takmaz, Mario Giulianelli, Nicolo' Brandizzi, Raquel Fernández, Sandro Pezzelle |  |
| 601 |  |  [A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing](https://doi.org/10.18653/v1/2023.findings-acl.259) |  | 0 | Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should... | Mingming Sun, Ping Li, Ye Ma |  |
| 602 |  |  [AMR-TST: Abstract Meaning Representation-based Text Style Transfer](https://doi.org/10.18653/v1/2023.findings-acl.260) |  | 0 | Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph... | Dingxian Wang, Guandong Xu, Kaize Shi, Li He, Qing Li, Xueyao Sun |  |
| 603 |  |  [Understanding the Cooking Process with English Recipe Text](https://doi.org/10.18653/v1/2023.findings-acl.261) |  | 0 | Translating procedural text, like recipes, into a graphical representation can be important for visualizing the text, and can offer a machine-readable formalism for use in software. There are proposals for translating recipes into a flow graph representation, where each node represents an... | Anthony Hunter, Yi Fan |  |
| 604 |  |  [Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding](https://doi.org/10.18653/v1/2023.findings-acl.262) |  | 0 | In open-ended natural-language generation, existing text decoding methods typically struggle to produce text which is both diverse and high-quality. Greedy and beam search are known to suffer from text degeneration and linguistic diversity issues, while temperature, top-k, and nucleus sampling... | Dan Jurafsky, Luke MelasKyriazi, Mirac Suzgun |  |
| 605 |  |  [RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.263) |  | 0 | Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are... | Bing Xiang, Dan Roth, Juliette Burger, Lan Liu, Peng Qi, Rujun Han, William Yang Wang, Yuhao Zhang, Zhiheng Huang |  |
| 606 |  |  [SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations](https://doi.org/10.18653/v1/2023.findings-acl.264) |  | 0 | Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model’s decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular,... | LouisPhilippe Morency, Victoria Lin |  |
| 607 |  |  [Reinforcement Learning for Topic Models](https://doi.org/10.18653/v1/2023.findings-acl.265) |  | 0 | We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modifications: modernize... | Jeremy Costello, Marek Z. Reformat |  |
| 608 |  |  [Contextualized Soft Prompts for Extraction of Event Arguments](https://doi.org/10.18653/v1/2023.findings-acl.266) |  | 0 | Event argument extraction (EAE) is a sub-task of event extraction where the goal is to identify roles of entity mentions for events in text. The current state-of-the-art approaches for this problem explore prompt-based methods to prompt pre-trained language models for arguments over input context.... | Chien Van Nguyen, Hieu Man, Thien Huu Nguyen |  |
| 609 |  |  [TextVerifier: Robustness Verification for Textual Classifiers with Certifiable Guarantees](https://doi.org/10.18653/v1/2023.findings-acl.267) |  | 0 | When textual classifiers are deployed in safety-critical workflows, they must withstand the onslaught of AI-enabled model confusion caused by adversarial examples with minor alterations. In this paper, the main objective is to provide a formal verification framework, called TextVerifier, with... | Siqi Sun, Wenjie Ruan |  |
| 610 |  |  [OASum: Large-Scale Open Domain Aspect-based Summarization](https://doi.org/10.18653/v1/2023.findings-acl.268) |  | 0 | Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users’ interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few... | Dong Yu, Kaiqiang Song, Linda R. Petzold, Sangwoo Cho, Xianjun Yang, Xiaoman Pan, Xiaoyang Wang |  |
| 611 |  |  [On the Limitations of Simulating Active Learning](https://doi.org/10.18653/v1/2023.findings-acl.269) |  | 0 | Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve data efficiency over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process,... | Katerina Margatina, Nikolaos Aletras |  |
| 612 |  |  [Towards Alleviating the Object Bias in Prompt Tuning-based Factual Knowledge Extraction](https://doi.org/10.18653/v1/2023.findings-acl.270) |  | 0 | Many works employed prompt tuning methods to automatically optimize prompt queries and extract the factual knowledge stored in Pre-trained Language Models. In this paper, we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias. To... | Chao Kong, Dongyuan Lu, Jitao Sang, Yuhang Wang |  |
| 613 |  |  [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport](https://doi.org/10.18653/v1/2023.findings-acl.271) |  | 0 | Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling... | Francis Iannacci, Jinjin Zhao, Srinivasan Sengamedu Hanumantha Rao, Weijie Xu, Xiaoyu Jiang |  |
| 614 |  |  [Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages](https://doi.org/10.18653/v1/2023.findings-acl.272) |  | 0 | Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and... | Adam Lopez, Diego Marcheggiani, Roi Blanco, Seraphina GoldfarbTarrant |  |
| 615 |  |  [Complementary Explanations for Effective In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.273) |  | 0 | Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations... | Asli Celikyilmaz, Greg Durrett, Ramakanth Pasunuru, Srinivasan Iyer, Veselin Stoyanov, Xi Ye |  |
| 616 |  |  [MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types](https://doi.org/10.18653/v1/2023.findings-acl.274) |  | 0 | With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated... | Achille Fokoue, Alexander Gray, Diwakar Mahajan, Ibrahim Abdelaziz, Keerthiram Murugesan, Maxwell Crouse, Pavan Kapanipathi, R. Chulaka Gunasekara, Salim Roukos, Sarathkrishna Swaminathan, Soham Dan, Subhajit Chaudhury |  |
| 617 |  |  [RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding](https://doi.org/10.18653/v1/2023.findings-acl.275) |  | 0 | Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between... | Bryan Wilie, Min Zeng, Nayeon Lee, Pascale Fung, Tiezheng Yu, Zihan Liu, Ziwei Ji |  |
| 618 |  |  [Transformer Language Models Handle Word Frequency in Prediction Head](https://doi.org/10.18653/v1/2023.findings-acl.276) |  | 0 | Prediction head is a crucial component of Transformer language models. Despite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.In this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters. Our... | Goro Kobayashi, Kentaro Inui, Sho Yokoi, Tatsuki Kuribayashi |  |
| 619 |  |  [Prompted LLMs as Chatbot Modules for Long Open-domain Conversation](https://doi.org/10.18653/v1/2023.findings-acl.277) |  | 0 | In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using... | Dimitris Papailiopoulos, Gibbeum Lee, Jongho Park, Kangwook Lee, Volker Hartmann |  |
| 620 |  |  [Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.278) |  | 0 | Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to Prompt pre-trained language models (PLMs) To be Consistent, for improving the factuality assessment capability of PLMs in the few-shot and... | Fengzhu Zeng, Wei Gao |  |
| 621 |  |  [Model Analysis & Evaluation for Ambiguous Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.279) |  | 0 | Ambiguous questions are a challenge for Question Answering models, as they require answers that cover multiple interpretations of the original query. To this end, these models are required to generate long-form answers that often combine conflicting pieces of information. Although recent advances... | Irene Papadopoulou, Konstantinos Papakostas |  |
| 622 |  |  [Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models](https://doi.org/10.18653/v1/2023.findings-acl.280) |  | 0 | Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also... | Ali Emami, Jad Kabbara, Robert Morabito |  |
| 623 |  |  [Critic-Guided Decoding for Controlled Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.281) |  | 0 | Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality... | Hwanhee Lee, Hwaran Lee, Joonsuk Park, Kang Min Yoo, Kyomin Jung, Minbeom Kim |  |
| 624 |  |  [MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations](https://doi.org/10.18653/v1/2023.findings-acl.282) |  | 0 | Patients who effectively manage their symptoms often demonstrate higher levels of engagement in conversations and interventions with healthcare practitioners. This engagement is multifaceted, encompassing cognitive and social dimensions. Consequently, it is crucial for AI systems to understand the... | Heidi Ann Scharf Donovan, Malihe Alikhani, Sabit Hassan, Yan Wang |  |
| 625 |  |  [SEAG: Structure-Aware Event Causality Generation](https://doi.org/10.18653/v1/2023.findings-acl.283) |  | 0 | Extracting event causality underlies a broad spectrum of natural language processing applications. Cutting-edge methods break this task into Event Detection and Event Causality Identification. Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of... | Chengfeng Dou, Chongyang Tao, Fang Wang, Haiyan Zhao, Xiaoying Bai, Yongqiang Zhao, Zhengwei Tao, Zhi Jin |  |
| 626 |  |  [Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.284) |  | 0 | Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of... | Dehan Kong, Hui Xue, Longtao Huang, Ruixiang Tang |  |
| 627 |  |  [A Two-Stage Decoder for Efficient ICD Coding](https://doi.org/10.18653/v1/2023.findings-acl.285) |  | 0 | Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and... | Abhinav Ramesh Kashyap, Stefan Winkler, ThanhTung Nguyen, Viktor Schlegel |  |
| 628 |  |  [Asymmetric feature interaction for interpreting model predictions](https://doi.org/10.18653/v1/2023.findings-acl.286) |  | 0 | In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the... | Haode Zhang, Jianghong Ma, Xiaolei Lu |  |
| 629 |  |  [Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo](https://doi.org/10.18653/v1/2023.findings-acl.287) |  | 0 | Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard “ground truth” as the distribution of all labels that a... | Alexander Ororbia, Ashiqur R. KhudaBukhsh, Christopher Homan, Raj Bhensadadia, Tharindu Cyril Weerasooriya |  |
| 630 |  |  [Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.288) |  | 0 | Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to... | Pradeepika Verma, Pranav Ajit Nair, Sukomal Pal |  |
| 631 |  |  [ClaimDiff: Comparing and Contrasting Claims on Contentious Issues](https://doi.org/10.18653/v1/2023.findings-acl.289) |  | 0 | With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers,... | Hwaran Lee, Ingyu Seong, Joonsuk Park, Minjoon Seo, Minsuk Chang, Miyoung Ko |  |
| 632 |  |  [Unsupervised Paraphrasing of Multiword Expressions](https://doi.org/10.18653/v1/2023.findings-acl.290) |  | 0 | We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the... | Jey Han Lau, Takashi Wada, Timothy Baldwin, Yuji Matsumoto |  |
| 633 |  |  [G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network](https://doi.org/10.18653/v1/2023.findings-acl.291) |  | 0 | The generalization ability of pre-trained language models (Plms) in downstream tasks is heavily influenced by fine-tuning. The objective of fine-tuning is to transform the latent representation of Plms from a universal space to a target space, allowing the model to be applied to downstream tasks... | Min Zhang, Rongxiang Weng, Wensen Cheng |  |
| 634 |  |  [Unified Language Representation for Question Answering over Text, Tables, and Images](https://doi.org/10.18653/v1/2023.findings-acl.292) |  | 0 | When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal... | Bowen Yu, Cheng Fu, Fei Huang, Haiyang Yu, Yongbin Li |  |
| 635 |  |  [A Set Prediction Network For Extractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.293) |  | 0 | Extractive summarization focuses on extracting salient sentences from the source document and incorporating them in the summary without changing their wording or structure. The naive approach for extractive summarization is sentence classification, which makes independent binary decisions for each... | Weiming Lu, Xiaoxia Cheng, Yongliang Shen |  |
| 636 |  |  [Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.294) |  | 0 | Location information can support social media analyses by providing geographic context. Some of the most accurate and popular Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which fail to handle informal or noisy location names. We propose... | Alexandra DeLucia, Chenyu Zhang, Jingyu Zhang, Mark Dredze |  |
| 637 |  |  [Predicting Numerals in Text Using Nearest Neighbor Language Models](https://doi.org/10.18653/v1/2023.findings-acl.295) |  | 0 | Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals. However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the... | Akiko Aizawa, Taku Sakamoto |  |
| 638 |  |  [HonestBait: Forward References for Attractive but Faithful Headline Generation](https://doi.org/10.18653/v1/2023.findings-acl.296) |  | 0 | Current methods for generating attractive headlines often learn directly from data, which bases attractiveness on the number of user clicks and views. Although clicks or views do reflect user interest, they can fail to reveal how much interest is raised by the writing style and how much is due to... | ChihYao Chen, Dennis Wu, LunWei Ku |  |
| 639 |  |  [Few Shot Rationale Generation using Self-Training with Dual Teachers](https://doi.org/10.18653/v1/2023.findings-acl.297) |  | 0 | Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly process, recent models rely on large pretrained language models... | Aditya Srikanth Veerubhotla, György Szarvas, Jun Yin, Lahari Poddar, Sharanya Eswaran |  |
| 640 |  |  [Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints](https://doi.org/10.18653/v1/2023.findings-acl.298) |  | 0 | Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the... | Cheonbok Park, Dayeon Ki, HyoungGyu Lee, Jaegul Choo, Koanho Lee, Yujin Baek |  |
| 641 |  |  [NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.299) |  | 0 | Large-scale datasets in the real world inevitably involve label noise. Deep models can gradually overfit noisy labels and thus degrade model generalization. To mitigate the effects of label noise, learning with noisy labels (LNL) methods are designed to achieve better generalization performance.... | Bing Qin, Hao Zhang, Minji Tang, Ting Liu, Tingting Wu, Xiao Ding |  |
| 642 |  |  [Sampling Better Negatives for Distantly Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.300) |  | 0 | Distantly supervised named entity recognition (DS-NER) has been proposed to exploit the automatically labeled training data instead of human annotations. The distantly annotated datasets are often noisy and contain a considerable number of false negatives. The recent approach uses a weighted... | Lidong Bing, Lu Xu, Wei Lu |  |
| 643 |  |  [Prototype-Based Interpretability for Legal Citation Prediction](https://doi.org/10.18653/v1/2023.findings-acl.301) |  | 0 | Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this... | Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu |  |
| 644 |  |  [LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models](https://doi.org/10.18653/v1/2023.findings-acl.302) |  | 0 | Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors.... | Philipp Wicke |  |
| 645 |  |  [Making Better Use of Training Corpus: Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation](https://doi.org/10.18653/v1/2023.findings-acl.303) |  | 0 | In this paper, we aim to adapt the idea of retrieval-based neural approaches to the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes... | Guoxin Yu, Haiyun Jiang, Lemao Liu, Shuming Shi, Xiang Ao |  |
| 646 |  |  [Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention](https://doi.org/10.18653/v1/2023.findings-acl.304) |  | 0 | As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems. Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider. In recent... | Jiyi Li, Longfei Yang, Sheng Li, Takahiro Shinozaki |  |
| 647 |  |  [Improved Visual Story Generation with Adaptive Context Modeling](https://doi.org/10.18653/v1/2023.findings-acl.305) |  | 0 | Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all... | Bing Qin, Duyu Tang, Shuming Shi, Xiaocheng Feng, Xinmiao Yu, Yuchen Ren, Zhangyin Feng |  |
| 648 |  |  [Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.306) |  | 0 | We focus on dialogue reading comprehension (DRC) that extracts answers from dialogues. Compared to standard RC tasks, DRC has raised challenges because of the complex speaker information and noisy dialogue context. Essentially, the challenges come from the speaker-centric nature of dialogue... | Fandong Meng, Jiangnan Li, Jie Zhou, Mo Yu, Peng Fu, Weiping Wang, Zheng Lin |  |
| 649 |  |  [Speech-to-Speech Translation for a Real-world Unwritten Language](https://doi.org/10.18653/v1/2023.findings-acl.307) |  | 0 | We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training... | Ann Lee, Changhan Wang, Hirofumi Inaguma, Holger Schwenk, Hongyu Gong, Jingfei Du, Juan Pino, Justine Kao, Kevin Tran, Paden Tomasello, PaulAmbroise Duquenne, PengJen Chen, Sravya Popuri, WeiNing Hsu, Yilin Yang, YuAn Chung |  |
| 650 |  |  [Code Execution with Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.308) |  | 0 | Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well... | Alexey Svyatkovskiy, Chenxiao Liu, Daxin Jiang, Nan Duan, Neel Sundaresan, Shengyu Fu, Shuai Lu, Weizhu Chen |  |
| 651 |  |  [BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.309) |  | 0 | It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost... | Bin Ni, Bowen Tan, Eric P. Xing, Hengzhe Zhang, Kaiwen Tang, Shibo Hao, Xiyan Shao, Zhiting Hu |  |
| 652 |  |  [Sequential Path Signature Networks for Personalised Longitudinal Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.310) |  | 0 | Longitudinal user modeling can provide a strong signal for various downstream tasks. Despite the rapid progress in representation learning, dynamic aspects of modelling individuals’ language have only been sparsely addressed. We present a novel extension of neural sequential models using the notion... | Adam Tsakalidis, Maria Liakata, Peter Foster, Talia Tseriotou, Terence Lyons |  |
| 653 |  |  [A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.311) |  | 0 | Recent studies have pointed out that many well-developed Visual Question Answering (VQA) systems suffer from bias problem. Despite the remarkable performance gained on In-Distribution (ID) datasets, the VQA model might merely capture the superficial correlation from question to answer rather than... | Bojie Hu, Fengshuo Zhang, Jian Liu, Jinan Xu, Yahan Yu, Yu Li, Yufeng Chen |  |
| 654 |  |  [Trigger-Argument based Explanation for Event Detection](https://doi.org/10.18653/v1/2023.findings-acl.312) |  | 0 | Event Detection (ED) is a critical task that aims to identify events of certain types in plain text. Neural models have achieved great success on ED, thus coming with a desire for higher interpretability. Existing works mainly exploit words or phrases of the input text to explain models’ inner... | Freddy Lécué, Jeff Z. Pan, Jiaoyan Chen, Juanzi Li, Ru Li, Yong Guan |  |
| 655 |  |  [Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections](https://doi.org/10.18653/v1/2023.findings-acl.313) |  | 0 | Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a... | Dan Goldwasser, Lyle H. Ungar, Maria Leonor Pacheco, Ming Yin, Tunazzina Islam |  |
| 656 |  |  [NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery](https://doi.org/10.18653/v1/2023.findings-acl.314) |  | 0 | Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take... | Farhad Moghimifar, Gholamreza Haffari, Shilin Qu, Tongtong Wu, YuanFang Li |  |
| 657 |  |  [VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations](https://doi.org/10.18653/v1/2023.findings-acl.315) |  | 0 | Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without... | HoangQuoc NguyenSon, Isao Echizen, Kazuhide Fukushima, Seira Hidano, Shinsaku Kiyomoto |  |
| 658 |  |  [Fusion or Defusion? Flexible Vision-and-Language Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.316) |  | 0 | Existing approaches in the vision-and-language pre-training (VLP) paradigm mainly deploy either fusion-based encoders or dual-encoders, failing to achieve both effectiveness and efficiency in downstream multimodal tasks. In this paper, we build a flexible VLP model by incorporating cross-modal... | Haitao Zheng, Jingang Wang, Qifan Wang, Rongyi Sun, Wei Wu, Yifeng Ding, Yunsen Xian, Ziran Li |  |
| 659 |  |  [COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP](https://doi.org/10.18653/v1/2023.findings-acl.317) |  | 0 | Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this... | Agustin Martin Picard, Fanny Jourdan, JeanMichel Loubes, Laurent Risser, Nicholas Asher, Thomas Fel |  |
| 660 |  |  [Code-Switched Text Synthesis in Unseen Language Pairs](https://doi.org/10.18653/v1/2023.findings-acl.318) |  | 0 | Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs... | Avik Ray, IHung Hsu, Jing Huang, Nanyun Peng, Shubham Garg |  |
| 661 |  |  [Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning](https://doi.org/10.18653/v1/2023.findings-acl.319) |  | 0 | Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking... | Gerasimos Spanakis, JustusJonas Erker, Stefan Schaffer |  |
| 662 |  |  [Data-Efficient French Language Modeling with CamemBERTa](https://doi.org/10.18653/v1/2023.findings-acl.320) |  | 0 | Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and... | Benoît Sagot, Djamé Seddah, Wissam Antoun |  |
| 663 |  |  [Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text](https://doi.org/10.18653/v1/2023.findings-acl.321) |  | 0 | While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly... | Adam Ishay, Joohyung Lee, Zhun Yang |  |
| 664 |  |  [Evaluating the Factual Consistency of Large Language Models Through News Summarization](https://doi.org/10.18653/v1/2023.findings-acl.322) |  | 0 | While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that... | Anisha Mascarenhas, Colin Raffel, Derek Tam, Mohit Bansal, Sarah Kwan, Shiyue Zhang |  |
| 665 |  |  [Text Generation Model Enhanced with Semantic Information in Aspect Category Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.323) |  | 0 | Aspect Category Sentiment Analysis (ACSA) is one of the main subtasks of sentiment analysis, which aims at predicting polarity over a given aspect category. Recently, generative methods emerge as an efficient way to utilize a pre-trained language model for solving ACSA. However, those methods fail... | Kiyoaki Shirai, Natthawut Kertkeidkachorn, Tu Tran |  |
| 666 |  |  [Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting](https://doi.org/10.18653/v1/2023.findings-acl.324) |  | 0 | We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to... | Hwee Tou Ng, Ruixi Lin |  |
| 667 |  |  [CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.325) |  | 0 | In order to solve the inefficient parameter update and storage issues of fine-tuning in Natural Language Generation (NLG) tasks, prompt-tuning methods have emerged as lightweight alternatives. Furthermore, efforts to reduce the gap between pre-training and fine-tuning have shown successful results... | JeeHyong Lee, YunSeok Choi |  |
| 668 |  |  [Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale](https://doi.org/10.18653/v1/2023.findings-acl.326) |  | 0 | In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge... | Anna Rumshisky, Dan Pechi, Shree Thatte, Vijeta Deshpande, Vladislav Lialin |  |
| 669 |  |  [Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter](https://doi.org/10.18653/v1/2023.findings-acl.327) |  | 0 | Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive... | Lei Li, Sishuo Chen, Wenkai Yang, Xiaohan Bi, Xu Sun, Yi Liu |  |
| 670 |  |  [Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.328) |  | 0 | Text classification with extremely weak supervision (EWS) imposes stricter supervision constraints compared to regular weakly supervise classification. Absolutely no labeled training samples or hand-crafted rules specific to the evaluation data are allowed. Such restrictions limit state-of-the-art... | Jihwa Lee, Kyungho Kim, Seongmin Park |  |
| 671 |  |  [GVdoc - Graph-based Visual DOcument Classification](https://doi.org/10.18653/v1/2023.findings-acl.329) |  | 0 | The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time... | Ashish Verma, Catherine FineganDollak, Fnu Mohbat, Mohammed J. Zaki |  |
| 672 |  |  [A Sequence-to-Sequence&Set Model for Text-to-Table Generation](https://doi.org/10.18653/v1/2023.findings-acl.330) |  | 0 | Recently, the text-to-table generation task has attracted increasing attention due to its wide applications. In this aspect, the dominant model formalizes this task as a sequence-to-sequence generation task and serializes each table into a token sequence during training by concatenating all rows in... | Jinsong Su, Liangying Shao, Tong Li, Xiaoli Wang, Xuling Zheng, Zhihao Wang |  |
| 673 |  |  [Automatic Readability Assessment for Closely Related Languages](https://doi.org/10.18653/v1/2023.findings-acl.331) |  | 0 | In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models’ accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted... | Ekaterina Kochmar, Joseph Marvin Imperial |  |
| 674 |  |  [Towards Robust Ranker for Text Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.332) |  | 0 | A neural ranker plays an indispensable role in the de facto ‘retrieval & rerank’ pipeline, but its training still lags behind due to the weak negative mining during contrastive learning. Compared to retrievers boosted by self-adversarial (i.e., in-distribution) negative mining, the ranker’s heavy... | Binxing Jiao, Can Xu, Chongyang Tao, Daxin Jiang, Guodong Long, Tao Shen, Xiubo Geng, Yucheng Zhou |  |
| 675 |  |  [Semi-Supervised Domain Adaptation for Emotion-Related Tasks](https://doi.org/10.18653/v1/2023.findings-acl.333) |  | 0 | Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often... | Cornelia Caragea, Mahshid Hosseini |  |
| 676 |  |  [Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy](https://doi.org/10.18653/v1/2023.findings-acl.334) |  | 0 | AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain... | Anuradha Welivita, Pearl Pu |  |
| 677 |  |  [ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language Representations](https://doi.org/10.18653/v1/2023.findings-acl.335) |  | 0 | Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which... | Heinz Koeppl, Hinrich Schütze, Jindong Gu, Ruotong Liao, Volker Tresp, Yao Zhang, Yujia Gu, Zhen Han, Zifeng Ding |  |
| 678 |  |  [Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.336) |  | 0 | Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore,... | Hamed Khanpour, Hamid Palangi, Radames Cruz Moreno, Somayeh Ghanbarzadeh, Yan Huang |  |
| 679 |  |  [TextObfuscator: Making Pre-trained Language Model a Privacy Protector via Obfuscating Word Representations](https://doi.org/10.18653/v1/2023.findings-acl.337) |  | 0 | In real-world applications, pre-trained language models are typically deployed on the cloud, allowing clients to upload data and perform compute-intensive inference remotely. To avoid sharing sensitive data directly with service providers, clients can upload numerical representations rather than... | Qi Zhang, Ruotian Ma, Tao Gui, Xin Zhou, Xuanjing Huang, Yi Lu, Yibo Zhang, Yong Ding, Yuran Wang |  |
| 680 |  |  [Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training](https://doi.org/10.18653/v1/2023.findings-acl.338) |  | 0 | Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new... | Kelly Marchisio, Mikel Artetxe, Patrick S. H. Lewis, Yihong Chen |  |
| 681 |  |  [DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.339) |  | 0 | Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios. Typically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.However, when applied to... | Bo Lv, Fan Yang, Nayu Liu, Ping Luo, Shaojie Dai, Xin Liu, Yue Yu |  |
| 682 |  |  [Exploring Robust Overfitting for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.340) |  | 0 | We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial... | Bin Zhu, Yanghui Rao |  |
| 683 |  |  [Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations](https://doi.org/10.18653/v1/2023.findings-acl.341) |  | 0 | There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the... | Jifan Chen, Lan Liu, Patrick Ng, Rui Dong, William Yang Wang, Xinchi Chen, Yuhao Zhang, Zhiheng Huang |  |
| 684 |  |  [D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias](https://doi.org/10.18653/v1/2023.findings-acl.342) |  | 0 | Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without... | Malihe Alikhani, Sabit Hassan |  |
| 685 |  |  [Language Anisotropic Cross-Lingual Model Editing](https://doi.org/10.18653/v1/2023.findings-acl.343) |  | 0 | Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping... | Min Zhang, Wanxiang Che, Yang Xu, Yutai Hou |  |
| 686 |  |  [Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.344) |  | 0 | There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even... | Brendan King, Jeffrey Flanigan |  |
| 687 |  |  [Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation](https://doi.org/10.18653/v1/2023.findings-acl.345) |  | 0 | Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained... | Chunliu Wang, Huiyuan Lai, Johan Bos, Malvina Nissim |  |
| 688 |  |  [Multi-modal Sarcasm Generation: Dataset and Solution](https://doi.org/10.18653/v1/2023.findings-acl.346) |  | 0 | As an interesting and challenging task, sarcasm generation has attracted widespread attention. Although very recent studies have made promising progress, none of them considers generating a sarcastic description for a given image - as what people are doing on Twitter. In this paper, we present a... | Dongsheng Xu, Peizhi Zhao, Qingbao Huang, Wenye Zhao |  |
| 689 |  |  [Rethinking Semi-supervised Learning with Language Models](https://doi.org/10.18653/v1/2023.findings-acl.347) |  | 0 | Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive... | Emine Yilmaz, Francesco Tonolini, Gabriella Kazai, Nikolaos Aletras, Yunlong Jiao, Zhengxiang Shi |  |
| 690 |  |  [Retrieval-Based Transformer for Table Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.348) |  | 0 | Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources,... | Alfio Gliozzo, Ankita Rajaram Naik, Gaetano Rossiello, Michael R. Glass, Xueqing Wu |  |
| 691 |  |  [ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER](https://doi.org/10.18653/v1/2023.findings-acl.349) |  | 0 | Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train... | Aitzaz Ahmad, Henry Jin, Karan Aggarwal |  |
| 692 |  |  [Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages](https://doi.org/10.18653/v1/2023.findings-acl.350) |  | 0 | Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings... | David Marecek, Jirí Balhar, Tomasz Limisiewicz |  |
| 693 |  |  [The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding](https://doi.org/10.18653/v1/2023.findings-acl.351) |  | 0 | In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based... | Adam Pauls, Anusha Balakrishnan, Dan Klein, Hao Fang, Harsh Jhamtani, Jacob Andreas, Jason Eisner, Jayant Krishnamurthy, Jean Crawford, John Bufe |  |
| 694 |  |  [Know What I don't Know: Handling Ambiguous and Unknown Questions for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.352) |  | 0 | The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a plausible SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions.... | Bing Wang, JianGuang Lou, Yan Gao, Zhoujun Li |  |
| 695 |  |  [Rethinking Document-Level Relation Extraction: A Reality Check](https://doi.org/10.18653/v1/2023.findings-acl.353) |  | 0 | Recently, numerous efforts have continued to push up performance boundaries of document-level relation extraction (DocRE) and have claimed significant progress in DocRE. In this paper, we do not aim at proposing a novel model for DocRE. Instead, we take a closer look at the field to see if these... | Jing Li, Min Zhang, Shuai Zhang, Yequan Wang |  |
| 696 |  |  [Optimizing Test-Time Query Representations for Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.354) |  | 0 | Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by... | Danqi Chen, Jaewoo Kang, Jinhyuk Lee, Jungsoo Park, Mujeen Sung |  |
| 697 |  |  [A Customized Text Sanitization Mechanism with Differential Privacy](https://doi.org/10.18653/v1/2023.findings-acl.355) |  | 0 | As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on a relaxed notion of metric local... | Cen Chen, Chengyu Wang, Fengran Mo, Jamie Cui, JianYun Nie, Sai Chen, Yanhao Wang |  |
| 698 |  |  [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://doi.org/10.18653/v1/2023.findings-acl.356) |  | 0 | Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple,... | Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Peng Lu, Philippe Langlais |  |
| 699 |  |  [Frustratingly Easy Label Projection for Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.findings-acl.357) |  | 0 | Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto... | Alan Ritter, Chao Jiang, Wei Xu, Yang Chen |  |
| 700 |  |  [Enhancing Hierarchical Text Classification through Knowledge Graph Integration](https://doi.org/10.18653/v1/2023.findings-acl.358) |  | 0 | Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their... | Enhong Chen, Kai Zhang, Kehang Wang, Qi Liu, Yanghai Zhang, Ye Liu, Zhenya Huang |  |
| 701 |  |  [How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.359) |  | 0 | The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this... | Chen Zhang, Dongyan Zhao, Jiuheng Lin, Xiao Liu, Yansong Feng, Yuxuan Lai |  |
| 702 |  |  [An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text](https://doi.org/10.18653/v1/2023.findings-acl.360) |  | 0 | Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder... | Ilias Chalkidis, Yova Kementchedjhieva |  |
| 703 |  |  [Domain Incremental Lifelong Learning in an Open World](https://doi.org/10.18653/v1/2023.findings-acl.361) |  | 0 | Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either... | Bowen Yu, Fei Huang, Hao Lang, Yi Dai, Yinhe Zheng, Yongbin Li |  |
| 704 |  |  [Improving Knowledge Graph Completion with Generative Hard Negative Mining](https://doi.org/10.18653/v1/2023.findings-acl.362) |  | 0 | Contrastive learning has recently shown great potential to improve text-based knowledge graph completion (KGC). In this paper, we propose to learn a more semantically structured entity representation space in text-based KGC via hard negatives mining. Specifically, we novelly leverage a... | Dingyao Yu, Shikun Zhang, Tong Mo, Wei Ye, Weiping Li, Zile Qiao |  |
| 705 |  |  [Visually-Enhanced Phrase Understanding](https://doi.org/10.18653/v1/2023.findings-acl.363) |  | 0 | Large-scale vision-language pre-training has exhibited strong performance in various visual and textual understanding tasks. Recently, the textual encoders of multi-modal pre-trained models have been shown to generate high-quality textual representations, which often outperform models that are... | ChaoWei Huang, ChenAn Li, TsuYuan Hsu, YunNung Chen |  |
| 706 |  |  [Reasoning in Large Language Models Through Symbolic Math Word Problems](https://doi.org/10.18653/v1/2023.findings-acl.364) |  | 0 | Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic... | Nikunj Saunshi, Vedant Gaur |  |
| 707 |  |  [It's not Sexually Suggestive; It's Educative \| Separating Sex Education from Suggestive Content on TikTok videos](https://doi.org/10.18653/v1/2023.findings-acl.365) |  | 0 | We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator’s point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex... | Enfa George, Mihai Surdeanu |  |
| 708 |  |  [Dynamic Structured Neural Topic Model with Self-Attention Mechanism](https://doi.org/10.18653/v1/2023.findings-acl.366) |  | 0 | This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies. Our model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism. Additionally, we... | Ichiro Sakata, Junichiro Mori, Masaru Isonuma, Nozomu Miyamoto, Sho Takase |  |
| 709 |  |  [Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.367) |  | 0 | This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance... | Ming Chen, Qiang Wang, Xinhui Hu |  |
| 710 |  |  [Commonsense Knowledge Transfer for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.368) |  | 0 | Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the... | Ronan Le Bras, Wangchunshu Zhou, Yejin Choi |  |
| 711 |  |  [Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection](https://doi.org/10.18653/v1/2023.findings-acl.369) |  | 0 | Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model’s representations. However, current methods are restricted to removing only linearly encoded information.... | Kira Radinsky, Shadi Iskander, Yonatan Belinkov |  |
| 712 |  |  [Focal Training and Tagger Decouple for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.370) |  | 0 | In this paper, we investigate how to improve tagging-based Grammatical Error Correction models. We address two issues of current tagging-based approaches, label imbalance issue, and tagging entanglement issue. Then we propose to down-weight the loss of well-classified labels using Focal Loss and... | Min Yang, Minghuan Tan, Ruifeng Xu |  |
| 713 |  |  [LET: Leveraging Error Type Information for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.371) |  | 0 | Grammatical error correction (GEC) aims to correct errors in given sentences and is significant to many downstream natural language understanding tasks. Recent work introduces the idea of grammatical error detection (GED) to improve the GEC task performance. In contrast, these explicit multi-stage... | Chengyin Xu, Chun Yuan, Hongjia Li, Lei Li, Lingyu Yang, Shutao Xia |  |
| 714 |  |  [On the Role of Parallel Data in Cross-lingual Transfer Learning](https://doi.org/10.18653/v1/2023.findings-acl.372) |  | 0 | While prior work has established that the use of parallel data is conducive for cross-lingual learning, it is unclear if the improvements come from the data itself, or if it is the modeling of parallel interactions that matters. Exploring this, we examine the usage of unsupervised machine... | Machel Reid, Mikel Artetxe |  |
| 715 |  |  [CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-acl.373) |  | 0 | Attribute Value Extraction (AVE) aims to automatically obtain attribute value pairs from product descriptions to aid e-commerce. Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at... | Dong Yang, Guilin Qi, Liubin Wang, Mengdi Zhou, Tianxing Wu, Wentao Deng, Xinnan Guo, Yang Li, Yong Pan, Yongrui Chen |  |
| 716 |  |  [Phrase Retrieval for Open Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.374) |  | 0 | Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the... | Jinheon Baek, Jong Park, Soyeong Jeong, Sung Ju Hwang |  |
| 717 |  |  [Unlearning Bias in Language Models by Partitioning Gradients](https://doi.org/10.18653/v1/2023.findings-acl.375) |  | 0 | Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural... | Anish Kasi, Charles Yu, Heng Ji, Pengfei Yu, Sullam Jeoung |  |
| 718 |  |  [Meta-training with Demonstration Retrieval for Efficient Few-shot Learning](https://doi.org/10.18653/v1/2023.findings-acl.376) |  | 0 | Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Meta-training allows one to leverage smaller models for few-shot generalization in a domain-general and task-agnostic manner; however, these methods alone results in... | Aaron Mueller, Hamed Firooz, Kanika Narang, Lambert Mathias, Qifan Wang |  |
| 719 |  |  [VCSUM: A Versatile Chinese Meeting Summarization Dataset](https://doi.org/10.18653/v1/2023.findings-acl.377) |  | 0 | Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We... | Ding Liang, Han Wu, Haochen Tan, Linqi Song, Mingjie Zhan, Zhaohui Hou |  |
| 720 |  |  [LEDA: a Large-Organization Email-Based Decision-Dialogue-Act Analysis Dataset](https://doi.org/10.18653/v1/2023.findings-acl.378) |  | 0 | Collaboration increasingly happens online. This is especially true for large groups working on global tasks, with collaborators all around the globe. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of... | Colin Perkins, Gareth Tyson, Ignacio Castro, Matthew Purver, Mladen Karan, Patrick Healey, Prashant Khare, Ravi Shekhar, Stephen McQuistin |  |
| 721 |  |  [Negation Scope Refinement via Boundary Shift Loss](https://doi.org/10.18653/v1/2023.findings-acl.379) |  | 0 | Negation in natural language may affect many NLP applications, e.g., information extraction and sentiment analysis. The key sub-task of negation detection is negation scope resolution which aims to extract the portion of a sentence that is being negated by a negation cue (e.g., keyword “not” and... | Aixin Sun, Yin Wu |  |
| 722 |  |  [Towards Diverse and Effective Question-Answer Pair Generation from Children Storybooks](https://doi.org/10.18653/v1/2023.findings-acl.380) |  | 0 | Recent advances in QA pair generation (QAG) have raised interest in applying this technique to the educational field. However, the diversity of QA types remains a challenge despite its contributions to comprehensive learning and assessment of children. In this paper, we propose a QAG framework that... | Changwoo Chun, Heuiseok Lim, Hyeonseok Moon, Jeongwook Kim, Jinsung Kim, Songeun Lee, Sugyeong Eo, Sungsoo Park, Yuna Hur |  |
| 723 |  |  [Pulling Out All The Full Stops: Punctuation Sensitivity in Neural Machine Translation and Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.381) |  | 0 | Much of the work testing machine translation systems for robustness and sensitivity has been adversarial or tended towards testing noisy input such as spelling errors, or non-standard input such as dialects. In this work, we take a step back to investigate a sensitivity problem that can seem... | Prathyusha Jwalapuram |  |
| 724 |  |  [Reimagining Retrieval Augmented Language Models for Answering Queries](https://doi.org/10.18653/v1/2023.findings-acl.382) |  | 0 | We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the... | Alon Y. Halevy, Pedro Rodriguez, Richard James, WangChiew Tan, Wentau Yih, Xi Victoria Lin, Yuliang Li |  |
| 725 |  |  [Numeric Magnitude Comparison Effects in Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.383) |  | 0 | Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g.,... | Khushi Bhardwaj, Raj Sanjay Shah, Reba Koenen, Sashank Varma, Vijay Marupudi |  |
| 726 |  |  [Multi-Relational Probabilistic Event Representation Learning via Projected Gaussian Embedding](https://doi.org/10.18653/v1/2023.findings-acl.384) |  | 0 | Event representation learning has been shown beneficial in various downstream tasks. Current event representation learning methods, which mainly focus on capturing the semantics of events via deterministic vector embeddings, have made notable progress. However, they ignore two important properties:... | Congzhi Zhang, Deyu Zhou, Linhai Zhang |  |
| 727 |  |  [PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.385) |  | 0 | Pragmatic reasoning about another speaker’s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks “do you have a minute?”, instead of interpreting it literally as a query... | Christopher D. Manning, Jing Huang, Nina Du, Peng Qi |  |
| 728 |  |  [Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks](https://doi.org/10.18653/v1/2023.findings-acl.386) |  | 0 | Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to... | Deepak Kumar, Lukas Hauzenberger, Markus Schedl, Navid Rekabsaz, Shahed Masoudian |  |
| 729 |  |  [Scientific Fact-Checking: A Survey of Resources and Approaches](https://doi.org/10.18653/v1/2023.findings-acl.387) |  | 0 | The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant... | Florian Matthes, Juraj Vladika |  |
| 730 |  |  [Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-acl.388) |  | 0 | Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called... | Chiyu Song, Haofei Yu, Hongliang He, Leyang Cui, Pengfei Fang, Zhenzhong Lan |  |
| 731 |  |  [DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.389) |  | 0 | A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these... | Amr Keleg, Walid Magdy |  |
| 732 |  |  [Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.390) |  | 0 | The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges,... | Feijun Jiang, Haozhe Yang, Jianlong Wu, Liqiang Nie, Ning Ding, Tian Gan, Xianqiang Gao |  |
| 733 |  |  [Structure-Discourse Hierarchical Graph for Conditional Question Answering on Long Documents](https://doi.org/10.18653/v1/2023.findings-acl.391) |  | 0 | Conditional question answering on long documents aims to find probable answers and identify conditions that need to be satisfied to make the answers correct over long documents. Existing approaches solve this task by segmenting long documents into multiple sections, and attending information at... | Chen Li, Dongyan Zhao, Haowei Du, Yang Li, Yansong Feng, Yunshi Lan |  |
| 734 |  |  [COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements](https://doi.org/10.18653/v1/2023.findings-acl.392) |  | 0 | Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance “your English is very good” may implicitly signal an... | Akhila Yerukola, Hao Zhu, Jena D. Hwang, Maarten Sap, Swabha Swayamdipta, Thomas Davidson, Xuhui Zhou |  |
| 735 |  |  [Distilling Calibrated Knowledge for Stance Detection](https://doi.org/10.18653/v1/2023.findings-acl.393) |  | 0 | Stance detection aims to determine the position of an author toward a target and provides insights into people’s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which... | Cornelia Caragea, Yingjie Li |  |
| 736 |  |  [PTCSpell: Pre-trained Corrector Based on Character Shape and Pinyin for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.findings-acl.394) |  | 0 | Chinese spelling correction (CSC) is a challenging task with the goal of correcting each wrong character in Chinese texts. Incorrect characters in a Chinese text are mainly due to the similar shape and similar pronunciation of Chinese characters. Recently, the paradigm of pre-training and... | Hang Yu, Jianbao Huang, Qian Liu, Xiao Wei |  |
| 737 |  |  [Disentangling Text Representation With Counter-Template For Unsupervised Opinion Summarization](https://doi.org/10.18653/v1/2023.findings-acl.395) |  | 0 | Approaches for unsupervised opinion summarization are generally based on the reconstruction model and generate a summary by decoding the aggregated representation of inputs. Recent work has shown that aggregating via simple average leads to vector degeneration, generating the generic summary. To... | Deyu Zhou, Yanyue Zhang |  |
| 738 |  |  [Evaluation of Question Generation Needs More References](https://doi.org/10.18653/v1/2023.findings-acl.396) |  | 0 | Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different... | Hyeongdon Moon, Hyojun Go, Hyun Seung Lee, Myeongho Jeong, Seungtaek Choi, Shinhyeok Oh, Yunsung Lee |  |
| 739 |  |  [XtremeCLIP: Extremely Parameter-efficient Tuning for Low-resource Vision Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.397) |  | 0 | Recently, Contrastive Visual-Language Pre-training (CLIP) has demonstrated remarkable capability in various Visual Language Understanding (VLU) tasks. Yet, most CLIP-based methods require tasks-specific designs and sufficient training data. In this paper, we introduce a simple yet efficient... | Cen Chen, Chengyu Wang, Chuanqi Tan, Jianing Wang, Moming Tang, Songfang Huang, Weining Qian |  |
| 740 |  |  [FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing](https://doi.org/10.18653/v1/2023.findings-acl.398) |  | 0 | Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the... | Donghong Ji, Fei Li, Gholamreza Haffari, Lizhen Qu, Quan Hung Tran, Terry Yue Zhuo, Yuyang Chai, Zhuang Li |  |
| 741 |  |  [Target-Oriented Relation Alignment for Cross-Lingual Stance Detection](https://doi.org/10.18653/v1/2023.findings-acl.399) |  | 0 | Stance detection is an important task in text mining and social media analytics, aiming to automatically identify the user’s attitude toward a specific target from text, and has wide applications in a variety of domains. Previous work on stance detection has mainly focused on monolingual setting.... | Hanxuan Yang, Nan Xu, Ruike Zhang, Wenji Mao, Yuan Tian |  |
| 742 |  |  [NonFactS: NonFactual Summary Generation for Factuality Evaluation in Document Summarization](https://doi.org/10.18653/v1/2023.findings-acl.400) |  | 0 | Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document... | Amir Soleimani, Christof Monz, Marcel Worring |  |
| 743 |  |  [When to Read Documents or QA History: On Unified and Selective Open-domain QA](https://doi.org/10.18653/v1/2023.findings-acl.401) |  | 0 | This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly... | Kyungjae Lee, Moontae Lee, Sangeun Han, Seungwon Hwang |  |
| 744 |  |  [Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization](https://doi.org/10.18653/v1/2023.findings-acl.402) |  | 0 | Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of... | Heng Ji, Hou Pong Chan, Qi Zeng |  |
| 745 |  |  [A Multi-dimensional study on Bias in Vision-Language models](https://doi.org/10.18653/v1/2023.findings-acl.403) |  | 0 | In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities. This paper presents the first multi-dimensional analysis of bias in... | Debora Nozza, Gabriele Ruggeri |  |
| 746 |  |  [Correction of Errors in Preference Ratings from Automated Metrics for Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.404) |  | 0 | A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreements with human judgments. In this paper, we propose to apply automated metrics for Text Generation in a preference-based evaluation... | Don Tuggener, Jan Deriu, Mark Cieliebak, Pius von Däniken |  |
| 747 |  |  [PEER: Pre-training ELECTRA Extended by Ranking](https://doi.org/10.18653/v1/2023.findings-acl.405) |  | 0 | The BERT model and its variants have made great achievements in many downstream natural language processing tasks. The achievements of these models, however, demand highly expensive pre-training computation cost. To address this pre-training efficiency issue, the ELECTRA model is proposed to use a... | Fei Huang, Ru He, Songfang Huang, Wei Wang |  |
| 748 |  |  [ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.406) |  | 0 | Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this... | Bowen Cao, Hongxiang Li, Qichen Ye, Xuxin Cheng, Yuexian Zou, Zhihong Zhu |  |
| 749 |  |  [Guiding Dialogue Agents to Complex Semantic Targets by Dynamically Completing Knowledge Graph](https://doi.org/10.18653/v1/2023.findings-acl.407) |  | 0 | In the target-oriented dialogue, the representation and achievement of targets are two interrelated essential issues. In current approaches, the target is typically supposed to be a single object represented as a word, which makes it relatively easy to achieve the target through dialogue with the... | Anqi Liu, Bo Wang, Dongming Zhao, Kun Huang, Ruifang He, Yue Tan, Yuexian Hou |  |
| 750 |  |  [Chain of Thought Prompting Elicits Knowledge Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.408) |  | 0 | The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are... | Dingjun Wu, Jing Zhang, Xinmei Huang |  |
| 751 |  |  [TACR: A Table Alignment-based Cell Selection Method for HybridQA](https://doi.org/10.18653/v1/2023.findings-acl.409) |  | 0 | Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns,... | Börje Karlsson, Jian Wu, JianGuang Lou, Manabu Okumura, Yan Gao, Yicheng Xu |  |
| 752 |  |  [Modeling Cross-Cultural Pragmatic Inference with Codenames Duet](https://doi.org/10.18653/v1/2023.findings-acl.410) |  | 0 | Pragmatic reference enables efficient interpersonal communication. Prior work uses simple reference games to test models of pragmatic reasoning, often with unidentified speakers and listeners. In practice, however, speakers’ sociocultural background shapes their pragmatic assumptions. For example,... | Aryan J. Pariani, Caleb Ziems, Diyi Yang, Fred Morstatter, Omar Shaikh, William Held |  |
| 753 |  |  [Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games](https://doi.org/10.18653/v1/2023.findings-acl.411) |  | 0 | Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first... | Aryan J. Pariani, Bolin Lai, Diyi Yang, Fiona Ryan, Hongxin Zhang, James M. Rehg, Miao Liu, Shirley Anugrah Hayati, Wenqi Jia |  |
| 754 |  |  [Long to reign over us: A Case Study of Machine Translation and a New Monarch](https://doi.org/10.18653/v1/2023.findings-acl.412) |  | 0 | Novel terminology and changes in terminology are often a challenge for machine translation systems. The passing of Queen Elizabeth II and the accession of King Charles III provide a striking example of translation shift in the real world, particularly in translation contexts that have ambiguity.... | Rebecca Knowles, Samuel Larkin |  |
| 755 |  |  [A Unified Generative Approach to Product Attribute-Value Identification](https://doi.org/10.18653/v1/2023.findings-acl.413) |  | 0 | Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., ⟨Material, Cotton⟩) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values,... | Keiji Shinzato, Naoki Yoshinaga, WeiTe Chen, Yandi Xia |  |
| 756 |  |  [K-UniMorph: Korean Universal Morphology and its Feature Schema](https://doi.org/10.18653/v1/2023.findings-acl.414) |  | 0 | We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that... | Chulwoo Park, Eunkyul Leah Jo, Jungyeul Park, Kyungtae Lim, Kyuwon Kim, Xihan Wu |  |
| 757 |  |  [How does the brain process syntactic structure while listening?](https://doi.org/10.18653/v1/2023.findings-acl.415) |  | 0 | Syntactic parsing is the task of assigning a syntactic structure to a sentence. There are two popular syntactic parsing methods: constituency and dependency parsing. Recent works have used syntactic embeddings based on constituency trees, incremental top-down parsing, and other word syntactic... | Manish Gupta, Mounika Marreddy, Raju S. Bapi, Subba Reddy Oota |  |
| 758 |  |  [Towards Imperceptible Document Manipulations against Neural Ranking Models](https://doi.org/10.18653/v1/2023.findings-acl.416) |  | 0 | Adversarial attacks have gained traction in order to identify vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce noticeable errors. Moreover, current methods rely heavily on using a well-imitated surrogate NRM to guarantee the attack effect, making them... | Ben He, Le Sun, Xuanang Chen, Yingfei Sun, Zheng Ye |  |
| 759 |  |  [Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models](https://doi.org/10.18653/v1/2023.findings-acl.417) |  | 0 | Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the “Ask an Expert” framework in which the model is trained with access to an “expert” which it can... | Jason Naradowsky, Qiang Zhang, Yusuke Miyao |  |
| 760 |  |  [SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation](https://doi.org/10.18653/v1/2023.findings-acl.418) |  | 0 | Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting... | Ichiro Sakata, Junichiro Mori, Masaru Isonuma, Tetsu Kasanishi |  |
| 761 |  |  [Revisiting Sample Size Determination in Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.419) |  | 0 | Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation. It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource... | Changsheng Zhao, Ernie Chang, Muhammad Hassan Rashid, PinJie Lin, Vera Demberg, Vikas Chandra, Yangyang Shi |  |
| 762 |  |  [TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition](https://doi.org/10.18653/v1/2023.findings-acl.420) |  | 0 | Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue... | Bing Qin, Shilong Wang, Weixiang Zhao, Yanyan Zhao |  |
| 763 |  |  [Residual Prompt Tuning: improving prompt tuning with residual reparameterization](https://doi.org/10.18653/v1/2023.findings-acl.421) |  | 0 | Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is... | Amjad Almahairi, Anastasia Razdaibiedina, Jimmy Ba, Madian Khabsa, Mike Lewis, Rui Hou, Yuning Mao |  |
| 764 |  |  [Attend, Select and Eliminate: Accelerating Multi-turn Response Selection with Dual-attention-based Content Elimination](https://doi.org/10.18653/v1/2023.findings-acl.422) |  | 0 | Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs. To alleviate this problem and make the PLM-based response selection model both effective and efficient, we... | Chang Liu, Chongyang Tao, Dongyan Zhao, Jianxin Liang, Jiazhan Feng |  |
| 765 |  |  [Medical Dialogue Generation via Dual Flow Modeling](https://doi.org/10.18653/v1/2023.findings-acl.423) |  | 0 | Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical... | Jian Wang, Kaishuai Xu, Wenjie Li, Wenjun Hou, Yi Cheng |  |
| 766 |  |  [Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition](https://doi.org/10.18653/v1/2023.findings-acl.424) |  | 0 | Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We... | Chang Dong Yoo, Heting Gao, Jialu Li, Junkai Wu, Junrui Ni, Kai Chieh Chang, Liming Wang, Mark HasegawaJohnson, Xulin Fan |  |
| 767 |  |  [Distinguishing Address vs. Reference Mentions of Personal Names in Text](https://doi.org/10.18653/v1/2023.findings-acl.425) |  | 0 | Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off. While this distinction is... | Aida Mostafazadeh Davani, Melissa Ferguson, Stav Atir, Vinodkumar Prabhakaran |  |
| 768 |  |  ["Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors](https://doi.org/10.18653/v1/2023.findings-acl.426) |  | 0 | Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to... | Jimmy Lin, Matthew Y. R. Yang, Mikhail Tsirlin, Raphael Tang, Yiqin Dai, Zhiying Jiang |  |
| 769 |  |  [LR-Sum: Summarization for Less-Resourced Languages](https://doi.org/10.18653/v1/2023.findings-acl.427) |  | 0 | We introduce LR-Sum, a new permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages.LR-Sum contains human-written summaries for 40 languages, many of which are less-resourced. We describe our process for extracting and... | Chester PalenMichel, Constantine Lignos |  |
| 770 |  |  [RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question](https://doi.org/10.18653/v1/2023.findings-acl.428) |  | 0 | Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the... | Alireza Mohammadshahi, Angela Fan, James Henderson, Majid Yazdani, Marzieh Saeidi, Pouya Yanki, Thomas Scialom |  |
| 771 |  |  [Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.429) |  | 0 | Languages are dynamic entities, where the meanings associated with words constantly change with time. Detecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions. Existing work on semantic variation prediction have... | Danushka Bollegala, Taichi Aida |  |
| 772 |  |  [TranSFormer: Slow-Fast Transformer for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.430) |  | 0 | Learning multiscale Transformer models has been evidenced as a viable approach to augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into... | Bei Li, Jingbo Zhu, Tong Xiao, Xu Tan, Yi Jing, Zhen Xing |  |
| 773 |  |  [Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation](https://doi.org/10.18653/v1/2023.findings-acl.431) |  | 0 | Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the... | Jian Guan, Minlie Huang |  |
| 774 |  |  [Digging out Discrimination Information from Generated Samples for Robust Visual Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.432) |  | 0 | Visual Question Answering (VQA) aims to answer a textual question based on a given image. Nevertheless, recent studies have shown that VQA models tend to capture the biases to answer the question, instead of using the reasoning ability, resulting in poor generalisation ability. To alleviate the... | Mingkui Tan, Qi Wu, Qingyao Wu, Yaowei Wang, Zhiquan Wen |  |
| 775 |  |  [Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications](https://doi.org/10.18653/v1/2023.findings-acl.433) |  | 0 | Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups. In this work, we develop and validate an interpretable approach for measuring scholarly jargon from text. Expanding the... | David Bamman, Jesse Dodge, Katherine A. Keith, Li Lucy |  |
| 776 |  |  [Trade-Offs Between Fairness and Privacy in Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.434) |  | 0 | Protecting privacy in contemporary NLP models is gaining in importance. So does the need to mitigate social biases of such models. But can we have both at the same time? Existing research suggests that privacy preservation comes at the price of worsening biases in classification tasks. In this... | Cleo Matzken, Ivan Habernal, Steffen Eger |  |
| 777 |  |  [CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset](https://doi.org/10.18653/v1/2023.findings-acl.435) |  | 0 | The cross-domain text-to-SQL task aims to build a system that can parse user questions into SQL on complete unseen databases, and the single-domain text-to-SQL task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications.... | Hanchong Zhang, Jieyu Li, Kai Yu, Lu Chen, Ruisheng Cao, Yefeng Zheng, Yu Huang, Yunyan Zhang |  |
| 778 |  |  [Silver Syntax Pre-training for Cross-Domain Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.436) |  | 0 | Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot... | Barbara Plank, Elisa Bassignana, Filip Ginter, Rob van der Goot, Sampo Pyysalo |  |
| 779 |  |  [FastDiff 2: Revisiting and Incorporating GANs and Diffusion Models in High-Fidelity Speech Synthesis](https://doi.org/10.18653/v1/2023.findings-acl.437) |  | 0 | Generative adversarial networks (GANs) and denoising diffusion probabilistic models (DDPMs) have recently achieved impressive performances in image and audio synthesis. After revisiting their success in conditional speech synthesis, we find that 1) GANs sacrifice sample diversity for quality and... | Chenye Cui, Jinglin Liu, Rongjie Huang, Yi Ren, Zhou Zhao, Ziyue Jiang |  |
| 780 |  |  [Uncovering Hidden Consequences of Pre-training Objectives in Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2023.findings-acl.438) |  | 0 | Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance. Yet the design of these pre-training objectives leads to behavioural differences that can be uncovered with specific... | Rico Sennrich, Tannon Kew |  |
| 781 |  |  [Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity](https://doi.org/10.18653/v1/2023.findings-acl.439) |  | 0 | Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the... | Alexander Fraser, Alina Fastowski, Jindrich Libovický, Katharina Hämmerl |  |
| 782 |  |  [Revisiting Sentence Union Generation as a Testbed for Text Consolidation](https://doi.org/10.18653/v1/2023.findings-acl.440) |  | 0 | Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information. However, these tasks... | Asi Shefer, Avi Caciularu, Eran Hirsch, Ido Dagan, Ruben Wolhandler, Valentina Pyatkin |  |
| 783 |  |  [Distilling Reasoning Capabilities into Smaller Language Models](https://doi.org/10.18653/v1/2023.findings-acl.441) |  | 0 | Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get... | Alessandro Stolfo, Kumar Shridhar, Mrinmaya Sachan |  |
| 784 |  |  [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment](https://doi.org/10.18653/v1/2023.findings-acl.442) |  | 0 | The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper... | Jinglin Liu, Lichao Zhang, Rongjie Huang, Ruiqi Li, Zhou Zhao |  |
| 785 |  |  [A New Task and Dataset on Detecting Attacks on Human Rights Defenders](https://doi.org/10.18653/v1/2023.findings-acl.443) |  | 0 | The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that... | Alejandro Jaimes, Aoife Cahill, Di Lu, Joel R. Tetreault, Shihao Ran |  |
| 786 |  |  [Improving Language Model Integration for Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.444) |  | 0 | The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that... | Christian Herold, Hermann Ney, Mohammad Zeineldeen, Yingbo Gao |  |
| 787 |  |  [Type Enhanced BERT for Correcting NER Errors](https://doi.org/10.18653/v1/2023.findings-acl.445) |  | 0 | We introduce the task of correcting named entity recognition (NER) errors without re-training model. After an NER model is trained and deployed in production,it makes prediction errors, which usually need to be fixed quickly. To address this problem, we firstly construct a gazetteer containing... | Chen Chen, Dong Du, Feng Zhang, Kuai Li, Peijie Yu, Tao Yang, Tianming Du |  |
| 788 |  |  [Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework](https://doi.org/10.18653/v1/2023.findings-acl.446) |  | 0 | Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural... | Lifan Yuan, Wei Wei, Yangyi Chen, Yichi Zhang |  |
| 789 |  |  [DUB: Discrete Unit Back-translation for Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.447) |  | 0 | How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way... | Dong Zhang, Mingxuan Wang, Rong Ye, Tom Ko, Yaqian Zhou |  |
| 790 |  |  [Knowledge Graph Embeddings using Neural Ito Process: From Multiple Walks to Stochastic Trajectories](https://doi.org/10.18653/v1/2023.findings-acl.448) |  | 0 | Knowledge graphs mostly exhibit a mixture of branching relations, e.g., hasFriend, and complex structures, e.g., hierarchy and loop. Most knowledge graph embeddings have problems expressing them, because they model a specific relation r from a head h to tails by starting at the node embedding of h... | Bo Xiong, Jens Lehmann, Majid Mohammadi, Mirza Mohtashim Alam, Mojtaba Nayyeri, Mst. Mahfuja Akter, Steffen Staab |  |
| 791 |  |  [Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction](https://doi.org/10.18653/v1/2023.findings-acl.449) |  | 0 | Grammatical Error Correction (GEC) is the task of correcting errorful sentences into grammatically correct, semantically consistent, and coherent sentences. Popular GEC models either use large-scale synthetic corpora or use a large number of human-designed rules. The former is costly to train,... | Dongyan Zhao, Hejing Cao |  |
| 792 |  |  [Prediction and Calibration: Complex Reasoning over Knowledge Graph with Bi-directional Directed Acyclic Graph Neural Network](https://doi.org/10.18653/v1/2023.findings-acl.450) |  | 0 | Answering complex logical queries is a challenging task for knowledge graph (KG) reasoning. Recently, query embedding (QE) has been proposed to encode queries and entities into the same vector space, and obtain answers based on numerical computation. However, such models obtain the node... | Jun Zhao, Kang Liu, Li Cai, Shizhu He, Yao Xu |  |
| 793 |  |  [Prompt-Based Metric Learning for Few-Shot NER](https://doi.org/10.18653/v1/2023.findings-acl.451) |  | 0 | Few-shot named entity recognition (NER) targets generalizing to unseen labels and/or domains with few labeled examples. Existing metric learning methods compute token-level similarities between query and support sets, but are not able to fully incorporate label semantics into modeling. To address... | Yanan Zheng, Yanru Chen, Zhilin Yang |  |
| 794 |  |  [OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking](https://doi.org/10.18653/v1/2023.findings-acl.452) |  | 0 | Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking.... | Heng Ji, Sha Li, Xueqing Wu |  |
| 795 |  |  [I run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets](https://doi.org/10.18653/v1/2023.findings-acl.453) |  | 0 | A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as “like” or “as”. The simile phenomena are unique and complex in a real-life dialogue scene where... | Changxin Ke, Churui Sun, Longxuan Ma, Shuhan Zhou, Ting Liu, Weinan Zhang |  |
| 796 |  |  [Controllable Conversation Generation with Conversation Structures via Diffusion Models](https://doi.org/10.18653/v1/2023.findings-acl.454) |  | 0 | Generating coherent conversation is an important and challenging long text generation task, as it has various applications such as daily entertainment, children education or building conversational AI to facilitate human-computer interaction. However, current generation models often fail to... | Diyi Yang, Jiaao Chen |  |
| 797 |  |  [Few-shot Low-resource Knowledge Graph Completion with Reinforced Task Generation](https://doi.org/10.18653/v1/2023.findings-acl.455) |  | 0 | Despite becoming a prevailing paradigm for organizing knowledge, most knowledge graphs (KGs) suffer from the low-resource issue due to the deficiency of data sources. The enrichment of KGs by automatic knowledge graph completion is impeded by the intrinsic long-tail property of KGs. In spite of... | Qiannan Zhang, Shichao Pei, Xiangliang Zhang |  |
| 798 |  |  [Incomplete Utterance Rewriting as Sequential Greedy Tagging](https://doi.org/10.18653/v1/2023.findings-acl.456) |  | 0 | The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at... | Yunshan Chen |  |
| 799 |  |  [Exploiting Commonsense Knowledge about Objects for Visual Activity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.457) |  | 0 | Situation recognition is the task of recognizing the activity depictedin an image, including the people and objects involved. Previousmodels for this task typically train a classifier to identify theactivity using a backbone image feature extractor. We propose thatcommonsense knowledge about the... | Ellen Riloff, Tianyu Jiang |  |
| 800 |  |  [Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.458) |  | 0 | Temporal Knowledge Graph Completion aims to complete missing entities or relations under temporal constraints. Previous tensor decomposition-based models for TKGC only independently consider the combination of one single relation with one single timestamp, ignoring the global nature of the... | Junfan Chen, Likang Xiao, Richong Zhang, Zijie Chen |  |
| 801 |  |  [Another Dead End for Morphological Tags? Perturbed Inputs and Parsing](https://doi.org/10.18653/v1/2023.findings-acl.459) |  | 0 | The usefulness of part-of-speech tags for parsing has been heavily questioned due to the success of word-contextualized parsers. Yet, most studies are limited to coarse-grained tags and high quality written content; while we know little about their influence when it comes to models in production... | Alberto MuñozOrtiz, David Vilares |  |
| 802 |  |  [HeGeL: A Novel Dataset for Geo-Location from Hebrew Text](https://doi.org/10.18653/v1/2023.findings-acl.460) |  | 0 | The task of textual geolocation — retrieving the coordinates of a place based on a free-form language description — calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are... | Itai Mondshine, Itzhak Omer, Reut Tsarfaty, Sagi Dalyot, Tal Bauman, Tzuf PazArgaman |  |
| 803 |  |  [Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making](https://doi.org/10.18653/v1/2023.findings-acl.461) |  | 0 | Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent... | Sijie Cheng, Wei Wang, Xuanjie Fang, Yang Liu |  |
| 804 |  |  [Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization](https://doi.org/10.18653/v1/2023.findings-acl.462) |  | 0 | Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target... | Hongru Wang, KamFai Wong, Liang Chen, WaiChung Kwan, Yang Deng, Zezhong Wang |  |
| 805 |  |  [Cost-effective Distillation of Large Language Models](https://doi.org/10.18653/v1/2023.findings-acl.463) |  | 0 | Knowledge distillation (KD) involves training a small “student” model to replicate the strong performance of a high-capacity “teacher” model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability.... | Sayantan Dasgupta, Timothy Baldwin, Trevor Cohn |  |
| 806 |  |  [Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.464) |  | 0 | Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the... | Jeehyun Lee, MyoungWan Koo, Namo Bang |  |
| 807 |  |  [I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors](https://doi.org/10.18653/v1/2023.findings-acl.465) |  | 0 | Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic... | Arkadiy Saakyan, Artemis Panagopoulou, Marianna Apidianaki, Olivia Winn, Smaranda Muresan, Tuhin Chakrabarty, Yue Yang |  |
| 808 |  |  [Text Augmentation Using Dataset Reconstruction for Low-Resource Classification](https://doi.org/10.18653/v1/2023.findings-acl.466) |  | 0 | In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves... | Adir Rahamim, Ateret AnabyTavor, Esther Goldbraich, Guy Uziel |  |
| 809 |  |  [LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning](https://doi.org/10.18653/v1/2023.findings-acl.467) |  | 0 | A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as ‘always’ or... | Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 810 |  |  [Learned Adapters Are Better Than Manually Designed Adapters](https://doi.org/10.18653/v1/2023.findings-acl.468) |  | 0 | Recently, a series of works have looked into further improving the adapter-based tuning by manually designing better adapter architectures. Understandably, these manually designed solutions are sub-optimal. In this work, we propose the Learned Adapter framework to automatically learn the optimal... | Ming Tan, Peng Wang, Wei Zhu, Yuming Zhang |  |
| 811 |  |  [Automatic Identification of Code-Switching Functions in Speech Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.469) |  | 0 | Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications. Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new... | Jeffrey Flanigan, Ritu Belani |  |
| 812 |  |  [Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets](https://doi.org/10.18653/v1/2023.findings-acl.470) |  | 0 | Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of... | Handong Zhao, Junda Wu, Ricardo Henao, Rui Wang, Ruiyi Zhang, Subrata Mitra, Sungchul Kim, Tong Yu |  |
| 813 |  |  [Interpreting Sentiment Composition with Latent Semantic Tree](https://doi.org/10.18653/v1/2023.findings-acl.471) |  | 0 | As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and... | Cao Liu, Jiansong Chen, Jun Zhao, Kang Liu, Yuanzhe Zhang, Zhongtao Jiang |  |
| 814 |  |  [Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.472) |  | 0 | Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive... | James Zou, Jeff Z. HaoChen, Michihiro Yasunaga, Percy Liang, Serena Yeung, Yuhui Zhang, Zhengping Zhou |  |
| 815 |  |  [Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents](https://doi.org/10.18653/v1/2023.findings-acl.473) |  | 0 | We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard... | Graham Horwood, Miguel Ballesteros, Muhammad Khalifa, Shuai Wang, Sunil Mallya, Yogarshi Vyas |  |
| 816 |  |  [Extracting Shopping Interest-Related Product Types from the Web](https://doi.org/10.18653/v1/2023.findings-acl.474) |  | 0 | Recommending a diversity of product types (PTs) is important for a good shopping experience when customers are looking for products around their high-level shopping interests (SIs) such as hiking. However, the SI-PT connection is typically absent in e-commerce product catalogs and expensive to... | Chao Zhang, Colin Lockard, Prashant Shiralkar, Yinghao Li |  |
| 817 |  |  [Multilingual Pre-training with Self-supervision from Global Co-occurrence Information](https://doi.org/10.18653/v1/2023.findings-acl.475) |  | 0 | Global co-occurrence information is the primary source of structural information on multilingual corpora, and we find that analogical/parallel compound words across languages have similar co-occurrence counts/frequencies (normalized) giving weak but stable self-supervision for cross-lingual... | Bin Fang, Xi Ai |  |
| 818 |  |  [Low-Rank Updates of pre-trained Weights for Multi-Task Learning](https://doi.org/10.18653/v1/2023.findings-acl.476) |  | 0 | Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we... | Alexandre Audibert, Konstantin Usevich, Marianne Clausel, MassihReza Amini |  |
| 819 |  |  [Sequential Integrated Gradients: a simple but effective method for explaining language models](https://doi.org/10.18653/v1/2023.findings-acl.477) |  | 0 | Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence... | Joseph Enguehard |  |
| 820 |  |  [DiffuDetox: A Mixed Diffusion Model for Text Detoxification](https://doi.org/10.18653/v1/2023.findings-acl.478) |  | 0 | Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their... | Ali Pesaranghader, Griffin Floto, Manasa Bharadwaj, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Scott Sanner, Zhenwei Tang |  |
| 821 |  |  [Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.479) |  | 0 | Extractive summarization aims to select a set of salient sentences from the source document to form a summary. Context information has been considered one of the key factors for this task. Meanwhile, there also exist other pattern factors that can identify sentence importance, such as sentence... | Ruifeng Yuan, Shichao Sun, Wenjie Li, Zili Wang, Ziqiang Cao |  |
| 822 |  |  [Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers](https://doi.org/10.18653/v1/2023.findings-acl.480) |  | 0 | This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled... | ChinYew Lin, Jiahai Wang, Jian Yin, Nan Duan, Tiejun Zhao, Tingting Ma, Wanjun Zhong |  |
| 823 |  |  [Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking](https://doi.org/10.18653/v1/2023.findings-acl.481) |  | 0 | We propose a simple approach for the abstractive summarization of long legal opinions that takes into account the argument structure of the document. Legal opinions often contain complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main... | Diane J. Litman, Mohamed Elaraby, Yang Zhong |  |
| 824 |  |  [Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation](https://doi.org/10.18653/v1/2023.findings-acl.482) |  | 0 | Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers.... | Haoyi Wu, Kewei Tu |  |
| 825 |  |  [Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data](https://doi.org/10.18653/v1/2023.findings-acl.483) |  | 0 | Self-training has been shown to be helpful in addressing data scarcity for many domains, including vision, speech, and language. Specifically, self-training, or pseudo-labeling, labels unsupervised data and adds that to the training pool. In this work, we investigate and use pseudo-labeling for a... | Hendra Setiawan, Matthias Sperber, Mozhdeh Gheini, Tatiana Likhomanenko |  |
| 826 |  |  [Word-level Prefix/Suffix Sense Detection: A Case Study on Negation Sense with Few-shot Learning](https://doi.org/10.18653/v1/2023.findings-acl.484) |  | 0 | Morphological analysis is an important research issue in the field of natural language processing. In this study, we propose a context-free morphological analysis task, namely word-level prefix/suffix sense detection, which deals with the ambiguity of sense expressed by prefix/suffix. To research... | Shoushan Li, Yameng Li, Ying Chen, Zicheng Li |  |
| 827 |  |  [End-to-End Simultaneous Speech Translation with Differentiable Segmentation](https://doi.org/10.18653/v1/2023.findings-acl.485) |  | 0 | End-to-end simultaneous speech translation (SimulST) outputs translation while receiving the streaming speech inputs (a.k.a. streaming speech translation), and hence needs to segment the speech inputs and then translate based on the current received speech. However, segmenting the speech inputs at... | Shaolei Zhang, Yang Feng |  |
| 828 |  |  [Joint Generator-Ranker Learning for Natural Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.486) |  | 0 | Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that... | Nan Duan, Song Wang, Weizhou Shen, Weizhu Chen, Xiaojun Quan, Yelong Shen, Yeyun Gong |  |
| 829 |  |  [Multilingual Sequence-to-Sequence Models for Hebrew NLP](https://doi.org/10.18653/v1/2023.findings-acl.487) |  | 0 | Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous... | Hila Noga, Idan Szpektor, Matan Eyal, Reut Tsarfaty, Roee Aharoni |  |
| 830 |  |  [Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints](https://doi.org/10.18653/v1/2023.findings-acl.488) |  | 0 | Multilingual Knowledge Graph Completion (mKGC) aim at solving queries in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although... | Jun Zhao, Kang Liu, Li Cai, Ran Song, Shengxiang Gao, Shizhu He, Zhengtao Yu |  |
| 831 |  |  [Towards Better Hierarchical Text Classification with Data Generation](https://doi.org/10.18653/v1/2023.findings-acl.489) |  | 0 | Hierarchical text classification (HTC) focuses on classifying one text into multiple labels, which are organized as a hierarchical taxonomy. Due to its wide involution in realistic scenarios, HTC attracts long-term attention from both industry and academia. However, the high cost of hierarchical... | Dan Qiao, Guannan Zhang, Jinxiong Chang, Juntao Li, Min Zhang, Qishen Zhang, Yue Wang, Zhongyi Liu |  |
| 832 |  |  [History repeats: Overcoming catastrophic forgetting for event-centric temporal knowledge graph completion](https://doi.org/10.18653/v1/2023.findings-acl.490) |  | 0 | Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could... | Aram Galstyan, Mehrnoosh Mirtaheri, Mohammad Rostami |  |
| 833 |  |  [Multi-Agent Language Learning: Symbolic Mapping](https://doi.org/10.18653/v1/2023.findings-acl.491) |  | 0 | The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a ‘natural’ way to help agents learn a compositional and symmetric language in complex settings like dialog... | Yicheng Feng, Zongqing Lu |  |
| 834 |  |  [Scaling Laws for BERT in Low-Resource Settings](https://doi.org/10.18653/v1/2023.findings-acl.492) |  | 0 | Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters,... | Aitor Soroa, Gorka Urbizu, Iñaki San Vicente, Rodrigo Agerri, Xabier Saralegi |  |
| 835 |  |  [Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.findings-acl.493) |  | 0 | Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while... | Ben Liu, Miao Peng, Min Peng, Wenjie Xu, Xu Jia |  |
| 836 |  |  [Is Continuous Prompt a Combination of Discrete Prompts? Towards a Novel View for Interpreting Continuous Prompts](https://doi.org/10.18653/v1/2023.findings-acl.494) |  | 0 | The broad adoption of continuous prompts has brought state-of-the-art results on a diverse array of downstream natural language processing (NLP) tasks. Nonetheless, little attention has been paid to the interpretability and transferability of continuous prompts. Faced with the challenges, we... | Gongshen Liu, Hanyi Wang, Haodong Zhao, Tianjie Ju, Yubin Zheng |  |
| 837 |  |  [Putting Natural in Natural Language Processing](https://doi.org/10.18653/v1/2023.findings-acl.495) |  | 0 | Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on... | Grzegorz Chrupala |  |
| 838 |  |  [Impact of Adversarial Training on Robustness and Generalizability of Language Models](https://doi.org/10.18653/v1/2023.findings-acl.496) |  | 0 | Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth... | Enes Altinisik, Hassan Sajjad, Husrev T. Sencar, Safa Messaoud, Sanjay Chawla |  |
| 839 |  |  [Benchmarking Diverse-Modal Entity Linking with Generative Models](https://doi.org/10.18653/v1/2023.findings-acl.497) |  | 0 | Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified... | Alexander Hanbo Li, Bing Xiang, ChungWei Hang, Henghui Zhu, Jie Ma, Patrick Ng, Pramuditha Perera, Sheng Zhang, Sijia Wang, Vittorio Castelli, William Yang Wang, Zhiguo Wang |  |
| 840 |  |  [Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge](https://doi.org/10.18653/v1/2023.findings-acl.498) |  | 0 | In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker’s emotion. Besides, external commonsense knowledge has been applied to enhance the system’s understandings of the speaker’s... | Hua Cai, Qing Xu, Weifeng Ge, Weilin Shen, Xiangyang Xue, Xiaomei Wang, Xiaoqing Zheng, Xuli Shen |  |
| 841 |  |  [Additive manifesto decomposition: A policy domain aware method for understanding party positioning](https://doi.org/10.18653/v1/2023.findings-acl.499) |  | 0 | Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense... | Dmitry Nikolaev, Sebastian Padó, Tanise Ceron |  |
| 842 |  |  [Similarizing the Influence of Words with Contrastive Learning to Defend Word-level Adversarial Text Attack](https://doi.org/10.18653/v1/2023.findings-acl.500) |  | 0 | Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words. Previous search methods for word-level attacks assume that the information in the important words is more influential on prediction than... | Chao Zheng, He Wang, Jing Yang, Liming Wang, Pengwei Zhan, Xiao Huang |  |
| 843 |  |  [Responsibility Perspective Transfer for Italian Femicide News](https://doi.org/10.18653/v1/2023.findings-acl.501) |  | 0 | Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader’s perception of who is to blame for the violence, possibly reinforcing... | Benedetta Muscato, Gosse Minnema, Huiyuan Lai, Malvina Nissim |  |
| 844 |  |  [Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models](https://doi.org/10.18653/v1/2023.findings-acl.502) |  | 0 | Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this... | Anne Lauscher, Björn Ross, Eddie L. Ungless |  |
| 845 |  |  [Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding](https://doi.org/10.18653/v1/2023.findings-acl.503) |  | 0 | The Wav2Vec and its variants have achieved unprecedented success in computational auditory and speech processing. Meanwhile, neural encoding studies that integrate the superb representation capability of Wav2Vec and link those representations to brain activities have provided novel insights into a... | Dajiang Zhu, David Liu, Junwei Han, Lei Guo, Lin Zhao, Mengyue Zhou, Tianming Liu, Xintao Hu, Xu Liu, Zhengliang Liu, Zihao Wu |  |
| 846 |  |  [Deeply Coupled Cross-Modal Prompt Learning](https://doi.org/10.18653/v1/2023.findings-acl.504) |  | 0 | Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning,... | Fei Tan, Jinghui Lu, Rui Zhao, Wei Tang, Xuejing Liu, Zhaojun Guo |  |
| 847 |  |  [Opinion Tree Parsing for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.505) |  | 0 | Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. These models avoid explicit modeling of structure between sentiment elements, which are succinct yet lack desirable properties such as structure... | Guodong Zhou, Xiaotong Jiang, Xiaoyi Bao, Yue Zhang, Zhongqing Wang |  |
| 848 |  |  [CoMix: Guide Transformers to Code-Mix using POS structure and Phonetics](https://doi.org/10.18653/v1/2023.findings-acl.506) |  | 0 | Code-mixing is ubiquitous in multilingual societies, which makes it vital to build models for code-mixed data to power human language interfaces. Existing multilingual transformer models trained on pure corpora lack the ability to intermix words of one language into the structure of another. These... | Gaurav Arora, Srujana Merugu, Vivek Sembium |  |
| 849 |  |  [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://doi.org/10.18653/v1/2023.findings-acl.507) |  | 0 | Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However,... | Alex Ratner, ChenYu Lee, ChengYu Hsieh, ChihKuan Yeh, ChunLiang Li, Hootan Nakhost, Ranjay Krishna, Tomas Pfister, Yasuhisa Fujii |  |
| 850 |  |  [Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech](https://doi.org/10.18653/v1/2023.findings-acl.508) |  | 0 | Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be... | Chunlei Zhang, Dong Yu, Rongjie Huang, Yi Ren, Zhou Zhao |  |
| 851 |  |  [Duplex Diffusion Models Improve Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.509) |  | 0 | Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a... | Xianchao Wu |  |
| 852 |  |  [Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.510) |  | 0 | Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole... | Linhan Zhang, Wei Wang, Yuxin Jiang |  |
| 853 |  |  [PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.511) |  | 0 | While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision... | Dongyan Zhao, Jiahao Liu, Jingang Wang, Qifan Wang, Rui Yan, Wei Wu, Yang Yang, Yunsen Xian, Zhuocheng Gong |  |
| 854 |  |  [Synthetic Pre-Training Tasks for Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.512) |  | 0 | Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model.... | Graeme Blackwood, Julian J. McAuley, Rameswar Panda, Rogério Feris, Zexue He |  |
| 855 |  |  [IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.513) |  | 0 | In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are... | Shijin Wang, Yiming Cui, Zihang Xu, Ziqing Yang |  |
| 856 |  |  [Adversarial Training for Low-Resource Disfluency Correction](https://doi.org/10.18653/v1/2023.findings-acl.514) |  | 0 | Disfluencies commonly occur in conversational speech. Speech with disfluencies can result in noisy Automatic Speech Recognition (ASR) transcripts, which affects downstream tasks like machine translation. In this paper, we propose an adversarially-trained sequence-tagging model for Disfluency... | Preethi Jyothi, Pushpak Bhattacharyya, Vineet Bhat |  |
| 857 |  |  [Computer says "No": The Case Against Empathetic Conversational AI](https://doi.org/10.18653/v1/2023.findings-acl.515) |  | 0 | Emotions are an integral part of human cognition and they guide not only our understanding of the world but also our actions within it. As such, whether we soothe or flame an emotion is not inconsequential. Recent work in conversational AI has focused on responding empathetically to users,... | Alba Curry, Amanda Cercas Curry |  |
| 858 |  |  [Stubborn Lexical Bias in Data and Models](https://doi.org/10.18653/v1/2023.findings-acl.516) |  | 0 | In NLP, recent work has seen increased focus on spurious correlations between various features and labels in training data, and how these influence model behavior. However, the presence and effect of such correlations are typically examined feature by feature. We investigate the cumulative impact... | Jesse Dodge, Noah A. Smith, Sofia Serrano |  |
| 859 |  |  [Distilling Efficient Language-Specific Models for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2023.findings-acl.517) |  | 0 | Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs’ language... | Alan Ansell, Anna Korhonen, Edoardo Maria Ponti, Ivan Vulic |  |
| 860 |  |  [An Extensive Exploration of Back-Translation in 60 Languages](https://doi.org/10.18653/v1/2023.findings-acl.518) |  | 0 | Back-translation is a data augmentation technique that has been shown to improve model quality through the creation of synthetic training bitext. Early studies showed the promise of the technique and follow on studies have produced additional refinements. We have undertaken a broad investigation... | Kevin Duh, Paul McNamee |  |
| 861 |  |  [AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.519) |  | 0 | Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same... | Ru Zhou, Shenglong Yu, Wenya Guo, Xiaojie Yuan, Xumeng Liu, Ying Zhang |  |
| 862 |  |  [Forecasting Earnings Surprises from Conference Call Transcripts](https://doi.org/10.18653/v1/2023.findings-acl.520) |  | 0 | There is a multitude of textual data relevant to the financial markets, spanning genres such as financial news, earnings conference calls, and social media posts. Earnings conference calls are one of the most important to information flow as they reflect a direct communication between company... | Nicholas Andrews, Ross Koval, Xifeng Yan |  |
| 863 |  |  [MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.521) |  | 0 | Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external... | Carolina Scarton, Robert Flynn, Sebastian T. Vincent |  |
| 864 |  |  [Evaluation for Change](https://doi.org/10.18653/v1/2023.findings-acl.522) |  | 0 | Evaluation is the central means for assessing, understanding, and communicating about NLP models. In this position paper, we argue evaluation should be more than that: it is a force for driving change, carrying a sociological and political character beyond its technical dimensions. As a force,... | Rishi Bommasani |  |
| 865 |  |  [Reconstruction Probing](https://doi.org/10.18653/v1/2023.findings-acl.523) |  | 0 | We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation... | Abdelrahim Qaddoumi, Alex Warstadt, Jatin Khilnani, Najoung Kim |  |
| 866 |  |  [Towards Distribution-shift Robust Text Classification of Emotional Content](https://doi.org/10.18653/v1/2023.findings-acl.524) |  | 0 | Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which... | Aldo Gangemi, Luana Bulla, Misael Mongiovì |  |
| 867 |  |  [Multi-lingual and Multi-cultural Figurative Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.525) |  | 0 | Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is... | Alham Fikri Aji, Anubha Kabra, Aremu Anuoluwapo, Emmy Liu, Genta Indra Winata, Graham Neubig, Perez Ogayo, Samuel Cahyawijaya, Simran Khanuja |  |
| 868 |  |  [Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table](https://doi.org/10.18653/v1/2023.findings-acl.526) |  | 0 | Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate... | Edward Choi, Seonhee Cho, Sunjun Kweon, Yeonsu Kwon, Yohan Jo |  |
| 869 |  |  [What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning](https://doi.org/10.18653/v1/2023.findings-acl.527) |  | 0 | Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning... | Danqi Chen, Howard Chen, Jane Pan, Tianyu Gao |  |
| 870 |  |  [Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.528) |  | 0 | Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar... | Ercong Nie, Helmut Schmid, Hinrich Schütze, Sheng Liang |  |
| 871 |  |  [Unsupervised Summarization Re-ranking](https://doi.org/10.18653/v1/2023.findings-acl.529) |  | 0 | With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts.... | Mathieu Ravaut, Nancy F. Chen, Shafiq R. Joty |  |
| 872 |  |  [GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.530) |  | 0 | Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing... | Changjian Wang, Dongsheng Li, Yuxin Yang, Zexin Jian, Zhen Huang, Zhihua Wen, Zhiliang Tian |  |
| 873 |  |  [So many design choices: Improving and interpreting neural agent communication in signaling games](https://doi.org/10.18653/v1/2023.findings-acl.531) |  | 0 | Emergent language games are experimental protocols designed to model how communication may arise among a group of agents. In this paper, we focus on how to improve performances of neural agents playing a signaling game: a sender is exposed to an image and generates a sequence of symbols that is... | Timothee Mickus, Timothée Bernard |  |
| 874 |  |  [Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.532) |  | 0 | As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After... | Fanyu Wang, Zhenping Xie |  |
| 875 |  |  [Fixed Input Parameterization for Efficient Prompting](https://doi.org/10.18653/v1/2023.findings-acl.533) |  | 0 | Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead.... | Eunbi Choi, Joel Jang, Joonwon Jang, Minjoon Seo, Yongrae Jo |  |
| 876 |  |  [Data Augmentation for Low-Resource Keyphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.534) |  | 0 | Keyphrase generation is the task of summarizing the contents of any given article into a few salient phrases (or keyphrases). Existing works for the task mostly rely on large-scale annotated datasets, which are not easy to acquire. Very few works address the problem of keyphrase generation in... | Cornelia Caragea, Jishnu Ray Chowdhury, Krishna Garg |  |
| 877 |  |  [BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.535) |  | 0 | We present a large-scale video subtitle translation dataset, \*BigVideo\*, to facilitate the study of multi-modality machine translation. Compared with the widely used \*How2\* and \*VaTeX\* datasets, \*BigVideo\* is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981... | Degen Huang, Jinsong Su, Liyan Kang, Luyang Huang, Mingxuan Wang, Ningxin Peng, Peihao Zhu, Shanbo Cheng, Zewei Sun |  |
| 878 |  |  [Constructing Procedural Graphs with Multiple Dependency Relations: A New Dataset and Baseline](https://doi.org/10.18653/v1/2023.findings-acl.536) |  | 0 | Current structured and semi-structured knowledge bases mainly focus on representing descriptive knowledge but ignore another commonsense knowledge (Procedural Knowledge). To structure the procedural knowledge, existing methods are proposed to automatically generate flow graphs from procedural... | Bihan Zhou, Haopeng Ren, Yi Cai, Yushi Zeng, Zetao Lian |  |
| 879 |  |  [Multi-Dimensional Evaluation of Text Summarization with In-Context Learning](https://doi.org/10.18653/v1/2023.findings-acl.537) |  | 0 | Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or... | Chunting Zhou, Graham Neubig, Patrick Fernandes, Pengfei Liu, Sameer Jain, Swarnashree Mysore Sathyendra, Vaishakh Keshava |  |
| 880 |  |  [Learning to Rank Utterances for Query-Focused Meeting Summarization](https://doi.org/10.18653/v1/2023.findings-acl.538) |  | 0 | Query-focused meeting summarization(QFMS) aims to generate a specific summary for the given query according to the meeting transcripts. Due to the conflict between long meetings and limited input size, previous works mainly adopt extract-then-summarize methods, which use extractors to simulate... | Xingxian Liu, Yajing Xu |  |
| 881 |  |  [Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.539) |  | 0 | Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET... | Anoop Kumar, Aram Galstyan, Govind Thattai, Greg Ver Steeg, Neal Lawton |  |
| 882 |  |  [Aligning Offline Metrics and Human Judgments of Value for Code Generation Models](https://doi.org/10.18653/v1/2023.findings-acl.540) |  | 0 | Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code are most often evaluated in terms of their functional correctness (i.e., whether generations pass... | Adam Fourney, Forough PoursabziSangdeh, Gagan Bansal, Han Liu, Saleema Amershi, Victor Dibia |  |
| 883 |  |  [Do transformer models do phonology like a linguist?](https://doi.org/10.18653/v1/2023.findings-acl.541) |  | 0 | Neural sequence-to-sequence models have been very successful at tasks in phonology and morphology that seemingly require a capacity for intricate linguistic generalisations. In this paper, we perform a detailed breakdown of the power of such models to capture various phonological generalisations... | Mans Hulden, Saliha Muradoglu |  |
| 884 |  |  [DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.542) |  | 0 | The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation... | Felipe Pérez, Maksims Volkovs, Rasa Hosseinzadeh, Sajad Norouzi |  |
| 885 |  |  [Retrieval-augmented Video Encoding for Instructional Captioning](https://doi.org/10.18653/v1/2023.findings-acl.543) |  | 0 | Instructional videos make learning knowledge more efficient, by providing a detailed multimodal context of each procedure in instruction.A unique challenge posed by instructional videos is key-object degeneracy, where any single modality fails to sufficiently capture the key objects referred to in... | Jihyuk Kim, Minji Seo, Minsoo Kim, Seungtaek Choi, Seungwon Hwang, YeonJoon Jung |  |
| 886 |  |  [Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training](https://doi.org/10.18653/v1/2023.findings-acl.544) |  | 0 | Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities. Previous work addresses this issue by approaches such as tunable parameters reduction or data augmentation. However, they either limit the trained... | Dong Yu, Haitao Mi, Lifeng Jin, Linfeng Song, Sai Ashish Somayajula |  |
| 887 |  |  [Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models](https://doi.org/10.18653/v1/2023.findings-acl.545) |  | 0 | Recent studies in natural language processing (NLP) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical Chinese first came to Japan approximately 2,000 years ago. It was... | Daisuke Kawahara, Hao Wang, Hirofumi Shimizu |  |
| 888 |  |  [Adaptive Attention for Sparse-based Long-sequence Transformer](https://doi.org/10.18653/v1/2023.findings-acl.546) |  | 0 | Recently, Transformers have been widely used in various fields and have achieved remarkable results. But it is still difficult for Transformer-based models to process longer sequences because self-attention in them scales quadratically with the sequence length. Although some models attempt to use... | Qing Yang, Xuanyu Zhang, Zhepeng Lv |  |
| 889 |  |  [Sentiment Analysis using the Relationship between Users and Products](https://doi.org/10.18653/v1/2023.findings-acl.547) |  | 0 | In product reviews, user and product aspects are useful in sentiment analysis. Nevertheless, previous studies mainly focus on modeling user and product aspects without considering the relationship between users and products. The relationship between users and products is typically helpful in... | Kiyoaki Shirai, Natthawut Kertkeidkachorn |  |
| 890 |  |  [Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks](https://doi.org/10.18653/v1/2023.findings-acl.548) |  | 0 | Multilingual language models (MLLMs) like mBERTpromise to extend the benefits of NLP research to low-resource languages (LRLs). However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from... | Animesh Mukherjee, Arijit Nag, Bidisha Samanta, Niloy Ganguly, Soumen Chakrabarti |  |
| 891 |  |  [Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data](https://doi.org/10.18653/v1/2023.findings-acl.549) |  | 0 | Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which... | Hwee Tou Ng, Lidong Bing, Lu Xu, Qingyu Tan |  |
| 892 |  |  [Solving Cosine Similarity Underestimation between High Frequency Words by \ell₂ Norm Discounting](https://doi.org/10.18653/v1/2023.findings-acl.550) |  | 0 | Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words CITATION.This similarity underestimation problem is particularly severe for high... | Danushka Bollegala, Saeth Wannasuphoprasit, Yi Zhou |  |
| 893 |  |  [Do Large Language Models Know What They Don't Know?](https://doi.org/10.18653/v1/2023.findings-acl.551) |  | 0 | Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of... | Jiawen Wu, Qipeng Guo, Qiushi Sun, Xipeng Qiu, Xuanjing Huang, Zhangyue Yin |  |
| 894 |  |  [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://doi.org/10.18653/v1/2023.findings-acl.552) |  | 0 | CLIP (Contrastive Language–Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When... | BoWen Zhang, Guang Liu, Ledell Wu, Qinghong Yang, Zhongzhi Chen |  |
| 895 |  |  [RHGN: Relation-gated Heterogeneous Graph Network for Entity Alignment in Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-acl.553) |  | 0 | Entity Alignment, which aims to identify equivalent entities from various Knowledge Graphs (KGs), is a fundamental and crucial task in knowledge graph fusion. Existing methods typically use triple or neighbor information to represent entities, and then align those entities using similarity... | Enhong Chen, Jiaxian Yan, Kai Zhang, Linan Yue, Xukai Liu, Ye Liu, Zhenya Huang |  |
| 896 |  |  [Feature Interactions Reveal Linguistic Structure in Language Models](https://doi.org/10.18653/v1/2023.findings-acl.554) |  | 0 | We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural... | Jaap Jumelet, Willem H. Zuidema |  |
| 897 |  |  [Clustering-Aware Negative Sampling for Unsupervised Sentence Representation](https://doi.org/10.18653/v1/2023.findings-acl.555) |  | 0 | Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate... | Fanqi Wan, Jinghao Deng, Rui Wang, Tao Yang, Xiaojun Quan |  |
| 898 |  |  [An Effective Deployment of Contrastive Learning in Multi-label Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.556) |  | 0 | The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in... | Aimin Yang, Dong Zhou, Gang Wang, Guanqiu Qin, Nankai Lin |  |
| 899 |  |  [Segment-Level and Category-Oriented Network for Knowledge-Based Referring Expression Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.557) |  | 0 | Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant... | Liuwu Li, Qingbao Huang, Qiong Liu, Xin Wu, Yi Cai, Yuqi Bu |  |
| 900 |  |  [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://doi.org/10.18653/v1/2023.findings-acl.558) |  | 0 | Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled... | JiRong Wen, Junyi Li, Tianyi Tang, Wayne Xin Zhao |  |
| 901 |  |  [From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment](https://doi.org/10.18653/v1/2023.findings-acl.559) |  | 0 | Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities.... | Haiwei Zhang, Xiangrui Cai, Xiaojie Yuan, Yike Wu, Ying Zhang, Yu Zhao |  |
| 902 |  |  [It is a Bird Therefore it is a Robin: On BERT's Internal Consistency Between Hypernym Knowledge and Logical Words](https://doi.org/10.18653/v1/2023.findings-acl.560) |  | 0 | The lexical knowledge of NLP systems shouldbe tested (i) for their internal consistency(avoiding groundedness issues) and (ii) bothfor content words and logical words. In thispaper we propose a new method to test the understandingof the hypernymy relationship bymeasuring its antisymmetry according... | Emmanuel Chemla, Nicolas Guérin |  |
| 903 |  |  [Defending against Insertion-based Textual Backdoor Attacks via Attribution](https://doi.org/10.18653/v1/2023.findings-acl.561) |  | 0 | Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against... | Chaowei Xiao, Jiazhao Li, V. G. Vinod Vydiswaran, Wei Ping, Zhuofeng Wu |  |
| 904 |  |  [ActiveAED: A Human in the Loop Improves Annotation Error Detection](https://doi.org/10.18653/v1/2023.findings-acl.562) |  | 0 | Manually annotated datasets are crucial for training and evaluating Natural Language Processing models. However, recent work has discovered that even widely-used benchmark datasets contain a substantial number of erroneous annotations. This problem has been addressed with Annotation Error Detection... | Barbara Plank, Leon Weber |  |
| 905 |  |  [Assessing Word Importance Using Models Trained for Semantic Tasks](https://doi.org/10.18653/v1/2023.findings-acl.563) |  | 0 | Many NLP tasks require to automatically identify the most significant words in a text. In this work, we derive word significance from models trained to solve semantic task: Natural Language Inference and Paraphrase Identification. Using an attribution method aimed to explain the predictions of... | Dávid Javorský, François Yvon, Ondrej Bojar |  |
| 906 |  |  [In-context Examples Selection for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.564) |  | 0 | Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the... | Chunting Zhou, Luke Zettlemoyer, Marjan Ghazvininejad, Mike Lewis, Sweta Agrawal |  |
| 907 |  |  [PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition](https://doi.org/10.18653/v1/2023.findings-acl.565) |  | 0 | The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are... | Alex Fabrikant, Dan Roth, Senaka Buthpitiya, Sihao Chen, Tal Schuster |  |
| 908 |  |  [CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.566) |  | 0 | Speech or text representation generated by pre-trained models contains modal-specific information that could be combined for benefiting spoken language understanding (SLU) tasks. In this work, we propose a novel pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training (CIF-PT). It... | Jun Zhang, Linhao Dong, Lu Lu, Peihao Wu, Zejun Ma, Zhecheng An |  |
| 909 |  |  [Improving Diachronic Word Sense Induction with a Nonparametric Bayesian method](https://doi.org/10.18653/v1/2023.findings-acl.567) |  | 0 | Diachronic Word Sense Induction (DWSI) is the task of inducing the temporal representations of a word meaning from the context, as a set of senses and their prevalence over time. We introduce two new models for DWSI, based on topic modelling techniques: one is based on Hierarchical Dirichlet... | Ashjan Alsulaimani, Erwan Moreau |  |
| 910 |  |  [What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection](https://doi.org/10.18653/v1/2023.findings-acl.568) |  | 0 | Mental health disorders (MHD) are increasingly prevalent worldwide and constitute one of the greatest challenges facing our healthcare systems and modern societies in general. In response to this societal challenge, there has been a surge in digital mental health research geared towards the... | Daniel Wiechmann, Elma Kerz, Sourabh Zanwar, Xiaofei Li, Yu Qiao |  |
| 911 |  |  [Adaptive Contrastive Knowledge Distillation for BERT Compression](https://doi.org/10.18653/v1/2023.findings-acl.569) |  | 0 | In this paper, we propose a new knowledge distillation approach called adaptive contrastive knowledge distillation (ACKD) for BERT compression. Different from existing knowledge distillation methods for BERT that implicitly learn discriminative student features by mimicking the teacher features, we... | Jiaheng Liu, Jinyang Guo, Ke Xu, Ruihao Gong, Xianglong Liu, Yuqing Ma, Zining Wang |  |
| 912 |  |  [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://doi.org/10.18653/v1/2023.findings-acl.570) |  | 0 | The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing... | Jingcheng Yin, Jingwen Leng, Meng Yang, Minwei Feng, Xinbing Wang, Zhouhan Lin, Ziwei He |  |
| 913 |  |  [Zero-Shot Classification by Logical Reasoning on Natural Language Explanations](https://doi.org/10.18653/v1/2023.findings-acl.571) |  | 0 | Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as “it has a slim straight... | Chi Han, Heng Ji, Hengzhi Pei, Xinya Du |  |
| 914 |  |  [Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.572) |  | 0 | Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly... | Cheng Ji, Jianxin Li, Lihong Wang, Qian Li, Shiyao Cui, Shu Guo, Xutan Peng |  |
| 915 |  |  [Pruning Pre-trained Language Models with Principled Importance and Self-regularization](https://doi.org/10.18653/v1/2023.findings-acl.573) |  | 0 | Iterative pruning is one of the most effective compression methods for pre-trained language models. We discovered that finding the optimal pruning decision is an equality-constrained 0-1 Integer Linear Programming problem. The solution to this optimization problem leads to a principled importance... | Kenny Q. Zhu, Siyu Ren |  |
| 916 |  |  [The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code](https://doi.org/10.18653/v1/2023.findings-acl.574) |  | 0 | Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given... | Chen Zhang, Da Yin, Dongyan Zhao, Xiao Liu, Yansong Feng |  |
| 917 |  |  [Learning to Leverage High-Order Medical Knowledge Graph for Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.575) |  | 0 | Automatic medical entity and relation extraction is essential for daily electronic medical record (EMR) analysis, and has attracted a lot of academic attention. Tremendous progress has been made in recent years. However, medical terms are difficult to understand, and their relations are more... | Junlan Feng, Yi Huang, Zhe Yang |  |
| 918 |  |  [Data-Efficient Finetuning Using Cross-Task Nearest Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.576) |  | 0 | Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a... | Hamish Ivison, Hannaneh Hajishirzi, Noah A. Smith, Pradeep Dasigi |  |
| 919 |  |  [CoAug: Combining Augmentation of Labels and Labelling Rules](https://doi.org/10.18653/v1/2023.findings-acl.577) |  | 0 | Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted... | Bingqing Wang, Jun Araki, Liu Ren, Rakesh R. Menon, Zhe Feng, Zhengyu Zhou |  |
| 920 |  |  [Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks](https://doi.org/10.18653/v1/2023.findings-acl.578) |  | 0 | Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit... | Aiwei Liu, Fei Huang, Lijie Wen, Pengjun Xie, Philip S. Yu, Xuming Hu, Yong Jiang, Zhongqiang Huang |  |
| 921 |  |  [World Models for Math Story Problems](https://doi.org/10.18653/v1/2023.findings-acl.579) |  | 0 | Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained... | Abulhair Saparov, Andreas Opedal, Mrinmaya Sachan, Niklas Stoehr |  |
| 922 |  |  [AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.580) |  | 0 | Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE... | Ahmed Hassan Awadallah, Ganesh Jawahar, Jianfeng Gao, Laks V. S. Lakshmanan, Muhammad AbdulMageed, Subhabrata Mukherjee, Sébastien Bubeck, Xiaodong Liu, Young Jin Kim |  |
| 923 |  |  [Language Agnostic Multilingual Information Retrieval with Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.581) |  | 0 | Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available.... | Deguang Kong, Kunlun Liu, Peng Qi, William Yang Wang, Xinchi Chen, Xiyang Hu, Zhiheng Huang |  |
| 924 |  |  [Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods](https://doi.org/10.18653/v1/2023.findings-acl.582) |  | 0 | A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement – if multiple... | Jan Snajder, Josip Jukic, Martin Tutek |  |
| 925 |  |  [Enhancing Cross-lingual Transfer via Phonemic Transcription Integration](https://doi.org/10.18653/v1/2023.findings-acl.583) |  | 0 | Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing... | Chenwei Zhang, Eugene Rohrbaugh, Hoang Nguyen, Philip S. Yu, Tao Zhang |  |
| 926 |  |  [Human-in-the-loop Abstractive Dialogue Summarization](https://doi.org/10.18653/v1/2023.findings-acl.584) |  | 0 | Abstractive dialogue summarization has received increasing attention recently. Despite the fact that most of the current dialogue summarization systems are trained to maximize the likelihood of human-written summaries and have achieved significant results, there is still a huge gap in generating... | Diyi Yang, Jiaao Chen, Mohan Dodda |  |
| 927 |  |  [A Multi-task Learning Framework for Quality Estimation](https://doi.org/10.18653/v1/2023.findings-acl.585) |  | 0 | Quality Estimation (QE) is the task of evaluating machine translation output in the absence of reference translation. Conventional approaches to QE involve training separate models at different levels of granularity viz., word-level, sentence-level, and document-level, which sometimes lead to... | Constantin Orasan, Diptesh Kanojia, Paramveer Choudhary, Pushpak Bhattacharyya, Sourabh Dattatray Deoghare, Tharindu Ranasinghe |  |
| 928 |  |  [The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.586) |  | 0 | Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy... | Feng Yao, Hao Peng, Juanzi Li, Kaisheng Zeng, Lei Hou, Weixing Shen, Xiaozhi Wang, Zhiyuan Liu |  |
| 929 |  |  [Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers](https://doi.org/10.18653/v1/2023.findings-acl.587) |  | 0 | The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a... | David Schlangen, Philipp Sadler, Sherzod Hakimov |  |
| 930 |  |  [Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.588) |  | 0 | Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to... | Kanagasabai Rajaraman, Saravanan Rajamanickam, Wei Shi |  |
| 931 |  |  [Multilingual Multi-Figurative Language Detection](https://doi.org/10.18653/v1/2023.findings-acl.589) |  | 0 | Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but... | Antonio Toral, Huiyuan Lai, Malvina Nissim |  |
| 932 |  |  [Zero-shot Visual Question Answering with Language Model Feedback](https://doi.org/10.18653/v1/2023.findings-acl.590) |  | 0 | In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). As... | JiRong Wen, Junyi Li, Tianyi Tang, Wayne Xin Zhao, Yifan Du |  |
| 933 |  |  [Prompted Opinion Summarization with GPT-3.5](https://doi.org/10.18653/v1/2023.findings-acl.591) |  | 0 | Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of... | Adithya Bhaskar, Alexander R. Fabbri, Greg Durrett |  |
| 934 |  |  [Sentence Ordering with a Coherence Verifier](https://doi.org/10.18653/v1/2023.findings-acl.592) |  | 0 | This paper presents a novel sentence ordering method by plugging a coherence verifier (CoVer) into pair-wise ranking-based and sequence generation-based methods. It does not change the model parameters of the baseline, and only verifies the coherence of candidate (partial) orders produced by the... | Jiefu Gong, Sainan Jia, Shijin Wang, Ting Liu, Wei Song |  |
| 935 |  |  [GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.593) |  | 0 | Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023’s ‘Reality Check’ theme, we present GUMSum, a small but carefully... | Amir Zeldes, Yang Janet Liu |  |
| 936 |  |  [Improving Grammatical Error Correction with Multimodal Feature Integration](https://doi.org/10.18653/v1/2023.findings-acl.594) |  | 0 | Grammatical error correction (GEC) is a promising task aimed at correcting errors in a text. Many methods have been proposed to facilitate this task with remarkable results. However, most of them only focus on enhancing textual feature extraction without exploring the usage of other modalities’... | Derek F. Wong, Jinpeng Hu, Lidia S. Chao, Tao Fang, TsungHui Chang, Xiang Wan |  |
| 937 |  |  [Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification](https://doi.org/10.18653/v1/2023.findings-acl.595) |  | 0 | Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the... | Renliang Sun, Wei Xu, Xiaojun Wan |  |
| 938 |  |  [Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction](https://doi.org/10.18653/v1/2023.findings-acl.596) |  | 0 | The semantic frame induction tasks are defined as a clustering of words into the frames that they evoke, and a clustering of their arguments according to the frame element roles that they should fill. In this paper, we address the latter task of argument clustering, which aims to acquire frame... | Koichi Takeda, Kosuke Yamada, Ryohei Sasano |  |
| 939 |  |  [Leveraging Synthetic Targets for Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.597) |  | 0 | In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups,... | Oleksii Hrinchuk, Oleksii Kuchaiev, Sarthak Mittal |  |
| 940 |  |  [Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models](https://doi.org/10.18653/v1/2023.findings-acl.598) |  | 0 | Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a... | Andy Rosenbaum, Anna Rumshisky, Qin Lu, Saleh Soltan, Tobias Falke, Wael Hamza |  |
| 941 |  |  [Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.599) |  | 0 | Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic... | Hao Fei, Meishan Zhang, Min Zhang, TatSeng Chua |  |
| 942 |  |  [Spontaneous gestures encoded by hand positions improve language models: An Information-Theoretic motivated study](https://doi.org/10.18653/v1/2023.findings-acl.600) |  | 0 | The multi-modality nature of human communication has been utilized to enhance the performance of language modeling-related tasks. Driven by the development of large-scale end-to-end learning techniques and the availability of multi-modal data, it becomes possible to represent non-verbal... | Yang Cheng, Yang Xu |  |
| 943 |  |  [Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences](https://doi.org/10.18653/v1/2023.findings-acl.601) |  | 0 | Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose... | Chaojun Wang, Wai Lam, Yang Liu |  |
| 944 |  |  [Controlled Text Generation with Hidden Representation Transformations](https://doi.org/10.18653/v1/2023.findings-acl.602) |  | 0 | We propose CHRT (Control HiddenRepresentation Transformation) – a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model... | Amita Misra, Ankit Chadha, Emilio Ferrara, Hana Koorehdavoudi, Masud Moshtaghi, Vaibhav Kumar |  |
| 945 |  |  [Visual Coherence Loss for Coherent and Visually Grounded Story Generation](https://doi.org/10.18653/v1/2023.findings-acl.603) |  | 0 | Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2)... | Asad B. Sayeed, Bernt Schiele, Qiankun Zheng, Vera Demberg, Xudong Hong |  |
| 946 |  |  [AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks](https://doi.org/10.18653/v1/2023.findings-acl.604) |  | 0 | Tabular data analysis is performed everyday across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived... | Dongmei Zhang, Jialiang Xu, Mengyu Zhou, Mingjie Zhou, Shi Han, Tianle Li, Xiao Lv, Xinyi He, Yijia Shao, Zejian Yuan |  |
| 947 |  |  [Large Language Models Are Partially Primed in Pronoun Interpretation](https://doi.org/10.18653/v1/2023.findings-acl.605) |  | 0 | While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases... | Chenyu You, Kexun Zhang, Qingcheng Zeng, Rob Voigt, SuetYing Lam |  |
| 948 |  |  [Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors](https://doi.org/10.18653/v1/2023.findings-acl.606) |  | 0 | In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus... | Chrysoula Zerva, Edmund Dervakos, George Filandrianos, Giorgos Stamou, Orfeas MenisMastromichalakis |  |
| 949 |  |  [A Pilot Study on Dialogue-Level Dependency Parsing for Chinese](https://doi.org/10.18653/v1/2023.findings-acl.607) |  | 0 | Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies.... | Gongyao Jiang, Meishan Zhang, Min Zhang, Shuang Liu |  |
| 950 |  |  [On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.608) |  | 0 | While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target... | Baobao Chang, Dongdong Zhang, Furu Wei, Liang Chen, Shuming Ma |  |
| 951 |  |  [ORCA: A Challenging Benchmark for Arabic Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.609) |  | 0 | Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for... | AbdelRahim A. Elmadany, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 952 |  |  [Delving into the Openness of CLIP](https://doi.org/10.18653/v1/2023.findings-acl.610) |  | 0 | Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can... | Guangxiang Zhao, Lei Li, Shuhuai Ren, Xu Sun, Xuancheng Ren |  |
| 953 |  |  [From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework](https://doi.org/10.18653/v1/2023.findings-acl.611) |  | 0 | Textual adversarial attacks can discover models’ weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic... | Bo Yuan, Dehan Kong, Ganqu Cui, Hanlu Wu, Heng Ji, Hongcheng Gao, Hui Xue, Lifan Yuan, Longtao Huang, Maosong Sun, Ning Shi, Yangyi Chen, Zhiyuan Liu |  |
| 954 |  |  [An Empirical Study of Sentiment-Enhanced Pre-Training for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.612) |  | 0 | Aspect-Based Sentiment Analysis (ABSA) aims to recognize fine-grained opinions and sentiments of users, which is an important problem in sentiment analysis. Recent work has shown that Sentiment-enhanced Pre-Training (SPT) can substantially improve the performance of various ABSA tasks. However,... | Bin Liang, Bing Qin, Ruifeng Xu, Shiwei Chen, Yice Zhang, Yifan Yang |  |
| 955 |  |  [NatCS: Eliciting Natural Customer Support Dialogues](https://doi.org/10.18653/v1/2023.findings-acl.613) |  | 0 | Despite growing interest in applications based on natural customer support conversations,there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark... | Arshit Gupta, Emily Moeng, James Gung, Saab Mansour, Wesley Rose, Yi Zhang |  |
| 956 |  |  [Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method](https://doi.org/10.18653/v1/2023.findings-acl.614) |  | 0 | The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the model’s intermediate layer features... | Jie Tang, Peng Zhang, Shicheng Tan, Shu Zhao, Weng Lam Tam, Wenwen Gong, Yuanchun Wang |  |
| 957 |  |  [Diable: Efficient Dialogue State Tracking as Operations on Tables](https://doi.org/10.18653/v1/2023.findings-acl.615) |  | 0 | Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the... | Chao Shang, Lluís Màrquez, Momchil Hardalov, Pietro Lesci, Yoshinari Fujinuma |  |
| 958 |  |  [Neural Topic Modeling based on Cycle Adversarial Training and Contrastive Learning](https://doi.org/10.18653/v1/2023.findings-acl.616) |  | 0 | Neural topic models have been widely used to extract common topics across documents. Recently, contrastive learning has been applied to variational autoencoder-based neural topic models, achieving promising results. However, due to the limitation of the unidirectional structure of the variational... | Boyu Wang, Deyu Zhou, Jiandong Ding, Linhai Zhang, Yi Cao |  |
| 959 |  |  [Alleviating Exposure Bias via Multi-level Contrastive Learning and Deviation Simulation in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.617) |  | 0 | Most Transformer based abstractive summarization systems have a severe mismatch between training and inference, i.e., exposure bias. From diverse perspectives, we introduce a simple multi-level contrastive learning framework for abstractive summarization (SimMCS) and a tailored sparse decoder... | Jiawen Xie, Qi Su, Shaoting Zhang, Xiaofan Zhang |  |
| 960 |  |  [Mapping Brains with Language Models: A Survey](https://doi.org/10.18653/v1/2023.findings-acl.618) |  | 0 | Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how... | Anders Søgaard, Antonia Karamolegkou, Mostafa Abdou |  |
| 961 |  |  [Parameter-Efficient Finetuning for Robust Continual Multilingual Learning](https://doi.org/10.18653/v1/2023.findings-acl.619) |  | 0 | We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance... | Kartikeya Badola, Partha Talukdar, Shachi Dave |  |
| 962 |  |  [Interpretable Multimodal Misinformation Detection with Logic Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.620) |  | 0 | Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the... | Haoliang Li, Hui Liu, Wenya Wang |  |
| 963 |  |  [Semantic-conditioned Dual Adaptation for Cross-domain Query-based Visual Segmentation](https://doi.org/10.18653/v1/2023.findings-acl.621) |  | 0 | Visual segmentation from language queries has attracted significant research interest. Despite the effectiveness, existing works require expensive labeling and suffer severe degradation when deployed to an unseen domain. In this paper, we investigate a novel task Cross-domain Query-based Visual... | Linjun Li, Tao Jin, Wang Lin, Xize Cheng, Ye Wang, Zhou Zhao |  |
| 964 |  |  [Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans](https://doi.org/10.18653/v1/2023.findings-acl.622) |  | 0 | Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three... | Diego Frassinelli, Hyewon Jang, Qi Yu |  |
| 965 |  |  [Taxonomy of Problems in Lexical Semantics](https://doi.org/10.18653/v1/2023.findings-acl.623) |  | 0 | Semantic tasks are rarely formally defined, and the exact relationship between them is an open question. We introduce a taxonomy that elucidates the connection between several problems in lexical semantics, including monolingual and cross-lingual variants. Our theoretical framework is based on the... | Bradley Hauer, Grzegorz Kondrak |  |
| 966 |  |  [Making Pre-trained Language Models both Task-solvers and Self-calibrators](https://doi.org/10.18653/v1/2023.findings-acl.624) |  | 0 | Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it’s equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently... | Heng Ji, Xingyao Wang, Yangyi Chen |  |
| 967 |  |  [EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding](https://doi.org/10.18653/v1/2023.findings-acl.625) |  | 0 | The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate... | Bilal Taha, Dae Yon Hwang, Yaroslav Nechaev |  |
| 968 |  |  [Denoising Enhanced Distantly Supervised Ultrafine Entity Typing](https://doi.org/10.18653/v1/2023.findings-acl.626) |  | 0 | Recently, the task of distantly supervised (DS) ultra-fine entity typing has received significant attention. However, DS data is noisy and often suffers from missing or wrong labeling issues resulting in low precision and low recall. This paper proposes a novel ultra-fine entity typing model with... | Hongliang Fei, Ping Li, Yue Zhang |  |
| 969 |  |  [INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition](https://doi.org/10.18653/v1/2023.findings-acl.627) |  | 0 | Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning. However, these pre-trained speech models suffer from representational bias as they tend to better represent those... | Chang Dong Yoo, Eunseop Yoon, Hee Suk Yoon, John B. Harvill, Mark HasegawaJohnson |  |
| 970 |  |  [Local Temperature Beam Search: Avoid Neural Text DeGeneration via Enhanced Calibration](https://doi.org/10.18653/v1/2023.findings-acl.628) |  | 0 | Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the “self-loop”. However, the... | Dongkyu Lee, Gyeonghun Kim, Janghoon Han, Nevin L. Zhang, Stanley Jungkyu Choi, Taesuk Hong, Yireun Kim |  |
| 971 |  |  [Explanation Graph Generation via Generative Pre-training over Synthetic Graphs](https://doi.org/10.18653/v1/2023.findings-acl.629) |  | 0 | The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy be- tween unstructured user queries and structured explanation graphs.... | Han Cui, Qi Shi, Shangzhan Li, Yu Zhang |  |
| 972 |  |  [NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts](https://doi.org/10.18653/v1/2023.findings-acl.630) |  | 0 | We introduce NaSGEC, a new dataset to facilitate research on Chinese grammatical error correction (CGEC) for native speaker texts from multiple domains. Previous CGEC research primarily focuses on correcting texts from a single domain, especially learner essays. To broaden the target domain, we... | Bo Zhang, Chen Li, Fei Huang, Haochen Jiang, Min Zhang, Yue Zhang, Zhenghua Li |  |
| 973 |  |  [FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models](https://doi.org/10.18653/v1/2023.findings-acl.631) |  | 0 | It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for... | Rachel Rudinger, Shramay Palta |  |
| 974 |  |  [FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.632) |  | 0 | With increasing concerns about data privacy, there is an increasing necessity of fine-tuning pre-trained language models (PLMs) for adapting to downstream tasks located in end-user devices or local clients without transmitting data to the central server. This urgent necessity therefore calls the... | Lizhen Qu, Qifan Wang, Yong Dai, Yuanhang Yang, Yue Yu, Zenglin Xu, Zhuo Zhang |  |
| 975 |  |  [MixPAVE: Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction](https://doi.org/10.18653/v1/2023.findings-acl.633) |  | 0 | The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus... | Dongfang Liu, Fuli Feng, Jingang Wang, Li Yang, Madian Khabsa, Qifan Wang, Sinong Wang, Xiaojun Quan, Yu Chen, Zenglin Xu |  |
| 976 |  |  [SlowBERT: Slow-down Attacks on Input-adaptive Multi-exit BERT](https://doi.org/10.18653/v1/2023.findings-acl.634) |  | 0 | For pretrained language models such as Google’s BERT, recent research designs several input-adaptive inference mechanisms to improve the efficiency on cloud and edge devices. In this paper, we reveal a new attack surface on input-adaptive multi-exit BERT, where the adversary imperceptibly modifies... | Mi Zhang, Min Yang, Shengyao Zhang, Xudong Pan |  |
| 977 |  |  [Compositional Mathematical Encoding for Math Word Problems](https://doi.org/10.18653/v1/2023.findings-acl.635) |  | 0 | Solving math word problem (MWP) remains a challenging task, as it requires to understand both the semantic meanings of the text and the mathematical logic among quantities, i.e., for both semantics modal and quantity modal learning. Current MWP encoders work in a uni-modal setting and map the given... | Jie Shao, Jipeng Zhang, Kehan Guo, Xiangliang Zhang, Xiaodong Wu, Zhenwen Liang |  |
| 978 |  |  [PREADD: Prefix-Adaptive Decoding for Controlled Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.636) |  | 0 | We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts.... | Dan Klein, Jonathan Pei, Kevin Yang |  |
| 979 |  |  [EventOA: An Event Ontology Alignment Benchmark Based on FrameNet and Wikidata](https://doi.org/10.18653/v1/2023.findings-acl.637) |  | 0 | Event ontology provides a shared and formal specification about what happens in the real world and can benefit many natural language understanding tasks. However, the independent development of event ontologies often results in heterogeneous representations that raise the need for establishing... | Chenhao Wang, Jun Zhao, Kang Liu, Ru Li, Shaoru Guo, Yubo Chen |  |
| 980 |  |  [Enhancing Continual Relation Extraction via Classifier Decomposition](https://doi.org/10.18653/v1/2023.findings-acl.638) |  | 0 | Continual relation extraction (CRE) models aim at handling emerging new relations while avoiding catastrophically forgetting old ones in the streaming data. Though improvements have been shown by previous CRE studies, most of them only adopt a vanilla strategy when models first learn... | Binghuai Lin, Heming Xia, Peiyi Wang, Tianyu Liu, Yunbo Cao, Zhifang Sui |  |
| 981 |  |  [A Comparative Analysis of the Effectiveness of Rare Tokens on Creative Expression using ramBERT](https://doi.org/10.18653/v1/2023.findings-acl.639) |  | 0 | Until now, few studies have been explored on Automated Creative Essay Scoring (ACES), in which a pre-trained model automatically labels an essay as a creative or a non-creative. Since the creativity evaluation of essays is very subjective, each evaluator often has his or her own criteria for... | ByungWon On, Deokgi Kim, Ingyu Lee, Youbin Lee |  |
| 982 |  |  [MTR: A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.640) |  | 0 | A long-standing difficulty in AI is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets... | Caoyun Fan, Hao He, Jidong Tian, Wenqing Chen, Yaohui Jin, Yitian Li |  |
| 983 |  |  [NewsMet : A 'do it all' Dataset of Contemporary Metaphors in News Headlines](https://doi.org/10.18653/v1/2023.findings-acl.641) |  | 0 | Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated... | Aik Beng Ng, Rohan Joseph, Simon See, Sunny Rai, Timothy Liu |  |
| 984 |  |  [Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs](https://doi.org/10.18653/v1/2023.findings-acl.642) |  | 0 | Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view... | Binxuan Huang, Chenwei Zhang, Christos Faloutsos, Daheng Wang, Jingbo Shang, Wei Wang, Xian Li, Yan Liang, Yizhou Sun, Zhengyang Wang, Zijie Huang |  |
| 985 |  |  [Noise-Robust Training with Dynamic Loss and Contrastive Learning for Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.643) |  | 0 | Distantly-supervised named entity recognition (NER) aims at training networks with distantly-labeled data, which is automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. Distant supervision may induce incomplete and noisy labels, so recent... | Jintao Du, Shuheng Zhou, Zhiyuan Ma |  |
| 986 |  |  [Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation](https://doi.org/10.18653/v1/2023.findings-acl.644) |  | 0 | Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically — Human often... | Aixin Sun, Muhao Chen, Xinze Li, Yixin Cao |  |
| 987 |  |  [End-to-End Task-Oriented Dialogue Systems Based on Schema](https://doi.org/10.18653/v1/2023.findings-acl.645) |  | 0 | This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains,... | Ken Fukuda, Wiradee Imrattanatrai |  |
| 988 |  |  [HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language](https://doi.org/10.18653/v1/2023.findings-acl.646) |  | 0 | This paper presents “HaVQA”, the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result,... | Aneesh Bose, Guneet Singh Kohli, Habeebah A. Kakudi, Ibrahim Said Ahmad, Idris Abdulmumin, Ketan Kotwal, Ondrej Bojar, Sayan Deb Sarkar, Shamsuddeen Hassan Muhammad, Shantipriya Parida |  |
| 989 |  |  [Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction](https://doi.org/10.18653/v1/2023.findings-acl.647) |  | 0 | We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidence jointly learns to identify: (i) the relevant evidences to the given claim (ii) the veracity of the claim. We propose to disentangle the per-evidence... | Martin Fajcik, Pavel Smrz, Petr Motlícek |  |
| 990 |  |  [StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints](https://doi.org/10.18653/v1/2023.findings-acl.648) |  | 0 | We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism: hierarchical structure information... | LeMinh Nguyen, Phuong Nguyen, Truong Do |  |
| 991 |  |  [GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks](https://doi.org/10.18653/v1/2023.findings-acl.649) |  | 0 | Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to... | Aiwei Liu, Chenwei Zhang, Irwin King, Philip S. Yu, Xin Zhang, Xuming Hu, Zeqi Tan |  |
| 992 |  |  [WebDP: Understanding Discourse Structures in Semi-Structured Web Documents](https://doi.org/10.18653/v1/2023.findings-acl.650) |  | 0 | Web documents have become rich data resources in current era, and understanding their discourse structure will potentially benefit various downstream document processing applications. Unfortunately, current discourse analysis and document intelligence research mostly focus on either discourse... | Hao Xiang, Hongyu Lin, Le Sun, Meng Liao, Peilin Liu, Xianpei Han |  |
| 993 |  |  [Tab-CoT: Zero-shot Tabular Chain of Thought](https://doi.org/10.18653/v1/2023.findings-acl.651) |  | 0 | The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit highly structured steps. Recent efforts also started investigating... | Wei Lu, Ziqi Jin |  |
| 994 |  |  [KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition](https://doi.org/10.18653/v1/2023.findings-acl.652) |  | 0 | Symptom diagnosis in medical conversations aims to correctly extract both symptom entities and their status from the doctor-patient dialogue. In this paper, we propose a novel framework called KNSE for symptom status recognition (SSR), where the SSR is formulated as a natural language inference... | Shiqi Wei, Wei Chen, Xuanjing Huang, Zhongyu Wei |  |
| 995 |  |  [Augmenting Large Language Model Translators via Translation Memories](https://doi.org/10.18653/v1/2023.findings-acl.653) |  | 0 | Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to “understand” prompts... | Abudurexiti Reheman, Bei Li, Chunliang Zhang, Jingbo Zhu, Tong Xiao, Yinqiao Li, Yongyu Mu, Yuchun Fan, Zhiquan Cao |  |
| 996 |  |  [Character Coreference Resolution in Movie Screenplays](https://doi.org/10.18653/v1/2023.findings-acl.654) |  | 0 | Movie screenplays have a distinct narrative structure. It segments the story into scenes containing interleaving descriptions of actions, locations, and character dialogues.A typical screenplay spans several scenes and can include long-range dependencies between characters and events.A holistic... | Sabyasachee Baruah, Shrikanth Narayanan |  |
| 997 |  |  [Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph](https://doi.org/10.18653/v1/2023.findings-acl.655) |  | 0 | Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and... | Deyu Li, Jian Liao, Jianxing Zheng, Ruili Pu, Suge Wang, Yang Li |  |
| 998 |  |  [LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing](https://doi.org/10.18653/v1/2023.findings-acl.656) |  | 0 | Transformer has become an important technique for natural language processing tasks with great success. However, it usually requires huge storage space and computational cost, making it difficult to be deployed on resource-constrained edge devices. To compress and accelerate Transformer, we propose... | Guobing Gan, Peng Zhang, Sunzhu Li, Xiuqing Lv, Yueheng Sun |  |
| 999 |  |  [Multi-hop Evidence Retrieval for Cross-document Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.657) |  | 0 | Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations,along with the challenge... | IHung Hsu, Keming Lu, Mingyu Derek Ma, Muhao Chen, Wenxuan Zhou |  |
| 1000 |  |  [Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree](https://doi.org/10.18653/v1/2023.findings-acl.658) |  | 0 | Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases.... | Anna Sotnikova, Connor Baumler, Hal Daumé III |  |
| 1001 |  |  [PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2023.findings-acl.659) |  | 0 | Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by... | KaiWei Chang, KuanHao Huang, Yixin Wan |  |
| 1002 |  |  [DePlot: One-shot visual language reasoning by plot-to-table translation](https://doi.org/10.18653/v1/2023.findings-acl.660) |  | 0 | Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited,... | Chenxi Pang, Fangyu Liu, Francesco Piccinno, Julian Martin Eisenschlos, Kenton Lee, Mandar Joshi, Nigel Collier, Syrine Krichene, Wenhu Chen, Yasemin Altun |  |
| 1003 |  |  [Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.findings-acl.661) |  | 0 | Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed... | Jie Zhou, Maosong Sun, Weize Chen, Xu Han, Yankai Lin, Zhiyuan Liu |  |
| 1004 |  |  [Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation](https://doi.org/10.18653/v1/2023.findings-acl.662) |  | 0 | Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by... | Jay Yoon Lee, Lifu Huang, Zhiyang Xu |  |
| 1005 |  |  [How Well Do Large Language Models Perform on Faux Pas Tests?](https://doi.org/10.18653/v1/2023.findings-acl.663) |  | 0 | Motivated by the question of the extent to which large language models “understand” social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which... | Guy Zwirn, Natalie Shapira, Yoav Goldberg |  |
| 1006 |  |  [Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference](https://doi.org/10.18653/v1/2023.findings-acl.664) |  | 0 | Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or... | Ronan Le Bras, Wangchunshu Zhou, Yejin Choi |  |
| 1007 |  |  [ISLTranslate: Dataset for Translating Indian Sign Language](https://doi.org/10.18653/v1/2023.findings-acl.665) |  | 0 | Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of... | Abhinav Joshi, Ashutosh Modi, Susmit Agrawal |  |
| 1008 |  |  [LMentry: A Language Model Benchmark of Elementary Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.666) |  | 0 | As the performance of large language models rapidly improves, benchmarks are getting larger and more complex as well. We present LMentry, a benchmark that avoids this “arms race” by focusing on a compact set of tasks that are trivial to humans, e.g. writing a sentence containing a specific word,... | Avia Efrat, Omer Levy, Or Honovich |  |
| 1009 |  |  [Differentiable Instruction Optimization for Cross-Task Generalization](https://doi.org/10.18653/v1/2023.findings-acl.667) |  | 0 | Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks. Although various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task... | Ichiro Sakata, Junichiro Mori, Masaru Isonuma |  |
| 1010 |  |  [Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.668) |  | 0 | Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging CITATION, especially in the context of math word problem solving. Additionally, it is... | Wei Lu, Zhanming Jie |  |
| 1011 |  |  [How does the task complexity of masked pretraining objectives affect downstream performance?](https://doi.org/10.18653/v1/2023.findings-acl.669) |  | 0 | Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a... | Atsuki Yamaguchi, Gaku Morio, Hiroaki Ozaki, Terufumi Morishita, Yasuhiro Sogawa |  |
| 1012 |  |  [AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets](https://doi.org/10.18653/v1/2023.findings-acl.670) |  | 0 | High-quality data is essential for conversational recommendation systems and serves as the cornerstone of the network architecture development and training strategy design. Existing works contribute heavy human efforts to manually labeling or designing and extending recommender dialogue templates.... | Junwei Bao, Shuguang Cui, Xiaodong He, Xiaoguang Han, Youzheng Wu, Yu Lu, Zichen Ma |  |
| 1013 |  |  [Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals](https://doi.org/10.18653/v1/2023.findings-acl.671) |  | 0 | The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how & knowing-that task that requires the model to answer factoid-style, procedure-style, and... | Dingnan Jin, Hongru Liang, Jia Liu, Jiancheng Lv, Weihong Du, Wenqiang Lei, Zujie Wen |  |
| 1014 |  |  [Deep Span Representations for Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.672) |  | 0 | Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long entities, a coupling... | Enwei Zhu, Jinpeng Li, Yiyang Liu |  |
| 1015 |  |  [Disambiguated Lexically Constrained Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.673) |  | 0 | Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This... | Chuanqi Dong, Jinpeng Zhang, Ke Wang, Min Zhang, Nini Xiao, Xiangyu Duan, Yuqi Zhang |  |
| 1016 |  |  [Curating Datasets for Better Performance with Example Training Dynamics](https://doi.org/10.18653/v1/2023.findings-acl.674) |  | 0 | The landscape of NLP research is dominated by large-scale models training on colossal datasets, relying on data quantity rather than quality. As an alternative to this landscape, we propose a method for weighing the relative importance of examples in a dataset based on their Example Training... | Aviad SarShalom, Roy Schwartz |  |
| 1017 |  |  [Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking](https://doi.org/10.18653/v1/2023.findings-acl.675) |  | 0 | We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.We... | Iñigo Urteaga, MoulayZaïdane Draïdia, Shahram Khadivi, Tomer Lancewicki |  |
| 1018 |  |  [ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages](https://doi.org/10.18653/v1/2023.findings-acl.676) |  | 0 | Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they... | Chao Pang, Hao Tian, Hua Wu, Shuohuan Wang, Yekun Chai, Yu Sun |  |
| 1019 |  |  [PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts](https://doi.org/10.18653/v1/2023.findings-acl.677) |  | 0 | A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user’s goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models.... | James Caverlee, Xiangjue Dong, Yun He, Ziwei Zhu |  |
| 1020 |  |  [Understanding Programs by Exploiting (Fuzzing) Test Cases](https://doi.org/10.18653/v1/2023.findings-acl.678) |  | 0 | Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs... | Hao Chen, Jianyu Zhao, Yifeng He, Yiwen Guo, Yuyang Rong |  |
| 1021 |  |  [Hybrid Hierarchical Retrieval for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.679) |  | 0 | Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems. Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform... | Lan Liu, Manoj Ghuhan Arivazhagan, Peng Qi, William Yang Wang, Xinchi Chen, Zhiheng Huang |  |
| 1022 |  |  [Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages](https://doi.org/10.18653/v1/2023.findings-acl.680) |  | 0 | In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has... | Andrea Amelio Ravelli, Dominique Brunato, Felice Dell'Orletta, Irene Dini |  |
| 1023 |  |  [Understanding Differential Search Index for Text Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.681) |  | 0 | The Differentiable Search Index (DSI) is a novel information retrieval (IR) framework that utilizes a differentiable function to generate a sorted list of document identifiers in response to a given query. However, due to the black-box nature of the end-to-end neural architecture, it remains to be... | Ben He, Le Sun, Xiaoyang Chen, Yanjiang Liu, Yingfei Sun |  |
| 1024 |  |  [Masked Audio Text Encoders are Effective Multi-Modal Rescorers](https://doi.org/10.18653/v1/2023.findings-acl.682) |  | 0 | Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space... | Anshu Bhatia, Jinglun Cai, Monica Sunkara, Sravan Bodapati, Xiao Pan, Xilai Li |  |
| 1025 |  |  [Replace and Report: NLP Assisted Radiology Report Generation](https://doi.org/10.18653/v1/2023.findings-acl.683) |  | 0 | Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional... | Kaveri Kale, Kshitij S. Jadhav, Pushpak Bhattacharyya |  |
| 1026 |  |  [Pre-trained Personalized Review Summarization with Effective Salience Estimation](https://doi.org/10.18653/v1/2023.findings-acl.684) |  | 0 | Personalized review summarization in recommender systems is a challenging task of generating condensed summaries for product reviews while preserving the salient content of reviews. Recently, Pretrained Language Models (PLMs) have become a new paradigm in text generation for the strong ability of... | Hongtao Liu, Hongyan Xu, Qing Yang, Wenjun Wang, Zhepeng Lv |  |
| 1027 |  |  [CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.685) |  | 0 | Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on factual errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more... | Alexander R. Fabbri, ChienSheng Wu, Jesse Vig, Nazneen Rajani, Prafulla Kumar Choubey, Wenhao Liu |  |
| 1028 |  |  [OpineSum: Entailment-based self-training for abstractive opinion summarization](https://doi.org/10.18653/v1/2023.findings-acl.686) |  | 0 | A typical product or place often has hundreds of reviews, and summarization of these texts is an important and challenging problem. Recent progress on abstractive summarization in domains such as news has been driven by supervised systems trained on hundreds of thousands of news articles paired... | Annie Louis, Joshua Maynez |  |
| 1029 |  |  [A Call for Standardization and Validation of Text Style Transfer Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.687) |  | 0 | Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and... | Marius Kloft, Mayank Kumar Nagda, Phil Ostheimer, Sophie Fellenz |  |
| 1030 |  |  [Bridging the Granularity Gap for Acoustic Modeling](https://doi.org/10.18653/v1/2023.findings-acl.688) |  | 0 | While Transformer has become the de-facto standard for speech, modeling upon the fine-grained frame-level features remains an open challenge of capturing long-distance dependencies and distributing the attention weights. We propose Progressive Down-Sampling (PDS) which gradually compresses the... | Anxiang Ma, Chen Xu, Chengbo Jiao, Chi Hu, Huizhen Wang, Jingbo Zhu, Tong Xiao, Xiaoqian Liu, Xin Zeng, Yuhao Zhang |  |
| 1031 |  |  [MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System](https://doi.org/10.18653/v1/2023.findings-acl.689) |  | 0 | Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The... | Bin Liang, Chenran Cai, Libo Qin, Qiguang Chen, Ruifeng Xu, Shijue Huang, Wanxiang Che, Yudi Zhang |  |
| 1032 |  |  [Learn to Not Link: Exploring NIL Prediction in Entity Linking](https://doi.org/10.18653/v1/2023.findings-acl.690) |  | 0 | Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We... | Fangwei Zhu, Hailong Jin, Jifan Yu, Juanzi Li, Lei Hou, Zhifang Sui |  |
| 1033 |  |  [On Text-based Personality Computing: Challenges and Future Directions](https://doi.org/10.18653/v1/2023.findings-acl.691) |  | 0 | Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the NLP research community. These challenges are organized by the following topics: personality taxonomies, measurement quality,... | Anastasia Giachanou, Ayoub Bagheri, Daniel L. Oberski, ErikJan van Kesteren, Laura Boeschoten, Mahdi Shafiee Kamalabad, Qixiang Fang |  |
| 1034 |  |  [Structured Pruning for Efficient Generative Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.692) |  | 0 | The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deploymentin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we find that in... | Chaofan Tao, Haoli Bai, Jiansheng Wei, Lu Hou, Ngai Wong, Ping Luo, Qun Liu, Xin Jiang |  |
| 1035 |  |  [Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks](https://doi.org/10.18653/v1/2023.findings-acl.693) |  | 0 | Retrieval-augmented methods have received increasing attention to support downstream tasks by leveraging useful information from external resources. Recent studies mainly focus on exploring retrieval to solve knowledge-intensive (KI) tasks. However, the potential of retrieval for most... | Peng Li, Sijie Cheng, Yang Liu, Yile Wang, Zhicheng Guo |  |
| 1036 |  |  [Contextualized Semantic Distance between Highly Overlapped Texts](https://doi.org/10.18653/v1/2023.findings-acl.694) |  | 0 | Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system’s understanding and guides the generation. Since... | Hai Zhao, Letian Peng, Zuchao Li |  |
| 1037 |  |  [Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training](https://doi.org/10.18653/v1/2023.findings-acl.695) |  | 0 | Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the... | Andrew Yates, Changtong Zan, Dacheng Tao, Liang Ding, Yibin Lei, Yu Cao |  |
| 1038 |  |  [Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS](https://doi.org/10.18653/v1/2023.findings-acl.696) |  | 0 | Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use. While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators. This norm assumes that multiple annotators are... | Maitrey Mehta, Vivek Srikumar |  |
| 1039 |  |  [Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification](https://doi.org/10.18653/v1/2023.findings-acl.697) |  | 0 | Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies... | Jasmina Bogojeska, Jonas Kuhn, Katsiaryna Mirylenka, Lukas Wertz |  |
| 1040 |  |  [Improving Classroom Dialogue Act Recognition from Limited Labeled Data with Self-Supervised Contrastive Learning Classifiers](https://doi.org/10.18653/v1/2023.findings-acl.698) |  | 0 | Recognizing classroom dialogue acts has significant promise for yielding insight into teaching, student learning, and classroom dynamics. However, obtaining K-12 classroom dialogue data with labels is a significant challenge, and therefore, developing data-efficient methods for classroom dialogue... | Bradford W. Mott, James C. Lester, Jonathan P. Rowe, Snigdha Chaturvedi, Vikram Kumaran |  |
| 1041 |  |  [Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation](https://doi.org/10.18653/v1/2023.findings-acl.699) |  | 0 | Visual temporal-aligned translation aims to transform the visual sequence into natural words, including important applicable tasks such as lipreading and fingerspelling recognition. However, various performance habits of specific words by different speakers or signers can lead to visual ambiguity,... | Linjun Li, Rongjie Huang, Tao Jin, Wang Lin, Xize Cheng, Ye Wang, Zhou Zhao |  |
| 1042 |  |  [Enhancing Cross-lingual Prompting with Dual Prompt Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.700) |  | 0 | Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. hao and Schütze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In... | Lidong Bing, Meng Zhou, Xin Li, Yue Jiang |  |
| 1043 |  |  [Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI](https://doi.org/10.18653/v1/2023.findings-acl.701) |  | 0 | Users’ physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday... | Alex Mei, Sharon Levy, William Yang Wang |  |
| 1044 |  |  [Multijugate Dual Learning for Low-Resource Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.702) |  | 0 | Dialogue data in real scenarios tend to be sparsely available, rendering data-starved end-to-end dialogue systems trained inadequately. We discover that data utilization efficiency in low-resource scenarios can be enhanced by mining alignment information uncertain utterance and deterministic... | Linyang Li, Shimin Li, Xiaotian Zhang, Xipeng Qiu, Yanjun Zheng |  |
| 1045 |  |  [A Class-Rebalancing Self-Training Framework for Distantly-Supervised Named Entity Recognition](https://doi.org/10.18653/v1/2023.findings-acl.703) |  | 0 | Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly,... | Gaoang Wang, Hongwei Wang, Peng Peng, Qi Li, Tingyu Xie |  |
| 1046 |  |  [MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation](https://doi.org/10.18653/v1/2023.findings-acl.704) |  | 0 | Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical... | Asli Celikyilmaz, Mohit Bansal, Ramakanth Pasunuru, Swarnadeep Saha, Xinyan Yu |  |
| 1047 |  |  [Learning by Analogy: Diverse Questions Generation in Math Word Problem](https://doi.org/10.18653/v1/2023.findings-acl.705) |  | 0 | Solving math word problem (MWP) with AI techniques has recently made great progress with the success of deep neural networks (DNN), but it is far from being solved. We argue that the ability of learning by analogy is essential for an MWP solver to better understand same problems which may typically... | Jie Yao, Kaizhu Huang, Maizhen Ning, Qiufeng Wang, Wei Wang, Xiaowei Huang, Zihao Zhou |  |
| 1048 |  |  [Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.706) |  | 0 | We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning... | Albert Y. S. Lam, Haode Zhang, Haowen Liang, LiMing Zhan, XiaoMing Wu |  |
| 1049 |  |  [Improving Contrastive Learning of Sentence Embeddings from AI Feedback](https://doi.org/10.18653/v1/2023.findings-acl.707) |  | 0 | Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation... | Linyang Li, Qinyuan Cheng, Tianxiang Sun, Xiaogui Yang, Xipeng Qiu |  |
| 1050 |  |  [Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog](https://doi.org/10.18653/v1/2023.findings-acl.708) |  | 0 | Traditional end-to-end task-oriented dialog systems first convert dialog context into belief state and action state before generating the system response. The system response performance is significantly affected by the quality of the belief state and action state. We first explore what dialog... | Haipeng Sun, Junwei Bao, Xiaodong He, Youzheng Wu |  |
| 1051 |  |  [Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.709) |  | 0 | The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model... | Bowen Jin, Jiawei Han, Jimeng Sun, Pengcheng Jiang, Shivam Agarwal, Xuan Wang |  |
| 1052 |  |  [Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion](https://doi.org/10.18653/v1/2023.findings-acl.710) |  | 0 | Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion... | Cutter Dalton, Dananjay Srinivas, Greg Durrett, Junyi Jessy Li, WeiJen Ko, Yating Wu |  |
| 1053 |  |  [An Integrated Approach for Political Bias Prediction and Explanation Based on Discursive Structure](https://doi.org/10.18653/v1/2023.findings-acl.711) |  | 0 | One crucial aspect of democracy is fair information sharing. While it is hard to prevent biases in news, they should be identified for better transparency. We propose an approach to automatically characterize biases that takes into account structural differences and that is efficient for long... | Chloé Braud, Nicolas Devatine, Philippe Muller |  |
| 1054 |  |  [Smart Word Suggestions for Writing Assistance](https://doi.org/10.18653/v1/2023.findings-acl.712) |  | 0 | Enhancing word usage is a desired feature for writing assistance. To further advance research in this area, this paper introduces “Smart Word Suggestions” (SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario.... | Chenshuo Wang, Dongyan Zhao, Jonathan Tien, Shaoguang Mao, Tao Ge, Wenshan Wu, Xun Wang, Yan Xia |  |
| 1055 |  |  [JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions](https://doi.org/10.18653/v1/2023.findings-acl.713) |  | 0 | Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We proposea new commonsense reasoning dataset based on human’s Interactive Fiction (IF) gameplaywalkthroughs as human players... | Chuang Gan, Michael A. Greenspan, Mo Yu, Murray Campbell, Xiaodan Zhu, Xiaoxiao Guo, Yi Gu, Yufei Feng |  |
| 1056 |  |  [A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models](https://doi.org/10.18653/v1/2023.findings-acl.714) |  | 0 | Distillation from Weak Teacher (DWT) is a method of transferring knowledge from a smaller, weaker teacher model to a larger student model to improve its performance. Previous studies have shown that DWT can be effective in the vision domain and natural language processing (NLP) pre-training stage.... | Alexander Min, Davis Liang, Hayeon Lee, Jongpil Kim, Rui Hou, Sung Ju Hwang |  |
| 1057 |  |  [SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation](https://doi.org/10.18653/v1/2023.findings-acl.715) |  | 0 | Logical data-to-text generation is a representative task in measuring the capabilities of both language generation and complex reasoning. Despite the introduction of reasoning skills in generation, existing works still rely on neural language models to output the final table description. However,... | Lemao Liu, Lingpeng Kong, Rui Yan, Shuming Shi, Tingchen Fu, Xueliang Zhao |  |
| 1058 |  |  [Boosting Event Extraction with Denoised Structure-to-Text Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.716) |  | 0 | Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data... | Bo Wang, Chong Feng, Dawei Yin, Ge Shi, Heyan Huang, Shuaiqiang Wang, Tong Zhou, Xiao Liu, Xiaochi Wei |  |
| 1059 |  |  [Detecting Adversarial Samples through Sharpness of Loss Landscape](https://doi.org/10.18653/v1/2023.findings-acl.717) |  | 0 | Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep... | Menghan Zhang, Qi Zhang, Qin Liu, Rui Zheng, Shihan Dou, Tao Gui, Xuanjing Huang, Yuhao Zhou, Zhongyu Wei |  |
| 1060 |  |  [A Simple, Yet Effective Approach to Finding Biases in Code Generation](https://doi.org/10.18653/v1/2023.findings-acl.718) |  | 0 | Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from... | Henryk Michalewski, Mateusz Malinowski, Spyridon Mouselinos |  |
| 1061 |  |  [Membership Inference Attacks against Language Models via Neighbourhood Comparison](https://doi.org/10.18653/v1/2023.findings-acl.719) |  | 0 | Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend toassign higher... | Bernhard Schölkopf, Fatemehsadat Mireshghallah, Justus Mattern, Mrinmaya Sachan, Taylor BergKirkpatrick, Zhijing Jin |  |
| 1062 |  |  [CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation](https://doi.org/10.18653/v1/2023.findings-acl.720) |  | 0 | We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL)... | Kahini Wadhawan, Rahul Madhavan, Rishabh Garg, Sameep Mehta |  |
| 1063 |  |  [Can Diffusion Model Achieve Better Performance in Text Generation ? Bridging the Gap between Training and Inference !](https://doi.org/10.18653/v1/2023.findings-acl.721) |  | 0 | Diffusion models have been successfully adapted to text generation tasks by mapping the discrete text into the continuous space. However, there exist nonnegligible gaps between training and inference, owing to the absence of the forward process during inference. Thus, the model only predicts based... | Juntao Li, Keyan Zhou, Min Zhang, Pinzheng Wang, Zecheng Tang, Ziqiang Cao |  |
| 1064 |  |  [Topic-Guided Self-Introduction Generation for Social Media Users](https://doi.org/10.18653/v1/2023.findings-acl.722) |  | 0 | Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user’s personal interests. While most prior work profiling users with tags (e.g., ages),... | Chunpu Xu, Jing Li, Min Yang, Piji Li |  |
| 1065 |  |  [Recyclable Tuning for Continual Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.723) |  | 0 | Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when... | Cheng Qian, Huadong Wang, Jie Zhou, Maosong Sun, Ruobing Xie, Xu Han, Yankai Lin, Yujia Qin, Zhiyuan Liu |  |
| 1066 |  |  [BLOCSUM: Block Scope-based Source Code Summarization via Shared Block Representation](https://doi.org/10.18653/v1/2023.findings-acl.724) |  | 0 | Code summarization, which aims to automatically generate natural language descriptions from source code, has become an essential task in software development for better program understanding. Abstract Syntax Tree (AST), which represents the syntax structure of the source code, is helpful when... | Hyojun Kim, JeeHyong Lee, YunSeok Choi |  |
| 1067 |  |  [HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.725) |  | 0 | With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works... | Qun Liu, Wenya Guo, Xiaojun Meng, Xin Jiang, Yadao Wang, Yasheng Wang, Zhengkun Zhang, Zhenglu Yang |  |
| 1068 |  |  [Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations](https://doi.org/10.18653/v1/2023.findings-acl.726) |  | 0 | We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing... | ChoJui Hsieh, Jianhan Xu, Jinshu Lin, KaiWei Chang, Xiaoqing Zheng, Xuanjing Huang, Zixuan Ling |  |
| 1069 |  |  [Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training](https://doi.org/10.18653/v1/2023.findings-acl.727) |  | 0 | Relation Extraction (RE) is the task of identifying semantic relation between real-world entities mentioned in text. Despite significant progress in RE research, a remaining challenge for RE concerns the lack of training data for data-hungry deep learning models. Cost of annotation and difficulty... | Amir Pouran Ben Veyseh, Bonan Min, Franck Dernoncourt, Thien Huu Nguyen |  |
| 1070 |  |  [Disfluency Generation for More Robust Dialogue Systems](https://doi.org/10.18653/v1/2023.findings-acl.728) |  | 0 | Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only... | Benjamin Marie |  |
| 1071 |  |  [Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting](https://doi.org/10.18653/v1/2023.findings-acl.729) |  | 0 | Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly... | Aixin Sun, Bing Li, Chen Chen, KwokYan Lam, Yufei Wang |  |
| 1072 |  |  [Revisiting Pathologies of Neural Models under Input Reduction](https://doi.org/10.18653/v1/2023.findings-acl.730) |  | 0 | We revisit the question of why neural models tend to produce high-confidence predictions on inputs that appear nonsensical to humans. Previous work has suggested that the models fail to assign low probabilities to such inputs due to model overconfidence. We evaluate various regularization methods... | Canasai Kruengkrai, Junichi Yamagishi |  |
| 1073 |  |  [Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.731) |  | 0 | Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way... | Fei Yuan, Jingjing Xu, Lei Li, Lingpeng Kong, Wenhao Zhu, Yinquan Lu, Yu Qiao |  |
| 1074 |  |  [FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference](https://doi.org/10.18653/v1/2023.findings-acl.732) |  | 0 | Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal... | Fei Sha, Joshua Ainslie, Michiel de Jong, Nicholas FitzGerald, Sumit Sanghai, William W. Cohen, Yury Zemlyanskiy |  |
| 1075 |  |  [Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark](https://doi.org/10.18653/v1/2023.findings-acl.733) |  | 0 | Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing... | Esben Kran, Fazl Barez, Ioannis Konstas, Jason HoelscherObermaier, Julia Persson |  |
| 1076 |  |  [Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data](https://doi.org/10.18653/v1/2023.findings-acl.734) |  | 0 | This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for... | Chenyan Xiong, Ge Yu, Shi Yu, Xinze Li, Yu Gu, Zhenghao Liu, Zhiyuan Liu |  |
| 1077 |  |  [Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt](https://doi.org/10.18653/v1/2023.findings-acl.735) |  | 0 | We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for... | Daling Wang, Pengfei Hong, Qi Sun, Shi Feng, Soujanya Poria, Wenfang Wu, Xiaocui Yang, Yifei Zhang |  |
| 1078 |  |  [Predicting Human Translation Difficulty Using Automatic Word Alignment](https://doi.org/10.18653/v1/2023.findings-acl.736) |  | 0 | Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these... | Charles Kemp, Ekaterina Vylomova, Trevor Cohn, Zheng Wei Lim |  |
| 1079 |  |  [Know Where You're Going: Meta-Learning for Parameter-Efficient Fine-Tuning](https://doi.org/10.18653/v1/2023.findings-acl.737) |  | 0 | A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on... | Jonathan May, Mozhdeh Gheini, Xuezhe Ma |  |
| 1080 |  |  [Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking](https://doi.org/10.18653/v1/2023.findings-acl.738) |  | 0 | Neural information retrieval (IR) systems have progressed rapidly in recent years, in large part due to the release of publicly available benchmarking tasks. Unfortunately, some dimensions of this progress are illusory: the majority of the popular IR benchmarks today focus exclusively on downstream... | Avi Sil, Christopher Potts, Jon SaadFalcon, Keshav Santhanam, Martin Franz, Matei Zaharia, Md. Arafat Sultan, Omar Khattab, Radu Florian, Salim Roukos |  |
| 1081 |  |  [AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese](https://doi.org/10.18653/v1/2023.findings-acl.739) |  | 0 | Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern... | Abhijnan Nath, Nikhil Krishnaswamy, Sheikh Mannan |  |
| 1082 |  |  [An Exploratory Study on Model Compression for Text-to-SQL](https://doi.org/10.18653/v1/2023.findings-acl.740) |  | 0 | Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require... | Bin Chen, Jian Su, Shuo Sun, Shuqi Sun, Yingzhan Lin, Yuchen Zhang, Yuze Gao |  |
| 1083 |  |  [FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models](https://doi.org/10.18653/v1/2023.findings-acl.741) |  | 0 | Stutter removal is an essential scenario in the field of speech editing. However, when the speech recording contains stutters, the existing text-based speech editing approaches still suffer from: 1) the over-smoothing problem in the edited speech; 2) lack of robustness due to the noise introduced... | Jialong Zuo, Qian Yang, Rongjie Huang, Yi Ren, Zhenhui Ye, Zhou Zhao, Ziyue Jiang |  |
| 1084 |  |  [HyHTM: Hyperbolic Geometry-based Hierarchical Topic Model](https://doi.org/10.18653/v1/2023.findings-acl.742) |  | 0 | Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be... | Balaji Krishnamurthy, Nikaash Puri, Nikitha Srikanth, Simra Shahid, Sumit Bhatia, Tanay Anand |  |
| 1085 |  |  [KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding](https://doi.org/10.18653/v1/2023.findings-acl.743) |  | 0 | Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of... | Jifan Yu, Juanzi Li, Lei Hou, Shulin Cao, Xin Lv, Yantao Liu, Zijun Yao |  |
| 1086 |  |  [DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies](https://doi.org/10.18653/v1/2023.findings-acl.744) |  | 0 | Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However,... | Dinesh Raghu, Mausam, Rocktim Jyoti Das, Vishal Vivek Saley |  |
| 1087 |  |  [Scale-Invariant Infinite Hierarchical Topic Model](https://doi.org/10.18653/v1/2023.findings-acl.745) |  | 0 | Hierarchical topic models have been employed to organize a large number of diverse topics from corpora into a latent tree structure. However, existing models yield fragmented topics with overlapping themes whose expected probability becomes exponentially smaller along the depth of the tree. To... | Daichi Mochihashi, Shusei Eshima |  |
| 1088 |  |  [RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training](https://doi.org/10.18653/v1/2023.findings-acl.746) |  | 0 | Multilingual vision-language (V&L) pre-training has achieved remarkable progress in learning universal representations across different modalities and languages. In spite of recent success, there still remain challenges limiting further improvements of V&L pre-trained models in multilingual... | Chulun Zhou, Fandong Meng, Jie Zhou, Jinan Xu, Jinsong Su, Yunlong Liang |  |
| 1089 |  |  [Deep Equilibrium Non-Autoregressive Sequence Learning](https://doi.org/10.18653/v1/2023.findings-acl.747) |  | 0 | In this work, we argue that non-autoregressive (NAR) sequence generative models can equivalently be regarded as an iterative refinement process towards the target sequence, implying an underlying dynamical system of NAR model: z = f (z, x) → y. In such a way, the optimal prediction of a NAR model... | Hao Zhou, Yi Zhou, Zaixiang Zheng |  |
| 1090 |  |  [ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.748) |  | 0 | With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create... | Chao Zhang, Jiaming Shen, Rongzhi Zhang, Yu Meng, Yuchen Zhuang, Yue Yu |  |
| 1091 |  |  [Race, Gender, and Age Biases in Biomedical Masked Language Models](https://doi.org/10.18653/v1/2023.findings-acl.749) |  | 0 | Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora.... | Junghwan Kim, Kristen Marie Johnson, Michelle Kim |  |
| 1092 |  |  [Neighboring Words Affect Human Interpretation of Saliency Explanations](https://doi.org/10.18653/v1/2023.findings-acl.750) |  | 0 | Word-level saliency explanations (“heat maps over words”) are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to... | Alon Jacovi, Heike Adel, Hendrik Schuff, Ngoc Thang Vu, Yoav Goldberg |  |
| 1093 |  |  [HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models](https://doi.org/10.18653/v1/2023.findings-acl.751) |  | 0 | Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable... | Elnaz Nouri, Swaroop Mishra |  |
| 1094 |  |  [Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification](https://doi.org/10.18653/v1/2023.findings-acl.752) |  | 0 | Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies... | Anni Zou, Hai Zhao, Zhuosheng Zhang |  |
| 1095 |  |  [DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect](https://doi.org/10.18653/v1/2023.findings-acl.753) |  | 0 | Recently, binaural audio synthesis (BAS) has emerged as a promising research field for its applications in augmented and virtual realities. Binaural audio helps ususers orient themselves and establish immersion by providing the brain with interaural time differences reflecting spatial information.... | Jinglin Liu, Qian Chen, Qinglin Zhang, Siqi Zheng, Wen Wang, Zhenhui Ye, Zhou Zhao |  |
| 1096 |  |  [Easy-to-Hard Learning for Information Extraction](https://doi.org/10.18653/v1/2023.findings-acl.754) |  | 0 | Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has... | Chang Gao, Lidong Bing, Wai Lam, Wenxuan Zhang |  |
| 1097 |  |  [SConE: Simplified Cone Embeddings with Symbolic Operators for Complex Logical Queries](https://doi.org/10.18653/v1/2023.findings-acl.755) |  | 0 | Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (FOL) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods... | Chau D. M. Nguyen, Michael Stewart, Tim French, Wei Liu |  |
| 1098 |  |  [Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors](https://doi.org/10.18653/v1/2023.findings-acl.756) |  | 0 | The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually... | Juan Cao, Peng Qi, TatSeng Chua, Wei Ji, Yufeng Shen, Yuyang Zhao |  |
| 1099 |  |  [An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts](https://doi.org/10.18653/v1/2023.findings-acl.757) |  | 0 | With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for... | Amirmohammad Shahbandegan, Amrit Chadha, Muskan Garg, Vijay Mago |  |
| 1100 |  |  [Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control](https://doi.org/10.18653/v1/2023.findings-acl.758) |  | 0 | Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the... | LouisPhilippe Morency, Paul Pu Liang, Ruslan Salakhutdinov, Xiang Fan, Yiwei Lyu |  |
| 1101 |  |  [Connectivity Patterns are Task Embeddings](https://doi.org/10.18653/v1/2023.findings-acl.759) |  | 0 | Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as... | Mingming Sun, Minlong Peng, Qi Zhang, Rui Zheng, Tao Gui, Xuanjing Huang, Yuansen Zhang, Zhiheng Xi, Zhongyu Wei |  |
| 1102 |  |  [Improving Autoregressive Grammatical Error Correction with Non-autoregressive Models](https://doi.org/10.18653/v1/2023.findings-acl.760) |  | 0 | Grammatical Error Correction (GEC) aims to correct grammatical errors in sentences. We find that autoregressive models tend to assign low probabilities to tokens that need corrections. Here we introduce additional signals to the training of GEC models so that these systems can learn to better... | Baoyu Hou, Chi Hu, Hang Cao, Jingbo Zhu, Tong Xiao, Zhiquan Cao |  |
| 1103 |  |  [SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives](https://doi.org/10.18653/v1/2023.findings-acl.761) |  | 0 | Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from the same... | Enrique Alfonseca, Fedor Moiseev, Gustavo Hernández Abrego, Imed Zitouni, Péter Dornbach, Zhe Dong |  |
| 1104 |  |  [On the Strength of Sequence Labeling and Generative Models for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2023.findings-acl.762) |  | 0 | Generative models have achieved great success in aspect sentiment triplet extraction tasks. However, existing methods ignore the mutual informative clues between aspect and opinion terms and may generate false paired triplets. Furthermore, the inherent limitations of generative models, i.e., the... | Shen Zhou, Tieyun Qian |  |
| 1105 |  |  [Revisiting Non-Autoregressive Translation at Scale](https://doi.org/10.18653/v1/2023.findings-acl.763) |  | 0 | In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT... | Jinsong Su, Junfeng Yao, Longyue Wang, Zhaopeng Tu, Zhihao Wang |  |
| 1106 |  |  [Improving Radiology Summarization with Radiograph and Anatomy Prompts](https://doi.org/10.18653/v1/2023.findings-acl.764) |  | 0 | The impression is crucial for the referring physicians to grasp key information since it is concluded from the findings and reasoning of radiologists. To alleviate the workload of radiologists and reduce repetitive human labor in impression writing, many researchers have focused on automatic... | Jinpeng Hu, TsungHui Chang, Xiang Wan, Yang Liu, Zhihong Chen |  |
| 1107 |  |  [Explanation Regeneration via Information Bottleneck](https://doi.org/10.18653/v1/2023.findings-acl.765) |  | 0 | Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the... | Lingpeng Kong, Qintong Li, Wei Bi, Zhiyong Wu |  |
| 1108 |  |  [Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization](https://doi.org/10.18653/v1/2023.findings-acl.766) |  | 0 | The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model... | Haifeng Wang, Hua Wu, Liwen Zhang, Pengzhi Gao, Zhongjun He |  |
| 1109 |  |  [ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision](https://doi.org/10.18653/v1/2023.findings-acl.767) |  | 0 | Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive... | Jiawei Han, Ming Zhong, Minhao Jiang, Siru Ouyang, Vivian Hu, Xuan Wang, Yizhu Jiao |  |
| 1110 |  |  [Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.768) |  | 0 | We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to... | James R. Glass, ShangWen Li, Wei Fang, Wentau Yih, YungSung Chuang |  |
| 1111 |  |  [Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets](https://doi.org/10.18653/v1/2023.findings-acl.769) |  | 0 | We propose a semi-supervised text classifier based on self-training using one positive and one negative property of neural networks. One of the weaknesses of self-training is the semantic drift problem, where noisy pseudo-labels accumulate over iterations and consequently the error rate soars. In... | Payam Karisani |  |
| 1112 |  |  [Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training](https://doi.org/10.18653/v1/2023.findings-acl.770) |  | 0 | Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations... | Christopher Potts, Jing Huang, Kyle Mahowald, Zhengxuan Wu |  |
| 1113 |  |  [Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning](https://doi.org/10.18653/v1/2023.findings-acl.771) |  | 0 | Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive... | Bernd Bischl, Daniel Saggau, Ilias Chalkidis, Mina Rezaei |  |
| 1114 |  |  [QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition](https://doi.org/10.18653/v1/2023.findings-acl.772) |  | 0 | Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (i.e., textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of... | Chuanpeng Yang, Fuqing Zhu, Songlin Hu, Yan Zhou, Yaxin Liu, Ziming Li |  |
| 1115 |  |  [Language acquisition: do children and language models follow similar learning stages?](https://doi.org/10.18653/v1/2023.findings-acl.773) |  | 0 | During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning... | JeanRémi King, Linnea Evanson, Yair Lakretz |  |
| 1116 |  |  [The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing](https://doi.org/10.18653/v1/2023.findings-acl.774) |  | 0 | In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query... | Chris Biemann, Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck |  |
| 1117 |  |  [UniCOQE: Unified Comparative Opinion Quintuple Extraction As A Set](https://doi.org/10.18653/v1/2023.findings-acl.775) |  | 0 | Comparative Opinion Quintuple Extraction (COQE) aims to identify comparative opinion sentences in product reviews, extract comparative opinion elements in the sentences, and then incorporate them into quintuples. Existing methods decompose the COQE task into multiple primary subtasks and then solve... | Feng Xu, Jianfei Yu, Rui Xia, Zinong Yang |  |
| 1118 |  |  [Response-conditioned Turn-taking Prediction](https://doi.org/10.18653/v1/2023.findings-acl.776) |  | 0 | Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it... | Bing'er Jiang, Erik Ekstedt, Gabriel Skantze |  |
| 1119 |  |  [A Unified One-Step Solution for Aspect Sentiment Quad Prediction](https://doi.org/10.18653/v1/2023.findings-acl.777) |  | 0 | Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity,... | Haiqin Yang, Hao Mou, Junbo Yang, Junxian Zhou, Yuxuan He |  |
| 1120 |  |  [On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning](https://doi.org/10.18653/v1/2023.findings-acl.778) |  | 0 | Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide... | Chenghao Xiao, Noura Al Moubayed, Yang Long |  |
| 1121 |  |  [Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.779) |  | 0 | Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows... | Dietrich Klakow, Marius Mosbach, Shauli Ravfogel, Tiago Pimentel, Yanai Elazar |  |
| 1122 |  |  [Common Law Annotations: Investigating the Stability of Dialog System Output Annotations](https://doi.org/10.18653/v1/2023.findings-acl.780) |  | 0 | Metrics for Inter-Annotator Agreement (IAA), like Cohen’s Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure or reproducibility. While researchers are encouraged to increase... | Aditya Singhal, Alexandra DeLucia, Britney Ngaw, João Sedoc, Lining Zhang, Nikita Nangia, Praneeth Ganedi, Rubing Li, Ryan Guan, Seunggun Lee, Shalaka Vaidya, Zijun Yuan |  |
| 1123 |  |  [HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models](https://doi.org/10.18653/v1/2023.findings-acl.781) |  | 0 | Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a... | Deyi Xiong, Yuqi Ren |  |
| 1124 |  |  [PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data](https://doi.org/10.18653/v1/2023.findings-acl.782) |  | 0 | Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs. Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality... | Fatemeh Azadi, Heshaam Faili, Mohammad Javad Dousti |  |
| 1125 |  |  [Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities](https://doi.org/10.18653/v1/2023.findings-acl.783) |  | 0 | Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs. Conversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives. However,... | Riccardo Orlando, Roberto Navigli, Simone Conia |  |
| 1126 |  |  [DSPM-NLG: A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System](https://doi.org/10.18653/v1/2023.findings-acl.784) |  | 0 | In few-shot settings, fully conveying the semantic information of the dialogue act is a crucial challenge for Natural Language Generation (NLG) in the task-oriented dialogue system. An interesting fact is that NLG and Spoken Language Understanding (SLU) are a natural dual problem pair. Suppose the... | Ai Ti Aw, Bowei Zou, Rui Fan, Tingting He, Yufan Wang |  |
| 1127 |  |  [TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.findings-acl.785) |  | 0 | Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt (Xiang et al., 2022) has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three... | Bang Wang, Chao Liang, Wei Xiang |  |
| 1128 |  |  [Evaluating Factuality in Cross-lingual Summarization](https://doi.org/10.18653/v1/2023.findings-acl.786) |  | 0 | Cross-lingual summarization aims to help people efficiently grasp the core idea of the document written in a foreign language. Modern text summarization models generate highly fluent but often factually inconsistent outputs, which has received heightened attention in recent research. However, the... | Mingqi Gao, Wenqing Wang, Xiaojun Wan, Yuemei Xu |  |
| 1129 |  |  [On the Correspondence between Compositionality and Imitation in Emergent Neural Communication](https://doi.org/10.18653/v1/2023.findings-acl.787) |  | 0 | Compositionality is a hallmark of human language that not only enables linguistic generalization, but also potentially facilitates acquisition. When simulating language emergence with neural networks, compositionality has been shown to improve communication performance; however, its impact on... | Emily Cheng, Mathieu Rita, Thierry Poibeau |  |
| 1130 |  |  [The Coreference under Transformation Labeling Dataset: Entity Tracking in Procedural Texts Using Event Models](https://doi.org/10.18653/v1/2023.findings-acl.788) |  | 0 | We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to... | Bingyang Ye, Eben Holderness, James Pustejovsky, Jingxuan Tu, Kyeongmin Rim, Marc Verhagen |  |
| 1131 |  |  [Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution](https://doi.org/10.18653/v1/2023.findings-acl.789) |  | 0 | Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language... | Kenton Murray, Tianjian Li |  |
| 1132 |  |  [Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation](https://doi.org/10.18653/v1/2023.findings-acl.790) |  | 0 | In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for... | ChenHua Huang, HanCheng Yu, HuiJuan Wang, JuiChing Tsou, KaiYu Hsieh, YaoChung Fan, YuAn Shih |  |
| 1133 |  |  [Lexical Translation Inconsistency-Aware Document-Level Translation Repair](https://doi.org/10.18653/v1/2023.findings-acl.791) |  | 0 | Following the idea of “one translation per discourse”, in this paper we aim to improve translation consistency via document-level translation repair (DocRepair), i.e., automatic post-editing on translations of documents. To this end, we propose a lexical translation inconsistency-aware DocRepair to... | Hao Yang, Junhui Li, Shimin Tao, Zhen Zhang |  |
| 1134 |  |  [CausalDialogue: Modeling Utterance-level Causality in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.792) |  | 0 | Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further... | Alon Albalak, Connor Pryor, Lise Getoor, Michael Saxon, Wenda Xu, William Yang Wang, YiLin Tuan |  |
| 1135 |  |  [Towards Unified Spoken Language Understanding Decoding via Label-aware Compact Linguistics Representations](https://doi.org/10.18653/v1/2023.findings-acl.793) |  | 0 | Joint intent detection and slot filling models have shown promising success in recent years due to the high correlations between the two tasks. However, previous works independently decode the two tasks, which could result in misaligned predictions for both tasks. To address this shortcoming, we... | Dongsheng Chen, Xuxin Cheng, Yuexian Zou, Zhihong Zhu, Zhiqi Huang |  |
| 1136 |  |  [Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses](https://doi.org/10.18653/v1/2023.findings-acl.794) |  | 0 | A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user.... | Greg Durrett, Justin F. Rousseau, Liyan Tang, Yanshan Wang, Yifan Peng, Ying Ding |  |
| 1137 |  |  [Language Modeling with Latent Situations](https://doi.org/10.18653/v1/2023.findings-acl.795) |  | 0 | Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition... | Belinda Z. Li, Jacob Andreas, Maxwell I. Nye |  |
| 1138 |  |  [Can Cross-Lingual Transferability of Multilingual Transformers Be Activated Without End-Task Data?](https://doi.org/10.18653/v1/2023.findings-acl.796) |  | 0 | Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer... | Heyan Huang, XianLing Mao, Zewen Chi |  |
| 1139 |  |  [Focus-aware Response Generation in Inquiry Conversation](https://doi.org/10.18653/v1/2023.findings-acl.797) |  | 0 | Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation... | Adam Jatowt, Changlong Sun, Fei Wu, Jun Feng, Kun Kuang, Weiming Lu, Yating Zhang, Yiquan Wu |  |
| 1140 |  |  [A Hierarchical Explanation Generation Method Based on Feature Interaction Detection](https://doi.org/10.18653/v1/2023.findings-acl.798) |  | 0 | The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical... | Jun Zhao, Kang Liu, Yiming Ju, Yuanzhe Zhang |  |
| 1141 |  |  [Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning](https://doi.org/10.18653/v1/2023.findings-acl.799) |  | 0 | Efficient finetuning of pretrained language transformers is becoming increasingly prevalent for solving natural language processing tasks. While effective, it can still require a large number of tunable parameters. This can be a drawback for low-resource applications and training with... | Aram Galstyan, Greg Ver Steeg, Umang Gupta |  |
| 1142 |  |  [A Diffusion Model for Event Skeleton Generation](https://doi.org/10.18653/v1/2023.findings-acl.800) |  | 0 | Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a... | Bing Qin, Fangqi Zhu, Haiqin Yang, Jun Gao, Lin Zhang, Ruifeng Xu |  |
| 1143 |  |  [Nonparametric Decoding for Generative Retrieval](https://doi.org/10.18653/v1/2023.findings-acl.801) |  | 0 | The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval... | Hanseok Oh, Hoyeon Chang, Hyunji Lee, Jaeyoung Kim, Minjoon Seo, Sohee Yang, Vladimir Karpukhin, Yi Lu |  |
| 1144 |  |  [Aspect-aware Unsupervised Extractive Opinion Summarization](https://doi.org/10.18653/v1/2023.findings-acl.802) |  | 0 | Extractive opinion summarization extracts sentences from users’ reviews to represent the prevalent opinions about a product or service. However, the extracted sentences can be redundant and may miss some important aspects, especially for centroid-based extractive summarization models (Radev et al.,... | Haoyuan Li, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 1145 |  |  [GNN-SL: Sequence Labeling Based on Nearest Examples via GNN](https://doi.org/10.18653/v1/2023.findings-acl.803) |  | 0 | To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging... | Guoyin Wang, Jiwei Li, Lingjuan Lyu, Rongbin Ouyang, Shuhe Wang, Tianwei Zhang, Yuxian Meng |  |
| 1146 |  |  [Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction](https://doi.org/10.18653/v1/2023.findings-acl.804) |  | 0 | Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely... | Wei Hu, Xinyi Wang, Zitao Wang |  |
| 1147 |  |  [Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond](https://doi.org/10.18653/v1/2023.findings-acl.805) |  | 0 | Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both... | Alolika Gon, Andrew McCallum, HawShiuan Chang, Hong Yu, Zonghai Yao |  |
| 1148 |  |  [GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective](https://doi.org/10.18653/v1/2023.findings-acl.806) |  | 0 | Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks,... | Hanmeng Liu, Jindong Wang, Libo Qin, Linyi Yang, Shuibai Zhang, Xing Xie, Yafu Li, Yidong Wang, Yue Zhang |  |
| 1149 |  |  [Investigating the Saliency of Sentiment Expressions in Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.807) |  | 0 | We examine the behaviour of an aspect-based sentiment classifier built by fine-tuning the BERT BASE model on the SemEval 2016 English dataset. In a set of masking experiments, we examine the extent to which the tokens identified as salient by LIME and a gradient-based method are being used by the... | Jennifer Foster, Joachim Wagner |  |
| 1150 |  |  [DMLM: Descriptive Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.808) |  | 0 | Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of... | Edoardo Barba, Niccolò Campolungo, Roberto Navigli |  |
| 1151 |  |  [Reproducibility in NLP: What Have We Learned from the Checklist?](https://doi.org/10.18653/v1/2023.findings-acl.809) |  | 0 | Scientific progress in NLP rests on the reproducibility of researchers’ claims. The \*CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining... | Ian Magnusson, Jesse Dodge, Noah A. Smith |  |
| 1152 |  |  [Domain Generalization via Switch Knowledge Distillation for Robust Review Representation](https://doi.org/10.18653/v1/2023.findings-acl.810) |  | 0 | Applying neural models injected with in-domain user and product information to learn review representations of unseen or anonymous users incurs an obvious obstacle in content-based recommender systems. For the generalization of the in-domain classifier, most existing models train an extra... | Dan Xu, Jin Wang, LiangChih Yu, Xuejie Zhang, You Zhang |  |
| 1153 |  |  [On Search Strategies for Document-Level Neural Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.811) |  | 0 | Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture... | Christian Herold, Hermann Ney |  |
| 1154 |  |  [Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.findings-acl.812) |  | 0 | Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability. MRC models based on LMs may overuse the name... | Jiazheng Zhu, Shaojuan Wu, Xiaowang Zhang, Yuexian Hou, Zhiyong Feng |  |
| 1155 |  |  [Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias](https://doi.org/10.18653/v1/2023.findings-acl.813) |  | 0 | While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and... | David Beaver, Junyi Jessy Li, Kyle Mahowald, Venkata Subrahmanyan Govindarajan |  |
| 1156 |  |  [SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme](https://doi.org/10.18653/v1/2023.findings-acl.814) |  | 0 | Although lyrics generation has achieved significant progress in recent years, it has limited practical applications because the generated lyrics cannot be performed without composing compatible melodies. In this work, we bridge this practical gap by proposing a song rewriting system which rewrites... | DitYan Yeung, Liangyou Li, Qun Liu, Yusen Sun |  |
| 1157 |  |  [Triplet-Free Knowledge-Guided Response Generation](https://doi.org/10.18653/v1/2023.findings-acl.815) |  | 0 | Generating vivid and informative responses (e.g., comments for social posts and utterances for dialogues) is challenging without giving relevant knowledge. Prior works focus on constructing the ”latent” knowledge first and then learning how to ”ground” it based on pseudo (context, knowledge,... | Baoyuan Wang, Dongming Li, Jianfeng Liu |  |
| 1158 |  |  [Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.findings-acl.816) |  | 0 | Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art... | Lizhong Chen, Matthew Raffel |  |
| 1159 |  |  [Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance](https://doi.org/10.18653/v1/2023.findings-acl.817) |  | 0 | Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial... | Dingyi Zeng, Hong Qu, Shaohuan Cheng, Wanlong Liu |  |
| 1160 |  |  [Exploring the Impact of Vision Features in News Image Captioning](https://doi.org/10.18653/v1/2023.findings-acl.818) |  | 0 | The task of news image captioning aims to generate a detailed caption which describes the specific information of an image in a news article. However, we find that recent state-of-art models can achieve competitive performance even without vision features. To resolve the impact of vision features... | Junzhe Zhang, Xiaojun Wan |  |
| 1161 |  |  [Using Collostructional Analysis to evaluate BERT's representation of linguistic constructions](https://doi.org/10.18653/v1/2023.findings-acl.819) |  | 0 | Collostructional analysis is a technique devised to find correlations between particular words and linguistic constructions in order to analyse meaning associations of these constructions. Contrasting collostructional analysis results with output from BERT might provide insights into the way BERT... | Jelke Bloem, Tim Veenboer |  |
| 1162 |  |  [Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation](https://doi.org/10.18653/v1/2023.findings-acl.820) |  | 0 | Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way... | Emery Fine, Hélène Sauzéon, PierreYves Oudeyer, Rania Abdelghani, Tong Wang, Xingdi Yuan, YenHsiang Wang |  |
| 1163 |  |  [Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.821) |  | 0 | Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge... | Fan Qian, Guibin Zheng, Jiqing Han, Tieran Zheng, Yongjun He |  |
| 1164 |  |  [Theory of Mind in Freely-Told Children's Narratives: A Classification Approach](https://doi.org/10.18653/v1/2023.findings-acl.822) |  | 0 | Children are the focal point for studying the link between language and Theory of Mind (ToM) competence. Language and ToM are often studied with younger children and standardized tests, but as both are social competences, data and methods with higher ecological validity are critical. We leverage a... | Bram van Dijk, Marco Spruit, Max J. van Duijn |  |
| 1165 |  |  [Better Language Models of Code through Self-Improvement](https://doi.org/10.18653/v1/2023.findings-acl.823) |  | 0 | Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve... | Hung Quoc To, Jin L. C. Guo, Nghi D. Q. Bui, Tien N. Nguyen |  |
| 1166 |  |  [Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them](https://doi.org/10.18653/v1/2023.findings-acl.824) |  | 0 | BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported... | Aakanksha Chowdhery, Denny Zhou, Ed H. Chi, Hyung Won Chung, Jason Wei, Mirac Suzgun, Nathan Scales, Nathanael Schärli, Quoc V. Le, Sebastian Gehrmann, Yi Tay |  |
| 1167 |  |  [Score It All Together: A Multi-Task Learning Study on Automatic Scoring of Argumentative Essays](https://doi.org/10.18653/v1/2023.findings-acl.825) |  | 0 | When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. On the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from... | Andrea Horbach, Marie Bexte, Yuning Ding |  |
| 1168 |  |  [Data Sampling and (In)stability in Machine Translation Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.826) |  | 0 | We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling... | Chikiu Lo, Rebecca Knowles |  |
| 1169 |  |  [Probing Graph Decomposition for Argument Pair Extraction](https://doi.org/10.18653/v1/2023.findings-acl.827) |  | 0 | Argument pair extraction (APE) aims to extract interactive argument pairs from two passages within a discussion. The key challenge of APE is to effectively capture the complex context-aware interactive relations of arguments between the two passages. In this paper, we elicit relational semantic... | Bin Liang, Geng Tu, Jianzhu Bao, Min Yang, Ruifeng Xu, Yang Sun, Yice Zhang |  |
| 1170 |  |  [DiffuSum: Generation Enhanced Extractive Summarization with Diffusion](https://doi.org/10.18653/v1/2023.findings-acl.828) |  | 0 | Extractive summarization aims to form a summary by directly extracting sentences from the source document. Existing works mostly formulate it as a sequence labeling problem by making individual sentence label predictions. This paper proposes DiffuSum, a novel paradigm for extractive summarization,... | Haopeng Zhang, Jiawei Zhang, Xiao Liu |  |
| 1171 |  |  [Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding](https://doi.org/10.18653/v1/2023.findings-acl.829) |  | 0 | This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. Recent works tackled this task by improving query... | Edison MarreseTaylor, Erica Kido Shimomoto, Hideki Nakayama, Hiroya Takamura, Ichiro Kobayashi, Yusuke Miyao |  |
| 1172 |  |  [A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information](https://doi.org/10.18653/v1/2023.findings-acl.830) |  | 0 | Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity... | Alvaro Soto, MarieFrancine Moens, Vladimir Araujo |  |
| 1173 |  |  [Pay Attention to Implicit Attribute Values: A Multi-modal Generative Framework for AVE Task](https://doi.org/10.18653/v1/2023.findings-acl.831) |  | 0 | Attribute Value Extraction (AVE) boosts many e-commerce platform services such as targeted recommendation, product retrieval and question answering. Most previous studies adopt an extractive framework such as named entity recognition (NER) to capture subtokens in the product descriptions as the... | Guanting Dong, Hongzhi Zhang, Peiguang Li, Shensi Wang, Sirui Wang, Yunsen Xian, Yupeng Zhang, Zhoujun Li |  |
| 1174 |  |  [CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding](https://doi.org/10.18653/v1/2023.findings-acl.832) |  | 0 | Story generation and understanding—as with all NLG/NLU tasks—has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural... | Chris CallisonBurch, Lara J. Martin, Yijiang River Dong |  |
| 1175 |  |  [Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases](https://doi.org/10.18653/v1/2023.findings-acl.833) |  | 0 | NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such... | Roy Schwartz, Yuval Reif |  |
| 1176 |  |  [Context-Aware Document Simplification](https://doi.org/10.18653/v1/2023.findings-acl.834) |  | 0 | To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output... | Claire Gardent, Joël Legrand, Liam Cripwell |  |
| 1177 |  |  [Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.findings-acl.835) |  | 0 | Existing knowledge-enhanced methods have achieved remarkable results in certain Q&A tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and... | Fei Huang, Guohai Xu, Ji Zhang, Luo Si, Ming Yan, Qianglong Chen, Yin Zhang |  |
| 1178 |  |  [Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games](https://doi.org/10.18653/v1/2023.findings-acl.836) |  | 0 | Text-based games present an exciting test-bed for reinforcement learning algorithms in the natural language environment. In these adventure games, an agent must learn to interact with the environment through text in order to accomplish tasks, facing large and combinational action space as well as... | Anjie Zhu, Jie Shao, PengFei Zhang, Yi Zhang, Zi Huang |  |
| 1179 |  |  [SSP: Self-Supervised Post-training for Conversational Search](https://doi.org/10.18653/v1/2023.findings-acl.837) |  | 0 | Conversational search has been regarded as the next-generation search paradigm. Constrained by data scarcity, most existing methods distill the well-trained ad-hoc retriever to the conversational retriever. However, these methods, which usually initialize parameters by query reformulation to... | JiRong Wen, Quan Tu, Rui Yan, Shen Gao, Xiaolong Wu, Zhao Cao |  |
| 1180 |  |  [Towards Reference-free Text Simplification Evaluation with a BERT Siamese Network Architecture](https://doi.org/10.18653/v1/2023.findings-acl.838) |  | 0 | Text simplification (TS) aims to modify sentences to make their both content and structure easier to understand. Traditional n-gram matching-based TS evaluation metrics heavily rely on the exact token match and human-annotated simplified sentences. In this paper, we present a novel... | DitYan Yeung, Esin Durmus, Xinran Zhao |  |
| 1181 |  |  [Causal interventions expose implicit situation models for commonsense language understanding](https://doi.org/10.18653/v1/2023.findings-acl.839) |  | 0 | Accounts of human language processing have long appealed to implicit “situation models” that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC),... | Adele Goldberg, James L. McClelland, Robert D. Hawkins, Takateru Yamakoshi |  |
| 1182 |  |  [Iterative Nearest Neighbour Machine Translation for Unsupervised Domain Adaptation](https://doi.org/10.18653/v1/2023.findings-acl.840) |  | 0 | Unsupervised domain adaptation of machine translation, which adapts a pre-trained translation model to a specific domain without in-domain parallel data, has drawn extensive attention in recent years. However, most existing methods focus on the fine-tuning based techniques, which is non-extensible.... | Hui Huang, Muyun Yang, Shuangzhi Wu, Tiejun Zhao, Xinnian Liang, Zefan Zhou |  |
| 1183 |  |  [PruMUX: Augmenting Data Multiplexing with Model Compression](https://doi.org/10.18653/v1/2023.findings-acl.841) |  | 0 | As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without... | Kai Li, Karthik Narasimhan, Vishvak Murahari, Yushan Su |  |
| 1184 |  |  [With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.842) |  | 0 | In this paper, we describe our work on social bias detection in a low-resource multilingual setting in which the languages are from two very divergent families- Indo-European (English, Hindi, and Italian) and Altaic (Korean). Currently, the majority of the social bias datasets available are in... | Nihar Sahoo, Niteesh Mallela, Pushpak Bhattacharyya |  |
| 1185 |  |  [Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness](https://doi.org/10.18653/v1/2023.findings-acl.843) |  | 0 | As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings... | Bing Qin, Weixiang Zhao, Xin Lu, Yanyan Zhao |  |
| 1186 |  |  [Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents](https://doi.org/10.18653/v1/2023.findings-acl.844) |  | 0 | Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models... | Catherine Chen, Dan Klein, Doug Downey, Gabriel Stanovsky, Kyle Lo, Zejiang Shen |  |
| 1187 |  |  [Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words](https://doi.org/10.18653/v1/2023.findings-acl.845) |  | 0 | Efforts have been made to apply topic seed words to improve the topic interpretability of topic models. However, due to the semantic diversity of natural language, supervisions from seed words could be ambiguous, making it hard to be incorporated into the current neural topic models. In this paper,... | Chao Chen, Junfeng Zhao, Xin Gao, Xu Chu, Yang Lin, Yasha Wang |  |
| 1188 |  |  [Learning from Children: Improving Image-Caption Pretraining via Curriculum](https://doi.org/10.18653/v1/2023.findings-acl.846) |  | 0 | Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem – it requires multiple concepts (nouns) from captions to be aligned to several objects in... | Alireza Zareian, Bo Wu, Hammad A. Ayyubi, Rahul Lokesh, ShihFu Chang |  |
| 1189 |  |  [Discovering Language Model Behaviors with Model-Written Evaluations](https://doi.org/10.18653/v1/2023.findings-acl.847) |  | 0 | As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically... | Amanda Askell, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Catherine Olsson, Christopher Olah, Craig Pettit, Da Yan, Daniela Amodei, Danny Hernandez, Dario Amodei, Dawn Drain, Deep Ganguli, Dustin Li, Edwin Chen, Eli TranJohnson, Ethan Perez, Evan Hubinger, Guro Khundadze, Jack Clark, Jackson Kernion, James Landis, Jamie Kerr, Jared Kaplan, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Karina Nguyen, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Roger B. Grosse, Sam McCandlish, Sam Ringer, Samuel R. Bowman, Sandipan Kundu, Saurav Kadavath, Scott Heiner, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy TelleenLawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac HatfieldDodds |  |
| 1190 |  |  [Cross-Domain Argument Quality Estimation](https://doi.org/10.18653/v1/2023.findings-acl.848) |  | 0 | Argumentation is one of society’s foundational pillars, and, sparked by advances in NLP, and the vast availability of text data, automated mining of arguments receives increasing attention. A decisive property of arguments is their strength or quality. While there are works on the automated... | Evgeniy Faerman, Max Berrendorf, Michael Fromm, Thomas Seidl |  |
| 1191 |  |  [DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis](https://doi.org/10.18653/v1/2023.findings-acl.849) |  | 0 | The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap... | Bobo Li, Donghong Ji, Fei Li, Hao Fei, Jingye Li, Jinsong Zhang, Lizi Liao, Shengqiong Wu, TatSeng Chua, Yijiang Liu, Yuhan Wu |  |
| 1192 |  |  [GeoDRL: A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning](https://doi.org/10.18653/v1/2023.findings-acl.850) |  | 0 | Ensuring both interpretability and correctness is a great challenge in automated geometry problem solving (GPS), and the scarcity of labeled data hinders learning mathematical reasoning from samples. Therefore, we present GeoDRL, a self-learning geometry problem solving framework that integrates... | Di Fu, Liangcai Gao, Shuai Peng, Yijun Liang, Zhi Tang |  |
| 1193 |  |  [Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction](https://doi.org/10.18653/v1/2023.findings-acl.851) |  | 0 | Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works... | Hang Gao, Liqi Zhang, Mengting Hu, Minlie Huang, Shiwan Zhao, Yike Wu, Yinhao Bai, Zhen Zhang |  |
| 1194 |  |  [Adversarial Knowledge Stimulated Contrastive Prompting for Few-shot Language Learners](https://doi.org/10.18653/v1/2023.findings-acl.852) |  | 0 | Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models(PLMs) on few-shot Natural Language Understanding (NLU) tasks by employing task-specific prompts. Yet, PLMsare unfamiliar with prompt-style expressionsduring pre-training, which limits the few-shotlearning... | Changlin Zhao, Fei Xu, Kai Zheng, Qi Zhang, Qingfeng Sun, Tengchao Lv, Yaming Yang, Yeyong Pi |  |
| 1195 |  |  [Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios](https://doi.org/10.18653/v1/2023.findings-acl.853) |  | 0 | Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a... | Ai Ti Aw, Bowei Zou, Jie Mei, Rui Fan, Tingting He, Yufan Wang |  |
| 1196 |  |  [Typology Guided Multilingual Position Representations: Case on Dependency Parsing](https://doi.org/10.18653/v1/2023.findings-acl.854) |  | 0 | Recent multilingual models benefit from strong unified semantic representation models. However, due to conflict linguistic regularities, ignoring language-specific features during multilingual learning may suffer from negative transfer. In this work, we analyze the relationbetween a language’s... | Tao Ji, Xiaoling Wang, Yuanbin Wu |  |
| 1197 |  |  [Learning Event-aware Measures for Event Coreference Resolution](https://doi.org/10.18653/v1/2023.findings-acl.855) |  | 0 | Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event... | Hai Zhao, Yao Yao, Zuchao Li |  |
| 1198 |  |  [Second Language Acquisition of Neural Language Models](https://doi.org/10.18653/v1/2023.findings-acl.856) |  | 0 | With the success of neural language models (LMs), their language acquisition has gained much attention. This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition. Specifically, we trained bilingual LMs with... | Hiroki Ouchi, Miyu Oba, Taro Watanabe, Tatsuki Kuribayashi |  |
| 1199 |  |  [On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection](https://doi.org/10.18653/v1/2023.findings-acl.857) |  | 0 | Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability.... | Jin Ma, Qi Zhang, Shihan Dou, Songyang Gao, Xuanjing Huang, Ying Shan |  |
| 1200 |  |  [Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks](https://doi.org/10.18653/v1/2023.findings-acl.858) |  | 0 | The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering. Recent studies have demonstrated that Chain-of-Thought (CoT) prompts can lead to significant improvements in tasks such as... | Fangyi Yu, Frank Schilder, Lee Quartey |  |
| 1201 |  |  [End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar](https://doi.org/10.18653/v1/2023.findings-acl.859) |  | 0 | End-to-end Aspect-based Sentiment Analysis (EASA) is a natural language processing (NLP) task that involves extracting aspect terms and identifying the sentiments for them, which provides a fine-grained level of text analysis and thus requires a deep understanding of the running text. Many previous... | Bo Hu, Fei Xia, Weidong Chen, Yan Song, Yuanhe Tian |  |
| 1202 |  |  [ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.findings-acl.860) |  | 0 | Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific... | Baoxun Wang, Di Niu, Feiran Sun, Lei Yang, Mingjun Zhao, Shiang Qi, Weidong Guo, Xiaoli Wang, Yakun Yu |  |
| 1203 |  |  [On Degrees of Freedom in Defining and Testing Natural Language Understanding](https://doi.org/10.18653/v1/2023.findings-acl.861) |  | 0 | Natural language understanding (NLU) studies often exaggerate or underestimate the capabilities of systems, thereby limiting the reproducibility of their findings. These erroneous evaluations can be attributed to the difficulty of defining and testing NLU adequately. In this position paper, we... | Saku Sugawara, Shun Tsugita |  |
| 1204 |  |  [AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking](https://doi.org/10.18653/v1/2023.findings-acl.862) |  | 0 | Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and... | Heyan Huang, Wei Wei, XianLing Mao, Yuxiang Nie |  |
| 1205 |  |  [Adaptive Ordered Information Extraction with Deep Reinforcement Learning](https://doi.org/10.18653/v1/2023.findings-acl.863) |  | 0 | Information extraction (IE) has been studied extensively. The existing methods always follow a fixed extraction order for complex IE tasks with multiple elements to be extracted in one instance such as event extraction. However, we conduct experiments on several complex IE datasets and observe that... | Chuanjun Ji, Jiaqing Liang, Wenhao Huang, Yanghua Xiao, Zhixu Li |  |
| 1206 |  |  [Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport](https://doi.org/10.18653/v1/2023.findings-acl.864) |  | 0 | Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learningbased models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings,... | Ginny Y. Wong, Hang Yin, Simon See, Weizhi Fei, Yangqiu Song, Zihao Wang |  |
| 1207 |  |  [RISE: Leveraging Retrieval Techniques for Summarization Evaluation](https://doi.org/10.18653/v1/2023.findings-acl.865) |  | 0 | Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained... | David C. Uthus, Jianmo Ni |  |
| 1208 |  |  [On the Difference of BERT-style and CLIP-style Text Encoders](https://doi.org/10.18653/v1/2023.findings-acl.866) |  | 0 | Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing, e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent... | Benyou Wang, Guiming Chen, Shizhe Diao, Xiang Wan, Zhihong Chen |  |
| 1209 |  |  [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://doi.org/10.18653/v1/2023.findings-acl.867) |  | 0 | Concurrent with the rapid progress in neural network-based models in NLP, the need for creating explanations for the predictions of these black-box models has risen steadily. Yet, especially for complex inputs like texts or images, existing interpretability methods still struggle with deriving... | Marc Felix Brinner, Sina Zarrieß |  |
| 1210 |  |  [NusaCrowd: Open Source Initiative for Indonesian NLP Resources](https://doi.org/10.18653/v1/2023.findings-acl.868) |  | 0 | We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the... | Ade Romadhony, Alham Fikri Aji, Ali Akbar Septiandri, Arie Ardiyanti Suryani, Ayu Purwarianti, Bryan Wilie, Cahya Wirawan, Christian Wibisono, Cuk Tho, Dan Su, David Moeljadi, Dyah Damapuspita, Fajri Koto, Frederikus Hudi, Genta Indra Winata, Graham Neubig, Haryo Akbarianto Wibowo, Herry Sujaini, Holy Lovenia, Ichwanul Muslim Karo Karo, Ika Alfina, Ilham Firdausi Putra, Ivan Halim Parmonangan, James Jaya, Jennifer Santoso, Karissa Vincentio, Kaustubh D. Dhole, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Muhammad Satrio Wicaksono, Pascale Fung, Rahmad Mahendra, Rifki Afina Putri, Ryan Hadiwijaya, Ryandito Diandaru, Sakriani Sakti, Samsul Rahmadani, Samuel Cahyawijaya, Sebastian Ruder, Tiezheng Yu, Timothy Baldwin, Tirana Fatyanosa, Vito Ghifari, Wenliang Dai, Yan Xu, Yulianti Oenang, Ziwei Ji |  |
| 1211 |  |  [Transcribing Vocal Communications of Domestic Shiba lnu Dogs](https://doi.org/10.18653/v1/2023.findings-acl.869) |  | 0 | How animals communicate and whether they have languages is a persistent curiosity of human beings. However, the study of animal communications has been largely restricted to data from field recordings or in a controlled environment, which is expensive and limited in scale and variety. In this... | Chunhao Zhang, Jieyi Huang, Kenny Q. Zhu, Mengyue Wu |  |
| 1212 |  |  [SkillQG: Learning to Generate Question for Reading Comprehension Assessment](https://doi.org/10.18653/v1/2023.findings-acl.870) |  | 0 | We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. Existing question generation systems widely differentiate questions by literal information such as question words and answer types to generate... | Bang Liu, Lingfei Wu, Siliang Tang, Xiaoqiang Wang |  |
| 1213 |  |  [Improving Long Dialogue Summarization with Semantic Graph Representation](https://doi.org/10.18653/v1/2023.findings-acl.871) |  | 0 | Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into... | Bobby Yilun Hua, Kathleen R. McKeown, Zhaoyuan Deng |  |
| 1214 |  |  [Model Intrinsic Features of Fine-tuning based Text Summarization Models for Factual Consistency](https://doi.org/10.18653/v1/2023.findings-acl.872) |  | 0 | In this study, we analyze the model intrinsic features of a summarization model by varying the fine-tuning objectives and datasets. We fine-tune BART models combining three fine-tuning objectives (negative log-likelihood, unlikelihood, and contrastive loss) and two datasets (CNN/DailyMail and XSum)... | Bongkyu Hwang, Jaewoong Yun, Jongyoon Song, Nohil Park, Seongho Joe, Sungroh Yoon, Youngjune Gwon |  |
| 1215 |  |  [EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning](https://doi.org/10.18653/v1/2023.findings-acl.873) |  | 0 | Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and... | Tiannan Wang, Wangchunshu Zhou, Xinsong Zhang, Yan Zeng |  |
| 1216 |  |  [DP-BART for Privatized Text Rewriting under Local Differential Privacy](https://doi.org/10.18653/v1/2023.findings-acl.874) |  | 0 | Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic... | Ivan Habernal, Timour Igamberdiev |  |
| 1217 |  |  [Robustness of Learning from Task Instructions](https://doi.org/10.18653/v1/2023.findings-acl.875) |  | 0 | Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and... | Hanzi Xu, Hongyu Zhao, Hongyuan Mei, Jiasheng Gu, Liangyu Nie, Wenpeng Yin |  |
| 1218 |  |  [Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling](https://doi.org/10.18653/v1/2023.findings-acl.876) |  | 0 | In this paper, we propose an alternative to the classic masked language modeling (MLM) pre-training paradigm, where the objective is altered from the reconstruction of the exact identity of randomly selected masked subwords to the prediction of their latent semantic properties. We coin the proposed... | Gábor Berend |  |
| 1219 |  |  [Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization](https://doi.org/10.18653/v1/2023.findings-acl.877) |  | 0 | In text summarization, extractivity is defined as a measurement of the degree of overlap between a source document and its summary. Previous research has shown that the extractivity level of training data can influence both output extractivity and the amount of factual information (i.e.... | Jana Diesner, Ly Dinh, Sullam Jeoung, Yubin Ge |  |
| 1220 |  |  [Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering](https://doi.org/10.18653/v1/2023.findings-acl.878) |  | 0 | The nodes in the commonsense knowledge graph (CSKG) are normally represented by free-form short text (e.g., word or phrase). Different nodes may represent the same concept. This leads to the problems of edge sparsity and node redundancy, which challenges CSKG representation and completion. On the... | Rui Xia, Siwei Wu, Xiangqing Shen |  |
| 1221 |  |  [Incorporating Factuality Inference to Identify Document-level Event Factuality](https://doi.org/10.18653/v1/2023.findings-acl.879) |  | 0 | Document-level Event Factuality Identification (DEFI) refers to identifying the degree of certainty that a specific event occurs in a document. Previous studies on DEFI failed to link the document-level event factuality with various sentence-level factuality values in the same document. In this... | Heng Zhang, Peifeng Li, Xiaoxu Zhu, Zhong Qian |  |
| 1222 |  |  [Hybrid and Collaborative Passage Reranking](https://doi.org/10.18653/v1/2023.findings-acl.880) |  | 0 | In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked... | Houqiang Li, Jiaxin Shi, Wengang Zhou, Zongmeng Zhang |  |
| 1223 |  |  [Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence](https://doi.org/10.18653/v1/2023.findings-acl.881) |  | 0 | Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some... | Haoran Li, Mingshi Xu, Yangqiu Song |  |
| 1224 |  |  [Learning Query Adaptive Anchor Representation for Inductive Relation Prediction](https://doi.org/10.18653/v1/2023.findings-acl.882) |  | 0 | Relation prediction on knowledge graphs (KGs) attempts to infer the missing links between entities. Most previous studies are limited to the transductive setting where all entities must be seen during the training, making them unable to perform reasoning on emerging entities. Recently, the... | Guangyou Zhou, Jimmy Xiangji Huang, Jin Liu, Yi Zhang, Zhiwen Xie |  |
| 1225 |  |  [Context or Knowledge is Not Always Necessary: A Contrastive Learning Framework for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.findings-acl.883) |  | 0 | Emotion recognition in conversations (ERC) aims to detect the emotion of utterances in conversations. Existing efforts generally focus on modeling context- and knowledge-sensitive dependencies. However, it is observed that the emotions of many utterances can be correctly detected without context or... | Bin Liang, Geng Tu, Min Yang, Ruibin Mao, Ruifeng Xu |  |
| 1226 |  |  [Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization](https://doi.org/10.18653/v1/2023.findings-acl.884) |  | 0 | Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic... | Hui Wang, Luyao Cheng, Qian Chen, Qinglin Zhang, Siqi Zheng, Yafeng Chen |  |
| 1227 |  |  [Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages](https://doi.org/10.18653/v1/2023.findings-acl.885) |  | 0 | While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to... | Alessandro Moschitti, Ankit Chadha, Shivanshu Gupta, Yoshitomo Matsubara |  |
| 1228 |  |  [Run Like a Girl! Sport-Related Gender Bias in Language and Vision](https://doi.org/10.18653/v1/2023.findings-acl.886) |  | 0 | Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. We analyze gender bias in two Language and Vision datasets. Consistent with prior work, we find that both datasets underrepresent women, which promotes their... | Eleonora Gualdoni, Gemma Boleda, Sophia Harrison |  |
| 1229 |  |  [People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts](https://doi.org/10.18653/v1/2023.findings-acl.887) |  | 0 | Although pre-trained named entity recognition (NER) models are highly accurate on modern corpora, they underperform on historical texts due to differences in language OCR errors. In this work, we develop a new NER corpus of 3.6M sentences from late medieval charters written mainly in Czech, Latin,... | Ales Horák, Kristina Luger, Michal Stefánik, Tereza Vrabcová, Vit Novotny |  |
| 1230 |  |  [Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence](https://doi.org/10.18653/v1/2023.findings-acl.888) |  | 0 | We present a new fact-checking benchmark, Check-COVID, that requires systems to verify claims about COVID-19 from news using evidence from scientific articles. This approach to fact-checking is particularly challenging as it requires checking internet text written in everyday language against... | Amith Ananthram, Gengyu Wang, Kate Harwood, Kathleen R. McKeown, Lawrence Chillrud, Melanie Subbiah |  |
| 1231 |  |  [Early Exit with Disentangled Representation and Equiangular Tight Frame](https://doi.org/10.18653/v1/2023.findings-acl.889) |  | 0 | Dynamic early exit has demonstrated great potential in coping with the sharply increasing number of pre-trained language model parameters, which can achieve a good trade-off between performance and efficiency. The existing early exit paradigm relies on training parametrical internal classifiers at... | Jikai Wang, Juntao Li, Min Zhang, Qiang Chen, Wenliang Chen, Yixin Ji |  |
| 1232 |  |  [Tokenization with Factorized Subword Encoding](https://doi.org/10.18653/v1/2023.findings-acl.890) |  | 0 | In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete... | David Samuel, Lilja Øvrelid |  |
| 1233 |  |  [Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers](https://doi.org/10.18653/v1/2023.findings-acl.891) |  | 0 | How well do language models deal with quantification? In this study, we focus on ‘few’-type quantifiers, as in ‘few children like toys’, which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and ‘few’-type... | Benjamin K. Bergen, James A. Michaelov |  |
| 1234 |  |  ["A Little is Enough": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.892) |  | 0 | Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality... | Akshay Batheja, Pushpak Bhattacharyya |  |
| 1235 |  |  [How effective is machine translation on low-resource code-switching? A case study comparing human and automatic metrics](https://doi.org/10.18653/v1/2023.findings-acl.893) |  | 0 | This paper presents an investigation into the differences between processing monolingual input and code-switching (CSW) input in the context of machine translation (MT). Specifically, we compare the performance of three MT systems (Google, mBART-50 and M2M-100-big) in terms of their ability to... | Christopher Bryant, Li Nguyen, Oliver Mayeux, Zheng Yuan |  |
| 1236 |  |  [Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks](https://doi.org/10.18653/v1/2023.findings-acl.894) |  | 0 | Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models.... | David Schlangen, Sherzod Hakimov |  |
| 1237 |  |  [On the Expressivity Role of LayerNorm in Transformers' Attention](https://doi.org/10.18653/v1/2023.findings-acl.895) |  | 0 | Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm’s only role is to normalize the... | Eran Yahav, Shaked Brody, Uri Alon |  |
| 1238 |  |  [DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation](https://doi.org/10.18653/v1/2023.findings-acl.896) |  | 0 | Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make... | ChaeHun Park, Daniel Rim, Jaegul Choo, Seungil Chad Lee |  |
| 1239 |  |  [Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.findings-acl.897) |  | 0 | Sparsely gated Mixture of Experts (MoE) models have been shown to be a compute-efficient method to scale model capacity for multilingual machine translation. However, for low-resource tasks, MoE models severely over-fit. We show effective regularization strategies, namely dropout techniques for MoE... | Anna Y. Sun, Maha Elbayad, Shruti Bhosale |  |
| 1240 |  |  [Intent Discovery with Frame-guided Semantic Regularization and Augmentation](https://doi.org/10.18653/v1/2023.findings-acl.898) |  | 0 | Most existing intent discovery methods leverage representation learning and clustering to transfer the prior knowledge of known intents to unknown ones. The learned representations are limited to the syntactic forms of sentences, therefore, fall short of recognizing adequate variations under the... | Jingyuan Yang, Rui Zhang, Wei Peng, Yajing Sun |  |
| 1241 |  |  [An Empirical Comparison of LM-based Question and Answer Generation Methods](https://doi.org/10.18653/v1/2023.findings-acl.899) |  | 0 | Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g. a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish... | Asahi Ushio, Fernando AlvaManchego, José CamachoCollados |  |
| 1242 |  |  [Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding](https://doi.org/10.18653/v1/2023.findings-acl.900) |  | 0 | With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in... | Chengwei Qin, Daling Wang, Qian Li, Shafiq Joty, Shi Feng, Yifei Zhang |  |
| 1243 |  |  [Decouple knowledge from paramters for plug-and-play language modeling](https://doi.org/10.18653/v1/2023.findings-acl.901) |  | 0 | Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge... | Dongyan Zhao, Rui Yan, Xin Cheng, Xiuying Chen, Yankai Lin |  |
| 1244 |  |  [One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.1) |  | 0 | User simulators are agents designed to imitate human users; recent advances have found that Task-oriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a sub-optimal ToD system if it is tailored to only one ad... | Benyou Wang, Fei Mi, Qun Liu, Xiang Wan, Xin Jiang, Yajiao Liu, Yasheng Wang, Yichun Yin |  |
| 1245 |  |  [SafeConv: Explaining and Correcting Conversational Unsafe Behavior](https://doi.org/10.18653/v1/2023.acl-long.2) |  | 0 | One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this... | Dong Yu, Haitao Mi, Lifeng Jin, Linfeng Song, Mian Zhang, Wenliang Chen |  |
| 1246 |  |  [Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better](https://doi.org/10.18653/v1/2023.acl-long.3) |  | 0 | While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard... | David Dale, Elena Voita, Loïc Barrault, Marta R. Costajussà |  |
| 1247 |  |  [Explainable Recommendation with Personalized Review Retrieval and Aspect Learning](https://doi.org/10.18653/v1/2023.acl-long.4) |  | 0 | Explainable recommendation is a technique that combines prediction and generation tasks to produce more persuasive results. Among these tasks, textual generation demands large amounts of data to achieve satisfactory accuracy. However, historical user reviews of items are often insufficient, making... | Hao Cheng, Hao Liao, Kezhong Lu, Mingyang Zhou, Shuo Wang, Wei Zhang, Wensheng Lu |  |
| 1248 |  |  [Binary and Ternary Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.5) |  | 0 | Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have... | Aasish Pappu, Barlas Oguz, Raghuraman Krishnamoorthi, Yangyang Shi, Zechun Liu |  |
| 1249 |  |  [Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking](https://doi.org/10.18653/v1/2023.acl-long.6) |  | 0 | In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in... | Björn Bebensee, Haejun Lee |  |
| 1250 |  |  [EM Pre-training for Multi-party Dialogue Response Generation](https://doi.org/10.18653/v1/2023.acl-long.7) |  | 0 | Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response... | Hai Zhao, Yiyang Li |  |
| 1251 |  |  [ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER](https://doi.org/10.18653/v1/2023.acl-long.8) |  | 0 | Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional... | Dinesh Manocha, Manan Suri, Ramaneswaran S., Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi |  |
| 1252 |  |  [Natural Language to Code Generation in Interactive Data Science Notebooks](https://doi.org/10.18653/v1/2023.acl-long.9) |  | 0 | Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural... | Abhishek Rao, Charles Sutton, Henryk Michalewski, Joshua Howland, Kefan Xiao, Kensen Shi, Michele Catasta, Oleksandr Polozov, Paige Bailey, Pengcheng Yin, WenDing Li, Yeming Wen |  |
| 1253 |  |  [Subset Retrieval Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.10) |  | 0 | k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000... | Eiichiro Sumita, Hideki Tanaka, Hiroyuki Deguchi, Masao Utiyama, Taro Watanabe, Yusuke Matsui |  |
| 1254 |  |  [MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning](https://doi.org/10.18653/v1/2023.acl-long.11) |  | 0 | Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning... | Xiaojun Wan, Xu Zhang |  |
| 1255 |  |  [Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies](https://doi.org/10.18653/v1/2023.acl-long.12) |  | 0 | Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’).... | Iria deDiosFlores, Juan Garcia Amboage, Marcos García |  |
| 1256 |  |  [Open-ended Long Text Generation via Masked Language Modeling](https://doi.org/10.18653/v1/2023.acl-long.13) |  | 0 | Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference... | Juntao Li, Min Zhang, Xiaobo Liang, Zecheng Tang |  |
| 1257 |  |  [A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces](https://doi.org/10.18653/v1/2023.acl-long.14) |  | 0 | We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to... | Gabriella Chronis, Katrin Erk, Kyle Mahowald |  |
| 1258 |  |  [Holographic CCG Parsing](https://doi.org/10.18653/v1/2023.acl-long.15) |  | 0 | We propose a method for formulating CCG as a recursive composition in a continuous vector space. Recent CCG supertagging and parsing models generally demonstrate high performance, yet rely on black-box neural architectures to implicitly model phrase structure dependencies. Instead, we leverage the... | Daichi Mochihashi, Ryosuke Yamaki, Tadahiro Taniguchi |  |
| 1259 |  |  [Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning](https://doi.org/10.18653/v1/2023.acl-long.16) |  | 0 | Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. Nonetheless, in real-world scenarios of UIE applications, new... | Bing Han, Feng Wei, Yin Jie, Yuxi Qian, Zhenghong Hao, Zujie Liang |  |
| 1260 |  |  [Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation](https://doi.org/10.18653/v1/2023.acl-long.17) |  | 0 | Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text... | Fang Fang, Ping Guo, Wei Ma, Yanan Cao, Yubing Ren, Zheng Lin |  |
| 1261 |  |  [WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.18) |  | 0 | A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs. Due to lack of annotated data, existing factual consistency metrics usually train evaluation models on synthetic texts or directly transfer from other... | Jiachen Liu, Sujian Li, Wei Li, Wenhao Wu, Xinyan Xiao, Yajuan Lyu |  |
| 1262 |  |  [AMR-based Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.19) |  | 0 | Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch... | Aiwei Liu, Fukun Ma, Lijie Wen, Philip S. Yu, Shuang Li, Xuming Hu, Yawen Yang |  |
| 1263 |  |  [Text Adversarial Purification as Defense against Adversarial Attacks](https://doi.org/10.18653/v1/2023.acl-long.20) |  | 0 | Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean... | Demin Song, Linyang Li, Xipeng Qiu |  |
| 1264 |  |  [SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres](https://doi.org/10.18653/v1/2023.acl-long.21) |  | 0 | Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured... | Bryan Hooi, Ningyu Zhang, Shengyu Mao, Shumin Deng |  |
| 1265 |  |  [Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection](https://doi.org/10.18653/v1/2023.acl-long.22) |  | 0 | Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable... | Christopher Clarke, Gaurav Mittal, Jason Mars, Matthew Hall, Mei Chen, Sandra Sajeev, Ye Yu |  |
| 1266 |  |  [What about "em"? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns](https://doi.org/10.18653/v1/2023.acl-long.23) |  | 0 | As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns, we need more research on identity-inclusive NLP. Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized... | Anne Lauscher, Archie Crowley, Debora Nozza, Dirk Hovy, Ehm Miltersen |  |
| 1267 |  |  [What Is Overlap Knowledge in Event Argument Extraction? APE: A Cross-datasets Transfer Learning Model for EAE](https://doi.org/10.18653/v1/2023.acl-long.24) |  | 0 | The EAE task extracts a structured event record from an event text. Most existing approaches train the EAE model on each dataset independently and ignore the overlap knowledge across datasets. However, insufficient event records in a single dataset often prevent the existing model from achieving... | Jinyu Guo, Kai Shuang, Kaihang Zhang, Xinyue Yang, Xuyang Yao |  |
| 1268 |  |  [Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation](https://doi.org/10.18653/v1/2023.acl-long.25) |  | 0 | Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address... | Baosong Yang, Boxing Chen, Dayiheng Liu, Jun Xie, Kexin Yang, Mingfeng Xue, Wenqiang Lei |  |
| 1269 |  |  [Knowledge of cultural moral norms in large language models](https://doi.org/10.18653/v1/2023.acl-long.26) |  | 0 | Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models... | Aida Ramezani, Yang Xu |  |
| 1270 |  |  [Songs Across Borders: Singable and Controllable Neural Lyric Translation](https://doi.org/10.18653/v1/2023.acl-long.27) |  | 0 | The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by... | Longshen Ou, MinYen Kan, Xichu Ma, Ye Wang |  |
| 1271 |  |  [Fantastic Expressions and Where to Find Them: Chinese Simile Generation with Multiple Constraints](https://doi.org/10.18653/v1/2023.acl-long.28) |  | 0 | Similes occur in the creative context of describing a concept (i.e., tenor) by making a literally false yet figuratively meaningful comparison to another (i.e., vehicle). Previous efforts form simile generation as a context-free generation task, focusing on simile-style transfer or writing a simile... | Baosong Yang, Dayiheng Liu, Jun Xie, Kexin Yang, Wenqiang Lei, Xiangpeng Wei, Zhengyuan Liu |  |
| 1272 |  |  [Revealing Single Frame Bias for Video-and-Language Learning](https://doi.org/10.18653/v1/2023.acl-long.29) |  | 0 | Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs... | Jie Lei, Mohit Bansal, Tamara L. Berg |  |
| 1273 |  |  [Learning with Partial Annotations for Event Detection](https://doi.org/10.18653/v1/2023.acl-long.30) |  | 0 | Event detection (ED) seeks to discover and classify event instances in plain texts. Previous methods for ED typically adopt supervised learning, requiring fully labeled and high-quality training data. However, in a real-world application, we may not obtain clean training data but only partially... | Dianbo Sui, Haoyan Liu, Jian Liu, Kang Liu, Zhe Zhao |  |
| 1274 |  |  [World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models](https://doi.org/10.18653/v1/2023.acl-long.31) |  | 0 | The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can... | Jiayi Pan, Joyce Chai, Ziqiao Ma |  |
| 1275 |  |  [A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models](https://doi.org/10.18653/v1/2023.acl-long.32) |  | 0 | We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description... | Alessandro Stolfo, Bernhard Schölkopf, Kumar Shridhar, Mrinmaya Sachan, Zhijing Jin |  |
| 1276 |  |  [Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information](https://doi.org/10.18653/v1/2023.acl-long.33) |  | 0 | The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based... | Aline Villavicencio, Bohao Yang, Chenghua Lin, Kun Zhao, Wenge Rong, Xiaohui Cui |  |
| 1277 |  |  [Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions](https://doi.org/10.18653/v1/2023.acl-long.34) |  | 0 | Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation.... | Ece Kamar, John Joon Young Chung, Saleema Amershi |  |
| 1278 |  |  [Pruning Pre-trained Language Models Without Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-long.35) |  | 0 | To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little... | Deqing Wang, Feng Xia, Fuzhen Zhuang, Ruobing Xie, Ting Jiang |  |
| 1279 |  |  [When Does Translation Require Context? A Data-driven, Multilingual Exploration](https://doi.org/10.18653/v1/2023.acl-long.36) |  | 0 | Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation,... | André F. T. Martins, Emmy Liu, Graham Neubig, Kayo Yin, Patrick Fernandes |  |
| 1280 |  |  [Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection](https://doi.org/10.18653/v1/2023.acl-long.37) |  | 0 | Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set,... | Linmei Hu, Liqiang Nie, Weixin Li, Yingxia Shao, Ziwei Chen |  |
| 1281 |  |  [LexSym: Compositionality as Lexical Symmetry](https://doi.org/10.18653/v1/2023.acl-long.38) |  | 0 | In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation.... | Ekin Akyürek, Jacob Andreas |  |
| 1282 |  |  [Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition](https://doi.org/10.18653/v1/2023.acl-long.39) |  | 0 | Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper,... | Jun Sun, Shoukang Han, ShuKai Zheng, Taihao Li, Xiaoning Zhang, YuPing Ruan, Yulong Liu, Yuxin Huang |  |
| 1283 |  |  [CASN: Class-Aware Score Network for Textual Adversarial Detection](https://doi.org/10.18653/v1/2023.acl-long.40) |  | 0 | Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data... | Dacheng Tao, Liang Ding, Qi Zhang, Rong Bao, Rui Zheng |  |
| 1284 |  |  [Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest](https://doi.org/10.18653/v1/2023.acl-long.41) |  | 0 | Large neural networks can now generate jokes, but do they really “understand” humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks... | Ana Marasovic, Jack Hessel, Jeff Da, Jena D. Hwang, Lillian Lee, Robert Mankoff, Rowan Zellers, Yejin Choi |  |
| 1285 |  |  [Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation](https://doi.org/10.18653/v1/2023.acl-long.42) |  | 0 | The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects,... | Bradley McDonnell, Dan Jurafsky, Martijn Bartelds, Martijn Wieling, Nay San |  |
| 1286 |  |  [CLCL: Non-compositional Expression Detection with Contrastive Learning and Curriculum Learning](https://doi.org/10.18653/v1/2023.acl-long.43) |  | 0 | Non-compositional expressions present a substantial challenge for natural language processing (NLP) systems, necessitating more intricate processing compared to general language tasks, even with large pre-trained language models. Their non-compositional nature and limited availability of data... | Jianing Zhou, Suma Bhat, Ziheng Zeng |  |
| 1287 |  |  [Multi-VALUE: A Framework for Cross-Dialectal English NLP](https://doi.org/10.18653/v1/2023.acl-long.44) |  | 0 | Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts.... | Caleb Ziems, Diyi Yang, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, William Held |  |
| 1288 |  |  [Self-Edit: Fault-Aware Code Editor for Code Generation](https://doi.org/10.18653/v1/2023.acl-long.45) |  | 0 | Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit... | Ge Li, Jia Li, Kechi Zhang, Zhi Jin, Zhuo Li |  |
| 1289 |  |  [ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning](https://doi.org/10.18653/v1/2023.acl-long.46) |  | 0 | Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all... | Colin Raffel, Elad Venezian, Leshem Choshen, Noam Slonim, Shachar DonYehiya |  |
| 1290 |  |  [Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization](https://doi.org/10.18653/v1/2023.acl-long.47) |  | 0 | The neural metrics recently received considerable attention from the research community in the automatic evaluation of machine translation. Unlike text-based metrics that have interpretable and consistent evaluation mechanisms for various data sources, the reliability of neural metrics in assessing... | Cuilian Zhang, Derek F. Wong, Lidia S. Chao, Min Zhang, Runzhe Zhan, Xuebo Liu |  |
| 1291 |  |  [Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling](https://doi.org/10.18653/v1/2023.acl-long.48) |  | 0 | Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT... | Andrew McCallum, HawShiuan Chang, Kathryn Ricci, RueiYao Sun |  |
| 1292 |  |  [On-the-fly Cross-lingual Masking for Multilingual Pre-training](https://doi.org/10.18653/v1/2023.acl-long.49) |  | 0 | In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward... | Bin Fang, Xi Ai |  |
| 1293 |  |  [How About Kind of Generating Hedges using End-to-End Neural Models?](https://doi.org/10.18653/v1/2023.acl-long.50) |  | 0 | Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, “face threat”) to one’s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this... | Alafate Abulimiti, Chloé Clavel, Justine Cassell |  |
| 1294 |  |  [DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models](https://doi.org/10.18653/v1/2023.acl-long.51) |  | 0 | With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To... | Benjamin Hoover, David Munechika, Duen Horng Chau, Evan Montoya, Haoyang Yang, Zijie J. Wang |  |
| 1295 |  |  [From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization](https://doi.org/10.18653/v1/2023.acl-long.52) |  | 0 | Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments. KPA extracts the main points in the data as a list of concise sentences or phrases, termed Key Points, and quantifies their prevalence. While key points are more expressive... | Arie Cattan, Lilach Eden, Roy BarHaim, Yoav Kantor |  |
| 1296 |  |  [When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications](https://doi.org/10.18653/v1/2023.acl-long.53) |  | 0 | Open Information Extraction (OpenIE) has been used in the pipelines of various NLP tasks. Unfortunately, there is no clear consensus on which models to use in which tasks. Muddying things further is the lack of comparisons that take differing training sets into account. In this paper, we present an... | ChengXiang Zhai, Ishan Jindal, Kevin ChenChuan Chang, Kevin Pei, Yunyao Li |  |
| 1297 |  |  [Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning](https://doi.org/10.18653/v1/2023.acl-long.54) |  | 0 | Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are... | Ashiqur R. KhudaBukhsh, Christopher Homan, Saloni Poddar, Sarah Luger, Tharindu Cyril Weerasooriya |  |
| 1298 |  |  [Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA](https://doi.org/10.18653/v1/2023.acl-long.55) |  | 0 | Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. ‘Selective prediction’ partly addresses the above... | Chitta Baral, Neeraj Varshney |  |
| 1299 |  |  [UniLG: A Unified Structure-aware Framework for Lyrics Generation](https://doi.org/10.18653/v1/2023.acl-long.56) |  | 0 | As a special task of natural language generation, conditional lyrics generation needs to consider the structure of generated lyrics and the relationship between lyrics and music. Due to various forms of conditions, a lyrics generation system is expected to generate lyrics conditioned on different... | Fan Lou, Jiatong Shi, Qin Jin, Shuai Guo, Tao Qian, Xiang Yin, Yuning Wu |  |
| 1300 |  |  [FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.57) |  | 0 | The generalization problem on KBQA has drawn considerable attention. Existing research suffers from the generalization issue brought by the entanglement in the coarse-grained modeling of the logical expression, or inexecutability issues due to the fine-grained modeling of disconnected classes and... | Cuiping Li, Hong Chen, Jing Zhang, Juanzi Li, Lingxi Zhang, Shulin Cao, Xinmei Huang, Yanling Wang |  |
| 1301 |  |  [Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models](https://doi.org/10.18653/v1/2023.acl-long.58) |  | 0 | Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained... | Dagmar Gromann, Lennart Wachowiak |  |
| 1302 |  |  [Being Right for Whose Right Reasons?](https://doi.org/10.18653/v1/2023.acl-long.59) |  | 0 | Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are ‘right for the right reasons’. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a... | Anders Søgaard, Laura Cabello, Terne Sasha Thorn Jakobsen |  |
| 1303 |  |  [ALERT: Adapt Language Models to Reasoning Tasks](https://doi.org/10.18653/v1/2023.acl-long.60) |  | 0 | Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply... | Asli Celikyilmaz, Badr AlKhamissi, Gargi Ghosh, Mona T. Diab, Olga Golovneva, Ping Yu, Siddharth Verma, Tianlu Wang, Zhijing Jin |  |
| 1304 |  |  [Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages](https://doi.org/10.18653/v1/2023.acl-long.61) |  | 0 | The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An... | Amir Hossein Kargaran, André F. T. Martins, Ayyoob Imani, Chunlan Ma, François Yvon, Helmut Schmid, Hinrich Schütze, Masoud Jalili Sabet, Nora Kassner, Peiqin Lin, Silvia Severini |  |
| 1305 |  |  [Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction](https://doi.org/10.18653/v1/2023.acl-long.62) |  | 0 | Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model... | Haibin Chen, Huawen Feng, Junhao Zheng, Junlong Liu, Qianli Ma, Xichen Shang |  |
| 1306 |  |  [Pretrained Bidirectional Distillation for Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.63) |  | 0 | Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a... | Mei Tu, Yimeng Zhuang |  |
| 1307 |  |  [Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.64) |  | 0 | Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question... | Hanock Kwak, Jisu Jeong, JungWoo Ha, Kyungmin Kim, Kyuyong Shin, SangWoo Lee, Seungjae Jung, Wonjae Kim |  |
| 1308 |  |  [Improving Continual Relation Extraction by Distinguishing Analogous Semantics](https://doi.org/10.18653/v1/2023.acl-long.65) |  | 0 | Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the... | Wei Hu, Wenzheng Zhao, Yuanning Cui |  |
| 1309 |  |  [Improving Pretraining Techniques for Code-Switched NLP](https://doi.org/10.18653/v1/2023.acl-long.66) |  | 0 | Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world’s languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages... | Preethi Jyothi, Richeek Das, Sahasra Ranjan, Shreya Pathak |  |
| 1310 |  |  [A Theory of Unsupervised Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.67) |  | 0 | Unsupervised speech recognition ({pasted macro ‘ASRU’}/) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and... | Chang Dong Yoo, Liming Wang, Mark HasegawaJohnson |  |
| 1311 |  |  [ThinkSum: Probabilistic reasoning over sets using large language models](https://doi.org/10.18653/v1/2023.acl-long.68) |  | 0 | Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more... | Batu Ozturkler, Nebojsa Jojic, Nikolay Malkin, Zhen Wang |  |
| 1312 |  |  [NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist](https://doi.org/10.18653/v1/2023.acl-long.69) |  | 0 | In this study, we analyze automatic evaluation metrics for Natural Language Generation (NLG), specifically task-agnostic metrics and human-aligned metrics. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak... | Iftitahu Ni'mah, Meng Fang, Mykola Pechenizkiy, Vlado Menkovski |  |
| 1313 |  |  [DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations](https://doi.org/10.18653/v1/2023.acl-long.70) |  | 0 | In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer... | Ang Lv, Gao Xing, Ji Zhang, Jinpeng Li, Rui Yan, Yuhan Chen |  |
| 1314 |  |  [TECHS: Temporal Logical Graph Networks for Explainable Extrapolation Reasoning](https://doi.org/10.18653/v1/2023.acl-long.71) |  | 0 | Extrapolation reasoning on temporal knowledge graphs (TKGs) aims to forecast future facts based on past counterparts. There are two main challenges: (1) incorporating the complex information, including structural dependencies, temporal dynamics, and hidden logical rules; (2) implementing... | Erik Cambria, Fangzhi Xu, Jun Liu, Qika Lin, Rui Mao |  |
| 1315 |  |  [Consistency Regularization Training for Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.72) |  | 0 | Existing neural models have difficulty generalizing to unseen combinations of seen components. To achieve compositional generalization, models are required to consistently interpret (sub)expressions across contexts. Without modifying model architectures, we improve the capability of Transformer on... | Fandong Meng, Jiali Zeng, Jie Zhou, Yafu Li, Yongjing Yin, Yue Zhang |  |
| 1316 |  |  [NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation](https://doi.org/10.18653/v1/2023.acl-long.73) |  | 0 | In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the... | Chenfei Wu, Fan Yang, Houqiang Li, Huan Yang, Jianfeng Wang, Jianlong Fu, Lijuan Wang, Linjie Li, Ming Gong, Minheng Ni, Nan Duan, Shengming Yin, Shuguang Liu, Xiaodong Wang, Zhengyuan Yang, Zicheng Liu |  |
| 1317 |  |  [Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe](https://doi.org/10.18653/v1/2023.acl-long.74) |  | 0 | Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path... | David Levitan, Girish Kumar, Hoda Shajari, Huan Sun, Huseyin A. Inan, Julia McAnallen, Robert Sim, Xiang Yue, Xuechen Li |  |
| 1318 |  |  [A Close Look into the Calibration of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.75) |  | 0 | Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For... | Ganqu Cui, Heng Ji, Lifan Yuan, Yangyi Chen, Zhiyuan Liu |  |
| 1319 |  |  [DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.76) |  | 0 | Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to... | Baolin Peng, Jianfeng Gao, Michel Galley, Pengcheng He, Yu Li, Zhou Yu |  |
| 1320 |  |  [MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction](https://doi.org/10.18653/v1/2023.acl-long.77) |  | 0 | Given a text query, the task of Natural Language Video Localization (NLVL) is to localize a temporal moment in an untrimmed video that semantically matches the query. In this paper, we adopt a proposal-based solution that generates proposals (i.e. candidate moments) and then select the best... | Aixin Sun, Hao Zhang, Jing Wang, Xiaoli Li |  |
| 1321 |  |  [Diverse Demonstrations Improve In-context Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.78) |  | 0 | In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional... | Ben Bogin, Itay Levy, Jonathan Berant |  |
| 1322 |  |  [Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering](https://doi.org/10.18653/v1/2023.acl-long.79) |  | 0 | Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample... | Jiacheng Ye, Lingpeng Kong, Yaoxiang Wang, Zhiyong Wu |  |
| 1323 |  |  [On the Efficacy of Sampling Adapters](https://doi.org/10.18653/v1/2023.acl-long.80) |  | 0 | Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model’s sampling distribution, such as top-p or top-k... | Clara Meister, Ethan Wilcox, Luca Malagutti, Ryan Cotterell, Tiago Pimentel |  |
| 1324 |  |  [Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.81) |  | 0 | Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain... | Jianfei Yu, Qiankun Zhao, Rui Xia |  |
| 1325 |  |  [Compositional Data Augmentation for Abstractive Conversation Summarization](https://doi.org/10.18653/v1/2023.acl-long.82) |  | 0 | Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level... | Diyi Yang, Jiaao Chen, Jiawei Han, Siru Ouyang |  |
| 1326 |  |  [PMAES: Prompt-mapping Contrastive Learning for Cross-prompt Automated Essay Scoring](https://doi.org/10.18653/v1/2023.acl-long.83) |  | 0 | Current cross-prompt automated essay scoring (AES) is a challenging task due to the large discrepancies between different prompts, such as different genres and expressions. The main goal of current cross-prompt AES systems is to learn enough shared features between the source and target prompts to... | Xia Li, Yuan Chen |  |
| 1327 |  |  [Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models](https://doi.org/10.18653/v1/2023.acl-long.84) |  | 0 | To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any... | Dan Jurafsky, Esin Durmus, Myra Cheng |  |
| 1328 |  |  [On Prefix-tuning for Lightweight Out-of-distribution Detection](https://doi.org/10.18653/v1/2023.acl-long.85) |  | 0 | Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we... | Jianbing Zhang, Xinyu Dai, Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu |  |
| 1329 |  |  [GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding](https://doi.org/10.18653/v1/2023.acl-long.86) |  | 0 | Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we... | Alexander Podolskiy, Andrey Bout, Irina Piontkovskaya, Konstantin Yakovlev, Sergey I. Nikolenko |  |
| 1330 |  |  [Measuring Progress in Fine-grained Vision-and-Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.87) |  | 0 | While pretraining on large-scale image–text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack “fine-grained” understanding, such as the ability to recognise relationships, verbs, and numbers in images.... | Aida Nematzadeh, Aishwarya Agrawal, Emanuele Bugliarello, Laurent Sartran, Lisa Anne Hendricks |  |
| 1331 |  |  [Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information](https://doi.org/10.18653/v1/2023.acl-long.88) |  | 0 | Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach... | Hong Yu, Minhwa Lee, Rishabh Garodia, Sunjae Kwon, Zhichao Yang |  |
| 1332 |  |  [Chain-of-Skills: A Configurable Model for Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.89) |  | 0 | The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and... | Eric Nyberg, Hao Cheng, Jianfeng Gao, Kaixin Ma, Xiaodong Liu, Yu Zhang |  |
| 1333 |  |  [Elaboration-Generating Commonsense Question Answering at Scale](https://doi.org/10.18653/v1/2023.acl-long.90) |  | 0 | In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful... | Hannaneh Hajishirzi, Noah A. Smith, Vivek Srikumar, Wenya Wang |  |
| 1334 |  |  [Neural Unsupervised Reconstruction of Protolanguage Word Forms](https://doi.org/10.18653/v1/2023.acl-long.91) |  | 0 | We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with... | Andre He, Dan Klein, Nicholas Tomlin |  |
| 1335 |  |  [DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation](https://doi.org/10.18653/v1/2023.acl-long.92) |  | 0 | Self-training emerges as an important research line on domain adaptation. By taking the model’s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise)... | Dongsheng Li, Menglong Lu, Yang Liu, Yunxiang Zhao, Zhen Huang, Zhiliang Tian |  |
| 1336 |  |  [On Evaluating Multilingual Compositional Generalization with Translated Datasets](https://doi.org/10.18653/v1/2023.acl-long.93) |  | 0 | Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across... | Daniel Hershcovich, Zi Wang |  |
| 1337 |  |  [FAA: Fine-grained Attention Alignment for Cascade Document Ranking](https://doi.org/10.18653/v1/2023.acl-long.94) |  | 0 | Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high... | Chongyang Tao, Daxin Jiang, Dongyan Zhao, Jiazhan Feng, Tao Shen, Xiubo Geng, Zhen Li |  |
| 1338 |  |  [Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.95) |  | 0 | Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of... | Bang Liu, Junming Shao, Zhong Zhang |  |
| 1339 |  |  [Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach](https://doi.org/10.18653/v1/2023.acl-long.96) |  | 0 | Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To... | Bo Wang, Jinfeng Zhou, Minlie Huang, Zhuang Chen |  |
| 1340 |  |  [Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling](https://doi.org/10.18653/v1/2023.acl-long.97) |  | 0 | In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the... | Fan Wang, Hua Wu, Huang He, Mingzhu Cai, Siqi Bao, Xin Tian |  |
| 1341 |  |  [Why Aren't We NER Yet? Artifacts of ASR Errors in Named Entity Recognition in Spontaneous Speech Transcripts](https://doi.org/10.18653/v1/2023.acl-long.98) |  | 0 | Transcripts of spontaneous human speech present a significant obstacle for traditional NER models. The lack of grammatical structure of spoken utterances and word errors introduced by the ASR make downstream NLP tasks challenging. In this paper, we examine in detail the complex relationship between... | Adrian Szymczak, Krzysztof Surdyk, Lukasz Augustyniak, Mikolaj Morzy, Piotr Szymanski, Piotr Zelasko |  |
| 1342 |  |  [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://doi.org/10.18653/v1/2023.acl-long.99) |  | 0 | While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding... | Jamie Callan, Jimmy Lin, Luyu Gao, Xueguang Ma |  |
| 1343 |  |  [White-Box Multi-Objective Adversarial Attack on Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.100) |  | 0 | Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One... | Cong Liu, Yingfan Gao, Yufei Li, Zexin Li |  |
| 1344 |  |  [A Cautious Generalization Goes a Long Way: Learning Morphophonological Rules](https://doi.org/10.18653/v1/2023.acl-long.101) |  | 0 | Explicit linguistic knowledge, encoded by resources such as rule-based morphological analyzers, continues to prove useful in downstream NLP tasks, especially for low-resource languages and dialects. Rules are an important asset in descriptive linguistic grammars. However, creating such resources is... | Ellen Broselow, Jordan Kodner, Owen Rambow, Salam Khalifa, Sarah R. B. Payne |  |
| 1345 |  |  [Few-shot Adaptation Works with UnpredicTable Data](https://doi.org/10.18653/v1/2023.acl-long.102) |  | 0 | Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public... | Ethan Perez, Jonathan Jao, Jun Shern Chan, Jérémy Scheurer, Michael Pieler |  |
| 1346 |  |  [Cross-lingual Science Journalism: Select, Simplify and Rewrite Summaries for Non-expert Readers](https://doi.org/10.18653/v1/2023.acl-long.103) |  | 0 | Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science... | Mehwish Fatima, Michael Strube |  |
| 1347 |  |  [HuCurl: Human-induced Curriculum Discovery](https://doi.org/10.18653/v1/2023.acl-long.104) |  | 0 | We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the... | Hadi Amiri, Mohamed Elgaar |  |
| 1348 |  |  [kNN-TL: k-Nearest-Neighbor Transfer Learning for Low-Resource Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.105) |  | 0 | Transfer learning has been shown to be an effective technique for enhancing the performance of low-resource neural machine translation (NMT). This is typically achieved through either fine-tuning a child model with a pre-trained parent model, or by utilizing the out- put of the parent model during... | Derek F. Wong, Lidia S. Chao, Min Zhang, Shudong Liu, Wenxiang Jiao, Xuebo Liu, Zhaocong Li |  |
| 1349 |  |  [Do language models have coherent mental models of everyday things?](https://doi.org/10.18653/v1/2023.acl-long.106) |  | 0 | When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To... | Bhavana Dalvi Mishra, Peter Clark, Yuling Gu |  |
| 1350 |  |  [Rogue Scores](https://doi.org/10.18653/v1/2023.acl-long.107) |  | 0 | Correct, comparable, and reproducible model evaluation is essential for progress in machine learning. Over twenty years, thousands of language and vision models have been evaluated with a popular metric called ROUGE. Does this widespread benchmark metric meet these three evaluation criteria? This... | Max Grusky |  |
| 1351 |  |  [Instruction Induction: From Few Examples to Natural Language Task Descriptions](https://doi.org/10.18653/v1/2023.acl-long.108) |  | 0 | Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language... | Omer Levy, Or Honovich, Samuel R. Bowman, Uri Shaham |  |
| 1352 |  |  [In-Context Analogical Reasoning with Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.109) |  | 0 | Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or... | Joyce Chai, Richard L. Lewis, Shane Storks, Xiaoyang Hu |  |
| 1353 |  |  [Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering](https://doi.org/10.18653/v1/2023.acl-long.110) |  | 0 | The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering... | Arman Cohan, Avi Caciularu, Ido Dagan, Jacob Goldberger, Matthew E. Peters |  |
| 1354 |  |  [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.111) |  | 0 | It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process,... | Chun Yuan, Mu Li, Xingjian Shi, Yi Zhu, Yuxin Ren, Zihan Zhong |  |
| 1355 |  |  [REV: Information-Theoretic Evaluation of Free-Text Rationales](https://doi.org/10.18653/v1/2023.acl-long.112) |  | 0 | Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new... | Faeze Brahman, Hanjie Chen, Swabha Swayamdipta, Xiang Ren, Yangfeng Ji, Yejin Choi |  |
| 1356 |  |  [ELQA: A Corpus of Metalinguistic Questions and Answers about English](https://doi.org/10.18653/v1/2023.acl-long.113) |  | 0 | We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general... | Amir Zeldes, Keisuke Sakaguchi, Nathan Schneider, Shabnam Behzad |  |
| 1357 |  |  [Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking](https://doi.org/10.18653/v1/2023.acl-long.114) |  | 0 | Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively... | Dacheng Tao, Li Guo, Liang Ding, Qingyue Wang, Shi Wang, Yanan Cao, Yibing Zhan, Zheng Lin |  |
| 1358 |  |  [BIG-C: a Multimodal Multi-Purpose Dataset for Bemba](https://doi.org/10.18653/v1/2023.acl-long.115) |  | 0 | We present BIG-C (Bemba Image Grounded Conversations), a large multimodal dataset for Bemba. While Bemba is the most populous language of Zambia, it exhibits a dearth of resources which render the development of language technologies or language processing research almost impossible. The dataset is... | Antonios Anastasopoulos, Claytone Sikasote, Eunice Mukonde, Md Mahfuz Ibn Alam |  |
| 1359 |  |  [Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues](https://doi.org/10.18653/v1/2023.acl-long.116) |  | 0 | User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user’s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes,... | Animesh Prasad, Emine Yilmaz, Gabriella Kazai, Nikolaos Aletras, Yue Feng, Yunlong Jiao |  |
| 1360 |  |  [Robust Multi-bit Natural Language Watermarking through Invariant Features](https://doi.org/10.18653/v1/2023.acl-long.117) |  | 0 | Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security... | Jiho Jang, KiYoon Yoo, Nojun Kwak, Wonhyuk Ahn |  |
| 1361 |  |  [KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.118) |  | 0 | With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge... | Shangbin Feng, Wenqian Zhang, Yulia Tsvetkov, Zhaoxuan Tan, Zhenyu Lei |  |
| 1362 |  |  [AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction](https://doi.org/10.18653/v1/2023.acl-long.119) |  | 0 | Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally... | Bingcong Xue, Lei Zou, Ruoyu Zhang, Yanzeng Li |  |
| 1363 |  |  [Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.120) |  | 0 | The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is... | David Wan, Mohit Bansal, Shiyue Zhang |  |
| 1364 |  |  [Improving Translation Quality Estimation with Bias Mitigation](https://doi.org/10.18653/v1/2023.acl-long.121) |  | 0 | State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation... | Hui Di, Hui Huang, Kehai Chen, Muyun Yang, Shuangzhi Wu, Tiejun Zhao |  |
| 1365 |  |  [Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation](https://doi.org/10.18653/v1/2023.acl-long.122) |  | 0 | We propose a genetic algorithm (GA) based method for modifying n-best lists produced by a machine translation (MT) system. Our method offers an innovative approach to improving MT quality and identifying weaknesses in evaluation metrics. Using common GA operations (mutation and crossover) on a list... | Josef Jon, Ondrej Bojar |  |
| 1366 |  |  [MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions](https://doi.org/10.18653/v1/2023.acl-long.123) |  | 0 | Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users’ values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our... | Bin Wang, Fei Mi, Hao Sun, Jianwei Cui, Minlie Huang, Qun Liu, Wei Liu, Yasheng Wang, Zhexin Zhang |  |
| 1367 |  |  [Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion](https://doi.org/10.18653/v1/2023.acl-long.124) |  | 0 | Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in... | Binghuai Lin, Damai Dai, Shaoxiang Wu, Tianyu Liu, Yunbo Cao, Zhifang Sui, Ziwei Qin |  |
| 1368 |  |  [SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval](https://doi.org/10.18653/v1/2023.acl-long.125) |  | 0 | In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised... | Binxing Jiao, Daxin Jiang, Furu Wei, Liang Wang, Linjun Yang, Nan Yang, Rangan Majumder, Xiaolong Huang |  |
| 1369 |  |  [From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained](https://doi.org/10.18653/v1/2023.acl-long.126) |  | 0 | For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly... | Hongliang Dai, Ziqian Zeng |  |
| 1370 |  |  [Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers](https://doi.org/10.18653/v1/2023.acl-long.127) |  | 0 | To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model’s predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the... | Amit Sharma, Parikshit Bansal |  |
| 1371 |  |  [What Makes Pre-trained Language Models Better Zero-shot Learners?](https://doi.org/10.18653/v1/2023.acl-long.128) |  | 0 | Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is... | Brian Mac Namee, Dongsheng Zhu, Fei Tan, Jinghui Lu, Rui Zhao, Weidong Han |  |
| 1372 |  |  [Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations](https://doi.org/10.18653/v1/2023.acl-long.129) |  | 0 | Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using... | Hannaneh Hajishirzi, Iz Beltagy, Luke Zettlemoyer, Sewon Min, Xinxi Lyu |  |
| 1373 |  |  [Learning Optimal Policy for Simultaneous Machine Translation via Binary Search](https://doi.org/10.18653/v1/2023.acl-long.130) |  | 0 | Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token.... | Shaolei Zhang, Shoutao Guo, Yang Feng |  |
| 1374 |  |  [Better Simultaneous Translation with Monotonic Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.131) |  | 0 | Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix... | Jing Wu, Jun Xiao, Kai Fan, Shushu Wang, Wei Luo, Zhongqiang Huang |  |
| 1375 |  |  [StoryARG: a corpus of narratives and personal experiences in argumentative texts](https://doi.org/10.18653/v1/2023.acl-long.132) |  | 0 | Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do – and yet, this phenomenon is largely... | Gabriella Lapesa, Neele Falk |  |
| 1376 |  |  [Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue](https://doi.org/10.18653/v1/2023.acl-long.133) |  | 0 | Factual correctness is often the limiting factor in practical applications of natural language generation in high-stakes domains such as healthcare. An essential requirement for maintaining factuality is the ability to deal with rare tokens. This paper focuses on rare tokens that appear in both the... | Anitha Kannan, Ilya Valmianski, Maksim Eremeev, Xavier Amatriain |  |
| 1377 |  |  [Sequence Parallelism: Long Sequence Training from System Perspective](https://doi.org/10.18653/v1/2023.acl-long.134) |  | 0 | Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism,... | Chaitanya Baranwal, Fuzhao Xue, Shenggui Li, Yang You, Yongbin Li |  |
| 1378 |  |  [MUSTIE: Multimodal Structural Transformer for Web Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.135) |  | 0 | The task of web information extraction is to extract target fields of an object from web pages, such as extracting the name, genre and actor from a movie page. Recent sequential modeling approaches have achieved state-of-the-art results on web information extraction. However, most of these methods... | Dongfang Liu, Fuli Feng, Hamed Firooz, Jingang Wang, Madian Khabsa, Qifan Wang, Shaoliang Nie, Sinong Wang, Xiaojun Quan, Zenglin Xu |  |
| 1379 |  |  [Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In](https://doi.org/10.18653/v1/2023.acl-long.136) |  | 0 | Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic... | Chenyan Xiong, Shi Yu, Zhiyuan Liu, Zichun Yu |  |
| 1380 |  |  [TableVLM: Multi-modal Pre-training for Table Structure Recognition](https://doi.org/10.18653/v1/2023.acl-long.137) |  | 0 | Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially... | Chengsong Huang, Jinshu Lin, Leiyuan Chen, Xiaoqing Zheng, Xuanjing Huang |  |
| 1381 |  |  [Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?](https://doi.org/10.18653/v1/2023.acl-long.138) |  | 0 | Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor... | Jiashu Xu, Mingyu Derek Ma, Muhao Chen |  |
| 1382 |  |  [Dynamic Routing Transformer Network for Multimodal Sarcasm Detection](https://doi.org/10.18653/v1/2023.acl-long.139) |  | 0 | Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal... | Nan Xu, Ruike Zhang, Wenji Mao, Yuan Tian |  |
| 1383 |  |  [What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary](https://doi.org/10.18653/v1/2023.acl-long.140) |  | 0 | Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector... | Adi Zicher, Amir Globerson, Jonathan Berant, Liat Bezalel, Ori Ram, Yonatan Belinkov |  |
| 1384 |  |  [Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach](https://doi.org/10.18653/v1/2023.acl-long.141) |  | 0 | We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a... | Chao Zhang, Jiaming Shen, Jieyu Zhang, Ran Xu, Rongzhi Zhang, Yue Yu |  |
| 1385 |  |  [Training-free Neural Architecture Search for RNNs and Transformers](https://doi.org/10.18653/v1/2023.acl-long.142) |  | 0 | Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of... | Aaron Serianni, Jugal Kalita |  |
| 1386 |  |  [CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1, 500+ Language Pairs](https://doi.org/10.18653/v1/2023.acl-long.143) |  | 0 | We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization... | Abhik Bhattacharjee, Rifat Shahriyar, Tahmid Hasan, Wasi Uddin Ahmad, YongBin Kang, YuanFang Li |  |
| 1387 |  |  [Improving Gradient Trade-offs between Tasks in Multi-task Text Classification](https://doi.org/10.18653/v1/2023.acl-long.144) |  | 0 | Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. However, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since... | Binxing Fang, Heyan Chai, Jinhao Cui, Min Zhang, Qing Liao, Ye Wang |  |
| 1388 |  |  [Bi-Phone: Modeling Inter Language Phonetic Influences in Text](https://doi.org/10.18653/v1/2023.acl-long.145) |  | 0 | A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine... | Abhirut Gupta, Ambarish Jash, Ananya B. Sai, Aravindan Raghuveer, James S. Ren, Richard Sproat, Sukhdeep S. Sodhi, Yuri Vasilevski |  |
| 1389 |  |  [Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment](https://doi.org/10.18653/v1/2023.acl-long.146) |  | 0 | Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the... | Hao Fei, Shengqiong Wu, TatSeng Chua, Wei Ji |  |
| 1390 |  |  [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.147) |  | 0 | Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly... | EePeng Lim, Lei Wang, Roy KaWei Lee, Wanyu Xu, Yihuai Lan, Yunshi Lan, Zhiqiang Hu |  |
| 1391 |  |  [RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models](https://doi.org/10.18653/v1/2023.acl-long.148) |  | 0 | To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the... | Shitao Xiao, Yingxia Shao, Zhao Cao, Zheng Liu |  |
| 1392 |  |  [DecompX: Explaining Transformers Decisions by Propagating Token Decomposition](https://doi.org/10.18653/v1/2023.acl-long.149) |  | 0 | An emerging solution for explaining Transformer-based models is to use vector-based analysis on how the representations are formed. However, providing a faithful vector-based explanation for a multi-layer model could be challenging in three aspects: (1) Incorporating all components into the... | Ali Modarressi, Ehsan Aghazadeh, Mohammad Taher Pilehvar, Mohsen Fayyaz, Yadollah Yaghoobzadeh |  |
| 1393 |  |  [Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step](https://doi.org/10.18653/v1/2023.acl-long.150) |  | 0 | Chain-of-thought prompting (e.g., “Let’s think step-by-ste”) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show... | Jack Hessel, KaiWei Chang, Liunian Harold Li, Xiang Ren, Yejin Choi, Youngjae Yu |  |
| 1394 |  |  [Generating EDU Extracts for Plan-Guided Summary Re-Ranking](https://doi.org/10.18653/v1/2023.acl-long.151) |  | 0 | Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant,... | Alexander R. Fabbri, Faisal Ladhak, Griffin Adams, Kathleen R. McKeown, Noémie Elhadad |  |
| 1395 |  |  [A Survey on Asking Clarification Questions Datasets in Conversational Systems](https://doi.org/10.18653/v1/2023.acl-long.152) |  | 0 | The ability to understand a user’s underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users’ true intent from their queries or utterances arise as an essential... | Aldo Lipani, Emine Yilmaz, Hossein A. Rahmani, Qiang Zhang, Xi Wang, Yue Feng |  |
| 1396 |  |  [Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters](https://doi.org/10.18653/v1/2023.acl-long.153) |  | 0 | Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its... | Boshi Wang, Huan Sun, Jiaming Shen, Luke Zettlemoyer, Sewon Min, Xiang Deng, You Wu |  |
| 1397 |  |  [Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.154) |  | 0 | For many languages, machine translation progress is hindered by the lack of reliable training data. Models are trained on whatever pre-existing datasets may be available and then augmented with synthetic data, because it is often not economical to pay for the creation of large-scale datasets. But... | Angela Fan, Cynthia Gao, Elahe Kalbassi, Francisco Guzmán, Jean Maillard, Kaushik Ram Sadagopan, Philipp Koehn, Vedanuj Goswami |  |
| 1398 |  |  [RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks](https://doi.org/10.18653/v1/2023.acl-long.155) |  | 0 | Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during... | Jiahai Wang, Qinliang Su, Xiaopeng Zheng, Zhaoyang Wang, Zhiyue Liu |  |
| 1399 |  |  [Gradient-based Intra-attention Pruning on Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.156) |  | 0 | Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention... | Shijin Wang, Xin Yao, Yiming Cui, Ziqing Yang |  |
| 1400 |  |  [Learning to Substitute Spans towards Improving Compositional Generalization](https://doi.org/10.18653/v1/2023.acl-long.157) |  | 0 | Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias.... | Defu Lian, Ying Wei, Zhaoyi Li |  |
| 1401 |  |  [DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.acl-long.158) |  | 0 | Empathy is a crucial factor in open-domain conversations, which naturally shows one’s caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this... | Guanqun Bi, Lei Shen, Meng Chen, Xiaodong He, Yanan Cao, Yuqiang Xie, Zheng Lin |  |
| 1402 |  |  [BREAK: Breaking the Dialogue State Tracking Barrier with Beam Search and Re-ranking](https://doi.org/10.18653/v1/2023.acl-long.159) |  | 0 | Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue... | Heeyoung Kwak, Janghoon Han, Joongbo Shin, Kyomin Jung, Seungpil Won |  |
| 1403 |  |  [Faithful Low-Resource Data-to-Text Generation through Cycle Training](https://doi.org/10.18653/v1/2023.acl-long.160) |  | 0 | Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient... | Marcus D. Collins, Nikhita Vedula, Oleg Rokhlenko, Shervin Malmasi, Simone Filice, Zhuoer Wang |  |
| 1404 |  |  [Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing](https://doi.org/10.18653/v1/2023.acl-long.161) |  | 0 | Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. Toincrease the performance stability, previous debiasing... | Bing Qin, Jingshuo Liu, Li Du, Ting Liu, Xiao Ding, Zhouhao Sun |  |
| 1405 |  |  [Dynamic and Efficient Inference for Text Generation via BERT Family](https://doi.org/10.18653/v1/2023.acl-long.162) |  | 0 | Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning... | Juntao Li, Lijun Wu, Min Zhang, Xiaobo Liang, Ziqiang Cao |  |
| 1406 |  |  [Learning to Generate Equitable Text in Dialogue from Biased Training Data](https://doi.org/10.18653/v1/2023.acl-long.163) |  | 0 | The ingrained principles of fairness in a dialogue system’s decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts... | Anthony Sicilia, Malihe Alikhani |  |
| 1407 |  |  [Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.164) |  | 0 | Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models... | Baoyuan Wang, Jingsheng Gao, Ke Ji, Yixin Lian |  |
| 1408 |  |  [Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.165) |  | 0 | The goal of multimodal abstractive summarization (MAS) is to produce a concise summary given the multimodal data (text and vision). Existing studies on MAS mainly focus on how to effectively use the extracted visual features, having achieved impressive success on the high-resource English dataset.... | Fandong Meng, Jiaan Wang, Jie Zhou, Jinan Xu, Yufeng Chen, Yunlong Liang |  |
| 1409 |  |  [Helping a Friend or Supporting a Cause? Disentangling Active and Passive Cosponsorship in the U.S. Congress](https://doi.org/10.18653/v1/2023.acl-long.166) |  | 0 | In the U.S. Congress, legislators can use active and passive cosponsorship to support bills. We show that these two types of cosponsorship are driven by two different motivations: the backing of political colleagues and the backing of the bill’s content. To this end, we develop an Encoder+RGCN... | Christoph Gote, Frank Schweitzer, Giuseppe Russo, Laurence Brandenberger, Sophia Schlosser |  |
| 1410 |  |  [TREA: Tree-Structure Reasoning Schema for Conversational Recommendation](https://doi.org/10.18653/v1/2023.acl-long.167) |  | 0 | Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of... | Dangyang Chen, Wei Wei, Wendi Li, Wenfeng Xie, XianLing Mao, Xiaoye Qu, Ye Yuan |  |
| 1411 |  |  [CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality](https://doi.org/10.18653/v1/2023.acl-long.168) |  | 0 | There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while... | Bing Li, Binhua Li, Can Ma, Chengyang Fang, Fei Huang, Liang Li, Rongyu Cao, Ruiying Geng, Yongbin Li |  |
| 1412 |  |  [Multilingual Multifaceted Understanding of Online News in Terms of Genre, Framing, and Persuasion Techniques](https://doi.org/10.18653/v1/2023.acl-long.169) |  | 0 | We present a new multilingual multifacet dataset of news articles, each annotated for genre (objective news reporting vs. opinion vs. satire), framing (what key aspects are highlighted), and persuasion techniques (logical fallacies, emotional appeals, ad hominem attacks, etc.). The persuasion... | Giovanni Da San Martino, Jakub Piskorski, Nicolas Stefanovitch, Nikolaos Nikolaidis, Preslav Nakov |  |
| 1413 |  |  [Learning Action Conditions from Instructional Manuals for Instruction Understanding](https://doi.org/10.18653/v1/2023.acl-long.170) |  | 0 | The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action... | Alexander Spangher, Caiqi Zhang, Nanyun Peng, Qingyuan Hu, TeLin Wu |  |
| 1414 |  |  [StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation](https://doi.org/10.18653/v1/2023.acl-long.171) |  | 0 | Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora.... | Lydia B. Chilton, Yulun Du |  |
| 1415 |  |  [Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning](https://doi.org/10.18653/v1/2023.acl-long.172) |  | 0 | Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study... | Caiming Xiong, ChienSheng Wu, Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty |  |
| 1416 |  |  [Do PLMs Know and Understand Ontological Knowledge?](https://doi.org/10.18653/v1/2023.acl-long.173) |  | 0 | Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge,... | Chengyue Jiang, Kewei Tu, Pengjun Xie, Weiqi Wu, Yong Jiang |  |
| 1417 |  |  [CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection](https://doi.org/10.18653/v1/2023.acl-long.174) |  | 0 | Establishing retrieval-based dialogue systems that can select appropriate responses from the pre-built index has gained increasing attention. Recent common practice is to construct a two-stage pipeline with a fast retriever (e.g., bi-encoder) for first-stage recall followed by a smart response... | Chang Liu, Chongyang Tao, Daxin Jiang, Jiazhan Feng, Juntao Li, Tao Shen, Xiubo Geng |  |
| 1418 |  |  [Exploring How Generative Adversarial Networks Learn Phonological Representations](https://doi.org/10.18653/v1/2023.acl-long.175) |  | 0 | This paper explores how Generative Adversarial Networks (GANs) learn representations of phonological phenomena. We analyze how GANs encode contrastive and non-contrastive nasality in French and English vowels by applying the ciwGAN architecture (Begus, 2021). Begus claims that ciwGAN encodes... | Jingyi Chen, Micha Elsner |  |
| 1419 |  |  [Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis](https://doi.org/10.18653/v1/2023.acl-long.176) |  | 0 | We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is... | Andrey Kutuzov, Iris Luden, Mario Giulianelli, Raquel Fernández |  |
| 1420 |  |  [Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.177) |  | 0 | Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to... | Hao Yan, Saurabh Srivastava, Sida I. Wang, Wentau Yih, Yintao Tai, Ziyu Yao |  |
| 1421 |  |  [InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation](https://doi.org/10.18653/v1/2023.acl-long.178) |  | 0 | Automatic image captioning evaluation is critical for benchmarking and promoting advances in image captioning research. Existing metrics only provide a single score to measure caption qualities, which are less explainable and informative. Instead, we humans can easily identify the problems of... | Anwen Hu, Liang Zhang, Qin Jin, Shizhe Chen |  |
| 1422 |  |  [An Invariant Learning Characterization of Controlled Text Generation](https://doi.org/10.18653/v1/2023.acl-long.179) |  | 0 | Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic... | Amir Feder, Carolina Zheng, Claudia Shi, David M. Blei, Keyon Vafa |  |
| 1423 |  |  [HistRED: A Historical Document-Level Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.180) |  | 0 | Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok.... | Jaegul Choo, Minseok Choi, Soyoung Yang, Youngwoo Cho |  |
| 1424 |  |  [A Critical Evaluation of Evaluations for Long-form Question Answering](https://doi.org/10.18653/v1/2023.acl-long.181) |  | 0 | Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts... | Eunsol Choi, Fangyuan Xu, Mohit Iyyer, Yixiao Song |  |
| 1425 |  |  [HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation](https://doi.org/10.18653/v1/2023.acl-long.182) |  | 0 | Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple... | Chuanqi Tan, Fei Huang, Hongyi Yuan, Songfang Huang, Zheng Yuan |  |
| 1426 |  |  [Generating User-Engaging News Headlines](https://doi.org/10.18653/v1/2023.acl-long.183) |  | 0 | The potential choices for news article headlines are enormous, and finding the right balance between conveying the essential message and capturing the reader’s attention is key to effective headlining. However, presenting the same news headline to all readers is a suboptimal strategy, because it... | Dong Yu, Fei Liu, Hong Yu, Hongwei Wang, Kaiqiang Song, Pengshan Cai, Sangwoo Cho, Xiaoyang Wang |  |
| 1427 |  |  [Word sense extension](https://doi.org/10.18653/v1/2023.acl-long.184) |  | 0 | Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a... | Lei Yu, Yang Xu |  |
| 1428 |  |  [PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism](https://doi.org/10.18653/v1/2023.acl-long.185) |  | 0 | We investigate response generation for multi-turn dialogue in generative chatbots. Existing generative modelsbased on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the history, which makesmodels unable to capture the subtle variability observed in different... | Daling Wang, Hinrich Schütze, Shi Feng, Yifei Zhang, Yongkang Liu |  |
| 1429 |  |  [Decoding Symbolism in Language Models](https://doi.org/10.18653/v1/2023.acl-long.186) |  | 0 | This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on... | Adriana Kovashka, Meiqi Guo, Rebecca Hwa |  |
| 1430 |  |  [A Survey on Zero Pronoun Translation](https://doi.org/10.18653/v1/2023.acl-long.187) |  | 0 | Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to... | Linfeng Song, Longyue Wang, Mingzhou Xu, Shuming Shi, Siyou Liu, Zhaopeng Tu |  |
| 1431 |  |  [We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit](https://doi.org/10.18653/v1/2023.acl-long.188) |  | 0 | Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the... | Alessandro Lenci, Davide Testa, Emmanuele Chersoni |  |
| 1432 |  |  [MPCHAT: Towards Multimodal Persona-Grounded Conversation](https://doi.org/10.18653/v1/2023.acl-long.189) |  | 0 | In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker’s personal... | Gunhee Kim, Jaewoo Ahn, Sangdoo Yun, Yeda Song |  |
| 1433 |  |  [DOC: Improving Long Story Coherence With Detailed Outline Control](https://doi.org/10.18653/v1/2023.acl-long.190) |  | 0 | We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more... | Dan Klein, Kevin Yang, Nanyun Peng, Yuandong Tian |  |
| 1434 |  |  [Dual-Alignment Pre-training for Cross-lingual Sentence Embedding](https://doi.org/10.18653/v1/2023.acl-long.191) |  | 0 | Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully... | Furu Wei, Haizhen Huang, Jian Jiao, Qi Zhang, Qiang Lou, Shaohan Huang, Weiwei Deng, ZhiHong Deng, Zihan Zhang, Ziheng Li |  |
| 1435 |  |  [Exploring Better Text Image Translation with Multimodal Codebook](https://doi.org/10.18653/v1/2023.acl-long.192) |  | 0 | Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly... | Bin Wang, Degen Huang, Jian Luan, Jiawei Yu, Jinsong Su, Wen Zhang, Xiang Li, Zhibin Lan |  |
| 1436 |  |  [FEDLEGAL: The First Real-World Federated Learning Benchmark for Legal NLP](https://doi.org/10.18653/v1/2023.acl-long.193) |  | 0 | The inevitable private information in legal data necessitates legal artificial intelligence to study privacy-preserving and decentralized learning methods. Federated learning (FL) has merged as a promising technique for multiple participants to collaboratively train a shared model while efficiently... | Hui Wang, Jingyuan Zhang, Lizhen Qu, Xiangjing Hu, Yating Zhang, Zenglin Xu, Zhuo Zhang |  |
| 1437 |  |  [A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning](https://doi.org/10.18653/v1/2023.acl-long.194) |  | 0 | Parameter-Efficient Tuning (PET) has shown remarkable performance by fine-tuning only a small number of parameters of the pre-trained language models (PLMs) for the downstream tasks, while it is also possible to construct backdoor attacks due to the vulnerability of pre-trained weights. However, a... | Naibin Gu, Peng Fu, Weiping Wang, Xiyu Liu, Zheng Lin, Zhengxiao Liu |  |
| 1438 |  |  [History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling](https://doi.org/10.18653/v1/2023.acl-long.195) |  | 0 | Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range... | Binhua Li, Binyuan Hui, Bowen Li, Hao Sun, Liwei Deng, Yan Zhang, Yang Li, Yongbin Li, Yunshi Lan |  |
| 1439 |  |  [From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation](https://doi.org/10.18653/v1/2023.acl-long.196) |  | 0 | Entailment Graphs (EGs) have been constructed based on extracted corpora as a strong and explainable form to indicate context-independent entailment relation in natural languages. However, EGs built by previous methods often suffer from the severe sparsity issues, due to limited corpora available... | Dongyan Zhao, Yansong Feng, Zhibin Chen |  |
| 1440 |  |  [Alleviating Over-smoothing for Unsupervised Sentence Representation](https://doi.org/10.18653/v1/2023.acl-long.197) |  | 0 | Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the... | Bowen Cao, Daxin Jiang, Jia Li, Jian Pei, Jianhui Chang, Linjun Shou, Ming Gong, Nuo Chen |  |
| 1441 |  |  [Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model](https://doi.org/10.18653/v1/2023.acl-long.198) |  | 0 | The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs... | Alexandre Berard, Vassilina Nikoulina, Yeskendir Koishekenov |  |
| 1442 |  |  [DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.acl-long.199) |  | 0 | Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust... | Christopher Hidey, Diyi Yang, Eric Zhu, Fei Liu, Rahul Goel, Rushin Shah, William Held |  |
| 1443 |  |  [From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.200) |  | 0 | Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model’s robustness to... | Cha Zhang, Dinei A. F. Florêncio, Florian Luisier, Kayhan Batmanghelich, Li Sun |  |
| 1444 |  |  [MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling](https://doi.org/10.18653/v1/2023.acl-long.201) |  | 0 | We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP... | Bang Liu, Santiago Miret, Yu Song |  |
| 1445 |  |  [Code4Struct: Code Generation for Few-Shot Event Structure Prediction](https://doi.org/10.18653/v1/2023.acl-long.202) |  | 0 | Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure... | Heng Ji, Sha Li, Xingyao Wang |  |
| 1446 |  |  [GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles](https://doi.org/10.18653/v1/2023.acl-long.203) |  | 0 | Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder... | IHung Hsu, KaiWei Chang, KuanHao Huang, Nanyun Peng, Tanmay Parekh |  |
| 1447 |  |  [Efficient Semiring-Weighted Earley Parsing](https://doi.org/10.18653/v1/2023.acl-long.204) |  | 0 | We present Earley’s (1970) context-free parsing algorithm as a deduction system, incorporating various known and new speed-ups. In particular, our presentation supports a known worst-case runtime improvement from Earley’s (1970) O(N3\|G\|\|R\|), which is unworkable for the large grammars that arise... | Andreas Opedal, Jason Eisner, Ran Zmigrod, Ryan Cotterell, Tim Vieira |  |
| 1448 |  |  [Tree-Based Representation and Generation of Natural and Mathematical Language](https://doi.org/10.18653/v1/2023.acl-long.205) |  | 0 | Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or... | Alexander Scarlatos, Andrew S. Lan |  |
| 1449 |  |  [ParaLS: Lexical Substitution via Pretrained Paraphraser](https://doi.org/10.18653/v1/2023.acl-long.206) |  | 0 | Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However,... | Jipeng Qiang, Kang Liu, Yi Zhu, Yun Li, Yunhao Yuan |  |
| 1450 |  |  [Peer-Label Assisted Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.207) |  | 0 | Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with... | Feifei Wang, Junru Song, Yang Yang |  |
| 1451 |  |  [Free Lunch for Efficient Textual Commonsense Integration in Language Models](https://doi.org/10.18653/v1/2023.acl-long.208) |  | 0 | Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP... | Wanyun Cui, Xingran Chen |  |
| 1452 |  |  [A Probabilistic Framework for Discovering New Intents](https://doi.org/10.18653/v1/2023.acl-long.209) |  | 0 | Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not... | Guofeng Quan, Xipeng Qiu, Yunhua Zhou |  |
| 1453 |  |  [MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.210) |  | 0 | Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the... | Leonhard Hennig, Philippe Thomas, Sebastian Möller |  |
| 1454 |  |  [Towards Higher Pareto Frontier in Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.211) |  | 0 | Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others.... | Baohang Li, Bing Qin, Xiaocheng Feng, Xinwei Geng, YiChong Huang |  |
| 1455 |  |  [Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization](https://doi.org/10.18653/v1/2023.acl-long.212) |  | 0 | By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin. However, due to the high computational cost, the huge number of parameters also restricts the... | JiRong Wen, Kun Zhou, Peiyu Liu, Wayne Xin Zhao, ZeFeng Gao |  |
| 1456 |  |  [Entity Tracking in Language Models](https://doi.org/10.18653/v1/2023.acl-long.213) |  | 0 | Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to... | Najoung Kim, Sebastian Schuster |  |
| 1457 |  |  [A Textual Dataset for Situated Proactive Response Selection](https://doi.org/10.18653/v1/2023.acl-long.214) |  | 0 | Recent data-driven conversational models are able to return fluent, consistent, and informative responses to many kinds of requests and utterances in task-oriented scenarios. However, these responses are typically limited to just the immediate local topic instead of being wider-ranging and... | Eduard H. Hovy, HyeongSik Kim, Jun Araki, Naoki Otani |  |
| 1458 |  |  [DiffusionNER: Boundary Diffusion for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.215) |  | 0 | In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion... | Dongsheng Li, Kaitao Song, Weiming Lu, Xu Tan, Yongliang Shen, Yueting Zhuang |  |
| 1459 |  |  [WACO: Word-Aligned Contrastive Learning for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.216) |  | 0 | End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity... | Lei Li, Rong Ye, Siqi Ouyang |  |
| 1460 |  |  [Cross-lingual Continual Learning](https://doi.org/10.18653/v1/2023.acl-long.217) |  | 0 | The longstanding goal of multi-lingual learning has been to develop a universal cross-lingual model that can withstand the changes in multi-lingual data distributions. There has been a large amount of work to adapt such multi-lingual models to unseen target languages. However, the majority of work... | Jonathan May, Meryem M'hamdi, Xiang Ren |  |
| 1461 |  |  [Faithful Question Answering with Monte-Carlo Planning](https://doi.org/10.18653/v1/2023.acl-long.218) |  | 0 | Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based... | Changshui Zhang, Dong Yu, Hong Zhao, Hongming Zhang, Ruixin Hong |  |
| 1462 |  |  [Unbalanced Optimal Transport for Unbalanced Word Alignment](https://doi.org/10.18653/v1/2023.acl-long.219) |  | 0 | Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful... | Han Bao, Sho Yokoi, Yuki Arase |  |
| 1463 |  |  [Guiding Computational Stance Detection with Expanded Stance Triangle Framework](https://doi.org/10.18653/v1/2023.acl-long.220) |  | 0 | Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational... | Hai Leong Chieu, Nancy F. Chen, Yong Keong Yap, Zhengyuan Liu |  |
| 1464 |  |  [Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast](https://doi.org/10.18653/v1/2023.acl-long.221) |  | 0 | Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the... | Bing Liu, Dongyan Zhao, Nan Duan, Yaobo Liang, Yiduo Guo |  |
| 1465 |  |  [Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning](https://doi.org/10.18653/v1/2023.acl-long.222) |  | 0 | In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this... | Chunyan Miao, Erik Cambria, Lidong Bing, Ran Zhou, Xin Li |  |
| 1466 |  |  [MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks](https://doi.org/10.18653/v1/2023.acl-long.223) |  | 0 | Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates... | Anette Frank, Letitia Parcalabescu |  |
| 1467 |  |  [Towards Boosting the Open-Domain Chatbot with Human Feedback](https://doi.org/10.18653/v1/2023.acl-long.224) |  | 0 | Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses. This phenomenon might mainly result from the deficiency of annotated human-human conversations and the misalignment with human preference. In... | Fan Wang, Haifeng Wang, Hua Lu, Hua Wu, Huang He, Siqi Bao |  |
| 1468 |  |  [Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations](https://doi.org/10.18653/v1/2023.acl-long.225) |  | 0 | Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of... | Wai Lam, Wenxuan Zhang, Yang Deng, Yifei Yuan |  |
| 1469 |  |  [UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.226) |  | 0 | Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find... | Hang Yan, Xiaonan Li, Xipeng Qiu, Xuanjing Huang, Yu Sun, Yunhua Zhou |  |
| 1470 |  |  [Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model](https://doi.org/10.18653/v1/2023.acl-long.227) |  | 0 | Existing bias mitigation methods require social-group-specific word pairs (e.g., “man” – “woman”) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating... | Ali Omrani, Alireza Salkhordeh Ziabari, Brendan Kennedy, Charles Yu, Heng Ji, Mohammad Atari, Morteza Dehghani, Preni Golazizian |  |
| 1471 |  |  [Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation](https://doi.org/10.18653/v1/2023.acl-long.228) |  | 0 | Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human... | Alexander R. Fabbri, Caiming Xiong, ChienSheng Wu, Dragomir Radev, Linyong Nan, Pengfei Liu, Ruilin Han, Shafiq Joty, Simeng Han, Yilun Zhao, Yixin Liu |  |
| 1472 |  |  [FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information](https://doi.org/10.18653/v1/2023.acl-long.229) |  | 0 | Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use... | Alexander H. Feng, Andrew Zhu, Chris CallisonBurch, Karmanya Aggarwal, Lara J. Martin |  |
| 1473 |  |  [A fine-grained comparison of pragmatic language understanding in humans and language models](https://doi.org/10.18653/v1/2023.acl-long.230) |  | 0 | Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an... | Edward Gibson, Evelina Fedorenko, Jennifer Hu, Olessia Jouravlev, Sammy Floyd |  |
| 1474 |  |  [Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning](https://doi.org/10.18653/v1/2023.acl-long.231) |  | 0 | Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred as disconnected reasoning problem. To alleviate this... | Hanjiang Lai, Qinkang Gong, Wangzhen Guo, Yanghui Rao |  |
| 1475 |  |  [Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning](https://doi.org/10.18653/v1/2023.acl-long.232) |  | 0 | Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP)... | Fan Zhou, Liu Yu, Ting Zhong, Yi Yang, Yuzhou Mao |  |
| 1476 |  |  [Parameter-Efficient Fine-Tuning without Introducing New Latency](https://doi.org/10.18653/v1/2023.acl-long.233) |  | 0 | Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication... | Baohao Liao, Christof Monz, Yan Meng |  |
| 1477 |  |  [MANNER: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.234) |  | 0 | This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational... | Fei Huang, Jinyuan Fang, Pengjun Xie, Xiaobin Wang, Yong Jiang, Zaiqiao Meng |  |
| 1478 |  |  [MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages](https://doi.org/10.18653/v1/2023.acl-long.235) |  | 0 | We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots.... | Aaron Nash, Ana Sanchez, Charith Peris, Christopher Hench, Gökhan Tür, Jack FitzGerald, Kay Rottmann, Laurie Crist, Liam Urbach, Misha Britan, Prem Natarajan, Richa Singh, Scott Mackie, Swetha Ranganath, Vishesh Kakarala, Wouter Leeuwis |  |
| 1479 |  |  [Distilling Script Knowledge from Large Language Models for Constrained Language Planning](https://doi.org/10.18653/v1/2023.acl-long.236) |  | 0 | In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with... | Charles Robert Jankowski, Deqing Yang, Jiangjie Chen, Siyu Yuan, Soham Shah, Xuyang Ge, Yanghua Xiao, Ziquan Fu |  |
| 1480 |  |  [REDFM: a Filtered and Multilingual Relation Extraction Dataset](https://doi.org/10.18653/v1/2023.acl-long.237) |  | 0 | Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation... | AxelCyrille Ngonga Ngomo, PereLluís Huguet Cabot, Roberto Navigli, Simone Tedeschi |  |
| 1481 |  |  [Modeling Appropriate Language in Argumentation](https://doi.org/10.18653/v1/2023.acl-long.238) |  | 0 | Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions.... | Felix Lange, Henning Wachsmuth, Martin Potthast, Shahbaz Syed, Timon Ziegenbein |  |
| 1482 |  |  [CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels](https://doi.org/10.18653/v1/2023.acl-long.239) |  | 0 | Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive... | Hyunsoo Cho, Sanggoo Lee, Youna Kim |  |
| 1483 |  |  [MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction](https://doi.org/10.18653/v1/2023.acl-long.240) |  | 0 | Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment... | Qingyan Guo, Yujiu Yang, Zhibin Gou |  |
| 1484 |  |  [ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.241) |  | 0 | Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their... | Aram Galstyan, Nanyun Peng, Rujun Han, Sarik Ghazarian, Yijia Shao |  |
| 1485 |  |  [Explanation-based Finetuning Makes Models More Robust to Spurious Cues](https://doi.org/10.18653/v1/2023.acl-long.242) |  | 0 | Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs’ reliance on... | Chris CallisonBurch, Josh Magnus Ludan, Marianna Apidianaki, Qing Lyu, Saurabh Shah, Tai Nguyen, Yixuan Meng |  |
| 1486 |  |  [CAME: Confidence-guided Adaptive Memory Efficient Optimization](https://doi.org/10.18653/v1/2023.acl-long.243) |  | 0 | Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads.... | Xiaozhe Ren, Xin Jiang, Yang Luo, Yang You, Zangwei Zheng, Zhuo Jiang |  |
| 1487 |  |  [On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://doi.org/10.18653/v1/2023.acl-long.244) |  | 0 | Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more... | Diyi Yang, Hongxin Zhang, Michael S. Bernstein, Omar Shaikh, William Held |  |
| 1488 |  |  [Solving Math Word Problems via Cooperative Reasoning induced Language Models](https://doi.org/10.18653/v1/2023.acl-long.245) |  | 0 | Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision... | Jiaxing Zhang, Junjie Wang, Lin Zhang, Ruyi Gan, Xinyu Zhu, Yongfeng Huang, Yujiu Yang, Yuxiang Zhang |  |
| 1489 |  |  [Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model](https://doi.org/10.18653/v1/2023.acl-long.246) |  | 0 | Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data... | Chantal Amrhein, Florian Schottmann, Rico Sennrich, Samuel Läubli |  |
| 1490 |  |  [Early Discovery of Disappearing Entities in Microblogs](https://doi.org/10.18653/v1/2023.acl-long.247) |  | 0 | We make decisions by reacting to changes in the real world, particularly the emergence and disappearance of impermanent entities such as restaurants, services, and events. Because we want to avoid missing out on opportunities or making fruitless actions after those entities have disappeared, it is... | Masashi Toyoda, Naoki Yoshinaga, Satoshi Akasaki |  |
| 1491 |  |  [DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](https://doi.org/10.18653/v1/2023.acl-long.248) |  | 0 | We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds.... | Kuanning Wang, Qiong Tang, Tianxiang Sun, Xipeng Qiu, Xuanjing Huang, Zhengfu He |  |
| 1492 |  |  [Lifting the Curse of Capacity Gap in Distilling Language Models](https://doi.org/10.18653/v1/2023.acl-long.249) |  | 0 | Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap... | Benyou Wang, Chen Zhang, Dawei Song, Jiahao Liu, Jingang Wang, Yang Yang, Yunsen Xian |  |
| 1493 |  |  [Towards Faithful Dialogues via Focus Learning](https://doi.org/10.18653/v1/2023.acl-long.250) |  | 0 | Maintaining faithfulness between responses and knowledge is an important research topic for building reliable knowledge-grounded dialogue systems. Existing models heavily rely on elaborate data engineering or increasing the model’s parameters ignoring to track the tokens that significantly... | Heyan Huang, Xingsheng Zhang, Yifan Deng, Yue Hu |  |
| 1494 |  |  [Back Translation for Speech-to-text Translation Without Transcripts](https://doi.org/10.18653/v1/2023.acl-long.251) |  | 0 | The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes... | Qingkai Fang, Yang Feng |  |
| 1495 |  |  [Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation](https://doi.org/10.18653/v1/2023.acl-long.252) |  | 0 | A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied... | Ibrahim Taha Aksu, MinYen Kan, Nancy F. Chen |  |
| 1496 |  |  [Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation](https://doi.org/10.18653/v1/2023.acl-long.253) |  | 0 | Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing... | Chen Tang, Chenghua Lin, Frank Guerin, Hongbo Zhang, Tyler Loakman |  |
| 1497 |  |  [Multi-modal Action Chain Abductive Reasoning](https://doi.org/10.18653/v1/2023.acl-long.254) |  | 0 | Abductive Reasoning, has long been considered to be at the core ability of humans, which enables us to infer the most plausible explanation of incomplete known phenomena in daily life. However, such critical reasoning capability is rarely investigated for contemporary AI systems under such limited... | Fei Wu, Jiahe Xu, Jiaxu Miao, Kairong Han, Mengze Li, Shengyu Zhang, Shiliang Pu, Tianbao Wang, Wenqiao Zhang, Zhou Zhao |  |
| 1498 |  |  [Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change](https://doi.org/10.18653/v1/2023.acl-long.255) |  | 0 | Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical... | Canming Huang, Weinan He, Yongmei Liu, Zhanhao Xiao |  |
| 1499 |  |  [Unified Demonstration Retriever for In-Context Learning](https://doi.org/10.18653/v1/2023.acl-long.256) |  | 0 | In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval:... | Guotong Xie, Hang Yan, Kai Lv, Tianyang Lin, Wei Zhu, Xiaoling Wang, Xiaonan Li, Xipeng Qiu, Yuan Ni |  |
| 1500 |  |  [Movie101: A New Movie Understanding Benchmark](https://doi.org/10.18653/v1/2023.acl-long.257) |  | 0 | To help the visually impaired enjoy movies, automatic movie narrating systems are expected to narrate accurate, coherent, and role-aware plots when there are no speaking lines of actors. Existing works benchmark this challenge as a normal video captioning task via some simplifications, such as... | Anwen Hu, Liang Zhang, Qi Zhang, Qin Jin, Zihao Yue, Ziheng Wang |  |
| 1501 |  |  [Enhancing Language Representation with Constructional Information for Natural Language Understanding](https://doi.org/10.18653/v1/2023.acl-long.258) |  | 0 | Natural language understanding (NLU) is an essential branch of natural language processing, which relies on representations generated by pre-trained language models (PLMs). However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the... | Jianwang Wu, Jiawei Peng, Lvxiaowei Xu, Ming Cai, Tianxiang Wang, Zhilin Gong |  |
| 1502 |  |  [Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.259) |  | 0 | Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle... | Haijun Shan, Meng Han, Qi Zhang, Siyuan Wang, Xuanjing Huang, Zhihao Fan, Zhongyu Wei |  |
| 1503 |  |  [DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships](https://doi.org/10.18653/v1/2023.acl-long.260) |  | 0 | In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE... | Chenzhengyi Liu, Jie Huang, Kerui Zhu, Kevin ChenChuan Chang |  |
| 1504 |  |  [Incorporating Attribution Importance for Improving Faithfulness Metrics](https://doi.org/10.18653/v1/2023.acl-long.261) |  | 0 | Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such... | Nikolaos Aletras, Zhixue Zhao |  |
| 1505 |  |  [Reward Gaming in Conditional Text Generation](https://doi.org/10.18653/v1/2023.acl-long.262) |  | 0 | To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are... | Ankur P. Parikh, He He, Richard Yuanzhe Pang, Thibault Sellam, Vishakh Padmakumar |  |
| 1506 |  |  [Hidden Schema Networks](https://doi.org/10.18653/v1/2023.acl-long.263) |  | 0 | Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the... | César Ojeda Marin, Kostadin Cvejoski, Lukas Conrads, Pascal Welke, Ramsés J. Sánchez |  |
| 1507 |  |  [Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations](https://doi.org/10.18653/v1/2023.acl-long.264) |  | 0 | Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts... | Lidong Bing, Linlin Liu, Luo Si, Megh Thakkar, Shafiq Joty, Xin Li, Xingxuan Li |  |
| 1508 |  |  [An Ordinal Latent Variable Model of Conflict Intensity](https://doi.org/10.18653/v1/2023.acl-long.265) |  | 0 | Measuring the intensity of events is crucial for monitoring and tracking armed conflict. Advances in automated event extraction have yielded massive data sets of “who did what to whom” micro-records that enable data-driven approaches to monitoring conflict. The Goldstein scale is a widely-used... | Aaron Schein, Josef Valvoda, Lucas Torroba Hennigen, Niklas Stoehr, Robert West, Ryan Cotterell |  |
| 1509 |  |  [Multilingual Conceptual Coverage in Text-to-Image Models](https://doi.org/10.18653/v1/2023.acl-long.266) |  | 0 | We propose “Conceptual Coverage Across Languages” (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess “conceptual coverage” of a given target... | Michael Saxon, William Yang Wang |  |
| 1510 |  |  [Pre-Training to Learn in Context](https://doi.org/10.18653/v1/2023.acl-long.267) |  | 0 | In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly... | Furu Wei, Li Dong, Minlie Huang, Yuxian Gu |  |
| 1511 |  |  [Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers](https://doi.org/10.18653/v1/2023.acl-long.268) |  | 0 | In recent years machine translation has become very successful for high-resource language pairs. This has also sparked new interest in research on the automatic translation of low-resource languages, including Indigenous languages. However, the latter are deeply related to the ethnic and cultural... | Elisabeth Mager, Katharina Kann, Manuel Mager, Ngoc Thang Vu |  |
| 1512 |  |  [Revisiting non-English Text Simplification: A Unified Multilingual Benchmark](https://doi.org/10.18653/v1/2023.acl-long.269) |  | 0 | Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple... | Michael J. Ryan, Tarek Naous, Wei Xu |  |
| 1513 |  |  [Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments](https://doi.org/10.18653/v1/2023.acl-long.270) |  | 0 | A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring... | Xiang Deng, Yu Gu, Yu Su |  |
| 1514 |  |  [Privacy-Preserving Domain Adaptation of Semantic Parsers](https://doi.org/10.18653/v1/2023.acl-long.271) |  | 0 | Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this... | Fatemehsadat Mireshghallah, Jason Eisner, Richard Shin, Tatsunori Hashimoto, Yu Su |  |
| 1515 |  |  [Guide the Many-to-One Assignment: Open Information Extraction via IoU-aware Optimal Transport](https://doi.org/10.18653/v1/2023.acl-long.272) |  | 0 | Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how... | Jingyuan Zhang, Jintao Liu, Kaiwen Wei, Li Jin, Linhao Zhang, Xian Sun, Xiao Li, Yiran Yang, Zequn Zhang, Zhi Guo |  |
| 1516 |  |  [Actively Supervised Clustering for Open Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.273) |  | 0 | Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline, which simultaneously learns relation representations and assignments in the first stage, then manually labels relation for each cluster. However, unsupervised objectives struggle to explicitly... | Jun Zhao, Mingming Sun, Minlong Peng, Qi Zhang, Tao Gui, Yongxin Zhang, Zhongyu Wei |  |
| 1517 |  |  [ConvGQR: Generative Query Reformulation for Conversational Search](https://doi.org/10.18653/v1/2023.acl-long.274) |  | 0 | In conversational search, the user’s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing... | Fengran Mo, JianYun Nie, Kaiyu Huang, Kelong Mao, Yihong Wu, Yutao Zhu |  |
| 1518 |  |  [KILM: Knowledge Injection into Encoder-Decoder Language Models](https://doi.org/10.18653/v1/2023.acl-long.275) |  | 0 | Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a... | Aishwarya Padmakumar, Devamanyu Hazarika, Dilek HakkaniTür, Mahdi Namazifar, Yan Xu, Yang Liu |  |
| 1519 |  |  [VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions](https://doi.org/10.18653/v1/2023.acl-long.276) |  | 0 | Video-grounded dialogue understanding is a challenging problem that requires machine to perceive, parse and reason over situated semantics extracted from weakly aligned video and dialogues. Most existing benchmarks treat both modalities the same as a frame-independent visual understanding task,... | Dongyan Zhao, Jinpeng Li, Xueliang Zhao, Yueqian Wang, Yuxuan Wang, Zilong Zheng |  |
| 1520 |  |  [NLPeer: A Unified Resource for the Computational Study of Peer Review](https://doi.org/10.18653/v1/2023.acl-long.277) |  | 0 | Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed... | Ilia Kuznetsov, Iryna Gurevych, Nils Dycke |  |
| 1521 |  |  [IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures](https://doi.org/10.18653/v1/2023.acl-long.278) |  | 0 | Various datasets have been proposed to promote the development of Table Question Answering (TQA) technique. However, the problem setting of existing TQA benchmarks suffers from two limitations. First, they directly provide models with explicit table structures where row headers and column headers... | Mingyu Zheng, Qiaoqiao She, Weiping Wang, Wenbin Jiang, Yajuan Lyu, Yang Hao, Zheng Lin |  |
| 1522 |  |  [Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.279) |  | 0 | This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model’s performance on low-resource summarization... | Baolin Peng, Chenguang Zhu, Hany Hassan, Jianfeng Gao, Michael Zeng, Pengcheng He, Ruochen Xu, Song Wang, Wayne Xiong, Xuedong Huang, Yang Liu, Yu Shi |  |
| 1523 |  |  [Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models' Memories](https://doi.org/10.18653/v1/2023.acl-long.280) |  | 0 | Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper,... | Jiawei Wang, Ruijia Xu, Shizhe Diao, Tianyang Xu, Tong Zhang |  |
| 1524 |  |  [Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model](https://doi.org/10.18653/v1/2023.acl-long.281) |  | 0 | Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt... | Chenghu Zhou, Jiexing Qi, Luoyi Fu, Shuqian Sheng, Xinbing Wang, Yi Xu, Zhouhan Lin |  |
| 1525 |  |  [Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications](https://doi.org/10.18653/v1/2023.acl-long.282) |  | 0 | Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We... | Chi Xu, Han Cheol Moon, Megh Thakkar, Ruochen Zhao, Shafiq R. Joty |  |
| 1526 |  |  [SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes](https://doi.org/10.18653/v1/2023.acl-long.283) |  | 0 | Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a... | Lei Li, Mingxuan Wang, Wenda Xu, William Yang Wang, Xian Qian |  |
| 1527 |  |  [Tokenization and the Noiseless Channel](https://doi.org/10.18653/v1/2023.acl-long.284) |  | 0 | Subword tokenization is a key part of most NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by... | Clara Meister, Juan Luis Gastaldi, Li Du, Mrinmaya Sachan, Ryan Cotterell, Vilém Zouhar |  |
| 1528 |  |  [Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers](https://doi.org/10.18653/v1/2023.acl-long.285) |  | 0 | Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate... | Jiaxi Li, Wei Lu |  |
| 1529 |  |  [MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning](https://doi.org/10.18653/v1/2023.acl-long.286) |  | 0 | With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an... | Dong Wang, Huimin Zeng, Lanyu Shang, Yang Zhang, Zhenrui Yue |  |
| 1530 |  |  [Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection](https://doi.org/10.18653/v1/2023.acl-long.287) |  | 0 | With the popularity of social media, detecting sentiment from multimodal posts (e.g. image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with... | Lei Shen, Longbiao Wang, Meng Chen, Ruosong Yang, Shaozu Yuan, Yiwei Wei, Zhangmeizhi Li |  |
| 1531 |  |  [COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective](https://doi.org/10.18653/v1/2023.acl-long.288) |  | 0 | Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile,... | Ginny Y. Wong, Hongming Zhang, Jiayao Zhang, Quyet V. Do, Simon See, Tianqing Fang, Weiqi Wang, Yangqiu Song, Zhaowei Wang |  |
| 1532 |  |  [MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization](https://doi.org/10.18653/v1/2023.acl-long.289) |  | 0 | Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that... | Md. Shad Akhtar, Ramaneswaran S., Shivam Sharma, Tanmoy Chakraborty, Udit Arora |  |
| 1533 |  |  [WikiHowQA: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering](https://doi.org/10.18653/v1/2023.acl-long.290) |  | 0 | Answering non-factoid questions (NFQA) is a challenging task, requiring passage-level answers that are difficult to construct and evaluate. Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents. Meanwhile, modern models can... | Falk Scholer, Mark Sanderson, Sofya Filippova, Valeria BolotovaBaranova, Vladislav Blinov |  |
| 1534 |  |  [Making Language Models Better Reasoners with Step-Aware Verifier](https://doi.org/10.18653/v1/2023.acl-long.291) |  | 0 | Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To... | Bei Chen, JianGuang Lou, Qiang Fu, Shizhuo Zhang, Weizhu Chen, Yifei Li, Zeqi Lin |  |
| 1535 |  |  [Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations](https://doi.org/10.18653/v1/2023.acl-long.292) |  | 0 | Discourse analysis is an important task because it models intrinsic semantic structures between sentences in a document. Discourse markers are natural representations of discourse in our daily language. One challenge is that the markers as well as pre-defined and human-labeled discourse relations... | Dongyu Ru, Lin Qiu, Xipeng Qiu, Yue Zhang, Zheng Zhang |  |
| 1536 |  |  [MISGENDERED: Limits of Large Language Models in Understanding Pronouns](https://doi.org/10.18653/v1/2023.acl-long.293) |  | 0 | Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider... | Sameer Singh, Sunipa Dev, Tamanna Hossain |  |
| 1537 |  |  [Reasoning with Language Model Prompting: A Survey](https://doi.org/10.18653/v1/2023.acl-long.294) |  | 0 | Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce... | Chuanqi Tan, Fei Huang, Huajun Chen, Ningyu Zhang, Shumin Deng, Shuofei Qiao, Xiang Chen, Yixin Ou, Yunzhi Yao |  |
| 1538 |  |  [Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation](https://doi.org/10.18653/v1/2023.acl-long.295) |  | 0 | One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of... | Benoît Sagot, Cordelia Schmid, Ivan Laptev, Matthieu Futeral, Rachel Bawden |  |
| 1539 |  |  [Hybrid Knowledge Transfer for Improved Cross-Lingual Event Detection via Hierarchical Sample Selection](https://doi.org/10.18653/v1/2023.acl-long.296) |  | 0 | In this paper, we address the Event Detection task under a zero-shot cross-lingual setting where a model is trained on a source language but evaluated on a distinct target language for which there is no labeled data available. Most recent efforts in this field follow a direct transfer approach in... | Franck Dernoncourt, Luis GuzmanNateras, Thien Huu Nguyen |  |
| 1540 |  |  [BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training](https://doi.org/10.18653/v1/2023.acl-long.297) |  | 0 | Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher... | Chengqi Zhao, Jiajun Chen, Mingxuan Wang, Shujian Huang, Tao Wang, Yiming Yan |  |
| 1541 |  |  [Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment](https://doi.org/10.18653/v1/2023.acl-long.298) |  | 0 | Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the... | LouisPhilippe Morency, Paul Pu Liang, Rohan Pandey, Rulin Shao, Ruslan Salakhutdinov |  |
| 1542 |  |  [Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona](https://doi.org/10.18653/v1/2023.acl-long.299) |  | 0 | The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes... | Bo Wang, Dongming Zhao, Kun Huang, Miao Fang, Ruifang He, Yihong Tang, Yuexian Hou |  |
| 1543 |  |  [Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge](https://doi.org/10.18653/v1/2023.acl-long.300) |  | 0 | Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while... | Eunsol Choi, Greg Durrett, Michael J. Q. Zhang, Shankar Padmanabhan, Yasumasa Onoe |  |
| 1544 |  |  [Explaining How Transformers Use Context to Build Predictions](https://doi.org/10.18653/v1/2023.acl-long.301) |  | 0 | Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model’s prediction, it is still unclear how prior words affect the model’s decision throughout the layers. In this work, we leverage recent advances in... | Gerard I. Gállego, Ioannis Tsiamas, Javier Ferrando, Marta R. Costajussà |  |
| 1545 |  |  [DISCO: Distilling Counterfactuals with Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.302) |  | 0 | Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in... | Antoine Bosselut, Ashish Sabharwal, Kyle Richardson, Qiyue Gao, Zeming Chen |  |
| 1546 |  |  [Non-Sequential Graph Script Induction via Multimedia Grounding](https://doi.org/10.18653/v1/2023.acl-long.303) |  | 0 | Online resources such as WikiHow compile a wide range of scripts for performing everyday tasks, which can assist models in learning to reason about procedures. However, the scripts are always presented in a linear manner, which does not reflect the flexibility displayed by people executing tasks in... | Heng Ji, Manling Li, Mohit Bansal, Sha Li, ShihFu Chang, Xudong Lin, Yu Zhou |  |
| 1547 |  |  [SCOTT: Self-Consistent Chain-of-Thought Distillation](https://doi.org/10.18653/v1/2023.acl-long.304) |  | 0 | Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even... | Bing Yin, Peifeng Wang, Xiang Ren, Yifan Gao, Zheng Li, Zhengyang Wang |  |
| 1548 |  |  [Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.305) |  | 0 | Leveraging knowledge from electronic health records (EHRs) to predict a patient’s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents... | Nayeon Kim, Sun Kim, Yinhua Piao |  |
| 1549 |  |  [Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization](https://doi.org/10.18653/v1/2023.acl-long.306) |  | 0 | For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the... | Dongqi Liu, Vera Demberg, Yifan Wang |  |
| 1550 |  |  [Evaluating Open-Domain Question Answering in the Era of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.307) |  | 0 | Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative... | Charles L. A. Clarke, Davood Rafiei, Ehsan Kamalloo, Nouha Dziri |  |
| 1551 |  |  [No clues good clues: out of context Lexical Relation Classification](https://doi.org/10.18653/v1/2023.acl-long.308) |  | 0 | The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs “well-formed” verbalized text to interact with it,... | Carlos Bobed Lisbona, Jordi Bernad, Jorge Gracia, Lacramioara Dranca, Lucia Pitarch |  |
| 1552 |  |  [Won't Get Fooled Again: Answering Questions with False Premises](https://doi.org/10.18653/v1/2023.acl-long.309) |  | 0 | Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to... | Huadong Wang, Maosong Sun, Shengding Hu, Xingyi Cheng, Yifan Luo, Zhiyuan Liu |  |
| 1553 |  |  [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://doi.org/10.18653/v1/2023.acl-long.310) |  | 0 | Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate... | Akshat Pandey, Ferhan Ture, Gefei Yang, Jimmy Lin, Karun Kumar, Linqing Liu, Pontus Stenetorp, Raphael Tang, Zhiying Jiang |  |
| 1554 |  |  [Zero-shot Faithful Factual Error Correction](https://doi.org/10.18653/v1/2023.acl-long.311) |  | 0 | Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans’ ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about... | Heng Ji, Hou Pong Chan, KungHsiang Huang |  |
| 1555 |  |  [Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification](https://doi.org/10.18653/v1/2023.acl-long.312) |  | 0 | Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In... | Chris CallisonBurch, Heng Ji, Jiawei Han, Manling Li, Ruining Zhao, Sha Li |  |
| 1556 |  |  [Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts](https://doi.org/10.18653/v1/2023.acl-long.313) |  | 0 | Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment... | Adithya Kulkarni, Mohna Chakraborty, Qi Li |  |
| 1557 |  |  [Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging](https://doi.org/10.18653/v1/2023.acl-long.314) |  | 0 | Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s).... | Fabian David Schmidt, Goran Glavas, Ivan Vulic |  |
| 1558 |  |  [Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training](https://doi.org/10.18653/v1/2023.acl-long.315) |  | 0 | In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training... | Ao Luo, Wangchunshu Zhou, Xinsong Zhang, Yan Zeng, Ziming Cheng |  |
| 1559 |  |  [Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars](https://doi.org/10.18653/v1/2023.acl-long.316) |  | 0 | We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To... | Roger Levy, Songlin Yang, Yoon Kim |  |
| 1560 |  |  [Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions](https://doi.org/10.18653/v1/2023.acl-long.317) |  | 0 | Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to... | Arkil Patel, Phil Blunsom, Satwik Bhattamishra, Varun Kanade |  |
| 1561 |  |  [Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation](https://doi.org/10.18653/v1/2023.acl-long.318) |  | 0 | Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the... | Anil Bandhakavi, Manvi Goel, Md. Shad Akhtar, Rishabh Gupta, Shaily Desai, Tanmoy Chakraborty |  |
| 1562 |  |  [DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation](https://doi.org/10.18653/v1/2023.acl-long.319) |  | 0 | State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring... | Anmol Reddy Mekala, D. Chandra Sekhara Hetha Havya, Ganesh Ramakrishnan, Mayank Kothyari, Preethi Jyothi, Rishabh K. Iyer, Suraj Kothawade |  |
| 1563 |  |  [Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework](https://doi.org/10.18653/v1/2023.acl-long.320) |  | 0 | As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and... | Chengwei Qin, Lidong Bing, Ruochen Zhao, Shafiq Joty, Xingxuan Li |  |
| 1564 |  |  [Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.321) |  | 0 | k-Nearest neighbor machine translation (kNN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value... | Baosong Yang, Dayiheng Liu, Huan Lin, Jinsong Su, Jun Xie, Min Zhang, Suhang Wu, Xiangpeng Wei, Zhiwei Cao |  |
| 1565 |  |  [Node Placement in Argument Maps: Modeling Unidirectional Relations in High & Low-Resource Scenarios](https://doi.org/10.18653/v1/2023.acl-long.322) |  | 0 | Argument maps structure discourse into nodes in a tree with each node being an argument that supports or opposes its parent argument. This format is more comprehensible and less redundant compared to an unstructured one. Exploring those maps and maintaining their structure by placing new arguments... | Eva Maria Vecchi, Gabriella Lapesa, Iman Jundi, Neele Falk |  |
| 1566 |  |  [Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review](https://doi.org/10.18653/v1/2023.acl-long.323) |  | 0 | In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to... | Fred Philippy, Shohreh Haddadan, Siwen Guo |  |
| 1567 |  |  [Toward Human-Like Evaluation for Natural Language Generation with Error Analysis](https://doi.org/10.18653/v1/2023.acl-long.324) |  | 0 | The pretrained language model (PLM) based metrics have been successfully used in evaluating language generation tasks. Recent studies of the human evaluation community show that considering both major errors (e.g. mistranslated tokens) and minor errors (e.g. imperfections in fluency) can produce... | Dacheng Tao, Derek F. Wong, Kanjian Zhang, Liang Ding, Liping Xie, Qingyu Lu |  |
| 1568 |  |  [Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.325) |  | 0 | Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots... | Hao Zhou, Hongyi Wu, Man Lan, Yadong Zhang, Yuanbin Wu |  |
| 1569 |  |  [What is the best recipe for character-level encoder-only modelling?](https://doi.org/10.18653/v1/2023.acl-long.326) |  | 0 | This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions... | Kris Cao |  |
| 1570 |  |  [Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training](https://doi.org/10.18653/v1/2023.acl-long.327) |  | 0 | Multilingual Vision-Language Pre-training (VLP) is a promising but challenging topic due to the lack of large-scale multilingual image-text pairs. Existing works address the problem by translating English data into other languages, which is intuitive and the generated data is usually limited in... | Jingjing Chen, Qi Zhang, Xuanjing Huang, Zejun Li, Zhihao Fan, Zhongyu Wei |  |
| 1571 |  |  [Learning "O" Helps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER](https://doi.org/10.18653/v1/2023.acl-long.328) |  | 0 | As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for... | Junzhe Wang, Qi Zhang, Ruotian Ma, Tao Gui, Xiang Gao, Xin Zhou, Xuanting Chen, Yun Wen Chen, Zhang Lin |  |
| 1572 |  |  [Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination](https://doi.org/10.18653/v1/2023.acl-long.329) |  | 0 | In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual... | Hao Fei, Meishan Zhang, Min Zhang, Qian Liu, TatSeng Chua |  |
| 1573 |  |  [CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.330) |  | 0 | Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge... | Börje Karlsson, ChinYew Lin, Huiqiang Jiang, Qianhui Wu, Tiejun Zhao, Tingting Ma |  |
| 1574 |  |  [Dialect-robust Evaluation of Generated Text](https://doi.org/10.18653/v1/2023.acl-long.331) |  | 0 | Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are... | Aditya Siddhant, Dan Garrette, Elizabeth Clark, Jacob Eisenstein, Jiao Sun, Sebastian Gehrmann, Thibault Sellam, Timothy Dozat, Tu Vu |  |
| 1575 |  |  [Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.332) |  | 0 | In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. We identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO,... | Bo Qin, Haibo Wang, Huaao Zhang, Ming Chen, Qiang Wang, Zelin Shi |  |
| 1576 |  |  [Language model acceptability judgements are not always robust to context](https://doi.org/10.18653/v1/2023.acl-long.333) |  | 0 | Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during... | Aaron Mueller, Adina Williams, Jon Gauthier, Kanishka Misra, Keren Fuentes, Koustuv Sinha, Roger Levy |  |
| 1577 |  |  [RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations](https://doi.org/10.18653/v1/2023.acl-long.334) |  | 0 | Despite significant progress having been made in question answering on tabular data (Table QA), it’s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the... | Boyu Mi, Chen Zhao, Dragomir Radev, Linyong Nan, Wenlin Zhang, Xiangru Tang, Yilun Zhao, Zhenting Qi |  |
| 1578 |  |  [Morphological Inflection: A Reality Check](https://doi.org/10.18653/v1/2023.acl-long.335) |  | 0 | Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high... | Jordan Kodner, Salam Khalifa, Sarah R. B. Payne, Zoey Liu |  |
| 1579 |  |  [TOME: A Two-stage Approach for Model-based Retrieval](https://doi.org/10.18653/v1/2023.acl-long.336) |  | 0 | Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which... | Haifeng Wang, Hua Wu, JiRong Wen, Jing Liu, Ruiyang Ren, Wayne Xin Zhao |  |
| 1580 |  |  [Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner](https://doi.org/10.18653/v1/2023.acl-long.337) |  | 0 | We propose a novel approach to automatically generate distractors for cloze exercises for English language learners, using round-trip neural machine translation. A carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the... | Alla Rozovskaya, Frank Palma Gomez, Michael Flor, Subhadarshi Panda |  |
| 1581 |  |  [Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks](https://doi.org/10.18653/v1/2023.acl-long.338) |  | 0 | Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense... | Anette Frank, Juri Opitz, Moritz Plenz, Philipp Cimiano, Philipp Heinisch |  |
| 1582 |  |  [miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.339) |  | 0 | This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning... | Moin Nabi, Tassilo Klein |  |
| 1583 |  |  [Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency](https://doi.org/10.18653/v1/2023.acl-long.340) |  | 0 | The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads... | Mandar Sharma, Naren Ramakrishnan, Nikhil Muralidhar |  |
| 1584 |  |  [Forgotten Knowledge: Examining the Citational Amnesia in NLP](https://doi.org/10.18653/v1/2023.acl-long.341) |  | 0 | Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal... | Diyi Yang, Janvijay Singh, Mukund Rungta, Saif M. Mohammad |  |
| 1585 |  |  [Measuring the Instability of Fine-Tuning](https://doi.org/10.18653/v1/2023.acl-long.342) |  | 0 | Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most of these studies only used the standard... | Dong Nguyen, Yupei Du |  |
| 1586 |  |  [FairPrism: Evaluating Fairness-Related Harms in Text Generation](https://doi.org/10.18653/v1/2023.acl-long.343) |  | 0 | It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of... | Alexandra Olteanu, Aubrie Amstutz, Chad Atalla, Dan Vann, Emily Sheng, Eve Fleisig, Hal Daumé III, Hanna M. Wallach, Su Lin Blodgett |  |
| 1587 |  |  [Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback](https://doi.org/10.18653/v1/2023.acl-long.344) |  | 0 | Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source... | Avinatan Hassidim, Gal Elidan, Geoffrey Cideron, Idan Szpektor, Johan Ferret, Lior Shani, Léonard Hussenot, Matthieu Geist, Nikola Momchev, Nino Vieillard, Olivier Bachem, Olivier Pietquin, Orgad Keller, Paul Roit, Piotr Stanczyk, Robert Dadashi, Roee Aharoni, Sabela Ramos Garea, Sertan Girgin |  |
| 1588 |  |  [SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams](https://doi.org/10.18653/v1/2023.acl-long.345) |  | 0 | Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities:(1) spatial and temporal understanding of the situated and real-time user scenes,(2) capability of grounding the actively perceived visuals of users... | Andrea Madotto, Babak Damavandi, Mahmoud Azab, Nanyun Peng, Pedro Rodríguez, Satwik Kottur, Seungwhan Moon, TeLin Wu |  |
| 1589 |  |  [Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment](https://doi.org/10.18653/v1/2023.acl-long.346) |  | 0 | In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource... | Eshaan Tanwar, Manish Borthakur, Subhabrata Dutta, Tanmoy Chakraborty |  |
| 1590 |  |  [APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.347) |  | 0 | Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data... | Chenguang Zhu, Reid Pryzant, Shuohang Wang, Soumya Sanyal, Wenhao Yu, Xiang Ren, Yichong Xu, Ziyi Yang |  |
| 1591 |  |  [MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering](https://doi.org/10.18653/v1/2023.acl-long.348) |  | 0 | Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions... | Andrew Yates, Evangelos Kanoulas, Maarten de Rijke, Vaishali Pal |  |
| 1592 |  |  [To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.349) |  | 0 | Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard... | Chaozhuo Li, Hao Sun, Jianan Zhao, Qi Zhang, Rui Li, Weihao Han, Weiwei Deng, Xing Xie, Xu Chen, Yanming Shen, Yujing Wang |  |
| 1593 |  |  [CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation](https://doi.org/10.18653/v1/2023.acl-long.350) |  | 0 | Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through... | Huimin Wang, KamFai Wong, WaiChung Kwan, Yefeng Zheng |  |
| 1594 |  |  [Long-Tailed Question Answering in an Open World](https://doi.org/10.18653/v1/2023.acl-long.351) |  | 0 | Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not... | Fei Huang, Hao Lang, Yi Dai, Yinhe Zheng, Yongbin Li |  |
| 1595 |  |  [Parallel Context Windows for Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.352) |  | 0 | When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method... | Amnon Shashua, Ehud Karpas, Inbal Magar, Kevin LeytonBrown, Nir Ratner, Omri Abend, Ori Ram, Yoav Levine, Yoav Shoham, Yonatan Belinkov |  |
| 1596 |  |  [Efficient Transformers with Dynamic Token Pooling](https://doi.org/10.18653/v1/2023.acl-long.353) |  | 0 | Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as... | Adrian Lancucki, Edoardo Maria Ponti, Jan Chorowski, Piotr Nawrot |  |
| 1597 |  |  [Did the Models Understand Documents? Benchmarking Models for Language Understanding in Document-Level Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.354) |  | 0 | Document-level relation extraction (DocRE) attracts more research interest recently. While models achieve consistent performance gains in DocRE, their underlying decision rules are still understudied: Do they make the right predictions according to rationales? In this paper, we take the first step... | Bingsheng Chen, Haotian Chen, Xiangdong Zhou |  |
| 1598 |  |  [ContraCLM: Contrastive Learning For Causal Language Model](https://doi.org/10.18653/v1/2023.acl-long.355) |  | 0 | Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess... | Baishakhi Ray, Bing Xiang, Dejiao Zhang, Feng Nan, Ming Tan, Nihal Jain, Parminder Bhatia, Ramesh Nallapati, Wasi Uddin Ahmad, Xiaofei Ma, Xiaopeng Li, Zijian Wang |  |
| 1599 |  |  [Advancing Multi-Criteria Chinese Word Segmentation Through Criterion Classification and Denoising](https://doi.org/10.18653/v1/2023.acl-long.356) |  | 0 | Recent research on multi-criteria Chinese word segmentation (MCCWS) mainly focuses on building complex private structures, adding more handcrafted features, or introducing complex optimization processes. In this work, we show that through a simple yet elegant input-hint-based MCCWS model, we can... | ChunYi Lin, HungYu Kao, TzuHsuan Chou |  |
| 1600 |  |  [Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2023.acl-long.357) |  | 0 | Multi-level implicit discourse relation recognition (MIDRR) aims at identifying hierarchical discourse relations among arguments. Previous methods achieve the promotion through fine-tuning PLMs. However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately... | Haodong Zhao, Jing Xu, Mengnan Xiao, Ruifang He |  |
| 1601 |  |  [Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model](https://doi.org/10.18653/v1/2023.acl-long.358) |  | 0 | Neural language models have achieved superior performance. However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions. In... | Chunlei Jing, Jing Yang, Jingying Li, Liming Wang, Pengwei Zhan, Xiao Huang |  |
| 1602 |  |  [Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children's Fairy Tales](https://doi.org/10.18653/v1/2023.acl-long.359) |  | 0 | Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale,... | Dakuo Wang, Guangxuan Xu, Nanyun Peng, Paulina Toro Isaza, Toye Oloko, Yufang Hou |  |
| 1603 |  |  [FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue](https://doi.org/10.18653/v1/2023.acl-long.360) |  | 0 | Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods... | Chen Zeng, Jingang Wang, Keqing He, Weihao Zeng, Weiran Xu, Yejie Wang, Yunsen Xian |  |
| 1604 |  |  [LAMBADA: Backward Chaining for Automated Reasoning in Natural Language](https://doi.org/10.18653/v1/2023.acl-long.361) |  | 0 | Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a... | Deepak Ramachandran, Deepti Bhatia, Mehran Kazemi, Najoung Kim, Xin Xu |  |
| 1605 |  |  [PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives](https://doi.org/10.18653/v1/2023.acl-long.362) |  | 0 | Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understandhow the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also... | Antoine Bosselut, Beatriz Borges, Deniz Bayazit, Hiromi Wakaki, Saya Kanno, Silin Gao, Soyoung Oh, Yuki Mitsufuji |  |
| 1606 |  |  [OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment](https://doi.org/10.18653/v1/2023.acl-long.363) |  | 0 | Speech Recognition builds a bridge between the multimedia streaming (audio-only, visual-only or audio-visual) and the corresponding text transcription. However, when training the specific model of new domain, it often gets stuck in the lack of new-domain utterances, especially the labeled visual... | Linjun Li, Tao Jin, Wang Lin, Xinyu Duan, Xize Cheng, Zhou Zhao |  |
| 1607 |  |  [Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models](https://doi.org/10.18653/v1/2023.acl-long.364) |  | 0 | Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline. Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it... | Fei Mi, Hongru Wang, Jianzhu Bao, KamFai Wong, Lifeng Shang, Rui Wang, Ruifeng Xu, Yasheng Wang, Yi Chen, Yitong Li |  |
| 1608 |  |  [BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval](https://doi.org/10.18653/v1/2023.acl-long.365) |  | 0 | Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching... | Huawei Shen, Liang Pang, Shicheng Xu, Xueqi Cheng |  |
| 1609 |  |  [Multiview Identifiers Enhanced Generative Retrieval](https://doi.org/10.18653/v1/2023.acl-long.366) |  | 0 | Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a... | Furu Wei, Liang Wang, Nan Yang, Wenjie Li, Yongqi Li |  |
| 1610 |  |  [Prompting Language Models for Linguistic Structure](https://doi.org/10.18653/v1/2023.acl-long.367) |  | 0 | Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach... | Hila Gonen, Luke Zettlemoyer, Terra Blevins |  |
| 1611 |  |  [Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis](https://doi.org/10.18653/v1/2023.acl-long.368) |  | 0 | Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences... | Agam Shah, Sudheer Chava, Suvan Paturi |  |
| 1612 |  |  [RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.369) |  | 0 | Semantic matching is a mainstream paradigm of zero-shot relation extraction, which matches a given input with a corresponding label description. The entities in the input should exactly match their hypernyms in the description, while the irrelevant contexts should be ignored when matching. However,... | Jun Zhao, Junzhe Wang, Mingming Sun, Minlong Peng, Qi Zhang, Tao Gui, WenYu Zhan, Xin Zhao, Zhongyu Wei |  |
| 1613 |  |  [SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration](https://doi.org/10.18653/v1/2023.acl-long.370) |  | 0 | The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful... | Alice Oh, Byoung Pil Kim, EunJu Lee, Gunhee Kim, Hwaran Lee, Joonsuk Park, JungWoo Ha, Meeyoung Cha, Sangchul Park, Seokhee Hong, Takyoung Kim, Yejin Choi, Yong Lim |  |
| 1614 |  |  [Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation](https://doi.org/10.18653/v1/2023.acl-long.371) |  | 0 | Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources... | Alice Oh, Gyu Tae Kim, Gyuwan Kim, Junhee Cho, Kihyo Park, Minjoon Seo, Soyoung Yoon, Sungjoon Park |  |
| 1615 |  |  [FLamE: Few-shot Learning from Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-long.372) |  | 0 | Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present... | Chenhao Tan, Yangqiaoyu Zhou, Yiming Zhang |  |
| 1616 |  |  [Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning](https://doi.org/10.18653/v1/2023.acl-long.373) |  | 0 | Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an... | Achille Fokoue, Alexander Gray, Asim Munawar, Daiki Kimura, Keerthiram Murugesan, Michiaki Tatsubori, Pavan Kapanipathi, Prithviraj Sen, Rosario UcedaSosa, Sarathkrishna Swaminathan, Subhajit Chaudhury |  |
| 1617 |  |  [Counterfactual Debiasing for Fact Verification](https://doi.org/10.18653/v1/2023.acl-long.374) |  | 0 | Fact verification aims to automatically judge the veracity of a claim according to several pieces of evidence. Due to the manual construction of datasets, spurious correlations between claim patterns and its veracity (i.e., biases) inevitably exist. Recent studies show that models usually learn... | Liang Wang, Qiang Liu, Shu Wu, Weizhi Xu |  |
| 1618 |  |  [What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics](https://doi.org/10.18653/v1/2023.acl-long.375) |  | 0 | Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people’s social attitudes. We show how word preferences in a large language model... | Barend Beekhuizen, Julia Watson, Suzanne Stevenson |  |
| 1619 |  |  [Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View](https://doi.org/10.18653/v1/2023.acl-long.376) |  | 0 | We revisit the multimodal entity and relation extraction from a translation point of view. Special attention is paid on the misalignment issue in text-image datasets which may mislead the learning. We are motivated by the fact that the cross-modal misalignment is a similar problem of cross-lingual... | Changmeng Zheng, Junhao Feng, Qing Li, Xiaoyong Wei, Yi Cai |  |
| 1620 |  |  [Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.377) |  | 0 | A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define... | Jey Han Lau, Jianzhong Qi, Rongxin Zhu |  |
| 1621 |  |  [Improving the Robustness of Summarization Systems with Dual Augmentation](https://doi.org/10.18653/v1/2023.acl-long.378) |  | 0 | A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise. To create... | Chengqi Zhang, Chongyang Tao, Guodong Long, Mingzhe Li, Xiangliang Zhang, Xin Gao, Xiuying Chen |  |
| 1622 |  |  [Interpretable Math Word Problem Solution Generation via Step-by-step Planning](https://doi.org/10.18653/v1/2023.acl-long.379) |  | 0 | Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate... | Andrew S. Lan, Mengxue Zhang, Weiqi Feng, Zhichao Yang, Zichao Wang |  |
| 1623 |  |  [TemplateGEC: Improving Grammatical Error Correction with Detection Template](https://doi.org/10.18653/v1/2023.acl-long.380) |  | 0 | Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC,... | Derek F. Wong, Heyan Huang, Min Zhang, Peiyuan Gong, Shuo Wang, Xuebo Liu, Yang Gao, Yinghao Li |  |
| 1624 |  |  [Deep Model Compression Also Helps Models Capture Ambiguity](https://doi.org/10.18653/v1/2023.acl-long.381) |  | 0 | Natural language understanding (NLU) tasks face a non-trivial amount of ambiguous samples where veracity of their labels is debatable among annotators. NLU models should thus account for such ambiguity, but they approximate the human opinion distributions quite poorly and tend to produce... | Hancheol Park, Jong C. Park |  |
| 1625 |  |  [Are Experts Needed? On Human Evaluation of Counselling Reflection Generation](https://doi.org/10.18653/v1/2023.acl-long.382) |  | 0 | Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts.... | Daniele Riboni, Diego Reforgiato Recupero, Ehud Reiter, Rim Helaoui, Simone Balloccu, Zixiu Wu |  |
| 1626 |  |  [PairSpanBERT: An Enhanced Language Model for Bridging Resolution](https://doi.org/10.18653/v1/2023.acl-long.383) |  | 0 | We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either... | Hideo Kobayashi, Vincent Ng, Yufang Hou |  |
| 1627 |  |  [Compounding Geometric Operations for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.384) |  | 0 | Geometric transformations including translation, rotation, and scaling are commonly used operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE). Inspired by the synergy, we propose a new KGE model by leveraging all three... | Bin Wang, C.C. Jay Kuo, Xiou Ge, YunCheng Wang |  |
| 1628 |  |  [Few-shot In-context Learning on Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.385) |  | 0 | Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized... | Alex Zhuang, Tianle Li, Wenhu Chen, Xueguang Ma, Yu Gu, Yu Su |  |
| 1629 |  |  [Fact-Checking Complex Claims with Program-Guided Reasoning](https://doi.org/10.18653/v1/2023.acl-long.386) |  | 0 | Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved... | Anh Tuan Luu, Liangming Pan, MinYen Kan, Preslav Nakov, William Yang Wang, Xiaobao Wu, Xinyuan Lu |  |
| 1630 |  |  [Patton: Language Model Pretraining on Text-Rich Networks](https://doi.org/10.18653/v1/2023.acl-long.387) |  | 0 | A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of... | Bowen Jin, Jiawei Han, Qi Zhu, Wentao Zhang, Xinyang Zhang, Yu Meng, Yu Zhang |  |
| 1631 |  |  [Soft Language Clustering for Multilingual Model Pre-training](https://doi.org/10.18653/v1/2023.acl-long.388) |  | 0 | Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose... | Binghuai Lin, Fandong Meng, Jiali Zeng, Jie Zhou, Yi Jing, Yongjing Yin, Yufan Jiang, Yunbo Cao |  |
| 1632 |  |  [Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach](https://doi.org/10.18653/v1/2023.acl-long.389) |  | 0 | A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However,... | Hadi Amiri, Nidhi Vakil |  |
| 1633 |  |  [When and how to paraphrase for named entity recognition?](https://doi.org/10.18653/v1/2023.acl-long.390) |  | 0 | While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in... | Aviral Joshi, Hanoz Bhathena, Namrata Mukhija, Prateek Singh, Saket Sharma, Sashank Santhanam, Yiyun Zhao |  |
| 1634 |  |  [UniEvent: Unified Generative Model with Multi-Dimensional Prefix for Zero-Shot Event-Relational Reasoning](https://doi.org/10.18653/v1/2023.acl-long.391) |  | 0 | Reasoning about events and their relations attracts surging research efforts since it is regarded as an indispensable ability to fulfill various event-centric or common-sense reasoning tasks. However, these tasks often suffer from limited data availability due to the labor-intensive nature of their... | Chengfeng Dou, Chongyang Tao, Haiyan Zhao, Tao Shen, Yongqiang Zhao, Zhengwei Tao, Zhi Jin |  |
| 1635 |  |  [Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales](https://doi.org/10.18653/v1/2023.acl-long.392) |  | 0 | Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a... | Aaron Chan, Brihi Joshi, Qifan Wang, Sahana Ramnath, Shaoliang Nie, Xiang Ren, Yejin Choi, Zhewei Tong, Ziyi Liu |  |
| 1636 |  |  [Automatic Annotation of Direct Speech in Written French Narratives](https://doi.org/10.18653/v1/2023.acl-long.393) |  | 0 | The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many... | Elena V. Epure, Gaspard Michel, Noé Durandard, VietAnh Tran |  |
| 1637 |  |  [Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations](https://doi.org/10.18653/v1/2023.acl-long.394) |  | 0 | Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from... | Hyunjae Kim, Jaehyo Yoo, Jaewoo Kang, Seunghyun Yoon |  |
| 1638 |  |  [Dynamic Transformers Provide a False Sense of Efficiency](https://doi.org/10.18653/v1/2023.acl-long.395) |  | 0 | Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a trade-off between efficiency and accuracy, where the saving of computation comes... | Cong Liu, Haizhou Li, Robby T. Tan, Simin Chen, Wei Yang, Yiming Chen, Zexin Li |  |
| 1639 |  |  [Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features](https://doi.org/10.18653/v1/2023.acl-long.396) |  | 0 | A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate... | Ester Hlavnova, Sebastian Ruder |  |
| 1640 |  |  [Local Byte Fusion for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.397) |  | 0 | Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus may not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes oversegment... | Junjie Hu, Makesh Narsimhan Sreedhar, Xiangpeng Wan, Yu Cheng |  |
| 1641 |  |  [Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation](https://doi.org/10.18653/v1/2023.acl-long.398) |  | 0 | Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse... | Benjamin Minixhofer, Ivan Vulic, Jonas Pfeiffer |  |
| 1642 |  |  [Multi-target Backdoor Attacks for Code Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-long.399) |  | 0 | Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks... | Kangjie Chen, Shangqing Liu, Tianwei Zhang, Xiaofei Xie, Yang Liu, Yanzhou Li |  |
| 1643 |  |  [Learning Better Masking for Better Language Model Pre-training](https://doi.org/10.18653/v1/2023.acl-long.400) |  | 0 | Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the... | Dongjie Yang, Hai Zhao, Zhuosheng Zhang |  |
| 1644 |  |  [VisText: A Benchmark for Semantically Rich Chart Captioning](https://doi.org/10.18653/v1/2023.acl-long.401) |  | 0 | Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive... | Angie W. Boggust, Arvind Satyanarayan, Benny J. Tang |  |
| 1645 |  |  [Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora](https://doi.org/10.18653/v1/2023.acl-long.402) |  | 0 | Grammatical error correction (GEC) is the task of correcting typos, spelling, punctuation and grammatical issues in text. Approaching the problem as a sequence-to-sequence task, we compare the use of a common subword unit vocabulary and byte-level encoding. Initial synthetic training data is... | Haukur Barri Símonarson, Haukur Páll Jónsson, Petur Orri Ragnarsson, Svanhvít Lilja Ingólfsdóttir, Vilhjalmur Thorsteinsson, Vésteinn Snæbjarnarson |  |
| 1646 |  |  [Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text](https://doi.org/10.18653/v1/2023.acl-long.403) |  | 0 | Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and... | Börje Karlsson, ChinYew Lin, Haonan Yin, Huiqiang Jiang, Qianhui Wu |  |
| 1647 |  |  [Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection](https://doi.org/10.18653/v1/2023.acl-long.404) |  | 0 | Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the... | Apoorva Singh, Prince Jha, Raghav Jain, Sriparna Saha |  |
| 1648 |  |  [MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation](https://doi.org/10.18653/v1/2023.acl-long.405) |  | 0 | Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with... | Can Xu, Chongyang Tao, Dongyan Zhao, Jiazhan Feng, Pu Zhao, Qingfeng Sun, Qingwei Lin, Yaming Yang |  |
| 1649 |  |  [ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models](https://doi.org/10.18653/v1/2023.acl-long.406) |  | 0 | State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge... | Jonas Belouadi, Steffen Eger |  |
| 1650 |  |  [Envisioning Future from the Past: Hierarchical Duality Learning for Multi-Turn Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.407) |  | 0 | In this paper, we define a widely neglected property in dialogue text, duality, which is a hierarchical property that is reflected in human behaviours in daily conversations: Based on the logic in a conversation (or a sentence), people can infer follow-up utterances (or tokens) based on the... | Ang Lv, Jinpeng Li, Rui Yan, Shufang Xie |  |
| 1651 |  |  [DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.408) |  | 0 | Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention... | Duzhen Zhang, Feilong Chen, Xiuyi Chen |  |
| 1652 |  |  [Consistent Prototype Learning for Few-Shot Continual Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.409) |  | 0 | Few-shot continual relation extraction aims to continually train a model on incrementally few-shot data to learn new relations while avoiding forgetting old ones. However, current memory-based methods are prone to overfitting memory samples, resulting in insufficient activation of old relations and... | Hui Wu, Xiaodong Shi, Xiudi Chen |  |
| 1653 |  |  [Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.410) |  | 0 | The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues... | Ambrish Rawat, Gabriele Picco, Giulio Zizzo, Myles Foley, Taesung Lee, Yufang Hou |  |
| 1654 |  |  [Large Language Models Meet NL2Code: A Survey](https://doi.org/10.18653/v1/2023.acl-long.411) |  | 0 | The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in... | Bei Chen, Bei Guan, Bingchao Wu, Daoguang Zan, Dianjie Lu, Fengji Zhang, JianGuang Lou, Yongji Wang |  |
| 1655 |  |  [When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP](https://doi.org/10.18653/v1/2023.acl-long.412) |  | 0 | Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work – sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works.... | Jingwei Ni, Markus Leippold, Mrinmaya Sachan, Qian Wang, Zhijing Jin |  |
| 1656 |  |  [Enhancing Grammatical Error Correction Systems with Explanations](https://doi.org/10.18653/v1/2023.acl-long.413) |  | 0 | Grammatical error correction systems improve written communication by detecting and correcting language mistakes. To help language learners better understand why the GEC system makes a certain correction, the causes of errors (evidence words) and the corresponding error types are two key factors.... | Leyang Cui, Sen Yang, Shuming Shi, Wai Lam, Yuejiao Fei, Zhenzhong Lan |  |
| 1657 |  |  [Linguistic representations for fewer-shot relation extraction across domains](https://doi.org/10.18653/v1/2023.acl-long.414) |  | 0 | Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot... | Carolyn P. Rosé, Ritam Dutt, Sireesh Gururaja, Tinglong Liao |  |
| 1658 |  |  [DarkBERT: A Language Model for the Dark Side of the Internet](https://doi.org/10.18653/v1/2023.acl-long.415) |  | 0 | Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers.... | Eugene Jang, Jian Cui, JinWoo Chung, Seungwon Shin, Yongjae Lee, Youngjin Jin |  |
| 1659 |  |  [MDACE: MIMIC Documents Annotated with Code Evidence](https://doi.org/10.18653/v1/2023.acl-long.416) |  | 0 | We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply... | April Russell, Benjamin Striner, Edmond Lu, Hua Cheng, Matthew Gormley, Rana Jafari, Russell Klopfer |  |
| 1660 |  |  [Towards Zero-Shot Multilingual Transfer for Code-Switched Responses](https://doi.org/10.18653/v1/2023.acl-long.417) |  | 0 | Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a... | BiingHwang Juang, Changsheng Zhao, Ernie Chang, Pierce Chuang, TingWei Wu, Vikas Chandra, Yangyang Shi |  |
| 1661 |  |  [One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning](https://doi.org/10.18653/v1/2023.acl-long.418) |  | 0 | Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage. Parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters when being applied to broader ranges of tasks. To... | Guangtao Zeng, Peiyuan Zhang, Wei Lu |  |
| 1662 |  |  [Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk](https://doi.org/10.18653/v1/2023.acl-long.419) |  | 0 | Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as... | Benyou Wang, Jianquan Li, Prayag Tiwari, Qianqian Xie, Xiangbo Wu, Xiaokang Liu |  |
| 1663 |  |  [Convergence and Diversity in the Control Hierarchy](https://doi.org/10.18653/v1/2023.acl-long.420) |  | 0 | Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a... | Alexandra Butoi, David Chiang, Ryan Cotterell |  |
| 1664 |  |  [ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.421) |  | 0 | Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent... | Di Niu, Jiuding Yang, Weidong Guo, Yakun Yu, Yu Xu |  |
| 1665 |  |  [Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic](https://doi.org/10.18653/v1/2023.acl-long.422) |  | 0 | Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often... | Connor Pryor, Deepak Ramachandran, Jeremiah Z. Liu, Lise Getoor, Mehran Kazemi, Quan Yuan, Tania BedraxWeiss |  |
| 1666 |  |  [Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark](https://doi.org/10.18653/v1/2023.acl-long.423) |  | 0 | Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies... | Bin Zhu, Binxing Jiao, Fangzhao Wu, Guangzhong Sun, Jingwei Yi, Lingjuan Lyu, Shangxi Wu, Tong Xu, Wenjun Peng, Xing Xie |  |
| 1667 |  |  [Answering Ambiguous Questions via Iterative Prompting](https://doi.org/10.18653/v1/2023.acl-long.424) |  | 0 | In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question,one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to... | Hengyi Cai, Hongshen Chen, Maarten de Rijke, Pengjie Ren, Weiwei Sun, Zhaochun Ren, Zhumin Chen |  |
| 1668 |  |  [A Dataset of Argumentative Dialogues on Scientific Papers](https://doi.org/10.18653/v1/2023.acl-long.425) |  | 0 | With recent advances in question-answering models, various datasets have been collected to improve and study the effectiveness of these models on scientific texts. Questions and answers in these datasets explore a scientific paper by seeking factual information from the paper’s content. However,... | Federico Ruggeri, Iryna Gurevych, Mohsen Mesgar |  |
| 1669 |  |  [Massively Multilingual Lexical Specialization of Multilingual Transformers](https://doi.org/10.18653/v1/2023.acl-long.426) |  | 0 | While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good... | Goran Glavas, Simone Paolo Ponzetto, Tommaso Green |  |
| 1670 |  |  [RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs](https://doi.org/10.18653/v1/2023.acl-long.427) |  | 0 | Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques... | Afra Feyza Akyürek, Ashwin Kalyan, Derry Tanti Wijaya, Ekin Akyürek, Niket Tandon, Peter Clark |  |
| 1671 |  |  [WebIE: Faithful and Robust Information Extraction on the Web](https://doi.org/10.18653/v1/2023.acl-long.428) |  | 0 | Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have... | Alham Fikri Aji, Andrea Pierleoni, Chenxi Whitehouse, Christos Christodoulopoulos, Clara Vania |  |
| 1672 |  |  [NormBank: A Knowledge Bank of Situational Social Norms](https://doi.org/10.18653/v1/2023.acl-long.429) |  | 0 | We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame,... | Alon Y. Halevy, Caleb Ziems, Diyi Yang, Jane DwivediYu, YiChia Wang |  |
| 1673 |  |  [DIP: Dead code Insertion based Black-box Attack for Programming Language Model](https://doi.org/10.18653/v1/2023.acl-long.430) |  | 0 | Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However,... | CheolWon Na, JeeHyong Lee, YunSeok Choi |  |
| 1674 |  |  [Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks](https://doi.org/10.18653/v1/2023.acl-long.431) |  | 0 | Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document’s coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that... | Michael Strube, Wei Liu, Xiyan Fu |  |
| 1675 |  |  [HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification](https://doi.org/10.18653/v1/2023.acl-long.432) |  | 0 | Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain... | Chong Zhang, He Zhu, Junjie Huang, Junran Wu, Ke Xu |  |
| 1676 |  |  [Contextual Knowledge Learning for Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.433) |  | 0 | Incorporating conversational context and knowledge into dialogue generation models has been essential for improving the quality of the generated responses. The context, comprising utterances from previous dialogue exchanges, is used as a source of content for response generation and as a means of... | Ke Zhou, Natasa MilicFrayling, Wen Zheng |  |
| 1677 |  |  [Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.434) |  | 0 | Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many... | Jiayi Wang, Ke Wang, Xin Ge, Yu Zhao, Yuqi Zhang |  |
| 1678 |  |  [Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated Parallel Corpus of Novels](https://doi.org/10.18653/v1/2023.acl-long.435) |  | 0 | Several recent papers claim to have achieved human parity at sentence-level machine translation (MT)—especially between high-resource language pairs. In response, the MT community has, in part, shifted its focus to document-level translation. Translating documents requires a deeper understanding of... | Dongdong Zhang, Mrinmaya Sachan, Ryan Cotterell, Shuming Ma, Tianyu Liu, Yuchen Eleanor Jiang |  |
| 1679 |  |  [CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.436) |  | 0 | End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but... | Qingkai Fang, Yan Zhou, Yang Feng |  |
| 1680 |  |  [On the Evaluation of Neural Selective Prediction Methods for Natural Language Processing](https://doi.org/10.18653/v1/2023.acl-long.437) |  | 0 | We provide a survey and empirical comparison of the state-of-the-art in neural selective classification for NLP tasks. We also provide a methodological blueprint, including a novel metric called refinement that provides a calibrated evaluation of confidence functions for selective prediction.... | Mark Hopkins, Zhengyao Gu |  |
| 1681 |  |  [Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment](https://doi.org/10.18653/v1/2023.acl-long.438) |  | 0 | Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing... | Chao Wang, Fei Huang, Haoyu Gao, Min Yang, Tianshu Yu, TingEn Lin, Wentao Ma, Yongbin Li, Yuchuan Wu |  |
| 1682 |  |  [Text Style Transfer with Contrastive Transfer Pattern Mining](https://doi.org/10.18653/v1/2023.acl-long.439) |  | 0 | Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this... | Jingxuan Han, Licheng Zhang, Quan Wang, Weidong Chen, Yan Song, Zhendong Mao |  |
| 1683 |  |  [Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning](https://doi.org/10.18653/v1/2023.acl-long.440) |  | 0 | With emerging online topics as a source for numerous new events, detecting unseen / rare event types presents an elusive challenge for existing event detection methods, where only limited data access is provided for training. To address the data scarcity problem in event detection, we propose... | Dong Wang, Heng Ji, Huimin Zeng, Mengfei Lan, Zhenrui Yue |  |
| 1684 |  |  [Text Style Transfer Back-Translation](https://doi.org/10.18653/v1/2023.acl-long.441) |  | 0 | Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT... | Daimeng Wei, Hao Yang, Hengchao Shang, Jiaxin Guo, Minghan Wang, Xiaoyu Chen, Zhanglin Wu, Zhengzhe Yu, Zongyao Li |  |
| 1685 |  |  [Generating Visual Spatial Description via Holistic 3D Scene Understanding](https://doi.org/10.18653/v1/2023.acl-long.442) |  | 0 | Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this... | Hao Fei, Jianguo Wei, Meishan Zhang, Min Zhang, TatSeng Chua, Wei Ji, Yu Zhao |  |
| 1686 |  |  [Continual Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.443) |  | 0 | While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of... | Maosong Sun, Peng Li, Yang Liu, Yuanchi Zhang |  |
| 1687 |  |  [Query Refinement Prompts for Closed-Book Long-Form QA](https://doi.org/10.18653/v1/2023.acl-long.444) |  | 0 | Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to... | Dipanjan Das, Kellie Webster, Michael Collins, Reinald Kim Amplayo, Shashi Narayan |  |
| 1688 |  |  [CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding](https://doi.org/10.18653/v1/2023.acl-long.445) |  | 0 | This paper tackles an emerging and challenging problem of long video temporal grounding (VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference... | ChongWah Ngo, Difei Gao, Kun Yan, Lei Ji, Mike Zheng Shou, Nan Duan, Wanjun Zhong, Wing Kwong Chan, Zhijian Hou |  |
| 1689 |  |  [Few-Shot Document-Level Event Argument Extraction](https://doi.org/10.18653/v1/2023.acl-long.446) |  | 0 | Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring... | Linda R. Petzold, Xianjun Yang, Yujie Lu |  |
| 1690 |  |  [ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation](https://doi.org/10.18653/v1/2023.acl-long.447) |  | 0 | Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine... | Anoop Kumar, Aram Galstyan, IHung Hsu, KaiWei Chang, KuanHao Huang, Varun Iyer |  |
| 1691 |  |  [Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.448) |  | 0 | Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the... | Jian Liu, Jinan Xu, Shuaibo Wang, Songming Zhang, Wenjuan Han, Yufeng Chen, Yunlong Liang |  |
| 1692 |  |  [Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering](https://doi.org/10.18653/v1/2023.acl-long.449) |  | 0 | Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best... | Feifei Pan, Jaydeep Sen, Samarth Bharadwaj, Saneem A. Chemmengath, Soumen Chakrabarti, Vishwajeet Kumar, Yash Gupta |  |
| 1693 |  |  [HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level](https://doi.org/10.18653/v1/2023.acl-long.450) |  | 0 | Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal... | Haihong E, Haoran Luo, Kaiyang Wan, Meina Song, Mingzhi Sun, Tianyu Yao, Wei Lin, Yikai Guo, Yuhao Yang, Zichen Tang |  |
| 1694 |  |  [ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning](https://doi.org/10.18653/v1/2023.acl-long.451) |  | 0 | This paper explores the task of radiology report generation, which aims at generating free-text descriptions for a set of radiographs. One significant challenge of this task is how to correctly maintain the consistency between the images and the lengthy report. Previous research explored solving... | Jiang Liu, Kaishuai Xu, Wenjie Li, Wenjun Hou, Yi Cheng |  |
| 1695 |  |  [Data Curation Alone Can Stabilize In-context Learning](https://doi.org/10.18653/v1/2023.acl-long.452) |  | 0 | In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in... | Robin Jia, TingYun Chang |  |
| 1696 |  |  [MidMed: Towards Mixed-Type Dialogues for Medical Consultation](https://doi.org/10.18653/v1/2023.acl-long.453) |  | 0 | Most medical dialogue systems assume that patients have clear goals (seeking a diagnosis, medicine querying, etc.) before medical consultation. However, in many real situations, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary... | Chuan Wang, Haitao Leng, Kui Xue, Shaoting Zhang, Xiaofan Zhang, Xiaoming Shi, Zeming Liu |  |
| 1697 |  |  [FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning](https://doi.org/10.18653/v1/2023.acl-long.454) |  | 0 | Large pre-trained models are capable of few-shot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder... | Hannaneh Hajishirzi, Iz Beltagy, Matthew E. Peters, Qinyuan Ye, Xiang Ren |  |
| 1698 |  |  [S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.455) |  | 0 | Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose... | Benfeng Xu, Dai Dai, Quan Wang, Yajuan Lyu, Yongdong Zhang, Zhendong Mao |  |
| 1699 |  |  [DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.456) |  | 0 | Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3),... | Ahmed Hassan Awadallah, Tianlong Chen, Weizhu Chen, Xuxi Chen, Yu Cheng, Zhangyang Wang |  |
| 1700 |  |  [CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation](https://doi.org/10.18653/v1/2023.acl-long.457) |  | 0 | Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which... | Bo Wang, Chujie Zheng, Jinfeng Zhou, Minlie Huang, Zheng Zhang |  |
| 1701 |  |  [Comparative evaluation of boundary-relaxed annotation for Entity Linking performance](https://doi.org/10.18653/v1/2023.acl-long.458) |  | 0 | Entity Linking performance has a strong reliance on having a large quantity of high-quality annotated training data available. Yet, manual annotation of named entities, especially their boundaries, is ambiguous, error-prone, and raises many inconsistencies between annotators. While imprecise... | Eiji Aramaki, Gabriel Herman Bernardim Andrade, Shuntaro Yada |  |
| 1702 |  |  [Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?](https://doi.org/10.18653/v1/2023.acl-long.459) |  | 0 | The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when... | Alan Ritter, Shuheng Liu |  |
| 1703 |  |  [READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises](https://doi.org/10.18653/v1/2023.acl-long.460) |  | 0 | For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and... | Chenglei Si, Maosong Sun, Xiaozhi Wang, Yingfa Chen, Zhengyan Zhang, Zhiyuan Liu |  |
| 1704 |  |  [MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification](https://doi.org/10.18653/v1/2023.acl-long.461) |  | 0 | Target-dependent sentiment classification (TSC) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets. While socially important, TSC in the news domain remains... | Adrian Popescu, Armelle Brun, Evan Dufraisse, Julien Tourille, Jérôme Deshayes |  |
| 1705 |  |  [A New Dataset and Empirical Study for Sentence Simplification in Chinese](https://doi.org/10.18653/v1/2023.acl-long.462) |  | 0 | Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this... | Renliang Sun, Shiping Yang, Xiaojun Wan |  |
| 1706 |  |  [Factual or Contextual? Disentangling Error Types in Entity Description Generation](https://doi.org/10.18653/v1/2023.acl-long.463) |  | 0 | In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an... | Ani Nenkova, Hal Daumé III, Navita Goyal |  |
| 1707 |  |  [Weakly Supervised Vision-and-Language Pre-training with Relative Representations](https://doi.org/10.18653/v1/2023.acl-long.464) |  | 0 | Weakly supervised vision-and-language pre-training (WVLP), which learns cross-modal representations with limited cross-modal supervision, has been shown to effectively reduce the data cost of pre-training while maintaining decent performance on downstream tasks. However, current WVLP methods use... | Chi Chen, Maosong Sun, Peng Li, Yang Liu |  |
| 1708 |  |  [HermEs: Interactive Spreadsheet Formula Prediction via Hierarchical Formulet Expansion](https://doi.org/10.18653/v1/2023.acl-long.465) |  | 0 | We propose HermEs, the first approach for spreadsheet formula prediction via HiEraRchical forMulet ExpanSion, where hierarchical expansion means generating formulas following the underlying parse tree structure, and Formulet refers to commonly-used multi-level patterns mined from real formula parse... | Dongmei Zhang, Haoyu Dong, Ran Jia, Shi Han, Wanrong He, Xiao Lv, Xingzhuo Guo, Yihuai Gao, Zhichao Fan, Zhitao Hou |  |
| 1709 |  |  [ArgU: A Controllable Factual Argument Generator](https://doi.org/10.18653/v1/2023.acl-long.466) |  | 0 | Effective argumentation is essential towards a purposeful conversation with a satisfactory outcome. For example, persuading someone to reconsider smoking might involve empathetic, well founded arguments based on facts and expert opinions about its ill-effects and the consequences on one’s family.... | Rohini K. Srihari, Sougata Saha |  |
| 1710 |  |  [Learning Answer Generation using Supervision from Automatic Question Answering Evaluators](https://doi.org/10.18653/v1/2023.acl-long.467) |  | 0 | Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we... | Alessandro Moschitti, Matteo Gabburo, Rik KoncelKedziorski, Siddhant Garg |  |
| 1711 |  |  [RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation](https://doi.org/10.18653/v1/2023.acl-long.468) |  | 0 | Endowing chatbots with a consistent persona is essential to an engaging conversation, yet it remains an unresolved challenge. In this work, we propose a new retrieval-enhanced approach for personalized response generation. Specifically, we design a hierarchical transformer retriever trained on... | Hyundong Cho, Jonathan May, Marjorie Freedman, Shuai Liu, Xuezhe Ma |  |
| 1712 |  |  [Don't Parse, Choose Spans! Continuous and Discontinuous Constituency Parsing via Autoregressive Span Selection](https://doi.org/10.18653/v1/2023.acl-long.469) |  | 0 | We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection. Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and... | Kewei Tu, Songlin Yang |  |
| 1713 |  |  [Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.470) |  | 0 | Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their... | Achille Fokoue, Maxwell Crouse, Pavan Kapanipathi, Ramón Fernandez Astudillo, Subhajit Chaudhury, Tahira Naseem, Tim Klinger |  |
| 1714 |  |  [AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression](https://doi.org/10.18653/v1/2023.acl-long.471) |  | 0 | Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second,... | Hongzhan Chen, Qifan Wang, Rui Wang, Siyue Wu, Xiaojun Quan |  |
| 1715 |  |  [(QA)²: Question Answering with Questionable Assumptions](https://doi.org/10.18653/v1/2023.acl-long.472) |  | 0 | Naturally occurring information-seeking questions often contain questionable assumptions—assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for... | Jackson Petty, Najoung Kim, Phu Mon Htut, Samuel R. Bowman |  |
| 1716 |  |  [Attributable and Scalable Opinion Summarization](https://doi.org/10.18653/v1/2023.acl-long.473) |  | 0 | We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent... | Hao Tang, Mirella Lapata, Tom Hosking |  |
| 1717 |  |  [Targeted Data Generation: Finding and Fixing Model Weaknesses](https://doi.org/10.18653/v1/2023.acl-long.474) |  | 0 | Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to... | Fereshte Khani, Marco Túlio Ribeiro, Zexue He |  |
| 1718 |  |  [HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation](https://doi.org/10.18653/v1/2023.acl-long.475) |  | 0 | To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning... | Anchun Gui, Han Xiao |  |
| 1719 |  |  [CFSum Coarse-to-Fine Contribution Network for Multimodal Summarization](https://doi.org/10.18653/v1/2023.acl-long.476) |  | 0 | Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are... | Chengqing Zong, Haitao Lin, Junnan Zhu, Min Xiao, Yu Zhou |  |
| 1720 |  |  [On "Scientific Debt" in NLP: A Case for More Rigour in Language Model Pre-Training Research](https://doi.org/10.18653/v1/2023.acl-long.477) |  | 0 | This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of... | Adhiguna Kuncoro, Alham Fikri Aji, Genta Indra Winata, Haryo Akbarianto Wibowo, Made Nindyatama Nityasya, Phil Blunsom, Radityo Eko Prasojo |  |
| 1721 |  |  [End-to-end Knowledge Retrieval with Multi-modal Queries](https://doi.org/10.18653/v1/2023.acl-long.478) |  | 0 | We investigate knowledge retrieval with multi-modal queries, i.e. queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ... | Chitta Baral, Man Luo, Tejas Gokhale, Yezhou Yang, Zhiyuan Fang |  |
| 1722 |  |  [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.479) |  | 0 | Direct speech-to-speech translation (S2ST) aims to convert speech from one language into another, and has demonstrated significant progress to date. Despite the recent success, current S2ST models still suffer from distinct degradation in noisy environments and fail to translate visual speech... | Huadai Liu, Jinglin Liu, Jinzheng He, Lichao Zhang, Linjun Li, Rongjie Huang, Xiang Yin, Xize Cheng, Yi Ren, Zhenhui Ye, Zhou Zhao |  |
| 1723 |  |  [Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection](https://doi.org/10.18653/v1/2023.acl-long.480) |  | 0 | Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent... | Fei Ding, Feng Zhang, Tengjiao Wang, Wei Chen |  |
| 1724 |  |  [VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets](https://doi.org/10.18653/v1/2023.acl-long.481) |  | 0 | The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their... | Gerasimos Spanakis, Gijs van Dijck, Nils Rethmeier, Vageesh Saxena |  |
| 1725 |  |  [Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method](https://doi.org/10.18653/v1/2023.acl-long.482) |  | 0 | Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be... | Rui Wang, Yiming Wang, Zhuosheng Zhang |  |
| 1726 |  |  [Efficient Shapley Values Estimation by Amortization for Text Classification](https://doi.org/10.18653/v1/2023.acl-long.483) |  | 0 | Despite the popularity of Shapley Values in explaining neural text classification models, computing them is prohibitive for large pretrained models due to a large number of model evaluations. In practice, Shapley Values are often estimated with a small number of stochastic model evaluations.... | Bing Xiang, Chenghao Yang, Fan Yin, He He, KaiWei Chang, Xiaofei Ma |  |
| 1727 |  |  [PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks](https://doi.org/10.18653/v1/2023.acl-long.484) |  | 0 | Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the... | Lidong Bing, Wai Lam, Weiwen Xu, Xin Li, Yang Deng |  |
| 1728 |  |  [Dynamic Regularization in UDA for Transformers in Multimodal Classification](https://doi.org/10.18653/v1/2023.acl-long.485) |  | 0 | Multimodal machine learning is a cutting-edge field that explores ways to incorporate information from multiple sources into models. As more multimodal data becomes available, this field has become increasingly relevant. This work focuses on two key challenges in multimodal machine learning. The... | Adrián Pastor LópezMonroy, Fernando SánchezVega, Ivonne MonterAldana |  |
| 1729 |  |  [Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing](https://doi.org/10.18653/v1/2023.acl-long.486) |  | 0 | Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization... | Gosia Mikolajczak, Jiatong Li, Lea Frermann, Shima Khanehzar |  |
| 1730 |  |  [bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2023.acl-long.487) |  | 0 | We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking,... | Dragomir Radev, Galia Angelova, Ivan Koychev, Kiril Simov, Momchil Hardalov, Pepa Atanasova, Petya Osenova, Preslav Nakov, Todor Mihaylov, Veselin Stoyanov |  |
| 1731 |  |  [DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-long.488) |  | 0 | Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo... | Laks V. S. Lakshmanan, Xiaoyuan Yi, Xing Xie, Xiting Wang, Yuxi Feng |  |
| 1732 |  |  [What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?](https://doi.org/10.18653/v1/2023.acl-long.489) |  | 0 | Humans can effortlessly understand the coordinate structure of sentences such as “Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, \*respectively\*”. In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and... | Anders Søgaard, Daniel Hershcovich, Ruixiang Cui, Seolhwa Lee |  |
| 1733 |  |  [BLIND: Bias Removal With No Demographics](https://doi.org/10.18653/v1/2023.acl-long.490) |  | 0 | Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce... | Hadas Orgad, Yonatan Belinkov |  |
| 1734 |  |  [How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks](https://doi.org/10.18653/v1/2023.acl-long.491) |  | 0 | Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks – malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility... | Maxime Cordy, Salah Ghamizi, Salijona Dyrmishi |  |
| 1735 |  |  [Soft Alignment Objectives for Robust Adaptation of Language Generation](https://doi.org/10.18653/v1/2023.acl-long.492) |  | 0 | Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended... | Marek Kadlcík, Michal Stefánik, Petr Sojka |  |
| 1736 |  |  [The CRINGE Loss: Learning what language not to model](https://doi.org/10.18653/v1/2023.acl-long.493) |  | 0 | Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of... | Jason Weston, Jing Xu, Kurt Shuster, Leonard Adolphs, Sainbayar Sukhbaatar, Tianyu Gao |  |
| 1737 |  |  [Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process](https://doi.org/10.18653/v1/2023.acl-long.494) |  | 0 | Dialogue systems have received increasing attention while automatically evaluating their performance remains challenging. User satisfaction estimation (USE) has been proposed as an alternative. It assumes that the performance of a dialogue system can be measured by user satisfaction and uses an... | Emine Yilmaz, Fanghua Ye, Zhiyuan Hu |  |
| 1738 |  |  [Towards Identifying Fine-Grained Depression Symptoms from Memes](https://doi.org/10.18653/v1/2023.acl-long.495) |  | 0 | The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized... | Chenye Zhao, Cornelia Caragea, Marvin Solberg, Naincy Kumari, Shweta Yadav, Tanmay Sharma |  |
| 1739 |  |  [SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks](https://doi.org/10.18653/v1/2023.acl-long.496) |  | 0 | Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely... | Ankita Pasad, ChyiJiunn Lin, Felix Wu, Hungyi Lee, Karen Livescu, Roshan S. Sharma, Shinji Watanabe, Siddhant Arora, Suwon Shon, WeiLun Wu |  |
| 1740 |  |  [My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave](https://doi.org/10.18653/v1/2023.acl-long.497) |  | 0 | News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult... | David Chong, Pavan Holur, Timothy R. Tangherlini, Vwani Roychowdhury |  |
| 1741 |  |  [Characterizing and Measuring Linguistic Dataset Drift](https://doi.org/10.18653/v1/2023.acl-long.498) |  | 0 | NLP models often degrade in performance when real world data distributions differ markedly from training data. However, existing dataset drift metrics in NLP have generally not considered specific dimensions of linguistic drift that affect model performance, and they have not been validated in... | Dan Roth, Kishaloy Halder, Miguel Ballesteros, Neha Anna John, Tyler A. Chang, Yassine Benajiba, Yogarshi Vyas |  |
| 1742 |  |  [WebCPM: Interactive Web Search for Chinese Long-form Question Answering](https://doi.org/10.18653/v1/2023.acl-long.499) |  | 0 | Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates... | Dian Jin, Fanchao Qi, Huadong Wang, Jie Zhou, Kunlun Zhu, Lan Yan, Maosong Sun, Ning Ding, Ruobing Xie, Shihao Liang, Xu Han, Yankai Lin, Yujia Qin, Zhiyuan Liu, Zihan Cai |  |
| 1743 |  |  [Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model](https://doi.org/10.18653/v1/2023.acl-long.500) |  | 0 | Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a... | Bifan Wei, Hongwei Zeng, Jun Liu, Weiping Fu |  |
| 1744 |  |  [FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.501) |  | 0 | The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target... | ChenYu Lee, ChunLiang Li, Guolong Su, Hao Zhang, Joshua Ainslie, Kihyuk Sohn, Nan Hua, Nikolay Glushnev, Renshen Wang, Shangbang Long, Siyang Qin, Timothy Dozat, Tomas Pfister, Vincent Perot, Xiang Zhang, Yasuhisa Fujii |  |
| 1745 |  |  [MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies](https://doi.org/10.18653/v1/2023.acl-long.502) |  | 0 | Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way... | David S. Rosenberg, Mark Dredze, Mohit Bansal, Ozan Irsoy, Shijie Wu, Shiyue Zhang, Steven Lu |  |
| 1746 |  |  [Knowledgeable Parameter Efficient Tuning Network for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-long.503) |  | 0 | Commonsense question answering is important for making decisions about everyday matters. Although existing commonsense question answering works based on fully fine-tuned PLMs have achieved promising results, they suffer from prohibitive computation costs as well as poor interpretability. Some works... | Hanyu Zhao, Linmei Hu, Yequan Wang, Yingxia Shao, Ziwang Zhao |  |
| 1747 |  |  [BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric](https://doi.org/10.18653/v1/2023.acl-long.504) |  | 0 | End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we... | Alexandre Mourachko, Holger Schwenk, Justine Kao, Marta R. Costajussà, Mingda Chen, PaulAmbroise Duquenne, Pierre Andrews |  |
| 1748 |  |  [NLPositionality: Characterizing Design Biases of Datasets and Models](https://doi.org/10.18653/v1/2023.acl-long.505) |  | 0 | Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because... | Jenny T. Liang, Katharina Reinecke, Maarten Sap, Ronan Le Bras, Sebastin Santy |  |
| 1749 |  |  [Backpack Language Models](https://doi.org/10.18653/v1/2023.acl-long.506) |  | 0 | We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative... | Christopher D. Manning, John Hewitt, John Thickstun, Percy Liang |  |
| 1750 |  |  [WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.507) |  | 0 | We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our... | Eugene Jang, HoChun Herbert Chang, Jonathan May, Virginia K. Felkner |  |
| 1751 |  |  [Grounded Multimodal Named Entity Recognition on Social Media](https://doi.org/10.18653/v1/2023.acl-long.508) |  | 0 | In recent years, Multimodal Named Entity Recognition (MNER) on social media has attracted considerable attention. However, existing MNER studies only extract entity-type pairs in text, which is useless for multimodal knowledge graph construction and insufficient for entity disambiguation. To solve... | Jianfei Yu, Jieming Wang, Rui Xia, Ziyan Li |  |
| 1752 |  |  [Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference](https://doi.org/10.18653/v1/2023.acl-long.509) |  | 0 | Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to... | Haibin Chen, Huawen Feng, Junhao Zheng, Junlong Liu, Peitian Ma, Qianli Ma, Shengjie Qiu, Xichen Shang, Yue Wu |  |
| 1753 |  |  [Translation-Enhanced Multilingual Text-to-Image Generation](https://doi.org/10.18653/v1/2023.acl-long.510) |  | 0 | Research on text-to-image generation (TTI) still predominantly focuses on the English language due to the lack of annotated image-caption data in other languages; in the long run, this might widen inequitable access to TTI technology. In this work, we thus investigate multilingual TTI (termed mTTI)... | Anna Korhonen, ChingYun Chang, Ivan Vulic, Stephen Rawls, Yaoyiran Li |  |
| 1754 |  |  [Benchmarking Large Language Model Capabilities for Conditional Generation](https://doi.org/10.18653/v1/2023.acl-long.511) |  | 0 | Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques... | Joshua Maynez, Priyanka Agrawal, Sebastian Gehrmann |  |
| 1755 |  |  [lilGym: Natural Language Visual Reasoning with Reinforcement Learning](https://doi.org/10.18653/v1/2023.acl-long.512) |  | 0 | We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation... | Anne Wu, Kianté Brantley, Noriyuki Kojima, Yoav Artzi |  |
| 1756 |  |  [Unsupervised Melody-to-Lyrics Generation](https://doi.org/10.18653/v1/2023.acl-long.513) |  | 0 | Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as... | Alessandra Cervone, Anjali NarayanChen, Chenyang Tao, Gunnar A. Sigurdsson, Jing Huang, Nanyun Peng, Shereen Oraby, Tagyoung Chung, Wenbo Zhao, Yufei Tian |  |
| 1757 |  |  [Causality-aware Concept Extraction based on Knowledge-guided Prompting](https://doi.org/10.18653/v1/2023.acl-long.514) |  | 0 | Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as... | Deqing Yang, Jiaqing Liang, Jinxi Liu, Rui Xie, Shuyu Tian, Siyu Yuan, Yanghua Xiao |  |
| 1758 |  |  [Span-level Aspect-based Sentiment Analysis via Table Filling](https://doi.org/10.18653/v1/2023.acl-long.515) |  | 0 | In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding... | Linli Xu, Mao Zhang, Xing Sun, Yongxin Zhu, Yunfei Wu, Zhen Liu, Zhimin Bao |  |
| 1759 |  |  [Limitations of Language Models in Arithmetic and Symbolic Induction](https://doi.org/10.18653/v1/2023.acl-long.516) |  | 0 | Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing... | Hong Wang, Jing Qian, Shiyang Li, Xifeng Yan, Zekun Li |  |
| 1760 |  |  [EEL: Efficiently Encoding Lattices for Reranking](https://doi.org/10.18653/v1/2023.acl-long.517) |  | 0 | Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for “downstream” metrics can more closely optimize for... | Greg Durrett, Jiacheng Xu, Prasann Singhal, Xi Ye |  |
| 1761 |  |  [CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training](https://doi.org/10.18653/v1/2023.acl-long.518) |  | 0 | Improving text representation has attracted much attention to achieve expressive text-to-speech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech,... | Jinglin Liu, Jinzheng He, Rongjie Huang, Xiang Yin, Yi Ren, Zhenhui Ye, Zhou Zhao, Ziyue Jiang |  |
| 1762 |  |  [Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation](https://doi.org/10.18653/v1/2023.acl-long.519) |  | 0 | Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a... | Huajian Zhang, Jianhao Yan, Judy Li, Ming Zhong, Xianchao Zhu, Xuefeng Bai, Yafu Li, Yijie Zhou, Yue Zhang, Yueguan Wang, Yulong Chen |  |
| 1763 |  |  [Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation](https://doi.org/10.18653/v1/2023.acl-long.520) |  | 0 | Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two... | Danushka Bollegala, Xiaohang Tang, Yi Zhou |  |
| 1764 |  |  [How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech](https://doi.org/10.18653/v1/2023.acl-long.521) |  | 0 | When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children’s linguistic input? We explore... | Aditya Yedetore, R. Thomas McCoy, Robert Frank, Tal Linzen |  |
| 1765 |  |  [GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator](https://doi.org/10.18653/v1/2023.acl-long.522) |  | 0 | Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for... | Dongdong Zhang, Furu Wei, Haoyang Huang, Jian Yang, Li Dong, Liqun Yang, Shaohan Huang, Shuming Ma, Yuwei Yin, Zhoujun Li |  |
| 1766 |  |  [Linear Guardedness and its Implications](https://doi.org/10.18653/v1/2023.acl-long.523) |  | 0 | Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work,... | Ryan Cotterell, Shauli Ravfogel, Yoav Goldberg |  |
| 1767 |  |  [Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability](https://doi.org/10.18653/v1/2023.acl-long.524) |  | 0 | Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism—the... | Colin Cherry, Eleftheria Briakou, George F. Foster |  |
| 1768 |  |  [Open Set Relation Extraction via Unknown-Aware Training](https://doi.org/10.18653/v1/2023.acl-long.525) |  | 0 | The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, in which the relations remain the same during both training and testing. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of... | Jun Zhao, Qi Zhang, Tao Gui, WenYu Zhan, Xiang Gao, Xin Zhao, Xuanjing Huang, Yun Wen Chen, Zhongyu Wei |  |
| 1769 |  |  [Learning to Imagine: Visually-Augmented Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.526) |  | 0 | People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visually-augmented natural language... | JiRong Wen, Junyi Li, Tianyi Tang, Wayne Xin Zhao, Yifan Du, Yushuo Chen |  |
| 1770 |  |  [Generating Hashtags for Short-form Videos with Guided Signals](https://doi.org/10.18653/v1/2023.acl-long.527) |  | 0 | Short-form video hashtag recommendation (SVHR) aims to recommend hashtags to content creators from videos and corresponding descriptions. Most prior studies regard SVHR as a classification or ranking problem and select hashtags from a set of limited candidates. However, in reality, users can create... | Davis Liang, Hanchao Yu, Madian Khabsa, Pascale Fung, PoYao Huang, Shaoliang Nie, Tiezheng Yu, YiChia Wang, Yuning Mao |  |
| 1771 |  |  [NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints](https://doi.org/10.18653/v1/2023.acl-long.528) |  | 0 | Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic... | Mihai Surdeanu, Mohaddeseh Bastan, Niranjan Balasubramanian |  |
| 1772 |  |  [The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning](https://doi.org/10.18653/v1/2023.acl-long.529) |  | 0 | Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues.... | Gholamreza Haffari, Lizhen Qu, Philip R. Cohen, Raj Tumuluri, Zhuang Li |  |
| 1773 |  |  [Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the "What" and Focus on the "How"!](https://doi.org/10.18653/v1/2023.acl-long.530) |  | 0 | We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant... | Chen Chen, Dylan Walker, Venkatesh Saligrama |  |
| 1774 |  |  [Unsupervised Extractive Summarization of Emotion Triggers](https://doi.org/10.18653/v1/2023.acl-long.531) |  | 0 | Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events... | Cornelia Caragea, Hongli Zhan, Junyi Jessy Li, Tiberiu Sosea |  |
| 1775 |  |  [Document-Level Event Argument Extraction With a Chain Reasoning Paradigm](https://doi.org/10.18653/v1/2023.acl-long.532) |  | 0 | Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies. Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic... | Chen Liang, Haoyan Liu, Jian Liu, Jinan Xu, Zhe Zhao |  |
| 1776 |  |  [Pre-training Multi-party Dialogue Models with Latent Discourse Inference](https://doi.org/10.18653/v1/2023.acl-long.533) |  | 0 | Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that... | Hai Zhao, Wei Bi, Xinting Huang, Yiyang Li |  |
| 1777 |  |  [Interpreting Positional Information in Perspective of Word Order](https://doi.org/10.18653/v1/2023.acl-long.534) |  | 0 | The attention mechanism is a powerful and effective method utilized in natural language processing. However, it has been observed that this method is insensitive to positional information. Although several studies have attempted to improve positional encoding and investigate the influence of word... | Jin Liu, Ruochen Liu, Xilong Zhang, Xuefeng Liang |  |
| 1778 |  |  [I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation](https://doi.org/10.18653/v1/2023.acl-long.535) |  | 0 | Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are... | Chandra Bhagavatula, Doug Downey, Jena D. Hwang, Keisuke Sakaguchi, Lianhui Qin, Peter West, Ronan Le Bras, Swabha Swayamdipta, Ximing Lu, Yejin Choi |  |
| 1779 |  |  [More than Classification: A Unified Framework for Event Temporal Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.536) |  | 0 | Event temporal relation extraction (ETRE) is usually formulated as a multi-label classification task, where each type of relation is simply treated as a one-hot label. This formulation ignores the meaning of relations and wipes out their intrinsic dependency. After examining the relation... | Chang Liu, Dongyan Zhao, Quzhe Huang, Shengqi Zhu, Yansong Feng, Yutong Hu |  |
| 1780 |  |  [Multi-Source Test-Time Adaptation as Dueling Bandits for Extractive Question Answering](https://doi.org/10.18653/v1/2023.acl-long.537) |  | 0 | In this work, we study multi-source test-time model adaptation from user feedback, where K distinct models are established for adaptation. To allow efficient adaptation, we cast the problem as a stochastic decision-making process, aiming to determine the best adapted model after adaptation. We... | Hai Ye, Hwee Tou Ng, Qizhe Xie |  |
| 1781 |  |  [Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery](https://doi.org/10.18653/v1/2023.acl-long.538) |  | 0 | Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label... | Chen Zeng, Jingang Wang, Keqing He, Pei Wang, Weiran Xu, Xiaoshuai Song, Yunsen Xian, Yutao Mou |  |
| 1782 |  |  [DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering](https://doi.org/10.18653/v1/2023.acl-long.539) |  | 0 | Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause... | Fei Huang, Fei Mi, Minlie Huang, Pei Ke, Qun Liu, Xiaoyan Zhu, Yasheng Wang |  |
| 1783 |  |  [Backdooring Neural Code Search](https://doi.org/10.18653/v1/2023.acl-long.540) |  | 0 | Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence... | Bin Luo, Chunrong Fang, Guanhong Tao, Quanjun Zhang, Weisong Sun, Xiangyu Zhang, Yuchen Chen |  |
| 1784 |  |  [Concise Answers to Complex Questions: Summarization of Long-form Answers](https://doi.org/10.18653/v1/2023.acl-long.541) |  | 0 | Long-form question answering systems provide rich information by presenting paragraph-level answers, often containing optional background or auxiliary information. While such comprehensive answers are helpful, not all information is required to answer the question (e.g. users with domain knowledge... | Abhilash Potluri, Eunsol Choi, Fangyuan Xu |  |
| 1785 |  |  [Towards Better Entity Linking with Multi-View Enhanced Distillation](https://doi.org/10.18653/v1/2023.acl-long.542) |  | 0 | Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in... | Fang Fang, Haizhen Huang, Jianxun Lian, Qi Zhang, Weiwei Deng, Wen Zhang, Xinlong Wang, Yanan Cao, Yi Liu, Yuan Tian |  |
| 1786 |  |  [A Measure-Theoretic Characterization of Tight Language Models](https://doi.org/10.18653/v1/2023.acl-long.543) |  | 0 | Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite... | Clara Meister, Jason Eisner, Li Du, Lucas Torroba Hennigen, Ryan Cotterell, Tiago Pimentel |  |
| 1787 |  |  [PAED: Zero-Shot Persona Attribute Extraction in Dialogues](https://doi.org/10.18653/v1/2023.acl-long.544) |  | 0 | Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated... | Erik Cambria, Luyao Zhu, Rui Mao, Vlad Pandelea, Wei Li |  |
| 1788 |  |  [PromptRank: Unsupervised Keyphrase Extraction Using Prompt](https://doi.org/10.18653/v1/2023.acl-long.545) |  | 0 | The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to... | Aobo Kong, Hao Chen, Qicheng Li, Ruiqi Sun, Shiwan Zhao, Xiaoyan Bai, Yong Qin |  |
| 1789 |  |  [When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories](https://doi.org/10.18653/v1/2023.acl-long.546) |  | 0 | Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in... | Akari Asai, Alex Mallen, Daniel Khashabi, Hannaneh Hajishirzi, Rajarshi Das, Victor Zhong |  |
| 1790 |  |  [infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information](https://doi.org/10.18653/v1/2023.acl-long.547) |  | 0 | The success of NLP systems often relies on the availability of large, high-quality datasets. However, not all samples in these datasets are equally valuable for learning, as some may be redundant or noisy. Several methods for characterizing datasets based on model-driven meta-information (e.g.,... | Dongyeop Kang, Jaehyung Kim, Jinwoo Shin, Karin de Langis, Yekyung Kim |  |
| 1791 |  |  [SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models](https://doi.org/10.18653/v1/2023.acl-long.548) |  | 0 | Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as... | Aida Mostafazadeh Davani, Akshita Jha, Chandan K. Reddy, Shachi Dave, Sunipa Dev, Vinodkumar Prabhakaran |  |
| 1792 |  |  [Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations](https://doi.org/10.18653/v1/2023.acl-long.549) |  | 0 | Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task,... | Bailey Kuehl, Byron C. Wallace, Erin Bransom, Jay DeYoung, Lucy Lu Wang, Thinh Hung Truong, Yulia Otmakhova |  |
| 1793 |  |  [Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge](https://doi.org/10.18653/v1/2023.acl-long.550) |  | 0 | Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative... | Jiangjie Chen, Lei Li, Sijie Cheng, Wei Shi, Yanghua Xiao, Ziquan Fu |  |
| 1794 |  |  [An Inner Table Retriever for Robust Table Question Answering](https://doi.org/10.18653/v1/2023.acl-long.551) |  | 0 | Recent years have witnessed the thriving of pretrained Transformer-based language models for understanding semi-structured tables, with several applications, such as Table Question Answering (TableQA).These models are typically trained on joint tables and surrounding natural language text, by... | Adrià de Gispert, Bill Byrne, Gonzalo Iglesias, Rexhina Blloshmi, Weizhe Lin |  |
| 1795 |  |  [SIMSUM: Document-level Text Simplification via Simultaneous Summarization](https://doi.org/10.18653/v1/2023.acl-long.552) |  | 0 | Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification.... | Carsten Eickhoff, Martin Jaggi, Seyed Ali Bahrainian, Sofia Blinova, Xinyu Zhou |  |
| 1796 |  |  [SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation](https://doi.org/10.18653/v1/2023.acl-long.553) |  | 0 | Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on... | Huawei Shen, Junkai Zhou, Liang Pang, Xueqi Cheng |  |
| 1797 |  |  [NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic](https://doi.org/10.18653/v1/2023.acl-long.554) |  | 0 | Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models... | Xiaodan Zhu, Zi'ou Zheng |  |
| 1798 |  |  [Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction](https://doi.org/10.18653/v1/2023.acl-long.555) |  | 0 | A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to... | Adam S. Miner, Ashish Sharma, David Wadden, Inna E. Lin, Kevin Rushton, Khendra G. Lucas, Theresa Nguyen, Tim Althoff |  |
| 1799 |  |  [Dating Greek Papyri with Text Regression](https://doi.org/10.18653/v1/2023.acl-long.556) |  | 0 | Dating Greek papyri accurately is crucial not only to edit their texts but also to understand numerous other aspects of ancient writing, document and book production and circulation, as well as various other aspects of administration, everyday life and intellectual history of antiquity. Although a... | Asimina Paparigopoulou, Holger Essler, Isabelle MarthotSantaniello, John Pavlopoulos, Maria Konstantinidou |  |
| 1800 |  |  [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://doi.org/10.18653/v1/2023.acl-long.557) |  | 0 | Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within... | Ashish Sabharwal, Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot |  |
| 1801 |  |  [Direct Fact Retrieval from Knowledge Graphs without Entity Linking](https://doi.org/10.18653/v1/2023.acl-long.558) |  | 0 | There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However,... | Alham Fikri Aji, Jens Lehmann, Jinheon Baek, Sung Ju Hwang |  |
| 1802 |  |  [DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering](https://doi.org/10.18653/v1/2023.acl-long.559) |  | 0 | Question answering models commonly have access to two sources of “knowledge” during inference time: (1) parametric knowledge - the factual knowledge encoded in the model weights, and (2) contextual knowledge - external knowledge (e.g., a Wikipedia passage) given to the model to generate a grounded... | Ella Neeman, Idan Szpektor, Leshem Choshen, Omri Abend, Or Honovich, Roee Aharoni |  |
| 1803 |  |  [A New Direction in Stance Detection: Target-Stance Extraction in the Wild](https://doi.org/10.18653/v1/2023.acl-long.560) |  | 0 | Stance detection aims to detect the stance toward a corresponding target. Existing works use the assumption that the target is known in advance, which is often not the case in the wild. Given a text from social media platforms, the target information is often unknown due to implicit mentions in the... | Cornelia Caragea, Krishna Garg, Yingjie Li |  |
| 1804 |  |  [Improved Instruction Ordering in Recipe-Grounded Conversation](https://doi.org/10.18653/v1/2023.acl-long.561) |  | 0 | In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this... | Alan Ritter, Duong Minh Le, Ruohao Guo, Wei Xu |  |
| 1805 |  |  [Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions](https://doi.org/10.18653/v1/2023.acl-long.562) |  | 0 | While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have made their behavior somewhat opaque. To mitigate this opacity, this work presents a linear decomposition of final... | ByungDoh Oh, William Schuler |  |
| 1806 |  |  [Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization](https://doi.org/10.18653/v1/2023.acl-long.563) |  | 0 | Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of... | Lin Gui, Xinyu Wang, Yulan He |  |
| 1807 |  |  [Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training](https://doi.org/10.18653/v1/2023.acl-long.564) |  | 0 | Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the... | Lei Shen, Meng Chen, Xiaodong He, Yuming Zhao, Zhenyu Zhang |  |
| 1808 |  |  [Language Detoxification with Attribute-Discriminative Latent Space](https://doi.org/10.18653/v1/2023.acl-long.565) |  | 0 | Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to... | Jin Myung Kwak, Minseon Kim, Sung Ju Hwang |  |
| 1809 |  |  [Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction](https://doi.org/10.18653/v1/2023.acl-long.566) |  | 0 | Sarcasm, as a form of irony conveying mockery and contempt, has been widespread in social media such as Twitter and Weibo, where the sarcastic text is commonly characterized as an incongruity between the surface positive and negative situation. Naturally, it has an urgent demand to automatically... | Bo Xu, Changrong Min, Hongfei Lin, Liang Yang, Ximing Li, Zhilin Wang |  |
| 1810 |  |  [Adaptive and Personalized Exercise Generation for Online Language Learning](https://doi.org/10.18653/v1/2023.acl-long.567) |  | 0 | Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise... | Mrinmaya Sachan, Peng Cui |  |
| 1811 |  |  [NLP Reproducibility For All: Understanding Experiences of Beginners](https://doi.org/10.18653/v1/2023.acl-long.568) |  | 0 | As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their... | Joyce Chai, Keunwoo Peter Yu, Shane Storks, Ziqiao Ma |  |
| 1812 |  |  [Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA](https://doi.org/10.18653/v1/2023.acl-long.569) |  | 0 | Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group... | Benjamin Van Durme, Elias StengelEskin, Jimena GuallarBlasco, Yi Zhou |  |
| 1813 |  |  [UMRSpell: Unifying the Detection and Correction Parts of Pre-trained Models towards Chinese Missing, Redundant, and Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.570) |  | 0 | Chinese Spelling Correction (CSC) is the task of detecting and correcting misspelled charac- ters in Chinese texts. As an important step for various downstream tasks, CSC confronts two challenges: 1) Character-level errors consist not only of spelling errors but also of missing and redundant ones... | Liang Xu, Linlin Wang, Yujin Zhu, Zheyu He |  |
| 1814 |  |  [LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction](https://doi.org/10.18653/v1/2023.acl-long.571) |  | 0 | Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of... | Alex Fabrikant, Annie Louis, Donald Metzler, Jeremiah Milbauer, Mohammad Javad Hosseini, Tal Schuster |  |
| 1815 |  |  [Local Interpretation of Transformer Based on Linear Decomposition](https://doi.org/10.18653/v1/2023.acl-long.572) |  | 0 | In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the... | Jiajun Chen, Jianbing Zhang, Sen Yang, Shujian Huang, Wei Zou, Xinyu Dai |  |
| 1816 |  |  [DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions](https://doi.org/10.18653/v1/2023.acl-long.573) |  | 0 | Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable... | Graham Neubig, Luyu Gao, Pengfei Liu, Tongshuang Wu, Vijay Viswanathan |  |
| 1817 |  |  [Multilingual Event Extraction from Historical Newspaper Adverts](https://doi.org/10.18653/v1/2023.acl-long.574) |  | 0 | NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most... | Isabelle Augenstein, Nadav Borenstein, Natalia da Silva Perez |  |
| 1818 |  |  [BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency](https://doi.org/10.18653/v1/2023.acl-long.575) |  | 0 | Twitter bots are automatic programs operated by malicious actors to manipulate public opinion and spread misinformation. Research efforts have been made to automatically identify bots based on texts and networks on social media. Existing methods only leverage texts or networks alone, and while few... | Herun Wan, Jundong Li, Minnan Luo, Qinghua Zheng, Shangbin Feng, Wenqian Zhang, Zhenyu Lei, Zilong Chen |  |
| 1819 |  |  [Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions](https://doi.org/10.18653/v1/2023.acl-long.576) |  | 0 | When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create... | Avinash Kumar Singh, Indrajit Bhattacharya, Lovekesh Vig, Mausam, Mayur Patidar, Prayushi Faldu |  |
| 1820 |  |  [Understanding Client Reactions in Online Mental Health Counseling](https://doi.org/10.18653/v1/2023.acl-long.577) |  | 0 | Communication success relies heavily on reading participants’ reactions. Such feedback is especially important for mental health counselors, who must carefully consider the client’s progress and adjust their approach accordingly. However, previous NLP research on counseling has mainly focused on... | Anqi Li, Hongliang He, Huachuan Qiu, Lizhi Ma, Shuai Zhang, Yaling Mei, Zhenzhong Lan |  |
| 1821 |  |  [Nonlinear Structural Equation Model Guided Gaussian Mixture Hierarchical Topic Modeling](https://doi.org/10.18653/v1/2023.acl-long.578) |  | 0 | Hierarchical topic models, which can extract semantically meaningful topics from a textcorpus in an unsupervised manner and automatically organise them into a topic hierarchy, have been widely used to discover the underlying semantic structure of documents. However, the existing models often assume... | Hegang Chen, Pengbo Mao, Yanghui Rao, Yuyin Lu |  |
| 1822 |  |  [Revisiting Token Dropping Strategy in Efficient BERT Pretraining](https://doi.org/10.18653/v1/2023.acl-long.579) |  | 0 | Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream... | Bo Du, Dacheng Tao, Juhua Liu, Liang Ding, Min Zhang, Qihuang Zhong, Xuebo Liu |  |
| 1823 |  |  [The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers](https://doi.org/10.18653/v1/2023.acl-long.580) |  | 0 | Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers,... | Ariel Gera, Benjamin Sznajder, Eyal Shnarch, Noam Slonim, Ofir Arviv, R. Chulaka Gunasekara, Roni Friedman |  |
| 1824 |  |  [FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering](https://doi.org/10.18653/v1/2023.acl-long.581) |  | 0 | Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude... | Aman Chadha, Amit P. Sheth, Amitava Das, Anku Rani, Dwip Dalal, Megha Chakraborty, S. M. Towhidul Islam Tonmoy, Shreya Gautam |  |
| 1825 |  |  [Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages](https://doi.org/10.18653/v1/2023.acl-long.582) |  | 0 | We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families. The dataset contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person,... | Anoop Kunchukuttan, Arnav Mhaske, Harshit Kedia, Mitesh M. Khapra, Pratyush Kumar, Sumanth Doddapaneni, V. Rudra Murthy |  |
| 1826 |  |  [CREPE: Open-Domain Question Answering with False Presuppositions](https://doi.org/10.18653/v1/2023.acl-long.583) |  | 0 | When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of... | Hannaneh Hajishirzi, Luke Zettlemoyer, Sewon Min, Xinyan Yu |  |
| 1827 |  |  [Joint Document-Level Event Extraction via Token-Token Bidirectional Event Completed Graph](https://doi.org/10.18653/v1/2023.acl-long.584) |  | 0 | We solve the challenging document-level event extraction problem by proposing a joint exaction methodology that can avoid inefficiency and error propagation issues in classic pipeline methods. Essentially, we address the three crucial limitations in existing studies. First, the autoregressive... | Bolong Zheng, Changxuan Wan, Chenliang Li, Dexi Liu, Keli Xiao, Qizhi Wan, Rong Hu, Xiping Liu |  |
| 1828 |  |  [Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering](https://doi.org/10.18653/v1/2023.acl-long.585) |  | 0 | Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above... | Chaochao Chen, Mengling Hu, Weiming Liu, Xiaolin Zheng, Xinting Liao |  |
| 1829 |  |  [Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention](https://doi.org/10.18653/v1/2023.acl-long.586) |  | 0 | Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces... | Chengqing Zong, Rongchuan Tang, Yang Zhao, Yu Zhou |  |
| 1830 |  |  [What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization](https://doi.org/10.18653/v1/2023.acl-long.587) |  | 0 | Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or,... | Anna Ostropolets, Bichlien Nguyen, Budhaditya Deb, Griffin Adams, Jake Smith, Noémie Elhadad, Shufang Xie, Tristan Naumann, Yingce Xia, YuanJyue Chen |  |
| 1831 |  |  [Annotating Mentions Alone Enables Efficient Domain Adaptation for Coreference Resolution](https://doi.org/10.18653/v1/2023.acl-long.588) |  | 0 | Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, it remains a challenge to successfully transfer these models to new target domains containing many out-of-vocabulary spans and requiring differing annotation schemes. Typical... | Anjalie Field, Emma Strubell, Nupoor Gandhi |  |
| 1832 |  |  [A Universal Discriminator for Zero-Shot Generalization](https://doi.org/10.18653/v1/2023.acl-long.589) |  | 0 | Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a... | Haike Xu, Jing Zhou, Yanan Zheng, Zhilin Yang, Zongyu Lin |  |
| 1833 |  |  [Syntax and Geometry of Information](https://doi.org/10.18653/v1/2023.acl-long.590) |  | 0 | This paper presents an information-theoretical model of syntactic generalization. We study syntactic generalization from the perspective of the capacity to disentangle semantic and structural information, emulating the human capacity to assign a grammaticality judgment to semantically nonsensical... | Kata Gábor, Laurent Leblond, Raphaël Bailly |  |
| 1834 |  |  [GreenKGC: A Lightweight Knowledge Graph Completion Method](https://doi.org/10.18653/v1/2023.acl-long.591) |  | 0 | Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple score function. Yet, a higher-dimensional embedding space is usually required for a... | Bin Wang, C.C. Jay Kuo, Xiou Ge, Yuncheng Wang |  |
| 1835 |  |  [Unsupervised Open-domain Keyphrase Generation](https://doi.org/10.18653/v1/2023.acl-long.592) |  | 0 | In this work, we study the problem of unsupervised open-domain keyphrase generation, where the objective is a keyphrase generation model that can be built without using human-labeled data and can perform consistently across domains. To solve this problem, we propose a seq2seq model that consists of... | Kevin ChenChuan Chang, Lam Do, Pritom Saha Akash |  |
| 1836 |  |  [A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment](https://doi.org/10.18653/v1/2023.acl-long.593) |  | 0 | When communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS... | Chuan Wu, Jiyue Jiang, Lingpeng Kong, Qintong Li, Sheng Wang |  |
| 1837 |  |  [Plug-and-Play Knowledge Injection for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.594) |  | 0 | Injecting external knowledge can improve the performance of pre-trained language models (PLMs) on various downstream NLP tasks. However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks. In this work, we are the first to study how to... | Chaojun Xiao, Deming Ye, Huadong Wang, Jie Zhou, Maosong Sun, Peng Li, Xu Han, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Zhiyuan Zeng |  |
| 1838 |  |  [Two Birds One Stone: Dynamic Ensemble for OOD Intent Classification](https://doi.org/10.18653/v1/2023.acl-long.595) |  | 0 | Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does... | Jianqiang Yang, Pengyu Wang, Xipeng Qiu, Yunhua Zhou |  |
| 1839 |  |  [SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages](https://doi.org/10.18653/v1/2023.acl-long.596) |  | 0 | Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step,... | Caiming Xiong, ChienSheng Wu, Jesse Vig, Philippe Laban, Shafiq Joty, Wojciech Kryscinski |  |
| 1840 |  |  [Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?](https://doi.org/10.18653/v1/2023.acl-long.597) |  | 0 | Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has... | Dawei Yin, Harry Shomer, Jiayuan Ding, Jiliang Tang, Juanhui Li, Neil Shah, Yao Ma, Yiqi Wang |  |
| 1841 |  |  [A dynamic programming algorithm for span-based nested named-entity recognition in O(n²)](https://doi.org/10.18653/v1/2023.acl-long.598) |  | 0 | Span-based nested named-entity recognition (NER) has a cubic-time complexity using avariant of the CYK algorithm. We show that by adding a supplementary structural constraint on the search space, nested NER has a quadratic-time complexity, that is the same asymptotic complexity than the non-nested... | Caio Corro |  |
| 1842 |  |  [Target-Side Augmentation for Document-Level Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.599) |  | 0 | Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA)... | Guangsheng Bao, Yue Zhang, Zhiyang Teng |  |
| 1843 |  |  [Rethinking Masked Language Modeling for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.600) |  | 0 | In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor... | Hai Zhao, Hongqiu Wu, Shaohua Zhang, Yuchen Zhang |  |
| 1844 |  |  [A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues](https://doi.org/10.18653/v1/2023.acl-long.601) |  | 0 | Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language... | Baotian Hu, Lin Ma, Min Zhang, Xinyu Chen, Yunxin Li, Yuxin Ding |  |
| 1845 |  |  [Simple and Effective Unsupervised Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.602) |  | 0 | The amount of labeled data to train models for speech tasks is limited for most languages, however, the data scarcity is exacerbated for speech translation which requires labeled data covering two different languages. To address this issue, we study a simple and effective approach to build speech... | Changhan Wang, Hirofumi Inaguma, Ilia Kulikov, Juan Pino, Michael Auli, PengJen Chen, WeiNing Hsu, Yun Tang |  |
| 1846 |  |  [Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation](https://doi.org/10.18653/v1/2023.acl-long.603) |  | 0 | Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the... | Ai Ti Aw, Anh Tran Tai, Bowei Zou, Liangming Pan, Nancy F. Chen, Shafiq R. Joty, Xuan Long Do |  |
| 1847 |  |  [CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification](https://doi.org/10.18653/v1/2023.acl-long.604) |  | 0 | Document-level Event Causality Identification (DECI) aims to recognize causal relations between events within a document. Recent studies focus on building a document-level graph for cross-sentence reasoning, but ignore important causal structures — there are one or two “central” events that prevail... | Meiqi Chen, Yan Zhang, Yixin Cao, Zhiwei Liu |  |
| 1848 |  |  [f-Divergence Minimization for Sequence-Level Knowledge Distillation](https://doi.org/10.18653/v1/2023.acl-long.605) |  | 0 | Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework,... | Lili Mou, Wenyu Du, Yuqiao Wen, Zichao Li |  |
| 1849 |  |  [Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.606) |  | 0 | Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies... | Dou Hu, Lingwei Wei, Songlin Hu, Wei Zhou, Yinan Bao |  |
| 1850 |  |  [A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.607) |  | 0 | Document-level relation extraction (DocRE) aims to extract relations among entities within a document, which is crucial for applications like knowledge graph construction. Existing methods usually assume that entities and their mentions are identified beforehand, which falls short of real-world... | Lei Zou, Ruoyu Zhang, Yanzeng Li |  |
| 1851 |  |  [A Synthetic Data Generation Framework for Grounded Dialogues](https://doi.org/10.18653/v1/2023.acl-long.608) |  | 0 | Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained... | Aixin Sun, Fei Mi, Jianzhu Bao, Rui Wang, Ruifeng Xu, Yasheng Wang, Yitong Li |  |
| 1852 |  |  [MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages](https://doi.org/10.18653/v1/2023.acl-long.609) |  | 0 | In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both... | Aliyu Yusuf, Allahsera Auguste Tapo, Amelia V. Taylor, Andiswa Bukula, Apelete Agbolo, Aremu Anuoluwapo, Blessing K. Sibanda, Bonaventure F. P. Dossou, Catherine Gitau, Cheikh M. Bamba Dione, Chinedu Uchechukwu, Chris Chinenye Emezue, David Ifeoluwa Adelani, Derguene Mbaye, Dietrich Klakow, Edwin MunkohBuabeng, Elvis Mboning Tchiaze, Emile Niyomutabazi, Ester Chimhenga, Fatoumata Ouoba Kabore, Godson Kalipe, Gratien Atindogbe, Happy Buzaaba, Idris Akinade, Ikechukwu E. Onyenwe, Jesujoba O. Alabi, Jonathan Mukiibi, Kudzai Gotosa, Marien Nahimana, Muhammad Abdullahi, Patrick Mizha, Perez Ogayo, Peter Nabende, Rooweither Mabuya, Samuel Olanrewaju, Seydou Traore, Shamsuddeen Hassan Muhammad, Tajuddeen Gwadabe, Tebogo Macucwa, Thapelo Sindane, Théogène Musabeyezu, Tolulope Anu Adelani, Victoire Memdjokam Koagne, Vukosi Marivate |  |
| 1853 |  |  [Semantic Structure Enhanced Event Causality Identification](https://doi.org/10.18653/v1/2023.acl-long.610) |  | 0 | Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly... | Jiafeng Guo, Long Bai, Saiping Guan, Xiaolong Jin, Xueqi Cheng, Zhilei Hu, Zixuan Li |  |
| 1854 |  |  [Weakly-Supervised Spoken Video Grounding via Semantic Interaction Learning](https://doi.org/10.18653/v1/2023.acl-long.611) |  | 0 | The task of spoken video grounding aims to localize moments in videos that are relevant to descriptive spoken queries. However, extracting semantic information from speech and modeling the cross-modal correlation pose two critical challenges. Previous studies solve them by representing spoken... | Linjun Li, Shengyu Zhang, Tao Jin, Wang Lin, Xize Cheng, Ye Wang, Zhou Zhao |  |
| 1855 |  |  [Rehearsal-free Continual Language Learning via Efficient Parameter Isolation](https://doi.org/10.18653/v1/2023.acl-long.612) |  | 0 | We study the problem of defying catastrophic forgetting when learning a series of language processing tasks. Compared with previous methods, we emphasize the importance of not caching history tasks’ data, which makes the problem more challenging. Our proposed method applies the parameter isolation... | Congcong Jiang, Ling Wang, Tao Ji, Wenqiu Zeng, Xiaoling Wang, Xu Shao, Ye Chao, Yuanbin Wu, Yufang Liu, Zhencong Han, Zhicheng Wang |  |
| 1856 |  |  [Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification](https://doi.org/10.18653/v1/2023.acl-long.613) |  | 0 | Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution... | ChihYao Chen, LunWei Ku, TunMin Hung, YiLi Hsu |  |
| 1857 |  |  [Combo of Thinking and Observing for Outside-Knowledge VQA](https://doi.org/10.18653/v1/2023.acl-long.614) |  | 0 | Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language... | Huishan Ji, Qingyi Si, Weiping Wang, Yuchen Mo, Zheng Lin |  |
| 1858 |  |  [AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model](https://doi.org/10.18653/v1/2023.acl-long.615) |  | 0 | Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on... | IHung Hsu, KuanHao Huang, Nanyun Peng, Prem Natarajan, Zhiyu Xie |  |
| 1859 |  |  [Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships](https://doi.org/10.18653/v1/2023.acl-long.616) |  | 0 | Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering... | Agrima Seth, Athena Aghighi, David Jurgens, Jackson Sargent, Michael Geraci |  |
| 1860 |  |  [TART: Improved Few-shot Text Classification Using Task-Adaptive Reference Transformation](https://doi.org/10.18653/v1/2023.acl-long.617) |  | 0 | Meta-learning has emerged as a trending technique to tackle few-shot text classification and achieve state-of-the-art performance. However, the performance of existing approaches heavily depends on the inter-class variance of the support set. As a result, it can perform well on tasks when the... | ChangTien Lu, Fanglan Chen, Jianfeng He, Shuo Lei, Xuchao Zhang |  |
| 1861 |  |  [How Do In-Context Examples Affect Compositional Generalization?](https://doi.org/10.18653/v1/2023.acl-long.618) |  | 0 | Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how... | Bei Chen, Dongmei Zhang, JianGuang Lou, Nanning Zheng, Qiang Fu, Shengnan An, Zeqi Lin |  |
| 1862 |  |  [Attractive Storyteller: Stylized Visual Storytelling with Unpaired Text](https://doi.org/10.18653/v1/2023.acl-long.619) |  | 0 | Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or... | Dingyi Yang, Qin Jin |  |
| 1863 |  |  [Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation](https://doi.org/10.18653/v1/2023.acl-long.620) |  | 0 | Many machine learning-based low-code or no-code applications involve generating code that interacts with structured knowledge. For example, one of the most studied tasks in this area is generating SQL code from a natural language statement. Prior work shows that incorporating context information... | Benjamin Kleiner, Dejiao Zhang, Ming Tan, Parminder Bhatia, Ramesh Nallapati, Robert Giaquinto, Xiaofei Ma, Yang Li |  |
| 1864 |  |  [WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction](https://doi.org/10.18653/v1/2023.acl-long.621) |  | 0 | Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences.... | Masaaki Nagata, Qiyu Wu, Yoshimasa Tsuruoka |  |
| 1865 |  |  [Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models](https://doi.org/10.18653/v1/2023.acl-long.622) |  | 0 | Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than... | Alan Ritter, Junmo Kang, Wei Xu |  |
| 1866 |  |  [OD-RTE: A One-Stage Object Detection Framework for Relational Triple Extraction](https://doi.org/10.18653/v1/2023.acl-long.623) |  | 0 | The Relational Triple Extraction (RTE) task is a fundamental and essential information extraction task. Recently, the table-filling RTE methods have received lots of attention. Despite their success, they suffer from some inherent problems such as underutilizing regional information of triple. In... | Hongfei Lin, Jinzhong Ning, Yuanyuan Sun, Zhihao Yang, Zhizheng Wang |  |
| 1867 |  |  [I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons](https://doi.org/10.18653/v1/2023.acl-long.624) |  | 0 | We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the... | Andrew Zhu, Chris CallisonBurch, Jay Pujara, Jennifer Hu, Pei Zhou, Prithviraj Ammanabrolu, Xiang Ren, Yejin Choi |  |
| 1868 |  |  [Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning](https://doi.org/10.18653/v1/2023.acl-long.625) |  | 0 | Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we... | Qin Zhu, Tianxiang Sun, Xipeng Qiu, Xuanjing Huang, Zhengfu He |  |
| 1869 |  |  [Is GPT-3 a Good Data Annotator?](https://doi.org/10.18653/v1/2023.acl-long.626) |  | 0 | Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI,... | Bosheng Ding, Boyang Li, Chengwei Qin, Lidong Bing, Linlin Liu, Shafiq Joty, Yew Ken Chia |  |
| 1870 |  |  [Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog](https://doi.org/10.18653/v1/2023.acl-long.627) |  | 0 | Retrieving proper domain knowledge from an external database lies at the heart of end-to-end task-oriented dialog systems to generate informative responses. Most existing systems blend knowledge retrieval with response generation and optimize them with direct supervision from reference responses,... | Fanqi Wan, Ke Yang, Wei Bi, Weizhou Shen, Xiaojun Quan |  |
| 1871 |  |  [Few-shot Event Detection: An Empirical Study and a Unified View](https://doi.org/10.18653/v1/2023.acl-long.628) |  | 0 | Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models,... | Aixin Sun, Yixin Cao, Yubo Ma, Zehao Wang |  |
| 1872 |  |  [How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases](https://doi.org/10.18653/v1/2023.acl-long.629) |  | 0 | Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what... | Aaron Mueller, Tal Linzen |  |
| 1873 |  |  [ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations](https://doi.org/10.18653/v1/2023.acl-long.630) |  | 0 | Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life. We present ClarifyDelphi, an interactive system that learns to ask... | Chandra Bhagavatula, Jena D. Hwang, Liwei Jiang, Valentina Pyatkin, Vivek Srikumar, Ximing Lu, Yejin Choi |  |
| 1874 |  |  [HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation](https://doi.org/10.18653/v1/2023.acl-long.631) |  | 0 | Recent NLP models have shown the remarkable ability to effectively generalise ‘zero-shot’ to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every... | Akshita Bhagia, Hamish Ivison, Hannaneh Hajishirzi, Matthew E. Peters, Yizhong Wang |  |
| 1875 |  |  [Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations](https://doi.org/10.18653/v1/2023.acl-long.632) |  | 0 | In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given... | Chenglei Si, Dan Friedman, Danqi Chen, He He, Nitish Joshi, Shi Feng |  |
| 1876 |  |  [An Inclusive Notion of Text](https://doi.org/10.18653/v1/2023.acl-long.633) |  | 0 | Natural language processing (NLP) researchers develop models of grammar, meaning and communication based on written text. Due to task and data differences, what is considered text can vary substantially across studies. A conceptual framework for systematically capturing these differences is... | Ilia Kuznetsov, Iryna Gurevych |  |
| 1877 |  |  [AlignScore: Evaluating Factual Consistency with A Unified Alignment Function](https://doi.org/10.18653/v1/2023.acl-long.634) |  | 0 | Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI)... | Ruichen Li, Yichi Yang, Yuheng Zha, Zhiting Hu |  |
| 1878 |  |  [Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation](https://doi.org/10.18653/v1/2023.acl-long.635) |  | 0 | Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART... | Kun Ouyang, Liqiang Jing, Liqiang Nie, Mengzhao Jia, Xuemeng Song |  |
| 1879 |  |  [Counterfactual Active Learning for Out-of-Distribution Generalization](https://doi.org/10.18653/v1/2023.acl-long.636) |  | 0 | We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem,... | Fuli Feng, Hanwang Zhang, Wenjie Wang, Xiangnan He, Xun Deng, Yong Liao |  |
| 1880 |  |  [Multi-granularity Temporal Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.637) |  | 0 | Recently, question answering over temporal knowledge graphs (i.e., TKGQA) has been introduced and investigated, in quest of reasoning about dynamic factual knowledge. To foster research on TKGQA, a few datasets have been curated (e.g., CronQuestions and Complex-CronQuestions), and various models... | Jinzhi Liao, Xiang Zhao, Ziyang Chen |  |
| 1881 |  |  [A New Aligned Simple German Corpus](https://doi.org/10.18653/v1/2023.acl-long.638) |  | 0 | “Leichte Sprache”, the German counterpart to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German – German. It contains... | Christian Bauckhage, Malte Boßert, Moritz Busch, Pascal Welke, Vanessa Toborek |  |
| 1882 |  |  [Introducing Semantics into Speech Encoders](https://doi.org/10.18653/v1/2023.acl-long.639) |  | 0 | Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by... | Akshat Shrivastava, Alexei Baevski, Bing Liu, Changhan Wang, Derek Xu, GuanTing Lin, Hungyi Lee, LiangHsuan Tseng, ShangWen Li, Shuyan Dong, Suyoun Kim, Wei Wang, Yizhou Sun, Zhaojiang Lin |  |
| 1883 |  |  [Constrained Tuple Extraction with Interaction-Aware Network](https://doi.org/10.18653/v1/2023.acl-long.640) |  | 0 | Tuples extraction is a fundamental task for information extraction and knowledge graph construction. The extracted tuples are usually represented as knowledge triples consisting of subject, relation, and object. In practice, however, the validity of knowledge triples is associated with and changes... | Chunxia Zhang, Tianxiang Xu, Xiaojun Xue, Zhendong Niu |  |
| 1884 |  |  [MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning](https://doi.org/10.18653/v1/2023.acl-long.641) |  | 0 | Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work,... | Lifu Huang, Ying Shen, Zhiyang Xu |  |
| 1885 |  |  [Single Sequence Prediction over Reasoning Graphs for Multi-hop QA](https://doi.org/10.18653/v1/2023.acl-long.642) |  | 0 | Recent generative approaches for multi-hop question answering (QA) utilize the fusion-in-decoder method to generate a single sequence output which includes both a final answer and a reasoning path taken to arrive at that answer, such as passage titles and key facts from those passages. While such... | Gowtham Ramesh, Junjie Hu, Makesh Narsimhan Sreedhar |  |
| 1886 |  |  [Contrastive Error Attribution for Finetuned Language Models](https://doi.org/10.18653/v1/2023.acl-long.643) |  | 0 | Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a... | Esin Durmus, Faisal Ladhak, Tatsunori Hashimoto |  |
| 1887 |  |  [DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications](https://doi.org/10.18653/v1/2023.acl-long.644) |  | 0 | Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural... | Adam Ivankay, Mattia Rigotti, Pascal Frossard |  |
| 1888 |  |  [Neural Machine Translation for Mathematical Formulae](https://doi.org/10.18653/v1/2023.acl-long.645) |  | 0 | We tackle the problem of neural machine translation of mathematical formulae between ambiguous presentation languages and unambiguous content languages. Compared to neural machine translation on natural language, mathematical formulae have a much smaller vocabulary and much longer sequences of... | André GreinerPetter, Bela Gipp, Felix Petersen, Moritz Schubotz |  |
| 1889 |  |  [Query-Efficient Black-Box Red Teaming via Bayesian Optimization](https://doi.org/10.18653/v1/2023.acl-long.646) |  | 0 | The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of... | Deokjae Lee, Hwaran Lee, Hyun Oh Song, JinHwa Kim, JunYeong Lee, JungWoo Ha, SangWoo Lee |  |
| 1890 |  |  [SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control](https://doi.org/10.18653/v1/2023.acl-long.647) |  | 0 | Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design... | Sachin Kumar, Xiaochuang Han, Yulia Tsvetkov |  |
| 1891 |  |  [Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing](https://doi.org/10.18653/v1/2023.acl-long.648) |  | 0 | Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g., president, politician) of a given entity mention (e.g., Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and... | Chengyue Jiang, Kewei Tu, Pengjun Xie, Wenyang Hui, Xiaobin Wang, Yong Jiang |  |
| 1892 |  |  [MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.649) |  | 0 | Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation... | Chen Chen, Eng Siong Chng, Heqing Zou, Ruizhe Li, Yuchen Hu |  |
| 1893 |  |  [Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors](https://doi.org/10.18653/v1/2023.acl-long.650) |  | 0 | The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems’ outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated... | Alexander R. Fabbri, Greg Durrett, Jiacheng Xu, Justin F. Rousseau, Liyan Tang, Philippe Laban, Semih Yavuz, Tanya Goyal, Wojciech Kryscinski |  |
| 1894 |  |  [GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding](https://doi.org/10.18653/v1/2023.acl-long.651) |  | 0 | Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of... | Cong Liu, Guoping Hu, JiaChen Gu, Quan Liu, Zhenhua Ling |  |
| 1895 |  |  [Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks](https://doi.org/10.18653/v1/2023.acl-long.652) |  | 0 | Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be... | Akim Tsvigun, Alexander Panchenko, Artem Shelmanov, Artem Vazhentsev, Gleb Kuzmin, Maxim Panov, Mikhail Burtsev |  |
| 1896 |  |  [BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting](https://doi.org/10.18653/v1/2023.acl-long.653) |  | 0 | The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In... | Ahmed Baruwa, Alham Fikri Aji, David Ifeoluwa Adelani, Dragomir Radev, Edward Raff, Genta Indra Winata, Hailey Schoelkopf, Jungo Kasai, Khalid Almubarak, Lintang Sutawika, M. Saiful Bari, Niklas Muennighoff, Stella Biderman, Vassilina Nikoulina, Zheng Xin Yong |  |
| 1897 |  |  [Logic-driven Indirect Supervision: An Application to Crisis Counseling](https://doi.org/10.18653/v1/2023.acl-long.654) |  | 0 | Ensuring the effectiveness of text-based crisis counseling requires observing ongoing conversations and providing feedback, both labor-intensive tasks. Automatic analysis of conversations—at the full chat and utterance levels—may help support counselors and provide better care. While some... | Brent Kious, Katherine Axford, Mattia Medina Grespan, Meghan Broadbent, Vivek Srikumar, Xinyao Zhang, Zac E. Imel |  |
| 1898 |  |  [Grounding Characters and Places in Narrative Text](https://doi.org/10.18653/v1/2023.acl-long.655) |  | 0 | Tracking characters and locations throughout a story can help improve the understanding of its plot structure. Prior research has analyzed characters and locations from text independently without grounding characters to their locations in narrative time. Here, we address this gap by proposing a new... | Amanpreet Sihra, David Bamman, Elizabeth F. Evans, Matthew Wilkens, Sandeep Soni |  |
| 1899 |  |  [From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models](https://doi.org/10.18653/v1/2023.acl-long.656) |  | 0 | Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our... | Chan Young Park, Shangbin Feng, Yuhan Liu, Yulia Tsvetkov |  |
| 1900 |  |  [SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT](https://doi.org/10.18653/v1/2023.acl-long.657) |  | 0 | Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or... | Aditya Yadavalli, Alekhya Yadavalli, Vera Tobin |  |
| 1901 |  |  [Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.658) |  | 0 | In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on... | Albert Xu, Robin Jia, Xiang Ren |  |
| 1902 |  |  [Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?](https://doi.org/10.18653/v1/2023.acl-long.659) |  | 0 | Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt... | Chengwei Qin, Qian Li, Ruochen Zhao, Shafiq R. Joty |  |
| 1903 |  |  [Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale](https://doi.org/10.18653/v1/2023.acl-long.660) |  | 0 | Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all... | Dan Roth, Hritik Bansal, Karthik Gopalakrishnan, Katrin Kirchhoff, Saket Dingliwal, Sravan Bodapati |  |
| 1904 |  |  [Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya](https://doi.org/10.18653/v1/2023.acl-long.661) |  | 0 | Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated... | Fitsum Gaim, Hancheol Park, Jong Park, Wonsuk Yang |  |
| 1905 |  |  [ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain](https://doi.org/10.18653/v1/2023.acl-long.662) |  | 0 | The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some... | Barbara Plank, Mike Zhang, Rob van der Goot |  |
| 1906 |  |  [CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval](https://doi.org/10.18653/v1/2023.acl-long.663) |  | 0 | Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on various retrieval tasks. These methods, however, are orders of magnitude slower and need much more space to store their indices compared to their... | Asish Ghoshal, Barlas Oguz, Jimmy Lin, Minghan Li, ShengChieh Lin, Wentau Yih, Xilun Chen, Yashar Mehdad |  |
| 1907 |  |  [MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning](https://doi.org/10.18653/v1/2023.acl-long.664) |  | 0 | Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and... | Bang Yang, Fenglin Liu, Xian Wu, Xu Sun, Yaowei Wang, Yuexian Zou |  |
| 1908 |  |  [Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge](https://doi.org/10.18653/v1/2023.acl-long.665) |  | 0 | While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks – when the class label is very infrequent (e.g. < 5% of samples). Active learning has in general been proposed to alleviate such challenges,... | Christian C. Luhmann, H. Andrew Schwartz, Jonah Luby, Swanie Juhng, Syeda Mahwish, Vasudha Varadarajan, Xiaoran Liu |  |
| 1909 |  |  [In-sample Curriculum Learning by Sequence Completion for Natural Language Generation](https://doi.org/10.18653/v1/2023.acl-long.666) |  | 0 | Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by... | Haifeng Tang, Kenny Q. Zhu, Qi Jia, Yizhu Liu |  |
| 1910 |  |  [Product Question Answering in E-Commerce: A Survey](https://doi.org/10.18653/v1/2023.acl-long.667) |  | 0 | Product question answering (PQA), aiming to automatically provide instant responses to customer’s questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of... | Qian Yu, Wai Lam, Wenxuan Zhang, Yang Deng |  |
| 1911 |  |  [Towards Domain-Agnostic and Domain-Adaptive Dementia Detection from Spoken Language](https://doi.org/10.18653/v1/2023.acl-long.668) |  | 0 | Health-related speech datasets are often small and varied in focus. This makes it difficult to leverage them to effectively support healthcare goals. Robust transfer of linguistic features across different datasets orbiting the same goal carries potential to address this concern. To test this... | Natalie Parde, Shahla Farzana |  |
| 1912 |  |  [Generalizing Backpropagation for Gradient-Based Interpretability](https://doi.org/10.18653/v1/2023.acl-long.669) |  | 0 | Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model’s output with respect to its inputs. While these methods can indicate which input features may be important for the model’s prediction, they reveal little about the inner... | Alex Warstadt, Kevin Du, Lucas Torroba Hennigen, Niklas Stoehr, Ryan Cotterell |  |
| 1913 |  |  [UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language](https://doi.org/10.18653/v1/2023.acl-long.670) |  | 0 | Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training... | Qi Zhang, Xinyi Mou, Xuanjing Huang, Zhongyu Wei |  |
| 1914 |  |  [Generic Temporal Reasoning with Differential Analysis and Explanation](https://doi.org/10.18653/v1/2023.acl-long.671) |  | 0 | Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task... | Ben Zhou, Dan Roth, Haoyu Wang, Helen Jin, Yu Feng |  |
| 1915 |  |  [Model-Based Simulation for Optimising Smart Reply](https://doi.org/10.18653/v1/2023.acl-long.672) |  | 0 | Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of... | Benjamin Towle, Ke Zhou |  |
| 1916 |  |  [Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval](https://doi.org/10.18653/v1/2023.acl-long.673) |  | 0 | Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve... | Graham Neubig, John Wieting, Jonathan H. Clark, Taylor BergKirkpatrick, William W. Cohen |  |
| 1917 |  |  [On the Blind Spots of Model-Based Evaluation Metrics for Text Generation](https://doi.org/10.18653/v1/2023.acl-long.674) |  | 0 | In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric... | James R. Glass, Jingyu Zhang, Kyunghyun Cho, Sachin Kumar, Tianle Wang, Tianxing He, Yulia Tsvetkov |  |
| 1918 |  |  [Dealing with Semantic Underspecification in Multimodal NLP](https://doi.org/10.18653/v1/2023.acl-long.675) |  | 0 | Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the... | Sandro Pezzelle |  |
| 1919 |  |  [Trigger Warning Assignment as a Multi-Label Document Classification Problem](https://doi.org/10.18653/v1/2023.acl-long.676) |  | 0 | A trigger warning is used to warn people about potentially disturbing content. We introduce trigger warning assignment as a multi-label classification task, create the Webis Trigger Warning Corpus 2022, and with it the first dataset of 1 million fanfiction works from Archive of our Own with up to... | Benno Stein, Christopher Schröder, Magdalena Wolska, Martin Potthast, Matti Wiegmann, Ole Borchardt |  |
| 1920 |  |  [WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.677) |  | 0 | This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push... | Linchao Zhu, Wenjie Zhuo, Xiaohan Wang, Yi Yang, Yifan Sun |  |
| 1921 |  |  [Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms](https://doi.org/10.18653/v1/2023.acl-long.678) |  | 0 | This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have... | Changchang Liu, Huan Sun, Tianshu Zhang, WeiHan Lee, Yu Su |  |
| 1922 |  |  [Causality-Guided Multi-Memory Interaction Network for Multivariate Stock Price Movement Prediction](https://doi.org/10.18653/v1/2023.acl-long.679) |  | 0 | Over the past few years, we’ve witnessed an enormous interest in stock price movement prediction using AI techniques. In recent literature, auxiliary data has been used to improve prediction accuracy, such as textual news. When predicting a particular stock, we assume that information from other... | Di Luo, Rui Yan, Shuqi Li, Weiheng Liao, Xin Cheng |  |
| 1923 |  |  [DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization](https://doi.org/10.18653/v1/2023.acl-long.680) |  | 0 | Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these... | Jin Ma, Qi Zhang, Shihan Dou, Songyang Gao, Xiao Wang, Yan Liu, Ying Shan, Zhongyu Wei |  |
| 1924 |  |  [A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires](https://doi.org/10.18653/v1/2023.acl-long.681) |  | 0 | Social media is one of the most highly sought resources for analyzing characteristics of the language by its users. In particular, many researchers utilized various linguistic features of mental health problems from social media. However, existing approaches to detecting mental disorders face... | Hoyun Song, Huije Lee, Jisu Shin, Jong Park |  |
| 1925 |  |  [Downstream Datasets Make Surprisingly Good Pretraining Corpora](https://doi.org/10.18653/v1/2023.acl-long.682) |  | 0 | For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gainsare attributable to the massive background corpora... | Jeffrey P. Bigham, Kundan Krishna, Saurabh Garg, Zachary C. Lipton |  |
| 1926 |  |  [Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach](https://doi.org/10.18653/v1/2023.acl-long.683) |  | 0 | We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand... | Chenwei Zhang, Jingbo Shang, Jinho D. Choi, Liyan Xu, Xian Li |  |
| 1927 |  |  [XDailyDialog: A Multilingual Parallel Dialogue Corpus](https://doi.org/10.18653/v1/2023.acl-long.684) |  | 0 | High-quality datasets are significant to the development of dialogue models. However, most existing datasets for open-domain dialogue modeling are limited to a single language. The absence of multilingual open-domain dialog datasets not only limits the research on multilingual or cross-lingual... | Haifeng Wang, Jie Cai, Kaiping Peng, Mrinmaya Sachan, Peng Zhang, Ping Nie, Zeming Liu, ZhengYu Niu |  |
| 1928 |  |  [PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent](https://doi.org/10.18653/v1/2023.acl-long.685) |  | 0 | The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased... | Asif Ekbal, Kshitij Mishra, Priyanshu Priya |  |
| 1929 |  |  [Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.686) |  | 0 | Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling... | Lidong Bing, Sinno Jialin Pan, Wenxuan Zhang, Yue Deng |  |
| 1930 |  |  [Contrastive Decoding: Open-ended Text Generation as Optimization](https://doi.org/10.18653/v1/2023.acl-long.687) |  | 0 | Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a... | Ari Holtzman, Daniel Fried, Jason Eisner, Luke Zettlemoyer, Mike Lewis, Percy Liang, Tatsunori Hashimoto, Xiang Lisa Li |  |
| 1931 |  |  [Resolving Indirect Referring Expressions for Entity Selection](https://doi.org/10.18653/v1/2023.acl-long.688) |  | 0 | Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real... | Annie Louis, Filip Radlinski, Mohammad Javad Hosseini, Silvia Pareti |  |
| 1932 |  |  [Accelerating Transformer Inference for Translation via Parallel Decoding](https://doi.org/10.18653/v1/2023.acl-long.689) |  | 0 | Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the... | Andrea Santilli, Emanuele Rodolà, Emilian Postolache, Michele Mancusi, Riccardo Marin, Silvio Severino, Valentino Maiorca |  |
| 1933 |  |  [Hard Sample Aware Prompt-Tuning](https://doi.org/10.18653/v1/2023.acl-long.690) |  | 0 | Prompt-tuning based few-shot learning has garnered increasing attention in recent years due to its efficiency and promising capability. To achieve the best performance for NLP tasks with just a few samples, it is vital to include as many informative samples as possible and to avoid misleading ones.... | Jiahuan Zhang, Peng Li, Qi An, Yuanjian Xu, Zaiqing Nie |  |
| 1934 |  |  [WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events](https://doi.org/10.18653/v1/2023.acl-long.691) |  | 0 | Biographical event detection is a relevant task that allows for the exploration and comparison of the ways in which people’s lives are told and represented. This may support several real-life applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite... | Daniele Paolo Radicioni, Enrico Mensa, Marco Antonio Stranisci, Rossana Damiano, Tommaso Caselli, Viviana Patti |  |
| 1935 |  |  [Best-k Search Algorithm for Neural Text Generation](https://doi.org/10.18653/v1/2023.acl-long.692) |  | 0 | Modern natural language generation paradigms require a decoding strategy to obtain quality sequences out of the model. Beam search yields high-quality but low diversity outputs; stochastic approaches suffer from high variance and sometimes low quality. In this work, we propose a deterministic... | Caiming Xiong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou |  |
| 1936 |  |  [Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages](https://doi.org/10.18653/v1/2023.acl-long.693) |  | 0 | Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i)... | Anoop Kunchukuttan, Gowtham Ramesh, Mitesh M. Khapra, Pratyush Kumar, Rahul Aralikatte, Shreya Goyal, Sumanth Doddapaneni |  |
| 1937 |  |  [Transforming Visual Scene Graphs to Image Captions](https://doi.org/10.18653/v1/2023.acl-long.694) |  | 0 | We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words... | Chenliang Li, Fei Huang, Haiyang Xu, Jiawei Peng, Qinghao Ye, Songfang Huang, Xu Yang, Yu Zhang, Zhangzikang Li, Zihua Wang |  |
| 1938 |  |  [Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks](https://doi.org/10.18653/v1/2023.acl-long.695) |  | 0 | Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution... | Anna Y. Sun, Hirofumi Inaguma, Juan Pino, Ning Dong, Paden Tomasello, Xinyue Chen, Xutai Ma, Yun Tang |  |
| 1939 |  |  [Improving Domain Generalization for Prompt-Aware Essay Scoring via Disentangled Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.696) |  | 0 | Automated Essay Scoring (AES) aims to score essays written in response to specific prompts. Many AES models have been proposed, but most of them are either prompt-specific or prompt-adaptive and cannot generalize well on “unseen” prompts. This work focuses on improving the generalization ability of... | Hua Yu, Meng Liu, Qing Gu, Tianyi Gao, Yafeng Yin, Zhiwei Jiang, Zifeng Cheng |  |
| 1940 |  |  [What's the Meaning of Superhuman Performance in Today's NLU?](https://doi.org/10.18653/v1/2023.acl-long.697) |  | 0 | In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension.... | Alexander Koller, Daniel Hershcovich, Eduard H. Hovy, Ekaterina Shutova, Jan Hajic, Johan Bos, Rico Sennrich, Roberto Navigli, Simon Krek, Simone Tedeschi, Steven Schockaert, Thierry Declerck |  |
| 1941 |  |  [PromptNER: Prompt Locating and Typing for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.698) |  | 0 | Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict... | Rongsheng Zhang, Shuhui Wu, Weiming Lu, Wenqi Zhang, Yadong Xi, Yongliang Shen, Yueting Zhuang, Zeqi Tan |  |
| 1942 |  |  [Hints on the data for language modeling of synthetic languages with transformers](https://doi.org/10.18653/v1/2023.acl-long.699) |  | 0 | Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The... | Núria Bel, Rodolfo Zevallos |  |
| 1943 |  |  [Neural Machine Translation Methods for Translating Text to Sign Language Glosses](https://doi.org/10.18653/v1/2023.acl-long.700) |  | 0 | State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine... | Dele Zhu, Eleftherios Avramidis, Vera Czehmann |  |
| 1944 |  |  [Revisiting Event Argument Extraction: Can EAE Models Learn Better When Being Aware of Event Co-occurrences?](https://doi.org/10.18653/v1/2023.acl-long.701) |  | 0 | Event co-occurrences have been proved effective for event extraction (EE) in previous studies, but have not been considered for event argument extraction (EAE) recently. In this paper, we try to fill this gap between EE research and EAE research, by highlighting the question that “Can EAE models... | Buzhou Tang, Jingyue Hu, Yuxin He |  |
| 1945 |  |  [HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation](https://doi.org/10.18653/v1/2023.acl-long.702) |  | 0 | Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into... | Jiaqing Liang, Qianyu He, Yanghua Xiao, Yikai Zhang, Yuncheng Huang, Yunwen Chen |  |
| 1946 |  |  [Large-scale Lifelong Learning of In-context Instructions and How to Tackle It](https://doi.org/10.18653/v1/2023.acl-long.703) |  | 0 | Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the... | Jaeyoung Do, Jisoo Mok, Seunghak Yu, Sungjin Lee, Sungroh Yoon, Tara Taghavi |  |
| 1947 |  |  [Controllable Text Generation via Probability Density Estimation in the Latent Space](https://doi.org/10.18653/v1/2023.acl-long.704) |  | 0 | Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-specific classifiers or sampling one from relevant discrete samples. However, they cannot effectively model a complex space with diverse... | Bing Qin, Heng Gong, Lingyuan Zhang, Sicheng Ma, Weihong Zhong, Xiaocheng Feng, Yuxuan Gu |  |
| 1948 |  |  [Learning Latent Relations for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2023.acl-long.705) |  | 0 | Temporal Knowledge Graph (TKG) reasoning aims to predict future facts based on historical data. However, due to the limitations in construction tools and data sources, many important associations between entities may be omitted in TKG. We refer to these missing associations as latent relations.... | Liang Wang, Mengqi Zhang, Qiang Liu, Shu Wu, Yuwei Xia |  |
| 1949 |  |  [DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function](https://doi.org/10.18653/v1/2023.acl-long.706) |  | 0 | Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model.... | Enze Xie, Haiming Wang, Han Shi, Jian Yin, Jianhao Shen, Jing Xiong, Lin Li, Xiaodan Liang, Ye Yuan, Yichun Yin, Yujun Li, Zhenguo Li, Zhengying Liu |  |
| 1950 |  |  [Unsupervised Selective Rationalization with Noise Injection](https://doi.org/10.18653/v1/2023.acl-long.707) |  | 0 | A major issue with using deep learning models in sensitive applications is that they provide no explanation for their output. To address this problem, unsupervised selective rationalization produces rationales alongside predictions by chaining two jointly-trained components, a rationale generator... | Adam Storek, Kathleen R. McKeown, Melanie Subbiah |  |
| 1951 |  |  [Understanding In-Context Learning via Supportive Pretraining Data](https://doi.org/10.18653/v1/2023.acl-long.708) |  | 0 | In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work... | Asli Celikyilmaz, Daniel Simig, Tianlu Wang, Todor Mihaylov, Xiaochuang Han, Yulia Tsvetkov |  |
| 1952 |  |  [ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation](https://doi.org/10.18653/v1/2023.acl-long.709) |  | 0 | Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named... | Jiaxin Wen, Minlie Huang, Zhexin Zhang |  |
| 1953 |  |  [Effective Contrastive Weighting for Dense Query Expansion](https://doi.org/10.18653/v1/2023.acl-long.710) |  | 0 | Verbatim queries submitted to search engines often do not sufficiently describe the user’s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query’srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval... | Craig Macdonald, Iadh Ounis, Sean MacAvaney, Xiao Wang |  |
| 1954 |  |  [Improving the Detection of Multilingual Online Attacks with Rich Social Media Data from Singapore](https://doi.org/10.18653/v1/2023.acl-long.711) |  | 0 | Toxic content is a global problem, but most resources for detecting toxic content are in English. When datasets are created in other languages, they often focus exclusively on one language or dialect. In many cultural and geographical settings, however, it is common to code-mix languages, combining... | Bertie Vidgen, Janosch Haber, Matthew Chapman, Paul Röttger, Roy KaWei Lee, Vibhor Agarwal, Yong Keong Yap |  |
| 1955 |  |  [Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model](https://doi.org/10.18653/v1/2023.acl-long.712) |  | 0 | We use both Bayesian and neural models to dissect a data set of Chinese learners’ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions... | Jakob Prange, Man Ho Ivy Wong |  |
| 1956 |  |  [Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization](https://doi.org/10.18653/v1/2023.acl-long.713) |  | 0 | In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to... | Alexander R. Fabbri, Artidoro Pagnoni, ChienSheng Wu, Wojciech Kryscinski |  |
| 1957 |  |  [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://doi.org/10.18653/v1/2023.acl-long.714) |  | 0 | Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities in... | Chenxi Pang, Fangyu Liu, Francesco Piccinno, Julian Martin Eisenschlos, Kenton Lee, Mandar Joshi, Nigel Collier, Syrine Krichene, Yasemin Altun |  |
| 1958 |  |  [MGR: Multi-generator Based Rationalization](https://doi.org/10.18653/v1/2023.acl-long.715) |  | 0 | Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation... | Haozhao Wang, Jun Wang, Ruixuan Li, Wei Liu, Xinyang Li, Yang Qiu, Yuankai Zhang |  |
| 1959 |  |  [BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics](https://doi.org/10.18653/v1/2023.acl-long.716) |  | 0 | The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are:... | Alejandro Jaimes, Di Lu, Joel R. Tetreault, Ke Zhang, Liang Ma, Robert L. Logan IV, Shihao Ran, Shuyang Cao |  |
| 1960 |  |  [Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection](https://doi.org/10.18653/v1/2023.acl-long.717) |  | 0 | Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of... | Junjie Hu, Rheeya Uppaal, Yixuan Li |  |
| 1961 |  |  [UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization](https://doi.org/10.18653/v1/2023.acl-long.718) |  | 0 | The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable... | Chenguang Zhu, Michael Zeng, Ruochen Xu, Yang Liu, Yue Zhang, Yulong Chen, Ziyi Yang |  |
| 1962 |  |  [RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue](https://doi.org/10.18653/v1/2023.acl-long.719) |  | 0 | Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and... | Pengjie Ren, Shuo Zhang, Weiwei Sun, Zhaochun Ren, Zhen Zhang, Zhengliang Shi |  |
| 1963 |  |  [An AMR-based Link Prediction Approach for Document-level Event Argument Extraction](https://doi.org/10.18653/v1/2023.acl-long.720) |  | 0 | Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only... | Qipeng Guo, Xiangkun Hu, Xipeng Qiu, Yue Zhang, Yuqing Yang, Zheng Zhang |  |
| 1964 |  |  [PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://doi.org/10.18653/v1/2023.acl-long.721) |  | 0 | Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer:... | Bhargavi Paranjape, Hannaneh Hajishirzi, Qingqing Cao |  |
| 1965 |  |  [Gloss-Free End-to-End Sign Language Translation](https://doi.org/10.18653/v1/2023.acl-long.722) |  | 0 | In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation... | Bang Zhang, Ke Sun, Kezhou Lin, Linchao Zhu, Xiaohan Wang, Yi Yang |  |
| 1966 |  |  [TAGPRIME: A Unified Framework for Relational Structure Extraction](https://doi.org/10.18653/v1/2023.acl-long.723) |  | 0 | Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less... | IHung Hsu, KaiWei Chang, KuanHao Huang, Nanyun Peng, Prem Natarajan, Shuning Zhang, Wenxin Cheng |  |
| 1967 |  |  [Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers](https://doi.org/10.18653/v1/2023.acl-long.724) |  | 0 | This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key... | Alvin Cheung, Chenyan Xiong, Jianfeng Gao, Linyuan Gong, Payal Bajaj, Xia Song, Xiaodong Liu, Yiqing Xie |  |
| 1968 |  |  [BITE: Textual Backdoor Attacks with Iterative Trigger Injection](https://doi.org/10.18653/v1/2023.acl-long.725) |  | 0 | Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a “backdoor” into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the... | Jun Yan, Vansh Gupta, Xiang Ren |  |
| 1969 |  |  [A Crosslingual Investigation of Conceptualization in 1335 Languages](https://doi.org/10.18653/v1/2023.acl-long.726) |  | 0 | Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we... | Haotian Ye, Hinrich Schütze, Leonie Weissweiler, Philipp Wicke, Renhao Pei, Robert Zangenfeind, Yihong Liu |  |
| 1970 |  |  [Exploring and Verbalizing Academic Ideas by Concept Co-occurrence](https://doi.org/10.18653/v1/2023.acl-long.727) |  | 0 | Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept... | Bo Xue, Chenghu Zhou, Luoyi Fu, Shuqian Sheng, Xinbing Wang, Yi Xu |  |
| 1971 |  |  [mCLIP: Multilingual CLIP via Cross-lingual Transfer](https://doi.org/10.18653/v1/2023.acl-long.728) |  | 0 | Large-scale vision-language pretrained (VLP) models like CLIP have shown remarkable performance on various downstream cross-modal tasks. However, they are usually biased towards English due to the lack of sufficient non-English image-text pairs. Existing multilingual VLP methods often learn... | Guanhua Chen, Jia Pan, Lifeng Shang, Lu Hou, Qun Liu, Wenliang Dai, Wenping Wang, Xin Jiang, Yun Chen |  |
| 1972 |  |  [Distantly Supervised Course Concept Extraction in MOOCs with Academic Discipline](https://doi.org/10.18653/v1/2023.acl-long.729) |  | 0 | With the rapid growth of Massive Open Online Courses (MOOCs), it is expensive and time-consuming to extract high-quality knowledgeable concepts taught in the course by human effort to help learners grasp the essence of the course. In this paper, we propose to automatically extract course concepts... | Jifan Yu, Juanzi Li, Lei Hou, Mengying Lu, Yexing Du, Yuquan Wang |  |
| 1973 |  |  [Extrinsic Evaluation of Machine Translation Metrics](https://doi.org/10.18653/v1/2023.acl-long.730) |  | 0 | Automatic machine translation (MT) metrics are widely used to distinguish the quality of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the... | Alexandra Birch, Mark Steedman, Nikita Moghe, Tom Sherborne |  |
| 1974 |  |  [ExplainMeetSum: A Dataset for Explainable Meeting Summarization Aligned with Human Intent](https://doi.org/10.18653/v1/2023.acl-long.731) |  | 0 | To enhance the explainability of meeting summarization, we construct a new dataset called “ExplainMeetSum,” an augmented version of QMSum, by newly annotating evidence sentences that faithfully “explain” a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization,... | Hyun Kim, Minsoo Cho, SeungHoon Na |  |
| 1975 |  |  [A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation](https://doi.org/10.18653/v1/2023.acl-long.732) |  | 0 | Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction... | Xiaoheng Zhang, Yang Li |  |
| 1976 |  |  [CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning](https://doi.org/10.18653/v1/2023.acl-long.733) |  | 0 | Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed”... | Baixuan Xu, Chun Yi Louis Bo, Lei Chen, Tianqing Fang, Weiqi Wang, Yangqiu Song |  |
| 1977 |  |  [The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research](https://doi.org/10.18653/v1/2023.acl-long.734) |  | 0 | Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the... | Aurélie Névéol, Fanny Ducel, Jan Philip Wahle, Karën Fort, Mohamed Abdalla, Saif M. Mohammad, Terry Lima Ruas |  |
| 1978 |  |  [Language of Bargaining](https://doi.org/10.18653/v1/2023.acl-long.735) |  | 0 | Leveraging an established exercise in negotiation education, we build a novel dataset for studying how the use of language shapes bilateral bargaining. Our dataset extends existing work in two ways: 1) we recruit participants via behavioral labs instead of crowdsourcing platforms and allow... | Alexander Zentefis, Chenhao Tan, Mourad Heddaya, Rob Voigt, Solomon Dworkin |  |
| 1979 |  |  [Do Question Answering Modeling Improvements Hold Across Benchmarks?](https://doi.org/10.18653/v1/2023.acl-long.736) |  | 0 | Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence—two benchmarks have high concurrence on a set of modeling approaches... | Nelson F. Liu, Percy Liang, Robin Jia, Tony Lee |  |
| 1980 |  |  [VLN-Trans: Translator for the Vision and Language Navigation Agent](https://doi.org/10.18653/v1/2023.acl-long.737) |  | 0 | Language understanding is essential for the navigation agent to follow instructions. We observe two kinds of issues in the instructions that can make the navigation task challenging: 1. The mentioned landmarks are not recognizable by the navigation agent due to the different vision abilities of the... | Parisa Kordjamshidi, Yue Zhang |  |
| 1981 |  |  [Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.738) |  | 0 | Conventional knowledge distillation (KD) methods require access to the internal information of teachers, e.g., logits. However, such information may not always be accessible for large pre-trained language models (PLMs). In this work, we focus on decision-based KD for PLMs, where only teacher... | Peng Li, Qinhong Zhou, Yang Liu, Zonghan Yang |  |
| 1982 |  |  [Continual Contrastive Finetuning Improves Low-Resource Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.739) |  | 0 | Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity... | Hoifung Poon, Muhao Chen, Sheng Zhang, Tristan Naumann, Wenxuan Zhou |  |
| 1983 |  |  [KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment](https://doi.org/10.18653/v1/2023.acl-long.740) |  | 0 | Recent legislation of the “right to be forgotten” has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer... | Hongzhi Yin, KamFai Wong, Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng |  |
| 1984 |  |  [UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language](https://doi.org/10.18653/v1/2023.acl-long.741) |  | 0 | Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary,... | Bing Qin, Chi Liu, Haochun Wang, Nuwa Xi, Sendong Zhao, Ting Liu |  |
| 1985 |  |  [Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths](https://doi.org/10.18653/v1/2023.acl-long.742) |  | 0 | ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., head event, relation, tail event. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite... | Rui Xia, Siwei Wu, Xiangqing Shen |  |
| 1986 |  |  [Shrinking Embeddings for Hyper-Relational Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.743) |  | 0 | Link prediction on knowledge graphs (KGs) has been extensively studied on binary relational KGs, wherein each fact is represented by a triple. A significant amount of important knowledge, however, is represented by hyper-relational facts where each fact is composed of a primal triple and a set of... | Bo Xiong, Mojtaba Nayyeri, Shirui Pan, Steffen Staab |  |
| 1987 |  |  [CTC-based Non-autoregressive Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.744) |  | 0 | Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for... | Anxiang Ma, Chen Xu, Jingbo Zhu, Mingxuan Wang, Murun Yang, Qianqian Dong, Qingxuan Sun, Tom Ko, Tong Xiao, Xiaoqian Liu, Xiaowen Liu, Yuhao Zhang |  |
| 1988 |  |  [Attention as a Guide for Simultaneous Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.745) |  | 0 | In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDAtt (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns... | Marco Turchi, Matteo Negri, Sara Papi |  |
| 1989 |  |  [On Complementarity Objectives for Hybrid Retrieval](https://doi.org/10.18653/v1/2023.acl-long.746) |  | 0 | Dense retrieval has shown promising results in various information retrieval tasks, and hybrid retrieval, combined with the strength of sparse retrieval, has also been actively studied. A key challenge in hybrid retrieval is to make sparse and dense complementary to each other. Existing models have... | Dohyeon Lee, Kyungjae Lee, Seungtaek Choi, Seungwon Hwang, Sunghyun Park |  |
| 1990 |  |  [C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2023.acl-long.747) |  | 0 | Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor of, against, or neutral toward a target that is unseen during training. Despite the growing attention on ZSSD, most recent advances in this task are limited to English and do not pay much attention to other... | Chenye Zhao, Cornelia Caragea, Yingjie Li |  |
| 1991 |  |  [Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.748) |  | 0 | Unsupervised pre-training on millions of digital-born or scanned documents has shown promising advances in visual document understanding (VDU). While various vision-language pre-training objectives are studied in existing solutions, the document textline, as an intrinsic granularity in VDU, has... | Haoli Bai, Jiansheng Wei, Liangwei Wang, Lu Hou, Nian Xie, Qun Liu, Rongfu Zheng, Shuang Liu, Wentao Li, Xiaojun Meng, Xin Jiang, Yifeng Luo, Zhiguang Liu |  |
| 1992 |  |  [PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts](https://doi.org/10.18653/v1/2023.acl-long.749) |  | 0 | Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce... | Binyuan Hui, Fei Huang, Min Yang, Yongbin Li, Yunshui Li, Zhichao Yin |  |
| 1993 |  |  [MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning](https://doi.org/10.18653/v1/2023.acl-long.750) |  | 0 | Recent advances in pre-trained language models (PLMs) have facilitated the development ofcommonsense reasoning tasks. However, existing methods rely on multi-hop knowledgeretrieval and thus suffer low accuracy due toembedded noise in the acquired knowledge. In addition, these methods often attain... | Jiaxing Zhang, Lin Zhang, Liwei Wang, Ruyi Gan, Yanyang Li, Yichong Xu, Yongfeng Huang |  |
| 1994 |  |  [PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation](https://doi.org/10.18653/v1/2023.acl-long.751) |  | 0 | Image translation is a task that translates an image containing text in the source language to the target language. One major challenge with image translation is the modality gap between visual text inputs and textual inputs/outputs of machine translation (MT). In this paper, we propose PEIT, an... | Deyi Xiong, Shangjie Li, Shaolin Zhu, Yikun Lei |  |
| 1995 |  |  [Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection](https://doi.org/10.18653/v1/2023.acl-long.752) |  | 0 | The task of Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies... | Arnav Arora, Erik Arakelyan, Isabelle Augenstein |  |
| 1996 |  |  [DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles](https://doi.org/10.18653/v1/2023.acl-long.753) |  | 0 | A crucial component in the curation of KB for a scientific domain (e.g., materials science, food & nutrition, fuels) is information extraction from tables in the domain’s published research articles. To facilitate research in this direction, we define a novel NLP task of extracting compositions of... | Devanshi Khatsuriya, Kausik Hira, Mausam, Mohd Zaki, N. M. Anoop Krishnan, Tanishq Gupta |  |
| 1997 |  |  [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://doi.org/10.18653/v1/2023.acl-long.754) |  | 0 | Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity,... | Alisa Liu, Daniel Khashabi, Hannaneh Hajishirzi, Noah A. Smith, Swaroop Mishra, Yeganeh Kordi, Yizhong Wang |  |
| 1998 |  |  [Disentangled Phonetic Representation for Chinese Spelling Correction](https://doi.org/10.18653/v1/2023.acl-long.755) |  | 0 | Chinese Spelling Correction (CSC) aims to detect and correct erroneous characters in Chinese texts. Although efforts have been made to introduce phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic representations with character representations, which tends to weaken the... | Qifan Wang, Xiaojun Quan, Zihong Liang |  |
| 1999 |  |  [Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis](https://doi.org/10.18653/v1/2023.acl-long.756) |  | 0 | Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis... | Alexander Rudnicky, Peter J. Ramadge, TaChung Chi, TingHan Fan |  |
| 2000 |  |  [CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models](https://doi.org/10.18653/v1/2023.acl-long.757) |  | 0 | redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of... | Jiaxu Zhao, Ling Chen, Meng Fang, Mykola Pechenizkiy, Yitong Li, Zijing Shi |  |
| 2001 |  |  [Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback](https://doi.org/10.18653/v1/2023.acl-long.758) |  | 0 | Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In... | Jason Weston, Jing Xu, Kushal Arora, Megan Ung, Mojtaba Komeili, YLan Boureau |  |
| 2002 |  |  [Uncovering and Categorizing Social Biases in Text-to-SQL](https://doi.org/10.18653/v1/2023.acl-long.759) |  | 0 | Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where... | Elliott Ash, JianGuang Lou, Xiaokang Chen, Yan Gao, Yan Liu, Zhe Su |  |
| 2003 |  |  [On the Compositional Generalization in Versatile Open-domain Dialogue](https://doi.org/10.18653/v1/2023.acl-long.760) |  | 0 | Previous research has demonstrated the potential of multi-task learning to foster a conversational agent’s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse... | Lemao Liu, Rui Yan, Tingchen Fu, Xueliang Zhao |  |
| 2004 |  |  [What is the Real Intention behind this Question? Dataset Collection and Intention Classification](https://doi.org/10.18653/v1/2023.acl-long.761) |  | 0 | Asking and answering questions are inseparable parts of human social life. The primary purposes of asking questions are to gain knowledge or request help which has been the subject of question-answering studies. However, questions can also reflect negative intentions and include implicit offenses,... | Kourosh Meshgi, Maryam Sadat Mirzaei, Satoshi Sekine |  |
| 2005 |  |  [Conjunct Resolution in the Face of Verbal Omissions](https://doi.org/10.18653/v1/2023.acl-long.762) |  | 0 | Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of... | Reut Tsarfaty, Royi Rassin, Yoav Goldberg |  |
| 2006 |  |  [Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts](https://doi.org/10.18653/v1/2023.acl-long.763) |  | 0 | Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse... | Andrea Madotto, Heather Foran, Jing Xu, Megan Ung, Mounica Maddela, YLan Boureau |  |
| 2007 |  |  [Learning In-context Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2023.acl-long.764) |  | 0 | Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject... | Boxi Cao, Dai Dai, Hongyu Lin, Hua Wu, Jiawei Chen, Jie Lou, Le Sun, Wei Jia, Xianpei Han, Yaojie Lu |  |
| 2008 |  |  [Holistic Prediction on a Time-Evolving Attributed Graph](https://doi.org/10.18653/v1/2023.acl-long.765) |  | 0 | Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and... | Makoto Onizuka, Panagiotis Karras, Shohei Yamasaki, Yuya Sasaki |  |
| 2009 |  |  [Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional Random Field](https://doi.org/10.18653/v1/2023.acl-long.766) |  | 0 | Prior works on joint Information Extraction (IE) typically model instance (e.g., event triggers, entities, roles, relations) interactions by representation enhancement, type dependencies scoring, or global decoding. We find that the previous models generally consider binary type dependency scoring... | Kewei Tu, Wenjuan Han, Zhaohui Yan, Zilong Zheng, Zixia Jia |  |
| 2010 |  |  [Training Trajectories of Language Models Across Scales](https://doi.org/10.18653/v1/2023.acl-long.767) |  | 0 | Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In... | Chunting Zhou, Danqi Chen, Luke Zettlemoyer, Mengzhou Xia, Mikel Artetxe, Ramakanth Pasunuru, Veselin Stoyanov, Xi Victoria Lin |  |
| 2011 |  |  [A Diverse Set of Freely Available Linguistic Resources for Turkish](https://doi.org/10.18653/v1/2023.acl-long.768) |  | 0 | This study presents a diverse set of freely available linguistic resources for Turkish natural language processing, including corpora, pretrained models and education material. Although Turkish is spoken by a sizeable population of over 80 million people, Turkish linguistic resources for natural... | Duygu Altinok |  |
| 2012 |  |  [Measuring Consistency in Text-based Financial Forecasting Models](https://doi.org/10.18653/v1/2023.acl-long.769) |  | 0 | Financial forecasting has been an important and active area of machine learning research, as even the most modest advantages in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data,... | Linyi Yang, Yingpeng Ma, Yue Zhang |  |
| 2013 |  |  [Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.770) |  | 0 | Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement... | André F. T. Martins, Nuno Miguel Guerreiro, Pablo Piantanida, Pierre Colombo |  |
| 2014 |  |  [RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank](https://doi.org/10.18653/v1/2023.acl-long.771) |  | 0 | Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and... | Dongyan Zhao, Jiahao Liu, Jiduan Liu, Jingang Wang, Kai Chen, Qifan Wang, Rui Yan, Wei Wu, Yunsen Xian |  |
| 2015 |  |  [Entailment as Robust Self-Learner](https://doi.org/10.18653/v1/2023.acl-long.772) |  | 0 | Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU... | Hongyin Luo, James R. Glass, Jiaxin Ge, Yoon Kim |  |
| 2016 |  |  [ReCode: Robustness Evaluation of Code Generation Models](https://doi.org/10.18653/v1/2023.acl-long.773) |  | 0 | Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most... | Baishakhi Ray, Bing Xiang, Chenghao Yang, Dan Roth, Haifeng Qian, Mingyue Shang, Murali Krishna Ramanathan, Parminder Bhatia, Ramesh Nallapati, Samson Tan, Shiqi Wang, Varun Kumar, Zheng Li, Zijian Wang |  |
| 2017 |  |  [EPIC: Multi-Perspective Annotation of a Corpus of Irony](https://doi.org/10.18653/v1/2023.acl-long.774) |  | 0 | We present EPIC (English Perspectivist Irony Corpus), the first annotated corpus for irony analysis based on the principles of data perspectivism. The corpus contains short conversations from social media in five regional varieties of English, and it is annotated by contributors from five countries... | Alessandra Teresa Cignarella, Alessandro Pedrani, Bianca Scarlini, Cristina Bosco, Cristina Marco, Davide Bernardi, Raffaella Panizzon, Simona Frenda, Soda Marem Lo, Valerio Basile, Viviana Patti |  |
| 2018 |  |  [Dialogue Summarization with Static-Dynamic Structure Fusion Graph](https://doi.org/10.18653/v1/2023.acl-long.775) |  | 0 | Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years. Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web... | Dongyan Zhao, Jinpeng Li, Mingzhe Li, Rui Yan, Shen Gao, Xin Cheng, Xiuying Chen |  |
| 2019 |  |  [Large-Scale Correlation Analysis of Automated Metrics for Topic Models](https://doi.org/10.18653/v1/2023.acl-long.776) |  | 0 | Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling... | Hady W. Lauw, Jia Peng Lim |  |
| 2020 |  |  [U-CREAT: Unsupervised Case Retrieval using Events extrAcTion](https://doi.org/10.18653/v1/2023.acl-long.777) |  | 0 | The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR... | Abhinav Joshi, Akshat Sharma, Ashutosh Modi, Sai Kiran Tanikella |  |
| 2021 |  |  [ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis](https://doi.org/10.18653/v1/2023.acl-long.778) |  | 0 | Argument Quality Detection is an emerging field in NLP which has seen significant recent development. However, existing datasets in this field suffer from a lack of quality, quantity and diversity of topics and arguments, specifically the presence of vague arguments that are not persuasive in... | Omkar Joshi, Priya Pitre, Yashodhara Haribhakta |  |
| 2022 |  |  [Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework](https://doi.org/10.18653/v1/2023.acl-long.779) |  | 0 | Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually... | Baoxing Huai, Jia Su, Mingqi Gao, Xiaojun Wan, Zhefeng Wang |  |
| 2023 |  |  [Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker](https://doi.org/10.18653/v1/2023.acl-long.780) |  | 0 | Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply... | Alane Suhr, Melanie Sclar, Peter West, Sachin Kumar, Yejin Choi, Yulia Tsvetkov |  |
| 2024 |  |  [Don't Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text](https://doi.org/10.18653/v1/2023.acl-long.781) |  | 0 | Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five... | Alakananda Vempala, Ashim Gupta, Carter Wood Blum, Shalin Shah, Temma Choji, Vivek Srikumar, Yingjie Fei |  |
| 2025 |  |  [Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring](https://doi.org/10.18653/v1/2023.acl-long.782) |  | 0 | Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the... | Cong Wang, Qing Gu, Shiping Ge, Yafeng Yin, Zhiwei Jiang, Zifeng Cheng |  |
| 2026 |  |  [Mitigating Label Biases for In-context Learning](https://doi.org/10.18653/v1/2023.acl-long.783) |  | 0 | Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In... | Antoine Bosselut, Yifan Hou, Yu Fei, Zeming Chen |  |
| 2027 |  |  [QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations](https://doi.org/10.18653/v1/2023.acl-long.784) |  | 0 | Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for “shorebirds that are not sandpipers” or “science-fiction films shot in England”. To study the ability of retrieval... | Chaitanya Malaviya, Kenton Lee, Kristina Toutanova, MingWei Chang, Peter Shaw |  |
| 2028 |  |  [Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering](https://doi.org/10.18653/v1/2023.acl-long.785) |  | 0 | Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the... | Hu Zhang, Jiye Liang, Ru Li, Yujie Wang |  |
| 2029 |  |  [Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation](https://doi.org/10.18653/v1/2023.acl-long.786) |  | 0 | Argument summarisation is a promising but currently under-explored field. Recent work has aimed to provide textual summaries in the form of concise and salient short texts, i.e., key points (KPs), in a task known as Key Point Analysis (KPA). One of the main challenges in KPA is finding high-quality... | Goran Nenadic, Hao Li, Riza BatistaNavarro, Viktor Schlegel |  |
| 2030 |  |  [Ambiguous Learning from Retrieval: Towards Zero-shot Semantic Parsing](https://doi.org/10.18653/v1/2023.acl-long.787) |  | 0 | Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. Thus, minimizing the supervision effort is one of the key challenges in semantic parsing. In this paper, we propose the Retrieval as Ambiguous... | Cao Liu, Chunlei Xin, Fan Yang, Guanglu Wan, Hongyu Lin, Jiansong Chen, Le Sun, Shan Wu, Xianpei Han |  |
| 2031 |  |  [Explicit Syntactic Guidance for Neural Text Generation](https://doi.org/10.18653/v1/2023.acl-long.788) |  | 0 | Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a... | Jianhao Yan, Leyang Cui, Shuming Shi, Wei Bi, Yafu Li, Yongjing Yin, Yue Zhang |  |
| 2032 |  |  [What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric](https://doi.org/10.18653/v1/2023.acl-long.789) |  | 0 | Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a... | Catholijn M. Jonker, Enrico Liscio, Ionut Constantinescu, Kyriaki Kalimeri, Lorenzo Gatti, Oscar Araque, Pradeep Kumar Murukannaiah |  |
| 2033 |  |  [Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning](https://doi.org/10.18653/v1/2023.acl-long.790) |  | 0 | The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models. Prior OOV word embedding learning methods failed to model complex word formation well. In this paper, we propose a novel graph-based relation mining method, namely... | Hegang Chen, Yanghui Rao, Yuyin Lu, Ziran Liang |  |
| 2034 |  |  [Multimodal Persona Based Generation of Comic Dialogs](https://doi.org/10.18653/v1/2023.acl-long.791) |  | 0 | We focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous... | Aditya Mishra, Harsh Agrawal, Manish Gupta, Mausam |  |
| 2035 |  |  [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion](https://doi.org/10.18653/v1/2023.acl-long.792) |  | 0 | We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs... | Bill Yuchen Lin, Dongfu Jiang, Xiang Ren |  |
| 2036 |  |  [Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation](https://doi.org/10.18653/v1/2023.acl-long.793) |  | 0 | Existing controllable dialogue generation work focuses on the single-attribute control and lacks generalization capability to out-of-distribution multiple attribute combinations. In this paper, we explore the compositional generalization for multi-attribute controllable dialogue generation where a... | Jingang Wang, Keqing He, Lulu Zhao, Ruotong Geng, Wei Wu, Weihao Zeng, Weiran Xu |  |
| 2037 |  |  [Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization](https://doi.org/10.18653/v1/2023.acl-long.794) |  | 0 | Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only... | Hailin Jin, Minghang Zheng, Shaogang Gong, Yang Liu, Yuxin Peng |  |
| 2038 |  |  [IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages](https://doi.org/10.18653/v1/2023.acl-long.795) |  | 0 | The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other... | Ananya B. Sai, Anoop Kunchukuttan, Mitesh M. Khapra, Pratyush Kumar, Raj Dabre, Tanay Dixit, Vignesh Nagarajan |  |
| 2039 |  |  [Weaker Than You Think: A Critical Look at Weakly Supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.796) |  | 0 | Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated... | Andreas Stephan, Dawei Zhu, Dietrich Klakow, Marius Mosbach, Xiaoyu Shen |  |
| 2040 |  |  [Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases](https://doi.org/10.18653/v1/2023.acl-long.797) |  | 0 | As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA... | Mengnan Du, Xin Wang, Ying Wang, Yingji Li |  |
| 2041 |  |  [Towards Understanding Omission in Dialogue Summarization](https://doi.org/10.18653/v1/2023.acl-long.798) |  | 0 | Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of... | Dongsheng Li, Kaitao Song, Qi Zhang, Tao Gui, Xu Tan, Yicheng Zou, Zhongkai Fu |  |
| 2042 |  |  [Python Code Generation by Asking Clarification Questions](https://doi.org/10.18653/v1/2023.acl-long.799) |  | 0 | Code generation from text requires understanding the user’s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given... | André F. T. Martins, HaauSing Li, Iryna Gurevych, Mohsen Mesgar |  |
| 2043 |  |  [A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports](https://doi.org/10.18653/v1/2023.acl-long.800) |  | 0 | In this paper, we address the challenge of discovering financial signals in narrative financial reports. As these documents are often lengthy and tend to blend routine information with new information, it is challenging for professionals to discern critical financial signals. To this end, we... | Che Lin, ChengWei Lin, ChuanJu Wang, JiaHuei Ju, YuShiang Huang |  |
| 2044 |  |  [Improving the robustness of NLI models with minimax training](https://doi.org/10.18653/v1/2023.acl-long.801) |  | 0 | Natural language inference (NLI) models are susceptible to learning shortcuts, i.e. decision rules that spuriously correlate with the label. As a result, they achieve high in-distribution performance, but fail to generalize to out-of-distribution samples where such correlations do not hold. In this... | Andreas Vlachos, Michalis Korakakis |  |
| 2045 |  |  [USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis](https://doi.org/10.18653/v1/2023.acl-long.802) |  | 0 | Most previous studies on Structured Sentiment Analysis (SSA) have cast it as a problem of bi-lexical dependency parsing, which cannot address issues of overlap and discontinuity simultaneously. In this paper, we propose a niche-targeting and effective solution. Our approach involves creating a... | Hao Chen, Ruifan Li, Xiaojie Wang, Zepeng Zhai |  |
| 2046 |  |  [PAD-Net: An Efficient Framework for Dynamic Networks](https://doi.org/10.18653/v1/2023.acl-long.803) |  | 0 | Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model’s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given... | Boan Liu, Dacheng Tao, Daize Dong, Fuqiang Yu, Liang Ding, Shwai He |  |
| 2047 |  |  [Resolving Ambiguities in Text-to-Image Generative Models](https://doi.org/10.18653/v1/2023.acl-long.804) |  | 0 | Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for... | Apurv Verma, Aram Galstyan, Jwala Dhamala, KaiWei Chang, Ninareh Mehrabi, Palash Goyal, Qian Hu, Rahul Gupta, Richard S. Zemel, Varun Kumar |  |
| 2048 |  |  [Knowledge Unlearning for Mitigating Privacy Risks in Language Models](https://doi.org/10.18653/v1/2023.acl-long.805) |  | 0 | Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy... | Dongkeun Yoon, Joel Jang, Lajanugen Logeswaran, Minjoon Seo, Moontae Lee, Sohee Yang, Sungmin Cha |  |
| 2049 |  |  [Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor](https://doi.org/10.18653/v1/2023.acl-long.806) |  | 0 | Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a... | Omer Levy, Or Honovich, Thomas Scialom, Timo Schick |  |
| 2050 |  |  [To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.807) |  | 0 | Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only... | Dheeru Dua, Emma Strubell, Pat Verga, Sameer Singh |  |
| 2051 |  |  [A Survey for Efficient Open Domain Question Answering](https://doi.org/10.18653/v1/2023.acl-long.808) |  | 0 | Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising... | Dongkuan Xu, Meng Fang, Qin Zhang, Qingqing Cao, Shangsi Chen, Trevor Cohn, Xiaojun Chen |  |
| 2052 |  |  [Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities](https://doi.org/10.18653/v1/2023.acl-long.809) |  | 0 | The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages. This, however, comes with certain challenges in script normalization, particularly where the speakers of a language in a... | Antonios Anastasopoulos, Sina Ahmadi |  |
| 2053 |  |  [Compositional Generalization without Trees using Multiset Tagging and Latent Permutations](https://doi.org/10.18653/v1/2023.acl-long.810) |  | 0 | Seq2seq models have been shown to struggle with compositional generalization in semantic parsing, i.e. generalizing to unseen compositions of phenomena that the model handles correctly in isolation. We phrase semantic parsing as a two-step process: we first tag each input token with a multiset of... | Alexander Koller, Ivan Titov, Matthias Lindemann |  |
| 2054 |  |  [ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.811) |  | 0 | Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly... | Anahita Bhiwandiwalla, Bei Li, Chenfei Wu, Nan Duan, Shachar Rosenman, ShaoYen Tseng, Vasudev Lal, Wanxiang Che, Xiao Xu |  |
| 2055 |  |  [Finding the Pillars of Strength for Multi-Head Attention](https://doi.org/10.18653/v1/2023.acl-long.812) |  | 0 | Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely... | Erik Cambria, Han Lei, Jinjie Ni, Rui Mao, Zonglin Yang |  |
| 2056 |  |  [Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation](https://doi.org/10.18653/v1/2023.acl-long.813) |  | 0 | Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of... | Anh Tuan Luu, Anran Hao, Yandan Zheng |  |
| 2057 |  |  [Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering](https://doi.org/10.18653/v1/2023.acl-long.814) |  | 0 | Explainable question answering (XQA) aims to answer a given question and provide an explanation why the answer is selected. Existing XQA methods focus on reasoning on a single knowledge source, e.g., structured knowledge bases, unstructured corpora, etc. However, integrating information from... | Jiajie Zhang, Jiaxin Shi, Juanzi Li, Lei Hou, Qi Tian, Shulin Cao, Tingjian Zhang, Xin Lv |  |
| 2058 |  |  [Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation](https://doi.org/10.18653/v1/2023.acl-long.815) |  | 0 | Despite recent advances in detecting fake news generated by neural models, their results are not readily applicable to effective detection of human-written disinformation. What limits the successful transfer between them is the sizable gap between machine-generated fake news and human-authored... | Heng Ji, Kathleen R. McKeown, KungHsiang Huang, Preslav Nakov, Yejin Choi |  |
| 2059 |  |  [A Length-Extrapolatable Transformer](https://doi.org/10.18653/v1/2023.acl-long.816) |  | 0 | Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of... | Alon Benhaim, Barun Patra, Furu Wei, Li Dong, Shaohan Huang, Shuming Ma, Vishrav Chaudhary, Xia Song, Yutao Sun |  |
| 2060 |  |  [A Survey of Deep Learning for Mathematical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.817) |  | 0 | Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has... | KaiWei Chang, Liang Qiu, Pan Lu, Sean Welleck, Wenhao Yu |  |
| 2061 |  |  [A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training](https://doi.org/10.18653/v1/2023.acl-long.818) |  | 0 | Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a... | Amir Kantor, Nitay Calderon, Roi Reichart, Subhabrata Mukherjee |  |
| 2062 |  |  [Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation](https://doi.org/10.18653/v1/2023.acl-long.819) |  | 0 | In this paper, we reconsider the problem of (partial) false negative samples from the Mutual Information (MI) Maximization perspective, the traditional contrastive loss (like InfoNCE loss) will equally push away the anchor of all positive samples and negative samples regardless of their possible... | Chaoya Jiang, Fei Huang, Haiyang Xu, Shikun Zhang, Songfang Huang, Wei Ye |  |
| 2063 |  |  [Tell2Design: A Dataset for Language-Guided Floor Plan Generation](https://doi.org/10.18653/v1/2023.acl-long.820) |  | 0 | We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must... | Mohammed Haroon Dupty, Sam Joyce, Sicong Leng, Wee Sun Lee, Wei Lu, Yang Zhou |  |
| 2064 |  |  [Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations](https://doi.org/10.18653/v1/2023.acl-long.821) |  | 0 | Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth... | Bingsheng Yao, Dakuo Wang, James A. Hendler, Lucian Popa, Prithviraj Sen |  |
| 2065 |  |  [Rethinking Annotation: Can Language Learners Contribute?](https://doi.org/10.18653/v1/2023.acl-long.822) |  | 0 | Researchers have traditionally recruited native speakers to provide annotations for the widely used benchmark datasets. But there are languages for which recruiting native speakers is difficult, and it would help to get learners of those languages to annotate the data. In this paper, we investigate... | Alice Oh, Changyoon Lee, Dongyeop Kang, Haneul Yoo, Rifki Afina Putri, SoYeon Ahn, Youngin Lee |  |
| 2066 |  |  [Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling](https://doi.org/10.18653/v1/2023.acl-long.823) |  | 0 | Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and... | Hao Fei, Lidong Bing, Shengqiong Wu, TatSeng Chua, Yixin Cao |  |
| 2067 |  |  [MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations](https://doi.org/10.18653/v1/2023.acl-long.824) |  | 0 | Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and... | ShaoLun Huang, Tao Shi |  |
| 2068 |  |  [Learning Language-Specific Layers for Multilingual Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.825) |  | 0 | Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating... | Robin M. Schmidt, Stephan Peitz, Telmo Pires, YiHsiu Liao |  |
| 2069 |  |  [Personality Understanding of Fictional Characters during Book Reading](https://doi.org/10.18653/v1/2023.acl-long.826) |  | 0 | Comprehending characters’ personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of... | Fandong Meng, Jiangnan Li, Jie Zhou, Mo Yu, Shunyu Yao, Wenjie Pang, Xiao Zhou, Xiaochen Zhou |  |
| 2070 |  |  [StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing](https://doi.org/10.18653/v1/2023.acl-long.827) |  | 0 | Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more... | Jian Guan, Juan Liu, Minlie Huang, Xuekai Zhu |  |
| 2071 |  |  [Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.828) |  | 0 | Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of... | Hwee Tou Ng, Lidong Bing, Qingyu Tan |  |
| 2072 |  |  [Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings](https://doi.org/10.18653/v1/2023.acl-long.829) |  | 0 | Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference,... | Daniel Rotem, Jonathan Mamou, Michael Hassid, Roy Schwartz |  |
| 2073 |  |  [Large Language Models Are Reasoning Teachers](https://doi.org/10.18653/v1/2023.acl-long.830) |  | 0 | Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these... | Laura Schmid, Namgyu Ho, SeYoung Yun |  |
| 2074 |  |  [Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations](https://doi.org/10.18653/v1/2023.acl-long.831) |  | 0 | Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated... | Alexander M. Rush, Claire Cardie, Justin T. Chiu, Wenting Zhao |  |
| 2075 |  |  [PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification](https://doi.org/10.18653/v1/2023.acl-long.832) |  | 0 | We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to... | Ruohong Zhang, TaChung Chi, YauShian Wang, Yiming Yang |  |
| 2076 |  |  [Visually-augmented pretrained language models for NLP tasks without images](https://doi.org/10.18653/v1/2023.acl-long.833) |  | 0 | Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or... | Hangyu Guo, JiRong Wen, Kun Zhou, Qinyu Zhang, Wayne Xin Zhao |  |
| 2077 |  |  [Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning](https://doi.org/10.18653/v1/2023.acl-long.834) |  | 0 | In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with... | Armineh Nourbakhsh, Carolyn P. Rosé, Sameena Shah |  |
| 2078 |  |  [A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization](https://doi.org/10.18653/v1/2023.acl-long.835) |  | 0 | To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon... | Daniel Deutsch, Elizabeth Clark, João Sedoc, Khyathi Raghavi Chandu, Lining Zhang, Miruna Clinciu, Saad Mahamood, Sebastian Gehrmann, Simon Mille, Yixin Liu, Yufang Hou |  |
| 2079 |  |  [TAVT: Towards Transferable Audio-Visual Text Generation](https://doi.org/10.18653/v1/2023.acl-long.836) |  | 0 | Audio-visual text generation aims to understand multi-modality contents and translate them into texts. Although various transfer learning techniques of text generation have been proposed, they focused on uni-modal analysis (e.g. text-to-text, visual-to-text) and lack consideration of multi-modal... | Linjun Li, Tao Jin, Wang Lin, Wenwen Pan, Xize Cheng, Ye Wang, Zhou Zhao |  |
| 2080 |  |  [MeetingQA: Extractive Question-Answering on Meeting Transcripts](https://doi.org/10.18653/v1/2023.acl-long.837) |  | 0 | With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However,... | Archiki Prasad, Franck Dernoncourt, Hanieh Deilamsalehy, Mohit Bansal, Seunghyun Yoon, Trung Bui |  |
| 2081 |  |  [FERMAT: An Alternative to Accuracy for Numerical Reasoning](https://doi.org/10.18653/v1/2023.acl-long.838) |  | 0 | While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and... | Jasivan Alex Sivakumar, Nafise Sadat Moosavi |  |
| 2082 |  |  [Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2023.acl-long.839) |  | 0 | Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for open-domain chats,... | James D. Finch, Jinho D. Choi, Sarah E. Finch |  |
| 2083 |  |  [Decoder Tuning: Efficient Language Understanding as Decoding](https://doi.org/10.18653/v1/2023.acl-long.840) |  | 0 | With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to... | Ganqu Cui, Longtao Huang, Maosong Sun, Ning Ding, Wentao Li, Zhiyuan Liu |  |
| 2084 |  |  [The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources](https://doi.org/10.18653/v1/2023.acl-long.841) |  | 0 | Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained... | Adam Trischler, Akshatha Arodi, Alexandra Olteanu, Jackie Chi Kit Cheung, Kaheer Suleman, Martin Pömsl |  |
| 2085 |  |  [CREST: A Joint Framework for Rationalization and Counterfactual Text Generation](https://doi.org/10.18653/v1/2023.acl-long.842) |  | 0 | Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome... | Alexis Ross, André F. T. Martins, Marcos V. Treviso, Nuno Miguel Guerreiro |  |
| 2086 |  |  [Towards Unifying Multi-Lingual and Cross-Lingual Summarization](https://doi.org/10.18653/v1/2023.acl-long.843) |  | 0 | To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both... | Duo Zheng, Fandong Meng, Jiaan Wang, Jianfeng Qu, Jie Zhou, Yunlong Liang, Zhixu Li |  |
| 2087 |  |  [On Improving Summarization Factual Consistency from Natural Language Feedback](https://doi.org/10.18653/v1/2023.acl-long.844) |  | 0 | Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual... | Aaron Halfaker, Ahmed Hassan Awadallah, Budhaditya Deb, Dragomir Radev, Milagro Teruel, Yixin Liu |  |
| 2088 |  |  [From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models](https://doi.org/10.18653/v1/2023.acl-long.845) |  | 0 | Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in... | Julia Mendelsohn, Maarten Sap, Ronan Le Bras, Yejin Choi |  |
| 2089 |  |  [Exploring Large Language Models for Classical Philology](https://doi.org/10.18653/v1/2023.acl-long.846) |  | 0 | Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study... | Anette Frank, Frederick Riemenschneider |  |
| 2090 |  |  [LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding](https://doi.org/10.18653/v1/2023.acl-long.847) |  | 0 | Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different... | Huan Chen, Jinyang Tang, Ya Guo, Yi Tu |  |
| 2091 |  |  [Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2023.acl-long.848) |  | 0 | Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise... | Chen Chen, Chengwei Qin, Eng Siong Chng, QiuShi Zhu, Ruizhe Li, Yuchen Hu |  |
| 2092 |  |  [An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation](https://doi.org/10.18653/v1/2023.acl-long.849) |  | 0 | Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a... | Maosong Sun, Peng Li, Tao Li, Xuancheng Huang, Yang Liu, Zijun Liu |  |
| 2093 |  |  [Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion](https://doi.org/10.18653/v1/2023.acl-long.850) |  | 0 | Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many... | Hongcai Xu, Junpeng Bao, Wenbo Liu |  |
| 2094 |  |  [Dual Cache for Long Document Neural Coreference Resolution](https://doi.org/10.18653/v1/2023.acl-long.851) |  | 0 | Recent works show the effectiveness of cache-based neural coreference resolution models on long documents. These models incrementally process a long document from left to right and extract relations between mentions and entities in a cache, resulting in much lower memory and computation cost... | Qipeng Guo, Xiangkun Hu, Xipeng Qiu, Yue Zhang, Zheng Zhang |  |
| 2095 |  |  [Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.852) |  | 0 | In the real-world scenario, a longstanding goal of multilingual neural machine translation (MNMT) is that a single model can incrementally adapt to new language pairs without accessing previous training data. In this scenario, previous studies concentrate on overcoming catastrophic forgetting while... | Jin Ma, Kaiyu Huang, Peng Li, Ting Yao, Yang Liu |  |
| 2096 |  |  [DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media](https://doi.org/10.18653/v1/2023.acl-long.853) |  | 0 | Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work,... | Adrián Pastor LópezMonroy, David E. Losada, Luis Gonzalez, Manuel Montes, Mario Ezra Aragón |  |
| 2097 |  |  [Toward Interactive Dictation](https://doi.org/10.18653/v1/2023.acl-long.854) |  | 0 | Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with... | Adam Pauls, Belinda Z. Li, Jason Eisner, Sam Thomson |  |
| 2098 |  |  [CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors](https://doi.org/10.18653/v1/2023.acl-long.855) |  | 0 | Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it.... | Hang Yan, Peng Li, Qiong Tang, Tianxiang Sun, Xipeng Qiu, Xuanjing Huang, Yuanbin Wu |  |
| 2099 |  |  [Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.856) |  | 0 | In this paper, we elaborate upon recipes for building multilingual representation models that are not only competitive with existing state-of-the-art models but are also more parameter efficient, thereby promoting better adoption in resource-constrained scenarios and practical applications. We show... | Barun Patra, Furu Wei, Li Dong, Saksham Singhal, Shaohan Huang, Vishrav Chaudhary, Xia Song, Zewen Chi |  |
| 2100 |  |  [Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension](https://doi.org/10.18653/v1/2023.acl-long.857) |  | 0 | Open-retrieval conversational machine reading comprehension (OCMRC) simulates real-life conversational interaction scenes. Machines are required to make a decision of “Yes/No/Inquire” or generate a follow-up question when the decision is “Inquire” based on retrieved rule texts, user scenario, user... | Heyan Huang, XianLing Mao, Xiao Zhang, Zewen Chi |  |
| 2101 |  |  [LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming](https://doi.org/10.18653/v1/2023.acl-long.858) |  | 0 | Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale social media data and large pre-trained models, there is no guarantee these agents could also perform well in fast-growing scenarios, such as live... | Baoyuan Wang, Jingsheng Gao, Yixin Lian, Yuzhuo Fu, Ziyi Zhou |  |
| 2102 |  |  [Prompting PaLM for Translation: Assessing Strategies and Performance](https://doi.org/10.18653/v1/2023.acl-long.859) |  | 0 | Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT)... | Colin Cherry, David Vilar, George F. Foster, Jiaming Luo, Markus Freitag, Viresh Ratnakar |  |
| 2103 |  |  [Exploring Lottery Prompts for Pre-trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.860) |  | 0 | Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts,... | Haitao Zheng, Ning Ding, Pengjun Xie, Shengding Hu, Xiaobin Wang, Yulin Chen, Zhiyuan Liu |  |
| 2104 |  |  [A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations](https://doi.org/10.18653/v1/2023.acl-long.861) |  | 0 | Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently,... | Jianfei Yu, Rui Xia, Shijin Wang, Wenjie Zheng |  |
| 2105 |  |  [TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline](https://doi.org/10.18653/v1/2023.acl-long.862) |  | 0 | Temporal knowledge graph embedding (TKGE) models are commonly utilized to infer the missing facts and facilitate reasoning and decision-making in temporal knowledge graph based systems. However, existing methods fuse temporal information into entities, potentially leading to the evolution of entity... | Guanglai Gao, Jiang Li, Xiangdong Su |  |
| 2106 |  |  [Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition](https://doi.org/10.18653/v1/2023.acl-long.863) |  | 0 | Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated... | Barrett Martin Lattimer, Joyce Chai, Yuwei Bao |  |
| 2107 |  |  [Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination](https://doi.org/10.18653/v1/2023.acl-long.864) |  | 0 | This paper confirms that, in English binary coordinations, left conjuncts tend to be shorter than right conjuncts, regardless of the position of the governor of the coordination. We demonstrate that this tendency becomes stronger when length differences are greater, but only when the governor is on... | Adam Przepiórkowski, Michal Wozniak |  |
| 2108 |  |  [LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development](https://doi.org/10.18653/v1/2023.acl-long.865) |  | 0 | In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream... | Anders Søgaard, Catalina Goanta, Daniel Martin Katz, Ilias Chalkidis, Nicolas Garneau |  |
| 2109 |  |  [Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge](https://doi.org/10.18653/v1/2023.acl-long.866) |  | 0 | The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored... | Derek F. Wong, Liangxuan Yu, Min Zhang, Runzhe Zhan, Xuebo Liu, Yutong Wang |  |
| 2110 |  |  [NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models](https://doi.org/10.18653/v1/2023.acl-long.867) |  | 0 | Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting... | Kai Mei, Shiqing Ma, Yang Zhang, Zheng Li, Zhenting Wang |  |
| 2111 |  |  [Revisiting Relation Extraction in the era of Large Language Models](https://doi.org/10.18653/v1/2023.acl-long.868) |  | 0 | Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem... | Byron C. Wallace, Silvio Amir, Somin Wadhwa |  |
| 2112 |  |  [Pre-trained Language Models Can be Fully Zero-Shot Learners](https://doi.org/10.18653/v1/2023.acl-long.869) |  | 0 | How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or... | Lei Li, Ming Wu, Siqi Ouyang, Xuandong Zhao, Zhiguo Yu |  |
| 2113 |  |  [Can Large Language Models Be an Alternative to Human Evaluations?](https://doi.org/10.18653/v1/2023.acl-long.870) |  | 0 | Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural... | David ChengHan Chiang, Hungyi Lee |  |
| 2114 |  |  [HyperMixer: An MLP-based Low Cost Alternative to Transformers](https://doi.org/10.18653/v1/2023.acl-long.871) |  | 0 | Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple... | Arnaud Pannatier, Fabio Fehr, Florian Mai, François Fleuret, François Marelli, Haolin Chen, James Henderson |  |
| 2115 |  |  [UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units](https://doi.org/10.18653/v1/2023.acl-long.872) |  | 0 | Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and... | Ann Lee, Changhan Wang, Hirofumi Inaguma, Ilia Kulikov, Juan Pino, PengJen Chen, Shinji Watanabe, Sravya Popuri, YuAn Chung, Yun Tang |  |
| 2116 |  |  [Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression](https://doi.org/10.18653/v1/2023.acl-long.873) |  | 0 | In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground... | Chao Zhang, Philip C. Woodland, Wen Wu |  |
| 2117 |  |  [Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation](https://doi.org/10.18653/v1/2023.acl-long.874) |  | 0 | Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model... | Michael Strube, Wei Liu |  |
| 2118 |  |  [Plug-and-Play Document Modules for Pre-trained Models](https://doi.org/10.18653/v1/2023.acl-long.875) |  | 0 | Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally... | Chaojun Xiao, ChiMin Chan, Maosong Sun, Xiangyang Li, Xu Han, Yankai Lin, Zhao Cao, Zhengyan Zhang, Zhiyuan Liu, Zhonghua Li |  |
| 2119 |  |  [An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models](https://doi.org/10.18653/v1/2023.acl-long.876) |  | 0 | The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with... | Thomas Lukasiewicz, Zhongbin Xie |  |
| 2120 |  |  [Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models](https://doi.org/10.18653/v1/2023.acl-long.877) |  | 0 | The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion... | Guergana Savova, Lijing Wang, Steven Bethard, Timothy Miller, Yingya Li |  |
| 2121 |  |  [A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models](https://doi.org/10.18653/v1/2023.acl-long.878) |  | 0 | Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages.... | Arnav Chavan, Krithika Ramesh, Shrey Pandit, Sunayana Sitaram |  |
| 2122 |  |  [Ranking-Enhanced Unsupervised Sentence Representation Learning](https://doi.org/10.18653/v1/2023.acl-long.879) |  | 0 | Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that... | Alice Oh, Changmin Seo, Guoyin Wang, Jiwei Li, Puyang Xu, Sajal Choudhary, Sunghyun Park, Xiang Li, Yeon Seonwoo |  |
| 2123 |  |  [To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support](https://doi.org/10.18653/v1/2023.acl-long.880) |  | 0 | Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying... | Gabriella Skitalinskaya, Henning Wachsmuth |  |
| 2124 |  |  [Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments](https://doi.org/10.18653/v1/2023.acl-long.881) |  | 0 | We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets... | Alan Ritter, Ethan Mendes, Wei Xu, Yang Chen |  |
| 2125 |  |  [Composition-contrastive Learning for Sentence Embeddings](https://doi.org/10.18653/v1/2023.acl-long.882) |  | 0 | Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and... | Ruihong Huang, Sachin Chanchani |  |
| 2126 |  |  [Causes and Cures for Interference in Multilingual Translation](https://doi.org/10.18653/v1/2023.acl-long.883) |  | 0 | Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This... | Maha Elbayad, Omer Levy, Shruti Bhosale, Uri Shaham, Vedanuj Goswami |  |
| 2127 |  |  [Understanding and Bridging the Modality Gap for Speech Translation](https://doi.org/10.18653/v1/2023.acl-long.884) |  | 0 | How to achieve better end-to-end speech translation (ST) by leveraging (text) machine translation (MT) data? Among various existing techniques, multi-task learning is one of the effective ways to share knowledge between ST and MT in which additional MT data can help to learn source-to-target... | Qingkai Fang, Yang Feng |  |
| 2128 |  |  [Few-shot Reranking for Multi-hop QA via Language Model Prompting](https://doi.org/10.18653/v1/2023.acl-long.885) |  | 0 | We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first... | Honglak Lee, Lajanugen Logeswaran, Lu Wang, Moontae Lee, Muhammad Khalifa |  |
| 2129 |  |  [DICE: Data-Efficient Clinical Event Extraction with Generative Models](https://doi.org/10.18653/v1/2023.acl-long.886) |  | 0 | Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient... | Alexander Taylor, Mingyu Derek Ma, Nanyun Peng, Wei Wang |  |
| 2130 |  |  [XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations](https://doi.org/10.18653/v1/2023.acl-long.887) |  | 0 | Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications,... | Jun Wang, Rui Zhang, Yusen Zhang, Zhiguo Wang |  |
| 2131 |  |  [INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2023.acl-long.888) |  | 0 | Neural machine translation has achieved promising results on many translation tasks. However, previous studies have shown that neural models induce a non-smooth representation space, which harms its generalization results. Recently, kNN-MT has provided an effective paradigm to smooth the prediction... | Jiajun Chen, Jingjing Xu, Lingpeng Kong, Shujian Huang, Wenhao Zhu |  |
| 2132 |  |  [Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.889) |  | 0 | Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to... | Kun Huang, Kun Zhang, Pengfei Hong, Qi Sun, Soujanya Poria, Xiaocui Yang |  |
| 2133 |  |  [Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning](https://doi.org/10.18653/v1/2023.acl-long.890) |  | 0 | The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the... | Gaurav Verma, Shivaen Ramshetty, Srijan Kumar |  |
| 2134 |  |  [Crosslingual Generalization through Multitask Finetuning](https://doi.org/10.18653/v1/2023.acl-long.891) |  | 0 | Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned... | Adam Roberts, Albert Webson, Alham Fikri Aji, Colin Raffel, Dragomir Radev, Edward Raff, Hailey Schoelkopf, Khalid Almubarak, Lintang Sutawika, M. Saiful Bari, Niklas Muennighoff, Samuel Albanie, Sheng Shen, Stella Biderman, Teven Le Scao, Thomas Wang, Xiangru Tang, Zaid Alyafeai, Zheng Xin Yong |  |
| 2135 |  |  [Evaluate AMR Graph Similarity via Self-supervised Learning](https://doi.org/10.18653/v1/2023.acl-long.892) |  | 0 | In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and... | Fangzhen Lin, Ziyi Shou |  |
| 2136 |  |  [Analyzing Transformers in Embedding Space](https://doi.org/10.18653/v1/2023.acl-long.893) |  | 0 | Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters... | Ankit Gupta, Guy Dar, Jonathan Berant, Mor Geva |  |
| 2137 |  |  [Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning](https://doi.org/10.18653/v1/2023.acl-long.894) |  | 0 | In this paper, we present a novel approach for data-to-text generation that addresses the limitations of current methods that primarily focus on specific types of structured data. Our proposed method aims to improve performance in multi-task training, zero-shot and few-shot scenarios by providing a... | Alexander Hanbo Li, Bing Xiang, Bonan Min, Dan Roth, Evangelia Spiliopoulou, Jie Ma, Kathleen R. McKeown, Mingyue Shang, Patrick Ng, Vittorio Castelli, William Yang Wang, Zhiguo Wang |  |
| 2138 |  |  [FactKG: Fact Verification via Reasoning on Knowledge Graphs](https://doi.org/10.18653/v1/2023.acl-long.895) |  | 0 | In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their... | Edward Choi, James Thorne, Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo |  |
| 2139 |  |  [DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains](https://doi.org/10.18653/v1/2023.acl-long.896) |  | 0 | In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we... | Adrien Bazoge, Béatrice Daille, Emmanuel Morin, Mickael Rouvier, PierreAntoine Gourraud, Richard Dufour, Yanis Labrak |  |
| 2140 |  |  [Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction](https://doi.org/10.18653/v1/2023.acl-long.897) |  | 0 | Document-level Event Causality Identification (DECI) aims to extract causal relations between events in a document. It challenges conventional sentence-level task (SECI) with difficult long-text understanding. In this paper, we propose a novel DECI model (SENDIR) for better document-level... | Changsen Yuan, Heyan Huang, Yixin Cao, Yonggang Wen |  |
| 2141 |  |  [Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks](https://doi.org/10.18653/v1/2023.acl-long.898) |  | 0 | The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly due to limited datasets. Existing datasets suffer from a lack of fine-grained annotations, such as the toxic type and expressions... | Bo Xu, Changrong Min, Hongfei Lin, Junyu Lu, Liang Yang, Xiaokun Zhang |  |
| 2142 |  |  [SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations](https://doi.org/10.18653/v1/2023.acl-long.899) |  | 0 | We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we... | Ann Lee, Benoît Sagot, Changhan Wang, Holger Schwenk, Hongyu Gong, Jingfei Du, Juan Pino, Ning Dong, PaulAmbroise Duquenne, Vedanuj Goswami |  |
| 2143 |  |  [Character-Aware Models Improve Visual Text Rendering](https://doi.org/10.18653/v1/2023.acl-long.900) |  | 0 | Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word’s visual makeup as a series of glyphs. To quantify... | Adam Roberts, Chitwan Saharia, Dan Garrette, Irina Blok, Mohammad Norouzi, Noah Constant, RJ Mical, Rosanne Liu, Sharan Narang, William Chan |  |
| 2144 |  |  [IDRISI-RA: The First Arabic Location Mention Recognition Dataset of Disaster Tweets](https://doi.org/10.18653/v1/2023.acl-long.901) |  | 0 | Extracting geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, in locating incidents for planning rescue activities, and affected people for evacuation. Nevertheless, geolocation extraction is greatly understudied for... | Muhammad Imran, Reem Suwaileh, Tamer Elsayed |  |
| 2145 |  |  [FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction](https://doi.org/10.18653/v1/2023.acl-long.902) |  | 0 | Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does... | Bo Du, Hai Zhao, Lefei Zhang, Tianshuo Peng, Zuchao Li |  |
| 2146 |  |  [What Do NLP Researchers Believe? Results of the NLP Community Metasurvey](https://doi.org/10.18653/v1/2023.acl-long.903) |  | 0 | We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split... | Aaron Mueller, Alex Wang, Alicia Parrish, Angelica Chen, Ari Holtzman, Divyam Madaan, Jason Phang, Julian Michael, Nikita Nangia, Richard Yuanzhe Pang, Samuel R. Bowman |  |
| 2147 |  |  [Prototype-Guided Pseudo Labeling for Semi-Supervised Text Classification](https://doi.org/10.18653/v1/2023.acl-long.904) |  | 0 | Semi-supervised text classification (SSTC) aims at text classification with few labeled data and massive unlabeled data. Recent works achieve this task by pseudo-labeling methods, with the belief that the unlabeled and labeled data have identical data distribution, and assign the unlabeled data... | Jaein Kim, Junfan Chen, Lihong Wang, Richong Zhang, Weiyi Yang |  |
| 2148 |  |  [LENS: A Learnable Evaluation Metric for Text Simplification](https://doi.org/10.18653/v1/2023.acl-long.905) |  | 0 | Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making... | David Heineman, Mounica Maddela, Wei Xu, Yao Dou |  |
| 2149 |  |  [MeetingBank: A Benchmark Dataset for Meeting Summarization](https://doi.org/10.18653/v1/2023.acl-long.906) |  | 0 | As the number of recorded meetings increases, it becomes increasingly important to utilize summarization technology to create useful summaries of these recordings. However, there is a crucial lack of annotated meeting corpora for developing this technology, as it can be hard to collect meetings,... | Fei Liu, Franck Dernoncourt, Hanieh Deilamsalehy, Hassan Foroosh, Timothy Ganter, Yebowen Hu |  |
| 2150 |  |  [UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective](https://doi.org/10.18653/v1/2023.acl-long.907) |  | 0 | We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the... | Jiaxing Zhang, Junjie Wang, Junyu Lu, Pingjian Zhang, Ruyi Gan, Yang Ping, Yuxiang Zhang |  |
| 2151 |  |  [DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification](https://doi.org/10.18653/v1/2023.acl-long.908) |  | 0 | Text simplification is an intralingual translation task in which documents, or sentences of a complex source text are simplified for a target audience. The success of automatic text simplification systems is highly dependent on the quality of parallel data used for training and evaluation. To... | Laura Kallmeyer, Omar Momen, Regina Stodden |  |
| 2152 |  |  [A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text](https://doi.org/10.18653/v1/2023.acl-long.909) |  | 0 | Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process... | Baotian Hu, Lin Ma, Min Zhang, Yunxin Li, Yuxin Ding |  |
| 2153 |  |  [RARR: Researching and Revising What Language Models Say, Using Language Models](https://doi.org/10.18653/v1/2023.acl-long.910) |  | 0 | Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for... | Anthony Chen, Arun Tejasvi Chaganty, DaCheng Juan, Hongrae Lee, Kelvin Guu, Luyu Gao, Ni Lao, Panupong Pasupat, Vincent Y. Zhao, Yicheng Fan, Zhuyun Dai |  |
