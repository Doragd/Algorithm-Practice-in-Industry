# ACL2022

## 会议论文列表

本会议共有 1110 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [A Gentle Introduction to Deep Nets and Opportunities for the Future](https://doi.org/10.18653/v1/2022.acl-tutorials.1) |  | 0 | The first half of this tutorial will make deep nets more accessible to a broader audience, following “Deep Nets for Poets” and “A Gentle Introduction to Fine-Tuning.” We will also introduce GFT (general fine tuning), a little language for fine tuning deep nets with short (one line) programs that are as easy to code as regression in statistics packages such as R using glm (general linear models). Based on the success of these methods on a number of benchmarks, one might come away with the impression that deep nets are all we need. However, we believe the glass is half-full: while there is much that can be done with deep nets, there is always more to do. The second half of this tutorial will discuss some of these opportunities. | Kenneth Church, Valia Kordoni, Gary Marcus, Ernest Davis, Yanjun Ma, Zeyu Chen |  |
| 2 |  |  [Towards Reproducible Machine Learning Research in Natural Language Processing](https://doi.org/10.18653/v1/2022.acl-tutorials.2) |  | 0 | While recent progress in the field of ML has been significant, the reproducibility of these cutting-edge results is often lacking, with many submissions lacking the necessary information in order to ensure subsequent reproducibility. Despite proposals such as the Reproducibility Checklist and reproducibility criteria at several major conferences, the reflex for carrying out research with reproducibility in mind is lacking in the broader ML community. We propose this tutorial as a gentle introduction to ensuring reproducible research in ML, with a specific emphasis on computational linguistics and NLP. We also provide a framework for using reproducibility as a teaching tool in university-level computer science programs. | Ana Lucic, Maurits J. R. Bleeker, Samarth Bhargav, Jessica Zosa Forde, Koustuv Sinha, Jesse Dodge, Sasha Luccioni, Robert Stojnic |  |
| 3 |  |  [Knowledge-Augmented Methods for Natural Language Processing](https://doi.org/10.18653/v1/2022.acl-tutorials.3) |  | 0 | Knowledge in natural language processing (NLP) has been a rising trend especially after the advent of large scale pre-trained models. NLP models with attention to knowledge can i) access unlimited amount of external information; ii) delegate the task of storing knowledge from its parameter space to knowledge sources; iii) obtain up-to-date information; iv) make prediction results more explainable via selected knowledge. In this tutorial, we will introduce the key steps in integrating knowledge into NLP, including knowledge grounding from text, knowledge representation and fusing. In addition, we will introduce recent state-of-the-art applications in fusing knowledge into language understanding, language generation and commonsense reasoning. | Chenguang Zhu, Yichong Xu, Xiang Ren, Bill Y. Lin, Meng Jiang, Wenhao Yu |  |
| 4 |  |  [Non-Autoregressive Sequence Generation](https://doi.org/10.18653/v1/2022.acl-tutorials.4) |  | 0 | Non-autoregressive sequence generation (NAR) attempts to generate the entire or partial output sequences in parallel to speed up the generation process and avoid potential issues (e.g., label bias, exposure bias) in autoregressive generation. While it has received much research attention and has been applied in many sequence generation tasks in natural language and speech, naive NAR models still face many challenges to close the performance gap between state-of-the-art autoregressive models because of a lack of modeling power. In this tutorial, we will provide a thorough introduction and review of non-autoregressive sequence generation, in four sections: 1) Background, which covers the motivation of NAR generation, the problem definition, the evaluation protocol, and the comparison with standard autoregressive generation approaches. 2) Method, which includes different aspects: model architecture, objective function, training data, learning paradigm, and additional inference tricks. 3) Application, which covers different tasks in text and speech generation, and some advanced topics in applications. 4) Conclusion, in which we describe several research challenges and discuss the potential future research directions. We hope this tutorial can serve both academic researchers and industry practitioners working on non-autoregressive sequence generation. | Jiatao Gu, Xu Tan |  |
| 5 |  |  [Learning with Limited Text Data](https://doi.org/10.18653/v1/2022.acl-tutorials.5) |  | 0 | Natural Language Processing (NLP) has achieved great progress in the past decade on the basis of neural models, which often make use of large amounts of labeled data to achieve state-of-the-art performance. The dependence on labeled data prevents NLP models from being applied to low-resource settings and languages because of the time, money, and expertise that is often required to label massive amounts of textual data. Consequently, the ability to learn with limited labeled data is crucial for deploying neural systems to real-world NLP applications. Recently, numerous approaches have been explored to alleviate the need for labeled data in NLP such as data augmentation and semi-supervised learning. This tutorial aims to provide a systematic and up-to-date overview of these methods in order to help researchers and practitioners understand the landscape of approaches and the challenges associated with learning from limited labeled data, an emerging topic in the computational linguistics community. We will consider applications to a wide variety of NLP tasks (including text classification, generation, and structured prediction) and will highlight current challenges and future directions. | Diyi Yang, Ankur P. Parikh, Colin Raffel |  |
| 6 |  |  [Zero- and Few-Shot NLP with Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-tutorials.6) |  | 0 | The ability to efficiently learn from little-to-no data is critical to applying NLP to tasks where data collection is costly or otherwise difficult. This is a challenging setting both academically and practically—particularly because training neutral models typically require large amount of labeled data. More recently, advances in pretraining on unlabelled data have brought up the potential of better zero-shot or few-shot learning (Devlin et al., 2019; Brown et al., 2020). In particular, over the past year, a great deal of research has been conducted to better learn from limited data using large-scale language models. In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for zero- and few-shot learning with pretrained language models. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain. | Iz Beltagy, Arman Cohan, Robert L. Logan IV, Sewon Min, Sameer Singh |  |
| 7 |  |  [Vision-Language Pretraining: Current Trends and the Future](https://doi.org/10.18653/v1/2022.acl-tutorials.7) |  | 0 | In the last few years, there has been an increased interest in building multimodal (vision-language) models that are pretrained on larger but noisier datasets where the two modalities (e.g., image and text) loosely correspond to each other (e.g., Lu et al., 2019; Radford et al., 2021). Given a task (such as visual question answering), these models are then often fine-tuned on task-specific supervised datasets. (e.g., Lu et al., 2019; Chen et al.,2020; Tan and Bansal, 2019; Li et al., 2020a,b). In addition to the larger pretraining datasets, the transformer architecture (Vaswani et al., 2017) and in particular self-attention applied to two modalities are responsible for the impressive performance of the recent pretrained models on downstream tasks (Hendricks et al., 2021). In this tutorial, we focus on recent vision-language pretraining paradigms. Our goal is to first provide the background on image–language datasets, benchmarks, and modeling innovations before the multimodal pretraining area. Next we discuss the different family of models used for vision-language pretraining, highlighting their strengths and shortcomings. Finally, we discuss the limits of vision-language pretraining through statistical learning, and the need for alternative approaches such as causal representation learning. | Aishwarya Agrawal, Damien Teney, Aida Nematzadeh |  |
| 8 |  |  [Natural Language Processing for Multilingual Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-tutorials.8) |  | 0 | Recent advances in deep learning have also enabled fast progress in the research of task-oriented dialogue (ToD) systems. However, the majority of ToD systems are developed for English and merely a handful of other widely spoken languages, e.g., Chinese and German. This hugely limits the global reach and, consequently, transformative socioeconomic potential of such systems. In this tutorial, we will thus discuss and demonstrate the importance of (building) multilingual ToD systems, and then provide a systematic overview of current research gaps, challenges and initiatives related to multilingual ToD systems, with a particular focus on their connections to current research and challenges in multilingual and low-resource NLP. The tutorial will aim to provide answers or shed new light to the following questions: a) Why are multilingual dialogue systems so hard to build: what makes multilinguality for dialogue more challenging than for other NLP applications and tasks? b) What are the best existing methods and datasets for multilingual and cross-lingual (task-oriented) dialog systems? How are (multilingual) ToD systems usually evaluated? c) What are the promising future directions for multilingual ToD research: where can one draw inspiration from related NLP areas and tasks? | Evgeniia Razumovskaia, Goran Glavas, Olga Majewska, Edoardo Maria Ponti, Ivan Vulic |  |
| 9 |  |  [DoTAT: A Domain-oriented Text Annotation Tool](https://doi.org/10.18653/v1/2022.acl-demo.1) |  | 0 | We propose DoTAT, a domain-oriented text annotation tool. The tool designs and implements functions heavily in need in domain-oriented information extraction. Firstly, the tool supports a multi-person collaborative process with automatically merging and review, which can greatly improve the annotation accuracy. Secondly, the tool provides annotation of events, nested event and nested entity, which are frequently required in domain-related text structuring tasks. Finally, DoTAT provides visual annotation specification definition, automatic batch annotation and iterative annotation to improve annotation efficiency. Experiments on the ACE2005 dataset show that DoTAT can reduce the event annotation time by 19.7% compared with existing annotation tools. The accuracy without review is 84.09%, 1.35% higher than Brat and 2.59% higher than Webanno. The accuracy of DoTAT even reaches 93.76% with review. The demonstration video can be accessed from https://ecust-nlp-docker.oss-cn-shanghai.aliyuncs.com/dotat_demo.mp4. A live demo website is available at https://github.com/FXLP/MarkTool. | Yupian Lin, Tong Ruan, Ming Liang, Tingting Cai, Wen Du, Yi Wang |  |
| 10 |  |  [UKP-SQUARE: An Online Platform for Question Answering Research](https://doi.org/10.18653/v1/2022.acl-demo.2) |  | 0 | Recent advances in NLP and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval). Despite having a large number of powerful, specialized QA pipelines (which we refer to as Skills) that consider a single domain, model or setup, there exists no framework where users can easily explore and compare such pipelines and can extend them according to their needs. To address this issue, we present UKP-SQuARE, an extensible online QA platform for researchers which allows users to query and analyze a large collection of modern Skills via a user-friendly web interface and integrated behavioural tests. In addition, QA researchers can develop, manage, and share their custom Skills using our microservices that support a wide range of models (Transformers, Adapters, ONNX), datastores and retrieval techniques (e.g., sparse and dense). UKP-SQuARE is available on https://square.ukp-lab.de | Tim Baumgärtner, Kexin Wang, Rachneet Sachdeva, Gregor Geigle, Max Eichler, Clifton Poth, Hannah Sterz, Haritz Puerto, Leonardo F. R. Ribeiro, Jonas Pfeiffer, Nils Reimers, Gözde Gül Sahin, Iryna Gurevych |  |
| 11 |  |  [ViLMedic: a framework for research at the intersection of vision and language in medical AI](https://doi.org/10.18653/v1/2022.acl-demo.3) |  | 0 | There is a growing need to model interactions between data modalities (e.g., vision, language) — both to improve AI predictions on existing tasks and to enable new applications. In the recent field of multimodal medical AI, integrating multiple modalities has gained widespread popularity as multimodal models have proven to improve performance, robustness, require less training samples and add complementary information. To improve technical reproducibility and transparency for multimodal medical tasks as well as speed up progress across medical AI, we present ViLMedic, a Vision-and-Language medical library. As of 2022, the library contains a dozen reference implementations replicating the state-of-the-art results for problems that range from medical visual question answering and radiology report generation to multimodal representation learning on widely adopted medical datasets. In addition, ViLMedic hosts a model-zoo with more than twenty pretrained models for the above tasks designed to be extensible by researchers but also simple for practitioners. Ultimately, we hope our reproducible pipelines can enable clinical translation and create real impact. The library is available at https://github.com/jbdel/vilmedic. | JeanBenoit Delbrouck, Khaled Saab, Maya Varma, Sabri Eyuboglu, Pierre J. Chambon, Jared Dunnmon, Juan Zambrano, Akshay Chaudhari, Curtis P. Langlotz |  |
| 12 |  |  [TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-demo.4) |  | 0 | Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data. Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model. | Ziqing Yang, Yiming Cui, Zhigang Chen |  |
| 13 |  |  [AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark](https://doi.org/10.18653/v1/2022.acl-demo.5) |  | 0 | Open Information Extraction (OIE) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner. Intrinsic performance of OIE systems is difficult to measure due to the incompleteness of existing OIE benchmarks: ground truth extractions do not group all acceptable surface realizations of the same fact that can be extracted from a sentence. To measure performance of OIE systems more realistically, it is necessary to manually annotate complete facts (i.e., clusters of all acceptable surface realizations of the same fact) from input sentences. We propose AnnIE: an interactive annotation platform that facilitates such challenging annotation tasks and supports creation of complete fact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order to support different use case scenarios (i.e., benchmarks covering different types of facts) and different languages. We use AnnIE to build two complete OIE benchmarks: one with verb-mediated facts and another with facts encompassing named entities. We evaluate several OIE systems on our complete benchmarks created with AnnIE. We publicly release AnnIE (and all gold datasets generated with it) under non-restrictive license. | Niklas Friedrich, Kiril Gashteovski, Mingying Yu, Bhushan Kotnis, Carolin Lawrence, Mathias Niepert, Goran Glavas |  |
| 14 |  |  [AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters](https://doi.org/10.18653/v1/2022.acl-demo.6) |  | 0 | The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research. This also allows people outside of NLP to use such models and adapt them to specific use-cases. However, a certain amount of technical proficiency is still required which is an entry barrier for users who want to apply these models to a certain task but lack the necessary knowledge or resources. In this work, we aim to overcome this gap by providing a tool which allows researchers to leverage pretrained models without writing a single line of code. Built upon the parameter-efficient adapter modules for transfer learning, our AdapterHub Playground provides an intuitive interface, allowing the usage of adapters for prediction, training and analysis of textual data for a variety of NLP tasks. We present the tool’s architecture and demonstrate its advantages with prototypical use-cases, where we show that predictive performance can easily be increased in a few-shot learning scenario. Finally, we evaluate its usability in a user study. We provide the code and a live interface at https://adapter-hub.github.io/playground. | Tilman Beck, Bela Bohlender, Christina Viehmann, Vincent Hane, Yanik Adamson, Jaber Khuri, Jonas Brossmann, Jonas Pfeiffer, Iryna Gurevych |  |
| 15 |  |  [QiuNiu: A Chinese Lyrics Generation System with Passage-Level Input](https://doi.org/10.18653/v1/2022.acl-demo.7) |  | 0 | Lyrics generation has been a very popular application of natural language generation. Previous works mainly focused on generating lyrics based on a couple of attributes or keywords, rendering very limited control over the content of the lyrics. In this paper, we demonstrate the QiuNiu, a Chinese lyrics generation system which is conditioned on passage-level text rather than a few attributes or keywords. By using the passage-level text as input, the content of generated lyrics is expected to reflect the nuances of users’ needs. The QiuNiu system supports various forms of passage-level input, such as short stories, essays, poetry. The training of it is conducted under the framework of unsupervised machine translation, due to the lack of aligned passage-level text-to-lyrics corpus. We initialize the parameters of QiuNiu with a custom pretrained Chinese GPT-2 model and adopt a two-step process to finetune the model for better alignment between passage-level text and lyrics. Additionally, a postprocess module is used to filter and rerank the generated lyrics to select the ones of highest quality. The demo video of the system is available at https://youtu.be/OCQNzahqWgM. | Le Zhang, Rongsheng Zhang, Xiaoxi Mao, Yongzhu Chang |  |
| 16 |  |  [Automatic Gloss Dictionary for Sign Language Learners](https://doi.org/10.18653/v1/2022.acl-demo.8) |  | 0 | A multi-language dictionary is a fundamental tool for language learning, allowing the learner to look up unfamiliar words. Searching an unrecognized word in the dictionary does not usually require deep knowledge of the target language. However, this is not true for sign language, where gestural elements preclude this type of easy lookup. This paper introduces GlossFinder, an online tool supporting 2, 000 signs to assist language learners in determining the meaning of given signs. Unlike alternative systems of complex inputs, our system requires only that learners imitate the sign in front of a standard webcam. A user study conducted among sign language speakers of varying ability compared our system against existing alternatives and the interviews indicated a clear preference for our new system. This implies that GlossFinder can lower the barrier in sign language learning by addressing the common problem of sign finding and make it accessible to the wider community. | Chenchen Xu, Dongxu Li, Hongdong Li, Hanna Suominen, Ben Swift |  |
| 17 |  |  [PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts](https://doi.org/10.18653/v1/2022.acl-demo.9) |  | 0 | PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource. | Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik BenDavid, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir R. Radev, Mike TianJian Jiang, Alexander M. Rush |  |
| 18 |  |  [OpenPrompt: An Open-source Framework for Prompt-learning](https://doi.org/10.18653/v1/2022.acl-demo.10) |  | 0 | Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt- learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, verbalizing strategy, etc., that need to be considered in prompt-learning, practitioners face impediments to quickly adapting the de-sired prompt learning methods to their applications. In this paper, we present Open- Prompt, a unified easy-to-use toolkit to conduct prompt-learning over PLMs. OpenPrompt is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different PLMs, task for- mats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different NLP tasks without constraints. | Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, Maosong Sun |  |
| 19 |  |  [Guided K-best Selection for Semantic Parsing Annotation](https://doi.org/10.18653/v1/2022.acl-demo.11) |  | 0 | Collecting data for conversational semantic parsing is a time-consuming and demanding process. In this paper we consider, given an incomplete dataset with only a small amount of data, how to build an AI-powered human-in-the-loop process to enable efficient data collection. A guided K-best selection process is proposed, which (i) generates a set of possible valid candidates; (ii) allows users to quickly traverse the set and filter incorrect parses; and (iii) asks users to select the correct parse, with minimal modification when necessary. We investigate how to best support users in efficiently traversing the candidate set and locating the correct parse, in terms of speed and accuracy. In our user study, consisting of five annotators labeling 300 instances each, we find that combining keyword searching, where keywords can be used to query relevant candidates, and keyword suggestion, where representative keywords are automatically generated, enables fast and accurate annotation. | Anton Belyy, ChiehYang Huang, Jacob Andreas, Emmanouil Antonios Platanios, Sam Thomson, Richard Shin, Subhro Roy, Aleksandr Nisnevich, Charles Chen, Benjamin Van Durme |  |
| 20 |  |  [Hard and Soft Evaluation of NLP models with BOOtSTrap SAmpling - BooStSa](https://doi.org/10.18653/v1/2022.acl-demo.12) |  | 0 | Natural Language Processing (NLP) ‘s applied nature makes it necessary to select the most effective and robust models. Producing slightly higher performance is insufficient; we want to know whether this advantage will carry over to other data sets. Bootstrapped significance tests can indicate that ability. So while necessary, computing the significance of models’ performance differences has many levels of complexity. It can be tedious, especially when the experimental design has many conditions to compare and several runs of experiments. We present BooStSa, a tool that makes it easy to compute significance levels with the BOOtSTrap SAmpling procedure to evaluate models that predict not only standard hard labels but soft-labels (i.e., probability distributions over different classes) as well. | Tommaso Fornaciari, Alexandra Uma, Massimo Poesio, Dirk Hovy |  |
| 21 |  |  [COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System](https://doi.org/10.18653/v1/2022.acl-demo.13) |  | 0 | To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation. | Manling Li, Revanth Gangi Reddy, Ziqi Wang, YiShyuan Chiang, Tuan Manh Lai, Pengfei Yu, Zixuan Zhang, Heng Ji |  |
| 22 |  |  [TS-ANNO: An Annotation Tool to Build, Annotate and Evaluate Text Simplification Corpora](https://doi.org/10.18653/v1/2022.acl-demo.14) |  | 0 | We introduce TS-ANNO, an open-source web application for manual creation and for evaluation of parallel corpora for text simplification. TS-ANNO can be used for i) sentence–wise alignment, ii) rating alignment pairs (e.g., w.r.t. grammaticality, meaning preservation, ...), iii) annotating alignment pairs w.r.t. simplification transformations (e.g., lexical substitution, sentence splitting, ...), and iv) manual simplification of complex documents. For evaluation, TS-ANNO calculates inter-annotator agreement of alignments (i) and annotations (ii). | Regina Stodden, Laura Kallmeyer |  |
| 23 |  |  [Language Diversity: Visible to Humans, Exploitable by Machines](https://doi.org/10.18653/v1/2022.acl-demo.15) |  | 0 | The Universal Knowledge Core (UKC) is a large multilingual lexical database with a focus on language diversity and covering over two thousand languages. The aim of the database, as well as its tools and data catalogue, is to make the abstract notion of linguistic diversity visually understandable for humans and formally exploitable by machines. The UKC website lets users explore millions of individual words and their meanings, but also phenomena of cross-lingual convergence and divergence, such as shared interlingual meanings, lexicon similarities, cognate clusters, or lexical gaps. The UKC LiveLanguage Catalogue, in turn, provides access to the underlying lexical data in a computer-processable form, ready to be reused in cross-lingual applications. | Gábor Bella, Erdenebileg Byambadorj, Yamini Chandrashekar, Khuyagbaatar Batsuren, Danish Ashgar Cheema, Fausto Giunchiglia |  |
| 24 |  |  [CogKGE: A Knowledge Graph Embedding Toolkit and Benchmark for Representing Multi-source and Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.acl-demo.16) |  | 0 | In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge. For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge. For heterogeneous knowledge, besides structured triple facts, CogKGE leverages additional unstructured information, such as text descriptions, node types and temporal information, to enhance the meaning of embeddings. Designing CogKGE aims to provide a unified programming framework for KGE tasks and a series of knowledge representations for downstream tasks. As a research framework, CogKGE consists of five parts, including core, data, model, knowledge and adapter module. As a knowledge discovery toolkit, CogKGE provides pre-trained embedders to discover new facts, cluster entities and check facts. Furthermore, we construct two benchmark datasets for further research on multi-source heterogeneous KGE tasks: EventKG240K and CogNet360K. We also release an online system to discover knowledge visually. Source code, datasets and pre-trained embeddings are publicly available at GitHub, with a short instruction video. | Zhuoran Jin, Tianyi Men, Hongbang Yuan, Zhitao He, Dianbo Sui, Chenhao Wang, Zhipeng Xue, Yubo Chen, Jun Zhao |  |
| 25 |  |  [Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks](https://doi.org/10.18653/v1/2022.acl-demo.17) |  | 0 | We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench. | Tristan Thrush, Kushal Tirumala, Anmol Gupta, Max Bartolo, Pedro Rodriguez, Tariq Kane, William Gaviria Rojas, Peter Mattson, Adina Williams, Douwe Kiela |  |
| 26 |  |  [DataLab: A Platform for Data Analysis and Intervention](https://doi.org/10.18653/v1/2022.acl-demo.18) |  | 0 | Despite data’s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data. In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers. | Yang Xiao, Jinlan Fu, Weizhe Yuan, Vijay Viswanathan, Zhoumianze Liu, Yixin Liu, Graham Neubig, Pengfei Liu |  |
| 27 |  |  [Cue-bot: A Conversational Agent for Assistive Technology](https://doi.org/10.18653/v1/2022.acl-demo.19) |  | 0 | Intelligent conversational assistants have become an integral part of our lives for performing simple tasks. However, such agents, for example, Google bots, Alexa and others are yet to have any social impact on minority population, for example, for people with neurological disorders and people with speech, language and social communication disorders, sometimes with locked-in states where speaking or typing is a challenge. Language model technologies can be very powerful tools in enabling these users to carry out daily communication and social interactions. In this work, we present a system that users with varied levels of disabilties can use to interact with the world, supported by eye-tracking, mouse controls and an intelligent agent Cue-bot, that can represent the user in a conversation. The agent provides relevant controllable ‘cues’ to generate desirable responses quickly for an ongoing dialog context. In the context of usage of such systems for people with degenerative disorders, we present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system and show that our models perform significantly better than models without control and can also reduce user effort (fewer keystrokes) and speed up communication (typing time) significantly. | Shachi H. Kumar, Hsuan Su, Ramesh Manuvinakurike, Maximilian Pinaroc, Sai Prasad, Saurav Sahay, Lama Nachman |  |
| 28 |  |  [M-SENA: An Integrated Platform for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-demo.20) |  | 0 | M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations. The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules. In this paper, we first illustrate the overall architecture of the M-SENA platform and then introduce features of the core modules. Reliable baseline results of different modality features and MSA benchmarks are also reported. Moreover, we use model evaluation and analysis tools provided by M-SENA to present intermediate representation visualization, on-the-fly instance test, and generalization ability test results. The source code of the platform is publicly available at https://github.com/thuiar/M-SENA. | Huisheng Mao, Ziqi Yuan, Hua Xu, Wenmeng Yu, Yihe Liu, Kai Gao |  |
| 29 |  |  [HOSMEL: A Hot-Swappable Modularized Entity Linking Toolkit for Chinese](https://doi.org/10.18653/v1/2022.acl-demo.21) |  | 0 | We investigate the usage of entity linking (EL)in downstream tasks and present the first modularized EL toolkit for easy task adaptation. Different from the existing EL methods that dealwith all the features simultaneously, we modularize the whole model into separate parts witheach feature. This decoupled design enablesflexibly adding new features without retraining the whole model as well as flow visualization with better interpretability of the ELresult. We release the corresponding toolkit,HOSMEL, for Chinese, with three flexible usage modes, a live demo, and a demonstrationvideo. Experiments on two benchmarks forthe question answering task demonstrate thatHOSMEL achieves much less time and spaceconsumption as well as significantly better accuracy performance compared with existingSOTA EL methods. We hope the release ofHOSMEL will call for more attention to studyEL for downstream tasks in non-English languages. | Daniel Zhangli, Jing Zhang, Jifan Yu, Xiaokang Zhang, Peng Zhang, Jie Tang, Juanzi Li |  |
| 30 |  |  [BMInf: An Efficient Toolkit for Big Model Inference and Tuning](https://doi.org/10.18653/v1/2022.acl-demo.22) |  | 0 | In recent years, large-scale pre-trained language models (PLMs) containing billions of parameters have achieved promising results on various NLP tasks. Although we can pre-train these big models by stacking computing clusters at any cost, it is impractical to use such huge computing resources to apply big models for each downstream task. To address the computation bottleneck encountered in deploying big models in real-world scenarios, we introduce an open-source toolkit for big model inference and tuning (BMInf), which can support big model inference and tuning at extremely low computation cost. More specifically, at the algorithm level, we introduce model quantization and parameter-efficient tuning for efficient model inference and tuning. At the implementation level, we apply model offloading, model checkpointing, and CPU-GPU scheduling optimization to further reduce the computation and memory cost of big models. Based on above efforts, we can efficiently perform big model inference and tuning with a single GPU (even a consumer-level GPU like GTX 1060) instead of computing clusters, which is difficult for existing distributed learning toolkits for PLMs. BMInf is publicly released at https://github.com/OpenBMB/BMInf. | Xu Han, Guoyang Zeng, Weilin Zhao, Zhiyuan Liu, Zhengyan Zhang, Jie Zhou, Jun Zhang, Jia Chao, Maosong Sun |  |
| 31 |  |  [MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities](https://doi.org/10.18653/v1/2022.acl-demo.23) |  | 0 | Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other. Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels. | Yubo Ma, Zehao Wang, Mukai Li, Yixin Cao, Meiqi Chen, Xinze Li, Wenqi Sun, Kunquan Deng, Kun Wang, Aixin Sun, Jing Shao |  |
| 32 |  |  [SocioFillmore: A Tool for Discovering Perspectives](https://doi.org/10.18653/v1/2022.acl-demo.24) |  | 0 | SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the focus or the perspective that a text expresses in depicting an event. Our tool, whose rationale we also support through a large collection of human judgements, is theoretically grounded on frame semantics and cognitive linguistics, and implemented using the LOME frame semantic parser. We describe SOCIOFILLMORE’s development and functionalities, show how non-NLP researchers can easily interact with the tool, and present some example case studies which are already incorporated in the system, together with the kind of analysis that can be visualised. | Gosse Minnema, Sara Gemelli, Chiara Zanchi, Tommaso Caselli, Malvina Nissim |  |
| 33 |  |  [TimeLMs: Diachronic Language Models from Twitter](https://doi.org/10.18653/v1/2022.acl-demo.25) |  | 0 | Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models’ capacity to deal with future and out-of-distribution tweets, while making them competitive with standardized and more monolithic benchmarks. We also perform a number of qualitative analyses showing how they cope with trends and peaks in activity involving specific named entities or concept drift. TimeLMs is available at github.com/cardiffnlp/timelms. | Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, José CamachoCollados |  |
| 34 |  |  [Adaptor: Objective-Centric Adaptation Framework for Language Models](https://doi.org/10.18653/v1/2022.acl-demo.26) |  | 0 | This paper introduces Adaptor library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives. We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation. Adaptor aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of Adaptor in selected unsupervised domain adaptation scenarios. | Michal Stefánik, Vít Novotný, Nikola Groverová, Petr Sojka |  |
| 35 |  |  [QuickGraph: A Rapid Annotation Tool for Knowledge Graph Extraction from Technical Text](https://doi.org/10.18653/v1/2022.acl-demo.27) |  | 0 | Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators. Adoption of unsupervised techniques for automated annotation have thus become popular. However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases. While such resources are abundant for general domains, they are scarce for specialised technical domains. To tackle this challenge, we present QuickGraph, the first collaborative MT-IE annotation tool built with indirect weak supervision and clustering to maximise annotator productivity. QuickGraph’s main contribution is a set of novel features that enable knowledge graph extraction through rapid and consistent complex multi-task entity and relation annotation. In this paper, we discuss these key features and qualitatively compare QuickGraph to existing annotation tools. | Tyler Bikaun, Michael Stewart, Wei Liu |  |
| 36 |  |  [Frontmatter](https://aclanthology.org/2022.acl-srw.0) |  | 0 |  |  |  |
| 37 |  |  [Evaluating zero-shot transfers and multilingual models for dependency parsing and POS tagging within the low-resource language family Tupían](https://doi.org/10.18653/v1/2022.acl-srw.1) |  | 0 | This work presents two experiments with the goal of replicating the transferability of dependency parsers and POS taggers trained on closely related languages within the low-resource language family Tupían. The experiments include both zero-shot settings as well as multilingual models. Previous studies have found that even a comparably small treebank from a closely related language will improve sequence labelling considerably in such cases. Results from both POS tagging and dependency parsing confirm previous evidence that the closer the phylogenetic relation between two languages, the better the predictions for sequence labelling tasks get. In many cases, the results are improved if multiple languages from the same family are combined. This suggests that in addition to leveraging similarity between two related languages, the incorporation of multiple languages of the same family might lead to better results in transfer learning for NLP applications. | Frederic Blum |  |
| 38 |  |  [RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2022.acl-srw.2) |  | 0 | Joint relational triple extraction from unstructured text is an important task in information extraction. However, most existing works either ignore the semantic information of relations or predict subjects and objects sequentially. To address the issues, we introduce a new blank filling paradigm for the task, and propose a relation-first blank filling network (RFBFN). Specifically, we first detect potential relations maintained in the text to aid the following entity pair extraction. Then, we transform relations into relation templates with blanks which contain the fine-grained semantic representation of the relations. Finally, corresponding subjects and objects are extracted simultaneously by filling the blanks. We evaluate the proposed model on public benchmark datasets. Experimental results show our model outperforms current state-of-the-art methods. The source code of our work is available at: https://github.com/lizhe2016/RFBFN. | Zhe Li, Luoyi Fu, Xinbing Wang, Haisong Zhang, Chenghu Zhou |  |
| 39 |  |  [Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions](https://doi.org/10.18653/v1/2022.acl-srw.3) |  | 0 | In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with the emotion that a speaker put into the utterance (expressed emotion) and the emotion that a listener felt after listening to the utterance (experienced emotion). We built a dialogue corpus in Japanese using this method, and its statistical analysis revealed the differences between expressed and experienced emotions. We conducted experiments on recognition of the two kinds of emotions. The experimental results indicated the difficulty in recognizing experienced emotions and the effectiveness of multi-task learning of the two kinds of emotions. We hope that the constructed corpus will facilitate the study on emotion recognition in a dialogue and emotion-aware dialogue response generation. | Tatsuya Ide, Daisuke Kawahara |  |
| 40 |  |  [Darkness can not drive out darkness: Investigating Bias in Hate SpeechDetection Models](https://doi.org/10.18653/v1/2022.acl-srw.4) |  | 0 | It has become crucial to develop tools for automated hate speech and abuse detection. These tools would help to stop the bullies and the haters and provide a safer environment for individuals especially from marginalized groups to freely express themselves. However, recent research shows that machine learning models are biased and they might make the right decisions for the wrong reasons. In this thesis, I set out to understand the performance of hate speech and abuse detection models and the different biases that could influence them. I show that hate speech and abuse detection models are not only subject to social bias but also to other types of bias that have not been explored before. Finally, I investigate the causal effect of the social and intersectional bias on the performance and unfairness of hate speech detection models. | Fatma Elsafoury |  |
| 41 |  |  [Ethical Considerations for Low-resourced Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.5) |  | 0 | This paper considers some ethical implications of machine translation for low-resourced languages. I use Armenian as a case study and investigate specific needs for and concerns arising from the creation and deployment of improved machine translation between English and Armenian. To do this, I conduct stakeholder interviews and construct Value Scenarios (Nathan et al., 2007) from the themes that emerge. These scenarios illustrate some of the potential harms that low-resourced language communities may face due to the deployment of improved machine translation systems. Based on these scenarios, I recommend 1) collaborating with stakeholders in order to create more useful and reliable machine translation tools, and 2) determining which other forms of language technology should be developed alongside efforts to improve machine translation in order to mitigate harms rendered to vulnerable language communities. Both of these goals require treating low-resourced machine translation as a language-specific, rather than language-agnostic, task. | Levon Haroutunian |  |
| 42 |  |  [Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach](https://doi.org/10.18653/v1/2022.acl-srw.6) |  | 0 | Resolving dependencies among dialogue history is one of the main obstacles in the research on conversational question answering (QA). The conversational question rewrites (QR) task has been shown to be effective to solve this problem by reformulating questions in a self-contained form. However, QR datasets are limited and existing methods tend to depend on the assumption of the existence of corresponding QR datasets for every CQA dataset. This paper proposes a reinforcement learning approach that integrates QR and CQA tasks without corresponding labeled QR datasets. We train a QR model based on the reward signal obtained from the CQA, and the experimental results show that our approach can bring improvement over the pipeline approaches. | Etsuko Ishii, Bryan Wilie, Yan Xu, Samuel Cahyawijaya, Pascale Fung |  |
| 43 |  |  [What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification](https://doi.org/10.18653/v1/2022.acl-srw.7) |  | 0 | Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taillé et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets. | Elisa Bassignana, Barbara Plank |  |
| 44 |  |  [Logical Inference for Counting on Semi-structured Tables](https://doi.org/10.18653/v1/2022.acl-srw.8) |  | 0 | Recently, the Natural Language Inference (NLI) task has been studied for semi-structured tables that do not have a strict format. Although neural approaches have achieved high performance in various types of NLI, including NLI between semi-structured tables and texts, they still have difficulty in performing a numerical type of inference, such as counting. To handle a numerical type of inference, we propose a logical inference system for reasoning between semi-structured tables and texts. We use logical representations as meaning representations for tables and texts and use model checking to handle a numerical type of inference between texts and tables. To evaluate the extent to which our system can perform inference with numerical comparatives, we make an evaluation protocol that focuses on numerical understanding between semi-structured tables and texts in English. We show that our system can more robustly perform inference between tables and texts that requires numerical understanding compared with current neural approaches. | Tomoya Kurosawa, Hitomi Yanaka |  |
| 45 |  |  [GNNer: Reducing Overlapping in Span-based NER Using Graph Neural Networks](https://doi.org/10.18653/v1/2022.acl-srw.9) |  | 0 | There are two main paradigms for Named Entity Recognition (NER): sequence labelling and span classification. Sequence labelling aims to assign a label to each word in an input text using, for example, BIO (Begin, Inside and Outside) tagging, while span classification involves enumerating all possible spans in a text and classifying them into their labels. In contrast to sequence labelling, unconstrained span-based methods tend to assign entity labels to overlapping spans, which is generally undesirable, especially for NER tasks without nested entities. Accordingly, we propose GNNer, a framework that uses Graph Neural Networks to enrich the span representation to reduce the number of overlapping spans during prediction. Our approach reduces the number of overlapping spans compared to strong baseline while maintaining competitive metric performance. Code is available at https://github.com/urchade/GNNer. | Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois |  |
| 46 |  |  [Compositional Semantics and Inference System for Temporal Order based on Japanese CCG](https://doi.org/10.18653/v1/2022.acl-srw.10) |  | 0 | Natural Language Inference (NLI) is the task of determining whether a premise entails a hypothesis. NLI with temporal order is a challenging task because tense and aspect are complex linguistic phenomena involving interactions with temporal adverbs and temporal connectives. To tackle this, temporal and aspectual inference has been analyzed in various ways in the field of formal semantics. However, a Japanese NLI system for temporal order based on the analysis of formal semantics has not been sufficiently developed. We present a logic-based NLI system that considers temporal order in Japanese based on compositional semantics via Combinatory Categorial Grammar (CCG) syntactic analysis. Our system performs inference involving temporal order by using axioms for temporal relations and automated theorem provers. We evaluate our system by experimenting with Japanese NLI datasets that involve temporal order. We show that our system outperforms previous logic-based systems as well as current deep learning-based models. | Tomoki Sugimoto, Hitomi Yanaka |  |
| 47 |  |  [Combine to Describe: Evaluating Compositional Generalization in Image Captioning](https://doi.org/10.18653/v1/2022.acl-srw.11) |  | 0 | Compositionality – the ability to combine simpler concepts to understand & generate arbitrarily more complex conceptual structures – has long been thought to be the cornerstone of human language capacity. With the recent, notable success of neural models in various NLP tasks, attention has now naturally turned to the compositional capacity of these models. In this paper, we study the compositional generalization properties of image captioning models. We perform a set experiments under controlled conditions using model and data ablations, each designed to benchmark a particular facet of compositional generalization: systematicity is the ability of a model to create novel combinations of concepts out of those observed during training, productivity is here operationalised as the capacity of a model to extend its predictions beyond the length distribution it has observed during training, and substitutivity is concerned with the robustness of the model against synonym substitutions. While previous work has focused primarily on systematicity, here we provide a more in-depth analysis of the strengths and weaknesses of state of the art captioning models. Our findings demonstrate that the models we study here do not compositionally generalize in terms of systematicity and productivity, however, they are robust to some degree to synonym substitutions | Georgios Pantazopoulos, Alessandro Suglia, Arash Eshghi |  |
| 48 |  |  [Towards Unification of Discourse Annotation Frameworks](https://doi.org/10.18653/v1/2022.acl-srw.12) |  | 0 | Discourse information is difficult to represent and annotate. Among the major frameworks for annotating discourse information, RST, PDTB and SDRT are widely discussed and used, each having its own theoretical foundation and focus. Corpora annotated under different frameworks vary considerably. To make better use of the existing discourse corpora and achieve the possible synergy of different frameworks, it is worthwhile to investigate the systematic relations between different frameworks and devise methods of unifying the frameworks. Although the issue of framework unification has been a topic of discussion for a long time, there is currently no comprehensive approach which considers unifying both discourse structure and discourse relations and evaluates the unified framework intrinsically and extrinsically. We plan to use automatic means for the unification task and evaluate the result with structural complexity and downstream tasks. We will also explore the application of the unified framework in multi-task learning and graphical models. | Yingxue Fu |  |
| 49 |  |  [AMR Alignment for Morphologically-rich and Pro-drop Languages](https://doi.org/10.18653/v1/2022.acl-srw.13) |  | 0 | Alignment between concepts in an abstract meaning representation (AMR) graph and the words within a sentence is one of the important stages of AMR parsing. Although there exist high performing AMR aligners for English, unfortunately, these are not well suited for many languages where many concepts appear from morpho-semantic elements. For the first time in the literature, this paper presents an AMR aligner tailored for morphologically-rich and pro-drop languages by experimenting on the Turkish language being a prominent example of this language group. Our aligner focuses on the meaning considering the rich Turkish morphology and aligns AMR concepts that emerge from morphemes using a tree traversal approach without additional resources or rules. We evaluate our aligner over a manually annotated gold data set in terms of precision, recall and F1 score. Our aligner outperforms the Turkish adaptations of the previously proposed aligners for English and Portuguese by an F1 score of 0.87 and provides a relative error reduction of up to 76%. | Elif Kaplan, Gülsen Eryigit |  |
| 50 |  |  [Sketching a Linguistically-Driven Reasoning Dialog Model for Social Talk](https://doi.org/10.18653/v1/2022.acl-srw.14) |  | 0 | The capability of holding social talk (or casual conversation) and making sense of conversational content requires context-sensitive natural language understanding and reasoning, which cannot be handled efficiently by the current popular open-domain dialog systems and chatbots. Heavily relying on corpus-based machine learning techniques to encode and decode context-sensitive meanings, these systems focus on fitting a particular training dataset, but not tracking what is actually happening in a conversation, and therefore easily derail in a new context. This work sketches out a more linguistically-informed architecture to handle social talk in English, in which corpus-based methods form the backbone of the relatively context-insensitive components (e.g. part-of-speech tagging, approximation of lexical meaning and constituent chunking), while symbolic modeling is used for reasoning out the context-sensitive components, which do not have any consistent mapping to linguistic forms. All components are fitted into a Bayesian game-theoretic model to address the interactive and rational aspects of conversation. | Alex Luu |  |
| 51 |  |  [Scoping natural language processing in Indonesian and Malay for education applications](https://doi.org/10.18653/v1/2022.acl-srw.15) |  | 0 | Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview of Indonesian and Malay human language technologies and corpus work. We charted 657 included studies according to Hirschberg and Manning’s 2015 description of NLP, concluding that the field was dominated by exploratory corpus work, machine reading of text gathered from the Internet, and sentiment analysis. In this paper, we identify most published authors and research hubs, and make a number of recommendations to encourage future collaboration and efficiency within NLP in Indonesian and Malay. | Zara MaxwellSmith, Michelle Kohler, Hanna Suominen |  |
| 52 |  |  [English-Malay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation](https://doi.org/10.18653/v1/2022.acl-srw.16) |  | 0 | As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented the training bilingual lexicons using machine translation with the goal to improve the alignment precision of our cross-lingual word embeddings. We investigated the quality of the current state-of-the-art English-Malay bilingual lexicon and worked on improving its quality using Google Translate. We also examined the effect of Malay word coverage on the quality of cross-lingual word embeddings. Experimental results with a precision up till 28.17% show that the alignment precision of the cross-lingual word embeddings would inevitably degrade after 1-NN but a better seed lexicon and cleaner nearest neighbours can reduce the number of word pairs required to achieve satisfactory performance. As the English and Malay monolingual embeddings are pre-trained on informal language corpora, our proposed English-Malay embeddings alignment approach is also able to map non-standard Malay translations in the English nearest neighbours. | Ying Hao Lim, Jasy Suet Yan Liew |  |
| 53 |  |  [Towards Detecting Political Bias in Hindi News Articles](https://doi.org/10.18653/v1/2022.acl-srw.17) |  | 0 | Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles. We propose a transformer-based transfer learning method to fine-tune the pre-trained network on our data for this bias detection. As the required dataset for this particular task was not available, we created our dataset comprising 1388 Hindi news articles and their headlines from various Hindi news media outlets. We marked them on whether they are biased towards, against, or neutral to BJP, a political party, and the current ruling party at the centre in India. | Samyak Agrawal, Kshitij Gupta, Devansh Gautam, Radhika Mamidi |  |
| 54 |  |  [Restricted or Not: A General Training Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.18) |  | 0 | Restricted machine translation incorporates human prior knowledge into translation. It restricts the flexibility of the translation to satisfy the demands of translation in specific scenarios. Existing work typically imposes constraints on beam search decoding. Although this can satisfy the requirements overall, it usually requires a larger beam size and far longer decoding time than unrestricted translation, which limits the concurrent processing ability of the translation model in deployment, and thus its practicality. In this paper, we propose a general training framework that allows a model to simultaneously support both unrestricted and restricted translation by adopting an additional auxiliary training process without constraining the decoding process. This maintains the benefits of restricted translation but greatly reduces the extra time overhead of constrained decoding, thus improving its practicality. The effectiveness of our proposed training framework is demonstrated by experiments on both original (WAT21 En↔Ja) and simulated (WMT14 En→De and En→Fr) restricted translation benchmarks. | Zuchao Li, Masao Utiyama, Eiichiro Sumita, Hai Zhao |  |
| 55 |  |  [What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge](https://doi.org/10.18653/v1/2022.acl-srw.19) |  | 0 | There are limitations in learning language from text alone. Therefore, recent focus has been on developing multimodal models. However, few benchmarks exist that can measure what language models learn about language from multimodal training. We hypothesize that training on a visual modality should improve on the visual commonsense knowledge in language models. Therefore, we introduce two evaluation tasks for measuring visual commonsense knowledge in language models (code publicly available at: github.com/lovhag/measure-visual-commonsense-knowledge) and use them to evaluate different multimodal models and unimodal baselines. Primarily, we find that the visual commonsense knowledge is not significantly different between the multimodal models and unimodal baseline models trained on visual text data. | Lovisa Hagström, Richard Johansson |  |
| 56 |  |  [TeluguNER: Leveraging Multi-Domain Named Entity Recognition with Deep Transformers](https://doi.org/10.18653/v1/2022.acl-srw.20) |  | 0 | Named Entity Recognition (NER) is a successful and well-researched problem in English due to the availability of resources. The transformer models, specifically the masked-language models (MLM), have shown remarkable performance in NER during recent times. With growing data in different online platforms, there is a need for NER in other languages too. NER remains to be underexplored in Indian languages due to the lack of resources and tools. Our contributions in this paper include (i) Two annotated NER datasets for the Telugu language in multiple domains: Newswire Dataset (ND) and Medical Dataset (MD), and we combined ND and MD to form Combined Dataset (CD) (ii) Comparison of the finetuned Telugu pretrained transformer models (BERT-Te, RoBERTa-Te, and ELECTRA-Te) with other baseline models (CRF, LSTM-CRF, and BiLSTM-CRF) (iii) Further investigation of the performance of Telugu pretrained transformer models against the multilingual models mBERT, XLM-R, and IndicBERT. We find that pretrained Telugu language models (BERT-Te and RoBERTa) outperform the existing pretrained multilingual and baseline models in NER. On a large dataset (CD) of 38,363 sentences, the BERT-Te achieves a high F1-score of 0.80 (entity-level) and 0.75 (token-level). Further, these pretrained Telugu models have shown state-of-the-art performance on various existing Telugu NER datasets. We open-source our dataset, pretrained models, and code. | Suma Reddy Duggenpudi, Subba Reddy Oota, Mounika Marreddy, Radhika Mamidi |  |
| 57 |  |  [Using Neural Machine Translation Methods for Sign Language Translation](https://doi.org/10.18653/v1/2022.acl-srw.21) |  | 0 | We examine methods and techniques, proven to be helpful for the text-to-text translation of spoken languages in the context of gloss-to-text translation systems, where the glosses are the written representation of the signs. We present one of the first works that include experiments on both parallel corpora of the German Sign Language (PHOENIX14T and the Public DGS Corpus). We experiment with two NMT architectures with optimization of their hyperparameters, several tokenization methods and two data augmentation techniques (back-translation and paraphrasing). Through our investigation we achieve a substantial improvement of 5.0 and 2.2 BLEU scores for the models trained on the two corpora respectively. Our RNN models outperform our Transformer models, and the segmentation method we achieve best results with is BPE, whereas back-translation and paraphrasing lead to minor but not significant improvements. | Galina Angelova, Eleftherios Avramidis, Sebastian Möller |  |
| 58 |  |  [Flexible Visual Grounding](https://doi.org/10.18653/v1/2022.acl-srw.22) |  | 0 | Existing visual grounding datasets are artificially made, where every query regarding an entity must be able to be grounded to a corresponding image region, i.e., answerable. However, in real-world multimedia data such as news articles and social media, many entities in the text cannot be grounded to the image, i.e., unanswerable, due to the fact that the text is unnecessarily directly describing the accompanying image. A robust visual grounding model should be able to flexibly deal with both answerable and unanswerable visual grounding. To study this flexible visual grounding problem, we construct a pseudo dataset and a social media dataset including both answerable and unanswerable queries. In order to handle unanswerable visual grounding, we propose a novel method by adding a pseudo image region corresponding to a query that cannot be grounded. The model is then trained to ground to ground-truth regions for answerable queries and pseudo regions for unanswerable queries. In our experiments, we show that our model can flexibly process both answerable and unanswerable queries with high accuracy on our datasets. | Yongmin Kim, Chenhui Chu, Sadao Kurohashi |  |
| 59 |  |  [A large-scale computational study of content preservation measures for text style transfer and paraphrase generation](https://doi.org/10.18653/v1/2022.acl-srw.23) |  | 0 | Text style transfer and paraphrasing of texts are actively growing areas of NLP, dozens of methods for solving these tasks have been recently introduced. In both tasks, the system is supposed to generate a text which should be semantically similar to the input text. Therefore, these tasks are dependent on methods of measuring textual semantic similarity. However, it is still unclear which measures are the best to automatically evaluate content preservation between original and generated text. According to our observations, many researchers still use BLEU-like measures, while there exist more advanced measures including neural-based that significantly outperform classic approaches. The current problem is the lack of a thorough evaluation of the available measures. We close this gap by conducting a large-scale computational study by comparing 57 measures based on different principles on 19 annotated datasets. We show that measures based on cross-encoder models outperform alternative approaches in almost all cases. We also introduce the Mutual Implication Score (MIS), a measure that uses the idea of paraphrasing as a bidirectional entailment and outperforms all other measures on the paraphrase detection task and performs on par with the best measures in the text style transfer task. | Nikolay Babakov, David Dale, Varvara Logacheva, Alexander Panchenko |  |
| 60 |  |  [Explicit Object Relation Alignment for Vision and Language Navigation](https://doi.org/10.18653/v1/2022.acl-srw.24) |  | 0 | In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the visual environment, including landmarks and spatial relationships between the agent and landmarks. Empirically, our proposed method surpasses the baseline by a large margin on the R2R dataset. We provide a comprehensive analysis to show our model’s spatial reasoning ability and explainability. | Yue Zhang, Parisa Kordjamshidi |  |
| 61 |  |  [Mining Logical Event Schemas From Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-srw.25) |  | 0 | We present NESL (the Neuro-Episodic Schema Learner), an event schema learning system that combines large language models, FrameNet parsing, a powerful logical representation of language, and a set of simple behavioral schemas meant to bootstrap the learning process. In lieu of a pre-made corpus of stories, our dataset is a continuous feed of “situation samples” from a pre-trained language model, which are then parsed into FrameNet frames, mapped into simple behavioral schemas, and combined and generalized into complex, hierarchical schemas for a variety of everyday scenarios. We show that careful sampling from the language model can help emphasize stereotypical properties of situations and de-emphasize irrelevant details, and that the resulting schemas specify situations more comprehensively than those learned by other systems. | Lane Lawley, Lenhart K. Schubert |  |
| 62 |  |  [Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models](https://doi.org/10.18653/v1/2022.acl-srw.26) |  | 0 | Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike previous works we aim to make large language models able to perform detoxification without direct fine-tuning in a given language. Experiments show that multilingual models are capable of performing multilingual style transfer. However, tested state-of-the-art models are not able to perform cross-lingual detoxification and direct fine-tuning on exact language is currently inevitable and motivating the need of further research in this direction. | Daniil Moskovskiy, Daryna Dementieva, Alexander Panchenko |  |
| 63 |  |  [MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering](https://doi.org/10.18653/v1/2022.acl-srw.27) |  | 0 | Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary tensor and move beyond the standard CP decomposition (CITATION) by using a data-specific generalized version of it (CITATION). The generalization of the standard CP-ALS algorithm allows obtaining optimization gradients without a backpropagation mechanism. It reduces the memory needed in training while providing computational benefits. We propose a MEKER, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering. | Viktoria Chekalina, Anton Razzhigaev, Albert Sayapin, Evgeny Frolov, Alexander Panchenko |  |
| 64 |  |  [Discourse on ASR Measurement: Introducing the ARPOCA Assessment Tool](https://doi.org/10.18653/v1/2022.acl-srw.28) |  | 0 | Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our daily applications. On the other hand, the models’ interpretability and their computational cost have become more challenging, particularly when dealing with less-common languages or identifying regional variations of speakers. This research proposal will follow a four-stage process: 1) Proving an overview of acoustic features and feature extraction algorithms; 2) Exploring current ASR models, tools, and performance assessment techniques; 3) Aligning features with interpretable phonetic transcripts; and 4) Designing a prototype ARPOCA to increase awareness of regional language variation and improve models feedback by developing a semi-automatic acoustic features extraction using PRAAT in conjunction with phonetic transcription. | Megan Merz, Olga Scrivner |  |
| 65 |  |  [Pretrained Knowledge Base Embeddings for improved Sentential Relation Extraction](https://doi.org/10.18653/v1/2022.acl-srw.29) |  | 0 | In this work we put forward to combine pretrained knowledge base graph embeddings with transformer based language models to improve performance on the sentential Relation Extraction task in natural language processing. Our proposed model is based on a simple variation of existing models to incorporate off-task pretrained graph embeddings with an on-task finetuned BERT encoder. We perform a detailed statistical evaluation of the model on standard datasets. We provide evidence that the added graph embeddings improve the performance, making such a simple approach competitive with the state-of-the-art models that perform explicit on-task training of the graph embeddings. Furthermore, we ob- serve for the underlying BERT model an interesting power-law scaling behavior between the variance of the F1 score obtained for a relation class and its support in terms of training examples. | Andrea Papaluca, Daniel Krefl, Hanna Suominen, Artem Lenskiy |  |
| 66 |  |  [Improving Cross-domain, Cross-lingual and Multi-modal Deception Detection](https://doi.org/10.18653/v1/2022.acl-srw.30) |  | 0 | With the increase of deception and misinformation especially in social media, it has become crucial to be able to develop machine learning methods to automatically identify deceptive language. In this proposal, we identify key challenges underlying deception detection in cross-domain, cross-lingual and multi-modal settings. To improve cross-domain deception classification, we propose to use inter-domain distance to identify a suitable source domain for a given target domain. We propose to study the efficacy of multilingual classification models vs translation for cross-lingual deception classification. Finally, we propose to better understand multi-modal deception detection and explore methods to weight and combine information from multiple modalities to improve multi-modal deception classification. | Subhadarshi Panda, Sarah Ita Levitan |  |
| 67 |  |  [Automatic Generation of Distractors for Fill-in-the-Blank Exercises with Round-Trip Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.31) |  | 0 | In a fill-in-the-blank exercise, a student is presented with a carrier sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. We propose to automatically generate distractors using round-trip neural machine translation: the carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence and its round-trip translation. We show that using hundreds of translations for a given sentence allows us to generate a rich set of challenging distractors. Further, using multiple pivot languages produces a diverse set of candidates. The distractors are evaluated against a real corpus of cloze exercises and checked manually for validity. We demonstrate that the proposed method significantly outperforms two strong baselines. | Subhadarshi Panda, Frank Palma Gomez, Michael Flor, Alla Rozovskaya |  |
| 68 |  |  [On the Locality of Attention in Direct Speech Translation](https://doi.org/10.18653/v1/2022.acl-srw.32) |  | 0 | Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we analyze the layer-wise token contributions in the self-attention of the encoder, unveiling local diagonal patterns. To prove that some attention weights are avoidable, we propose to substitute the standard self-attention with a local efficient one, setting the amount of context used based on the results of the analysis. With this approach, our model matches the baseline performance, and improves the efficiency by skipping the computation of those weights that standard attention discards. | Belen Alastruey, Javier Ferrando, Gerard I. Gállego, Marta R. Costajussà |  |
| 69 |  |  [Extraction of Diagnostic Reasoning Relations for Clinical Knowledge Graphs](https://doi.org/10.18653/v1/2022.acl-srw.33) |  | 0 | Clinical knowledge graphs lack meaningful diagnostic relations (e.g. comorbidities, sign/symptoms), limiting their ability to represent real-world diagnostic processes. Previous methods in biomedical relation extraction have focused on concept relations, such as gene-disease and disease-drug, and largely ignored clinical processes. In this thesis, we leverage a clinical reasoning ontology and propose methods to extract such relations from a physician-facing point-of-care reference wiki and consumer health resource texts. Given the lack of data labeled with diagnostic relations, we also propose new methods of evaluating the correctness of extracted triples in the zero-shot setting. We describe a process for the intrinsic evaluation of new facts by triple confidence filtering and clinician manual review, as well extrinsic evaluation in the form of a differential diagnosis prediction task. | Vimig Socrates |  |
| 70 |  |  [Scene-Text Aware Image and Text Retrieval with Dual-Encoder](https://doi.org/10.18653/v1/2022.acl-srw.34) |  | 0 | We tackle the tasks of image and text retrieval using a dual-encoder model in which images and text are encoded independently. This model has attracted attention as an approach that enables efficient offline inferences by connecting both vision and language in the same semantic space; however, whether an image encoder as part of a dual-encoder model can interpret scene-text (i.e., the textual information in images) is unclear. We propose pre-training methods that encourage a joint understanding of the scene-text and surrounding visual information. The experimental results demonstrate that our methods improve the retrieval performances of the dual-encoder models. | Shumpei Miyawaki, Taku Hasegawa, Kyosuke Nishida, Takuma Kato, Jun Suzuki |  |
| 71 |  |  [Towards Fine-grained Classification of Climate Change related Social Media Text](https://doi.org/10.18653/v1/2022.acl-srw.35) |  | 0 | With climate change becoming a cause of concern worldwide, it becomes essential to gauge people’s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets, ClimateStance and ClimateEng, consisting of 3777 tweets each, posted during the 2019 United Nations Framework Convention on Climate Change and comprehensively outline the dataset collection, annotation methodology, and dataset composition. Secondly, we propose the task of Climate Change stance detection based on our proposed ClimateStance dataset. Thirdly, we propose a fine-grained classification based on the ClimateEng dataset, classifying social media text into five categories: Disaster, Ocean/Water, Agriculture/Forestry, Politics, and General. We benchmark both the datasets for climate change stance detection and fine-grained classification using state-of-the-art methods in text classification. We also create a Reddit-based dataset for both the tasks, ClimateReddit, consisting of 6262 pseudo-labeled comments along with 329 manually annotated comments for the label. We then perform semi-supervised experiments for both the tasks and benchmark their results using the best-performing model for the supervised experiments. Lastly, we provide insights into the ClimateStance and ClimateReddit using part-of-speech tagging and named-entity recognition. | Roopal Vaid, Kartikey Pant, Manish Shrivastava |  |
| 72 |  |  [Deep Neural Representations for Multiword Expressions Detection](https://doi.org/10.18653/v1/2022.acl-srw.36) |  | 0 | Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing. Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus, while traditional methods use statistical measures. In our approach, we want to integrate the concepts of those two approaches. We present a novel weakly supervised multiword expressions extraction method which focuses on their behaviour in various contexts. Our method uses a lexicon of English multiword lexical units acquired from The Oxford Dictionary of English as a reference knowledge base and leverages neural language modelling with deep learning architectures. In our approach, we do not need a corpus annotated specifically for the task. The only required components are: a lexicon of multiword units, a large corpus, and a general contextual embeddings model. We propose a method for building a silver dataset by spotting multiword expression occurrences and acquiring statistical collocations as negative samples. Sample representation has been inspired by representations used in Natural Language Inference and relation recognition. Very good results (F1=0.8) were obtained with CNN network applied to individual occurrences followed by weighted voting used to combine results from the whole corpus. The proposed method can be quite easily applied to other languages. | Kamil Kanclerz, Maciej Piasecki |  |
| 73 |  |  [A Checkpoint on Multilingual Misogyny Identification](https://doi.org/10.18653/v1/2022.acl-srw.37) |  | 0 | We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a transfer learning approach across multiple languages. That is, we train monolingual transformers with monolingual data, and multilingual transformers with both monolingual and multilingual data. Our models reach state-of-the-art performance on all three languages. The single-language BERT models perform the best, closely followed by different configurations of multilingual BERT models. The performance drops in zero-shot classification across languages. Our error analysis shows that multilingual and monolingual models tend to make the same mistakes. | Arianna Muti, Alberto BarrónCedeño |  |
| 74 |  |  [Using dependency parsing for few-shot learning in distributional semantics](https://doi.org/10.18653/v1/2022.acl-srw.38) |  | 0 | In this work, we explore the novel idea of employing dependency parsing information in the context of few-shot learning, the task of learning the meaning of a rare word based on a limited amount of context sentences. Firstly, we use dependency-based word embedding models as background spaces for few-shot learning. Secondly, we introduce two few-shot learning methods which enhance the additive baseline model by using dependencies. | Stefania Preda, Guy Emerson |  |
| 75 |  |  [A Dataset and BERT-based Models for Targeted Sentiment Analysis on Turkish Texts](https://doi.org/10.18653/v1/2022.acl-srw.39) |  | 0 | Targeted Sentiment Analysis aims to extract sentiment towards a particular target from a given text. It is a field that is attracting attention due to the increasing accessibility of the Internet, which leads people to generate an enormous amount of data. Sentiment analysis, which in general requires annotated data for training, is a well-researched area for widely studied languages such as English. For low-resource languages such as Turkish, there is a lack of such annotated data. We present an annotated Turkish dataset suitable for targeted sentiment analysis. We also propose BERT-based models with different architectures to accomplish the task of targeted sentiment analysis. The results demonstrate that the proposed models outperform the traditional sentiment analysis models for the targeted sentiment analysis task. | Mustafa Melih Mutlu, Arzucan Özgür |  |
| 76 |  |  [Frontmatter](https://aclanthology.org/2022.acl-short.0) |  | 0 |  |  |  |
| 77 |  |  [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://doi.org/10.18653/v1/2022.acl-short.1) |  | 0 | We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge. | Elad Ben Zaken, Yoav Goldberg, Shauli Ravfogel |  |
| 78 |  |  [Are Shortest Rationales the Best Explanations for Human Understanding?](https://doi.org/10.18653/v1/2022.acl-short.2) |  | 0 | Existing self-explaining models typically favor extracting the shortest possible rationales — snippets of an input text “responsible for” corresponding output — to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales. | Hua Shen, Tongshuang Wu, Wenbo Guo, TingHao Kenneth Huang |  |
| 79 |  |  [Analyzing Wrap-Up Effects through an Information-Theoretic Lens](https://doi.org/10.18653/v1/2022.acl-short.3) |  | 0 | Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence–or absence–of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects. | Clara Meister, Tiago Pimentel, Thomas Hikaru Clark, Ryan Cotterell, Roger Levy |  |
| 80 |  |  [Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-short.4) |  | 0 | Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via a machine reading comprehension (MRC) framework with two phases. The first phase employs an argument mining (AM) query to identify all arguments in two documents. The second phase considers each identified argument as an APE query to extract its paired arguments from another document, allowing to better capture the argument-level interactions. Also, this framework enables these two phases to be jointly trained in a single MRC model, thereby maximizing the mutual benefits of them. Experimental results demonstrate that our approach achieves the best performance, outperforming the state-of-the-art method by 7.11% in F1 score. | Jianzhu Bao, Jingyi Sun, Qinglin Zhu, Ruifeng Xu |  |
| 81 |  |  [High probability or low information? The probability-quality paradox in language generation](https://doi.org/10.18653/v1/2022.acl-short.5) |  | 0 | When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text—covering multiple tasks and common decoding strategies—suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance. | Clara Meister, Gian Wiher, Tiago Pimentel, Ryan Cotterell |  |
| 82 |  |  [Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-short.6) |  | 0 | Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning framework. We aim to bridge the gap between IND pre-training and OOD clustering. Experiments and analysis on two benchmark datasets show the effectiveness of our method. | Yutao Mou, Keqing He, Yanan Wu, Zhiyuan Zeng, Hong Xu, Huixing Jiang, Wei Wu, Weiran Xu |  |
| 83 |  |  [Voxel-informed Language Grounding](https://doi.org/10.18653/v1/2022.acl-short.7) |  | 0 | Natural language applied to natural 2D images describes a fundamentally 3D world. We present the Voxel-informed Language Grounder (VLG), a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model. We show that VLG significantly improves grounding accuracy on SNARE, an object reference game task. At the time of writing, VLG holds the top place on the SNARE leaderboard, achieving SOTA results with a 2.0% absolute improvement. | Rodolfo Corona, Shizhan Zhu, Dan Klein, Trevor Darrell |  |
| 84 |  |  [P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks](https://doi.org/10.18653/v1/2022.acl-short.8) |  | 0 | Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. | Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, Jie Tang |  |
| 85 |  |  [On Efficiently Acquiring Annotations for Multilingual Models](https://doi.org/10.18653/v1/2022.acl-short.9) |  | 0 | When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple languages using a single model performs substantially better than the aforementioned alternatives. We also demonstrate that active learning provides additional, complementary benefits. We show that this simple approach enables the model to be data efficient by allowing it to arbitrate its annotation budget to query languages it is less certain on. We illustrate the effectiveness of our proposed method on a diverse set of tasks: a classification task with 4 languages, a sequence tagging task with 4 languages and a dependency parsing task with 5 languages. Our proposed method, whilst simple, substantially outperforms the other viable alternatives for building a model in a multilingual setting under constrained budgets. | Joel Ruben Antony Moniz, Barun Patra, Matthew Gormley |  |
| 86 |  |  [Automatic Detection of Entity-Manipulated Text using Factual Knowledge](https://doi.org/10.18653/v1/2022.acl-short.10) |  | 0 | In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection. | Ganesh Jawahar, Muhammad AbdulMageed, Laks V. S. Lakshmanan |  |
| 87 |  |  [Does BERT Know that the IS-A Relation Is Transitive?](https://doi.org/10.18653/v1/2022.acl-short.11) |  | 0 | The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERT’s predictions do not fully obey the transitivity property of the IS-A relation. | Ruixi Lin, Hwee Tou Ng |  |
| 88 |  |  [Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-short.12) |  | 0 | Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines . | Chengyu Chuang, Yi Yang |  |
| 89 |  |  [Pixie: Preference in Implicit and Explicit Comparisons](https://doi.org/10.18653/v1/2022.acl-short.13) |  | 0 | We present Pixie, a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews. Unlike previous studies on preference classification, Pixie contains implicit (omitting an entity being compared) and indirect (lacking comparative linguistic cues) comparisons. We find that transformer-based pretrained models, finetuned on Pixie, achieve a weighted average F1 score of 83.34% and outperform the existing state-of-the-art preference classification model (73.99%). | Amanul Haque, Vaibhav Garg, Hui Guo, Munindar P. Singh |  |
| 90 |  |  [Counterfactual Explanations for Natural Language Interfaces](https://doi.org/10.18653/v1/2022.acl-short.14) |  | 0 | A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have minimally modified their utterance to achieve their desired goal. In particular, the user provides an utterance along with a demonstration of their desired goal; then, our algorithm synthesizes a paraphrase of their utterance that is guaranteed to achieve their goal. In two user studies, we demonstrate that our approach substantially improves user performance, and that it generates explanations that more closely match the user’s intent compared to two ablations. | George Tolkachev, Stephen Mell, Steve Zdancewic, Osbert Bastani |  |
| 91 |  |  [Predicting Difficulty and Discrimination of Natural Language Questions](https://doi.org/10.18653/v1/2022.acl-short.15) |  | 0 | Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across various datasets (Lalor et al., 2019; Vania et al., 2021; Rodriguez et al., 2021). In this work, we explore predictive models for directly estimating and explaining these traits for natural language questions in a question-answering context. We use HotpotQA for illustration. Our experiments show that it is possible to predict both difficulty and discrimination parameters for new questions, and these traits are correlated with features of questions, answers, and associated contexts. Our findings can have significant implications for the creation of new datasets and tests on the one hand and strategies such as active learning and curriculum learning on the other. | Matthew Byrd, Shashank Srivastava |  |
| 92 |  |  [How does the pre-training objective affect what large language models learn about linguistic properties?](https://doi.org/10.18653/v1/2022.acl-short.16) |  | 0 | Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training. | Ahmed Alajrami, Nikolaos Aletras |  |
| 93 |  |  [The Power of Prompt Tuning for Low-Resource Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-short.17) |  | 0 | Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution. | Nathan Schucher, Siva Reddy, Harm de Vries |  |
| 94 |  |  [Data Contamination: From Memorization to Exploitation](https://doi.org/10.18653/v1/2022.acl-short.18) |  | 0 | Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream datasets, and fine-tune them on the relevant task. Comparing performance between samples seen and unseen during pretraining enables us to define and quantify levels of memorization and exploitation. Experiments with two models and three downstream tasks show that exploitation exists in some cases, but in others the models memorize the contaminated data, but do not exploit it. We show that these two measures are affected by different factors such as the number of duplications of the contaminated data and the model size. Our results highlight the importance of analyzing massive web-scale datasets to verify that progress in NLP is obtained by better language understanding and not better data exploitation. | Inbal Magar, Roy Schwartz |  |
| 95 |  |  [Detecting Annotation Errors in Morphological Data with the Transformer](https://doi.org/10.18653/v1/2022.acl-short.19) |  | 0 | Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data. In this paper, we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms. We evaluate our error detection model on four languages by introducing three different types of artificial errors in the data: (1) typographic errors, where single characters in the data are inserted, replaced, or deleted; (2) linguistic confusion errors where two inflected forms are systematically swapped; and (3) self-adversarial errors where the Transformer model itself is used to generate plausible-looking, but erroneous forms by retrieving high-scoring predictions from the search beam. Results show that the Transformer model can with perfect, or near-perfect recall detect errors in all three scenarios, even when significant amounts of the annotated data (5%-30%) are corrupted on all languages tested. Precision varies across the languages and types of errors, but is high enough that the model can be very effectively used to flag suspicious entries in large data sets for further scrutiny by human annotators. | Ling Liu, Mans Hulden |  |
| 96 |  |  [Estimating the Entropy of Linguistic Distributions](https://doi.org/10.18653/v1/2022.acl-short.20) |  | 0 | Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies. | Aryaman Arora, Clara Meister, Ryan Cotterell |  |
| 97 |  |  [Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study](https://doi.org/10.18653/v1/2022.acl-short.21) |  | 0 | In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution for such cases and expand the UniMorph annotation schema to naturally address this phenomenon, in which verbs agree with multiple arguments using true affixes. We apply this extended schema to one such language, Georgian, and provide a human-verified, accurate and balanced morphological dataset for Georgian verbs. The dataset has 4 times more tables and 6 times more verb forms compared to the existing UniMorph dataset, covering all possible variants of argument marking, demonstrating the adequacy of our proposed scheme. Experiments on a reinflection task show that generalization is easy when the data is split at the form level, but extremely hard when splitting along lemma lines. Expanding the other languages in UniMorph according to this schema is expected to improve both the coverage, consistency and interpretability of this benchmark. | David Guriel, Omer Goldman, Reut Tsarfaty |  |
| 98 |  |  [DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization](https://doi.org/10.18653/v1/2022.acl-short.22) |  | 0 | Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks. | Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew O. Arnold, Bing Xiang, Dan Roth |  |
| 99 |  |  [Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension](https://doi.org/10.18653/v1/2022.acl-short.23) |  | 0 | Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a zero-shot dialogue comprehension model, we develop a novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input. However, the dialogue-narrative parallel corpus for such a pre-training strategy is currently unavailable. For this reason, we first construct a dialogue-narrative parallel corpus by automatically aligning movie subtitles and their synopses. We then pre-train a BART model on the data and evaluate its performance on four dialogue-based tasks that require comprehension. Experimental results show that our model not only achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities. The data and code are available at https://github.com/zhaochaocs/Diana. | Chao Zhao, Wenlin Yao, Dian Yu, Kaiqiang Song, Dong Yu, Jianshu Chen |  |
| 100 |  |  [Kronecker Decomposition for GPT Compression](https://doi.org/10.18653/v1/2022.acl-short.24) |  | 0 | GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly. | Ali Edalati, Marzieh S. Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh |  |
| 101 |  |  [Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction](https://doi.org/10.18653/v1/2022.acl-short.25) |  | 0 | A key challenge in attribute value extraction (AVE) from e-commerce sites is how to handle a large number of attributes for diverse products. Although this challenge is partially addressed by a question answering (QA) approach which finds a value in product data for a given query (attribute), it does not work effectively for rare and ambiguous queries. We thus propose simple knowledge-driven query expansion based on possible answers (values) of a query (attribute) for QA-based AVE. We retrieve values of a query (attribute) from the training data to expand the query. We train a model with two tricks, knowledge dropout and knowledge token mixing, which mimic the imperfection of the value knowledge in testing. Experimental results on our cleaned version of AliExpress dataset show that our method improves the performance of AVE (+6.08 macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro F1, respectively). | Keiji Shinzato, Naoki Yoshinaga, Yandi Xia, WeiTe Chen |  |
| 102 |  |  [Event-Event Relation Extraction using Probabilistic Box Embedding](https://doi.org/10.18653/v1/2022.acl-short.26) |  | 0 | To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line “died” after “storm”, then it is obvious that the “storm” happened before the “died”. Current framework of event relation extraction do not guarantee this coherence and thus enforces it via constraint loss function (Wang et al., 2020). In this work, we propose to modify the underlying ERE model to guarantee coherence by representing each event as a box representation (BERE) without applying explicit constraints. From our experiments, BERE also shows stronger conjunctive constraint satisfaction while performing on par or better in F1 compared to previous models with constraint injection. | EunJeong Hwang, JayYoon Lee, Tianyi Yang, Dhruvesh Patel, Dongxu Zhang, Andrew McCallum |  |
| 103 |  |  [Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation](https://doi.org/10.18653/v1/2022.acl-short.27) |  | 0 | End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data augmentation that leverages audio alignments, linguistic properties, and translation. First, we augment a transcription by sampling from a suffix memory that stores text and audio data. Second, we translate the augmented transcript. Finally, we recombine concatenated audio segments and the generated translation. Our method delivers consistent improvements of up to 0.9 and 1.1 BLEU points on top of augmentation with knowledge distillation on five language pairs on CoVoST 2 and on two language pairs on Europarl-ST, respectively. | Tsz Kin Lam, Shigehiko Schamoni, Stefan Riezler |  |
| 104 |  |  [Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure](https://doi.org/10.18653/v1/2022.acl-short.28) |  | 0 | Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function roles in telling a news story, for predicting sentence deletion. We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions, either as additional features or by jointly predicting sentence deletions and sentence categories. Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5% and 10.7% respectively using the two methods, and improves the overall F1-score by 3.6% and 4.3% respectively. | Bohan Zhang, Prafulla Kumar Choubey, Ruihong Huang |  |
| 105 |  |  [Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer](https://doi.org/10.18653/v1/2022.acl-short.29) |  | 0 | We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which consists of two training strategies that target adaptation to both language and task. Our approach achieves competitive performance without monolingual task-specific parallel data and can be applied to other style transfer tasks as well as to other languages. | Huiyuan Lai, Antonio Toral, Malvina Nissim |  |
| 106 |  |  [When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning](https://doi.org/10.18653/v1/2022.acl-short.30) |  | 0 | Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks. | Orion Weller, Kevin D. Seppi, Matt Gardner |  |
| 107 |  |  [Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.acl-short.31) |  | 0 | Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and have unsatisfactory generalization capability. In this paper, we propose a new approach to leveraging explicit lexico-logical alignments. It first identifies possible phrase-level alignments and injects them as additional contexts to guide the parsing procedure. Experimental results on Squall show that our approach can make better use of such alignments and obtains an absolute improvement of 3.4% compared with the current state-of-the-art. | Runxin Sun, Shizhu He, Chong Zhu, Yaohan He, Jinlong Li, Jun Zhao, Kang Liu |  |
| 108 |  |  [Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.acl-short.32) |  | 0 | A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and time-variability. Existing models for TKG reasoning focus on modeling fact sequences of a fixed length, which cannot discover complex evolutional patterns that vary in length. Furthermore, these models are all trained offline, which cannot well adapt to the changes of evolutional patterns from then on. Thus, we propose a new model, called Complex Evolutional Network (CEN), which uses a length-aware Convolutional Neural Network (CNN) to handle evolutional patterns of different lengths via an easy-to-difficult curriculum learning strategy. Besides, we propose to learn the model under the online setting so that it can adapt to the changes of evolutional patterns over time. Extensive experiments demonstrate that CEN obtains substantial performance improvement under both the traditional offline and the proposed online settings. | Zixuan Li, Saiping Guan, Xiaolong Jin, Weihua Peng, Yajuan Lyu, Yong Zhu, Long Bai, Wei Li, Jiafeng Guo, Xueqi Cheng |  |
| 109 |  |  [Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-short.33) |  | 0 | Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. The trained model predicts “accumulated” belief states in every turn, and joint goal accuracy and slot accuracy are mainly used to evaluate the prediction; however, we specify that the current evaluation metrics have a critical limitation when evaluating belief states accumulated as the dialogue proceeds, especially in the most used MultiWOZ dataset. Additionally, we propose relative slot accuracy to complement existing metrics. Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog. This study also encourages not solely the reporting of joint goal accuracy, but also various complementary metrics in DST tasks for the sake of a realistic evaluation. | Takyoung Kim, Hoonsang Yoon, Yukyung Lee, Pilsung Kang, Misuk Kim |  |
| 110 |  |  [LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory](https://doi.org/10.18653/v1/2022.acl-short.34) |  | 0 | LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes LM-BFF-MS—better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions, which include 1) prompts with multiple demonstrations based on automatic generation of multiple label words; and 2) soft demonstration memory which consists of multiple sequences of globally shared word embeddings for a similar context. Experiments conducted on eight NLP tasks show that LM-BFF-MS leads to improvements over LM-BFF on five tasks, particularly achieving 94.0 and 90.4 on SST-2 and MRPC, respectively. | Eunhwan Park, Dong Hyeon Jeon, Seonhoon Kim, Inho Kang, SeungHoon Na |  |
| 111 |  |  [Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances](https://doi.org/10.18653/v1/2022.acl-short.35) |  | 0 | Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief state, it is difficult to get a correct prediction once a misprediction has occurred. Thus, although being a useful metric, it can be harsh at times and underestimate the true potential of a DST model. Moreover, an improvement in JGA can sometimes decrease the performance of turn-level or non-cumulative belief state prediction due to inconsistency in annotations. So, using JGA as the only metric for model selection may not be ideal for all scenarios. In this work, we discuss various evaluation metrics used for DST along with their shortcomings. To address the existing issues, we propose a new evaluation metric named Flexible Goal Accuracy (FGA). FGA is a generalized version of JGA. But unlike JGA, it tries to give penalized rewards to mispredictions that are locally correct i.e. the root cause of the error is an earlier turn. By doing so, FGA considers the performance of both cumulative and turn-level prediction flexibly and provides a better insight than the existing metrics. We also show that FGA is a better discriminator of DST model performance. | Suvodip Dey, Ramamohan Kummara, Maunendra Sankar Desarkar |  |
| 112 |  |  [Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task](https://doi.org/10.18653/v1/2022.acl-short.36) |  | 0 | As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset. Specifically, none of the existing few-shot approaches (including the in-context learning of GPT-3) can attain a performance that is meaningfully different from the random baseline. Trying to fill this gap, we propose a new prompting technique, based on similarity metrics, which boosts few-shot performance to the level of fully supervised methods. Our simple adaptation shows that the failure of existing prompt-based techniques in semantic distinction is due to their improper configuration, rather than lack of relevant knowledge in the representations. We also show that this approach can be effectively extended to other downstream tasks for which a single prompt is sufficient. | Mohsen Tabasi, Kiamehr Rezaee, Mohammad Taher Pilehvar |  |
| 113 |  |  [Hierarchical Curriculum Learning for AMR Parsing](https://doi.org/10.18653/v1/2022.acl-short.37) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the model generalization. To bridge this gap, we propose a Hierarchical Curriculum Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula (IC). SC switches progressively from core to detail AMR semantic elements while IC transits from structure-simple to -complex AMR instances during training. Through these two warming-up processes, HCL reduces the difficulty of learning complex structures, thus the flat model can better adapt to the AMR hierarchy. Extensive experiments on AMR2.0, AMR3.0, structure-complex and out-of-distribution situations verify the effectiveness of HCL. | Peiyi Wang, Liang Chen, Tianyu Liu, Damai Dai, Yunbo Cao, Baobao Chang, Zhifang Sui |  |
| 114 |  |  [PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.acl-short.38) |  | 0 | Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (PARE) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using BERT. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query – this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art DS-RE models in both monolingual and multilingual DS-RE datasets. | Vipul Rathore, Kartikeya Badola, Parag Singla, Mausam |  |
| 115 |  |  [To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo](https://doi.org/10.18653/v1/2022.acl-short.39) |  | 0 | We present a debiased dataset for the Person-centric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who’s Waldo dataset. Given an image and a caption, PCVG requires pairing up a person’s name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who’s Waldo dataset compiled for this task contains a large number of biased samples that are solvable simply by heuristic methods; for instance, in many cases the first name in the sentence corresponds to the largest bounding box, or the sequence of names in the sentence corresponds to an exact left-to-right order in the image. Naturally, models trained on these biased data lead to over-estimation of performance on the benchmark. To enforce models being correct for the correct reasons, we design automated tools to filter and debias the original dataset by ruling out all examples of insufficient context, such as those with no verb or with a long chain of conjunct names in their captions. Our experiments show that our new sub-sampled dataset contains less bias with much lowered heuristic performances and widened gaps between heuristic and supervised methods. We also demonstrate the same benchmark model trained on our debiased training set outperforms that trained on the original biased (and larger) training set on our debiased test set. We argue our debiased dataset offers the PCVG task a more practical baseline for reliable benchmarking and future improvements. | Yiran Luo, Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Chitta Baral |  |
| 116 |  |  [Translate-Train Embracing Translationese Artifacts](https://doi.org/10.18653/v1/2022.acl-short.40) |  | 0 | Translate-train is a general training approach to multilingual tasks. The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages. However, its performance is often hampered by the artifacts in the translated texts (translationese). We discover that such artifacts have common patterns in different languages and can be modeled by deep learning, and subsequently propose an approach to conduct translate-train using Translationese Embracing the effect of Artifacts (TEA). TEA learns to mitigate such effect on the training data of a source language (whose original and translationese are both available), and applies the learned module to facilitate the inference on the target language. Extensive experiments on the multilingual QA dataset TyDiQA demonstrate that TEA outperforms strong baselines. | Sicheng Yu, Qianru Sun, Hao Zhang, Jing Jiang |  |
| 117 |  |  [C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References](https://doi.org/10.18653/v1/2022.acl-short.41) |  | 0 | We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match. | Xiang Yue, Xiaoman Pan, Wenlin Yao, Dian Yu, Dong Yu, Jianshu Chen |  |
| 118 |  |  [k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations](https://doi.org/10.18653/v1/2022.acl-short.42) |  | 0 | Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many Natural Language Processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these instances, the data reliability is under-reported, and a proposed k-rater reliability (kRR) should be used as the correct data reliability for aggregated datasets. It is a multi-rater generalization of inter-rater reliability (IRR). We conducted two replications of the WordSim-353 benchmark, and present empirical, analytical, and bootstrap-based methods for computing kRR on WordSim-353. These methods produce very similar results. We hope this discussion will nudge researchers to report kRR in addition to IRR. | Ka Wong, Praveen K. Paritosh |  |
| 119 |  |  [An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers](https://doi.org/10.18653/v1/2022.acl-short.43) |  | 0 | We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise. | Valentin Hofmann, Hinrich Schütze, Janet B. Pierrehumbert |  |
| 120 |  |  [SCD: Self-Contrastive Decorrelation of Sentence Embeddings](https://doi.org/10.18653/v1/2022.acl-short.44) |  | 0 | In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods. | Tassilo Klein, Moin Nabi |  |
| 121 |  |  [Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words](https://doi.org/10.18653/v1/2022.acl-short.45) |  | 0 | Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case. | Kaitlyn Zhou, Kawin Ethayarajh, Dallas Card, Dan Jurafsky |  |
| 122 |  |  [Revisiting the Compositional Generalization Abilities of Neural Sequence Models](https://doi.org/10.18653/v1/2022.acl-short.46) |  | 0 | Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We demonstrate that modifying the training distribution in simple and intuitive ways enables standard seq-to-seq models to achieve near-perfect generalization performance, thereby showing that their compositional generalization abilities were previously underestimated. We perform detailed empirical analysis of this phenomenon. Our results indicate that the generalization performance of models is highly sensitive to the characteristics of the training data which should be carefully considered while designing such benchmarks in future. | Arkil Patel, Satwik Bhattamishra, Phil Blunsom, Navin Goyal |  |
| 123 |  |  [A Copy-Augmented Generative Model for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.acl-short.47) |  | 0 | Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both extractive and generative readers. In particular, our model is built upon the powerful generative model FiD (CITATION). We enhance the original generative reader by incorporating a pointer network to encourage the model to directly copy words from the retrieved passages. We conduct experiments on the two benchmark datasets, Natural Questions and TriviaQA, and the empirical results demonstrate the performance gains of our proposed approach. | Shuang Liu, Dong Wang, Xiaoguang Li, Minghui Huang, Meizhen Ding |  |
| 124 |  |  [Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation](https://doi.org/10.18653/v1/2022.acl-short.48) |  | 0 | Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by humans. To tackle this problem, we propose a simple but effective Document Augmentation for dense Retrieval (DAR) framework, which augments the representations of documents with their interpolation and perturbation. We validate the performance of DAR on retrieval tasks with two benchmark datasets, showing that the proposed DAR significantly outperforms relevant baselines on the dense retrieval of both the labeled and unlabeled documents. | Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park |  |
| 125 |  |  [WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language](https://doi.org/10.18653/v1/2022.acl-short.49) |  | 0 | Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to attention the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training. | Federico Tavella, Viktor Schlegel, Marta Romeo, Aphrodite Galata, Angelo Cangelosi |  |
| 126 |  |  [Investigating person-specific errors in chat-oriented dialogue systems](https://doi.org/10.18653/v1/2022.acl-short.50) |  | 0 | Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from large-scale dialogue data of a specific character, i.e., target person, and analyzed errors related to that person. We found that person-specific errors can be divided into two types: errors in attributes and those in relations, each of which can be divided into two levels: self and other. The correspondence with an existing taxonomy of errors was also investigated, and person-specific errors that should be addressed in the future were clarified. | Koh Mitsuda, Ryuichiro Higashinaka, Tingxuan Li, Sen Yoshida |  |
| 127 |  |  [Direct parsing to sentiment graphs](https://doi.org/10.18653/v1/2022.acl-short.51) |  | 0 | This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions. | David Samuel, Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja Øvrelid, Erik Velldal |  |
| 128 |  |  [XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding](https://doi.org/10.18653/v1/2022.acl-short.52) |  | 0 | Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders’ success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of NLU. After training with a small number of extra adapting steps and finetuned, the proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in general language understanding evaluation (GLUE), situations with adversarial generations (SWAG) benchmarks, and readability benchmarks. We analyze the performance of XDBERT on GLUE to show that the improvement is likely visually grounded. | ChanJan Hsu, Hungyi Lee, Yu Tsao |  |
| 129 |  |  [As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning](https://doi.org/10.18653/v1/2022.acl-short.53) |  | 0 | Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This allows to pinpoint superfluous words in the translation and untranslated words in the source even in the absence of a reference translation. The accuracy of our method is comparable to a supervised method that requires a custom quality estimation model. | Jannis Vamvas, Rico Sennrich |  |
| 130 |  |  [How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks](https://doi.org/10.18653/v1/2022.acl-short.54) |  | 0 | This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar. | Bingzhi Li, Guillaume Wisniewski, Benoît Crabbé |  |
| 131 |  |  [Machine Translation for Livonian: Catering to 20 Speakers](https://doi.org/10.18653/v1/2022.acl-short.55) |  | 0 | Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other – enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. We rely on Livonian’s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released via OPUS and Huggingface repositories. | Matiss Rikters, Marili Tomingas, Tuuli Tuisk, Valts Ernstreits, Mark Fishel |  |
| 132 |  |  [Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games](https://doi.org/10.18653/v1/2022.acl-short.56) |  | 0 | Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large action space when the agent has not yet acquired enough knowledge about the environment. We propose CommExpl, an exploration technique that injects external commonsense knowledge, via a pretrained language model (LM), into the agent during training when the agent is the most uncertain about its next action. Our method exhibits improvement on the collected game scores during the training in four out of nine games from Jericho. Additionally, the produced trajectory of actions exhibit lower perplexity, when tested with a pretrained LM, indicating better closeness to human language. | Dongwon Ryu, Ehsan Shareghi, Meng Fang, Yunqiu Xu, Shirui Pan, Gholamreza Haffari |  |
| 133 |  |  [A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-short.57) |  | 0 | Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be compatibly plugged as inputs to infuse supplemental entity knowledge into PLMs. Compared to previous knowledge-enhanced PLMs, PELT only requires 0.2%-5% pre-computation with capability of acquiring knowledge from out-of-domain corpora for domain adaptation scenario. The experiments on knowledge-related tasks demonstrate that our method, PELT, can flexibly and effectively transfer entity knowledge from related corpora into PLMs with different architectures. Our code and models are publicly available at https://github.com/thunlp/PELT | Deming Ye, Yankai Lin, Peng Li, Maosong Sun, Zhiyuan Liu |  |
| 134 |  |  [S$^4$-Tuning: A Simple Cross-lingual Sub-network Tuning Method](https://doi.org/10.18653/v1/2022.acl-short.58) |  | 0 | The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples. However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios. To address two problems elegantly, we propose S4-Tuning, a Simple Cross-lingual Sub-network Tuning method. S4-Tuning first detects the most essential sub-network for each target language, and only updates it during fine-tuning.In this way, the language sub-networks lower the scale of trainable parameters, and hence better suit the low-resource scenarios.Meanwhile, the commonality and characteristics across languages are modeled by the overlapping and non-overlapping parts to ease the interference among languages.Simple but effective, S4-Tuning gains consistent improvements over vanilla fine-tuning on three multi-lingual tasks involving 37 different languages in total (XNLI, PAWS-X, and Tatoeba). | Runxin Xu, Fuli Luo, Baobao Chang, Songfang Huang, Fei Huang |  |
| 135 |  |  [Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification](https://doi.org/10.18653/v1/2022.acl-short.59) |  | 0 | Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are completely insensitive to potentially large errors in this certainty region of interest. We introduce a region-balanced calibration error metric that weights all certainty regions equally. When low and mid certainty estimates are taken into account, calibration error is typically larger than previously reported. We introduce a simple extension of temperature scaling, requiring no additional computation, that can reduce both traditional and region-balanced notions of calibration error over existing baselines. | Hillary Dawkins, Isar Nejadgholi |  |
| 136 |  |  [Developmental Negation Processing in Transformer Language Models](https://doi.org/10.18653/v1/2022.acl-short.60) |  | 0 | Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed. | Antonio Laverghetta Jr., John Licato |  |
| 137 |  |  [Canary Extraction in Natural Language Understanding Models](https://doi.org/10.18653/v1/2022.acl-short.61) |  | 0 | Natural Language Understanding (NLU) models can be trained on sensitive information such as phone numbers, zip-codes etc. Recent literature has focused on Model Inversion Attacks (ModIvA) that can extract training data from model parameters. In this work, we present a version of such an attack by extracting canaries inserted in NLU training data. In the attack, an adversary with open-box access to the model reconstructs the canaries contained in the model’s training set. We evaluate our approach by performing text completion on canaries and demonstrate that by using the prefix (non-sensitive) tokens of the canary, we can generate the full canary. As an example, our attack is able to reconstruct a four digit code in the training dataset of the NLU model with a probability of 0.5 in its best configuration. As countermeasures, we identify several defense mechanisms that, when combined, effectively eliminate the risk of ModIvA in our experiments. | Rahil Parikh, Christophe Dupuy, Rahul Gupta |  |
| 138 |  |  [On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations](https://doi.org/10.18653/v1/2022.acl-short.62) |  | 0 | Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics. | Yang Trista Cao, Yada Pruksachatkun, KaiWei Chang, Rahul Gupta, Varun Kumar, Jwala Dhamala, Aram Galstyan |  |
| 139 |  |  [Sequence-to-sequence AMR Parsing with Ancestor Information](https://doi.org/10.18653/v1/2022.acl-short.63) |  | 0 | AMR parsing is the task that maps a sentence to an AMR semantic graph automatically. The difficulty comes from generating the complex graph structure. The previous state-of-the-art method translates the AMR graph into a sequence, then directly fine-tunes a pretrained sequence-to-sequence Transformer model (BART). However, purely treating the graph as a sequence does not take advantage of structural information about the graph. In this paper, we design several strategies to add the important ancestor information into the Transformer Decoder. Our experiments show that we can improve the performance for both AMR 2.0 and AMR 3.0 dataset and achieve new state-of-the-art results. | Chen Yu, Daniel Gildea |  |
| 140 |  |  [Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning](https://doi.org/10.18653/v1/2022.acl-short.64) |  | 0 | Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt a method from multi-task learning, which relies on automated curriculum learning, to dynamically optimize for parsing performance on outlier languages. We show that this approach is significantly better than uniform and size-proportional sampling in the zero-shot setting. | Miryam de Lhoneux, Sheng Zhang, Anders Søgaard |  |
| 141 |  |  [PriMock57: A Dataset Of Primary Care Mock Consultations](https://doi.org/10.18653/v1/2022.acl-short.65) |  | 0 | Recent advances in Automatic Speech Recognition (ASR) have made it possible to reliably produce automatic transcripts of clinician-patient conversations. However, access to clinical datasets is heavily restricted due to patient privacy, thus slowing down normal research practices. We detail the development of a public access, high quality dataset comprising of 57 mocked primary care consultations, including audio recordings, their manual utterance-level transcriptions, and the associated consultation notes. Our work illustrates how the dataset can be used as a benchmark for conversational medical ASR as well as consultation note generation from transcripts. | Alex PapadopoulosKorfiatis, Francesco Moramarco, Radmila Sarac, Aleksandar Savkov |  |
| 142 |  |  [UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue](https://doi.org/10.18653/v1/2022.acl-short.66) |  | 0 | The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However, such pipeline methods would unavoidably suffer from the error propagation issue. This paper proposes to unify these two sub-tasks via sequentially generating the grounding knowledge and the response. We further develop a prompt-connected multi-task learning strategy to model the characteristics and connections of different tasks and introduce linear temperature scheduling to reduce the negative effect of irrelevant document information. Experimental results demonstrate the effectiveness of our framework. | Chang Gao, Wenxuan Zhang, Wai Lam |  |
| 143 |  |  [DMix: Adaptive Distance-aware Interpolative Mixup](https://doi.org/10.18653/v1/2022.acl-short.67) |  | 0 | Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities. We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations. We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities. | Ramit Sawhney, Megh Thakkar, Shrey Pandit, Ritesh Soun, Di Jin, Diyi Yang, Lucie Flek |  |
| 144 |  |  [Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.68) |  | 0 | We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method, so as to improve low-resource machine translation. We conduct experiments on benchmark datasets of My-En, Id-En and Tr-En translation scenarios. The test results show that our method produces substantial improvements, achieving the BLEU scores of 22.5, 28.0 and 18.1 respectively. In addition, the method is computationally efficient which reduces the consumption of training time by 63.8%, reaching the duration of 1.6 hours when training on a Tesla 16GB P100 GPU. All the models and source codes in the experiments will be made publicly available to support reproducible research. | Minhan Xu, Yu Hong |  |
| 145 |  |  [HYPHEN: Hyperbolic Hawkes Attention For Text Streams](https://doi.org/10.18653/v1/2022.acl-short.69) |  | 0 | Analyzing the temporal sequence of texts from sources such as social media, news, and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities. We propose a Hyperbolic Hawkes Attention Network (HYPHEN), which learns a data-driven hyperbolic space and models irregular powerlaw excitations using a hyperbolic Hawkes process. Through quantitative and exploratory experiments over financial NLP, suicide ideation detection, and political debate analysis we demonstrate HYPHEN’s practical applicability for modeling online text sequences in a geometry agnostic manner. | Shivam Agarwal, Ramit Sawhney, Sanchit Ahuja, Ritesh Soun, Sudheer Chava |  |
| 146 |  |  [A Risk-Averse Mechanism for Suicidality Assessment on Social Media](https://doi.org/10.18653/v1/2022.acl-short.70) |  | 0 | Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe consequences. We hence reformulate suicide risk assessment as a selective prioritized prediction problem over the Columbia Suicide Severity Risk Scale (C-SSRS). We propose SASI, a risk-averse and self-aware transformer-based hierarchical attention classifier, augmented to refrain from making uncertain predictions. We show that SASI is able to refrain from 83% of incorrect predictions on real-world Reddit data. Furthermore, we discuss the qualitative, practical, and ethical aspects of SASI for suicide risk assessment as a human-in-the-loop framework. | Ramit Sawhney, Atula Tejaswi Neerkaje, Manas Gaur |  |
| 147 |  |  [When classifying grammatical role, BERT doesn't care about word order... except when it matters](https://doi.org/10.18653/v1/2022.acl-short.71) |  | 0 | Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey “The chef chopped the onion,” not “The onion chopped the chef.” Recent work has shown large language models to be surprisingly word order invariant, but crucially has largely considered natural prototypical inputs, where compositional meaning mostly matches lexical expectations. To overcome this confound, we probe grammatical role representation in English BERT and GPT-2, on instances where lexical expectations are not sufficient, and word order knowledge is necessary for correct classification. Such non-prototypical instances are naturally occurring English sentences with inanimate subjects or animate objects, or sentences where we systematically swap the arguments to make sentences like “The onion chopped the chef”. We find that, while early layer embeddings are largely lexical, word order is in fact crucial in defining the later-layer representations of words in semantically non-prototypical positions. Our experiments isolate the effect of word order on the contextualization process, and highlight how models use context in the uncommon, but critical, instances where it matters. | Isabel Papadimitriou, Richard Futrell, Kyle Mahowald |  |
| 148 |  |  [Triangular Transfer: Freezing the Pivot for Triangular Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.72) |  | 0 | Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a transfer-learning-based approach that utilizes all types of auxiliary data. As we train auxiliary source-pivot and pivot-target translation models, we initialize some parameters of the pivot side with a pre-trained language model and freeze them to encourage both translation models to work in the same pivot language space, so that they can be smoothly transferred to the source-target translation model. Experiments show that our approach can outperform previous ones. | Meng Zhang, Liangyou Li, Qun Liu |  |
| 149 |  |  [Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge](https://doi.org/10.18653/v1/2022.acl-short.73) |  | 0 | Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context. We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping. Our conclusion is that the ability to make the distinction between shared and privately known statements along the dialogue is moderately present in the analysed models, but not always incrementally consistent, which may partially be due to the limited need for grounding interactions in the original task. | Brielen Madureira, David Schlangen |  |
| 150 |  |  [Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.74) |  | 0 | Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model’s calibration. Our code is released at https://github.com/PKUnlp-icler/MLS | Liang Chen, Runxin Xu, Baobao Chang |  |
| 151 |  |  [Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification](https://doi.org/10.18653/v1/2022.acl-short.75) |  | 0 | Multi-Label Text Classification (MLTC) is a fundamental and challenging task in natural language processing. Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text. To make up for this oversight, we propose a k nearest neighbor (kNN) mechanism which retrieves several neighbor instances and interpolates the model output with their labels. Moreover, we design a multi-label contrastive learning objective that makes the model aware of the kNN classification process and improves the quality of the retrieved neighbors while inference. Extensive experiments show that our method can bring consistent and significant performance improvement to multiple MLTC models including the state-of-the-art pretrained and non-pretrained ones. | Xi'ao Su, Ran Wang, Xinyu Dai |  |
| 152 |  |  [NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better](https://doi.org/10.18653/v1/2022.acl-short.76) |  | 0 | Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks. | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 153 |  |  [Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-short.77) |  | 0 | Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works adjust such trade-off only for sequence labeling approaches. In this paper, we propose a simple yet effective counterpart – Align-and-Predict Decoding (APD) for the most popular sequence-to-sequence models to offer more flexibility for the precision-recall trade-off. During inference, APD aligns the already generated sequence with input and adjusts scores of the following tokens. Experiments in both English and Chinese GEC benchmarks show that our approach not only adapts a single model to precision-oriented and recall-oriented inference, but also maximizes its potential to achieve state-of-the-art results. Our code is available at https://github.com/AutoTemp/Align-and-Predict. | Xin Sun, Houfeng Wang |  |
| 154 |  |  [On the Effect of Isotropy on VAE Representations of Text](https://doi.org/10.18653/v1/2022.acl-short.78) |  | 0 | Injecting desired geometric properties into text representations has attracted a lot of attention. A property that has been argued for, due to its better utilisation of representation space, is isotropy. In parallel, VAEs have been successful in areas of NLP, but are known for their sub-optimal utilisation of the representation space. To address an aspect of this, we investigate the impact of injecting isotropy during training of VAEs. We achieve this by using an isotropic Gaussian posterior (IGP) instead of the ellipsoidal Gaussian posterior. We illustrate that IGP effectively encourages isotropy in the representations, inducing a more discriminative latent space. Compared to vanilla VAE, this translates into a much better classification performance, robustness to input perturbation, and generative behavior. Additionally, we offer insights about the representational properties encouraged by IGP. | Lan Zhang, Wray L. Buntine, Ehsan Shareghi |  |
| 155 |  |  [Efficient Classification of Long Documents Using Transformers](https://doi.org/10.18653/v1/2022.acl-short.79) |  | 0 | Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of accuracy as well as time and space overheads. Our datasets cover binary, multi-class, and multi-label classification tasks and represent various ways information is organized in a long text (e.g. information that is critical to making the classification decision is at the beginning or towards the end of the document). Our results show that more complex models often fail to outperform simple baselines and yield inconsistent performance across datasets. These findings emphasize the need for future studies to consider comprehensive baselines and datasets that better represent the task of long document classification to develop robust models. | Hyunji Hayley Park, Yogarshi Vyas, Kashif Shah |  |
| 156 |  |  [Rewarding Semantic Similarity under Optimized Alignments for AMR-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-short.80) |  | 0 | A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference. Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting. We address these issues by proposing metrics that replace the greedy alignments in BERTScore with optimized ones. We compute them on a model’s trained token embeddings to prevent domain mismatch. Our model optimizing discrete alignment metrics consistently outperforms cross-entropy and BLEU reward baselines on AMR-to-text generation. In addition, we find that this approach enjoys stable training compared to a non-RL setting. | Lisa Jin, Daniel Gildea |  |
| 157 |  |  [An Analysis of Negation in Natural Language Understanding Corpora](https://doi.org/10.18653/v1/2022.acl-short.81) |  | 0 | This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present. | Md Mosharaf Hossain, Dhivya Chinnappa, Eduardo Blanco |  |
| 158 |  |  [Primum Non Nocere: Before working with Indigenous data, the ACL must confront ongoing colonialism](https://doi.org/10.18653/v1/2022.acl-short.82) |  | 0 | In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage with Indigenous languages. No research involving Indigenous language data can be considered ethical without first acknowledging that Indigenous languages are not merely very low resource languages. The toxic legacy of colonialism permeates every aspect of interaction between Indigenous communities and outside researchers. To this end, we propose that the ACL draft and adopt an ethical framework for NLP researchers and computational linguists wishing to engage in research involving Indigenous languages. | Lane Schwartz |  |
| 159 |  |  [Unsupervised multiple-choice question generation for out-of-domain Q&A fine-tuning](https://doi.org/10.18653/v1/2022.acl-short.83) |  | 0 | Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once. In this work, we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences. We show that the state-of-the-art model UnifiedQA can greatly benefit from such a system on a multiple-choice benchmark about physics, biology and chemistry it has never been trained on. We further show that improved performances may be obtained by selecting the most challenging distractors (wrong answers), with a dedicated ranker based on a pretrained RoBERTa model. | Guillaume Le Berre, Christophe Cerisara, Philippe Langlais, Guy Lapalme |  |
| 160 |  |  [Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models](https://doi.org/10.18653/v1/2022.acl-short.84) |  | 0 | Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata—i.e. under “wug test”-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters. | Ling Liu, Mans Hulden |  |
| 161 |  |  [Probing the Robustness of Trained Metrics for Conversational Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-short.85) |  | 0 | This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to giving high scores to responses generated by rather simple and obviously flawed strategies that our method converges on. For instance, simply copying parts of the conversation context to form a response yields competitive scores or even outperforms responses written by humans. | Jan Deriu, Don Tuggener, Pius von Däniken, Mark Cieliebak |  |
| 162 |  |  [Rethinking and Refining the Distinct Metric](https://doi.org/10.18653/v1/2022.acl-short.86) |  | 0 | Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their expectations. We provide both empirical and theoretical evidence to show that our method effectively removes the biases existing in the original distinct score. Our experiments show that our proposed metric, Expectation-Adjusted Distinct (EAD), correlates better with human judgment in evaluating response diversity.To assist future research, we provide an example implementation at https://github.com/lsy641/Expectation-Adjusted-Distinct. | Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, Minlie Huang |  |
| 163 |  |  [How reparametrization trick broke differentially-private text representation learning](https://doi.org/10.18653/v1/2022.acl-short.87) |  | 0 | As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it seems non-trivial to get it right when applying it to NLP. In this short paper, we formally analyze several recent NLP papers proposing text representation learning using DPText (Beigi et al., 2019a,b; Alnasser et al., 2021; Beigi et al., 2021) and reveal their false claims of being differentially private. Furthermore, we also show a simple yet general empirical sanity check to determine whether a given implementation of a DP mechanism almost certainly violates the privacy loss guarantees. Our main goal is to raise awareness and help the community understand potential pitfalls of applying differential privacy to text representation learning. | Ivan Habernal |  |
| 164 |  |  [Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution](https://doi.org/10.18653/v1/2022.acl-short.88) |  | 0 | We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit “connections” among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees and use a globally normalized model to solve it. This contrasts with related works where two separate models are trained for each of the tasks and additional logic is required to merge the outputs. Experimental results on two datasets show a boost of up to +5% F1-score on both coref and EL tasks, compared to their standalone counterparts. For a subset of hard cases, with individual mentions lacking the correct EL in their candidate entity list, we obtain a +50% increase in accuracy. | Klim Zaporojets, Johannes Deleu, Yiwei Jiang, Thomas Demeester, Chris Develder |  |
| 165 |  |  [A Flexible Multi-Task Model for BERT Serving](https://doi.org/10.18653/v1/2022.acl-short.89) |  | 0 | We present an efficient BERT-based multi-task (MT) framework that is particularly suitable for iterative and incremental development of the tasks. The proposed framework is based on the idea of partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the other layers frozen. For each task, we train independently a single-task (ST) model using partial fine-tuning. Then we compress the task-specific layers in each ST model using knowledge distillation. Those compressed ST models are finally merged into one MT model so that the frozen layers of the former are shared across the tasks. We exemplify our approach on eight GLUE tasks, demonstrating that it is able to achieve 99.6% of the performance of the full fine-tuning method, while reducing up to two thirds of its overhead. | Tianwen Wei, Jianwei Qi, Shenghuan He |  |
| 166 |  |  [Understanding Game-Playing Agents with Natural Language Annotations](https://doi.org/10.18653/v1/2022.acl-short.90) |  | 0 | We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of game-playing agents like AlphaGo Zero. We find these game concepts are nontrivially encoded in two distinct policy networks, one trained via imitation learning and another trained via reinforcement learning. Furthermore, mentions of domain-specific terms are most easily predicted from the later layers of both models, suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations. | Nicholas Tomlin, Andre He, Dan Klein |  |
| 167 |  |  [Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding](https://doi.org/10.18653/v1/2022.acl-short.91) |  | 0 | Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs).Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on the observation that the code expressions in EMRs vary from their descriptions in ICD. By aligning codes to concepts in UMLS, we collect synonyms of every code. Then, we propose a multiple synonyms matching network to leverage synonyms for better code representation learning, and finally help the code classification. Experiments on the MIMIC-III dataset show that our proposed method outperforms previous state-of-the-art methods. | Zheng Yuan, Chuanqi Tan, Songfang Huang |  |
| 168 |  |  [CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment](https://doi.org/10.18653/v1/2022.acl-short.92) |  | 0 | Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks. | Lütfi Kerem Senel, Timo Schick, Hinrich Schütze |  |
| 169 |  |  [On the Importance of Effectively Adapting Pretrained Language Models for Active Learning](https://doi.org/10.18653/v1/2022.acl-short.93) |  | 0 | Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL. | Katerina Margatina, Loïc Barrault, Nikolaos Aletras |  |
| 170 |  |  [A Recipe for Arbitrary Text Style Transfer with Large Language Models](https://doi.org/10.18653/v1/2022.acl-short.94) |  | 0 | In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor.’ | Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris CallisonBurch, Jason Wei |  |
| 171 |  |  [DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.acl-short.95) |  | 0 | Our goal is to study the novel task of distant supervision for multilingual relation extraction (Multi DS-RE). Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset. The only available dataset for this task, RELX-Distant (Köksal and Özgür, 2020), displays several unrealistic characteristics, leading to a systematic overestimation of model performance. To alleviate these concerns, we release a new benchmark dataset for the task, named DiS-ReX. We also modify the widely-used bag attention models using an mBERT encoder and provide the first baseline results on the proposed task. We show that DiS-ReX serves as a more challenging dataset than RELX-Distant, leaving ample room for future research in this domain. | Abhyuday Bhartiya, Kartikeya Badola, Mausam |  |
| 172 |  |  [(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance](https://doi.org/10.18653/v1/2022.acl-short.96) |  | 0 | In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON’s shared-tasks. With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided. In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the naïve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks. Our experiments with the three top-ranked systems on the SIGMORPHON’s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even high-resourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models. | Omer Goldman, David Guriel, Reut Tsarfaty |  |
| 173 |  |  [Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks](https://doi.org/10.18653/v1/2022.acl-short.97) |  | 0 | Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We propose an efficient data augmentation method, dub as text smoothing, by converting a sentence from its one-hot representation to controllable smoothed representation. We evaluate text smoothing on different datasets in a low-resource regime. Experimental results show that text smoothing outperforms various mainstream data augmentation methods by a substantial margin. Moreover, text smoothing can be combined with these data augmentation methods to achieve better performance. | Xing Wu, Chaochen Gao, Meng Lin, Liangjun Zang, Songlin Hu |  |
| 174 |  |  [Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022](https://aclanthology.org/volumes/2022.findings-acl/) |  | 0 |  | Smaranda Muresan, Preslav Nakov, Aline Villavicencio |  |
| 175 |  |  [Frontmatter](https://aclanthology.org/2022.findings-acl.0) |  | 0 |  |  |  |
| 176 |  |  ["Is Whole Word Masking Always Better for Chinese BERT?": Probing on Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-acl.1) |  | 0 | Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model. For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates us to investigate whether WWM leads to better context understanding ability for Chinese BERT. To achieve this, we introduce two probing tasks related to grammatical error correction and ask pretrained models to revise or insert tokens in a masked language modeling manner. We construct a dataset including labels for 19,075 tokens in 10,448 sentences. We train three Chinese BERT models with standard character-level masking (CLM), WWM, and a combination of CLM and WWM, respectively. Our major findings are as follows: First, when one character needs to be inserted or replaced, the model trained with CLM performs the best. Second, when more than one character needs to be handled, WWM is the key to better performance. Finally, when being fine-tuned on sentence-level downstream tasks, models trained with different masking strategies perform comparably. | Yong Dai, Linyang Li, Cong Zhou, Zhangyin Feng, Enbo Zhao, Xipeng Qiu, Piji Li, Duyu Tang |  |
| 177 |  |  [Compilable Neural Code Generation with Compiler Feedback](https://doi.org/10.18653/v1/2022.findings-acl.2) |  | 0 | Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language models on large-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of them account for compilability of the generated programs. To improve compilability of the generated programs, this paper proposes COMPCODER, a three-stage pipeline utilizing compiler feedback for compilable code generation, including language model fine-tuning, compilability reinforcement, and compilability discrimination. Comprehensive experiments on two code generation tasks demonstrate the effectiveness of our proposed approach, improving the success rate of compilation from 44.18 to 89.18 in code completion on average and from 70.3 to 96.2 in text-to-code generation, respectively, when comparing with the state-of-the-art CodeGPT. | Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, Qun Liu |  |
| 178 |  |  [Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.3) |  | 0 | The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data. To tackle this, the prior works have studied the possibility of utilizing the sentiment analysis (SA) datasets to assist in training the ABSA model, primarily via pretraining or multi-task learning. In this article, we follow this line, and for the first time, we manage to apply the Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems straightforward to use generated pseudo labels to handle this case of label granularity unification for two highly related tasks, we identify its major challenge in this paper and propose a novel framework, dubbed as Dual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the DPL as a general framework capable of combining other prior methods in the literature. Through extensive experiments, DPL has achieved state-of-the-art performance on standard benchmarks surpassing the prior work significantly. | Yiming Zhang, Min Zhang, Sai Wu, Junbo Zhao |  |
| 179 |  |  [Input-specific Attention Subnetworks for Adversarial Detection](https://doi.org/10.18653/v1/2022.findings-acl.4) |  | 0 | Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three features to discriminate between authentic and adversarial inputs. The resultant detector significantly improves (by over 7.5%) the state-of-the-art adversarial detection accuracy for the BERT encoder on 10 NLU datasets with 11 different adversarial attack types. We also demonstrate that our method (a) is more accurate for larger models which are likely to have more spurious correlations and thus vulnerable to adversarial attack, and (b) performs well even with modest training sets of adversarial examples. | Emil Biju, Anirudh Sriram, Pratyush Kumar, Mitesh M. Khapra |  |
| 180 |  |  [RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction](https://doi.org/10.18653/v1/2022.findings-acl.5) |  | 0 | Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt. | Yew Ken Chia, Lidong Bing, Soujanya Poria, Luo Si |  |
| 181 |  |  [Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?](https://doi.org/10.18653/v1/2022.findings-acl.6) |  | 0 | What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the impact of domain mismatch, and (5) language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title’s question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data. | EnShiun Annie Lee, Sarubi Thillainathan, Shravan Nayak, Surangika Ranathunga, David Ifeoluwa Adelani, Ruisi Su, Arya McCarthy |  |
| 182 |  |  [Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation](https://doi.org/10.18653/v1/2022.findings-acl.7) |  | 0 | Generating explanations for recommender systems is essential for improving their transparency, as users often wish to understand the reason for receiving a specified recommendation. Previous methods mainly focus on improving the generation quality, but often produce generic explanations that fail to incorporate user and item specific details. To resolve this problem, we present Multi-Scale Distribution Deep Variational Autoencoders (MVAE).These are deep hierarchical VAEs with a prior network that eliminates noise while retaining meaningful signals in the input, coupled with a recognition network serving as the source of information to guide the learning of the prior network. Further, the Multi-scale distribution Learning Framework (MLF) along with a Target Tracking Kullback-Leibler divergence (TKL) mechanism are proposed to employ multi KL divergences at different scales for more effective learning. Extensive empirical experiments demonstrate that our methods can generate explanations with concrete input-specific contents. | Zefeng Cai, Linlin Wang, Gerard de Melo, Fei Sun, Liang He |  |
| 183 |  |  [Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning](https://doi.org/10.18653/v1/2022.findings-acl.8) |  | 0 | Prompt-based paradigm has shown its competitive performance in many NLP tasks. However, its success heavily depends on prompt design, and the effectiveness varies upon the model and training data. In this paper, we propose a novel dual context-guided continuous prompt (DCCP) tuning method. To explore the rich contextual information in language structure and close the gap between discrete prompt tuning and continuous prompt tuning, DCCP introduces two auxiliary training objectives and constructs input in a pair-wise fashion. Experimental results demonstrate that our method is applicable to many NLP tasks, and can often outperform existing prompt tuning methods by a large margin in the few-shot setting. | Jie Zhou, Le Tian, Houjin Yu, Xiao Zhou, Hui Su, Jie Zhou |  |
| 184 |  |  [Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training](https://doi.org/10.18653/v1/2022.findings-acl.9) |  | 0 | Nested named entity recognition (NER) is a task in which named entities may overlap with each other. Span-based approaches regard nested NER as a two-stage span enumeration and classification task, thus having the innate ability to handle this task. However, they face the problems of error propagation, ignorance of span boundary, difficulty in long entity recognition and requirement on large-scale annotated data. In this paper, we propose Extract-Select, a span selection framework for nested NER, to tackle these problems. Firstly, we introduce a span selection framework in which nested entities with different input categories would be separately extracted by the extractor, thus naturally avoiding error propagation in two-stage span-based approaches. In the inference phase, the trained extractor selects final results specific to the given entity category. Secondly, we propose a hybrid selection strategy in the extractor, which not only makes full use of span boundary but also improves the ability of long entity recognition. Thirdly, we design a discriminator to evaluate the extraction result, and train both extractor and discriminator with generative adversarial training (GAT). The use of GAT greatly alleviates the stress on the dataset size. Experimental results on four benchmark datasets demonstrate that Extract-Select outperforms competitive nested NER models, obtaining state-of-the-art results. The proposed model also performs well when less labeled data are given, proving the effectiveness of GAT. | Peixin Huang, Xiang Zhao, Minghao Hu, Yang Fang, Xinyi Li, Weidong Xiao |  |
| 185 |  |  [Controlled Text Generation Using Dictionary Prior in Variational Autoencoders](https://doi.org/10.18653/v1/2022.findings-acl.10) |  | 0 | While variational autoencoders (VAEs) have been widely applied in text generation tasks, they are troubled by two challenges: insufficient representation capacity and poor controllability. The former results from the posterior collapse and restrictive assumption, which impede better representation learning. The latter arises as continuous latent variables in traditional formulations hinder VAEs from interpretability and controllability. In this paper, we propose Dictionary Prior (DPrior), a new data-driven prior that enjoys the merits of expressivity and controllability. To facilitate controlled text generation with DPrior, we propose to employ contrastive learning to separate the latent space into several parts. Extensive experiments on both language modeling and controlled text generation demonstrate the effectiveness of the proposed approach. | Xianghong Fang, Jian Li, Lifeng Shang, Xin Jiang, Qun Liu, DitYan Yeung |  |
| 186 |  |  [Challenges to Open-Domain Constituency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.11) |  | 0 | Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We analyze challenges to open-domain constituency parsing using a set of linguistic features on various strong constituency parsers. Primarily, we find that 1) BERT significantly increases parsers’ cross-domain performance by reducing their sensitivity on the domain-variant features.2) Compared with single metrics such as unigram distribution and OOV rate, challenges to open-domain constituency parsing arise from complex features, including cross-domain lexical and constituent structure variations. | Sen Yang, Leyang Cui, Ruoxi Ning, Di Wu, Yue Zhang |  |
| 187 |  |  [Going "Deeper": Structured Sememe Prediction via Transformer with Tree Attention](https://doi.org/10.18653/v1/2022.findings-acl.12) |  | 0 | Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction studies ignore the hierarchical structures of sememes, which are important in the sememe-based semantic description system. In this work, we tackle the structured sememe prediction problem for the first time, which is aimed at predicting a sememe tree with hierarchical structures rather than a set of sememes. We design a sememe tree generation model based on Transformer with adjusted attention mechanism, which shows its superiority over the baselines in experiments. We also conduct a series of quantitative and qualitative analyses of the effectiveness of our model. All the code and data of this paper are available at https://github.com/thunlp/STG. | Yining Ye, Fanchao Qi, Zhiyuan Liu, Maosong Sun |  |
| 188 |  |  [Table-based Fact Verification with Self-adaptive Mixture of Experts](https://doi.org/10.18653/v1/2022.findings-acl.13) |  | 0 | The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present in this paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE). Specifically, we have developed a mixture-of-experts neural network to recognize and execute different types of reasoning—the network is composed of multiple experts, each handling a specific part of the semantics for reasoning, whereas a management module is applied to decide the contribution of each expert network to the verification result. A self-adaptive method is developed to teach the management module combining results of different experts more efficiently without external knowledge. The experimental results illustrate that our framework achieves 85.1% accuracy on the benchmark dataset TabFact, comparable with the previous state-of-the-art models. We hope our framework can serve as a new baseline for table-based verification. Our code is available at https://github.com/THUMLP/SaMoE. | Yuxuan Zhou, Xien Liu, Kaiyin Zhou, Ji Wu |  |
| 189 |  |  [Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics](https://doi.org/10.18653/v1/2022.findings-acl.14) |  | 0 | Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year’s WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of i.i.d assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to report the results on unreliable datasets, because it may leads to inconsistent results with most of the other datasets. | Jiannan Xiang, Huayang Li, Yahui Liu, Lemao Liu, Guoping Huang, Defu Lian, Shuming Shi |  |
| 190 |  |  [Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information](https://doi.org/10.18653/v1/2022.findings-acl.15) |  | 0 | In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of sememe prediction for BabelNet synsets (SPBS) is presented, aiming to build a multilingual sememe KB based on BabelNet, a multilingual encyclopedia dictionary. By automatically predicting sememes for a BabelNet synset, the words in many languages in the synset would obtain sememe annotations simultaneously. However, previous SPBS methods have not taken full advantage of the abundant information in BabelNet. In this paper, we utilize the multilingual synonyms, multilingual glosses and images in BabelNet for SPBS. We design a multimodal information fusion model to encode and combine this information for sememe prediction. Experimental results show the substantial outperformance of our model over previous methods (about 10 MAP and F1 scores). All the code and data of this paper can be obtained at https://github.com/thunlp/MSGI. | Fanchao Qi, Chuancheng Lv, Zhiyuan Liu, Xiaojun Meng, Maosong Sun, HaiTao Zheng |  |
| 191 |  |  [Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding](https://doi.org/10.18653/v1/2022.findings-acl.16) |  | 0 | Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and arguments from the input text. With the rich semantics in the queries, our framework benefits from the attention mechanisms to better capture the semantic correlation between the event types or argument roles and the input text. Furthermore, the query-and-extract formulation allows our approach to leverage all available event annotations from various ontologies as a unified model. Experiments on ACE and ERE demonstrate that our approach achieves state-of-the-art performance on each dataset and significantly outperforms existing methods on zero-shot event extraction. | Sijia Wang, Mo Yu, Shiyu Chang, Lichao Sun, Lifu Huang |  |
| 192 |  |  [LEVEN: A Large-Scale Chinese Legal Event Detection Dataset](https://doi.org/10.18653/v1/2022.findings-acl.17) |  | 0 | Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream applications. To alleviate these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection dataset, with 8,116 legal documents and 150,977 human-annotated event mentions in 108 event types. Not only charge-related events, LEVEN also covers general events, which are critical for legal case understanding but neglected in existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and has dozens of times the data scale of others, which shall significantly promote the training and evaluation of LED methods. The results of extensive experiments indicate that LED is challenging and needs further effort. Moreover, we simply utilize legal events as side information to promote downstream applications. The method achieves improvements of average 2.2 points precision in low-resource judgment prediction, and 1.5 points mean average precision in unsupervised case retrieval, which suggests the fundamentality of LED. The source code and dataset can be obtained from https://github.com/thunlp/LEVEN. | Feng Yao, Chaojun Xiao, Xiaozhi Wang, Zhiyuan Liu, Lei Hou, Cunchao Tu, Juanzi Li, Yun Liu, Weixing Shen, Maosong Sun |  |
| 193 |  |  [Analyzing Dynamic Adversarial Training Data in the Limit](https://doi.org/10.18653/v1/2022.findings-acl.18) |  | 0 | To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples. | Eric Wallace, Adina Williams, Robin Jia, Douwe Kiela |  |
| 194 |  |  [AbductionRules: Training Transformers to Explain Unexpected Inputs](https://doi.org/10.18653/v1/2022.findings-acl.19) |  | 0 | Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model interpretability. This paper presents AbductionRules, a group of natural language datasets designed to train and test generalisable abduction over natural-language knowledge bases. We use these datasets to finetune pretrained Transformers and discuss their performance, finding that our models learned generalisable abductive techniques but also learned to exploit the structure of our data. Finally, we discuss the viability of this approach to abductive reasoning and ways in which it may be improved in future work. | Nathan Young, Qiming Bao, Joshua Bensemann, Michael Witbrock |  |
| 195 |  |  [On the Importance of Data Size in Probing Fine-tuned Models](https://doi.org/10.18653/v1/2022.findings-acl.20) |  | 0 | Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model’s linguistic knowledge. | Houman Mehrafarin, Sara Rajaee, Mohammad Taher Pilehvar |  |
| 196 |  |  [RuCCoN: Clinical Concept Normalization in Russian](https://doi.org/10.18653/v1/2022.findings-acl.21) |  | 0 | We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and present strong baselines obtained with state-of-the-art models such as SapBERT. At present, Russian medical NLP is lacking in both datasets and trained models, and we view this work as an important step towards filling this gap. Our dataset and annotation guidelines are available at https://github.com/AIRI-Institute/RuCCoN. | Alexandr Nesterov, Galina Zubkova, Zulfat Miftahutdinov, Vladimir Kokh, Elena Tutubalina, Artem Shelmanov, Anton Alekseev, Manvel Avetisian, Andrey Chertok, Sergey I. Nikolenko |  |
| 197 |  |  [A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings](https://doi.org/10.18653/v1/2022.findings-acl.22) |  | 0 | Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed Pseudo-Token BERT (PT-BERT), which is able to explore the pseudo-token space (i.e., latent semantic space) representation of a sentence while eliminating the impact of superficial features such as sentence length and syntax. Specifically, we introduce an additional pseudo token embedding layer independent of the BERT encoder to map each sentence into a sequence of pseudo tokens in a fixed length. Leveraging these pseudo sequences, we are able to construct same-length positive and negative pairs based on the attention mechanism to perform contrastive learning. In addition, we utilize both the gradient-updating and momentum-updating encoders to encode instances while dynamically maintaining an additional queue to store the representation of sentence embeddings, enhancing the encoder’s learning performance for negative examples. Experiments show that our model outperforms the state-of-the-art baselines on six standard semantic textual similarity (STS) tasks. Furthermore, experiments on alignments and uniformity losses, as well as hard examples with different sentence lengths and syntax, consistently verify the effectiveness of our method. | Haochen Tan, Wei Shao, Han Wu, Ke Yang, Linqi Song |  |
| 198 |  |  [Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion](https://doi.org/10.18653/v1/2022.findings-acl.23) |  | 0 | Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, Eider, that empowers DocRE by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an RE model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better RE performance. We further design a simple yet effective inference process that makes RE predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows Eider to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that Eider outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37/1.26 Ign F1/F1 on DocRED). | Yiqing Xie, Jiaming Shen, Sha Li, Yuning Mao, Jiawei Han |  |
| 199 |  |  [Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation](https://doi.org/10.18653/v1/2022.findings-acl.24) |  | 0 | Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising research direction is to learn shareable structures across multiple tasks with limited annotated data. The downstream multilingual applications may benefit from such a learning setup as most of the languages across the globe are low-resource and share some structures with other languages. In this paper, we propose a novel meta-learning framework (called Meta-XNLG) to learn shareable structures from typologically diverse languages based on meta-learning and language clustering. This is a step towards uniform cross-lingual transfer for unseen languages. We first cluster the languages based on language representations and identify the centroid language of each cluster. Then, a meta-learning algorithm is trained with all centroid languages and evaluated on the other languages in the zero-shot setting. We demonstrate the effectiveness of this modeling on two NLG tasks (Abstractive Text Summarization and Question Generation), 5 popular datasets and 30 typologically diverse languages. Consistent improvements over strong baselines demonstrate the efficacy of the proposed framework. The careful design of the model makes this end-to-end NLG setup less vulnerable to the accidental translation problem, which is a prominent concern in zero-shot cross-lingual NLG tasks. | Kaushal Kumar Maurya, Maunendra Sankar Desarkar |  |
| 200 |  |  [MR-P: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.findings-acl.25) |  | 0 | Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training objective. However, the decoding algorithm is equally important. We propose a simple, effective, and easy-to-implement decoding algorithm that we call MaskRepeat-Predict (MR-P). The MR-P algorithm gives higher priority to consecutive repeated tokens when selecting tokens to mask for the next iteration and stops the iteration after target tokens converge. We conduct extensive experiments on six translation directions with varying data sizes. The results show that MR-P significantly improves the performance with the same model parameters. Specifically, we achieve a BLEU increase of 1.39 points in the WMT’14 En-De translation task. | Hao Cheng, Zhihua Zhang |  |
| 201 |  |  [Open Relation Modeling: Learning to Define Relations between Entities](https://doi.org/10.18653/v1/2022.findings-acl.26) |  | 0 | Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities. | Jie Huang, Kevin Chang, Jinjun Xiong, WenMei Hwu |  |
| 202 |  |  [A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots](https://doi.org/10.18653/v1/2022.findings-acl.27) |  | 0 | A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds a Chinese dialog dataset SSD for boosting research on SSTOD. The dataset includes a total of 40K dialogs and 500K utterances from four different domains: Chinese names, phone numbers, ID numbers and license plate numbers. The data is well annotated with sub-slot values, slot values, dialog states and actions. We find some new linguistic phenomena and interactive manners in SSTOD which raise critical challenges of building dialog agents for the task. We test three state-of-the-art dialog models on SSTOD and find they cannot handle the task well on any of the four domains. We also investigate an improved model by involving slot knowledge in a plug-in manner. More work should be done to meet the new challenges raised from SSTOD which widely exists in real-life applications. The dataset and code are publicly available via https://github.com/shunjiu/SSTOD. | Sai Zhang, Yuwei Hu, Yuchuan Wu, Jiaman Wu, Yongbin Li, Jian Sun, Caixia Yuan, Xiaojie Wang |  |
| 203 |  |  [Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction](https://doi.org/10.18653/v1/2022.findings-acl.28) |  | 0 | Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. However, because natural language may contain ambiguity and variability, this is a difficult challenge. In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and enables the user to make corrections through natural-language feedback for individual steps. We focus on question answering over knowledge bases (KBQA) as an instantiation of our framework, aiming to increase the transparency of the parsing process and help the user trust the final answer. We construct INSPIRED, a crowdsourced dialogue dataset derived from the ComplexWebQuestions dataset. Our experiments show that this framework has the potential to greatly improve overall parse accuracy. Furthermore, we develop a pipeline for dialogue simulation to evaluate our framework w.r.t. a variety of state-of-the-art KBQA models without further crowdsourcing effort. The results demonstrate that our framework promises to be effective across such models. | Lingbo Mo, Ashley Lewis, Huan Sun, Michael White |  |
| 204 |  |  [MINER: Multi-Interest Matching Network for News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.29) |  | 0 | Personalized news recommendation is an essential technique to help users find interested news. Accurately matching user’s interests and candidate news is the key to news recommendation. Most existing methods learn a single user embedding from user’s historical behaviors to represent the reading interest. However, user interest is usually diverse and may not be adequately modeled by a single user embedding. In this paper, we propose a poly attention scheme to learn multiple interest vectors for each user, which encodes the different aspects of user interest. We further propose a disagreement regularization to make the learned interests vectors more diverse. Moreover, we design a category-aware attention weighting strategy that incorporates the news category information as explicit interest signals into the attention mechanism. Extensive experiments on the MIND news recommendation benchmark demonstrate that our approach significantly outperforms existing state-of-the-art methods. | Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin Jiang, Qun Liu |  |
| 205 |  |  [KSAM: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding](https://doi.org/10.18653/v1/2022.findings-acl.30) |  | 0 | Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses. However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. To this end, infusing knowledge from multiple sources becomes a trend. This paper proposes a novel approach Knowledge Source Aware Multi-Head Decoding, KSAM, to infuse multi-source knowledge into dialogue generation more efficiently. Rather than following the traditional single decoder paradigm, KSAM uses multiple independent source-aware decoder heads to alleviate three challenging problems in infusing multi-source knowledge, namely, the diversity among different knowledge sources, the indefinite knowledge alignment issue, and the insufficient flexibility/scalability in knowledge usage. Experiments on a Chinese multi-source knowledge-aligned dataset demonstrate the superior performance of KSAM against various competitive approaches. | Sixing Wu, Ying Li, Dawei Zhang, Zhonghai Wu |  |
| 206 |  |  [Towards Responsible Natural Language Annotation for the Varieties of Arabic](https://doi.org/10.18653/v1/2022.findings-acl.31) |  | 0 | When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content. | A. Stevie Bergman, Mona T. Diab |  |
| 207 |  |  [Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection](https://doi.org/10.18653/v1/2022.findings-acl.32) |  | 0 | Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific terms from pre-defined static dictionaries. While this has been demonstrated to improve the generalizability of classifiers, the coverage of such methods is limited and the dictionaries require regular manual updates from human experts. In this paper, we propose to automatically identify and reduce spurious correlations using attribution methods with dynamic refinement of the list of terms that need to be regularized during training. Our approach is flexible and improves the cross-corpora performance over previous work independently and in combination with pre-defined dictionaries. | Tulika Bose, Nikolaos Aletras, Irina Illina, Dominique Fohr |  |
| 208 |  |  [Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-acl.33) |  | 0 | Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and relieve the manual efforts of designers is to build an end-to-end dialogue system that can do reasoning itself while perceiving user’s utterances. In this work, we propose a novel method to incorporate the knowledge reasoning capability into dialog systems in a more scalable and generalizable manner. Our proposed method allows a single transformer model to directly walk on a large-scale knowledge graph to generate responses. To the best of our knowledge, this is the first work to have transformer models generate responses by reasoning over differentiable knowledge graphs. We investigate the reasoning abilities of the proposed method on both task-oriented and domain-specific chit-chat dialogues. Empirical results show that this method can effectively and efficiently incorporate a knowledge graph into a dialogue system with fully-interpretable reasoning paths. | YiLin Tuan, Sajjad Beygi, Maryam FazelZarandi, Qiaozi Gao, Alessandra Cervone, William Yang Wang |  |
| 209 |  |  [MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction](https://doi.org/10.18653/v1/2022.findings-acl.34) |  | 0 | Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer performance degradation on long documents due to discrepancy between sequence lengths which causes mismatch between representations of keyphrase candidates and the document. In this work, we propose a novel unsupervised embedding-based KPE approach, Masked Document Embedding Rank (MDERank), to address this problem by leveraging a mask strategy and ranking candidates by the similarity between embeddings of the source document and the masked document. We further develop a KPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised contrastive learning method, which is more compatible to MDERank than vanilla BERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the proposed MDERank outperforms state-of-the-art unsupervised KPE approach by average 1.80 F1@15 improvement. MDERank further benefits from KPEBERT and overall achieves average 3.53 F1@15 improvement over SIFRank. | Linhan Zhang, Qian Chen, Wen Wang, Chong Deng, Shiliang Zhang, Bing Li, Wei Wang, Xin Cao |  |
| 210 |  |  [Visualizing the Relationship Between Encoded Linguistic Information and Task Performance](https://doi.org/10.18653/v1/2022.findings-acl.35) |  | 0 | Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto Optimality. Its key idea is to obtain a set of models which are Pareto-optimal in terms of both objectives. From this viewpoint, we propose a method to optimize the Pareto-optimal models by formalizing it as a multi-objective optimization problem. We conduct experiments on two popular NLP tasks, i.e., machine translation and language modeling, and investigate the relationship between several kinds of linguistic information and task performances. Experimental results demonstrate that the proposed method is better than a baseline method. Our empirical findings suggest that some syntactic information is helpful for NLP tasks whereas encoding more syntactic information does not necessarily lead to better performance, because the model architecture is also an important factor. | Jiannan Xiang, Huayang Li, Defu Lian, Guoping Huang, Taro Watanabe, Lemao Liu |  |
| 211 |  |  [Efficient Argument Structure Extraction with Transfer Learning and Active Learning](https://doi.org/10.18653/v1/2022.findings-acl.36) |  | 0 | The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. In this work, we propose a novel context-aware Transformer-based argument structure prediction model which, on five different domains, significantly outperforms models that rely on features or only encode limited contexts. To tackle the difficulty of data annotation, we examine two complementary methods: (i) transfer learning to leverage existing annotated data to boost model performance in a new target domain, and (ii) active learning to strategically identify a small amount of samples for annotation. We further propose model-independent sample acquisition strategies, which can be generalized to diverse domains. With extensive experiments, we show that our simple-yet-effective acquisition strategies yield competitive results against three strong comparisons. Combined with transfer learning, substantial F1 score boost (5-25) can be further achieved during the early iterations of active learning across domains. | Xinyu Hua, Lu Wang |  |
| 212 |  |  [Plug-and-Play Adaptation for Continuously-updated QA](https://doi.org/10.18653/v1/2022.findings-acl.37) |  | 0 | Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task—Continuously-updated QA (CuQA)—in which multiple large-scale updates are made to LMs, and the performance is measured with respect to the success in adding and updating knowledge while retaining existing knowledge. We then present LMs with plug-in modules that effectively handle the updates. Experiments conducted on zsRE QA and NQ datasets show that our method outperforms existing approaches. We find that our method is 4x more effective in terms of updates/forgets ratio, compared to a fine-tuning baseline. | Kyungjae Lee, Wookje Han, Seungwon Hwang, Hwaran Lee, Joonsuk Park, SangWoo Lee |  |
| 213 |  |  [Reinforced Cross-modal Alignment for Radiology Report Generation](https://doi.org/10.18653/v1/2022.findings-acl.38) |  | 0 | Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians’ workload. In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality reports. Although previous studies attempt to facilitate the alignment via the co-attention mechanism under supervised settings, they suffer from lacking valid and accurate correspondences due to no annotation of such alignment. In this paper, we propose an approach with reinforcement learning (RL) over a cross-modal memory (CMM) to better align visual and textual features for radiology report generation. In detail, a shared memory is used to record the mappings between visual and textual information, and the proposed reinforced algorithm is performed to learn the signal from the reports to guide the cross-modal alignment even though such reports are not directly related to how images and texts are mapped. Experimental results on two English radiology report datasets, i.e., IU X-Ray and MIMIC-CXR, show the effectiveness of our approach, where the state-of-the-art results are achieved. We further conduct human evaluation and case study which confirm the validity of the reinforced algorithm in our approach. | Han Qin, Yan Song |  |
| 214 |  |  [What Works and Doesn't Work, A Deep Decoder for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.39) |  | 0 | Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep decoder in the Transformer model. Inspired by this discovery, we then propose approaches to improving it, with respect to model structure and model training, to make the deep decoder practical in NMT. Specifically, with respect to model structure, we propose a cross-attention drop mechanism to allow the decoder layers to perform their own different roles, to reduce the difficulty of deep-decoder learning. For model training, we propose a collapse reducing training approach to improve the stability and effectiveness of deep-decoder training. We experimentally evaluated our proposed Transformer NMT model structure modification and novel training methods on several popular machine translation benchmarks. The results showed that deepening the NMT model by increasing the number of decoder layers successfully prevented the deepened decoder from degrading to an unconditional language model. In contrast to prior work on deepening an NMT model on the encoder, our method can deepen the model on both the encoder and decoder at the same time, resulting in a deeper model and improved performance. | Zuchao Li, Yiran Wang, Masao Utiyama, Eiichiro Sumita, Hai Zhao, Taro Watanabe |  |
| 215 |  |  [SyMCoM - Syntactic Measure of Code Mixing A Study Of English-Hindi Code-Mixing](https://doi.org/10.18653/v1/2022.findings-acl.40) |  | 0 | Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have been proposed. Syntactical variety/patterns of code-mixing and their relationship vis-a-vis computational model’s performance is under explored. In this work, we investigate a collection of English(en)-Hindi(hi) code-mixed datasets from a syntactic lens to propose, SyMCoM, an indicator of syntactic variety in code-mixed text, with intuitive theoretical bounds. We train SoTA en-hi PoS tagger, accuracy of 93.4%, to reliably compute PoS tags on a corpus, and demonstrate the utility of SyMCoM by applying it on various syntactical categories on a collection of datasets, and compare datasets using the measure. | Prashant Kodali, Anmol Goel, Monojit Choudhury, Manish Shrivastava, Ponnurangam Kumaraguru |  |
| 216 |  |  [HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data](https://doi.org/10.18653/v1/2022.findings-acl.41) |  | 0 | A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities. Previous work in multiturn dialogue systems has primarily focused on either text or table information. In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically distributed over both unstructured and structured forms. We present a new dialogue dataset, HybriDialogue, which consists of crowdsourced natural conversations grounded on both Wikipedia text and tables. The conversations are created through the decomposition of complex multihop questions into simple, realistic multiturn dialogue interactions. We propose retrieval, system state tracking, and dialogue response generation tasks for our dataset and conduct baseline experiments for each. Our results show that there is still ample opportunity for improvement, demonstrating the importance of building stronger dialogue systems that can reason over the complex setting of informationseeking dialogue grounded on tables and text. | Kai Nakamura, Sharon Levy, YiLin Tuan, Wenhu Chen, William Yang Wang |  |
| 217 |  |  [NEWTS: A Corpus for News Topic-Focused Summarization](https://doi.org/10.18653/v1/2022.findings-acl.42) |  | 0 | Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed models (e.g., plug and play language models) have the capacity to condition the generated summaries on a desired range of themes. These capacities remain largely unused and unevaluated as there is no dedicated dataset that would support the task of topic-focused summarization. This paper introduces the first topical summarization corpus NEWTS, based on the well-known CNN/Dailymail dataset, and annotated via online crowd-sourcing. Each source article is paired with two reference summaries, each focusing on a different theme of the source document. We evaluate a representative range of existing techniques and analyze the effectiveness of different prompting methods. | Seyed Ali Bahrainian, Sheridan Feucht, Carsten Eickhoff |  |
| 218 |  |  [Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis](https://doi.org/10.18653/v1/2022.findings-acl.43) |  | 0 | Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways—from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this assumption of political users and show that commonly-used political-inference models do not generalize, indicating heterogeneous types of political users. The models remain imprecise at best for most users, regardless of which sources of data or methods are used. Across a 14-year longitudinal analysis, we demonstrate that the choice in definition of a political user has significant implications for behavioral analysis. Controlling for multiple factors, political users are more toxic on the platform and inter-party interactions are even more toxic—but not all political users behave this way. Last, we identify a subset of political users who repeatedly flip affiliations, showing that these users are the most controversial of all, acting as provocateurs by more frequently bringing up politics, and are more likely to be banned, suspended, or deleted. | Kenan Alkiek, Bohan Zhang, David Jurgens |  |
| 219 |  |  [Toward More Meaningful Resources for Lower-resourced Languages](https://doi.org/10.18653/v1/2022.findings-acl.44) |  | 0 | In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness. We explore the contents of the names stored in Wikidata for a few lower-resourced languages and find that many of them are not in fact in the languages they claim to be, requiring non-trivial effort to correct. We discuss quality issues present in WikiAnn and evaluate whether it is a useful supplement to hand-annotated data. We then discuss the importance of creating annotations for lower-resourced languages in a thoughtful and ethical way that includes the language speakers as part of the development process. We conclude with recommended guidelines for resource development. | Constantine Lignos, Nolan Holley, Chester PalenMichel, Jonne Sälevä |  |
| 220 |  |  [Better Quality Estimation for Low Resource Corpus Mining](https://doi.org/10.18653/v1/2022.findings-acl.45) |  | 0 | Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models. However, these models still lack the robustness to achieve general adoption. We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to out-of-domain examples. We propose a combination of multitask training, data augmentation and contrastive learning to achieve better and more robust QE performance. We show that our method improves QE performance significantly in the MLQE challenge and the robustness of QE models when tested in the Parallel Corpus Mining setup. We increase the accuracy in PCM by more than 0.80, making it on par with state-of-the-art PCM methods that use millions of sentence pairs to train their models. In comparison, we use a thousand times less data, 7K parallel sentences in total, and propose a novel low resource PCM method. | Muhammed Yusuf Kocyigit, Jiho Lee, Derry Wijaya |  |
| 221 |  |  [End-to-End Segmentation-based News Summarization](https://doi.org/10.18653/v1/2022.findings-acl.46) |  | 0 | In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned heading-style section summaries. Second, we propose a novel segmentation-based language generation model adapted from pre-trained language models that can jointly segment a document and produce the summary for each section. Experimental results on SegNews demonstrate that our model can outperform several state-of-the-art sequence-to-sequence generation models for this new task. | Yang Liu, Chenguang Zhu, Michael Zeng |  |
| 222 |  |  [Fast Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.47) |  | 0 | Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference corpus. kNN-MT is thus two-orders slower than vanilla MT models, making it hard to be applied to real-world applications, especially online services. In this work, we propose Fast kNN-MT to address this issue. Fast kNN-MT constructs a significantly smaller datastore for the nearest neighbor search: for each word in a source sentence, Fast kNN-MT first selects its nearest token-level neighbors, which is limited to tokens that are the same as the query token. Then at each decoding step, in contrast to using the entire corpus as the datastore, the search space is limited to target tokens corresponding to the previously selected reference source tokens. This strategy avoids search through the whole datastore for nearest neighbors and drastically improves decoding efficiency. Without loss of performance, Fast kNN-MT is two-orders faster than kNN-MT, and is only two times slower than the standard NMT model. Fast kNN-MT enables the practical use of kNN-MT systems in real-world MT applications. The code is available at https://github.com/ShannonAI/fast-knn-nmt. | Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, Jiwei Li |  |
| 223 |  |  [Extracting Latent Steering Vectors from Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.48) |  | 0 | Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space. | Nishant Subramani, Nivedita Suresh, Matthew E. Peters |  |
| 224 |  |  [Domain Generalisation of NMT: Fusing Adapters with Leave-One-Domain-Out Training](https://doi.org/10.18653/v1/2022.findings-acl.49) |  | 0 | Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to avoid information leaking to address the challenge of not knowing the test domain during training time. Empirical results on three language pairs show that our proposed fusion method outperforms other baselines up to +0.8 BLEU score on average. | ThuyTrang Vu, Shahram Khadivi, Dinh Q. Phung, Gholamreza Haffari |  |
| 225 |  |  [Reframing Instructional Prompts to GPTk's Language](https://doi.org/10.18653/v1/2022.findings-acl.50) |  | 0 | What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms. | Daniel Khashabi, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi |  |
| 226 |  |  [Read Top News First: A Document Reordering Approach for Multi-Document News Summarization](https://doi.org/10.18653/v1/2022.findings-acl.51) |  | 0 | A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before concatenating and summarizing them. The reordering makes the salient content easier to learn by the summarization model. Experiments show that our approach outperforms previous state-of-the-art methods with more complex architectures. | Chao Zhao, Tenghao Huang, Somnath Basu Roy Chowdhury, Muthu Kumar Chandrasekaran, Kathleen R. McKeown, Snigdha Chaturvedi |  |
| 227 |  |  [Human Language Modeling](https://doi.org/10.18653/v1/2022.findings-acl.52) |  | 0 | Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that human language is moderated by changing human states. We introduce, HaRT, a large-scale transformer model for solving HuLM, pre-trained on approximately 100,000 social media users, and demonstrate it’s effectiveness in terms of both language modeling (perplexity) for social media and fine-tuning for 4 downstream tasks spanning document- and user-levels. Results on all tasks meet or surpass the current state-of-the-art. | Nikita Soni, Matthew Matero, Niranjan Balasubramanian, H. Andrew Schwartz |  |
| 228 |  |  [Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging](https://doi.org/10.18653/v1/2022.findings-acl.53) |  | 0 | Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the prompting methods have to enumerate all n-grams token spans to find all the possible slots, which greatly slows down the prediction. To tackle this, we introduce an inverse paradigm for prompting. Different from the classic prompts mapping tokens to labels, we reversely predict slot values given slot types. Such inverse prompting only requires a one-turn prediction for each slot type and greatly speeds up the prediction. Besides, we propose a novel Iterative Prediction Strategy, from which the model learns to refine predictions by considering the relations between different slot types. We find, somewhat surprisingly, the proposed method not only predicts faster but also significantly improves the effect (improve over 6.1 F1-scores on 10-shot setting) and achieves new state-of-the-art performance. | Yutai Hou, Cheng Chen, Xianzhen Luo, Bohan Li, Wanxiang Che |  |
| 229 |  |  [Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding](https://doi.org/10.18653/v1/2022.findings-acl.54) |  | 0 | Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a wrong one. This pairwise classification task, however, cannot promote the development of practical neural decoders for two reasons. First, it has to enumerate all pairwise combinations in the test set, so it is inefficient to predict a word in a large vocabulary. Second, a perfect pairwise decoder cannot guarantee the performance on direct classification. To overcome these and go a step further to a realistic neural decoder, we propose a novel Cross-Modal Cloze (CMC) task which is to predict the target word encoded in the neural image with a context as prompt. Furthermore, to address this task, we propose a general approach that leverages the pre-trained language model to predict the target word. To validate our method, we perform experiments on more than 20 participants from two brain imaging datasets. Our method achieves 28.91% top-1 accuracy and 54.19% top-5 accuracy on average across all participants, significantly outperforming several baselines. This result indicates that our model can serve as a state-of-the-art baseline for the CMC task. More importantly, it demonstrates that it is feasible to decode a certain word within a large vocabulary from its neural brain activity. | Shuxian Zou, Shaonan Wang, Jiajun Zhang, Chengqing Zong |  |
| 230 |  |  [Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://doi.org/10.18653/v1/2022.findings-acl.55) |  | 0 | Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal—modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT–2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness. | Umang Gupta, Jwala Dhamala, Varun Kumar, Apurv Verma, Yada Pruksachatkun, Satyapriya Krishna, Rahul Gupta, KaiWei Chang, Greg Ver Steeg, Aram Galstyan |  |
| 231 |  |  [Domain Representative Keywords Selection: A Probabilistic Approach](https://doi.org/10.18653/v1/2022.findings-acl.56) |  | 0 | We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a distribution of candidate keywords. It provides more importance to the distinctive keywords of the target domain than common keywords contrasting with the context domain. To support the representativeness of the selected keywords towards the target domain, we introduce an optimization algorithm for selecting the subset from the generated candidate distribution. We have shown that the optimization algorithm can be efficiently implemented with a near-optimal approximation guarantee. Finally, extensive experiments on multiple domains demonstrate the superiority of our approach over other baselines for the tasks of keyword summary generation and trending keywords selection. | Pritom Saha Akash, Jie Huang, Kevin ChenChuan Chang, Yunyao Li, Lucian Popa, ChengXiang Zhai |  |
| 232 |  |  [Hierarchical Inductive Transfer for Continual Dialogue Learning](https://doi.org/10.18653/v1/2022.findings-acl.57) |  | 0 | Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge interference among diverse dialogue tasks. In this work, we propose a hierarchical inductive transfer framework to learn and deploy the dialogue skills continually and efficiently. First, we introduce the adapter module into pre-trained models for learning new dialogue tasks. As the only trainable module, it is beneficial for the dialogue system on the embedded devices to acquire new dialogue skills with negligible additional parameters. Then, for alleviating knowledge interference between tasks yet benefiting the regularization between them, we further design hierarchical inductive transfer that enables new tasks to use general knowledge in the base adapter without being misled by diverse knowledge in task-specific adapters. Empirical evaluation and analysis indicate that our framework obtains comparable performance under deployment-friendly model capacity. | Shaoxiong Feng, Xuancheng Ren, Kan Li, Xu Sun |  |
| 233 |  |  [Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation](https://doi.org/10.18653/v1/2022.findings-acl.58) |  | 0 | Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from an imitation learning perspective. We show that exposure bias leads to an accumulation of errors during generation, analyze why perplexity fails to capture this accumulation of errors, and empirically show that this accumulation results in poor generation quality. | Kushal Arora, Layla El Asri, Hareesh Bahuleyan, Jackie Chi Kit Cheung |  |
| 234 |  |  [Question Answering Infused Pre-training of General-Purpose Contextualized Representations](https://doi.org/10.18653/v1/2022.findings-acl.59) |  | 0 | We propose a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder’s token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-the-art results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. | Robin Jia, Mike Lewis, Luke Zettlemoyer |  |
| 235 |  |  [Automatic Song Translation for Tonal Languages](https://doi.org/10.18653/v1/2022.findings-acl.60) |  | 0 | This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words’ tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST—preserving meaning, singability and intelligibility—and design metrics for these criteria. We develop a new benchmark for English–Mandarin song translation and develop an unsupervised AST system, Guided AliGnment for Automatic Song Translation (GagaST), which combines pre-training with three decoding constraints. Both automatic and human evaluations show GagaST successfully balances semantics and singability. | Fenfei Guo, Chen Zhang, Zhirui Zhang, Qixin He, Kejun Zhang, Jun Xie, Jordan L. BoydGraber |  |
| 236 |  |  [Read before Generate! Faithful Long Form Question Answering with Machine Reading](https://doi.org/10.18653/v1/2022.findings-acl.61) |  | 0 | Long-form question answering (LFQA) aims to generate a paragraph-length answer for a given question. While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somewhat relevant content, one primary challenge lies in how to generate a faithful answer that has less hallucinated content. We propose a new end-to-end framework that jointly models answer generation and machine reading. The key idea is to augment the generation model with fine-grained, answer-related salient information which can be viewed as an emphasis on faithful facts. State-of-the-art results on two LFQA datasets, ELI5 and MS MARCO, demonstrate the effectiveness of our method, in comparison with strong baselines on automatic and human evaluation metrics. A detailed analysis further proves the competency of our methods in generating fluent, relevant, and more faithful answers. | Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung |  |
| 237 |  |  [A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.62) |  | 0 | Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each relation class implicitly with relation information, generally through designing complex network structures, like generating hybrid features, combining with contrastive learning or attention networks. We argue that relation information can be introduced more explicitly and effectively into the model. Thus, this paper proposes a direct addition approach to introduce relation information. Specifically, for each relation class, the relation representation is first generated by concatenating two views of relations (i.e., [CLS] token embedding and the mean value of embeddings of all tokens) and then directly added to the original prototype for both train and prediction. Experimental results on the benchmark dataset FewRel 1.0 show significant improvements and achieve comparable results to the state-of-the-art, which demonstrates the effectiveness of our proposed approach. Besides, further analyses verify that the direct addition is a much more effective way to integrate the relation representations and the original prototypes. | Yang Liu, Jinpeng Hu, Xiang Wan, TsungHui Chang |  |
| 238 |  |  [MIMICause: Representation and automatic extraction of causal relation types from clinical notes](https://doi.org/10.18653/v1/2022.findings-acl.63) |  | 0 | Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story communicated in the clinical notes and help make more informed decisions. In this work, we propose annotation guidelines, develop an annotated corpus and provide baseline scores to identify types and direction of causal relations between a pair of biomedical concepts in clinical notes; communicated implicitly or explicitly, identified either in a single sentence or across multiple sentences. We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2 shared task dataset and train four different language model based architectures. Annotation based on our guidelines achieved a high inter-annotator agreement i.e. Fleiss’ kappa (𝜅) score of 0.72, and our model for identification of causal relations achieved a macro F1 score of 0.56 on the test data. The high inter-annotator agreement for clinical text shows the quality of our annotation guidelines while the provided baseline F1 score sets the direction for future research towards understanding narratives in clinical texts. | Vivek Khetan, Md Imbesat Hassan Rizvi, Jessica Huber, Paige Bartusiak, Bogdan Sacaleanu, Andrew E. Fano |  |
| 239 |  |  [Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation](https://doi.org/10.18653/v1/2022.findings-acl.64) |  | 0 | How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn compressed sentence embeddings. Our method augments a small Transformer encoder model with learnable projection layers to produce compact representations while mimicking a large pre-trained language model to retain the sentence representation quality. We evaluate our method with different model sizes on both semantic textual similarity (STS) and semantic retrieval (SR) tasks. Experiments show that our method achieves 2.7-4.5 points performance gain on STS tasks compared with previous best representations of the same size. In SR tasks, our method improves retrieval speed (8.2×) and memory usage (8.0×) compared with state-of-the-art large models. Our implementation is available at https://github.com/XuandongZhao/HPD. | Xuandong Zhao, Zhiguo Yu, Ming Wu, Lei Li |  |
| 240 |  |  [Debiasing Event Understanding for Visual Commonsense Tasks](https://doi.org/10.18653/v1/2022.findings-acl.65) |  | 0 | We study event understanding as a critical step towards visual commonsense tasks. Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects. We propose to mitigate such biases with do-calculus, proposed in causality research, but overcoming its limited robustness, by an optimized aggregation with association-based prediction.We show the effectiveness of our approach, intrinsically by comparing our generated events with ground-truth event annotation, and extrinsically by downstream commonsense tasks. | Minji Seo, YeonJoon Jung, Seungtaek Choi, Seungwon Hwang, Bei Liu |  |
| 241 |  |  [Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-acl.66) |  | 0 | Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods. | Yao Zhang, Peiyao Li, Hongru Liang, Adam Jatowt, Zhenglu Yang |  |
| 242 |  |  [DeepStruct: Pretraining of Language Models for Structure Prediction](https://doi.org/10.18653/v1/2022.findings-acl.67) |  | 0 | We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available. | Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, Dawn Song |  |
| 243 |  |  [The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error](https://doi.org/10.18653/v1/2022.findings-acl.68) |  | 0 | Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution’s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes from the training to the test sample when these samples may be drawn from distinct distributions. While this can be estimated via distribution shift, we argue that this does not directly correlate with change in the observed error of a classifier (i.e. error-gap). Thus, we propose to use a statistic from the theoretical domain adaptation literature which can be directly tied to error-gap. We study the bias of this statistic as an estimator of error-gap both theoretically and through a large-scale empirical study of over 2400 experiments on 6 discourse datasets from domains including, but not limited to: news, biomedical texts, TED talks, Reddit posts, and fiction. Our results not only motivate our proposal and help us to understand its limitations, but also provide insight on the properties of discourse models and datasets which improve performance in domain adaptation. For instance, we find that non-news datasets are slightly easier to transfer to than news datasets when the training and test sets are very different. Our code and an associated Python package are available to allow practitioners to make more informed model and dataset choices. | Katherine Atwell, Anthony Sicilia, Seong Jae Hwang, Malihe Alikhani |  |
| 244 |  |  [Mukayese: Turkish NLP Strikes Back](https://doi.org/10.18653/v1/2022.findings-acl.69) |  | 0 | Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a solution, we present Mukayese, a set of NLP benchmarks for the Turkish language that contains several NLP tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in Turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: https://github.com/alisafaya/mukayese | Ali Safaya, Emirhan Kurtulus, Arda Göktogan, Deniz Yüret |  |
| 245 |  |  [Virtual Augmentation Supported Contrastive Learning of Sentence Representations](https://doi.org/10.18653/v1/2022.findings-acl.70) |  | 0 | Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge. This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual augmentation Supported Contrastive Learning of sentence representations (VaSCL). Originating from the interpretation that data augmentation essentially constructs the neighborhoods of each training instance, we, in turn, utilize the neighborhood to generate effective data augmentations. Leveraging the large training batch size of contrastive learning, we approximate the neighborhood of an instance via its K-nearest in-batch neighbors in the representation space. We then define an instance discrimination task regarding the neighborhood and generate the virtual augmentation in an adversarial training manner. We access the performance of VaSCL on a wide range of downstream tasks and set a new state-of-the-art for unsupervised sentence representation learning. | Dejiao Zhang, Wei Xiao, Henghui Zhu, Xiaofei Ma, Andrew O. Arnold |  |
| 246 |  |  [MoEfication: Transformer Feed-forward Layers are Mixtures of Experts](https://doi.org/10.18653/v1/2022.findings-acl.71) |  | 0 | Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication. | Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 247 |  |  [DS-TOD: Efficient Domain Specialization for Task-Oriented Dialog](https://doi.org/10.18653/v1/2022.findings-acl.72) |  | 0 | Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific knowledge useful for concrete downstream TOD domains. In this work, we investigate the effects of domain specialization of pretrained language models (PLMs) for TOD. Within our DS-TOD framework, we first automatically extract salient domain-specific terms, and then use them to construct DomainCC and DomainReddit – resources that we leverage for domain-specific pretraining, based on (i) masked language modeling (MLM) and (ii) response selection (RS) objectives, respectively. We further propose a resource-efficient and modular domain specialization by means of domain adapters – additional parameter-light layers in which we encode the domain knowledge. Our experiments with prominent TOD tasks – dialog state tracking (DST) and response retrieval (RR) – encompassing five domains from the MultiWOZ benchmark demonstrate the effectiveness of DS-TOD. Moreover, we show that the light-weight adapter-based specialization (1) performs comparably to full fine-tuning in single domain setups and (2) is particularly suitable for multi-domain specialization, where besides advantageous computational footprint, it can offer better TOD performance. | ChiaChien Hung, Anne Lauscher, Simone Paolo Ponzetto, Goran Glavas |  |
| 248 |  |  [Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-acl.73) |  | 0 | Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks. | Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang, Hai Zhao |  |
| 249 |  |  [Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns](https://doi.org/10.18653/v1/2022.findings-acl.74) |  | 0 | We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns—during fine-tuning—different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens—for each task and model layer—and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training. | Zihan Wang, Jiuxiang Gu, Jason Kuen, Handong Zhao, Vlad I. Morariu, Ruiyi Zhang, Ani Nenkova, Tong Sun, Jingbo Shang |  |
| 250 |  |  [Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment](https://doi.org/10.18653/v1/2022.findings-acl.75) |  | 0 | Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment. In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system’s performance itself, and 2) providing the model with the ability to explain the correctness or incorrectness of an answer. We collect a retrieval-based QA dataset, FeedbackQA, which contains interactive feedback from users. We collect this dataset by deploying a base QA system to crowdworkers who then engage with the system and provide feedback on the quality of its answers. The feedback contains both structured ratings and unstructured natural language explanations. We train a neural model with this feedback data that can generate explanations and re-score answer candidates. We show that feedback data not only improves the accuracy of the deployed QA system but also other stronger non-deployed systems. The generated explanations also help users make informed decisions about the correctness of answers. | Zichao Li, Prakhar Sharma, Xing Han Lù, Jackie Chi Kit Cheung, Siva Reddy |  |
| 251 |  |  [To be or not to be an Integer? Encoding Variables for Mathematical Text](https://doi.org/10.18653/v1/2022.findings-acl.76) |  | 0 | The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables. | Deborah Ferreira, Mokanarangan Thayaparan, Marco Valentino, Julia Rozanova, André Freitas |  |
| 252 |  |  [GRS: Combining Generation and Revision in Unsupervised Sentence Simplification](https://doi.org/10.18653/v1/2022.findings-acl.77) |  | 0 | We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision. We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation. This allows us to combine the advantages of generative and revision-based approaches: paraphrasing captures complex edit operations, and the use of explicit edit operations in an iterative manner provides controllability and interpretability. We demonstrate these advantages of GRS compared to existing methods on the Newsela and ASSET datasets. | Mohammad Dehghan, Dhruv Kumar, Lukasz Golab |  |
| 253 |  |  [BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages](https://doi.org/10.18653/v1/2022.findings-acl.78) |  | 0 | Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika. Then, we compare the morphologically inspired segmentation methods against Byte-Pair Encodings (BPEs) as inputs for machine translation (MT) when translating to and from Spanish. We show that for all language pairs except for Nahuatl, an unsupervised morphological segmentation algorithm outperforms BPEs consistently and that, although supervised methods achieve better segmentation scores, they under-perform in MT challenges. Finally, we contribute two new morphological segmentation datasets for Raramuri and Shipibo-Konibo, and a parallel corpus for Raramuri–Spanish. | Manuel Mager, Arturo Oncevay, Elisabeth Mager, Katharina Kann, Ngoc Thang Vu |  |
| 254 |  |  [Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.79) |  | 0 | We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference. We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more effectively than the softmax baseline. We show that MC Dropout is able to achieve decent performance without any distribution annotations while Re-Calibration can give further improvements with extra distribution annotations, suggesting the value of multiple annotations for one example in modeling the distribution of human judgements. Despite these improvements, the best results are still far below the estimated human upper-bound, indicating that predicting the distribution of human judgements is still an open, challenging problem with a large room for improvements. We showcase the common errors for MC Dropout and Re-Calibration. Finally, we give guidelines on the usage of these methods with different levels of data availability and encourage future work on modeling the human opinion distribution for language reasoning. | Xiang Zhou, Yixin Nie, Mohit Bansal |  |
| 255 |  |  [Morphological Processing of Low-Resource Languages: Where We Are and What's Next](https://doi.org/10.18653/v1/2022.findings-acl.80) |  | 0 | Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources. First, we survey recent developments in computational morphology with a focus on low-resource languages. Second, we argue that the field is ready to tackle the logical next challenge: understanding a language’s morphology from raw text alone. We perform an empirical study on a truly unsupervised version of the paradigm completion task and show that, while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement. The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes. | Adam Wiemerslage, Miikka Silfverberg, Changbing Yang, Arya McCarthy, Garrett Nicolai, Eliana Colunga, Katharina Kann |  |
| 256 |  |  [Learning and Evaluating Character Representations in Novels](https://doi.org/10.18653/v1/2022.findings-acl.81) |  | 0 | We address the problem of learning fixed-length vector representations of characters in novels. Recent advances in word embeddings have proven successful in learning entity representations from short texts, but fall short on longer documents because they do not capture full book-level information. To overcome the weakness of such text-based embeddings, we propose two novel methods for representing characters: (i) graph neural network-based embeddings from a full corpus-based character network; and (ii) low-dimensional embeddings constructed from the occurrence pattern of characters in each novel. We test the quality of these character embeddings using a new benchmark suite to evaluate character representations, encompassing 12 different tasks. We show that our representation techniques combined with text-based embeddings lead to the best character representations, outperforming text-based embeddings in four tasks. Our dataset and evaluation script will be made publicly available to stimulate additional work in this area. | Naoya Inoue, Charuta Pethe, Allen Kim, Steven Skiena |  |
| 257 |  |  [Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.findings-acl.82) |  | 0 | Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For multiple-choice exams there is often a negative marking scheme; there is a penalty for an incorrect answer. In terms of an MRC system this means that the system is required to have an idea of the uncertainty in the predicted answer. The second consideration is that many multiple-choice questions have the option of none-of-the-above (NOA) indicating that none of the answers is applicable, rather than there always being the correct answer in the list of choices. This paper investigates both of these issues by making use of predictive uncertainty. Whether the system should propose an answer is a direct application of answer uncertainty. There are two possibilities when considering the NOA option. The simplest is to explicitly build a system on data that includes this option. Alternatively uncertainty can be applied to detect whether the other options include the correct answer. If the system is not sufficiently confident it will select NOA. As there is no standard corpus available to investigate these topics, the ReClor corpus is modified by removing the correct answer from a subset of possible answers. A high-performance MRC system is used to evaluate whether answer uncertainty can be applied in these situations. It is shown that uncertainty does allow questions that the system is not confident about to be detected. Additionally it is shown that uncertainty outperforms a system explicitly built with an NOA option. | Vatsal Raina, Mark J. F. Gales |  |
| 258 |  |  [Measuring the Language of Self-Disclosure across Corpora](https://doi.org/10.18653/v1/2022.findings-acl.83) |  | 0 | Being able to reliably estimate self-disclosure – a key component of friendship and intimacy – from language is important for many psychology studies. We build single-task models on five self-disclosure corpora, but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean Pearson’s r=0.69) is much higher than the respective across data set accuracy (mean Pearson’s r=0.32), due to both variations in the corpora (e.g., medical vs. general topics) and labeling instructions (target variables: self-disclosure, emotional disclosure, intimacy). However, some lexical features, such as expression of negative emotions and use of first person personal pronouns such as ‘I’ reliably predict self-disclosure across corpora. We develop a multi-task model that yields better results, with an average Pearson’s r of 0.37 for out-of-corpora prediction. | AnnKatrin Reuel, Sebastian Peralta, João Sedoc, Garrick Sherman, Lyle H. Ungar |  |
| 259 |  |  [When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation](https://doi.org/10.18653/v1/2022.findings-acl.84) |  | 0 | Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively generate or re-weight augmented samples with respect to the task objective during training. However, these adaptive DA methods: (1) are computationally expensive and not sample-efficient, and (2) are designed merely for a specific setting. In this work, we present a universal DA technique, called Glitter, to overcome both issues. Glitter can be plugged into any DA method, making training sample-efficient without sacrificing performance. From a pre-generated pool of augmented samples, Glitter adaptively selects a subset of worst-case samples with maximal loss, analogous to adversarial DA. Without altering the training strategy, the task objective can be optimized on the selected subset. Our thorough experiments on the GLUE benchmark, SQuAD, and HellaSwag in three widely used training setups including consistency training, self-distillation and knowledge distillation reveal that Glitter is substantially faster to train and achieves a competitive performance, compared to strong baselines. | Ehsan Kamalloo, Mehdi Rezagholizadeh, Ali Ghodsi |  |
| 260 |  |  [Explaining Classes through Stable Word Attributions](https://doi.org/10.18653/v1/2022.findings-acl.85) |  | 0 | Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP. Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established. We explore explanations based on XLM-R and the Integrated Gradients input attribution method, and propose 1) the Stable Attribution Class Explanation method (SACX) to extract keyword lists of classes in text classification tasks, and 2) a framework for the systematic evaluation of the keyword lists. We find that explanations of individual predictions are prone to noise, but that stable explanations can be effectively identified through repeated training and explanation. We evaluate on web register data and show that the class explanations are linguistically meaningful and distinguishing of the classes. | Samuel Rönnqvist, AkiJuhani Kyröläinen, Amanda Myntti, Filip Ginter, Veronika Laippala |  |
| 261 |  |  [What to Learn, and How: Toward Effective Learning from Rationales](https://doi.org/10.18653/v1/2022.findings-acl.86) |  | 0 | Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction. Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training. | Samuel Carton, Surya Kanoria, Chenhao Tan |  |
| 262 |  |  [Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments](https://doi.org/10.18653/v1/2022.findings-acl.87) |  | 0 | Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and annotating the data – as opposed to giving companies and governments control over defining and combatting hate speech. This inclusive approach results in datasets more representative of actually occurring online speech and is likely to facilitate the removal of the social media content that marginalized communities view as causing the most harm. Based on XTREMESPEECH, we establish novel tasks with accompanying baselines, provide evidence that cross-country training is generally not feasible due to cultural differences between countries and perform an interpretability analysis of BERT’s predictions. | Antonis Maronikolakis, Axel Wisiorek, Leah Nann, Haris Jabbar, Sahana Udupa, Hinrich Schütze |  |
| 263 |  |  [Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists](https://doi.org/10.18653/v1/2022.findings-acl.88) |  | 0 | Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected. Instead, we propose a knowledge-free Entropy-based Attention Regularization (EAR) to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low self-attention entropy. We fine-tune BERT via EAR: the resulting model matches or exceeds state-of-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian.EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions. | Giuseppe Attanasio, Debora Nozza, Dirk Hovy, Elena Baralis |  |
| 264 |  |  [From BERT's Point of View: Revealing the Prevailing Contextual Differences](https://doi.org/10.18653/v1/2022.findings-acl.89) |  | 0 | Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from ‘BERT’s point of view’. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information. | Carolin Schuster, Simon Hegelich |  |
| 265 |  |  [Learning Bias-reduced Word Embeddings Using Dictionary Definitions](https://doi.org/10.18653/v1/2022.findings-acl.90) |  | 0 | Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging ̲dictionary ̲definitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively neutral dictionary definition representations. Existing debiasing algorithms typically need a pre-compiled list of seed words to represent the bias direction, along which biased information gets removed. Producing this list involves subjective decisions and it might be difficult to obtain for some types of biases. We automate the process of finding seed words: our algorithm starts from a single pair of initial seed words and automatically finds more words whose definitions display similar attributes traits. We demonstrate the effectiveness of our approach with benchmark evaluations and empirical analyses. Our code is available at https://github.com/haozhe-an/DD-GloVe. | Haozhe An, Xiaojiang Liu, Donald Zhang |  |
| 266 |  |  [Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy](https://doi.org/10.18653/v1/2022.findings-acl.91) |  | 0 | Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss limits the scores of positive and negative triplets to have a suitable margin. The recently proposed Limit-based Scoring Loss independently limits the range of positive and negative triplet scores. However, these loss frameworks use equal or fixed penalty terms to reduce the scores of positive and negative sample pairs, which is inflexible in optimization. Our intuition is that if a triplet score deviates far from the optimum, it should be emphasized. To this end, we propose Adaptive Limit Scoring Loss, which simply re-weights each triplet to highlight the less-optimized triplet scores. We apply this loss framework to several knowledge graph embedding models such as TransE, TransH and ComplEx. The experimental results on link prediction and triplet classification show that our proposed method has achieved performance on par with the state of the art. | Jinfa Yang, Xianghua Ying, Yongjie Shi, Xin Tong, Ruibin Wang, Taiyan Chen, Bowei Xing |  |
| 267 |  |  [OCR Improves Machine Translation for Low-Resource Languages](https://doi.org/10.18653/v1/2022.findings-acl.92) |  | 0 | We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and analyse most common errors. We show that OCR monolingual data is a valuable resource that can increase performance of Machine Translation models, when used in backtranslation. We then perform an ablation study to investigate how OCR errors impact Machine Translation performance and determine what is the minimum level of OCR quality needed for the monolingual data to be useful for Machine Translation. | Oana Ignat, Jean Maillard, Vishrav Chaudhary, Francisco Guzmán |  |
| 268 |  |  [CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations](https://doi.org/10.18653/v1/2022.findings-acl.93) |  | 0 | Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between “Jim yells at Bob” and “Bob is upset”). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM. | Changlong Yu, Hongming Zhang, Yangqiu Song, Wilfred Ng |  |
| 269 |  |  [Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming](https://doi.org/10.18653/v1/2022.findings-acl.94) |  | 0 | A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets. | Ayush Maheshwari, KrishnaTeja Killamsetty, Ganesh Ramakrishnan, Rishabh K. Iyer, Marina Danilevsky, Lucian Popa |  |
| 270 |  |  [Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction](https://doi.org/10.18653/v1/2022.findings-acl.95) |  | 0 | The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the effective semantic connections between distant clauses, leading to poor generalization ability towards position-insensitive data. To alleviate the problem, we propose a novel Multi-Granularity Semantic Aware Graph model (MGSAG) to incorporate fine-grained and coarse-grained semantic features jointly, without regard to distance limitation. In particular, we first explore semantic dependencies between clauses and keywords extracted from the document that convey fine-grained semantic features, obtaining keywords enhanced clause representations. Besides, a clause graph is also established to model coarse-grained semantic relations between clauses. Experimental results indicate that MGSAG surpasses the existing state-of-the-art ECPE models. Especially, MGSAG outperforms other models significantly in the condition of position-insensitive data. | Yinan Bao, Qianwen Ma, Lingwei Wei, Wei Zhou, Songlin Hu |  |
| 271 |  |  [Cross-lingual Inference with A Chinese Entailment Graph](https://doi.org/10.18653/v1/2022.findings-acl.96) |  | 0 |  | Tianyi Li, Sabine Weber, Mohammad Javad Hosseini, Liane Guillou, Mark Steedman |  |
| 272 |  |  [Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction](https://doi.org/10.18653/v1/2022.findings-acl.97) |  | 0 |  | Xuhang Xie, Xuesong Lu, Bei Chen |  |
| 273 |  |  [MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction](https://doi.org/10.18653/v1/2022.findings-acl.98) |  | 0 |  | Chenxi Zhu, Ziqiang Ying, Boyu Zhang, Feng Mao |  |
| 274 |  |  [S²SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.findings-acl.99) |  | 0 |  | Binyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Yanyang Li, Bowen Li, Jian Sun, Yongbin Li |  |
| 275 |  |  [Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers](https://doi.org/10.18653/v1/2022.findings-acl.100) |  | 0 |  | Mariano Felice, Shiva Taslimipoor, Paula Buttery |  |
| 276 |  |  [Co-training an Unsupervised Constituency Parser with Weak Supervision](https://doi.org/10.18653/v1/2022.findings-acl.101) |  | 0 |  | Nickil Maveli, Shay B. Cohen |  |
| 277 |  |  [HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information](https://doi.org/10.18653/v1/2022.findings-acl.102) |  | 0 |  | Qian Ruan, Malte Ostendorff, Georg Rehm |  |
| 278 |  |  [An Isotropy Analysis in the Multilingual BERT Embedding Space](https://doi.org/10.18653/v1/2022.findings-acl.103) |  | 0 |  | Sara Rajaee, Mohammad Taher Pilehvar |  |
| 279 |  |  [Multi-Stage Prompting for Knowledgeable Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-acl.104) |  | 0 |  | Zihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad Shoeybi, Bryan Catanzaro |  |
| 280 |  |  [DuReadervis: A Chinese Dataset for Open-domain Document Visual Question Answering](https://doi.org/10.18653/v1/2022.findings-acl.105) |  | 0 |  | Le Qi, Shangwen Lv, Hongyu Li, Jing Liu, Yu Zhang, Qiaoqiao She, Hua Wu, Haifeng Wang, Ting Liu |  |
| 281 |  |  [Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models](https://doi.org/10.18653/v1/2022.findings-acl.106) |  | 0 |  | Aaron Mueller, Robert Frank, Tal Linzen, Luheng Wang, Sebastian Schuster |  |
| 282 |  |  [C³KG: A Chinese Commonsense Conversation Knowledge Graph](https://doi.org/10.18653/v1/2022.findings-acl.107) |  | 0 |  | Dawei Li, Yanran Li, Jiayi Zhang, Ke Li, Chen Wei, Jianwei Cui, Bin Wang |  |
| 283 |  |  [Graph Neural Networks for Multiparallel Word Alignment](https://doi.org/10.18653/v1/2022.findings-acl.108) |  | 0 |  | Ayyoob Imani, Lütfi Kerem Senel, Masoud Jalili Sabet, François Yvon, Hinrich Schütze |  |
| 284 |  |  [Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors](https://doi.org/10.18653/v1/2022.findings-acl.109) |  | 0 |  | Yang Wu, Yanyan Zhao, Hao Yang, Song Chen, Bing Qin, Xiaohuan Cao, Wenting Zhao |  |
| 285 |  |  [A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge](https://doi.org/10.18653/v1/2022.findings-acl.110) |  | 0 |  | Tao Wang, Linhai Zhang, Chenchen Ye, Junxi Liu, Deyu Zhou |  |
| 286 |  |  [Effective Unsupervised Constrained Text Generation based on Perturbed Masking](https://doi.org/10.18653/v1/2022.findings-acl.111) |  | 0 |  | Yingwen Fu, Wenjie Ou, Zhou Yu, Yue Lin |  |
| 287 |  |  [Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.112) |  | 0 |  | Songlin Yang, Kewei Tu |  |
| 288 |  |  [End-to-End Speech Translation for Code Switched Speech](https://doi.org/10.18653/v1/2022.findings-acl.113) |  | 0 |  | Orion Weller, Matthias Sperber, Telmo Pires, Hendra Setiawan, Christian Gollan, Dominic Telaar, Matthias Paulik |  |
| 289 |  |  [A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking](https://doi.org/10.18653/v1/2022.findings-acl.114) |  | 0 |  | Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, Xudong Liu |  |
| 290 |  |  [Finding the Dominant Winning Ticket in Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.115) |  | 0 |  | Zhuocheng Gong, Di He, Yelong Shen, TieYan Liu, Weizhu Chen, Dongyan Zhao, JiRong Wen, Rui Yan |  |
| 291 |  |  [Thai Nested Named Entity Recognition Corpus](https://doi.org/10.18653/v1/2022.findings-acl.116) |  | 0 |  | Weerayut Buaphet, Can Udomcharoenchaikit, Peerat Limkonchotiwat, Attapol Rutherford, Sarana Nutanong |  |
| 292 |  |  [Two-Step Question Retrieval for Open-Domain QA](https://doi.org/10.18653/v1/2022.findings-acl.117) |  | 0 |  | Yeon Seonwoo, Juhee Son, Jiho Jin, SangWoo Lee, JiHoon Kim, JungWoo Ha, Alice Oh |  |
| 293 |  |  [Semantically Distributed Robust Optimization for Vision-and-Language Inference](https://doi.org/10.18653/v1/2022.findings-acl.118) |  | 0 |  | Tejas Gokhale, Abhishek Chaudhary, Pratyay Banerjee, Chitta Baral, Yezhou Yang |  |
| 294 |  |  [Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference](https://doi.org/10.18653/v1/2022.findings-acl.119) |  | 0 |  | YongHo Jung, JunHyung Park, JoonYoung Choi, Mingyu Lee, Junho Kim, KangMin Kim, SangKeun Lee |  |
| 295 |  |  [Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-acl.120) |  | 0 |  | Yuxia Wang, Minghan Wang, Yimeng Chen, Shimin Tao, Jiaxin Guo, Chang Su, Min Zhang, Hao Yang |  |
| 296 |  |  [Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers](https://doi.org/10.18653/v1/2022.findings-acl.121) |  | 0 |  | Jakob Smedegaard Andersen, Walid Maalej |  |
| 297 |  |  [Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?](https://doi.org/10.18653/v1/2022.findings-acl.122) |  | 0 |  | Mousumi Akter, Naman Bansal, Shubhra Kanti Karmaker Santu |  |
| 298 |  |  [Open Vocabulary Extreme Classification Using Generative Models](https://doi.org/10.18653/v1/2022.findings-acl.123) |  | 0 |  | Daniel Simig, Fabio Petroni, Pouya Yanki, Kashyap Popat, Christina Du, Sebastian Riedel, Majid Yazdani |  |
| 299 |  |  [Decomposed Meta-Learning for Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.124) |  | 0 |  | Tingting Ma, Huiqiang Jiang, Qianhui Wu, Tiejun Zhao, ChinYew Lin |  |
| 300 |  |  [TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge](https://doi.org/10.18653/v1/2022.findings-acl.125) |  | 0 |  | ChaoHong Tan, JiaChen Gu, Chongyang Tao, ZhenHua Ling, Can Xu, Huang Hu, Xiubo Geng, Daxin Jiang |  |
| 301 |  |  [EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition](https://doi.org/10.18653/v1/2022.findings-acl.126) |  | 0 |  | Zaijing Li, Fengxiao Tang, Ming Zhao, Yusen Zhu |  |
| 302 |  |  [Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text](https://doi.org/10.18653/v1/2022.findings-acl.127) |  | 0 |  | Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan |  |
| 303 |  |  [Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text](https://doi.org/10.18653/v1/2022.findings-acl.128) |  | 0 |  | Amir Pouran Ben Veyseh, Ning Xu, Quan Hung Tran, Varun Manjunatha, Franck Dernoncourt, Thien Huu Nguyen |  |
| 304 |  |  [Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph](https://doi.org/10.18653/v1/2022.findings-acl.129) |  | 0 |  | Yubo Chen, Yunqi Zhang, Yongfeng Huang |  |
| 305 |  |  [Document-Level Event Argument Extraction via Optimal Transport](https://doi.org/10.18653/v1/2022.findings-acl.130) |  | 0 |  | Amir Pouran Ben Veyseh, Minh Van Nguyen, Franck Dernoncourt, Bonan Min, Thien Huu Nguyen |  |
| 306 |  |  [N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.131) |  | 0 |  | Ibrahim Taha Aksu, Zhengyuan Liu, MinYen Kan, Nancy F. Chen |  |
| 307 |  |  [Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-acl.132) |  | 0 |  | Qingyu Tan, Ruidan He, Lidong Bing, Hwee Tou Ng |  |
| 308 |  |  [Calibration of Machine Reading Systems at Scale](https://doi.org/10.18653/v1/2022.findings-acl.133) |  | 0 |  | Shehzaad Dhuliawala, Leonard Adolphs, Rajarshi Das, Mrinmaya Sachan |  |
| 309 |  |  [Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples](https://doi.org/10.18653/v1/2022.findings-acl.134) |  | 0 |  | Jianhan Xu, Cenyuan Zhang, Xiaoqing Zheng, Linyang Li, ChoJui Hsieh, KaiWei Chang, Xuanjing Huang |  |
| 310 |  |  [Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects](https://doi.org/10.18653/v1/2022.findings-acl.135) |  | 0 |  | Go Inoue, Salam Khalifa, Nizar Habash |  |
| 311 |  |  [How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis](https://doi.org/10.18653/v1/2022.findings-acl.136) |  | 0 |  | Shaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong, Chengjie Sun, Bingquan Liu, Zhenzhou Ji, Xin Jiang, Qun Liu |  |
| 312 |  |  [Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models](https://doi.org/10.18653/v1/2022.findings-acl.137) |  | 0 |  | Simran Arora, Sen Wu, Enci Liu, Christopher Ré |  |
| 313 |  |  [Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense](https://doi.org/10.18653/v1/2022.findings-acl.138) |  | 0 |  | Wanyun Cui, Xingran Chen |  |
| 314 |  |  [Weighted self Distillation for Chinese word segmentation](https://doi.org/10.18653/v1/2022.findings-acl.139) |  | 0 |  | Rian He, Shubin Cai, Zhong Ming, Jialei Zhang |  |
| 315 |  |  [Sibylvariant Transformations for Robust Text Classification](https://doi.org/10.18653/v1/2022.findings-acl.140) |  | 0 |  | Fabrice HarelCanada, Muhammad Ali Gulzar, Nanyun Peng, Miryung Kim |  |
| 316 |  |  [DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.141) |  | 0 |  | Cheonbok Park, Hantae Kim, Ioan Calapodescu, Hyunchang Cho, Vassilina Nikoulina |  |
| 317 |  |  [Hey AI, Can You Solve Complex Tasks by Talking to Agents?](https://doi.org/10.18653/v1/2022.findings-acl.142) |  | 0 |  | Tushar Khot, Kyle Richardson, Daniel Khashabi, Ashish Sabharwal |  |
| 318 |  |  [Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion](https://doi.org/10.18653/v1/2022.findings-acl.143) |  | 0 |  | Yiqun Yao, Rada Mihalcea |  |
| 319 |  |  [BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.144) |  | 0 |  | Shuo Liang, Wei Wei, XianLing Mao, Fei Wang, Zhiyong He |  |
| 320 |  |  [IndicBART: A Pre-trained Model for Indic Natural Language Generation](https://doi.org/10.18653/v1/2022.findings-acl.145) |  | 0 |  | Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra, Pratyush Kumar |  |
| 321 |  |  [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models](https://doi.org/10.18653/v1/2022.findings-acl.146) |  | 0 |  | Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, Yinfei Yang |  |
| 322 |  |  [Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking](https://doi.org/10.18653/v1/2022.findings-acl.147) |  | 0 |  | Yuanhe Tian, Yan Song, Fei Xia |  |
| 323 |  |  [Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks](https://doi.org/10.18653/v1/2022.findings-acl.148) |  | 0 |  | Ashutosh Kumar, Aditya Joshi |  |
| 324 |  |  [Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts](https://doi.org/10.18653/v1/2022.findings-acl.149) |  | 0 |  | Wenhao Yu, Chenguang Zhu, Lianhui Qin, Zhihan Zhang, Tong Zhao, Meng Jiang |  |
| 325 |  |  [Dict-BERT: Enhancing Language Model Pre-training with Dictionary](https://doi.org/10.18653/v1/2022.findings-acl.150) |  | 0 |  | Wenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu, Shuohang Wang, Yichong Xu, Michael Zeng, Meng Jiang |  |
| 326 |  |  [A Feasibility Study of Answer-Unaware Question Generation for Education](https://doi.org/10.18653/v1/2022.findings-acl.151) |  | 0 |  | Liam Dugan, Eleni Miltsakaki, Shriyash Upadhyay, Etan Ginsberg, Hannah Gonzalez, DaHyeon Choi, Chuning Yuan, Chris CallisonBurch |  |
| 327 |  |  [Relevant CommonSense Subgraphs for "What if..." Procedural Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.152) |  | 0 |  | Chen Zheng, Parisa Kordjamshidi |  |
| 328 |  |  [Combining Feature and Instance Attribution to Detect Artifacts](https://doi.org/10.18653/v1/2022.findings-acl.153) |  | 0 |  | Pouya Pezeshkpour, Sarthak Jain, Sameer Singh, Byron C. Wallace |  |
| 329 |  |  [Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.154) |  | 0 |  | Aaron Reich, Jiaao Chen, Aastha Agrawal, Yanzhe Zhang, Diyi Yang |  |
| 330 |  |  [Label Semantics for Few Shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.155) |  | 0 |  | Jie Ma, Miguel Ballesteros, Srikanth Doss, Rishita Anubhai, Sunil Mallya, Yaser AlOnaizan, Dan Roth |  |
| 331 |  |  [Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem](https://doi.org/10.18653/v1/2022.findings-acl.156) |  | 0 |  | Khalil Mrini, Shaoliang Nie, Jiatao Gu, Sinong Wang, Maziar Sanjabi, Hamed Firooz |  |
| 332 |  |  [VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator](https://doi.org/10.18653/v1/2022.findings-acl.157) |  | 0 |  | Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson Piramuthu, Gökhan Tür, Devi Parikh, Dilek HakkaniTur |  |
| 333 |  |  [Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings](https://doi.org/10.18653/v1/2022.findings-acl.158) |  | 0 |  | Neeraj Varshney, Swaroop Mishra, Chitta Baral |  |
| 334 |  |  [Unsupervised Natural Language Inference Using PHL Triplet Generation](https://doi.org/10.18653/v1/2022.findings-acl.159) |  | 0 |  | Neeraj Varshney, Pratyay Banerjee, Tejas Gokhale, Chitta Baral |  |
| 335 |  |  [Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue](https://doi.org/10.18653/v1/2022.findings-acl.160) |  | 0 |  | Evgeniia Razumovskaia, Ivan Vulic, Anna Korhonen |  |
| 336 |  |  [Ranking-Constrained Learning with Rationales for Text Classification](https://doi.org/10.18653/v1/2022.findings-acl.161) |  | 0 |  | Juanyan Wang, Manali Sharma, Mustafa Bilgic |  |
| 337 |  |  [CaM-Gen: Causally Aware Metric-Guided Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.162) |  | 0 |  | Navita Goyal, Roodram Paneri, Ayush Agarwal, Udit Kalani, Abhilasha Sancheti, Niyati Chhaya |  |
| 338 |  |  [Training Dynamics for Text Summarization Models](https://doi.org/10.18653/v1/2022.findings-acl.163) |  | 0 |  | Tanya Goyal, Jiacheng Xu, Junyi Jessy Li, Greg Durrett |  |
| 339 |  |  [Richer Countries and Richer Representations](https://doi.org/10.18653/v1/2022.findings-acl.164) |  | 0 |  | Kaitlyn Zhou, Kawin Ethayarajh, Dan Jurafsky |  |
| 340 |  |  [BBQ: A hand-built bias benchmark for question answering](https://doi.org/10.18653/v1/2022.findings-acl.165) |  | 0 |  | Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, Samuel R. Bowman |  |
| 341 |  |  [Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble](https://doi.org/10.18653/v1/2022.findings-acl.166) |  | 0 |  | Xinjian Li, Florian Metze, David R. Mortensen, Shinji Watanabe, Alan W. Black |  |
| 342 |  |  [Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Under-Documented Languages](https://doi.org/10.18653/v1/2022.findings-acl.167) |  | 0 |  | Clarissa Forbes, Farhan Samir, Bruce Harold Oliver, Changbing Yang, Edith Coates, Garrett Nicolai, Miikka Silfverberg |  |
| 343 |  |  [Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask](https://doi.org/10.18653/v1/2022.findings-acl.168) |  | 0 |  | Bilal Ghanem, Lauren Lutz Coleman, Julia Rivard Dexter, Spencer McIntosh von der Ohe, Alona Fyshe |  |
| 344 |  |  [TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.169) |  | 0 |  | Megan Leszczynski, Daniel Y. Fu, Mayee F. Chen, Christopher Ré |  |
| 345 |  |  [Hierarchical Recurrent Aggregative Generation for Few-Shot NLG](https://doi.org/10.18653/v1/2022.findings-acl.170) |  | 0 |  | Giulio Zhou, Gerasimos Lampouras, Ignacio Iacobacci |  |
| 346 |  |  [Training Text-to-Text Transformers with Privacy Guarantees](https://doi.org/10.18653/v1/2022.findings-acl.171) |  | 0 |  | Natalia Ponomareva, Jasmijn Bastings, Sergei Vassilvitskii |  |
| 347 |  |  [Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers](https://doi.org/10.18653/v1/2022.findings-acl.172) |  | 0 |  | Christopher Schröder, Andreas Niekler, Martin Potthast |  |
| 348 |  |  [The impact of lexical and grammatical processing on generating code from natural language](https://doi.org/10.18653/v1/2022.findings-acl.173) |  | 0 |  | Nathanaël Beau, Benoît Crabbé |  |
| 349 |  |  [Seq2Path: Generating Sentiment Tuples as Paths of a Tree](https://doi.org/10.18653/v1/2022.findings-acl.174) |  | 0 |  | Yue Mao, Yi Shen, Jingchao Yang, Xiaoying Zhu, Longjun Cai |  |
| 350 |  |  [Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training](https://doi.org/10.18653/v1/2022.findings-acl.175) |  | 0 |  | Pengwei Zhan, Yang Wu, Shaolei Zhou, Yunjian Zhang, Liming Wang |  |
| 351 |  |  [Your fairness may vary: Pretrained language model fairness in toxic text classification](https://doi.org/10.18653/v1/2022.findings-acl.176) |  | 0 |  | Ioana Baldini, Dennis Wei, Karthikeyan Natesan Ramamurthy, Moninder Singh, Mikhail Yurochkin |  |
| 352 |  |  [ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.177) |  | 0 |  | Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, Enamul Hoque |  |
| 353 |  |  [A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification](https://doi.org/10.18653/v1/2022.findings-acl.178) |  | 0 |  | Dairui Liu, Derek Greene, Ruihai Dong |  |
| 354 |  |  [Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples](https://doi.org/10.18653/v1/2022.findings-acl.179) |  | 0 |  | Yu Xia, Quan Wang, Yajuan Lyu, Yong Zhu, Wenhao Wu, Sujian Li, Dai Dai |  |
| 355 |  |  [Phoneme transcription of endangered languages: an evaluation of recent ASR architectures in the single speaker scenario](https://doi.org/10.18653/v1/2022.findings-acl.180) |  | 0 |  | Gilles Boulianne |  |
| 356 |  |  [Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task](https://doi.org/10.18653/v1/2022.findings-acl.181) |  | 0 |  | Karim Lasri, Alessandro Lenci, Thierry Poibeau |  |
| 357 |  |  [Combining Static and Contextualised Multilingual Embeddings](https://doi.org/10.18653/v1/2022.findings-acl.182) |  | 0 |  | Katharina Hämmerl, Jindrich Libovický, Alexander Fraser |  |
| 358 |  |  [An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection](https://doi.org/10.18653/v1/2022.findings-acl.183) |  | 0 |  | Shengxuan Luo, Sheng Yu |  |
| 359 |  |  [Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold](https://doi.org/10.18653/v1/2022.findings-acl.184) |  | 0 |  | Sebastian Ruder, Ivan Vulic, Anders Søgaard |  |
| 360 |  |  [Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective](https://doi.org/10.18653/v1/2022.findings-acl.185) |  | 0 |  | Edoardo Manino, Julia Rozanova, Danilo S. Carvalho, André Freitas, Lucas C. Cordeiro |  |
| 361 |  |  [Improving Neural Political Statement Classification with Class Hierarchical Information](https://doi.org/10.18653/v1/2022.findings-acl.186) |  | 0 |  | Erenay Dayanik, André Blessing, Nico Blokker, Sebastian Haunss, Jonas Kuhn, Gabriella Lapesa, Sebastian Padó |  |
| 362 |  |  [Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-acl.187) |  | 0 |  | Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung |  |
| 363 |  |  [Co-VQA : Answering by Interactive Sub Question Sequence](https://doi.org/10.18653/v1/2022.findings-acl.188) |  | 0 |  | Ruonan Wang, Yuxi Qian, Fangxiang Feng, Xiaojie Wang, Huixing Jiang |  |
| 364 |  |  [A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation](https://doi.org/10.18653/v1/2022.findings-acl.189) |  | 0 |  | Tianxiang Sun, Xiangyang Liu, Wei Zhu, Zhichao Geng, Lingling Wu, Yilong He, Yuan Ni, Guotong Xie, Xuanjing Huang, Xipeng Qiu |  |
| 365 |  |  [Auxiliary tasks to boost Biaffine Semantic Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.190) |  | 0 |  | Marie Candito |  |
| 366 |  |  [Syntax-guided Contrastive Learning for Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-acl.191) |  | 0 |  | Shuai Zhang, Lijie Wang, Xinyan Xiao, Hua Wu |  |
| 367 |  |  [Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting](https://doi.org/10.18653/v1/2022.findings-acl.192) |  | 0 |  | Ilias Chalkidis, Anders Søgaard |  |
| 368 |  |  [ASCM: An Answer Space Clustered Prompting Method without Answer Engineering](https://doi.org/10.18653/v1/2022.findings-acl.193) |  | 0 |  | Zhen Wang, Yating Yang, Zhou Xi, Bo Ma, Lei Wang, Rui Dong, Azmat Anwar |  |
| 369 |  |  [Why don't people use character-level machine translation?](https://doi.org/10.18653/v1/2022.findings-acl.194) |  | 0 |  | Jindrich Libovický, Helmut Schmid, Alexander Fraser |  |
| 370 |  |  [Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2022.findings-acl.195) |  | 0 |  | Zhongli Li, Wenxuan Zhang, Chao Yan, Qingyu Zhou, Chao Li, Hongzhi Liu, Yunbo Cao |  |
| 371 |  |  [xGQA: Cross-Lingual Visual Question Answering](https://doi.org/10.18653/v1/2022.findings-acl.196) |  | 0 |  | Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, JanMartin O. Steitz, Stefan Roth, Ivan Vulic, Iryna Gurevych |  |
| 372 |  |  [Automatic Speech Recognition and Query By Example for Creole Languages Documentation](https://doi.org/10.18653/v1/2022.findings-acl.197) |  | 0 |  | Cécile Macaire, Didier Schwab, Benjamin Lecouteux, Emmanuel Schang |  |
| 373 |  |  [MReD: A Meta-Review Dataset for Structure-Controllable Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.198) |  | 0 |  | Chenhui Shen, Liying Cheng, Ran Zhou, Lidong Bing, Yang You, Luo Si |  |
| 374 |  |  [Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.199) |  | 0 |  | Sho Takase, Tatsuya Hiraoka, Naoaki Okazaki |  |
| 375 |  |  [Detecting Various Types of Noise for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.200) |  | 0 |  | Christian Herold, Jan Rosendahl, Joris Vanvinckenroye, Hermann Ney |  |
| 376 |  |  [DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training](https://doi.org/10.18653/v1/2022.findings-acl.201) |  | 0 |  | Luyang Huang, Guocheng Niu, Jiachen Liu, Xinyan Xiao, Hua Wu |  |
| 377 |  |  [HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.202) |  | 0 |  | Dongyang Li, Taolin Zhang, Nan Hu, Chengyu Wang, Xiaofeng He |  |
| 378 |  |  [Prompt-Driven Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.203) |  | 0 |  | Yafu Li, Yongjing Yin, Jing Li, Yue Zhang |  |
| 379 |  |  [On Controlling Fallback Responses for Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-acl.204) |  | 0 |  | Hongyuan Lu, Wai Lam, Hong Cheng, Helen Meng |  |
| 380 |  |  [CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions](https://doi.org/10.18653/v1/2022.findings-acl.205) |  | 0 |  | Tayfun Ates, Muhammed Samil Atesoglu, Cagatay Yigit, Ilker Kesen, Mert Kobas, Erkut Erdem, Aykut Erdem, Tilbe Göksun, Deniz Yuret |  |
| 381 |  |  [A Graph Enhanced BERT Model for Event Prediction](https://doi.org/10.18653/v1/2022.findings-acl.206) |  | 0 |  | Li Du, Xiao Ding, Yue Zhang, Ting Liu, Bing Qin |  |
| 382 |  |  [Long Time No See! Open-Domain Conversation with Long-Term Persona Memory](https://doi.org/10.18653/v1/2022.findings-acl.207) |  | 0 |  | Xinchao Xu, Zhibin Gou, Wenquan Wu, ZhengYu Niu, Hua Wu, Haifeng Wang, Shihang Wang |  |
| 383 |  |  [Lacking the Embedding of a Word? Look it up into a Traditional Dictionary](https://doi.org/10.18653/v1/2022.findings-acl.208) |  | 0 |  | Elena Sofia Ruzzetti, Leonardo Ranaldi, Michele Mastromattei, Francesca Fallucchi, Noemi Scarpato, Fabio Massimo Zanzotto |  |
| 384 |  |  [MTRec: Multi-Task Learning over BERT for News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.209) |  | 0 |  | Qiwei Bi, Jian Li, Lifeng Shang, Xin Jiang, Qun Liu, Hanfang Yang |  |
| 385 |  |  [Cross-domain Named Entity Recognition via Graph Matching](https://doi.org/10.18653/v1/2022.findings-acl.210) |  | 0 |  | Junhao Zheng, Haibin Chen, Qianli Ma |  |
| 386 |  |  [Assessing Multilingual Fairness in Pre-trained Multimodal Representations](https://doi.org/10.18653/v1/2022.findings-acl.211) |  | 0 |  | Jialu Wang, Yang Liu, Xin Eric Wang |  |
| 387 |  |  [More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models](https://doi.org/10.18653/v1/2022.findings-acl.212) |  | 0 |  | Jin Cheevaprawatdomrong, Alexandra Schofield, Attapol Rutherford |  |
| 388 |  |  [Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness](https://doi.org/10.18653/v1/2022.findings-acl.213) |  | 0 |  | Tejas Gokhale, Swaroop Mishra, Man Luo, Bhavdeep Singh Sachdeva, Chitta Baral |  |
| 389 |  |  [ASSIST: Towards Label Noise-Robust Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.214) |  | 0 |  | Fanghua Ye, Yue Feng, Emine Yilmaz |  |
| 390 |  |  [Graph Refinement for Coreference Resolution](https://doi.org/10.18653/v1/2022.findings-acl.215) |  | 0 |  | Lesly Miculicich, James Henderson |  |
| 391 |  |  [ECO v1: Towards Event-Centric Opinion Mining](https://doi.org/10.18653/v1/2022.findings-acl.216) |  | 0 |  | Ruoxi Xu, Hongyu Lin, Meng Liao, Xianpei Han, Jin Xu, Wei Tan, Yingfei Sun, Le Sun |  |
| 392 |  |  [Deep Reinforcement Learning for Entity Alignment](https://doi.org/10.18653/v1/2022.findings-acl.217) |  | 0 |  | Lingbing Guo, Yuqiang Han, Qiang Zhang, Huajun Chen |  |
| 393 |  |  [Breaking Down Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.218) |  | 0 |  | TingRui Chiang, YiPei Chen, YiTing Yeh, Graham Neubig |  |
| 394 |  |  [Mitigating Contradictions in Dialogue Based on Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-acl.219) |  | 0 |  | Weizhao Li, Junsheng Kong, Ben Liao, Yi Cai |  |
| 395 |  |  [ELLE: Efficient Lifelong Pre-training for Emerging Data](https://doi.org/10.18653/v1/2022.findings-acl.220) |  | 0 |  | Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 396 |  |  [EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in English](https://doi.org/10.18653/v1/2022.findings-acl.221) |  | 0 |  | Weicheng Ma, Samiha Datta, Lili Wang, Soroush Vosoughi |  |
| 397 |  |  [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://doi.org/10.18653/v1/2022.findings-acl.222) |  | 0 |  | Robert L. Logan IV, Ivana Balazevic, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel |  |
| 398 |  |  [uFACT: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.223) |  | 0 |  | Tisha Anders, Alexandru Coca, Bill Byrne |  |
| 399 |  |  [Good Night at 4 pm?! Time Expressions in Different Cultures](https://doi.org/10.18653/v1/2022.findings-acl.224) |  | 0 |  | Vered Shwartz |  |
| 400 |  |  [Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking](https://doi.org/10.18653/v1/2022.findings-acl.225) |  | 0 |  | Yifei Li, Pratheeksha Nair, Kellin Pelrine, Reihaneh Rabbany |  |
| 401 |  |  [OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.226) |  | 0 |  | Tong Niu, Kazuma Hashimoto, Yingbo Zhou, Caiming Xiong |  |
| 402 |  |  [Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective](https://doi.org/10.18653/v1/2022.findings-acl.227) |  | 0 |  | Osama Khalid, Jonathan Rusert, Padmini Srinivasan |  |
| 403 |  |  [Modeling Intensification for Sign Language Generation: A Computational Approach](https://doi.org/10.18653/v1/2022.findings-acl.228) |  | 0 |  | Mert Inan, Yang Zhong, Sabit Hassan, Lorna C. Quandt, Malihe Alikhani |  |
| 404 |  |  [Controllable Natural Language Generation with Contrastive Prefixes](https://doi.org/10.18653/v1/2022.findings-acl.229) |  | 0 |  | Jing Qian, Li Dong, Yelong Shen, Furu Wei, Weizhu Chen |  |
| 405 |  |  [Revisiting the Effects of Leakage on Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.230) |  | 0 |  | Nathaniel Krasner, Miriam Wanner, Antonios Anastasopoulos |  |
| 406 |  |  [Learning to Describe Solutions for Bug Reports Based on Developer Discussions](https://doi.org/10.18653/v1/2022.findings-acl.231) |  | 0 |  | Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, Raymond J. Mooney |  |
| 407 |  |  [Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense](https://doi.org/10.18653/v1/2022.findings-acl.232) |  | 0 |  | Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, Dongwon Lee |  |
| 408 |  |  [Improving Chinese Grammatical Error Detection via Data augmentation by Conditional Error Generation](https://doi.org/10.18653/v1/2022.findings-acl.233) |  | 0 |  | Tianchi Yue, Shulin Liu, Huihui Cai, Tao Yang, Shengkang Song, Tinghao Yu |  |
| 409 |  |  [Modular and Parameter-Efficient Multimodal Fusion with Prompting](https://doi.org/10.18653/v1/2022.findings-acl.234) |  | 0 |  | Sheng Liang, Mengjie Zhao, Hinrich Schütze |  |
| 410 |  |  [Synchronous Refinement for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.235) |  | 0 |  | Kehai Chen, Masao Utiyama, Eiichiro Sumita, Rui Wang, Min Zhang |  |
| 411 |  |  [HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2022.findings-acl.236) |  | 0 |  | Yanzhao Zheng, Haibin Wang, Baohua Dong, Xingjun Wang, Changshan Li |  |
| 412 |  |  [CRASpell: A Contextual Typo Robust Approach to Improve Chinese Spelling Correction](https://doi.org/10.18653/v1/2022.findings-acl.237) |  | 0 |  | Shulin Liu, Shengkang Song, Tianchi Yue, Tao Yang, Huihui Cai, Tinghao Yu, Shengli Sun |  |
| 413 |  |  [Gaussian Multi-head Attention for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.238) |  | 0 |  | Shaolei Zhang, Yang Feng |  |
| 414 |  |  [Composing Structure-Aware Batches for Pairwise Sentence Classification](https://doi.org/10.18653/v1/2022.findings-acl.239) |  | 0 |  | Andreas Waldis, Tilman Beck, Iryna Gurevych |  |
| 415 |  |  [Factual Consistency of Multilingual Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.240) |  | 0 |  | Constanza Fierro, Anders Søgaard |  |
| 416 |  |  [Selecting Stickers in Open-Domain Dialogue through Multitask Learning](https://doi.org/10.18653/v1/2022.findings-acl.241) |  | 0 |  | Zhexin Zhang, Yeshuang Zhu, Zhengcong Fei, Jinchao Zhang, Jie Zhou |  |
| 417 |  |  [ZiNet: Linking Chinese Characters Spanning Three Thousand Years](https://doi.org/10.18653/v1/2022.findings-acl.242) |  | 0 |  | Yang Chi, Fausto Giunchiglia, Daqian Shi, Xiaolei Diao, Chuntao Li, Hao Xu |  |
| 418 |  |  [How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?](https://doi.org/10.18653/v1/2022.findings-acl.243) |  | 0 |  | Hailong Jin, Tiansi Dong, Lei Hou, Juanzi Li, Hui Chen, Zelin Dai, Yincen Qu |  |
| 419 |  |  [AMR-DA: Data Augmentation by Abstract Meaning Representation](https://doi.org/10.18653/v1/2022.findings-acl.244) |  | 0 |  | Ziyi Shou, Yuxin Jiang, Fangzhen Lin |  |
| 420 |  |  [Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study](https://doi.org/10.18653/v1/2022.findings-acl.245) |  | 0 |  | Serra Sinem Tekiroglu, Helena Bonaldi, Margherita Fanton, Marco Guerini |  |
| 421 |  |  [Improving Robustness of Language Models from a Geometry-aware Perspective](https://doi.org/10.18653/v1/2022.findings-acl.246) |  | 0 |  | Bin Zhu, Zhaoquan Gu, Le Wang, Jinyin Chen, Qi Xuan |  |
| 422 |  |  [Task-guided Disentangled Tuning for Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.247) |  | 0 |  | Jiali Zeng, Yufan Jiang, Shuangzhi Wu, Yongjing Yin, Mu Li |  |
| 423 |  |  [Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding](https://doi.org/10.18653/v1/2022.findings-acl.248) |  | 0 |  | Rui Cao, Yihao Wang, Yuxin Liang, Ling Gao, Jie Zheng, Jie Ren, Zheng Wang |  |
| 424 |  |  [The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through](https://doi.org/10.18653/v1/2022.findings-acl.249) |  | 0 |  | Shruti Singh, Mayank Singh |  |
| 425 |  |  [Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.250) |  | 0 |  | Zheng Yuan, Chuanqi Tan, Songfang Huang, Fei Huang |  |
| 426 |  |  [UNIMO-2: End-to-End Unified Vision-Language Grounded Learning](https://doi.org/10.18653/v1/2022.findings-acl.251) |  | 0 |  | Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, Haifeng Wang |  |
| 427 |  |  [The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking](https://doi.org/10.18653/v1/2022.findings-acl.252) |  | 0 |  | Yinghui Li, Qingyu Zhou, Yangning Li, Zhongli Li, Ruiyang Liu, Rongyi Sun, Zizhen Wang, Chao Li, Yunbo Cao, HaiTao Zheng |  |
| 428 |  |  [XFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding](https://doi.org/10.18653/v1/2022.findings-acl.253) |  | 0 |  | Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei A. F. Florêncio, Cha Zhang, Furu Wei |  |
| 429 |  |  [Type-Driven Multi-Turn Corrections for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-acl.254) |  | 0 |  | Shaopeng Lai, Qingyu Zhou, Jiali Zeng, Zhongli Li, Chao Li, Yunbo Cao, Jinsong Su |  |
| 430 |  |  [Leveraging Knowledge in Multilingual Commonsense Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.255) |  | 0 |  | Yuwei Fang, Shuohang Wang, Yichong Xu, Ruochen Xu, Siqi Sun, Chenguang Zhu, Michael Zeng |  |
| 431 |  |  [Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2022.findings-acl.256) |  | 0 |  | Wei Xiang, Bang Wang, Lu Dai, Yijun Mo |  |
| 432 |  |  [One Agent To Rule Them All: Towards Multi-agent Conversational AI](https://doi.org/10.18653/v1/2022.findings-acl.257) |  | 0 |  | Christopher Clarke, Joseph Peper, Karthik Krishnamurthy, Walter Talamonti, Kevin Leach, Walter S. Lasecki, Yiping Kang, Lingjia Tang, Jason Mars |  |
| 433 |  |  [Word-level Perturbation Considering Word Length and Compositional Subwords](https://doi.org/10.18653/v1/2022.findings-acl.258) |  | 0 |  | Tatsuya Hiraoka, Sho Takase, Kei Uchiumi, Atsushi Keyaki, Naoaki Okazaki |  |
| 434 |  |  [Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging](https://doi.org/10.18653/v1/2022.findings-acl.259) |  | 0 |  | Houquan Zhou, Yang Li, Zhenghua Li, Min Zhang |  |
| 435 |  |  [Controlling the Focus of Pretrained Language Generation Models](https://doi.org/10.18653/v1/2022.findings-acl.260) |  | 0 |  | Jiabao Ji, Yoon Kim, James R. Glass, Tianxing He |  |
| 436 |  |  [Comparative Opinion Summarization via Collaborative Decoding](https://doi.org/10.18653/v1/2022.findings-acl.261) |  | 0 |  | Hayate Iso, Xiaolan Wang, Stefanos Angelidis, Yoshihiko Suhara |  |
| 437 |  |  [IsoScore: Measuring the Uniformity of Embedding Space Utilization](https://doi.org/10.18653/v1/2022.findings-acl.262) |  | 0 |  | William Rudman, Nate Gillman, Taylor Rayne, Carsten Eickhoff |  |
| 438 |  |  [A Natural Diet: Towards Improving Naturalness of Machine Translation Output](https://doi.org/10.18653/v1/2022.findings-acl.263) |  | 0 |  | Markus Freitag, David Vilar, David Grangier, Colin Cherry, George F. Foster |  |
| 439 |  |  [From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains](https://doi.org/10.18653/v1/2022.findings-acl.264) |  | 0 |  | Brodie Mather, Bonnie J. Dorr, Adam Dalton, William de Beaumont, Owen Rambow, Sonja SchmerGalunder |  |
| 440 |  |  [CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals](https://doi.org/10.18653/v1/2022.findings-acl.265) |  | 0 |  | Scott Novotney, Sreeparna Mukherjee, Zeeshan Ahmed, Andreas Stolcke |  |
| 441 |  |  [Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning](https://doi.org/10.18653/v1/2022.findings-acl.266) |  | 0 |  | Rina Galperin, Shachar Schnapp, Michael Elhadad |  |
| 442 |  |  [Aligned Weight Regularizers for Pruning Pretrained Neural Networks](https://doi.org/10.18653/v1/2022.findings-acl.267) |  | 0 |  | James O'Neill, Sourav Dutta, Haytham Assem |  |
| 443 |  |  [Consistent Representation Learning for Continual Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.268) |  | 0 |  | Kang Zhao, Hua Xu, Jiangong Yang, Kai Gao |  |
| 444 |  |  [Event Transition Planning for Open-ended Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.269) |  | 0 |  | Qintong Li, Piji Li, Wei Bi, Zhaochun Ren, Yuxuan Lai, Lingpeng Kong |  |
| 445 |  |  [Comprehensive Multi-Modal Interactions for Referring Image Segmentation](https://doi.org/10.18653/v1/2022.findings-acl.270) |  | 0 |  | Kanishk Jain, Vineet Gandhi |  |
| 446 |  |  [MetaWeighting: Learning to Weight Tasks in Multi-Task Learning](https://doi.org/10.18653/v1/2022.findings-acl.271) |  | 0 |  | Yuren Mao, Zekai Wang, Weiwei Liu, Xuemin Lin, Pengtao Xie |  |
| 447 |  |  [Improving Controllable Text Generation with Position-Aware Weighted Decoding](https://doi.org/10.18653/v1/2022.findings-acl.272) |  | 0 |  | Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, Bing Qin |  |
| 448 |  |  [Prompt Tuning for Discriminative Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.273) |  | 0 |  | Yuan Yao, Bowen Dong, Ao Zhang, Zhengyan Zhang, Ruobing Xie, Zhiyuan Liu, Leyu Lin, Maosong Sun, Jianyong Wang |  |
| 449 |  |  [Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.274) |  | 0 |  | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 450 |  |  [What does it take to bake a cake? The RecipeRef corpus and anaphora resolution in procedural text](https://doi.org/10.18653/v1/2022.findings-acl.275) |  | 0 |  | Biaoyan Fang, Timothy Baldwin, Karin Verspoor |  |
| 451 |  |  [MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.276) |  | 0 |  | Fangkai Jiao, Yangyang Guo, Xuemeng Song, Liqiang Nie |  |
| 452 |  |  [THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption](https://doi.org/10.18653/v1/2022.findings-acl.277) |  | 0 |  | Tianyu Chen, Hangbo Bao, Shaohan Huang, Li Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, Furu Wei |  |
| 453 |  |  [HLDC: Hindi Legal Documents Corpus](https://doi.org/10.18653/v1/2022.findings-acl.278) |  | 0 |  | Arnav Kapoor, Mudit Dhawan, Anmol Goel, T. H. Arjun, Akshala Bhatnagar, Vibhu Agrawal, Amul Agrawal, Arnab Bhattacharya, Ponnurangam Kumaraguru, Ashutosh Modi |  |
| 454 |  |  [Rethinking Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.279) |  | 0 |  | Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian Huang, Jiajun Chen, Lei Li |  |
| 455 |  |  [Incremental Intent Detection for Medical Domain with Contrast Replay Networks](https://doi.org/10.18653/v1/2022.findings-acl.280) |  | 0 |  | Guirong Bai, Shizhu He, Kang Liu, Jun Zhao |  |
| 456 |  |  [LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.281) |  | 0 |  | Canwen Xu, Daya Guo, Nan Duan, Julian J. McAuley |  |
| 457 |  |  [Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach](https://doi.org/10.18653/v1/2022.findings-acl.282) |  | 0 |  | Xin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou |  |
| 458 |  |  [EICO: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization](https://doi.org/10.18653/v1/2022.findings-acl.283) |  | 0 |  | Lei Zhao, Cheng Yao |  |
| 459 |  |  [Improving the Adversarial Robustness of NLP Models by Information Bottleneck](https://doi.org/10.18653/v1/2022.findings-acl.284) |  | 0 |  | Cenyuan Zhang, Xiang Zhou, Yixin Wan, Xiaoqing Zheng, KaiWei Chang, ChoJui Hsieh |  |
| 460 |  |  [Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.285) |  | 0 |  | Kai Zhang, Kun Zhang, Mengdi Zhang, Hongke Zhao, Qi Liu, Wei Wu, Enhong Chen |  |
| 461 |  |  [DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition](https://doi.org/10.18653/v1/2022.findings-acl.286) |  | 0 |  | Bowen Xing, Ivor W. Tsang |  |
| 462 |  |  [Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents](https://doi.org/10.18653/v1/2022.findings-acl.287) |  | 0 |  | Yicheng Zou, Hongwei Liu, Tao Gui, Junzhe Wang, Qi Zhang, Meng Tang, Haixiang Li, Daniel Wang |  |
| 463 |  |  [Modular Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-acl.288) |  | 0 |  | Junshen K. Chen, Dallas Card, Dan Jurafsky |  |
| 464 |  |  [Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation](https://doi.org/10.18653/v1/2022.findings-acl.289) |  | 0 |  | KiYoon Yoo, Jangho Kim, Jiho Jang, Nojun Kwak |  |
| 465 |  |  [Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers](https://doi.org/10.18653/v1/2022.findings-acl.290) |  | 0 |  | Rishabh Singh, Shirin Goshtasbpour |  |
| 466 |  |  [Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation](https://doi.org/10.18653/v1/2022.findings-acl.291) |  | 0 |  | Kevin Yang, Olivia Deng, Charles Chen, Richard Shin, Subhro Roy, Benjamin Van Durme |  |
| 467 |  |  [Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking](https://doi.org/10.18653/v1/2022.findings-acl.292) |  | 0 |  | Tuan Manh Lai, Heng Ji, ChengXiang Zhai |  |
| 468 |  |  [Local Structure Matters Most: Perturbation Study in NLU](https://doi.org/10.18653/v1/2022.findings-acl.293) |  | 0 |  | Louis Clouâtre, Prasanna Parthasarathi, Amal Zouaq, Sarath Chandar |  |
| 469 |  |  [Probing Factually Grounded Content Transfer with Factual Ablation](https://doi.org/10.18653/v1/2022.findings-acl.294) |  | 0 |  | Peter West, Chris Quirk, Michel Galley, Yejin Choi |  |
| 470 |  |  [ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference](https://doi.org/10.18653/v1/2022.findings-acl.295) |  | 0 |  | Kai Hui, Honglei Zhuang, Tao Chen, Zhen Qin, Jing Lu, Dara Bahri, Ji Ma, Jai Prakash Gupta, Cícero Nogueira dos Santos, Yi Tay, Donald Metzler |  |
| 471 |  |  [Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics](https://doi.org/10.18653/v1/2022.findings-acl.296) |  | 0 |  | Daniel Deutsch, Dan Roth |  |
| 472 |  |  [Prior Knowledge and Memory Enriched Transformer for Sign Language Translation](https://doi.org/10.18653/v1/2022.findings-acl.297) |  | 0 |  | Tao Jin, Zhou Zhao, Meng Zhang, Xingshan Zeng |  |
| 473 |  |  [Discontinuous Constituency and BERT: A Case Study of Dutch](https://doi.org/10.18653/v1/2022.findings-acl.298) |  | 0 |  | Konstantinos Kogkalidis, Gijs Wijnholds |  |
| 474 |  |  [Probing Multilingual Cognate Prediction Models](https://doi.org/10.18653/v1/2022.findings-acl.299) |  | 0 |  | Clémentine Fourrier, Benoît Sagot |  |
| 475 |  |  [A Neural Pairwise Ranking Model for Readability Assessment](https://doi.org/10.18653/v1/2022.findings-acl.300) |  | 0 |  | Justin Lee, Sowmya Vajjala |  |
| 476 |  |  [First the Worst: Finding Better Gender Translations During Beam Search](https://doi.org/10.18653/v1/2022.findings-acl.301) |  | 0 |  | Danielle Saunders, Rosie Sallis, Bill Byrne |  |
| 477 |  |  [Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.302) |  | 0 |  | Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, Juneyoung Park |  |
| 478 |  |  [Unsupervised Preference-Aware Language Identification](https://doi.org/10.18653/v1/2022.findings-acl.303) |  | 0 |  | Xingzhang Ren, Baosong Yang, Dayiheng Liu, Haibo Zhang, Xiaoyu Lv, Liang Yao, Jun Xie |  |
| 479 |  |  [Using NLP to quantify the environmental cost and diversity benefits of in-person NLP conferences](https://doi.org/10.18653/v1/2022.findings-acl.304) |  | 0 |  | Piotr Przybyla, Matthew Shardlow |  |
| 480 |  |  [Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking](https://doi.org/10.18653/v1/2022.findings-acl.305) |  | 0 |  | Tianyi Luo, Rui Meng, Xin Wang, Yang Liu |  |
| 481 |  |  [Chinese Synesthesia Detection: New Dataset and Models](https://doi.org/10.18653/v1/2022.findings-acl.306) |  | 0 |  | Xiaotong Jiang, Qingqing Zhao, Yunfei Long, Zhongqing Wang |  |
| 482 |  |  [Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem](https://doi.org/10.18653/v1/2022.findings-acl.307) |  | 0 |  | Qiang Zhang, Jason Naradowsky, Yusuke Miyao |  |
| 483 |  |  [On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark](https://doi.org/10.18653/v1/2022.findings-acl.308) |  | 0 |  | Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, Minlie Huang |  |
| 484 |  |  [Word Segmentation by Separation Inference for East Asian Languages](https://doi.org/10.18653/v1/2022.findings-acl.309) |  | 0 |  | Yu Tong, Jingzhi Guo, Jizhe Zhou, Ge Chen, Guokai Zheng |  |
| 485 |  |  [Unsupervised Chinese Word Segmentation with BERT Oriented Probing and Transformation](https://doi.org/10.18653/v1/2022.findings-acl.310) |  | 0 |  | Wei Li, Yuhan Song, Qi Su, Yanqiu Shao |  |
| 486 |  |  [E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.311) |  | 0 |  | Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, Hao Zhou |  |
| 487 |  |  [Implicit Relation Linking for Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2022.findings-acl.312) |  | 0 |  | Yao Zhao, Jiacheng Huang, Wei Hu, Qijin Chen, Xiaoxia Qiu, Chengfu Huo, Weijun Ren |  |
| 488 |  |  [Attention Mechanism with Energy-Friendly Operations](https://doi.org/10.18653/v1/2022.findings-acl.313) |  | 0 |  | Yu Wan, Baosong Yang, Dayiheng Liu, Rong Xiao, Derek F. Wong, Haibo Zhang, Boxing Chen, Lidia S. Chao |  |
| 489 |  |  [Probing BERT's priors with serial reproduction chains](https://doi.org/10.18653/v1/2022.findings-acl.314) |  | 0 |  | Takateru Yamakoshi, Thomas L. Griffiths, Robert D. Hawkins |  |
| 490 |  |  [Interpreting the Robustness of Neural NLP Models to Textual Perturbations](https://doi.org/10.18653/v1/2022.findings-acl.315) |  | 0 |  | Yunxiang Zhang, Liangming Pan, Samson Tan, MinYen Kan |  |
| 491 |  |  [Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations](https://doi.org/10.18653/v1/2022.findings-acl.316) |  | 0 |  | Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita Sharma, Damien Jose, Paul Bennett |  |
| 492 |  |  [A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation](https://doi.org/10.18653/v1/2022.findings-acl.317) |  | 0 |  | Giovanni Campagna, Sina J. Semnani, Ryan Kearns, Lucas Jun Koba Sato, Silei Xu, Monica Lam |  |
| 493 |  |  [GCPG: A General Framework for Controllable Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-acl.318) |  | 0 |  | Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong Yang, Haibo Zhang, Xue Zhao, Wenqing Yao, Boxing Chen |  |
| 494 |  |  [CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-acl.319) |  | 0 |  | Milan Gritta, Ruoyu Hu, Ignacio Iacobacci |  |
| 495 |  |  [Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer](https://doi.org/10.18653/v1/2022.findings-acl.320) |  | 0 |  | Nikolai Ilinykh, Simon Dobnik |  |
| 496 |  |  [Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise](https://doi.org/10.18653/v1/2022.findings-acl.321) |  | 0 |  | Noëmi Aepli, Rico Sennrich |  |
| 497 |  |  [Structural Supervision for Word Alignment and Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.322) |  | 0 |  | Lei Li, Kai Fan, Hongjia Li, Chun Yuan |  |
| 498 |  |  [Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization](https://doi.org/10.18653/v1/2022.findings-acl.323) |  | 0 |  | Kexun Zhang, Jiaao Chen, Diyi Yang |  |
| 499 |  |  [Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors](https://doi.org/10.18653/v1/2022.findings-acl.324) |  | 0 |  | Ryo Nagata, Manabu Kimura, Kazuaki Hanawa |  |
| 500 |  |  [Should We Trust This Summary? Bayesian Abstractive Summarization to The Rescue](https://doi.org/10.18653/v1/2022.findings-acl.325) |  | 0 |  | Alexios Gidiotis, Grigorios Tsoumakas |  |
| 501 |  |  [On the data requirements of probing](https://doi.org/10.18653/v1/2022.findings-acl.326) |  | 0 |  | Zining Zhu, Jixuan Wang, Bai Li, Frank Rudzicz |  |
| 502 |  |  [Translation Error Detection as Rationale Extraction](https://doi.org/10.18653/v1/2022.findings-acl.327) |  | 0 |  | Marina Fomicheva, Lucia Specia, Nikolaos Aletras |  |
| 503 |  |  [Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty](https://doi.org/10.18653/v1/2022.findings-acl.328) |  | 0 |  | Zi Lin, Jeremiah Zhe Liu, Jingbo Shang |  |
| 504 |  |  [Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework](https://doi.org/10.18653/v1/2022.findings-acl.329) |  | 0 |  | Zilong Wang, Jingbo Shang |  |
| 505 |  |  [On Length Divergence Bias in Textual Matching Models](https://doi.org/10.18653/v1/2022.findings-acl.330) |  | 0 |  | Lan Jiang, Tianshu Lyu, Yankai Lin, Chong Meng, Xiaoyong Lyu, Dawei Yin |  |
| 506 |  |  [What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation](https://doi.org/10.18653/v1/2022.findings-acl.331) |  | 0 |  | Sarik Ghazarian, Behnam Hedayatnia, Alexandros Papangelis, Yang Liu, Dilek HakkaniTur |  |
| 507 |  |  [Frontmatter](https://aclanthology.org/2022.acl-long.0) |  | 0 |  |  |  |
| 508 |  |  [AdapLeR: Speeding up Inference by Adaptive Length Reduction](https://doi.org/10.18653/v1/2022.acl-long.1) |  | 0 |  | Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar |  |
| 509 |  |  [Quantified Reproducibility Assessment of NLP Results](https://doi.org/10.18653/v1/2022.acl-long.2) |  | 0 |  | Anya Belz, Maja Popovic, Simon Mille |  |
| 510 |  |  [Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings](https://doi.org/10.18653/v1/2022.acl-long.3) |  | 0 |  | Sangwon Yu, Jongyoon Song, Heeseung Kim, Seongmin Lee, WooJong Ryu, Sungroh Yoon |  |
| 511 |  |  [AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level](https://doi.org/10.18653/v1/2022.acl-long.4) |  | 0 |  | Amit Seker, Elron Bandel, Dan Bareket, Idan Brusilovsky, Refael Shaked Greenfeld, Reut Tsarfaty |  |
| 512 |  |  [Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning](https://doi.org/10.18653/v1/2022.acl-long.5) |  | 0 |  | Moxin Li, Fuli Feng, Hanwang Zhang, Xiangnan He, Fengbin Zhu, TatSeng Chua |  |
| 513 |  |  [Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification](https://doi.org/10.18653/v1/2022.acl-long.6) |  | 0 |  | GeorgeEduard Zaharia, RazvanAlexandru Smadu, DumitruClementin Cercel, Mihai Dascalu |  |
| 514 |  |  [JointCL: A Joint Contrastive Learning Framework for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2022.acl-long.7) |  | 0 |  | Bin Liang, Qinglin Zhu, Xiang Li, Min Yang, Lin Gui, Yulan He, Ruifeng Xu |  |
| 515 |  |  [[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-long.8) |  | 0 |  | Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, Caiming Xiong |  |
| 516 |  |  [UniTranSeR: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System](https://doi.org/10.18653/v1/2022.acl-long.9) |  | 0 |  | Zhiyuan Ma, Jianjun Li, Guohui Li, Yongjing Cheng |  |
| 517 |  |  [Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-long.10) |  | 0 |  | Yue Feng, Aldo Lipani, Fanghua Ye, Qiang Zhang, Emine Yilmaz |  |
| 518 |  |  [Attention Temperature Matters in Abstractive Summarization Distillation](https://doi.org/10.18653/v1/2022.acl-long.11) |  | 0 |  | Shengqiang Zhang, Xingxing Zhang, Hangbo Bao, Furu Wei |  |
| 519 |  |  [Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.12) |  | 0 |  | Guanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, Furu Wei |  |
| 520 |  |  [TopWORDS-Seg: Simultaneous Text Segmentation and Word Discovery for Open-Domain Chinese Texts via Bayesian Inference](https://doi.org/10.18653/v1/2022.acl-long.13) |  | 0 |  | Changzai Pan, Maosong Sun, Ke Deng |  |
| 521 |  |  [An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.14) |  | 0 |  | Zhuoran Li, Chunming Hu, Xiaohui Guo, Junfan Chen, Wenyi Qin, Richong Zhang |  |
| 522 |  |  [Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature](https://doi.org/10.18653/v1/2022.acl-long.15) |  | 0 |  | Gianluca Moro, Luca Ragazzi, Lorenzo Valgimigli, Davide Freddi |  |
| 523 |  |  [Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm](https://doi.org/10.18653/v1/2022.acl-long.16) |  | 0 |  | Shaoyi Huang, Dongkuan Xu, Ian EnHsu Yen, Yijue Wang, SungEn Chang, Bingbing Li, Shiyang Chen, Mimi Xie, Sanguthevar Rajasekaran, Hang Liu, Caiwen Ding |  |
| 524 |  |  [CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.17) |  | 0 |  | Nishant Kambhatla, Logan Born, Anoop Sarkar |  |
| 525 |  |  [Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages](https://doi.org/10.18653/v1/2022.acl-long.18) |  | 0 |  | Vaidehi Patil, Partha P. Talukdar, Sunita Sarawagi |  |
| 526 |  |  [Long-range Sequence Modeling with Predictable Sparse Attention](https://doi.org/10.18653/v1/2022.acl-long.19) |  | 0 |  | Yimeng Zhuang, Jing Zhang, Mei Tu |  |
| 527 |  |  [Improving Personalized Explanation Generation through Visualization](https://doi.org/10.18653/v1/2022.acl-long.20) |  | 0 |  | Shijie Geng, Zuohui Fu, Yingqiang Ge, Lei Li, Gerard de Melo, Yongfeng Zhang |  |
| 528 |  |  [New Intent Discovery with Pre-training and Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.21) |  | 0 |  | Yuwei Zhang, Haode Zhang, LiMing Zhan, XiaoMing Wu, Albert Y. S. Lam |  |
| 529 |  |  [Modeling U.S. State-Level Policies by Extracting Winners and Losers from Legislative Texts](https://doi.org/10.18653/v1/2022.acl-long.22) |  | 0 |  | Maryam Davoodi, Eric Waltenburg, Dan Goldwasser |  |
| 530 |  |  [Structural Characterization for Dialogue Disentanglement](https://doi.org/10.18653/v1/2022.acl-long.23) |  | 0 |  | Xinbei Ma, Zhuosheng Zhang, Hai Zhao |  |
| 531 |  |  [Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems](https://doi.org/10.18653/v1/2022.acl-long.24) |  | 0 |  | Lingyu Zhu, Zhengkun Zhang, Jun Wang, Hongbin Wang, Haiying Wu, Zhenglu Yang |  |
| 532 |  |  [MISC: A Mixed Strategy-Aware Model integrating COMET for Emotional Support Conversation](https://doi.org/10.18653/v1/2022.acl-long.25) |  | 0 |  | Quan Tu, Yanran Li, Jianwei Cui, Bin Wang, JiRong Wen, Rui Yan |  |
| 533 |  |  [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://doi.org/10.18653/v1/2022.acl-long.26) |  | 0 |  | Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang |  |
| 534 |  |  [QuoteR: A Benchmark of Quote Recommendation for Writing](https://doi.org/10.18653/v1/2022.acl-long.27) |  | 0 |  | Fanchao Qi, Yanhui Yang, Jing Yi, Zhili Cheng, Zhiyuan Liu, Maosong Sun |  |
| 535 |  |  [Towards Comprehensive Patent Approval Predictions: Beyond Traditional Document Classification](https://doi.org/10.18653/v1/2022.acl-long.28) |  | 0 |  | Xiaochen Gao, Zhaoyi Hou, Yifei Ning, Kewen Zhao, Beilei He, Jingbo Shang, Vish Krishnan |  |
| 536 |  |  [Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering](https://doi.org/10.18653/v1/2022.acl-long.29) |  | 0 |  | YuJung Heo, EunSol Kim, Woo Suk Choi, ByoungTak Zhang |  |
| 537 |  |  [Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech](https://doi.org/10.18653/v1/2022.acl-long.30) |  | 0 |  | Yang Li, Cheng Yu, Guangzhi Sun, Hua Jiang, Fanglei Sun, Weiqin Zu, Ying Wen, Yang Yang, Jun Wang |  |
| 538 |  |  [Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models](https://doi.org/10.18653/v1/2022.acl-long.31) |  | 0 |  | Fatemehsadat Mireshghallah, Kartik Goyal, Taylor BergKirkpatrick |  |
| 539 |  |  [So Different Yet So Alike! Constrained Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2022.acl-long.32) |  | 0 |  | Abhinav Ramesh Kashyap, Devamanyu Hazarika, MinYen Kan, Roger Zimmermann, Soujanya Poria |  |
| 540 |  |  [e-CARE: a New Dataset for Exploring Explainable Causal Reasoning](https://doi.org/10.18653/v1/2022.acl-long.33) |  | 0 |  | Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin |  |
| 541 |  |  [Fantastic Questions and Where to Find Them: FairytaleQA - An Authentic Dataset for Narrative Comprehension](https://doi.org/10.18653/v1/2022.acl-long.34) |  | 0 |  | Ying Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bingsheng Yao, Tongshuang Wu, Zheng Zhang, Toby JiaJun Li, Nora Bradford, Branda Sun, Tran Bao Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, Mark Warschauer |  |
| 542 |  |  [KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base](https://doi.org/10.18653/v1/2022.acl-long.35) |  | 0 |  | Junzhuo Li, Deyi Xiong |  |
| 543 |  |  [Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment](https://doi.org/10.18653/v1/2022.acl-long.36) |  | 0 |  | Zijie Huang, Zheng Li, Haoming Jiang, Tianyu Cao, Hanqing Lu, Bing Yin, Karthik Subbian, Yizhou Sun, Wei Wang |  |
| 544 |  |  [Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization](https://doi.org/10.18653/v1/2022.acl-long.37) |  | 0 |  | Juncai Guo, Jin Liu, Yao Wan, Li Li, Pingyi Zhou |  |
| 545 |  |  [FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.38) |  | 0 |  | Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Chonghua Liao, Li Jian, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, Zhilin Yang |  |
| 546 |  |  [Learn to Adapt for Generalized Zero-Shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.39) |  | 0 |  | Yiwen Zhang, Caixia Yuan, Xiaojie Wang, Ziwei Bai, Yongbin Liu |  |
| 547 |  |  [TableFormer: Robust Transformer Modeling for Table-Text Encoding](https://doi.org/10.18653/v1/2022.acl-long.40) |  | 0 |  | Jingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, Shachi Paul |  |
| 548 |  |  [Perceiving the World: Question-guided Reinforcement Learning for Text-based Games](https://doi.org/10.18653/v1/2022.acl-long.41) |  | 0 |  | Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang |  |
| 549 |  |  [Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.42) |  | 0 |  | Ruipeng Jia, Xingxing Zhang, Yanan Cao, Zheng Lin, Shi Wang, Furu Wei |  |
| 550 |  |  [Few-Shot Class-Incremental Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.43) |  | 0 |  | Rui Wang, Tong Yu, Handong Zhao, Sungchul Kim, Subrata Mitra, Ruiyi Zhang, Ricardo Henao |  |
| 551 |  |  [Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation](https://doi.org/10.18653/v1/2022.acl-long.44) |  | 0 |  | Yingxiu Zhao, Zhiliang Tian, Huaxiu Yao, Yinhe Zheng, Dongkyu Lee, Yiping Song, Jian Sun, Nevin L. Zhang |  |
| 552 |  |  [Quality Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2022.acl-long.45) |  | 0 |  | Elron Bandel, Ranit Aharonov, Michal ShmueliScheuer, Ilya Shnayderman, Noam Slonim, Liat EinDor |  |
| 553 |  |  [Controllable Dictionary Example Generation: Generating Example Sentences for Specific Targeted Audiences](https://doi.org/10.18653/v1/2022.acl-long.46) |  | 0 |  | Xingwei He, SiuMing Yiu |  |
| 554 |  |  [AraT5: Text-to-Text Transformers for Arabic Language Generation](https://doi.org/10.18653/v1/2022.acl-long.47) |  | 0 |  | El Moatez Billah Nagoudi, AbdelRahim A. Elmadany, Muhammad AbdulMageed |  |
| 555 |  |  [Legal Judgment Prediction via Event Extraction with Constraints](https://doi.org/10.18653/v1/2022.acl-long.48) |  | 0 |  | Yi Feng, Chuanyi Li, Vincent Ng |  |
| 556 |  |  [Answer-level Calibration for Free-form Multiple Choice Question Answering](https://doi.org/10.18653/v1/2022.acl-long.49) |  | 0 |  | Sawan Kumar |  |
| 557 |  |  [Learning When to Translate for Streaming Speech](https://doi.org/10.18653/v1/2022.acl-long.50) |  | 0 |  | Qian Dong, Yaoming Zhu, Mingxuan Wang, Lei Li |  |
| 558 |  |  [Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking](https://doi.org/10.18653/v1/2022.acl-long.51) |  | 0 |  | Yingrui Yang, Yifan Qiao, Tao Yang |  |
| 559 |  |  [Early Stopping Based on Unlabeled Samples in Text Classification](https://doi.org/10.18653/v1/2022.acl-long.52) |  | 0 |  | Hongseok Choi, Dongha Choi, Hyunju Lee |  |
| 560 |  |  [Meta-learning via Language Model In-context Tuning](https://doi.org/10.18653/v1/2022.acl-long.53) |  | 0 |  | Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He |  |
| 561 |  |  [It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books](https://doi.org/10.18653/v1/2022.acl-long.54) |  | 0 |  | Bingsheng Yao, Dakuo Wang, Tongshuang Wu, Zheng Zhang, Toby JiaJun Li, Mo Yu, Ying Xu |  |
| 562 |  |  [Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning](https://doi.org/10.18653/v1/2022.acl-long.55) |  | 0 |  | Rongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, Chao Zhang |  |
| 563 |  |  [Constrained Multi-Task Learning for Bridging Resolution](https://doi.org/10.18653/v1/2022.acl-long.56) |  | 0 |  | Hideo Kobayashi, Yufang Hou, Vincent Ng |  |
| 564 |  |  [DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations](https://doi.org/10.18653/v1/2022.acl-long.57) |  | 0 |  | Sarik Ghazarian, Nuan Wen, Aram Galstyan, Nanyun Peng |  |
| 565 |  |  [HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.58) |  | 0 |  | Shuyang Cao, Lu Wang |  |
| 566 |  |  [De-Bias for Generative Extraction in Unified NER Task](https://doi.org/10.18653/v1/2022.acl-long.59) |  | 0 |  | Shuai Zhang, Yongliang Shen, Zeqi Tan, Yiquan Wu, Weiming Lu |  |
| 567 |  |  [An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels](https://doi.org/10.18653/v1/2022.acl-long.60) |  | 0 |  | Taylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, David Wingate |  |
| 568 |  |  [Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation](https://doi.org/10.18653/v1/2022.acl-long.61) |  | 0 |  | Xinyi Wang, Sebastian Ruder, Graham Neubig |  |
| 569 |  |  [Language-agnostic BERT Sentence Embedding](https://doi.org/10.18653/v1/2022.acl-long.62) |  | 0 |  | Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang |  |
| 570 |  |  [Nested Named Entity Recognition with Span-level Graphs](https://doi.org/10.18653/v1/2022.acl-long.63) |  | 0 |  | Juncheng Wan, Dongyu Ru, Weinan Zhang, Yong Yu |  |
| 571 |  |  [CogTaskonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP](https://doi.org/10.18653/v1/2022.acl-long.64) |  | 0 |  | Yifei Luo, Minghui Xu, Deyi Xiong |  |
| 572 |  |  [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://doi.org/10.18653/v1/2022.acl-long.65) |  | 0 |  | Hui Su, Weiwei Shi, Xiaoyu Shen, Xiao Zhou, Tuo Ji, Jiarui Fang, Jie Zhou |  |
| 573 |  |  [Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues](https://doi.org/10.18653/v1/2022.acl-long.66) |  | 0 |  | Qingxiu Dong, Ziwei Qin, Heming Xia, Tian Feng, Shoujie Tong, Haoran Meng, Lin Xu, Zhongyu Wei, Weidong Zhan, Baobao Chang, Sujian Li, Tianyu Liu, Zhifang Sui |  |
| 574 |  |  [Parallel Instance Query Network for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.67) |  | 0 |  | Yongliang Shen, Xiaobin Wang, Zeqi Tan, Guangwei Xu, Pengjun Xie, Fei Huang, Weiming Lu, Yueting Zhuang |  |
| 575 |  |  [ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation](https://doi.org/10.18653/v1/2022.acl-long.68) |  | 0 |  | Chang Liu, Xu Tan, Chongyang Tao, Zhenxin Fu, Dongyan Zhao, TieYan Liu, Rui Yan |  |
| 576 |  |  [Modeling Multi-hop Question Answering as Single Sequence Prediction](https://doi.org/10.18653/v1/2022.acl-long.69) |  | 0 |  | Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Nitish Shirish Keskar, Caiming Xiong |  |
| 577 |  |  [Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.70) |  | 0 |  | Linjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi Xiong, Shizhan Chen, Zhiqiang Zhuang, Zhiyong Feng |  |
| 578 |  |  [Multi-Granularity Structural Knowledge Distillation for Language Model Compression](https://doi.org/10.18653/v1/2022.acl-long.71) |  | 0 |  | Chang Liu, Chongyang Tao, Jiazhan Feng, Dongyan Zhao |  |
| 579 |  |  [Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts](https://doi.org/10.18653/v1/2022.acl-long.72) |  | 0 |  | Yue Guo, Yi Yang, Ahmed Abbasi |  |
| 580 |  |  [Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals](https://doi.org/10.18653/v1/2022.acl-long.73) |  | 0 |  | Zeming Liu, Jun Xu, Zeyang Lei, Haifeng Wang, ZhengYu Niu, Hua Wu |  |
| 581 |  |  [Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network](https://doi.org/10.18653/v1/2022.acl-long.74) |  | 0 |  | Ying Li, Shuaike Li, Min Zhang |  |
| 582 |  |  [A Closer Look at How Fine-tuning Changes BERT](https://doi.org/10.18653/v1/2022.acl-long.75) |  | 0 |  | Yichu Zhou, Vivek Srikumar |  |
| 583 |  |  [Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.76) |  | 0 |  | Wu Hong, Zhuosheng Zhang, Jinyuan Wang, Hai Zhao |  |
| 584 |  |  [FaiRR: Faithful and Robust Deductive Reasoning over Natural Language](https://doi.org/10.18653/v1/2022.acl-long.77) |  | 0 |  | Soumya Sanyal, Harman Singh, Xiang Ren |  |
| 585 |  |  [HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.78) |  | 0 |  | Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, JianGuang Lou, Dongmei Zhang |  |
| 586 |  |  [Doctor Recommendation in Online Health Forums via Expertise Learning](https://doi.org/10.18653/v1/2022.acl-long.79) |  | 0 |  | Xiaoxin Lu, Yubo Zhang, Jing Li, Shi Zong |  |
| 587 |  |  [Continual Prompt Tuning for Dialog State Tracking](https://doi.org/10.18653/v1/2022.acl-long.80) |  | 0 |  | Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu, Minlie Huang |  |
| 588 |  |  [There's a Time and Place for Reasoning Beyond the Image](https://doi.org/10.18653/v1/2022.acl-long.81) |  | 0 |  | Xingyu Fu, Ben Zhou, Ishaan Preetam Chandratreya, Carl Vondrick, Dan Roth |  |
| 589 |  |  [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://doi.org/10.18653/v1/2022.acl-long.82) |  | 0 |  | Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei Wu, Shi Han, Fan Cheng, Dongmei Zhang |  |
| 590 |  |  [Multimodal fusion via cortical network inspired losses](https://doi.org/10.18653/v1/2022.acl-long.83) |  | 0 |  | Shiv Shankar |  |
| 591 |  |  [Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension](https://doi.org/10.18653/v1/2022.acl-long.84) |  | 0 |  | Huibin Zhang, Zhengkun Zhang, Yao Zhang, Jun Wang, Yufan Li, Ning Jiang, Xin Wei, Zhenglu Yang |  |
| 592 |  |  [Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.85) |  | 0 |  | Swarnadeep Saha, Prateek Yadav, Mohit Bansal |  |
| 593 |  |  [Unsupervised Extractive Opinion Summarization Using Sparse Coding](https://doi.org/10.18653/v1/2022.acl-long.86) |  | 0 |  | Somnath Basu Roy Chowdhury, Chao Zhao, Snigdha Chaturvedi |  |
| 594 |  |  [LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution](https://doi.org/10.18653/v1/2022.acl-long.87) |  | 0 |  | George Michalopoulos, Ian McKillop, Alexander Wong, Helen H. Chen |  |
| 595 |  |  [Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation](https://doi.org/10.18653/v1/2022.acl-long.88) |  | 0 |  | Pei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, Dilek HakkaniTur |  |
| 596 |  |  [Flow-Adapter Architecture for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.89) |  | 0 |  | Yihong Liu, Haris Jabbar, Hinrich Schütze |  |
| 597 |  |  [Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning](https://doi.org/10.18653/v1/2022.acl-long.90) |  | 0 |  | Demian Gholipour Ghalandari, Chris Hokamp, Georgiana Ifrim |  |
| 598 |  |  [Tracing Origins: Coreference-aware Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.91) |  | 0 |  | Zhuosheng Zhang, Hai Zhao |  |
| 599 |  |  [WatClaimCheck: A new Dataset for Claim Entailment and Inference](https://doi.org/10.18653/v1/2022.acl-long.92) |  | 0 |  | Kashif Khan, Ruizhe Wang, Pascal Poupart |  |
| 600 |  |  [FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation](https://doi.org/10.18653/v1/2022.acl-long.93) |  | 0 |  | Moussa Kamal Eddine, Guokan Shang, Antoine J.P. Tixier, Michalis Vazirgiannis |  |
| 601 |  |  [A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation](https://doi.org/10.18653/v1/2022.acl-long.94) |  | 0 |  | Shashi Narayan, Gonçalo Simões, Yao Zhao, Joshua Maynez, Dipanjan Das, Michael Collins, Mirella Lapata |  |
| 602 |  |  [Synthetic Question Value Estimation for Domain Adaptation of Question Answering](https://doi.org/10.18653/v1/2022.acl-long.95) |  | 0 |  | Xiang Yue, Ziyu Yao, Huan Sun |  |
| 603 |  |  [Better Language Model with Hypernym Class Prediction](https://doi.org/10.18653/v1/2022.acl-long.96) |  | 0 |  | He Bai, Tong Wang, Alessandro Sordoni, Peng Shi |  |
| 604 |  |  [Tackling Fake News Detection by Continually Improving Social Context Representations using Graph Neural Networks](https://doi.org/10.18653/v1/2022.acl-long.97) |  | 0 |  | Nikhil Mehta, Maria Leonor Pacheco, Dan Goldwasser |  |
| 605 |  |  [Understanding Gender Bias in Knowledge Base Embeddings](https://doi.org/10.18653/v1/2022.acl-long.98) |  | 0 |  | Yupei Du, Qi Zheng, Yuanbin Wu, Man Lan, Yan Yang, Meirong Ma |  |
| 606 |  |  [Computational Historical Linguistics and Language Diversity in South Asia](https://doi.org/10.18653/v1/2022.acl-long.99) |  | 0 |  | Aryaman Arora, Adam Farris, Samopriya Basu, Suresh Kolichala |  |
| 607 |  |  [Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.100) |  | 0 |  | Faisal Ladhak, Esin Durmus, He He, Claire Cardie, Kathleen R. McKeown |  |
| 608 |  |  [Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang](https://doi.org/10.18653/v1/2022.acl-long.101) |  | 0 |  | Daphna Keidar, Andreas Opedal, Zhijing Jin, Mrinmaya Sachan |  |
| 609 |  |  [Spurious Correlations in Reference-Free Evaluation of Text Generation](https://doi.org/10.18653/v1/2022.acl-long.102) |  | 0 |  | Esin Durmus, Faisal Ladhak, Tatsunori Hashimoto |  |
| 610 |  |  [On The Ingredients of an Effective Zero-shot Semantic Parser](https://doi.org/10.18653/v1/2022.acl-long.103) |  | 0 |  | Pengcheng Yin, John Wieting, Avirup Sil, Graham Neubig |  |
| 611 |  |  [Bias Mitigation in Machine Translation Quality Estimation](https://doi.org/10.18653/v1/2022.acl-long.104) |  | 0 |  | Hanna Behnke, Marina Fomicheva, Lucia Specia |  |
| 612 |  |  [Unified Speech-Text Pre-training for Speech Translation and Recognition](https://doi.org/10.18653/v1/2022.acl-long.105) |  | 0 |  | Yun Tang, Hongyu Gong, Ning Dong, Changhan Wang, WeiNing Hsu, Jiatao Gu, Alexei Baevski, Xian Li, Abdelrahman Mohamed, Michael Auli, Juan Miguel Pino |  |
| 613 |  |  [Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability](https://doi.org/10.18653/v1/2022.acl-long.106) |  | 0 |  | Yoshinari Fujinuma, Jordan L. BoydGraber, Katharina Kann |  |
| 614 |  |  [Structured Pruning Learns Compact and Accurate Models](https://doi.org/10.18653/v1/2022.acl-long.107) |  | 0 |  | Mengzhou Xia, Zexuan Zhong, Danqi Chen |  |
| 615 |  |  [How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language](https://doi.org/10.18653/v1/2022.acl-long.108) |  | 0 |  | Shiyue Zhang, Benjamin Frey, Mohit Bansal |  |
| 616 |  |  [Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization](https://doi.org/10.18653/v1/2022.acl-long.109) |  | 0 |  | Sanjeev Kumar Karn, Ning Liu, Hinrich Schütze, Oladimeji Farri |  |
| 617 |  |  [Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-long.110) |  | 0 |  | Jiawei Zhou, Jason Eisner, Michael Newman, Emmanouil Antonios Platanios, Sam Thomson |  |
| 618 |  |  [Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures](https://doi.org/10.18653/v1/2022.acl-long.111) |  | 0 |  | Asaf Harari, Gilad Katz |  |
| 619 |  |  [SummN: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents](https://doi.org/10.18653/v1/2022.acl-long.112) |  | 0 |  | Yusen Zhang, Ansong Ni, Ziming Mao, Chen Henry Wu, Chenguang Zhu, Budhaditya Deb, Ahmed Hassan Awadallah, Dragomir R. Radev, Rui Zhang |  |
| 620 |  |  [Open Domain Question Answering with A Unified Knowledge Interface](https://doi.org/10.18653/v1/2022.acl-long.113) |  | 0 |  | Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg, Jianfeng Gao |  |
| 621 |  |  [Principled Paraphrase Generation with Parallel Corpora](https://doi.org/10.18653/v1/2022.acl-long.114) |  | 0 |  | Aitor Ormazabal, Mikel Artetxe, Aitor Soroa, Gorka Labaka, Eneko Agirre |  |
| 622 |  |  [GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.115) |  | 0 |  | Bosheng Ding, Junjie Hu, Lidong Bing, Sharifah Mahani Aljunied, Shafiq R. Joty, Luo Si, Chunyan Miao |  |
| 623 |  |  [Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation](https://doi.org/10.18653/v1/2022.acl-long.116) |  | 0 |  | Dongha Choi, Hongseok Choi, Hyunju Lee |  |
| 624 |  |  [Retrieval-guided Counterfactual Generation for QA](https://doi.org/10.18653/v1/2022.acl-long.117) |  | 0 |  | Bhargavi Paranjape, Matthew Lamm, Ian Tenney |  |
| 625 |  |  [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://doi.org/10.18653/v1/2022.acl-long.118) |  | 0 |  | Ziming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang Zhu, Ahmed Hassan Awadallah, Dragomir R. Radev |  |
| 626 |  |  [Searching for fingerspelled content in American Sign Language](https://doi.org/10.18653/v1/2022.acl-long.119) |  | 0 |  | Bowen Shi, Diane Brentari, Greg Shakhnarovich, Karen Livescu |  |
| 627 |  |  [Skill Induction and Planning with Latent Language](https://doi.org/10.18653/v1/2022.acl-long.120) |  | 0 |  | Pratyusha Sharma, Antonio Torralba, Jacob Andreas |  |
| 628 |  |  [Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation](https://doi.org/10.18653/v1/2022.acl-long.121) |  | 0 |  | Abelardo Carlos Martinez Lorenzo, Marco Maru, Roberto Navigli |  |
| 629 |  |  [Leveraging Similar Users for Personalized Language Modeling with Limited Data](https://doi.org/10.18653/v1/2022.acl-long.122) |  | 0 |  | Charles Welch, Chenxi Gu, Jonathan K. Kummerfeld, Verónica PérezRosas, Rada Mihalcea |  |
| 630 |  |  [DEEP: DEnoising Entity Pre-training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.123) |  | 0 |  | Junjie Hu, Hiroaki Hayashi, Kyunghyun Cho, Graham Neubig |  |
| 631 |  |  [Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network](https://doi.org/10.18653/v1/2022.acl-long.124) |  | 0 |  | Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He, Wenjie Pei, Ruifeng Xu |  |
| 632 |  |  [Composable Sparse Fine-Tuning for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.acl-long.125) |  | 0 |  | Alan Ansell, Edoardo Maria Ponti, Anna Korhonen, Ivan Vulic |  |
| 633 |  |  [Toward Annotator Group Bias in Crowdsourcing](https://doi.org/10.18653/v1/2022.acl-long.126) |  | 0 |  | Haochen Liu, Joseph Thekinen, Sinem Mollaoglu, Da Tang, Ji Yang, Youlong Cheng, Hui Liu, Jiliang Tang |  |
| 634 |  |  [Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation](https://doi.org/10.18653/v1/2022.acl-long.127) |  | 0 |  | Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, Marco Turchi |  |
| 635 |  |  [Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework](https://doi.org/10.18653/v1/2022.acl-long.128) |  | 0 |  | Zhihong Shao, Minlie Huang |  |
| 636 |  |  [Probing as Quantifying Inductive Bias](https://doi.org/10.18653/v1/2022.acl-long.129) |  | 0 |  | Alexander Immer, Lucas Torroba Hennigen, Vincent Fortuin, Ryan Cotterell |  |
| 637 |  |  [Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency](https://doi.org/10.18653/v1/2022.acl-long.130) |  | 0 |  | Yanyang Li, Fuli Luo, Runxin Xu, Songfang Huang, Fei Huang, Liwei Wang |  |
| 638 |  |  [GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.131) |  | 0 |  | Changye Li, David S. Knopman, Weizhe Xu, Trevor Cohen, Serguei Pakhomov |  |
| 639 |  |  [An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.132) |  | 0 |  | Nicholas Meade, Elinor PooleDayan, Siva Reddy |  |
| 640 |  |  [Exploring and Adapting Chinese GPT to Pinyin Input Method](https://doi.org/10.18653/v1/2022.acl-long.133) |  | 0 |  | Minghuan Tan, Yong Dai, Duyu Tang, Zhangyin Feng, Guoping Huang, Jing Jiang, Jiwei Li, Shuming Shi |  |
| 641 |  |  [Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates](https://doi.org/10.18653/v1/2022.acl-long.134) |  | 0 |  | Kunxun Qi, Hai Wan, Jianfeng Du, Haolan Chen |  |
| 642 |  |  [Sense Embeddings are also Biased - Evaluating Social Biases in Static and Contextualised Sense Embeddings](https://doi.org/10.18653/v1/2022.acl-long.135) |  | 0 |  | Yi Zhou, Masahiro Kaneko, Danushka Bollegala |  |
| 643 |  |  [Hybrid Semantics for Goal-Directed Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.136) |  | 0 |  | Connor Baumler, Soumya Ray |  |
| 644 |  |  [Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.137) |  | 0 |  | Georgios Katsimpras, Georgios Paliouras |  |
| 645 |  |  [BiTIIMT: A Bilingual Text-infilling Method for Interactive Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.138) |  | 0 |  | Yanling Xiao, Lemao Liu, Guoping Huang, Qu Cui, Shujian Huang, Shuming Shi, Jiajun Chen |  |
| 646 |  |  [Distributionally Robust Finetuning BERT for Covariate Drift in Spoken Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.139) |  | 0 |  | Samuel Broscheit, Quynh Do, Judith Gaspers |  |
| 647 |  |  [Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph](https://doi.org/10.18653/v1/2022.acl-long.140) |  | 0 |  | Yanzeng Li, Jiangxia Cao, Xin Cong, Zhenyu Zhang, Bowen Yu, Hongsong Zhu, Tingwen Liu |  |
| 648 |  |  [Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction](https://doi.org/10.18653/v1/2022.acl-long.141) |  | 0 |  | Kunyuan Pang, Haoyu Zhang, Jie Zhou, Ting Wang |  |
| 649 |  |  [Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation](https://doi.org/10.18653/v1/2022.acl-long.142) |  | 0 |  | Xinyu Pi, Bing Wang, Yan Gao, Jiaqi Guo, Zhoujun Li, JianGuang Lou |  |
| 650 |  |  [Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.143) |  | 0 |  | Chenze Shao, Yang Feng |  |
| 651 |  |  [Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages](https://doi.org/10.18653/v1/2022.acl-long.144) |  | 0 |  | Ehsan Aghazadeh, Mohsen Fayyaz, Yadollah Yaghoobzadeh |  |
| 652 |  |  [Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.145) |  | 0 |  | Chenhua Chen, Zhiyang Teng, Zhongqing Wang, Yue Zhang |  |
| 653 |  |  [Investigating Non-local Features for Neural Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.146) |  | 0 |  | Leyang Cui, Sen Yang, Yue Zhang |  |
| 654 |  |  [Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2022.acl-long.147) |  | 0 |  | Yi Chen, Jiayang Cheng, Haiyun Jiang, Lemao Liu, Haisong Zhang, Shuming Shi, Ruifeng Xu |  |
| 655 |  |  [A Variational Hierarchical Model for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2022.acl-long.148) |  | 0 |  | Yunlong Liang, Fandong Meng, Chulun Zhou, Jinan Xu, Yufeng Chen, Jinsong Su, Jie Zhou |  |
| 656 |  |  [On the Robustness of Question Rewriting Systems to Questions of Varying Hardness](https://doi.org/10.18653/v1/2022.acl-long.149) |  | 0 |  | Hai Ye, Hwee Tou Ng, Wenjuan Han |  |
| 657 |  |  [OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages](https://doi.org/10.18653/v1/2022.acl-long.150) |  | 0 |  | Prem Selvaraj, Gokul N. C., Pratyush Kumar, Mitesh M. Khapra |  |
| 658 |  |  [bert2BERT: Towards Reusable Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.151) |  | 0 |  | Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, Qun Liu |  |
| 659 |  |  [Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.152) |  | 0 |  | Yan Ling, Jianfei Yu, Rui Xia |  |
| 660 |  |  ["You might think about slightly revising the title": Identifying Hedges in Peer-tutoring Interactions](https://doi.org/10.18653/v1/2022.acl-long.153) |  | 0 |  | Yann Raphalen, Chloé Clavel, Justine Cassell |  |
| 661 |  |  [Efficient Cluster-Based k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.154) |  | 0 |  | Dexin Wang, Kai Fan, Boxing Chen, Deyi Xiong |  |
| 662 |  |  [Headed-Span-Based Projective Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.155) |  | 0 |  | Songlin Yang, Kewei Tu |  |
| 663 |  |  [Decoding Part-of-Speech from Human EEG Signals](https://doi.org/10.18653/v1/2022.acl-long.156) |  | 0 |  | Alex Murphy, Bernd Bohnet, Ryan T. McDonald, Uta Noppeney |  |
| 664 |  |  [Robust Lottery Tickets for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.157) |  | 0 |  | Rui Zheng, Bao Rong, Yuhao Zhou, Di Liang, Sirui Wang, Wei Wu, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 665 |  |  [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://doi.org/10.18653/v1/2022.acl-long.158) |  | 0 |  | Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang Wang, Juanzi Li, Wei Wu, Maosong Sun |  |
| 666 |  |  [Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages](https://doi.org/10.18653/v1/2022.acl-long.159) |  | 0 |  | Xu Han, Yuqi Luo, Weize Chen, Zhiyuan Liu, Maosong Sun, Botong Zhou, Fei Hao, Suncong Zheng |  |
| 667 |  |  [MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER](https://doi.org/10.18653/v1/2022.acl-long.160) |  | 0 |  | Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, Chunyan Miao |  |
| 668 |  |  [Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings](https://doi.org/10.18653/v1/2022.acl-long.161) |  | 0 |  | Shib Sankar Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri, Dhruvesh Patel, Xiang Li, Andrew McCallum |  |
| 669 |  |  [IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks](https://doi.org/10.18653/v1/2022.acl-long.162) |  | 0 |  | Liying Cheng, Lidong Bing, Ruidan He, Qian Yu, Yan Zhang, Luo Si |  |
| 670 |  |  [PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation](https://doi.org/10.18653/v1/2022.acl-long.163) |  | 0 |  | Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Hua Wu, Lifu Huang |  |
| 671 |  |  [CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation](https://doi.org/10.18653/v1/2022.acl-long.164) |  | 0 |  | Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, Minlie Huang |  |
| 672 |  |  [Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-long.165) |  | 0 |  | Jinyu Guo, Kai Shuang, Jijie Li, Zihan Wang, Yixuan Liu |  |
| 673 |  |  [Are Prompt-based Models Clueless?](https://doi.org/10.18653/v1/2022.acl-long.166) |  | 0 |  | Pride Kavumba, Ryo Takahashi, Yusuke Oda |  |
| 674 |  |  [Learning Confidence for Transformer-based Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.167) |  | 0 |  | Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, Mu Li |  |
| 675 |  |  [Things not Written in Text: Exploring Spatial Commonsense from Visual Signals](https://doi.org/10.18653/v1/2022.acl-long.168) |  | 0 |  | Xiao Liu, Da Yin, Yansong Feng, Dongyan Zhao |  |
| 676 |  |  [Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.169) |  | 0 |  | Songming Zhang, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jian Liu, Jie Zhou |  |
| 677 |  |  [ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer](https://doi.org/10.18653/v1/2022.acl-long.170) |  | 0 |  | Ningning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Victor Junqiu Wei, Qun Liu, Xin Jiang |  |
| 678 |  |  [Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks](https://doi.org/10.18653/v1/2022.acl-long.171) |  | 0 |  | Songlin Yang, Kewei Tu |  |
| 679 |  |  [Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.acl-long.172) |  | 0 |  | Liang Ding, Longyue Wang, Shuming Shi, Dacheng Tao, Zhaopeng Tu |  |
| 680 |  |  [Dependency Parsing as MRC-based Span-Span Prediction](https://doi.org/10.18653/v1/2022.acl-long.173) |  | 0 |  | Leilei Gan, Yuxian Meng, Kun Kuang, Xiaofei Sun, Chun Fan, Fei Wu, Jiwei Li |  |
| 681 |  |  [Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.174) |  | 0 |  | Hui Wu, Xiaodong Shi |  |
| 682 |  |  [Generating Scientific Claims for Zero-Shot Scientific Fact Checking](https://doi.org/10.18653/v1/2022.acl-long.175) |  | 0 |  | Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, Lucy Lu Wang |  |
| 683 |  |  [Modeling Dual Read/Write Paths for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.176) |  | 0 |  | Shaolei Zhang, Yang Feng |  |
| 684 |  |  [ExtEnD: Extractive Entity Disambiguation](https://doi.org/10.18653/v1/2022.acl-long.177) |  | 0 |  | Edoardo Barba, Luigi Procopio, Roberto Navigli |  |
| 685 |  |  [Hierarchical Sketch Induction for Paraphrase Generation](https://doi.org/10.18653/v1/2022.acl-long.178) |  | 0 |  | Tom Hosking, Hao Tang, Mirella Lapata |  |
| 686 |  |  [Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.179) |  | 0 |  | Keshav Kolluru, Muqeeth Mohammed, Shubham Mittal, Soumen Chakrabarti, Mausam |  |
| 687 |  |  [Text-to-Table: A New Way of Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.180) |  | 0 |  | Xueqing Wu, Jiacheng Zhang, Hang Li |  |
| 688 |  |  [Accelerating Code Search with Deep Hashing and Code Classification](https://doi.org/10.18653/v1/2022.acl-long.181) |  | 0 |  | Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Michael R. Lyu |  |
| 689 |  |  [Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions](https://doi.org/10.18653/v1/2022.acl-long.182) |  | 0 |  | Haitao Lin, Junnan Zhu, Lu Xiang, Yu Zhou, Jiajun Zhang, Chengqing Zong |  |
| 690 |  |  [ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification](https://doi.org/10.18653/v1/2022.acl-long.183) |  | 0 |  | Yucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long, Daxin Jiang |  |
| 691 |  |  [Measuring and Mitigating Name Biases in Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.184) |  | 0 |  | Jun Wang, Benjamin I. P. Rubinstein, Trevor Cohn |  |
| 692 |  |  [Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.185) |  | 0 |  | Wenxuan Wang, Wenxiang Jiao, Yongchang Hao, Xing Wang, Shuming Shi, Zhaopeng Tu, Michael R. Lyu |  |
| 693 |  |  [MSCTD: A Multimodal Sentiment Chat Translation Dataset](https://doi.org/10.18653/v1/2022.acl-long.186) |  | 0 |  | Yunlong Liang, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou |  |
| 694 |  |  [Learning Disentangled Textual Representations via Statistical Measures of Similarity](https://doi.org/10.18653/v1/2022.acl-long.187) |  | 0 |  | Pierre Colombo, Guillaume Staerman, Nathan Noiry, Pablo Piantanida |  |
| 695 |  |  [On the Sensitivity and Stability of Model Interpretations in NLP](https://doi.org/10.18653/v1/2022.acl-long.188) |  | 0 |  | Fan Yin, Zhouxing Shi, ChoJui Hsieh, KaiWei Chang |  |
| 696 |  |  [Down and Across: Introducing Crossword-Solving as a New NLP Benchmark](https://doi.org/10.18653/v1/2022.acl-long.189) |  | 0 |  | Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, Anna Rumshisky |  |
| 697 |  |  [Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets](https://doi.org/10.18653/v1/2022.acl-long.190) |  | 0 |  | Yuxiang Wu, Matt Gardner, Pontus Stenetorp, Pradeep Dasigi |  |
| 698 |  |  [GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.191) |  | 0 |  | Libo Qin, Qiguang Chen, Tianbao Xie, Qixin Li, JianGuang Lou, Wanxiang Che, MinYen Kan |  |
| 699 |  |  [Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER](https://doi.org/10.18653/v1/2022.acl-long.192) |  | 0 |  | DongHo Lee, Akshen Kadakia, Kangmin Tan, Mahak Agarwal, Xinyu Feng, Takashi Shibuya, Ryosuke Mitani, Toshiyuki Sekiya, Jay Pujara, Xiang Ren |  |
| 700 |  |  [Contextual Representation Learning beyond Masked Language Modeling](https://doi.org/10.18653/v1/2022.acl-long.193) |  | 0 |  | Zhiyi Fu, Wangchunshu Zhou, Jingjing Xu, Hao Zhou, Lei Li |  |
| 701 |  |  [Efficient Hyper-parameter Search for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2022.acl-long.194) |  | 0 |  | Yongqi Zhang, Zhanke Zhou, Quanming Yao, Yong Li |  |
| 702 |  |  [A Meta-framework for Spatiotemporal Quantity Extraction from Text](https://doi.org/10.18653/v1/2022.acl-long.195) |  | 0 |  | Qiang Ning, Ben Zhou, Hao Wu, Haoruo Peng, Chuchu Fan, Matt Gardner |  |
| 703 |  |  [Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer](https://doi.org/10.18653/v1/2022.acl-long.196) |  | 0 |  | Woojeong Jin, DongHo Lee, Chenguang Zhu, Jay Pujara, Xiang Ren |  |
| 704 |  |  [A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models](https://doi.org/10.18653/v1/2022.acl-long.197) |  | 0 |  | Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, Xiang Ren |  |
| 705 |  |  [Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation](https://doi.org/10.18653/v1/2022.acl-long.198) |  | 0 |  | Chengwei Qin, Shafiq R. Joty |  |
| 706 |  |  [Variational Graph Autoencoding as Cheap Supervision for AMR Coreference Resolution](https://doi.org/10.18653/v1/2022.acl-long.199) |  | 0 |  | Irene Li, Linfeng Song, Kun Xu, Dong Yu |  |
| 707 |  |  [Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations](https://doi.org/10.18653/v1/2022.acl-long.200) |  | 0 |  | Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Xiaobin Wang, Min Zhang |  |
| 708 |  |  [Sequence-to-Sequence Knowledge Graph Completion and Question Answering](https://doi.org/10.18653/v1/2022.acl-long.201) |  | 0 |  | Apoorv Saxena, Adrian Kochsiek, Rainer Gemulla |  |
| 709 |  |  [Learning to Mediate Disparities Towards Pragmatic Communication](https://doi.org/10.18653/v1/2022.acl-long.202) |  | 0 |  | Yuwei Bao, Sayan Ghosh, Joyce Chai |  |
| 710 |  |  [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.203) |  | 0 |  | Luyu Gao, Jamie Callan |  |
| 711 |  |  [Multimodal Dialogue Response Generation](https://doi.org/10.18653/v1/2022.acl-long.204) |  | 0 |  | Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, Daxin Jiang |  |
| 712 |  |  [CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.acl-long.205) |  | 0 |  | Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang Pu |  |
| 713 |  |  [Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.206) |  | 0 |  | Chulun Zhou, Fandong Meng, Jie Zhou, Min Zhang, Hongji Wang, Jinsong Su |  |
| 714 |  |  [BRIO: Bringing Order to Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.207) |  | 0 |  | Yixin Liu, Pengfei Liu, Dragomir R. Radev, Graham Neubig |  |
| 715 |  |  [Leveraging Relaxed Equilibrium by Lazy Transition for Sequence Modeling](https://doi.org/10.18653/v1/2022.acl-long.208) |  | 0 |  | Xi Ai, Bin Fang |  |
| 716 |  |  [FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework](https://doi.org/10.18653/v1/2022.acl-long.209) |  | 0 |  | Santiago Castro, Ruoyao Wang, Pingxuan Huang, Ian Stewart, Oana Ignat, Nan Liu, Jonathan C. Stroud, Rada Mihalcea |  |
| 717 |  |  [KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling](https://doi.org/10.18653/v1/2022.acl-long.210) |  | 0 |  | Xindi Wang, Robert E. Mercer, Frank Rudzicz |  |
| 718 |  |  [A Taxonomy of Empathetic Questions in Social Dialogs](https://doi.org/10.18653/v1/2022.acl-long.211) |  | 0 |  | Ekaterina Svikhnushina, Iuliana Voinea, Anuradha Welivita, Pearl Pu |  |
| 719 |  |  [Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.acl-long.212) |  | 0 |  | Hao Chen, Zepeng Zhai, Fangxiang Feng, Ruifan Li, Xiaojie Wang |  |
| 720 |  |  [ProtoTEx: Explaining Model Decisions with Prototype Tensors](https://doi.org/10.18653/v1/2022.acl-long.213) |  | 0 |  | Anubrata Das, Chitrank Gupta, Venelin Kovatchev, Matthew Lease, Junyi Jessy Li |  |
| 721 |  |  [Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data](https://doi.org/10.18653/v1/2022.acl-long.214) |  | 0 |  | Shuyan Zhou, Li Zhang, Yue Yang, Qing Lyu, Pengcheng Yin, Chris CallisonBurch, Graham Neubig |  |
| 722 |  |  [Cross-Modal Discrete Representation Learning](https://doi.org/10.18653/v1/2022.acl-long.215) |  | 0 |  | Alexander H. Liu, SouYoung Jin, ChengI Lai, Andrew Rouditchenko, Aude Oliva, James R. Glass |  |
| 723 |  |  [Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering](https://doi.org/10.18653/v1/2022.acl-long.216) |  | 0 |  | Jun Gao, Wei Wang, Changlong Yu, Huan Zhao, Wilfred Ng, Ruifeng Xu |  |
| 724 |  |  [Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations](https://doi.org/10.18653/v1/2022.acl-long.217) |  | 0 |  | Robert Wolfe, Aylin Caliskan |  |
| 725 |  |  [ConTinTin: Continual Learning from Task Instructions](https://doi.org/10.18653/v1/2022.acl-long.218) |  | 0 |  | Wenpeng Yin, Jia Li, Caiming Xiong |  |
| 726 |  |  [Automated Crossword Solving](https://doi.org/10.18653/v1/2022.acl-long.219) |  | 0 |  | Eric Wallace, Nicholas Tomlin, Albert Xu, Kevin Yang, Eshaan Pathak, Matthew L. Ginsberg, Dan Klein |  |
| 727 |  |  [Learned Incremental Representations for Parsing](https://doi.org/10.18653/v1/2022.acl-long.220) |  | 0 |  | Nikita Kitaev, Thomas Lu, Dan Klein |  |
| 728 |  |  [Knowledge Enhanced Reflection Generation for Counseling Dialogues](https://doi.org/10.18653/v1/2022.acl-long.221) |  | 0 |  | Siqi Shen, Verónica PérezRosas, Charles Welch, Soujanya Poria, Rada Mihalcea |  |
| 729 |  |  [Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines](https://doi.org/10.18653/v1/2022.acl-long.222) |  | 0 |  | Saadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska Roesner, Eunsol Choi, Yejin Choi |  |
| 730 |  |  [On Continual Model Refinement in Out-of-Distribution Data Streams](https://doi.org/10.18653/v1/2022.acl-long.223) |  | 0 |  | Bill Yuchen Lin, Sida Wang, Xi Victoria Lin, Robin Jia, Lin Xiao, Xiang Ren, Scott Yih |  |
| 731 |  |  [Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection](https://doi.org/10.18653/v1/2022.acl-long.224) |  | 0 |  | Bodhisattwa Prasad Majumder, Harsh Jhamtani, Taylor BergKirkpatrick, Julian J. McAuley |  |
| 732 |  |  [Generated Knowledge Prompting for Commonsense Reasoning](https://doi.org/10.18653/v1/2022.acl-long.225) |  | 0 |  | Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, Hannaneh Hajishirzi |  |
| 733 |  |  [Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data](https://doi.org/10.18653/v1/2022.acl-long.226) |  | 0 |  | Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, Michael Zeng |  |
| 734 |  |  [Life after BERT: What do Other Muppets Understand about Language?](https://doi.org/10.18653/v1/2022.acl-long.227) |  | 0 |  | Vladislav Lialin, Kevin Zhao, Namrata Shivagunde, Anna Rumshisky |  |
| 735 |  |  [Tailor: Generating and Perturbing Text with Semantic Controls](https://doi.org/10.18653/v1/2022.acl-long.228) |  | 0 |  | Alexis Ross, Tongshuang Wu, Hao Peng, Matthew E. Peters, Matt Gardner |  |
| 736 |  |  [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://doi.org/10.18653/v1/2022.acl-long.229) |  | 0 |  | Stephanie Lin, Jacob Hilton, Owain Evans |  |
| 737 |  |  [Adaptive Testing and Debugging of NLP Models](https://doi.org/10.18653/v1/2022.acl-long.230) |  | 0 |  | Marco Túlio Ribeiro, Scott M. Lundberg |  |
| 738 |  |  [Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning](https://doi.org/10.18653/v1/2022.acl-long.231) |  | 0 |  | Vivek Gupta, Shuo Zhang, Alakananda Vempala, Yujie He, Temma Choji, Vivek Srikumar |  |
| 739 |  |  [Interactive Word Completion for Plains Cree](https://doi.org/10.18653/v1/2022.acl-long.232) |  | 0 |  | William Lane, Atticus Harrigan, Antti Arppe |  |
| 740 |  |  [LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.233) |  | 0 |  | Dora Jambor, Dzmitry Bahdanau |  |
| 741 |  |  [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2022.acl-long.234) |  | 0 |  | Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, Ece Kamar |  |
| 742 |  |  [Direct Speech-to-Speech Translation With Discrete Units](https://doi.org/10.18653/v1/2022.acl-long.235) |  | 0 |  | Ann Lee, PengJen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, WeiNing Hsu |  |
| 743 |  |  [Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.236) |  | 0 |  | Meng Cao, Yue Dong, Jackie Chi Kit Cheung |  |
| 744 |  |  [EntSUM: A Data Set for Entity-Centric Extractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.237) |  | 0 |  | Mounica Maddela, Mayank Kulkarni, Daniel PreotiucPietro |  |
| 745 |  |  [Sentence-level Privacy for Document Embeddings](https://doi.org/10.18653/v1/2022.acl-long.238) |  | 0 |  | Casey Meehan, Khalil Mrini, Kamalika Chaudhuri |  |
| 746 |  |  [Dataset Geography: Mapping Language Data to Language Users](https://doi.org/10.18653/v1/2022.acl-long.239) |  | 0 |  | Fahim Faisal, Yinkai Wang, Antonios Anastasopoulos |  |
| 747 |  |  [ILDAE: Instance-Level Difficulty Analysis of Evaluation Data](https://doi.org/10.18653/v1/2022.acl-long.240) |  | 0 |  | Neeraj Varshney, Swaroop Mishra, Chitta Baral |  |
| 748 |  |  [Image Retrieval from Contextual Descriptions](https://doi.org/10.18653/v1/2022.acl-long.241) |  | 0 |  | Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Maria Ponti, Siva Reddy |  |
| 749 |  |  [Multilingual Molecular Representation Learning via Contrastive Pre-training](https://doi.org/10.18653/v1/2022.acl-long.242) |  | 0 |  | Zhihui Guo, Pramod Kumar Sharma, Andy Martinez, Liang Du, Robin Abraham |  |
| 750 |  |  [Investigating Failures of Automatic Translationin the Case of Unambiguous Gender](https://doi.org/10.18653/v1/2022.acl-long.243) |  | 0 |  | Adi Renduchintala, Adina Williams |  |
| 751 |  |  [Cross-Task Generalization via Natural Language Crowdsourcing Instructions](https://doi.org/10.18653/v1/2022.acl-long.244) |  | 0 |  | Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi |  |
| 752 |  |  [Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost](https://doi.org/10.18653/v1/2022.acl-long.245) |  | 0 |  | Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek |  |
| 753 |  |  [NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks](https://doi.org/10.18653/v1/2022.acl-long.246) |  | 0 |  | Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Singh Sachdeva, Peter Clark, Chitta Baral, Ashwin Kalyan |  |
| 754 |  |  [Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.247) |  | 0 |  | Ryan Steed, Swetasudha Panda, Ari Kobren, Michael L. Wick |  |
| 755 |  |  [Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement](https://doi.org/10.18653/v1/2022.acl-long.248) |  | 0 |  | Yangjun Zhang, Pengjie Ren, Wentao Deng, Zhumin Chen, Maarten de Rijke |  |
| 756 |  |  [How Do We Answer Complex Questions: Discourse Structure of Long-form Answers](https://doi.org/10.18653/v1/2022.acl-long.249) |  | 0 |  | Fangyuan Xu, Junyi Jessy Li, Eunsol Choi |  |
| 757 |  |  [Understanding Iterative Revision from Human-Written Text](https://doi.org/10.18653/v1/2022.acl-long.250) |  | 0 |  | Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa Lopez, Dongyeop Kang |  |
| 758 |  |  [Making Transformers Solve Compositional Tasks](https://doi.org/10.18653/v1/2022.acl-long.251) |  | 0 |  | Santiago Ontañón, Joshua Ainslie, Zachary Fisher, Vaclav Cvicek |  |
| 759 |  |  [Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.252) |  | 0 |  | Verna Dankers, Christopher G. Lucas, Ivan Titov |  |
| 760 |  |  [ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers](https://doi.org/10.18653/v1/2022.acl-long.253) |  | 0 |  | Haitian Sun, William W. Cohen, Ruslan Salakhutdinov |  |
| 761 |  |  [Prompt-free and Efficient Few-shot Learning with Language Models](https://doi.org/10.18653/v1/2022.acl-long.254) |  | 0 |  | Rabeeh Karimi Mahabadi, Luke Zettlemoyer, James Henderson, Lambert Mathias, Marzieh Saeidi, Veselin Stoyanov, Majid Yazdani |  |
| 762 |  |  [Continual Sequence Generation with Adaptive Compositional Modules](https://doi.org/10.18653/v1/2022.acl-long.255) |  | 0 |  | Yanzhe Zhang, Xuezhi Wang, Diyi Yang |  |
| 763 |  |  [An Investigation of the (In)effectiveness of Counterfactually Augmented Data](https://doi.org/10.18653/v1/2022.acl-long.256) |  | 0 |  | Nitish Joshi, He He |  |
| 764 |  |  [Inducing Positive Perspectives with Text Reframing](https://doi.org/10.18653/v1/2022.acl-long.257) |  | 0 |  | Caleb Ziems, Minzhi Li, Anthony Zhang, Diyi Yang |  |
| 765 |  |  [VALUE: Understanding Dialect Disparity in NLU](https://doi.org/10.18653/v1/2022.acl-long.258) |  | 0 |  | Caleb Ziems, Jiaao Chen, Camille Harris, Jessica Anderson, Diyi Yang |  |
| 766 |  |  [From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer](https://doi.org/10.18653/v1/2022.acl-long.259) |  | 0 |  | John Pavlopoulos, Léo Laugier, Alexandros Xenos, Jeffrey Sorensen, Ion Androutsopoulos |  |
| 767 |  |  [FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.260) |  | 0 |  | ChenYu Lee, ChunLiang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, Tomas Pfister |  |
| 768 |  |  [The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.261) |  | 0 |  | Caleb Ziems, Jane A. Yu, YiChia Wang, Alon Y. Halevy, Diyi Yang |  |
| 769 |  |  [Token Dropping for Efficient BERT Pretraining](https://doi.org/10.18653/v1/2022.acl-long.262) |  | 0 |  | Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, Denny Zhou |  |
| 770 |  |  [DialFact: A Benchmark for Fact-Checking in Dialogue](https://doi.org/10.18653/v1/2022.acl-long.263) |  | 0 |  | Prakhar Gupta, ChienSheng Wu, Wenhao Liu, Caiming Xiong |  |
| 771 |  |  [The Trade-offs of Domain Adaptation for Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.264) |  | 0 |  | David Grangier, Dan Iter |  |
| 772 |  |  [Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go](https://doi.org/10.18653/v1/2022.acl-long.265) |  | 0 |  | Ife Adebara, Muhammad AbdulMageed |  |
| 773 |  |  [Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-long.266) |  | 0 |  | Maksym Tarnavskyi, Artem N. Chernodub, Kostiantyn Omelianchuk |  |
| 774 |  |  [Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching](https://doi.org/10.18653/v1/2022.acl-long.267) |  | 0 |  | Alissa Ostapenko, Shuly Wintner, Melinda Fricke, Yulia Tsvetkov |  |
| 775 |  |  [Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling](https://doi.org/10.18653/v1/2022.acl-long.268) |  | 0 |  | Elena Álvarez Mellado, Constantine Lignos |  |
| 776 |  |  [Is Attention Explanation? An Introduction to the Debate](https://doi.org/10.18653/v1/2022.acl-long.269) |  | 0 |  | Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas François, Patrick Watrin |  |
| 777 |  |  [There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory](https://doi.org/10.18653/v1/2022.acl-long.270) |  | 0 |  | Tingchen Fu, Xueliang Zhao, Chongyang Tao, JiRong Wen, Rui Yan |  |
| 778 |  |  [Neural Pipeline for Zero-Shot Data-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-long.271) |  | 0 |  | Zdenek Kasner, Ondrej Dusek |  |
| 779 |  |  [Not always about you: Prioritizing community needs when developing endangered language technology](https://doi.org/10.18653/v1/2022.acl-long.272) |  | 0 |  | Zoey Liu, Crystal Richardson, Richard J. Hatcher, Emily Prud'hommeaux |  |
| 780 |  |  [Automatic Identification and Classification of Bragging in Social Media](https://doi.org/10.18653/v1/2022.acl-long.273) |  | 0 |  | Mali Jin, Daniel PreotiucPietro, A. Seza Dogruöz, Nikolaos Aletras |  |
| 781 |  |  [Automatic Error Analysis for Document-level Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.274) |  | 0 |  | Aliva Das, Xinya Du, Barry Wang, Kejian Shi, Jiayuan Gu, Thomas Porter, Claire Cardie |  |
| 782 |  |  [Learning Functional Distributional Semantics with Visual Data](https://doi.org/10.18653/v1/2022.acl-long.275) |  | 0 |  | Yinhong Liu, Guy Emerson |  |
| 783 |  |  [ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.276) |  | 0 |  | Sayan Ghosh, Shashank Srivastava |  |
| 784 |  |  [Chart-to-Text: A Large-Scale Benchmark for Chart Summarization](https://doi.org/10.18653/v1/2022.acl-long.277) |  | 0 |  | Shankar Kantharaj, Rixie Tiffany Ko Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, Shafiq R. Joty |  |
| 785 |  |  [Characterizing Idioms: Conventionality and Contingency](https://doi.org/10.18653/v1/2022.acl-long.278) |  | 0 |  | Michaela Socolof, Jackie Chi Kit Cheung, Michael Wagner, Timothy J. O'Donnell |  |
| 786 |  |  [Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP](https://doi.org/10.18653/v1/2022.acl-long.279) |  | 0 |  | Lukas Galke, Ansgar Scherp |  |
| 787 |  |  [Generative Pretraining for Paraphrase Evaluation](https://doi.org/10.18653/v1/2022.acl-long.280) |  | 0 |  | Jack Weston, Raphael Lenain, Udeepa Meepegama, Emil Fristed |  |
| 788 |  |  [Incorporating Stock Market Signals for Twitter Stance Detection](https://doi.org/10.18653/v1/2022.acl-long.281) |  | 0 |  | Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel Collier |  |
| 789 |  |  [Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.282) |  | 0 |  | Yong Cheng, Ankur Bapna, Orhan Firat, Yuan Cao, Pidong Wang, Wolfgang Macherey |  |
| 790 |  |  [Word Segmentation as Unsupervised Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.283) |  | 0 |  | Raquel G. Alhama |  |
| 791 |  |  [SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems](https://doi.org/10.18653/v1/2022.acl-long.284) |  | 0 |  | Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon L. Spruit, Dirk Hovy, YLan Boureau, Verena Rieser |  |
| 792 |  |  [Zero-Shot Cross-lingual Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.285) |  | 0 |  | Tom Sherborne, Mirella Lapata |  |
| 793 |  |  [The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study](https://doi.org/10.18653/v1/2022.acl-long.286) |  | 0 |  | Verna Dankers, Elia Bruni, Dieuwke Hupkes |  |
| 794 |  |  [Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents](https://doi.org/10.18653/v1/2022.acl-long.287) |  | 0 |  | Biao Zhang, Ankur Bapna, Melvin Johnson, Ali Dabirmoghaddam, Naveen Arivazhagan, Orhan Firat |  |
| 795 |  |  [Cross-Lingual Phrase Retrieval](https://doi.org/10.18653/v1/2022.acl-long.288) |  | 0 |  | Heqi Zheng, Xiao Zhang, Zewen Chi, Heyan Huang, Yan Tan, Tian Lan, Wei Wei, XianLing Mao |  |
| 796 |  |  [Improving Compositional Generalization with Self-Training for Data-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-long.289) |  | 0 |  | Sanket Vaibhav Mehta, Jinfeng Rao, Yi Tay, Mihir Kale, Ankur Parikh, Emma Strubell |  |
| 797 |  |  [MMCoQA: Conversational Question Answering over Text, Tables, and Images](https://doi.org/10.18653/v1/2022.acl-long.290) |  | 0 |  | Yongqi Li, Wenjie Li, Liqiang Nie |  |
| 798 |  |  [Effective Token Graph Modeling using a Novel Labeling Strategy for Structured Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.291) |  | 0 |  | Wenxuan Shi, Fei Li, Jingye Li, Hao Fei, Donghong Ji |  |
| 799 |  |  [PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks](https://doi.org/10.18653/v1/2022.acl-long.292) |  | 0 |  | Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, Daxin Jiang |  |
| 800 |  |  [Disentangled Sequence to Sequence Learning for Compositional Generalization](https://doi.org/10.18653/v1/2022.acl-long.293) |  | 0 |  | Hao Zheng, Mirella Lapata |  |
| 801 |  |  [RST Discourse Parsing with Second-Stage EDU-Level Pre-training](https://doi.org/10.18653/v1/2022.acl-long.294) |  | 0 |  | Nan Yu, Meishan Zhang, Guohong Fu, Min Zhang |  |
| 802 |  |  [SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.295) |  | 0 |  | Liang Wang, Wei Zhao, Zhuoyu Wei, Jingming Liu |  |
| 803 |  |  [Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?](https://doi.org/10.18653/v1/2022.acl-long.296) |  | 0 |  | Oliver Eberle, Stephanie Brandl, Jonas Pilot, Anders Søgaard |  |
| 804 |  |  [LexGLUE: A Benchmark Dataset for Legal Language Understanding in English](https://doi.org/10.18653/v1/2022.acl-long.297) |  | 0 |  | Ilias Chalkidis, Abhik Jana, Dirk Hartung, Michael J. Bommarito II, Ion Androutsopoulos, Daniel Martin Katz, Nikolaos Aletras |  |
| 805 |  |  [DiBiMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.298) |  | 0 |  | Niccolò Campolungo, Federico Martelli, Francesco Saina, Roberto Navigli |  |
| 806 |  |  [Improving Word Translation via Two-Stage Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.299) |  | 0 |  | Yaoyiran Li, Fangyu Liu, Nigel Collier, Anna Korhonen, Ivan Vulic |  |
| 807 |  |  [Scheduled Multi-task Learning for Neural Chat Translation](https://doi.org/10.18653/v1/2022.acl-long.300) |  | 0 |  | Yunlong Liang, Fandong Meng, Jinan Xu, Yufeng Chen, Jie Zhou |  |
| 808 |  |  [FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing](https://doi.org/10.18653/v1/2022.acl-long.301) |  | 0 |  | Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia Tomada, Sebastian Felix Schwemer, Anders Søgaard |  |
| 809 |  |  [Towards Abstractive Grounded Summarization of Podcast Transcripts](https://doi.org/10.18653/v1/2022.acl-long.302) |  | 0 |  | Kaiqiang Song, Chen Li, Xiaoyang Wang, Dong Yu, Fei Liu |  |
| 810 |  |  [FiNER: Financial Numeric Entity Recognition for XBRL Tagging](https://doi.org/10.18653/v1/2022.acl-long.303) |  | 0 |  | Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodromos Malakasiotis, Ion Androutsopoulos, Georgios Paliouras |  |
| 811 |  |  [Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation](https://doi.org/10.18653/v1/2022.acl-long.304) |  | 0 |  | Mingzhe Li, Xiexiong Lin, Xiuying Chen, Jinxiong Chang, Qishen Zhang, Feng Wang, Taifeng Wang, Zhongyi Liu, Wei Chu, Dongyan Zhao, Rui Yan |  |
| 812 |  |  [EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers](https://doi.org/10.18653/v1/2022.acl-long.305) |  | 0 |  | Bugeun Kim, Kyung Seo Ki, Sangkyu Rhim, Gahgene Gweon |  |
| 813 |  |  [Identifying the Human Values behind Arguments](https://doi.org/10.18653/v1/2022.acl-long.306) |  | 0 |  | Johannes Kiesel, Milad Alshomary, Nicolas Handke, Xiaoni Cai, Henning Wachsmuth, Benno Stein |  |
| 814 |  |  [BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation](https://doi.org/10.18653/v1/2022.acl-long.307) |  | 0 |  | Kiril Gashteovski, Mingying Yu, Bhushan Kotnis, Carolin Lawrence, Mathias Niepert, Goran Glavas |  |
| 815 |  |  [Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2022.acl-long.308) |  | 0 |  | Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou, Xinbing Wang, Zhouhan Lin |  |
| 816 |  |  [SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.309) |  | 0 |  | Mathieu Ravaut, Shafiq R. Joty, Nancy F. Chen |  |
| 817 |  |  [Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals](https://doi.org/10.18653/v1/2022.acl-long.310) |  | 0 |  | TeLin Wu, Alexander Spangher, Pegah Alipoormolabashi, Marjorie Freedman, Ralph M. Weischedel, Nanyun Peng |  |
| 818 |  |  [Zoom Out and Observe: News Environment Perception for Fake News Detection](https://doi.org/10.18653/v1/2022.acl-long.311) |  | 0 |  | Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang, Yongchun Zhu |  |
| 819 |  |  [Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models](https://doi.org/10.18653/v1/2022.acl-long.312) |  | 0 |  | Lorenzo Lupo, Marco Dinarelli, Laurent Besacier |  |
| 820 |  |  [Saliency as Evidence: Event Detection with Trigger Saliency Attribution](https://doi.org/10.18653/v1/2022.acl-long.313) |  | 0 |  | Jian Liu, Yufeng Chen, Jinan Xu |  |
| 821 |  |  [SRL4E - Semantic Role Labeling for Emotions: A Unified Evaluation Framework](https://doi.org/10.18653/v1/2022.acl-long.314) |  | 0 |  | Cesare Campagnano, Simone Conia, Roberto Navigli |  |
| 822 |  |  [Context Matters: A Pragmatic Study of PLMs' Negation Understanding](https://doi.org/10.18653/v1/2022.acl-long.315) |  | 0 |  | Reto Gubelmann, Siegfried Handschuh |  |
| 823 |  |  [Probing for Predicate Argument Structures in Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.316) |  | 0 |  | Simone Conia, Roberto Navigli |  |
| 824 |  |  [Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.317) |  | 0 |  | KuanHao Huang, IHung Hsu, Prem Natarajan, KaiWei Chang, Nanyun Peng |  |
| 825 |  |  [Identifying Moments of Change from Longitudinal User Text](https://doi.org/10.18653/v1/2022.acl-long.318) |  | 0 |  | Adam Tsakalidis, Federico Nanni, Anthony Hills, Jenny Chim, Jiayu Song, Maria Liakata |  |
| 826 |  |  [Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2022.acl-long.319) |  | 0 |  | Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, YiAn Lai, Yi Zhang |  |
| 827 |  |  [Graph Enhanced Contrastive Learning for Radiology Findings Summarization](https://doi.org/10.18653/v1/2022.acl-long.320) |  | 0 |  | Jinpeng Hu, Zhuo Li, Zhihong Chen, Zhen Li, Xiang Wan, TsungHui Chang |  |
| 828 |  |  [Semi-Supervised Formality Style Transfer with Consistency Training](https://doi.org/10.18653/v1/2022.acl-long.321) |  | 0 |  | Ao Liu, An Wang, Naoaki Okazaki |  |
| 829 |  |  [Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure](https://doi.org/10.18653/v1/2022.acl-long.322) |  | 0 |  | Yuan Chai, Yaobo Liang, Nan Duan |  |
| 830 |  |  [Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting](https://doi.org/10.18653/v1/2022.acl-long.323) |  | 0 |  | Ying Su, Hongming Zhang, Yangqiu Song, Tong Zhang |  |
| 831 |  |  [Nibbling at the Hard Core of Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.acl-long.324) |  | 0 |  | Marco Maru, Simone Conia, Michele Bevilacqua, Roberto Navigli |  |
| 832 |  |  [Large Scale Substitution-based Word Sense Induction](https://doi.org/10.18653/v1/2022.acl-long.325) |  | 0 |  | Matan Eyal, Shoval Sadde, Hillel TaubTabib, Yoav Goldberg |  |
| 833 |  |  [Can Synthetic Translations Improve Bitext Quality?](https://doi.org/10.18653/v1/2022.acl-long.326) |  | 0 |  | Eleftheria Briakou, Marine Carpuat |  |
| 834 |  |  [Unsupervised Dependency Graph Network](https://doi.org/10.18653/v1/2022.acl-long.327) |  | 0 |  | Yikang Shen, Shawn Tan, Alessandro Sordoni, Peng Li, Jie Zhou, Aaron C. Courville |  |
| 835 |  |  [WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types](https://doi.org/10.18653/v1/2022.acl-long.328) |  | 0 |  | Xuwu Wang, Junfeng Tian, Min Gui, Zhixu Li, Rui Wang, Ming Yan, Lihan Chen, Yanghua Xiao |  |
| 836 |  |  [Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.329) |  | 0 |  | Zaiqiao Meng, Fangyu Liu, Ehsan Shareghi, Yixuan Su, Charlotte Collins, Nigel Collier |  |
| 837 |  |  [Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT](https://doi.org/10.18653/v1/2022.acl-long.330) |  | 0 |  | Jing Zhao, Yifan Wang, Junwei Bao, Youzheng Wu, Xiaodong He |  |
| 838 |  |  [Compression of Generative Pre-trained Language Models via Quantization](https://doi.org/10.18653/v1/2022.acl-long.331) |  | 0 |  | Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong |  |
| 839 |  |  [Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration](https://doi.org/10.18653/v1/2022.acl-long.332) |  | 0 |  | Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, Xiaodan Liang |  |
| 840 |  |  [DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation](https://doi.org/10.18653/v1/2022.acl-long.333) |  | 0 |  | Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, Nan Duan |  |
| 841 |  |  [Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations](https://doi.org/10.18653/v1/2022.acl-long.334) |  | 0 |  | Wei Chen, Yeyun Gong, Can Xu, Huang Hu, Bolun Yao, Zhongyu Wei, Zhihao Fan, Xiaowu Hu, Bartuer Zhou, Biao Cheng, Daxin Jiang, Nan Duan |  |
| 842 |  |  [Textomics: A Dataset for Genomics Data Summary Generation](https://doi.org/10.18653/v1/2022.acl-long.335) |  | 0 |  | MuChun Wang, Zixuan Liu, Sheng Wang |  |
| 843 |  |  [A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space](https://doi.org/10.18653/v1/2022.acl-long.336) |  | 0 |  | Yuhao Zhang, Hongji Zhu, Yongliang Wang, Nan Xu, Xiaobo Li, Binqiang Zhao |  |
| 844 |  |  [Packed Levitated Marker for Entity and Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.337) |  | 0 |  | Deming Ye, Yankai Lin, Peng Li, Maosong Sun |  |
| 845 |  |  [An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.338) |  | 0 |  | Shiquan Yang, Rui Zhang, Sarah M. Erfani, Jey Han Lau |  |
| 846 |  |  [Impact of Evaluation Methodologies on Code Summarization](https://doi.org/10.18653/v1/2022.acl-long.339) |  | 0 |  | Pengyu Nie, Jiyang Zhang, Junyi Jessy Li, Raymond J. Mooney, Milos Gligoric |  |
| 847 |  |  [KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.acl-long.340) |  | 0 |  | Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, Michael Zeng |  |
| 848 |  |  [Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media](https://doi.org/10.18653/v1/2022.acl-long.341) |  | 0 |  | Pavan Holur, Tianyi Wang, Shadi Shahsavari, Timothy R. Tangherlini, Vwani P. Roychowdhury |  |
| 849 |  |  [Learning From Failure: Data Capture in an Australian Aboriginal Community](https://doi.org/10.18653/v1/2022.acl-long.342) |  | 0 |  | Éric Le Ferrand, Steven Bird, Laurent Besacier |  |
| 850 |  |  [Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.343) |  | 0 |  | Wenya Wang, Sinno Jialin Pan |  |
| 851 |  |  [CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues](https://doi.org/10.18653/v1/2022.acl-long.344) |  | 0 |  | Deepanway Ghosal, Siqi Shen, Navonil Majumder, Rada Mihalcea, Soujanya Poria |  |
| 852 |  |  [A Comparative Study of Faithfulness Metrics for Model Interpretability Methods](https://doi.org/10.18653/v1/2022.acl-long.345) |  | 0 |  | Chun Sik Chan, Huanqi Kong, Guanqing Liang |  |
| 853 |  |  [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](https://doi.org/10.18653/v1/2022.acl-long.346) |  | 0 |  | Tu Vu, Brian Lester, Noah Constant, Rami AlRfou', Daniel Cer |  |
| 854 |  |  [Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models](https://doi.org/10.18653/v1/2022.acl-long.347) |  | 0 |  | Biru Zhu, Yujia Qin, Fanchao Qi, Yangdong Deng, Zhiyuan Liu, Maosong Sun, Ming Gu |  |
| 855 |  |  [Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization](https://doi.org/10.18653/v1/2022.acl-long.348) |  | 0 |  | Zhenjie Zhao, Yufang Hou, Dakuo Wang, Mo Yu, Chengzhong Liu, Xiaojuan Ma |  |
| 856 |  |  [HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations](https://doi.org/10.18653/v1/2022.acl-long.349) |  | 0 |  | JiaChen Gu, ChaoHong Tan, Chongyang Tao, ZhenHua Ling, Huang Hu, Xiubo Geng, Daxin Jiang |  |
| 857 |  |  [The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature](https://doi.org/10.18653/v1/2022.acl-long.350) |  | 0 |  | Yulia Otmakhova, Karin Verspoor, Timothy Baldwin, Jey Han Lau |  |
| 858 |  |  [A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.351) |  | 0 |  | Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi |  |
| 859 |  |  [KNN-Contrastive Learning for Out-of-Domain Intent Classification](https://doi.org/10.18653/v1/2022.acl-long.352) |  | 0 |  | Yunhua Zhou, Peiju Liu, Xipeng Qiu |  |
| 860 |  |  [A Neural Network Architecture for Program Understanding Inspired by Human Behaviors](https://doi.org/10.18653/v1/2022.acl-long.353) |  | 0 |  | Renyu Zhu, Lei Yuan, Xiang Li, Ming Gao, Wenyuan Cai |  |
| 861 |  |  [FaVIQ: FAct Verification from Information-seeking Questions](https://doi.org/10.18653/v1/2022.acl-long.354) |  | 0 |  | Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 862 |  |  [Simulating Bandit Learning from User Feedback for Extractive Question Answering](https://doi.org/10.18653/v1/2022.acl-long.355) |  | 0 |  | Ge Gao, Eunsol Choi, Yoav Artzi |  |
| 863 |  |  [Beyond Goldfish Memory: Long-Term Open-Domain Conversation](https://doi.org/10.18653/v1/2022.acl-long.356) |  | 0 |  | Jing Xu, Arthur Szlam, Jason Weston |  |
| 864 |  |  [ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension](https://doi.org/10.18653/v1/2022.acl-long.357) |  | 0 |  | Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, Anna Rohrbach |  |
| 865 |  |  [Dynamic Prefix-Tuning for Generative Template-based Event Extraction](https://doi.org/10.18653/v1/2022.acl-long.358) |  | 0 |  | Xiao Liu, Heyan Huang, Ge Shi, Bo Wang |  |
| 866 |  |  [E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models](https://doi.org/10.18653/v1/2022.acl-long.359) |  | 0 |  | Mohammad Akbari, Amin BanitalebiDehkordi, Yong Zhang |  |
| 867 |  |  [PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization](https://doi.org/10.18653/v1/2022.acl-long.360) |  | 0 |  | Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan |  |
| 868 |  |  [Dynamic Global Memory for Document-level Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.361) |  | 0 |  | Xinya Du, Sha Li, Heng Ji |  |
| 869 |  |  [Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns](https://doi.org/10.18653/v1/2022.acl-long.362) |  | 0 |  | Daniel Wiechmann, Elma Kerz |  |
| 870 |  |  [Alternative Input Signals Ease Transfer in Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.363) |  | 0 |  | Simeng Sun, Angela Fan, James Cross, Vishrav Chaudhary, Chau Tran, Philipp Koehn, Francisco Guzmán |  |
| 871 |  |  [Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data](https://doi.org/10.18653/v1/2022.acl-long.364) |  | 0 |  | Colin Leong, Daniel Whitenack |  |
| 872 |  |  [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.365) |  | 0 |  | Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer |  |
| 873 |  |  [Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages](https://doi.org/10.18653/v1/2022.acl-long.366) |  | 0 |  | C. M. Downey, Shannon Drizin, Levon Haroutunian, Shivin Thukral |  |
| 874 |  |  [KinyaBERT: a Morphology-aware Kinyarwanda Language Model](https://doi.org/10.18653/v1/2022.acl-long.367) |  | 0 |  | Antoine Nzeyimana, Andre Niyongabo Rubungo |  |
| 875 |  |  [On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency](https://doi.org/10.18653/v1/2022.acl-long.368) |  | 0 |  | Seoyeon Park, Cornelia Caragea |  |
| 876 |  |  [IMPLI: Investigating NLI Models' Performance on Figurative Language](https://doi.org/10.18653/v1/2022.acl-long.369) |  | 0 |  | Kevin Stowe, Prasetya Ajie Utama, Iryna Gurevych |  |
| 877 |  |  [QAConv: Question Answering on Informative Conversations](https://doi.org/10.18653/v1/2022.acl-long.370) |  | 0 |  | ChienSheng Wu, Andrea Madotto, Wenhao Liu, Pascale Fung, Caiming Xiong |  |
| 878 |  |  [Prix-LM: Pretraining for Multilingual Knowledge Base Construction](https://doi.org/10.18653/v1/2022.acl-long.371) |  | 0 |  | Wenxuan Zhou, Fangyu Liu, Ivan Vulic, Nigel Collier, Muhao Chen |  |
| 879 |  |  [Semantic Composition with PSHRG for Derivation Tree Reconstruction from Graph-Based Meaning Representations](https://doi.org/10.18653/v1/2022.acl-long.372) |  | 0 |  | Chun Hei Lo, Wai Lam, Hong Cheng |  |
| 880 |  |  [HOLM: Hallucinating Objects with Language Models for Referring Expression Recognition in Partially-Observed Scenes](https://doi.org/10.18653/v1/2022.acl-long.373) |  | 0 |  | Volkan Cirik, LouisPhilippe Morency, Taylor BergKirkpatrick |  |
| 881 |  |  [Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models](https://doi.org/10.18653/v1/2022.acl-long.374) |  | 0 |  | Kabir Ahuja, Shanu Kumar, Sandipan Dandapat, Monojit Choudhury |  |
| 882 |  |  [∞-former: Infinite Memory Transformer](https://doi.org/10.18653/v1/2022.acl-long.375) |  | 0 |  | Pedro Henrique Martins, Zita Marinho, André F. T. Martins |  |
| 883 |  |  [Systematic Inequalities in Language Technology Performance across the World's Languages](https://doi.org/10.18653/v1/2022.acl-long.376) |  | 0 |  | Damián E. Blasi, Antonios Anastasopoulos, Graham Neubig |  |
| 884 |  |  [CaMEL: Case Marker Extraction without Labels](https://doi.org/10.18653/v1/2022.acl-long.377) |  | 0 |  | Leonie Weissweiler, Valentin Hofmann, Masoud Jalili Sabet, Hinrich Schütze |  |
| 885 |  |  [Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors](https://doi.org/10.18653/v1/2022.acl-long.378) |  | 0 |  | Isar Nejadgholi, Kathleen C. Fraser, Svetlana Kiritchenko |  |
| 886 |  |  [Reports of personal experiences and stories in argumentation: datasets and analysis](https://doi.org/10.18653/v1/2022.acl-long.379) |  | 0 |  | Neele Falk, Gabriella Lapesa |  |
| 887 |  |  [Non-neural Models Matter: a Re-evaluation of Neural Referring Expression Generation Systems](https://doi.org/10.18653/v1/2022.acl-long.380) |  | 0 |  | Fahime Same, Guanyi Chen, Kees van Deemter |  |
| 888 |  |  [Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion](https://doi.org/10.18653/v1/2022.acl-long.381) |  | 0 |  | Chen Zhao, Yu Su, Adam Pauls, Emmanouil Antonios Platanios |  |
| 889 |  |  [Predicate-Argument Based Bi-Encoder for Paraphrase Identification](https://doi.org/10.18653/v1/2022.acl-long.382) |  | 0 |  | Qiwei Peng, David J. Weir, Julie Weeds, Yekun Chai |  |
| 890 |  |  [MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective](https://doi.org/10.18653/v1/2022.acl-long.383) |  | 0 |  | Xiao Wang, Shihan Dou, Limao Xiong, Yicheng Zou, Qi Zhang, Tao Gui, Liang Qiao, Zhanzhan Cheng, Xuanjing Huang |  |
| 891 |  |  [Leveraging Wikipedia article evolution for promotional tone detection](https://doi.org/10.18653/v1/2022.acl-long.384) |  | 0 |  | Christine de Kock, Andreas Vlachos |  |
| 892 |  |  [From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology](https://doi.org/10.18653/v1/2022.acl-long.385) |  | 0 |  | Mark Dingemanse, Andreas Liesenfeld |  |
| 893 |  |  [Flooding-X: Improving BERT's Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning](https://doi.org/10.18653/v1/2022.acl-long.386) |  | 0 |  | Qin Liu, Rui Zheng, Bao Rong, Jingyi Liu, Zhihua Liu, Zhanzhan Cheng, Liang Qiao, Tao Gui, Qi Zhang, Xuanjing Huang |  |
| 894 |  |  [RoMe: A Robust Metric for Evaluating Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.387) |  | 0 |  | Md. Rashad Al Hasan Rony, Liubov Kovriguina, Debanjan Chaudhuri, Ricardo Usbeck, Jens Lehmann |  |
| 895 |  |  [Finding Structural Knowledge in Multimodal-BERT](https://doi.org/10.18653/v1/2022.acl-long.388) |  | 0 |  | Victor Milewski, Miryam de Lhoneux, MarieFrancine Moens |  |
| 896 |  |  [Fully Hyperbolic Neural Networks](https://doi.org/10.18653/v1/2022.acl-long.389) |  | 0 |  | Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou |  |
| 897 |  |  [Neural Machine Translation with Phrase-Level Universal Visual Representations](https://doi.org/10.18653/v1/2022.acl-long.390) |  | 0 |  | Qingkai Fang, Yang Feng |  |
| 898 |  |  [M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database](https://doi.org/10.18653/v1/2022.acl-long.391) |  | 0 |  | Jinming Zhao, Tenggan Zhang, Jingwen Hu, Yuchen Liu, Qin Jin, Xinchao Wang, Haizhou Li |  |
| 899 |  |  [Few-shot Named Entity Recognition with Self-describing Networks](https://doi.org/10.18653/v1/2022.acl-long.392) |  | 0 |  | Jiawei Chen, Qing Liu, Hongyu Lin, Xianpei Han, Le Sun |  |
| 900 |  |  [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://doi.org/10.18653/v1/2022.acl-long.393) |  | 0 |  | Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei |  |
| 901 |  |  [Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation](https://doi.org/10.18653/v1/2022.acl-long.394) |  | 0 |  | Francesco Moramarco, Alex PapadopoulosKorfiatis, Mark Perera, Damir Juric, Jack Flann, Ehud Reiter, Anya Belz, Aleksandar Savkov |  |
| 902 |  |  [Unified Structure Generation for Universal Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.395) |  | 0 |  | Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu |  |
| 903 |  |  [Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.acl-long.396) |  | 0 |  | Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, Hong Chen |  |
| 904 |  |  [Pre-training to Match for Unified Low-shot Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.397) |  | 0 |  | Fangchao Liu, Hongyu Lin, Xianpei Han, Boxi Cao, Le Sun |  |
| 905 |  |  [Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View](https://doi.org/10.18653/v1/2022.acl-long.398) |  | 0 |  | Boxi Cao, Hongyu Lin, Xianpei Han, Fangchao Liu, Le Sun |  |
| 906 |  |  [Evaluating Extreme Hierarchical Multi-label Classification](https://doi.org/10.18653/v1/2022.acl-long.399) |  | 0 |  | Enrique Amigó, Agustín D. Delgado |  |
| 907 |  |  [What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels](https://doi.org/10.18653/v1/2022.acl-long.400) |  | 0 |  | Carolina CuestaLázaro, Animesh Prasad, Trevor Wood |  |
| 908 |  |  [Measuring Fairness of Text Classifiers via Prediction Sensitivity](https://doi.org/10.18653/v1/2022.acl-long.401) |  | 0 |  | Satyapriya Krishna, Rahul Gupta, Apurv Verma, Jwala Dhamala, Yada Pruksachatkun, KaiWei Chang |  |
| 909 |  |  [RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.acl-long.402) |  | 0 |  | Kai Chen, Ye Wang, Yitong Li, Aiping Li |  |
| 910 |  |  [Feeding What You Need by Understanding What You Learned](https://doi.org/10.18653/v1/2022.acl-long.403) |  | 0 |  | Xiaoqiang Wang, Bang Liu, Fangli Xu, Bo Long, Siliang Tang, Lingfei Wu |  |
| 911 |  |  [Probing Simile Knowledge from Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.404) |  | 0 |  | Weijie Chen, Yongzhu Chang, Rongsheng Zhang, Jiashu Pu, Guandan Chen, Le Zhang, Yadong Xi, Yijiang Chen, Chang Su |  |
| 912 |  |  [An Effective and Efficient Entity Alignment Decoding Algorithm via Third-Order Tensor Isomorphism](https://doi.org/10.18653/v1/2022.acl-long.405) |  | 0 |  | Xin Mao, Meirong Ma, Hao Yuan, Jianchao Zhu, Zongyu Wang, Rui Xie, Wei Wu, Man Lan |  |
| 913 |  |  [Entailment Graph Learning with Textual Entailment and Soft Transitivity](https://doi.org/10.18653/v1/2022.acl-long.406) |  | 0 |  | Zhibin Chen, Yansong Feng, Dongyan Zhao |  |
| 914 |  |  [Logic Traps in Evaluating Attribution Scores](https://doi.org/10.18653/v1/2022.acl-long.407) |  | 0 |  | Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang, Kang Liu, Jun Zhao |  |
| 915 |  |  [Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network](https://doi.org/10.18653/v1/2022.acl-long.408) |  | 0 |  | Zheng Gong, Kun Zhou, Wayne Xin Zhao, Jing Sha, Shijin Wang, JiRong Wen |  |
| 916 |  |  [Multitasking Framework for Unsupervised Simple Definition Generation](https://doi.org/10.18653/v1/2022.acl-long.409) |  | 0 |  | Cunliang Kong, Yun Chen, Hengyuan Zhang, Liner Yang, Erhong Yang |  |
| 917 |  |  [Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.410) |  | 0 |  | Zhanming Jie, Jierui Li, Wei Lu |  |
| 918 |  |  [When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues](https://doi.org/10.18653/v1/2022.acl-long.411) |  | 0 |  | Shivani Kumar, Atharva Kulkarni, Md. Shad Akhtar, Tanmoy Chakraborty |  |
| 919 |  |  [Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning](https://doi.org/10.18653/v1/2022.acl-long.412) |  | 0 |  | Seonghyeon Lee, Dongha Lee, Seongbo Jang, Hwanjo Yu |  |
| 920 |  |  [Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge](https://doi.org/10.18653/v1/2022.acl-long.413) |  | 0 |  | Linhai Zhang, Xuemeng Hu, Boyu Wang, Deyu Zhou, QianWen Zhang, Yunbo Cao |  |
| 921 |  |  [Multi-View Document Representation Learning for Open-Domain Dense Retrieval](https://doi.org/10.18653/v1/2022.acl-long.414) |  | 0 |  | Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, Nan Duan |  |
| 922 |  |  [Graph Pre-training for AMR Parsing and Generation](https://doi.org/10.18653/v1/2022.acl-long.415) |  | 0 |  | Xuefeng Bai, Yulong Chen, Yue Zhang |  |
| 923 |  |  [Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills](https://doi.org/10.18653/v1/2022.acl-long.416) |  | 0 |  | Ori Yoran, Alon Talmor, Jonathan Berant |  |
| 924 |  |  [RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.acl-long.417) |  | 0 |  | Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, Caiming Xiong |  |
| 925 |  |  [Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling](https://doi.org/10.18653/v1/2022.acl-long.418) |  | 0 |  | Prathyusha Jwalapuram, Shafiq R. Joty, Xiang Lin |  |
| 926 |  |  [Just Rank: Rethinking Evaluation with Word and Sentence Similarities](https://doi.org/10.18653/v1/2022.acl-long.419) |  | 0 |  | Bin Wang, C.C. Jay Kuo, Haizhou Li |  |
| 927 |  |  [MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding](https://doi.org/10.18653/v1/2022.acl-long.420) |  | 0 |  | Junlong Li, Yiheng Xu, Lei Cui, Furu Wei |  |
| 928 |  |  [CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment](https://doi.org/10.18653/v1/2022.acl-long.421) |  | 0 |  | Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, Furu Wei |  |
| 929 |  |  [KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2022.acl-long.422) |  | 0 |  | Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, Hanwang Zhang |  |
| 930 |  |  [Debiased Contrastive Learning of Unsupervised Sentence Representations](https://doi.org/10.18653/v1/2022.acl-long.423) |  | 0 |  | Kun Zhou, Beichen Zhang, Wayne Xin Zhao, JiRong Wen |  |
| 931 |  |  [MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](https://doi.org/10.18653/v1/2022.acl-long.424) |  | 0 |  | Zhixing Tan, Xiangwen Zhang, Shuo Wang, Yang Liu |  |
| 932 |  |  [SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues](https://doi.org/10.18653/v1/2022.acl-long.425) |  | 0 |  | Ssu Chiu, Maolin Li, YenTing Lin, YunNung Chen |  |
| 933 |  |  [UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining](https://doi.org/10.18653/v1/2022.acl-long.426) |  | 0 |  | Jiacheng Li, Jingbo Shang, Julian J. McAuley |  |
| 934 |  |  [XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](https://doi.org/10.18653/v1/2022.acl-long.427) |  | 0 |  | Zewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song, XianLing Mao, Heyan Huang, Furu Wei |  |
| 935 |  |  [Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.428) |  | 0 |  | Chao Lou, Songlin Yang, Kewei Tu |  |
| 936 |  |  [Can Explanations Be Useful for Calibrating Black Box Models?](https://doi.org/10.18653/v1/2022.acl-long.429) |  | 0 |  | Xi Ye, Greg Durrett |  |
| 937 |  |  [OIE@OIA: an Adaptable and Efficient Open Information Extraction Framework](https://doi.org/10.18653/v1/2022.acl-long.430) |  | 0 |  | Xin Wang, Minlong Peng, Mingming Sun, Ping Li |  |
| 938 |  |  [ReACC: A Retrieval-Augmented Code Completion Framework](https://doi.org/10.18653/v1/2022.acl-long.431) |  | 0 |  | Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seungwon Hwang, Alexey Svyatkovskiy |  |
| 939 |  |  [Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED](https://doi.org/10.18653/v1/2022.acl-long.432) |  | 0 |  | Quzhe Huang, Shibo Hao, Yuan Ye, Shengqi Zhu, Yansong Feng, Dongyan Zhao |  |
| 940 |  |  [UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning](https://doi.org/10.18653/v1/2022.acl-long.433) |  | 0 |  | Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, Madian Khabsa |  |
| 941 |  |  [An Empirical Study of Memorization in NLP](https://doi.org/10.18653/v1/2022.acl-long.434) |  | 0 |  | Xiaosen Zheng, Jing Jiang |  |
| 942 |  |  [AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages](https://doi.org/10.18653/v1/2022.acl-long.435) |  | 0 |  | Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John E. Ortega, Ricardo Ramos, Annette Rios, Iván Vladimir Meza Ruíz, Gustavo Giménez Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer, Rolando CotoSolano, Ngoc Thang Vu, Katharina Kann |  |
| 943 |  |  [Towards Learning (Dis)-Similarity of Source Code from Program Contrasts](https://doi.org/10.18653/v1/2022.acl-long.436) |  | 0 |  | Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray, Saikat Chakraborty |  |
| 944 |  |  [Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News](https://doi.org/10.18653/v1/2022.acl-long.437) |  | 0 |  | Gary Ang, EePeng Lim |  |
| 945 |  |  [On Vision Features in Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.438) |  | 0 |  | Bei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma, Jingbo Zhu |  |
| 946 |  |  [CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.439) |  | 0 |  | Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca J. Passonneau, Rui Zhang |  |
| 947 |  |  [Cree Corpus: A Collection of nêhiyawêwin Resources](https://doi.org/10.18653/v1/2022.acl-long.440) |  | 0 |  | Daniela Teodorescu, Josie Matalski, Delaney Lothian, Denilson Barbosa, Carrie Demmans Epp |  |
| 948 |  |  [Learning to Rank Visual Stories From Human Ranking Data](https://doi.org/10.18653/v1/2022.acl-long.441) |  | 0 |  | ChiYang Hsu, YunWei Chu, Vincent Chen, KuanChieh Lo, Chacha Chen, TingHao (Kenneth) Huang, LunWei Ku |  |
| 949 |  |  [Universal Conditional Masked Language Pre-training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.442) |  | 0 |  | Pengfei Li, Liangyou Li, Meng Zhang, Minghao Wu, Qun Liu |  |
| 950 |  |  [CARETS: A Consistency And Robustness Evaluative Test Suite for VQA](https://doi.org/10.18653/v1/2022.acl-long.443) |  | 0 |  | Carlos E. Jimenez, Olga Russakovsky, Karthik Narasimhan |  |
| 951 |  |  [Phrase-aware Unsupervised Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.444) |  | 0 |  | Xiaotao Gu, Yikang Shen, Jiaming Shen, Jingbo Shang, Jiawei Han |  |
| 952 |  |  [Achieving Reliable Human Assessment of Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.445) |  | 0 |  | Tianbo Ji, Yvette Graham, Gareth J. F. Jones, Chenyang Lyu, Qun Liu |  |
| 953 |  |  [Updated Headline Generation: Creating Updated Summaries for Evolving News Stories](https://doi.org/10.18653/v1/2022.acl-long.446) |  | 0 |  | Sheena Panthaplackel, Adrian Benton, Mark Dredze |  |
| 954 |  |  [SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures](https://doi.org/10.18653/v1/2022.acl-long.447) |  | 0 |  | Megan Ung, Jing Xu, YLan Boureau |  |
| 955 |  |  [Compositional Generalization in Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.448) |  | 0 |  | Emily Goodwin, Siva Reddy, Timothy J. O'Donnell, Dzmitry Bahdanau |  |
| 956 |  |  [ASPECTNEWS: Aspect-Oriented Summarization of News Documents](https://doi.org/10.18653/v1/2022.acl-long.449) |  | 0 |  | Ojas Ahuja, Jiacheng Xu, Akshay Gupta, Kevin Horecka, Greg Durrett |  |
| 957 |  |  [MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes](https://doi.org/10.18653/v1/2022.acl-long.450) |  | 0 |  | Nianlong Gu, Elliott Ash, Richard H. R. Hahnloser |  |
| 958 |  |  [CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations](https://doi.org/10.18653/v1/2022.acl-long.451) |  | 0 |  | Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 959 |  |  [Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.452) |  | 0 |  | Freda Shi, Kevin Gimpel, Karen Livescu |  |
| 960 |  |  [Multilingual Detection of Personal Employment Status on Twitter](https://doi.org/10.18653/v1/2022.acl-long.453) |  | 0 |  | Manuel Tonneau, Dhaval Adjodah, João Palotti, Nir Grinberg, Samuel Fraiberger |  |
| 961 |  |  [MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data](https://doi.org/10.18653/v1/2022.acl-long.454) |  | 0 |  | Yilun Zhao, Yunxiang Li, Chenying Li, Rui Zhang |  |
| 962 |  |  [Transformers in the loop: Polarity in neural models of language](https://doi.org/10.18653/v1/2022.acl-long.455) |  | 0 |  | Lisa Bylinina, Alexey Tikhonov |  |
| 963 |  |  [Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.456) |  | 0 |  | Zhiwei He, Xing Wang, Rui Wang, Shuming Shi, Zhaopeng Tu |  |
| 964 |  |  [SDR: Efficient Neural Re-ranking using Succinct Document Representation](https://doi.org/10.18653/v1/2022.acl-long.457) |  | 0 |  | Nachshon Cohen, Amit Portnoy, Besnik Fetahu, Amir Ingber |  |
| 965 |  |  [The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications](https://doi.org/10.18653/v1/2022.acl-long.458) |  | 0 |  | Mina Valizadeh, Natalie Parde |  |
| 966 |  |  [SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher](https://doi.org/10.18653/v1/2022.acl-long.459) |  | 0 |  | Thai Le, Noseong Park, Dongwon Lee |  |
| 967 |  |  [Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding](https://doi.org/10.18653/v1/2022.acl-long.460) |  | 0 |  | Soumya Chatterjee, Sunita Sarawagi, Preethi Jyothi |  |
| 968 |  |  [Leveraging Task Transferability to Meta-learning for Clinical Section Classification with Limited Data](https://doi.org/10.18653/v1/2022.acl-long.461) |  | 0 |  | Zhuohao Chen, Jangwon Kim, Ram Bhakta, Mustafa Y. Sir |  |
| 969 |  |  [Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection](https://doi.org/10.18653/v1/2022.acl-long.462) |  | 0 |  | Rajkumar Pujari, Erik Oveson, Priyanka Kulkarni, Elnaz Nouri |  |
| 970 |  |  [Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings](https://doi.org/10.18653/v1/2022.acl-long.463) |  | 0 |  | Sidsel Boldsen, Patrizia Paggio |  |
| 971 |  |  [A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation](https://doi.org/10.18653/v1/2022.acl-long.464) |  | 0 |  | Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, Bill Dolan |  |
| 972 |  |  [Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice](https://doi.org/10.18653/v1/2022.acl-long.465) |  | 0 |  | Andreas Grivas, Nikolay Bogoychev, Adam Lopez |  |
| 973 |  |  [Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.466) |  | 0 |  | Yubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, Jing Shao |  |
| 974 |  |  [Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework](https://doi.org/10.18653/v1/2022.acl-long.467) |  | 0 |  | Shaolei Zhang, Yang Feng |  |
| 975 |  |  [A Statutory Article Retrieval Dataset in French](https://doi.org/10.18653/v1/2022.acl-long.468) |  | 0 |  | Antoine Louis, Gerasimos Spanakis |  |
| 976 |  |  [ParaDetox: Detoxification with Parallel Data](https://doi.org/10.18653/v1/2022.acl-long.469) |  | 0 |  | Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, Alexander Panchenko |  |
| 977 |  |  [Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color](https://doi.org/10.18653/v1/2022.acl-long.470) |  | 0 |  | Sidsel Boldsen, Manex Agirrezabal, Nora Hollenstein |  |
| 978 |  |  [Fine-Grained Controllable Text Generation Using Non-Residual Prompting](https://doi.org/10.18653/v1/2022.acl-long.471) |  | 0 |  | Fredrik Carlsson, Joey Öhman, Fangyu Liu, Severine Verlinden, Joakim Nivre, Magnus Sahlgren |  |
| 979 |  |  [Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features](https://doi.org/10.18653/v1/2022.acl-long.472) |  | 0 |  | Florian Lux, Ngoc Thang Vu |  |
| 980 |  |  [TwittIrish: A Universal Dependencies Treebank of Tweets in Modern Irish](https://doi.org/10.18653/v1/2022.acl-long.473) |  | 0 |  | Lauren Cassidy, Teresa Lynn, James Barry, Jennifer Foster |  |
| 981 |  |  [Length Control in Abstractive Summarization by Pretraining Information Selection](https://doi.org/10.18653/v1/2022.acl-long.474) |  | 0 |  | Yizhu Liu, Qi Jia, Kenny Q. Zhu |  |
| 982 |  |  [CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation](https://doi.org/10.18653/v1/2022.acl-long.475) |  | 0 |  | Zichu Fei, Qi Zhang, Tao Gui, Di Liang, Sirui Wang, Wei Wu, Xuanjing Huang |  |
| 983 |  |  [Word Order Does Matter and Shuffled Language Models Know It](https://doi.org/10.18653/v1/2022.acl-long.476) |  | 0 |  | Mostafa Abdou, Vinit Ravishankar, Artur Kulmizev, Anders Søgaard |  |
| 984 |  |  [An Empirical Study on Explanations in Out-of-Domain Settings](https://doi.org/10.18653/v1/2022.acl-long.477) |  | 0 |  | George Chrysostomou, Nikolaos Aletras |  |
| 985 |  |  [MILIE: Modular & Iterative Multilingual Open Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.478) |  | 0 |  | Bhushan Kotnis, Kiril Gashteovski, Daniel OñoroRubio, Ammar Shaker, Vanesa RodriguezTembras, Makoto Takamoto, Mathias Niepert, Carolin Lawrence |  |
| 986 |  |  [What Makes Reading Comprehension Questions Difficult?](https://doi.org/10.18653/v1/2022.acl-long.479) |  | 0 |  | Saku Sugawara, Nikita Nangia, Alex Warstadt, Samuel R. Bowman |  |
| 987 |  |  [From Simultaneous to Streaming Machine Translation by Leveraging Streaming History](https://doi.org/10.18653/v1/2022.acl-long.480) |  | 0 |  | Javier IranzoSánchez, Jorge Civera, Alfons JuanCíscar |  |
| 988 |  |  [A Rationale-Centric Framework for Human-in-the-loop Machine Learning](https://doi.org/10.18653/v1/2022.acl-long.481) |  | 0 |  | Jinghui Lu, Linyi Yang, Brian MacNamee, Yue Zhang |  |
| 989 |  |  [Challenges and Strategies in Cross-Cultural NLP](https://doi.org/10.18653/v1/2022.acl-long.482) |  | 0 |  | Daniel Hershcovich, Stella Frank, Heather C. Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, Anders Søgaard |  |
| 990 |  |  [Prototypical Verbalizer for Prompt-based Few-shot Tuning](https://doi.org/10.18653/v1/2022.acl-long.483) |  | 0 |  | Ganqu Cui, Shengding Hu, Ning Ding, Longtao Huang, Zhiyuan Liu |  |
| 991 |  |  [Clickbait Spoiling via Question Answering and Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.484) |  | 0 |  | Matthias Hagen, Maik Fröbe, Artur Jurk, Martin Potthast |  |
| 992 |  |  [BERT Learns to Teach: Knowledge Distillation with Meta Learning](https://doi.org/10.18653/v1/2022.acl-long.485) |  | 0 |  | Wangchunshu Zhou, Canwen Xu, Julian J. McAuley |  |
| 993 |  |  [STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation](https://doi.org/10.18653/v1/2022.acl-long.486) |  | 0 |  | Qingkai Fang, Rong Ye, Lei Li, Yang Feng, Mingxuan Wang |  |
| 994 |  |  [Integrating Vectorized Lexical Constraints for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.487) |  | 0 |  | Shuo Wang, Zhixing Tan, Yang Liu |  |
| 995 |  |  [MPII: Multi-Level Mutual Promotion for Inference and Interpretation](https://doi.org/10.18653/v1/2022.acl-long.488) |  | 0 |  | Yan Liu, Sanyuan Chen, Yazheng Yang, Qi Dai |  |
| 996 |  |  [StableMoE: Stable Routing Strategy for Mixture of Experts](https://doi.org/10.18653/v1/2022.acl-long.489) |  | 0 |  | Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, Furu Wei |  |
| 997 |  |  [Boundary Smoothing for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.490) |  | 0 |  | Enwei Zhu, Jinpeng Li |  |
| 998 |  |  [Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.acl-long.491) |  | 0 |  | Zihan Wang, Peiyi Wang, Lianzhe Huang, Xin Sun, Houfeng Wang |  |
| 999 |  |  [Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models](https://doi.org/10.18653/v1/2022.acl-long.492) |  | 0 |  | Mark Chu, Bhargav Srinivasa Desikan, Ethan O. Nadler, Donald Ruggiero Lo Sardo, Elise DarraghFord, Douglas Guilbeault |  |
| 1000 |  |  [Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering](https://doi.org/10.18653/v1/2022.acl-long.493) |  | 0 |  | Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, Lei Chen |  |
| 1001 |  |  [AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.494) |  | 0 |  | Xiao Li, Gong Cheng, Ziheng Chen, Yawei Sun, Yuzhong Qu |  |
| 1002 |  |  [CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing](https://doi.org/10.18653/v1/2022.acl-long.495) |  | 0 |  | Chen Liang, Pengcheng He, Yelong Shen, Weizhu Chen, Tuo Zhao |  |
| 1003 |  |  [Interpretability for Language Learners Using Example-Based Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-long.496) |  | 0 |  | Masahiro Kaneko, Sho Takase, Ayana Niwa, Naoaki Okazaki |  |
| 1004 |  |  [Rethinking Negative Sampling for Handling Missing Entity Annotations](https://doi.org/10.18653/v1/2022.acl-long.497) |  | 0 |  | Yangming Li, Lemao Liu, Shuming Shi |  |
| 1005 |  |  [Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning](https://doi.org/10.18653/v1/2022.acl-long.498) |  | 0 |  | Kang Zhou, Yuepei Li, Qi Li |  |
| 1006 |  |  [UniXcoder: Unified Cross-Modal Pre-training for Code Representation](https://doi.org/10.18653/v1/2022.acl-long.499) |  | 0 |  | Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, Jian Yin |  |
| 1007 |  |  [One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia](https://doi.org/10.18653/v1/2022.acl-long.500) |  | 0 |  | Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Timothy Baldwin, Jey Han Lau, Sebastian Ruder |  |
| 1008 |  |  [Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text](https://doi.org/10.18653/v1/2022.acl-long.501) |  | 0 |  | Yao Dou, Maxwell Forbes, Rik KoncelKedziorski, Noah A. Smith, Yejin Choi |  |
| 1009 |  |  [Transkimmer: Transformer Learns to Layer-wise Skim](https://doi.org/10.18653/v1/2022.acl-long.502) |  | 0 |  | Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, Minyi Guo |  |
| 1010 |  |  [SkipBERT: Efficient Inference with Shallow Layer Skipping](https://doi.org/10.18653/v1/2022.acl-long.503) |  | 0 |  | Jue Wang, Ke Chen, Gang Chen, Lidan Shou, Julian J. McAuley |  |
| 1011 |  |  [Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models](https://doi.org/10.18653/v1/2022.acl-long.504) |  | 0 |  | Ryokan Ri, Yoshimasa Tsuruoka |  |
| 1012 |  |  [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.505) |  | 0 |  | Ryokan Ri, Ikuya Yamada, Yoshimasa Tsuruoka |  |
| 1013 |  |  [Evaluating Factuality in Text Simplification](https://doi.org/10.18653/v1/2022.acl-long.506) |  | 0 |  | Ashwin Devaraj, William Sheffield, Byron C. Wallace, Junyi Jessy Li |  |
| 1014 |  |  [Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization](https://doi.org/10.18653/v1/2022.acl-long.507) |  | 0 |  | Aidan Pine, Dan Wells, Nathan Thanyehténhas Brinklow, Patrick Littell, Korin Richmond |  |
| 1015 |  |  [Sharpness-Aware Minimization Improves Language Model Generalization](https://doi.org/10.18653/v1/2022.acl-long.508) |  | 0 |  | Dara Bahri, Hossein Mobahi, Yi Tay |  |
| 1016 |  |  [Adversarial Authorship Attribution for Deobfuscation](https://doi.org/10.18653/v1/2022.acl-long.509) |  | 0 |  | Wanyue Zhai, Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan |  |
| 1017 |  |  [Weakly Supervised Word Segmentation for Computational Language Documentation](https://doi.org/10.18653/v1/2022.acl-long.510) |  | 0 |  | Shu Okabe, Laurent Besacier, François Yvon |  |
| 1018 |  |  [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://doi.org/10.18653/v1/2022.acl-long.511) |  | 0 |  | Mobashir Sadat, Cornelia Caragea |  |
| 1019 |  |  [Neural reality of argument structure constructions](https://doi.org/10.18653/v1/2022.acl-long.512) |  | 0 |  | Bai Li, Zining Zhu, Guillaume Thomas, Frank Rudzicz, Yang Xu |  |
| 1020 |  |  [On the Robustness of Offensive Language Classifiers](https://doi.org/10.18653/v1/2022.acl-long.513) |  | 0 |  | Jonathan Rusert, Zubair Shafiq, Padmini Srinivasan |  |
| 1021 |  |  [Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings](https://doi.org/10.18653/v1/2022.acl-long.514) |  | 0 |  | Kalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, Partha Talukdar |  |
| 1022 |  |  [ABC: Attention with Bounded-memory Control](https://doi.org/10.18653/v1/2022.acl-long.515) |  | 0 |  | Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, Noah A. Smith |  |
| 1023 |  |  [The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail](https://doi.org/10.18653/v1/2022.acl-long.516) |  | 0 |  | Samuel R. Bowman |  |
| 1024 |  |  [RELiC: Retrieving Evidence for Literary Claims](https://doi.org/10.18653/v1/2022.acl-long.517) |  | 0 |  | Katherine Thai, Yapei Chang, Kalpesh Krishna, Mohit Iyyer |  |
| 1025 |  |  [Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas](https://doi.org/10.18653/v1/2022.acl-long.518) |  | 0 |  | Raphael Schumann, Stefan Riezler |  |
| 1026 |  |  [Adapting Coreference Resolution Models through Active Learning](https://doi.org/10.18653/v1/2022.acl-long.519) |  | 0 |  | Michelle Yuan, Patrick Xia, Chandler May, Benjamin Van Durme, Jordan L. BoydGraber |  |
| 1027 |  |  [An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models](https://doi.org/10.18653/v1/2022.acl-long.520) |  | 0 |  | Sweta Agrawal, Marine Carpuat |  |
| 1028 |  |  [Memorisation versus Generalisation in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.521) |  | 0 |  | Michael Tänzer, Sebastian Ruder, Marek Rei |  |
| 1029 |  |  [ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments](https://doi.org/10.18653/v1/2022.acl-long.522) |  | 0 |  | Ruolan Yang, Zitong Li, Haifeng Tang, Kenny Q. Zhu |  |
| 1030 |  |  [Do self-supervised speech models develop human-like perception biases?](https://doi.org/10.18653/v1/2022.acl-long.523) |  | 0 |  | Juliette Millet, Ewan Dunbar |  |
| 1031 |  |  [Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions](https://doi.org/10.18653/v1/2022.acl-long.524) |  | 0 |  | Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, Xin Wang |  |
| 1032 |  |  [Learning to Generate Programs for Table Fact Verification via Structure-Aware Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.525) |  | 0 |  | Suixin Ou, Yongmei Liu |  |
| 1033 |  |  [Cluster & Tune: Boost Cold Start Performance in Text Classification](https://doi.org/10.18653/v1/2022.acl-long.526) |  | 0 |  | Eyal Shnarch, Ariel Gera, Alon Halfon, Lena Dankin, Leshem Choshen, Ranit Aharonov, Noam Slonim |  |
| 1034 |  |  [Overcoming a Theoretical Limitation of Self-Attention](https://doi.org/10.18653/v1/2022.acl-long.527) |  | 0 |  | David Chiang, Peter Cholak |  |
| 1035 |  |  [Prediction Difference Regularization against Perturbation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.528) |  | 0 |  | Dengji Guo, Zhengrui Ma, Min Zhang, Yang Feng |  |
| 1036 |  |  [Make the Best of Cross-lingual Transfer: Evidence from POS Tagging with over 100 Languages](https://doi.org/10.18653/v1/2022.acl-long.529) |  | 0 |  | Wietse de Vries, Martijn Wieling, Malvina Nissim |  |
| 1037 |  |  [Should a Chatbot be Sarcastic? Understanding User Preferences Towards Sarcasm Generation](https://doi.org/10.18653/v1/2022.acl-long.530) |  | 0 |  | Silviu Vlad Oprea, Steven R. Wilson, Walid Magdy |  |
| 1038 |  |  [How Do Seq2Seq Models Perform on End-to-End Data-to-Text Generation?](https://doi.org/10.18653/v1/2022.acl-long.531) |  | 0 |  | Xunjian Yin, Xiaojun Wan |  |
| 1039 |  |  [Probing for Labeled Dependency Trees](https://doi.org/10.18653/v1/2022.acl-long.532) |  | 0 |  | Max MüllerEberstein, Rob van der Goot, Barbara Plank |  |
| 1040 |  |  [DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation](https://doi.org/10.18653/v1/2022.acl-long.533) |  | 0 |  | Nitay Calderon, Eyal BenDavid, Amir Feder, Roi Reichart |  |
| 1041 |  |  [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://doi.org/10.18653/v1/2022.acl-long.534) |  | 0 |  | Jiapeng Wang, Lianwen Jin, Kai Ding |  |
| 1042 |  |  [Dependency-based Mixture Language Models](https://doi.org/10.18653/v1/2022.acl-long.535) |  | 0 |  | Zhixian Yang, Xiaojun Wan |  |
| 1043 |  |  [Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?](https://doi.org/10.18653/v1/2022.acl-long.536) |  | 0 |  | Subhabrata Dutta, Jeevesh Juneja, Dipankar Das, Tanmoy Chakraborty |  |
| 1044 |  |  [Entity-based Neural Local Coherence Modeling](https://doi.org/10.18653/v1/2022.acl-long.537) |  | 0 |  | Sungho Jeon, Michael Strube |  |
| 1045 |  |  ["That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks](https://doi.org/10.18653/v1/2022.acl-long.538) |  | 0 |  | Edoardo Mosca, Shreyash Agarwal, Javier RandoRamirez, Georg Groh |  |
| 1046 |  |  [Local Languages, Third Spaces, and other High-Resource Scenarios](https://doi.org/10.18653/v1/2022.acl-long.539) |  | 0 |  | Steven Bird |  |
| 1047 |  |  [That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory](https://doi.org/10.18653/v1/2022.acl-long.540) |  | 0 |  | Xuemei Tang, Qi Su |  |
| 1048 |  |  [Fair and Argumentative Language Modeling for Computational Argumentation](https://doi.org/10.18653/v1/2022.acl-long.541) |  | 0 |  | Carolin Holtermann, Anne Lauscher, Simone Paolo Ponzetto |  |
| 1049 |  |  [Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation](https://doi.org/10.18653/v1/2022.acl-long.542) |  | 0 |  | Ruiqing Zhang, Zhongjun He, Hua Wu, Haifeng Wang |  |
| 1050 |  |  [Can Pre-trained Language Models Interpret Similes as Smart as Human?](https://doi.org/10.18653/v1/2022.acl-long.543) |  | 0 |  | Qianyu He, Sijie Cheng, Zhixu Li, Rui Xie, Yanghua Xiao |  |
| 1051 |  |  [CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2022.acl-long.544) |  | 0 |  | Ningyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan Liang, Lei Li, Xin Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guotong Xie, Zhifang Sui, Baobao Chang, Hui Zong, Zheng Yuan, Linfeng Li, Jun Yan, Hongying Zan, Kunli Zhang, Buzhou Tang, Qingcai Chen |  |
| 1052 |  |  [Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization](https://doi.org/10.18653/v1/2022.acl-long.545) |  | 0 |  | Puyuan Liu, Chenyang Huang, Lili Mou |  |
| 1053 |  |  [Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.546) |  | 0 |  | Xiangpeng Wei, Heng Yu, Yue Hu, Rongxiang Weng, Weihua Luo, Rong Jin |  |
| 1054 |  |  [Lexical Knowledge Internalization for Neural Dialog Generation](https://doi.org/10.18653/v1/2022.acl-long.547) |  | 0 |  | Zhiyong Wu, Wei Bi, Xiang Li, Lingpeng Kong, Ben Kao |  |
| 1055 |  |  [Modeling Syntactic-Semantic Dependency Correlations in Semantic Role Labeling Using Mixture Models](https://doi.org/10.18653/v1/2022.acl-long.548) |  | 0 |  | Junjie Chen, Xiangheng He, Yusuke Miyao |  |
| 1056 |  |  [Learning the Beauty in Songs: Neural Singing Voice Beautifier](https://doi.org/10.18653/v1/2022.acl-long.549) |  | 0 |  | Jinglin Liu, Chengxi Li, Yi Ren, Zhiying Zhu, Zhou Zhao |  |
| 1057 |  |  [A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.550) |  | 0 |  | Yu Cao, Wei Bi, Meng Fang, Shuming Shi, Dacheng Tao |  |
| 1058 |  |  [LinkBERT: Pretraining Language Models with Document Links](https://doi.org/10.18653/v1/2022.acl-long.551) |  | 0 |  | Michihiro Yasunaga, Jure Leskovec, Percy Liang |  |
| 1059 |  |  [Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.acl-long.552) |  | 0 |  | Chao Shang, Guangtao Wang, Peng Qi, Jing Huang |  |
| 1060 |  |  [Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition](https://doi.org/10.18653/v1/2022.acl-long.553) |  | 0 |  | Liming Wang, Siyuan Feng, Mark HasegawaJohnson, Chang Dong Yoo |  |
| 1061 |  |  [Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions](https://doi.org/10.18653/v1/2022.acl-long.554) |  | 0 |  | HawShiuan Chang, Andrew McCallum |  |
| 1062 |  |  [Ditch the Gold Standard: Re-evaluating Conversational Question Answering](https://doi.org/10.18653/v1/2022.acl-long.555) |  | 0 |  | Huihan Li, Tianyu Gao, Manan Goenka, Danqi Chen |  |
| 1063 |  |  [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://doi.org/10.18653/v1/2022.acl-long.556) |  | 0 |  | Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp |  |
| 1064 |  |  [Situated Dialogue Learning through Procedural Environment Generation](https://doi.org/10.18653/v1/2022.acl-long.557) |  | 0 |  | Prithviraj Ammanabrolu, Renee Jia, Mark O. Riedl |  |
| 1065 |  |  [UniTE: Unified Translation Evaluation](https://doi.org/10.18653/v1/2022.acl-long.558) |  | 0 |  | Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek F. Wong, Lidia S. Chao |  |
| 1066 |  |  [Program Transfer for Answering Complex Questions over Knowledge Bases](https://doi.org/10.18653/v1/2022.acl-long.559) |  | 0 |  | Shulin Cao, Jiaxin Shi, Zijun Yao, Xin Lv, Jifan Yu, Lei Hou, Juanzi Li, Zhiyuan Liu, Jinghui Xiao |  |
| 1067 |  |  [EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.560) |  | 0 |  | Yulin Xu, Zhen Yang, Fandong Meng, Jie Zhou |  |
| 1068 |  |  [Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings](https://doi.org/10.18653/v1/2022.acl-long.561) |  | 0 |  | Jiangbin Zheng, Yile Wang, Ge Wang, Jun Xia, Yufei Huang, Guojiang Zhao, Yue Zhang, Stan Z. Li |  |
| 1069 |  |  [Multimodal Sarcasm Target Identification in Tweets](https://doi.org/10.18653/v1/2022.acl-long.562) |  | 0 |  | Jiquan Wang, Lin Sun, Yi Liu, Meizhi Shao, Zengwei Zheng |  |
| 1070 |  |  [Flexible Generation from Fragmentary Linguistic Input](https://doi.org/10.18653/v1/2022.acl-long.563) |  | 0 |  | Peng Qian, Roger Levy |  |
| 1071 |  |  [Revisiting Over-Smoothness in Text to Speech](https://doi.org/10.18653/v1/2022.acl-long.564) |  | 0 |  | Yi Ren, Xu Tan, Tao Qin, Zhou Zhao, TieYan Liu |  |
| 1072 |  |  [Coherence boosting: When your pretrained language model is not paying enough attention](https://doi.org/10.18653/v1/2022.acl-long.565) |  | 0 |  | Nikolay Malkin, Zhen Wang, Nebojsa Jojic |  |
| 1073 |  |  [Uncertainty Estimation of Transformer Predictions for Misclassification Detection](https://doi.org/10.18653/v1/2022.acl-long.566) |  | 0 |  | Artem Vazhentsev, Gleb Kuzmin, Artem Shelmanov, Akim Tsvigun, Evgenii Tsymbalov, Kirill Fedyanin, Maxim Panov, Alexander Panchenko, Gleb Gusev, Mikhail Burtsev, Manvel Avetisian, Leonid Zhukov |  |
| 1074 |  |  [VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena](https://doi.org/10.18653/v1/2022.acl-long.567) |  | 0 |  | Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, Albert Gatt |  |
| 1075 |  |  [The Grammar-Learning Trajectories of Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.568) |  | 0 |  | Leshem Choshen, Guy Hacohen, Daphna Weinshall, Omri Abend |  |
| 1076 |  |  [Generating Scientific Definitions with Controllable Complexity](https://doi.org/10.18653/v1/2022.acl-long.569) |  | 0 |  | Tal August, Katharina Reinecke, Noah A. Smith |  |
| 1077 |  |  [Label Semantic Aware Pre-training for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.570) |  | 0 |  | Aaron Mueller, Jason Krone, Salvatore Romeo, Saab Mansour, Elman Mansimov, Yi Zhang, Dan Roth |  |
| 1078 |  |  [ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation](https://doi.org/10.18653/v1/2022.acl-long.571) |  | 0 |  | Bei Li, Quan Du, Tao Zhou, Yi Jing, Shuhan Zhou, Xin Zeng, Tong Xiao, Jingbo Zhu, Xuebo Liu, Min Zhang |  |
| 1079 |  |  [A Comparison of Strategies for Source-Free Domain Adaptation](https://doi.org/10.18653/v1/2022.acl-long.572) |  | 0 |  | Xin Su, Yiyun Zhao, Steven Bethard |  |
| 1080 |  |  [Ethics Sheets for AI Tasks](https://doi.org/10.18653/v1/2022.acl-long.573) |  | 0 |  | Saif M. Mohammad |  |
| 1081 |  |  [Learning Disentangled Representations of Negation and Uncertainty](https://doi.org/10.18653/v1/2022.acl-long.574) |  | 0 |  | Jake Vasilakes, Chrysoula Zerva, Makoto Miwa, Sophia Ananiadou |  |
| 1082 |  |  [latent-GLAT: Glancing at Latent Variables for Parallel Text Generation](https://doi.org/10.18653/v1/2022.acl-long.575) |  | 0 |  | Yu Bao, Hao Zhou, Shujian Huang, Dongqi Wang, Lihua Qian, Xinyu Dai, Jiajun Chen, Lei Li |  |
| 1083 |  |  [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://doi.org/10.18653/v1/2022.acl-long.576) |  | 0 |  | Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang |  |
| 1084 |  |  [Deduplicating Training Data Makes Language Models Better](https://doi.org/10.18653/v1/2022.acl-long.577) |  | 0 |  | Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, Nicholas Carlini |  |
| 1085 |  |  [Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires](https://doi.org/10.18653/v1/2022.acl-long.578) |  | 0 |  | Thong Nguyen, Andrew Yates, Ayah Zirikly, Bart Desmet, Arman Cohan |  |
| 1086 |  |  [Internet-Augmented Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.579) |  | 0 |  | Mojtaba Komeili, Kurt Shuster, Jason Weston |  |
| 1087 |  |  [SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities](https://doi.org/10.18653/v1/2022.acl-long.580) |  | 0 |  | HsiangSheng Tsai, HengJui Chang, WenChin Huang, Zili Huang, Kushal Lakhotia, ShuWen Yang, Shuyan Dong, Andy T. Liu, ChengI Lai, Jiatong Shi, Xuankai Chang, Phil Hall, HsuanJui Chen, ShangWen Li, Shinji Watanabe, Abdelrahman Mohamed, Hungyi Lee |  |
| 1088 |  |  [Knowledge Neurons in Pretrained Transformers](https://doi.org/10.18653/v1/2022.acl-long.581) |  | 0 |  | Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei |  |
| 1089 |  |  [Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.582) |  | 0 |  | Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan Cardenas Guevara, Helen Yannakoudakis, Ekaterina Shutova |  |
| 1090 |  |  [French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English](https://doi.org/10.18653/v1/2022.acl-long.583) |  | 0 |  | Aurélie Névéol, Yoann Dupont, Julien Bezançon, Karën Fort |  |
| 1091 |  |  [Few-Shot Learning with Siamese Networks and Label Tuning](https://doi.org/10.18653/v1/2022.acl-long.584) |  | 0 |  | Thomas Müller, Guillermo PérezTorró, Marc FrancoSalvador |  |
| 1092 |  |  [Inferring Rewards from Language in Context](https://doi.org/10.18653/v1/2022.acl-long.585) |  | 0 |  | Jessy Lin, Daniel Fried, Dan Klein, Anca D. Dragan |  |
| 1093 |  |  [Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies](https://doi.org/10.18653/v1/2022.acl-long.586) |  | 0 |  | Angela Fan, Claire Gardent |  |
| 1094 |  |  [Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset](https://doi.org/10.18653/v1/2022.acl-long.587) |  | 0 |  | Anna Filighera, Siddharth Parihar, Tim Steuer, Tobias Meuser, Sebastian Ochs |  |
| 1095 |  |  [Towards Better Characterization of Paraphrases](https://doi.org/10.18653/v1/2022.acl-long.588) |  | 0 |  | Timothy Liu, De Wen Soh |  |
| 1096 |  |  [SummScreen: A Dataset for Abstractive Screenplay Summarization](https://doi.org/10.18653/v1/2022.acl-long.589) |  | 0 |  | Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel |  |
| 1097 |  |  [Sparsifying Transformer Models with Trainable Representation Pooling](https://doi.org/10.18653/v1/2022.acl-long.590) |  | 0 |  | Michal Pietruszka, Lukasz Borchmann, Lukasz Garncarek |  |
| 1098 |  |  [Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2022.acl-long.591) |  | 0 |  | Felix Stahlberg, Ilia Kulikov, Shankar Kumar |  |
| 1099 |  |  [FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning](https://doi.org/10.18653/v1/2022.acl-long.592) |  | 0 |  | Jing Zhou, Yanan Zheng, Jie Tang, Li Jian, Zhilin Yang |  |
| 1100 |  |  [Text-Free Prosody-Aware Generative Spoken Language Modeling](https://doi.org/10.18653/v1/2022.acl-long.593) |  | 0 |  | Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Rivière, Abdelrahman Mohamed, Emmanuel Dupoux, WeiNing Hsu |  |
| 1101 |  |  [Lite Unified Modeling for Discriminative Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.594) |  | 0 |  | Yilin Zhao, Hai Zhao, Libin Shen, Yinggong Zhao |  |
| 1102 |  |  [Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](https://doi.org/10.18653/v1/2022.acl-long.595) |  | 0 |  | Chihchan Tien, Shane SteinertThrelkeld |  |
| 1103 |  |  [End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding](https://doi.org/10.18653/v1/2022.acl-long.596) |  | 0 |  | Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang, Wenming Tan, Jin Wang, Peng Wang, Shiliang Pu, Fei Wu |  |
| 1104 |  |  [RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization](https://doi.org/10.18653/v1/2022.acl-long.597) |  | 0 |  | Hisashi Kamezawa, Noriki Nishida, Nobuyuki Shimizu, Takashi Miyazaki, Hideki Nakayama |  |
| 1105 |  |  [Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge](https://doi.org/10.18653/v1/2022.acl-long.598) |  | 0 |  | Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Claire Cardie |  |
| 1106 |  |  [Modeling Persuasive Discourse to Adaptively Support Students' Argumentative Writing](https://doi.org/10.18653/v1/2022.acl-long.599) |  | 0 |  | Thiemo Wambsganss, Christina Niklaus |  |
| 1107 |  |  [Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons](https://doi.org/10.18653/v1/2022.acl-long.600) |  | 0 |  | Akash Kumar Mohankumar, Mitesh M. Khapra |  |
| 1108 |  |  [The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments](https://doi.org/10.18653/v1/2022.acl-long.601) |  | 0 |  | Milad Alshomary, Roxanne El Baff, Timon Gurcke, Henning Wachsmuth |  |
| 1109 |  |  [Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection](https://doi.org/10.18653/v1/2022.acl-long.602) |  | 0 |  | Xin Huang, Ashish Khetan, Rene Bidart, Zohar Karnin |  |
| 1110 |  |  [Probing for the Usage of Grammatical Number](https://doi.org/10.18653/v1/2022.acl-long.603) |  | 0 |  | Karim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, Ryan Cotterell |  |
