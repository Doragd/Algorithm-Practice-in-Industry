# ACL2022

## 会议论文列表

本会议共有 1110 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [A Gentle Introduction to Deep Nets and Opportunities for the Future](https://doi.org/10.18653/v1/2022.acl-tutorials.1) |  | 0 | The first half of this tutorial will make deep nets more accessible to a broader audience, following “Deep Nets for Poets” and “A Gentle Introduction to Fine-Tuning.” We will also introduce GFT (general fine tuning), a little language for fine tuning deep nets with short (one line) programs that are as easy to code as regression in statistics packages such as R... | Ernest Davis, Gary Marcus, Kenneth Church, Valia Kordoni, Yanjun Ma, Zeyu Chen |  |
| 2 |  |  [Towards Reproducible Machine Learning Research in Natural Language Processing](https://doi.org/10.18653/v1/2022.acl-tutorials.2) |  | 0 | While recent progress in the field of ML has been significant, the reproducibility of these cutting-edge results is often lacking, with many submissions lacking the necessary information in order to ensure subsequent reproducibility. Despite proposals such as the Reproducibility Checklist and reproducibility criteria at several major conferences, the reflex for... | Ana Lucic, Jesse Dodge, Jessica Zosa Forde, Koustuv Sinha, Maurits J. R. Bleeker, Robert Stojnic, Samarth Bhargav, Sasha Luccioni |  |
| 3 |  |  [Knowledge-Augmented Methods for Natural Language Processing](https://doi.org/10.18653/v1/2022.acl-tutorials.3) |  | 0 | Knowledge in natural language processing (NLP) has been a rising trend especially after the advent of large scale pre-trained models. NLP models with attention to knowledge can i) access unlimited amount of external information; ii) delegate the task of storing knowledge from its parameter space to knowledge sources; iii) obtain up-to-date information; iv) make... | Bill Y. Lin, Chenguang Zhu, Meng Jiang, Wenhao Yu, Xiang Ren, Yichong Xu |  |
| 4 |  |  [Non-Autoregressive Sequence Generation](https://doi.org/10.18653/v1/2022.acl-tutorials.4) |  | 0 | Non-autoregressive sequence generation (NAR) attempts to generate the entire or partial output sequences in parallel to speed up the generation process and avoid potential issues (e.g., label bias, exposure bias) in autoregressive generation. While it has received much research attention and has been applied in many sequence generation tasks in natural language... | Jiatao Gu, Xu Tan |  |
| 5 |  |  [Learning with Limited Text Data](https://doi.org/10.18653/v1/2022.acl-tutorials.5) |  | 0 | Natural Language Processing (NLP) has achieved great progress in the past decade on the basis of neural models, which often make use of large amounts of labeled data to achieve state-of-the-art performance. The dependence on labeled data prevents NLP models from being applied to low-resource settings and languages because of the time, money, and expertise that... | Ankur P. Parikh, Colin Raffel, Diyi Yang |  |
| 6 |  |  [Zero- and Few-Shot NLP with Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-tutorials.6) |  | 0 | The ability to efficiently learn from little-to-no data is critical to applying NLP to tasks where data collection is costly or otherwise difficult. This is a challenging setting both academically and practically—particularly because training neutral models typically require large amount of labeled data. More recently, advances in pretraining on unlabelled data... | Arman Cohan, Iz Beltagy, Robert L. Logan IV, Sameer Singh, Sewon Min |  |
| 7 |  |  [Vision-Language Pretraining: Current Trends and the Future](https://doi.org/10.18653/v1/2022.acl-tutorials.7) |  | 0 | In the last few years, there has been an increased interest in building multimodal (vision-language) models that are pretrained on larger but noisier datasets where the two modalities (e.g., image and text) loosely correspond to each other (e.g., Lu et al., 2019; Radford et al., 2021). Given a task (such as visual question answering), these models are then... | Aida Nematzadeh, Aishwarya Agrawal, Damien Teney |  |
| 8 |  |  [Natural Language Processing for Multilingual Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-tutorials.8) |  | 0 | Recent advances in deep learning have also enabled fast progress in the research of task-oriented dialogue (ToD) systems. However, the majority of ToD systems are developed for English and merely a handful of other widely spoken languages, e.g., Chinese and German. This hugely limits the global reach and, consequently, transformative socioeconomic potential of... | Edoardo Maria Ponti, Evgeniia Razumovskaia, Goran Glavas, Ivan Vulic, Olga Majewska |  |
| 9 |  |  [DoTAT: A Domain-oriented Text Annotation Tool](https://doi.org/10.18653/v1/2022.acl-demo.1) |  | 0 | We propose DoTAT, a domain-oriented text annotation tool. The tool designs and implements functions heavily in need in domain-oriented information extraction. Firstly, the tool supports a multi-person collaborative process with automatically merging and review, which can greatly improve the annotation accuracy. Secondly, the tool provides annotation of events,... | Ming Liang, Tingting Cai, Tong Ruan, Wen Du, Yi Wang, Yupian Lin |  |
| 10 |  |  [UKP-SQUARE: An Online Platform for Question Answering Research](https://doi.org/10.18653/v1/2022.acl-demo.2) |  | 0 | Recent advances in NLP and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval). Despite having a large number of powerful, specialized QA pipelines... | Clifton Poth, Gregor Geigle, Gözde Gül Sahin, Hannah Sterz, Haritz Puerto, Iryna Gurevych, Jonas Pfeiffer, Kexin Wang, Leonardo F. R. Ribeiro, Max Eichler, Nils Reimers, Rachneet Sachdeva, Tim Baumgärtner |  |
| 11 |  |  [ViLMedic: a framework for research at the intersection of vision and language in medical AI](https://doi.org/10.18653/v1/2022.acl-demo.3) |  | 0 | There is a growing need to model interactions between data modalities (e.g., vision, language) — both to improve AI predictions on existing tasks and to enable new applications. In the recent field of multimodal medical AI, integrating multiple modalities has gained widespread popularity as multimodal models have proven to improve performance, robustness,... | Akshay Chaudhari, Curtis P. Langlotz, Jared Dunnmon, JeanBenoit Delbrouck, Juan Zambrano, Khaled Saab, Maya Varma, Pierre J. Chambon, Sabri Eyuboglu |  |
| 12 |  |  [TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-demo.4) |  | 0 | Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression.... | Yiming Cui, Zhigang Chen, Ziqing Yang |  |
| 13 |  |  [AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark](https://doi.org/10.18653/v1/2022.acl-demo.5) |  | 0 | Open Information Extraction (OIE) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner. Intrinsic performance of OIE systems is difficult to measure due to the incompleteness of existing OIE benchmarks: ground truth extractions do not group all acceptable surface realizations of the same... | Bhushan Kotnis, Carolin Lawrence, Goran Glavas, Kiril Gashteovski, Mathias Niepert, Mingying Yu, Niklas Friedrich |  |
| 14 |  |  [AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters](https://doi.org/10.18653/v1/2022.acl-demo.6) |  | 0 | The open-access dissemination of pretrained language models through online repositories has led to a democratization of state-of-the-art natural language processing (NLP) research. This also allows people outside of NLP to use such models and adapt them to specific use-cases. However, a certain amount of technical proficiency is still required which is an entry... | Bela Bohlender, Christina Viehmann, Iryna Gurevych, Jaber Khuri, Jonas Brossmann, Jonas Pfeiffer, Tilman Beck, Vincent Hane, Yanik Adamson |  |
| 15 |  |  [QiuNiu: A Chinese Lyrics Generation System with Passage-Level Input](https://doi.org/10.18653/v1/2022.acl-demo.7) |  | 0 | Lyrics generation has been a very popular application of natural language generation. Previous works mainly focused on generating lyrics based on a couple of attributes or keywords, rendering very limited control over the content of the lyrics. In this paper, we demonstrate the QiuNiu, a Chinese lyrics generation system which is conditioned on passage-level... | Le Zhang, Rongsheng Zhang, Xiaoxi Mao, Yongzhu Chang |  |
| 16 |  |  [Automatic Gloss Dictionary for Sign Language Learners](https://doi.org/10.18653/v1/2022.acl-demo.8) |  | 0 | A multi-language dictionary is a fundamental tool for language learning, allowing the learner to look up unfamiliar words. Searching an unrecognized word in the dictionary does not usually require deep knowledge of the target language. However, this is not true for sign language, where gestural elements preclude this type of easy lookup. This paper introduces... | Ben Swift, Chenchen Xu, Dongxu Li, Hanna Suominen, Hongdong Li |  |
| 17 |  |  [PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts](https://doi.org/10.18653/v1/2022.acl-demo.9) |  | 0 | PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively.... | Abheesht Sharma, Albert Webson, Alexander M. Rush, Andrea Santilli, Canwen Xu, Colin Raffel, Dragomir R. Radev, Gunjan Chhablani, Han Wang, Jason Alan Fries, Khalid Almubarak, M. Saiful Bari, Maged Saeed AlShaibani, Manan Dey, Mike TianJian Jiang, Nihal V. Nayak, Shanya Sharma, Srulik BenDavid, Stephen H. Bach, Taewoon Kim, Thibault Févry, Urmish Thakker, Victor Sanh, Xiangru Tang, Zaid Alyafeai, Zheng Xin Yong, Zhiqing Sun |  |
| 18 |  |  [OpenPrompt: An Open-source Framework for Prompt-learning](https://doi.org/10.18653/v1/2022.acl-demo.10) |  | 0 | Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (PLMs) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet,... | Haitao Zheng, Maosong Sun, Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu |  |
| 19 |  |  [Guided K-best Selection for Semantic Parsing Annotation](https://doi.org/10.18653/v1/2022.acl-demo.11) |  | 0 | Collecting data for conversational semantic parsing is a time-consuming and demanding process. In this paper we consider, given an incomplete dataset with only a small amount of data, how to build an AI-powered human-in-the-loop process to enable efficient data collection. A guided K-best selection process is proposed, which (i) generates a set of possible... | Aleksandr Nisnevich, Anton Belyy, Benjamin Van Durme, Charles Chen, ChiehYang Huang, Emmanouil Antonios Platanios, Jacob Andreas, Richard Shin, Sam Thomson, Subhro Roy |  |
| 20 |  |  [Hard and Soft Evaluation of NLP models with BOOtSTrap SAmpling - BooStSa](https://doi.org/10.18653/v1/2022.acl-demo.12) |  | 0 | Natural Language Processing (NLP) ‘s applied nature makes it necessary to select the most effective and robust models. Producing slightly higher performance is insufficient; we want to know whether this advantage will carry over to other data sets. Bootstrapped significance tests can indicate that ability. So while necessary, computing the significance of... | Alexandra Uma, Dirk Hovy, Massimo Poesio, Tommaso Fornaciari |  |
| 21 |  |  [COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System](https://doi.org/10.18653/v1/2022.acl-demo.13) |  | 0 | To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge... | Heng Ji, Manling Li, Pengfei Yu, Revanth Gangi Reddy, Tuan Manh Lai, YiShyuan Chiang, Ziqi Wang, Zixuan Zhang |  |
| 22 |  |  [TS-ANNO: An Annotation Tool to Build, Annotate and Evaluate Text Simplification Corpora](https://doi.org/10.18653/v1/2022.acl-demo.14) |  | 0 | We introduce TS-ANNO, an open-source web application for manual creation and for evaluation of parallel corpora for text simplification. TS-ANNO can be used for i) sentence–wise alignment, ii) rating alignment pairs (e.g., w.r.t. grammaticality, meaning preservation, ...), iii) annotating alignment pairs w.r.t. simplification transformations (e.g., lexical... | Laura Kallmeyer, Regina Stodden |  |
| 23 |  |  [Language Diversity: Visible to Humans, Exploitable by Machines](https://doi.org/10.18653/v1/2022.acl-demo.15) |  | 0 | The Universal Knowledge Core (UKC) is a large multilingual lexical database with a focus on language diversity and covering over two thousand languages. The aim of the database, as well as its tools and data catalogue, is to make the abstract notion of linguistic diversity visually understandable for humans and formally exploitable by machines. The UKC website... | Danish Ashgar Cheema, Erdenebileg Byambadorj, Fausto Giunchiglia, Gábor Bella, Khuyagbaatar Batsuren, Yamini Chandrashekar |  |
| 24 |  |  [CogKGE: A Knowledge Graph Embedding Toolkit and Benchmark for Representing Multi-source and Heterogeneous Knowledge](https://doi.org/10.18653/v1/2022.acl-demo.16) |  | 0 | In this paper, we propose CogKGE, a knowledge graph embedding (KGE) toolkit, which aims to represent multi-source and heterogeneous knowledge. For multi-source knowledge, unlike existing methods that mainly focus on entity-centric knowledge, CogKGE also supports the representations of event-centric, commonsense and linguistic knowledge. For heterogeneous... | Chenhao Wang, Dianbo Sui, Hongbang Yuan, Jun Zhao, Tianyi Men, Yubo Chen, Zhipeng Xue, Zhitao He, Zhuoran Jin |  |
| 25 |  |  [Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks](https://doi.org/10.18653/v1/2022.acl-demo.17) |  | 0 | We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking... | Adina Williams, Anmol Gupta, Douwe Kiela, Kushal Tirumala, Max Bartolo, Pedro Rodriguez, Peter Mattson, Tariq Kane, Tristan Thrush, William Gaviria Rojas |  |
| 26 |  |  [DataLab: A Platform for Data Analysis and Intervention](https://doi.org/10.18653/v1/2022.acl-demo.18) |  | 0 | Despite data’s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data. In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized... | Graham Neubig, Jinlan Fu, Pengfei Liu, Vijay Viswanathan, Weizhe Yuan, Yang Xiao, Yixin Liu, Zhoumianze Liu |  |
| 27 |  |  [Cue-bot: A Conversational Agent for Assistive Technology](https://doi.org/10.18653/v1/2022.acl-demo.19) |  | 0 | Intelligent conversational assistants have become an integral part of our lives for performing simple tasks. However, such agents, for example, Google bots, Alexa and others are yet to have any social impact on minority population, for example, for people with neurological disorders and people with speech, language and social communication disorders, sometimes... | Hsuan Su, Lama Nachman, Maximilian Pinaroc, Ramesh Manuvinakurike, Sai Prasad, Saurav Sahay, Shachi H. Kumar |  |
| 28 |  |  [M-SENA: An Integrated Platform for Multimodal Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-demo.20) |  | 0 | M-SENA is an open-sourced platform for Multimodal Sentiment Analysis. It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations. The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules.... | Hua Xu, Huisheng Mao, Kai Gao, Wenmeng Yu, Yihe Liu, Ziqi Yuan |  |
| 29 |  |  [HOSMEL: A Hot-Swappable Modularized Entity Linking Toolkit for Chinese](https://doi.org/10.18653/v1/2022.acl-demo.21) |  | 0 | We investigate the usage of entity linking (EL)in downstream tasks and present the first modularized EL toolkit for easy task adaptation. Different from the existing EL methods that dealwith all the features simultaneously, we modularize the whole model into separate parts witheach feature. This decoupled design enablesflexibly adding new features without... | Daniel Zhangli, Jie Tang, Jifan Yu, Jing Zhang, Juanzi Li, Peng Zhang, Xiaokang Zhang |  |
| 30 |  |  [BMInf: An Efficient Toolkit for Big Model Inference and Tuning](https://doi.org/10.18653/v1/2022.acl-demo.22) |  | 0 | In recent years, large-scale pre-trained language models (PLMs) containing billions of parameters have achieved promising results on various NLP tasks. Although we can pre-train these big models by stacking computing clusters at any cost, it is impractical to use such huge computing resources to apply big models for each downstream task. To address the... | Guoyang Zeng, Jia Chao, Jie Zhou, Jun Zhang, Maosong Sun, Weilin Zhao, Xu Han, Zhengyan Zhang, Zhiyuan Liu |  |
| 31 |  |  [MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities](https://doi.org/10.18653/v1/2022.acl-demo.23) |  | 0 | Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other. Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover... | Aixin Sun, Jing Shao, Kun Wang, Kunquan Deng, Meiqi Chen, Mukai Li, Wenqi Sun, Xinze Li, Yixin Cao, Yubo Ma, Zehao Wang |  |
| 32 |  |  [SocioFillmore: A Tool for Discovering Perspectives](https://doi.org/10.18653/v1/2022.acl-demo.24) |  | 0 | SOCIOFILLMORE is a multilingual tool which helps to bring to the fore the focus or the perspective that a text expresses in depicting an event. Our tool, whose rationale we also support through a large collection of human judgements, is theoretically grounded on frame semantics and cognitive linguistics, and implemented using the LOME frame semantic parser. We... | Chiara Zanchi, Gosse Minnema, Malvina Nissim, Sara Gemelli, Tommaso Caselli |  |
| 33 |  |  [TimeLMs: Diachronic Language Models from Twitter](https://doi.org/10.18653/v1/2022.acl-demo.25) |  | 0 | Despite its importance, the time variable has been largely neglected in the NLP and language model literature. In this paper, we present TimeLMs, a set of language models specialized on diachronic Twitter data. We show that a continual learning strategy contributes to enhancing Twitter-based language models’ capacity to deal with future and out-of-distribution... | Daniel Loureiro, Francesco Barbieri, José CamachoCollados, Leonardo Neves, Luis Espinosa Anke |  |
| 34 |  |  [Adaptor: Objective-Centric Adaptation Framework for Language Models](https://doi.org/10.18653/v1/2022.acl-demo.26) |  | 0 | This paper introduces Adaptor library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives. We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom... | Michal Stefánik, Nikola Groverová, Petr Sojka, Vít Novotný |  |
| 35 |  |  [QuickGraph: A Rapid Annotation Tool for Knowledge Graph Extraction from Technical Text](https://doi.org/10.18653/v1/2022.acl-demo.27) |  | 0 | Acquiring high-quality annotated corpora for complex multi-task information extraction (MT-IE) is an arduous and costly process for human-annotators. Adoption of unsupervised techniques for automated annotation have thus become popular. However, these techniques rely heavily on dictionaries, gazetteers, and knowledge bases. While such resources are abundant for... | Michael Stewart, Tyler Bikaun, Wei Liu |  |
| 36 |  |  [Frontmatter](https://aclanthology.org/2022.acl-srw.0) |  | 0 |  |  |  |
| 37 |  |  [Evaluating zero-shot transfers and multilingual models for dependency parsing and POS tagging within the low-resource language family Tupían](https://doi.org/10.18653/v1/2022.acl-srw.1) |  | 0 | This work presents two experiments with the goal of replicating the transferability of dependency parsers and POS taggers trained on closely related languages within the low-resource language family Tupían. The experiments include both zero-shot settings as well as multilingual models. Previous studies have found that even a comparably small treebank from a... | Frederic Blum |  |
| 38 |  |  [RFBFN: A Relation-First Blank Filling Network for Joint Relational Triple Extraction](https://doi.org/10.18653/v1/2022.acl-srw.2) |  | 0 | Joint relational triple extraction from unstructured text is an important task in information extraction. However, most existing works either ignore the semantic information of relations or predict subjects and objects sequentially. To address the issues, we introduce a new blank filling paradigm for the task, and propose a relation-first blank filling network... | Chenghu Zhou, Haisong Zhang, Luoyi Fu, Xinbing Wang, Zhe Li |  |
| 39 |  |  [Building a Dialogue Corpus Annotated with Expressed and Experienced Emotions](https://doi.org/10.18653/v1/2022.acl-srw.3) |  | 0 | In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with... | Daisuke Kawahara, Tatsuya Ide |  |
| 40 |  |  [Darkness can not drive out darkness: Investigating Bias in Hate SpeechDetection Models](https://doi.org/10.18653/v1/2022.acl-srw.4) |  | 0 | It has become crucial to develop tools for automated hate speech and abuse detection. These tools would help to stop the bullies and the haters and provide a safer environment for individuals especially from marginalized groups to freely express themselves. However, recent research shows that machine learning models are biased and they might make the right... | Fatma Elsafoury |  |
| 41 |  |  [Ethical Considerations for Low-resourced Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.5) |  | 0 | This paper considers some ethical implications of machine translation for low-resourced languages. I use Armenian as a case study and investigate specific needs for and concerns arising from the creation and deployment of improved machine translation between English and Armenian. To do this, I conduct stakeholder interviews and construct Value Scenarios (Nathan... | Levon Haroutunian |  |
| 42 |  |  [Integrating Question Rewrites in Conversational Question Answering: A Reinforcement Learning Approach](https://doi.org/10.18653/v1/2022.acl-srw.6) |  | 0 | Resolving dependencies among dialogue history is one of the main obstacles in the research on conversational question answering (QA). The conversational question rewrites (QR) task has been shown to be effective to solve this problem by reformulating questions in a self-contained form. However, QR datasets are limited and existing methods tend to depend on the... | Bryan Wilie, Etsuko Ishii, Pascale Fung, Samuel Cahyawijaya, Yan Xu |  |
| 43 |  |  [What Do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification](https://doi.org/10.18653/v1/2022.acl-srw.7) |  | 0 | Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taillé et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition... | Barbara Plank, Elisa Bassignana |  |
| 44 |  |  [Logical Inference for Counting on Semi-structured Tables](https://doi.org/10.18653/v1/2022.acl-srw.8) |  | 0 | Recently, the Natural Language Inference (NLI) task has been studied for semi-structured tables that do not have a strict format. Although neural approaches have achieved high performance in various types of NLI, including NLI between semi-structured tables and texts, they still have difficulty in performing a numerical type of inference, such as counting. To... | Hitomi Yanaka, Tomoya Kurosawa |  |
| 45 |  |  [GNNer: Reducing Overlapping in Span-based NER Using Graph Neural Networks](https://doi.org/10.18653/v1/2022.acl-srw.9) |  | 0 | There are two main paradigms for Named Entity Recognition (NER): sequence labelling and span classification. Sequence labelling aims to assign a label to each word in an input text using, for example, BIO (Begin, Inside and Outside) tagging, while span classification involves enumerating all possible spans in a text and classifying them into their labels. In... | Nadi Tomeh, Pierre Holat, Thierry Charnois, Urchade Zaratiana |  |
| 46 |  |  [Compositional Semantics and Inference System for Temporal Order based on Japanese CCG](https://doi.org/10.18653/v1/2022.acl-srw.10) |  | 0 | Natural Language Inference (NLI) is the task of determining whether a premise entails a hypothesis. NLI with temporal order is a challenging task because tense and aspect are complex linguistic phenomena involving interactions with temporal adverbs and temporal connectives. To tackle this, temporal and aspectual inference has been analyzed in various ways in... | Hitomi Yanaka, Tomoki Sugimoto |  |
| 47 |  |  [Combine to Describe: Evaluating Compositional Generalization in Image Captioning](https://doi.org/10.18653/v1/2022.acl-srw.11) |  | 0 | Compositionality – the ability to combine simpler concepts to understand & generate arbitrarily more complex conceptual structures – has long been thought to be the cornerstone of human language capacity. With the recent, notable success of neural models in various NLP tasks, attention has now naturally turned to the compositional capacity of these models. In... | Alessandro Suglia, Arash Eshghi, Georgios Pantazopoulos |  |
| 48 |  |  [Towards Unification of Discourse Annotation Frameworks](https://doi.org/10.18653/v1/2022.acl-srw.12) |  | 0 | Discourse information is difficult to represent and annotate. Among the major frameworks for annotating discourse information, RST, PDTB and SDRT are widely discussed and used, each having its own theoretical foundation and focus. Corpora annotated under different frameworks vary considerably. To make better use of the existing discourse corpora and achieve the... | Yingxue Fu |  |
| 49 |  |  [AMR Alignment for Morphologically-rich and Pro-drop Languages](https://doi.org/10.18653/v1/2022.acl-srw.13) |  | 0 | Alignment between concepts in an abstract meaning representation (AMR) graph and the words within a sentence is one of the important stages of AMR parsing. Although there exist high performing AMR aligners for English, unfortunately, these are not well suited for many languages where many concepts appear from morpho-semantic elements. For the first time in the... | Elif Kaplan, Gülsen Eryigit |  |
| 50 |  |  [Sketching a Linguistically-Driven Reasoning Dialog Model for Social Talk](https://doi.org/10.18653/v1/2022.acl-srw.14) |  | 0 | The capability of holding social talk (or casual conversation) and making sense of conversational content requires context-sensitive natural language understanding and reasoning, which cannot be handled efficiently by the current popular open-domain dialog systems and chatbots. Heavily relying on corpus-based machine learning techniques to encode and decode... | Alex Luu |  |
| 51 |  |  [Scoping natural language processing in Indonesian and Malay for education applications](https://doi.org/10.18653/v1/2022.acl-srw.15) |  | 0 | Indonesian and Malay are underrepresented in the development of natural language processing (NLP) technologies and available resources are difficult to find. A clear picture of existing work can invigorate and inform how researchers conceptualise worthwhile projects. Using an education sector project to motivate the study, we conducted a wide-ranging overview... | Hanna Suominen, Michelle Kohler, Zara MaxwellSmith |  |
| 52 |  |  [English-Malay Cross-Lingual Embedding Alignment using Bilingual Lexicon Augmentation](https://doi.org/10.18653/v1/2022.acl-srw.16) |  | 0 | As high-quality Malay language resources are still a scarcity, cross lingual word embeddings make it possible for richer English resources to be leveraged for downstream Malay text classification tasks. This paper focuses on creating an English-Malay cross-lingual word embeddings using embedding alignment by exploiting existing language resources. We augmented... | Jasy Suet Yan Liew, Ying Hao Lim |  |
| 53 |  |  [Towards Detecting Political Bias in Hindi News Articles](https://doi.org/10.18653/v1/2022.acl-srw.17) |  | 0 | Political propaganda in recent times has been amplified by media news portals through biased reporting, creating untruthful narratives on serious issues causing misinformed public opinions with interests of siding and helping a particular political party. This issue proposes a challenging NLP task of detecting political bias in news articles. We propose a... | Devansh Gautam, Kshitij Gupta, Radhika Mamidi, Samyak Agrawal |  |
| 54 |  |  [Restricted or Not: A General Training Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.18) |  | 0 | Restricted machine translation incorporates human prior knowledge into translation. It restricts the flexibility of the translation to satisfy the demands of translation in specific scenarios. Existing work typically imposes constraints on beam search decoding. Although this can satisfy the requirements overall, it usually requires a larger beam size and far... | Eiichiro Sumita, Hai Zhao, Masao Utiyama, Zuchao Li |  |
| 55 |  |  [What do Models Learn From Training on More Than Text? Measuring Visual Commonsense Knowledge](https://doi.org/10.18653/v1/2022.acl-srw.19) |  | 0 | There are limitations in learning language from text alone. Therefore, recent focus has been on developing multimodal models. However, few benchmarks exist that can measure what language models learn about language from multimodal training. We hypothesize that training on a visual modality should improve on the visual commonsense knowledge in language models.... | Lovisa Hagström, Richard Johansson |  |
| 56 |  |  [TeluguNER: Leveraging Multi-Domain Named Entity Recognition with Deep Transformers](https://doi.org/10.18653/v1/2022.acl-srw.20) |  | 0 | Named Entity Recognition (NER) is a successful and well-researched problem in English due to the availability of resources. The transformer models, specifically the masked-language models (MLM), have shown remarkable performance in NER during recent times. With growing data in different online platforms, there is a need for NER in other languages too. NER... | Mounika Marreddy, Radhika Mamidi, Subba Reddy Oota, Suma Reddy Duggenpudi |  |
| 57 |  |  [Using Neural Machine Translation Methods for Sign Language Translation](https://doi.org/10.18653/v1/2022.acl-srw.21) |  | 0 | We examine methods and techniques, proven to be helpful for the text-to-text translation of spoken languages in the context of gloss-to-text translation systems, where the glosses are the written representation of the signs. We present one of the first works that include experiments on both parallel corpora of the German Sign Language (PHOENIX14T and the Public... | Eleftherios Avramidis, Galina Angelova, Sebastian Möller |  |
| 58 |  |  [Flexible Visual Grounding](https://doi.org/10.18653/v1/2022.acl-srw.22) |  | 0 | Existing visual grounding datasets are artificially made, where every query regarding an entity must be able to be grounded to a corresponding image region, i.e., answerable. However, in real-world multimedia data such as news articles and social media, many entities in the text cannot be grounded to the image, i.e., unanswerable, due to the fact that the text... | Chenhui Chu, Sadao Kurohashi, Yongmin Kim |  |
| 59 |  |  [A large-scale computational study of content preservation measures for text style transfer and paraphrase generation](https://doi.org/10.18653/v1/2022.acl-srw.23) |  | 0 | Text style transfer and paraphrasing of texts are actively growing areas of NLP, dozens of methods for solving these tasks have been recently introduced. In both tasks, the system is supposed to generate a text which should be semantically similar to the input text. Therefore, these tasks are dependent on methods of measuring textual semantic similarity.... | Alexander Panchenko, David Dale, Nikolay Babakov, Varvara Logacheva |  |
| 60 |  |  [Explicit Object Relation Alignment for Vision and Language Navigation](https://doi.org/10.18653/v1/2022.acl-srw.24) |  | 0 | In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the... | Parisa Kordjamshidi, Yue Zhang |  |
| 61 |  |  [Mining Logical Event Schemas From Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-srw.25) |  | 0 | We present NESL (the Neuro-Episodic Schema Learner), an event schema learning system that combines large language models, FrameNet parsing, a powerful logical representation of language, and a set of simple behavioral schemas meant to bootstrap the learning process. In lieu of a pre-made corpus of stories, our dataset is a continuous feed of “situation samples”... | Lane Lawley, Lenhart K. Schubert |  |
| 62 |  |  [Exploring Cross-lingual Text Detoxification with Large Multilingual Language Models](https://doi.org/10.18653/v1/2022.acl-srw.26) |  | 0 | Detoxification is a task of generating text in polite style while preserving meaning and fluency of the original toxic text. Existing detoxification methods are monolingual i.e. designed to work in one exact language. This work investigates multilingual and cross-lingual detoxification and the behavior of large multilingual models in this setting. Unlike... | Alexander Panchenko, Daniil Moskovskiy, Daryna Dementieva |  |
| 63 |  |  [MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering](https://doi.org/10.18653/v1/2022.acl-srw.27) |  | 0 | Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary... | Albert Sayapin, Alexander Panchenko, Anton Razzhigaev, Evgeny Frolov, Viktoria Chekalina |  |
| 64 |  |  [Discourse on ASR Measurement: Introducing the ARPOCA Assessment Tool](https://doi.org/10.18653/v1/2022.acl-srw.28) |  | 0 | Automatic speech recognition (ASR) has evolved from a pipeline architecture with pronunciation dictionaries, phonetic features and language models to the end-to-end systems performing a direct translation from a raw waveform into a word sequence. With the increase in accuracy and the availability of pre-trained models, the ASR systems are now omnipresent in our... | Megan Merz, Olga Scrivner |  |
| 65 |  |  [Pretrained Knowledge Base Embeddings for improved Sentential Relation Extraction](https://doi.org/10.18653/v1/2022.acl-srw.29) |  | 0 | In this work we put forward to combine pretrained knowledge base graph embeddings with transformer based language models to improve performance on the sentential Relation Extraction task in natural language processing. Our proposed model is based on a simple variation of existing models to incorporate off-task pretrained graph embeddings with an on-task... | Andrea Papaluca, Artem Lenskiy, Daniel Krefl, Hanna Suominen |  |
| 66 |  |  [Improving Cross-domain, Cross-lingual and Multi-modal Deception Detection](https://doi.org/10.18653/v1/2022.acl-srw.30) |  | 0 | With the increase of deception and misinformation especially in social media, it has become crucial to be able to develop machine learning methods to automatically identify deceptive language. In this proposal, we identify key challenges underlying deception detection in cross-domain, cross-lingual and multi-modal settings. To improve cross-domain deception... | Sarah Ita Levitan, Subhadarshi Panda |  |
| 67 |  |  [Automatic Generation of Distractors for Fill-in-the-Blank Exercises with Round-Trip Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-srw.31) |  | 0 | In a fill-in-the-blank exercise, a student is presented with a carrier sentence with one word hidden, and a multiple-choice list that includes the correct answer and several inappropriate options, called distractors. We propose to automatically generate distractors using round-trip neural machine translation: the carrier sentence is translated from English into... | Alla Rozovskaya, Frank Palma Gomez, Michael Flor, Subhadarshi Panda |  |
| 68 |  |  [On the Locality of Attention in Direct Speech Translation](https://doi.org/10.18653/v1/2022.acl-srw.32) |  | 0 | Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we... | Belen Alastruey, Gerard I. Gállego, Javier Ferrando, Marta R. Costajussà |  |
| 69 |  |  [Extraction of Diagnostic Reasoning Relations for Clinical Knowledge Graphs](https://doi.org/10.18653/v1/2022.acl-srw.33) |  | 0 | Clinical knowledge graphs lack meaningful diagnostic relations (e.g. comorbidities, sign/symptoms), limiting their ability to represent real-world diagnostic processes. Previous methods in biomedical relation extraction have focused on concept relations, such as gene-disease and disease-drug, and largely ignored clinical processes. In this thesis, we leverage a... | Vimig Socrates |  |
| 70 |  |  [Scene-Text Aware Image and Text Retrieval with Dual-Encoder](https://doi.org/10.18653/v1/2022.acl-srw.34) |  | 0 | We tackle the tasks of image and text retrieval using a dual-encoder model in which images and text are encoded independently. This model has attracted attention as an approach that enables efficient offline inferences by connecting both vision and language in the same semantic space; however, whether an image encoder as part of a dual-encoder model can... | Jun Suzuki, Kyosuke Nishida, Shumpei Miyawaki, Taku Hasegawa, Takuma Kato |  |
| 71 |  |  [Towards Fine-grained Classification of Climate Change related Social Media Text](https://doi.org/10.18653/v1/2022.acl-srw.35) |  | 0 | With climate change becoming a cause of concern worldwide, it becomes essential to gauge people’s reactions. This can help educate and spread awareness about it and help leaders improve decision-making. This work explores the fine-grained classification and Stance detection of climate change-related social media text. Firstly, we create two datasets,... | Kartikey Pant, Manish Shrivastava, Roopal Vaid |  |
| 72 |  |  [Deep Neural Representations for Multiword Expressions Detection](https://doi.org/10.18653/v1/2022.acl-srw.36) |  | 0 | Effective methods for multiword expressions detection are important for many technologies related to Natural Language Processing. Most contemporary methods are based on the sequence labeling scheme applied to an annotated corpus, while traditional methods use statistical measures. In our approach, we want to integrate the concepts of those two approaches. We... | Kamil Kanclerz, Maciej Piasecki |  |
| 73 |  |  [A Checkpoint on Multilingual Misogyny Identification](https://doi.org/10.18653/v1/2022.acl-srw.37) |  | 0 | We address the problem of identifying misogyny in tweets in mono and multilingual settings in three languages: English, Italian, and Spanish. We explore model variations considering single and multiple languages both in the pre-training of the transformer and in the training of the downstream taskto explore the feasibility of detecting misogyny through a... | Alberto BarrónCedeño, Arianna Muti |  |
| 74 |  |  [Using dependency parsing for few-shot learning in distributional semantics](https://doi.org/10.18653/v1/2022.acl-srw.38) |  | 0 | In this work, we explore the novel idea of employing dependency parsing information in the context of few-shot learning, the task of learning the meaning of a rare word based on a limited amount of context sentences. Firstly, we use dependency-based word embedding models as background spaces for few-shot learning. Secondly, we introduce two few-shot learning... | Guy Emerson, Stefania Preda |  |
| 75 |  |  [A Dataset and BERT-based Models for Targeted Sentiment Analysis on Turkish Texts](https://doi.org/10.18653/v1/2022.acl-srw.39) |  | 0 | Targeted Sentiment Analysis aims to extract sentiment towards a particular target from a given text. It is a field that is attracting attention due to the increasing accessibility of the Internet, which leads people to generate an enormous amount of data. Sentiment analysis, which in general requires annotated data for training, is a well-researched area for... | Arzucan Özgür, Mustafa Melih Mutlu |  |
| 76 |  |  [Frontmatter](https://aclanthology.org/2022.acl-short.0) |  | 0 |  |  |  |
| 77 |  |  [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://doi.org/10.18653/v1/2022.acl-short.1) |  | 0 | We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse... | Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg |  |
| 78 |  |  [Are Shortest Rationales the Best Explanations for Human Understanding?](https://doi.org/10.18653/v1/2022.acl-short.2) |  | 0 | Existing self-explaining models typically favor extracting the shortest possible rationales — snippets of an input text “responsible for” corresponding output — to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most... | Hua Shen, TingHao Kenneth Huang, Tongshuang Wu, Wenbo Guo |  |
| 79 |  |  [Analyzing Wrap-Up Effects through an Information-Theoretic Lens](https://doi.org/10.18653/v1/2022.acl-short.3) |  | 0 | Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed... | Clara Meister, Roger Levy, Ryan Cotterell, Thomas Hikaru Clark, Tiago Pimentel |  |
| 80 |  |  [Have my arguments been replied to? Argument Pair Extraction as Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-short.4) |  | 0 | Argument pair extraction (APE) aims to automatically mine argument pairs from two interrelated argumentative documents. Existing studies typically identify argument pairs indirectly by predicting sentence-level relations between two documents, neglecting the modeling of the holistic argument-level interactions. Towards this issue, we propose to address APE via... | Jianzhu Bao, Jingyi Sun, Qinglin Zhu, Ruifeng Xu |  |
| 81 |  |  [High probability or low information? The probability-quality paradox in language generation](https://doi.org/10.18653/v1/2022.acl-short.5) |  | 0 | When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is... | Clara Meister, Gian Wiher, Ryan Cotterell, Tiago Pimentel |  |
| 82 |  |  [Disentangled Knowledge Transfer for OOD Intent Discovery with Unified Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-short.6) |  | 0 | Discovering Out-of-Domain(OOD) intents is essential for developing new skills in a task-oriented dialogue system. The key challenge is how to transfer prior IND knowledge to OOD clustering. Different from existing work based on shared intent representation, we propose a novel disentangled knowledge transfer method via a unified multi-head contrastive learning... | Hong Xu, Huixing Jiang, Keqing He, Wei Wu, Weiran Xu, Yanan Wu, Yutao Mou, Zhiyuan Zeng |  |
| 83 |  |  [Voxel-informed Language Grounding](https://doi.org/10.18653/v1/2022.acl-short.7) |  | 0 | Natural language applied to natural 2D images describes a fundamentally 3D world. We present the Voxel-informed Language Grounder (VLG), a language grounding model that leverages 3D geometric information in the form of voxel maps derived from the visual input using a volumetric reconstruction model. We show that VLG significantly improves grounding accuracy on... | Dan Klein, Rodolfo Corona, Shizhan Zhu, Trevor Darrell |  |
| 84 |  |  [P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks](https://doi.org/10.18653/v1/2022.acl-short.8) |  | 0 | Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence... | Jie Tang, Kaixuan Ji, Weng Tam, Xiao Liu, Yicheng Fu, Zhengxiao Du, Zhilin Yang |  |
| 85 |  |  [On Efficiently Acquiring Annotations for Multilingual Models](https://doi.org/10.18653/v1/2022.acl-short.9) |  | 0 | When tasked with supporting multiple languages for a given problem, two approaches have arisen: training a model for each language with the annotation budget divided equally among them, and training on a high-resource language followed by zero-shot transfer to the remaining languages. In this work, we show that the strategy of joint learning across multiple... | Barun Patra, Joel Ruben Antony Moniz, Matthew Gormley |  |
| 86 |  |  [Automatic Detection of Entity-Manipulated Text using Factual Knowledge](https://doi.org/10.18653/v1/2022.acl-short.10) |  | 0 | In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network... | Ganesh Jawahar, Laks V. S. Lakshmanan, Muhammad AbdulMageed |  |
| 87 |  |  [Does BERT Know that the IS-A Relation Is Transitive?](https://doi.org/10.18653/v1/2022.acl-short.11) |  | 0 | The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues... | Hwee Tou Ng, Ruixi Lin |  |
| 88 |  |  [Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-short.12) |  | 0 | Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some... | Chengyu Chuang, Yi Yang |  |
| 89 |  |  [Pixie: Preference in Implicit and Explicit Comparisons](https://doi.org/10.18653/v1/2022.acl-short.13) |  | 0 | We present Pixie, a manually annotated dataset for preference classification comprising 8,890 sentences drawn from app reviews. Unlike previous studies on preference classification, Pixie contains implicit (omitting an entity being compared) and indirect (lacking comparative linguistic cues) comparisons. We find that transformer-based pretrained models,... | Amanul Haque, Hui Guo, Munindar P. Singh, Vaibhav Garg |  |
| 90 |  |  [Counterfactual Explanations for Natural Language Interfaces](https://doi.org/10.18653/v1/2022.acl-short.14) |  | 0 | A key challenge facing natural language interfaces is enabling users to understand the capabilities of the underlying system. We propose a novel approach for generating explanations of a natural language interface based on semantic parsing. We focus on counterfactual explanations, which are post-hoc explanations that describe to the user how they could have... | George Tolkachev, Osbert Bastani, Stephen Mell, Steve Zdancewic |  |
| 91 |  |  [Predicting Difficulty and Discrimination of Natural Language Questions](https://doi.org/10.18653/v1/2022.acl-short.15) |  | 0 | Item Response Theory (IRT) has been extensively used to numerically characterize question difficulty and discrimination for human subjects in domains including cognitive psychology and education (Primi et al., 2014; Downing, 2003). More recently, IRT has been used to similarly characterize item difficulty and discrimination for natural language models across... | Matthew Byrd, Shashank Srivastava |  |
| 92 |  |  [How does the pre-training objective affect what large language models learn about linguistic properties?](https://doi.org/10.18653/v1/2022.acl-short.16) |  | 0 | Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties.... | Ahmed Alajrami, Nikolaos Aletras |  |
| 93 |  |  [The Power of Prompt Tuning for Low-Resource Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-short.17) |  | 0 | Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsing—the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we... | Harm de Vries, Nathan Schucher, Siva Reddy |  |
| 94 |  |  [Data Contamination: From Memorization to Exploitation](https://doi.org/10.18653/v1/2022.acl-short.18) |  | 0 | Pretrained language models are typically trained on massive web-based datasets, which are often “contaminated” with downstream test sets. It is not clear to what extent models exploit the contaminated data for downstream tasks. We present a principled method to study this question. We pretrain BERT models on joint corpora of Wikipedia and labeled downstream... | Inbal Magar, Roy Schwartz |  |
| 95 |  |  [Detecting Annotation Errors in Morphological Data with the Transformer](https://doi.org/10.18653/v1/2022.acl-short.19) |  | 0 | Annotation errors that stem from various sources are usually unavoidable when performing large-scale annotation of linguistic data. In this paper, we evaluate the feasibility of using the Transformer model to detect various types of annotator errors in morphological data sets that contain inflected word forms. We evaluate our error detection model on four... | Ling Liu, Mans Hulden |  |
| 96 |  |  [Estimating the Entropy of Linguistic Distributions](https://doi.org/10.18653/v1/2022.acl-short.20) |  | 0 | Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive... | Aryaman Arora, Clara Meister, Ryan Cotterell |  |
| 97 |  |  [Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a Georgian Case Study](https://doi.org/10.18653/v1/2022.acl-short.21) |  | 0 | In recent years, a flurry of morphological datasets had emerged, most notably UniMorph, aa multi-lingual repository of inflection tables. However, the flat structure of the current morphological annotation makes the treatment of some languages quirky, if not impossible, specifically in cases of polypersonal agreement. In this paper we propose a general solution... | David Guriel, Omer Goldman, Reut Tsarfaty |  |
| 98 |  |  [DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization](https://doi.org/10.18653/v1/2022.acl-short.22) |  | 0 | Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where... | Andrew O. Arnold, Bing Xiang, Dan Roth, Ming Tan, Parminder Bhatia, Ramesh Nallapati, Zheng Li, Zijian Wang |  |
| 99 |  |  [Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension](https://doi.org/10.18653/v1/2022.acl-short.23) |  | 0 | Comprehending a dialogue requires a model to capture diverse kinds of key information in the utterances, which are either scattered around or implicitly implied in different turns of conversations. Therefore, dialogue comprehension requires diverse capabilities such as paraphrasing, summarizing, and commonsense reasoning. Towards the objective of pre-training a... | Chao Zhao, Dian Yu, Dong Yu, Jianshu Chen, Kaiqiang Song, Wenlin Yao |  |
| 100 |  |  [Kronecker Decomposition for GPT Compression](https://doi.org/10.18653/v1/2022.acl-short.24) |  | 0 | GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be... | Ahmad Rashid, Ali Edalati, James J. Clark, Marzieh S. Tahaei, Mehdi Rezagholizadeh, Vahid Partovi Nia |  |
| 101 |  |  [Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction](https://doi.org/10.18653/v1/2022.acl-short.25) |  | 0 | A key challenge in attribute value extraction (AVE) from e-commerce sites is how to handle a large number of attributes for diverse products. Although this challenge is partially addressed by a question answering (QA) approach which finds a value in product data for a given query (attribute), it does not work effectively for rare and ambiguous queries. We thus... | Keiji Shinzato, Naoki Yoshinaga, WeiTe Chen, Yandi Xia |  |
| 102 |  |  [Event-Event Relation Extraction using Probabilistic Box Embedding](https://doi.org/10.18653/v1/2022.acl-short.26) |  | 0 | To understand a story with multiple events, it is important to capture the proper relations across these events. However, existing event relation extraction (ERE) framework regards it as a multi-class classification task and do not guarantee any coherence between different relation types, such as anti-symmetry. If a phone line “died” after “storm”, then it is... | Andrew McCallum, Dhruvesh Patel, Dongxu Zhang, EunJeong Hwang, JayYoon Lee, Tianyi Yang |  |
| 103 |  |  [Sample, Translate, Recombine: Leveraging Audio Alignments for Data Augmentation in End-to-end Speech Translation](https://doi.org/10.18653/v1/2022.acl-short.27) |  | 0 | End-to-end speech translation relies on data that pair source-language speech inputs with corresponding translations into a target language. Such data are notoriously scarce, making synthetic data augmentation by back-translation or knowledge distillation a necessary ingredient of end-to-end training. In this paper, we present a novel approach to data... | Shigehiko Schamoni, Stefan Riezler, Tsz Kin Lam |  |
| 104 |  |  [Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure](https://doi.org/10.18653/v1/2022.acl-short.28) |  | 0 | Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function... | Bohan Zhang, Prafulla Kumar Choubey, Ruihong Huang |  |
| 105 |  |  [Multilingual Pre-training with Language and Task Adaptation for Multilingual Text Style Transfer](https://doi.org/10.18653/v1/2022.acl-short.29) |  | 0 | We exploit the pre-trained seq2seq model mBART for multilingual text style transfer. Using machine translated data as well as gold aligned English sentences yields state-of-the-art results in the three target languages we consider. Besides, in view of the general scarcity of parallel data, we propose a modular approach for multilingual formality transfer, which... | Antonio Toral, Huiyuan Lai, Malvina Nissim |  |
| 106 |  |  [When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning](https://doi.org/10.18653/v1/2022.acl-short.30) |  | 0 | Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task... | Kevin D. Seppi, Matt Gardner, Orion Weller |  |
| 107 |  |  [Leveraging Explicit Lexico-logical Alignments in Text-to-SQL Parsing](https://doi.org/10.18653/v1/2022.acl-short.31) |  | 0 | Text-to-SQL aims to parse natural language questions into SQL queries, which is valuable in providing an easy interface to access large databases. Previous work has observed that leveraging lexico-logical alignments is very helpful to improve parsing performance. However, current attention-based approaches can only model such alignments at the token level and... | Chong Zhu, Jinlong Li, Jun Zhao, Kang Liu, Runxin Sun, Shizhu He, Yaohan He |  |
| 108 |  |  [Complex Evolutional Pattern Learning for Temporal Knowledge Graph Reasoning](https://doi.org/10.18653/v1/2022.acl-short.32) |  | 0 | A Temporal Knowledge Graph (TKG) is a sequence of KGs corresponding to different timestamps. TKG reasoning aims to predict potential facts in the future given the historical KG sequences. One key of this task is to mine and understand evolutional patterns of facts from these sequences. The evolutional patterns are complex in two aspects, length-diversity and... | Jiafeng Guo, Long Bai, Saiping Guan, Wei Li, Weihua Peng, Xiaolong Jin, Xueqi Cheng, Yajuan Lyu, Yong Zhu, Zixuan Li |  |
| 109 |  |  [Mismatch between Multi-turn Dialogue and its Evaluation Metric in Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-short.33) |  | 0 | Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions. A belief state, one of the core pieces of information, refers to the subject and its specific content, and appears in the form of domain-slot-value. The trained model predicts “accumulated” belief states in every turn, and joint... | Hoonsang Yoon, Misuk Kim, Pilsung Kang, Takyoung Kim, Yukyung Lee |  |
| 110 |  |  [LM-BFF-MS: Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory](https://doi.org/10.18653/v1/2022.acl-short.34) |  | 0 | LM-BFF (CITATION) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of LM-BFF, this paper proposes LM-BFF-MS—better few-shot fine-tuning of language models with multiple soft demonstrations by making its further extensions, which include 1) prompts with... | Dong Hyeon Jeon, Eunhwan Park, Inho Kang, Seonhoon Kim, SeungHoon Na |  |
| 111 |  |  [Towards Fair Evaluation of Dialogue State Tracking by Flexible Incorporation of Turn-level Performances](https://doi.org/10.18653/v1/2022.acl-short.35) |  | 0 | Dialogue State Tracking (DST) is primarily evaluated using Joint Goal Accuracy (JGA) defined as the fraction of turns where the ground-truth dialogue state exactly matches the prediction. Generally in DST, the dialogue state or belief state for a given turn contain all the intents shown by the user till that turn. Due to this cumulative nature of the belief... | Maunendra Sankar Desarkar, Ramamohan Kummara, Suvodip Dey |  |
| 112 |  |  [Exploiting Language Model Prompts Using Similarity Measures: A Case Study on the Word-in-Context Task](https://doi.org/10.18653/v1/2022.acl-short.36) |  | 0 | As a recent development in few-shot learning, prompt-based techniques have demonstrated promising potential in a variety of natural language processing tasks. However, despite proving competitive on most tasks in the GLUE and SuperGLUE benchmarks, existing prompt-based techniques fail on the semantic distinction task of the Word-in-Context (WiC) dataset.... | Kiamehr Rezaee, Mohammad Taher Pilehvar, Mohsen Tabasi |  |
| 113 |  |  [Hierarchical Curriculum Learning for AMR Parsing](https://doi.org/10.18653/v1/2022.acl-short.37) |  | 0 | Abstract Meaning Representation (AMR) parsing aims to translate sentences to semantic representation with a hierarchical structure, and is recently empowered by pretrained sequence-to-sequence models. However, there exists a gap between their flat training objective (i.e., equally treats all output tokens) and the hierarchical AMR structure, which limits the... | Baobao Chang, Damai Dai, Liang Chen, Peiyi Wang, Tianyu Liu, Yunbo Cao, Zhifang Sui |  |
| 114 |  |  [PARE: A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.acl-short.38) |  | 0 | Neural models for distantly supervised relation extraction (DS-RE) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the... | Kartikeya Badola, Mausam, Parag Singla, Vipul Rathore |  |
| 115 |  |  [To Find Waldo You Need Contextual Cues: Debiasing Who's Waldo](https://doi.org/10.18653/v1/2022.acl-short.39) |  | 0 | We present a debiased dataset for the Person-centric Visual Grounding (PCVG) task first proposed by Cui et al. (2021) in the Who’s Waldo dataset. Given an image and a caption, PCVG requires pairing up a person’s name mentioned in a caption with a bounding box that points to the person in the image. We find that the original Who’s Waldo dataset compiled for this... | Chitta Baral, Pratyay Banerjee, Tejas Gokhale, Yezhou Yang, Yiran Luo |  |
| 116 |  |  [Translate-Train Embracing Translationese Artifacts](https://doi.org/10.18653/v1/2022.acl-short.40) |  | 0 | Translate-train is a general training approach to multilingual tasks. The key idea is to use the translator of the target language to generate training data to mitigate the gap between the source and target languages. However, its performance is often hampered by the artifacts in the translated texts (translationese). We discover that such artifacts have common... | Hao Zhang, Jing Jiang, Qianru Sun, Sicheng Yu |  |
| 117 |  |  [C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References](https://doi.org/10.18653/v1/2022.acl-short.41) |  | 0 | We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i)... | Dian Yu, Dong Yu, Jianshu Chen, Wenlin Yao, Xiang Yue, Xiaoman Pan |  |
| 118 |  |  [k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations](https://doi.org/10.18653/v1/2022.acl-short.42) |  | 0 | Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many Natural Language Processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these... | Ka Wong, Praveen K. Paritosh |  |
| 119 |  |  [An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers](https://doi.org/10.18653/v1/2022.acl-short.43) |  | 0 | We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text... | Hinrich Schütze, Janet B. Pierrehumbert, Valentin Hofmann |  |
| 120 |  |  [SCD: Self-Contrastive Decorrelation of Sentence Embeddings](https://doi.org/10.18653/v1/2022.acl-short.44) |  | 0 | In this paper, we propose Self-Contrastive Decorrelation (SCD), a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually... | Moin Nabi, Tassilo Klein |  |
| 121 |  |  [Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words](https://doi.org/10.18653/v1/2022.acl-short.45) |  | 0 | Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates... | Dallas Card, Dan Jurafsky, Kaitlyn Zhou, Kawin Ethayarajh |  |
| 122 |  |  [Revisiting the Compositional Generalization Abilities of Neural Sequence Models](https://doi.org/10.18653/v1/2022.acl-short.46) |  | 0 | Compositional generalization is a fundamental trait in humans, allowing us to effortlessly combine known phrases to form novel sentences. Recent works have claimed that standard seq-to-seq models severely lack the ability to compositionally generalize. In this paper, we focus on one-shot primitive generalization as introduced by the popular SCAN benchmark. We... | Arkil Patel, Navin Goyal, Phil Blunsom, Satwik Bhattamishra |  |
| 123 |  |  [A Copy-Augmented Generative Model for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.acl-short.47) |  | 0 | Open-domain question answering is a challenging task with a wide variety of practical applications. Existing modern approaches mostly follow a standard two-stage paradigm: retriever then reader. In this article, we focus on improving the effectiveness of the reader module and propose a novel copy-augmented generative approach that integrates the merits of both... | Dong Wang, Meizhen Ding, Minghui Huang, Shuang Liu, Xiaoguang Li |  |
| 124 |  |  [Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation](https://doi.org/10.18653/v1/2022.acl-short.48) |  | 0 | Dense retrieval models, which aim at retrieving the most relevant document for an input query on a dense representation space, have gained considerable attention for their remarkable success. Yet, dense models require a vast amount of labeled training data for notable performance, whereas it is often challenging to acquire query-document pairs annotated by... | Jinheon Baek, Jong C. Park, Soyeong Jeong, Sukmin Cho, Sung Ju Hwang |  |
| 125 |  |  [WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language](https://doi.org/10.18653/v1/2022.acl-short.49) |  | 0 | Signed Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but has been overlooked by the NLP community thus far. In this paper, we bring to... | Angelo Cangelosi, Aphrodite Galata, Federico Tavella, Marta Romeo, Viktor Schlegel |  |
| 126 |  |  [Investigating person-specific errors in chat-oriented dialogue systems](https://doi.org/10.18653/v1/2022.acl-short.50) |  | 0 | Creating chatbots to behave like real people is important in terms of believability. Errors in general chatbots and chatbots that follow a rough persona have been studied, but those in chatbots that behave like real people have not been thoroughly investigated. We collected a large amount of user interactions of a generation-based chatbot trained from... | Koh Mitsuda, Ryuichiro Higashinaka, Sen Yoshida, Tingxuan Li |  |
| 127 |  |  [Direct parsing to sentiment graphs](https://doi.org/10.18653/v1/2022.acl-short.51) |  | 0 | This paper demonstrates how a graph-based semantic parser can be applied to the task of structured sentiment analysis, directly predicting sentiment graphs from text. We advance the state of the art on 4 out of 5 standard benchmark sets. We release the source code, models and predictions. | David Samuel, Erik Velldal, Jeremy Barnes, Lilja Øvrelid, Robin Kurtz, Stephan Oepen |  |
| 128 |  |  [XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding](https://doi.org/10.18653/v1/2022.acl-short.52) |  | 0 | Transformer-based models are widely used in natural language understanding (NLU) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders’ success in visual-language... | ChanJan Hsu, Hungyi Lee, Yu Tsao |  |
| 129 |  |  [As Little as Possible, as Much as Necessary: Detecting Over- and Undertranslations with Contrastive Conditioning](https://doi.org/10.18653/v1/2022.acl-short.53) |  | 0 | Omission and addition of content is a typical issue in neural machine translation. We propose a method for detecting such phenomena with off-the-shelf translation models. Using contrastive conditioning, we compare the likelihood of a full sequence under a translation model to the likelihood of its parts, given the corresponding source or target sequence. This... | Jannis Vamvas, Rico Sennrich |  |
| 130 |  |  [How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks](https://doi.org/10.18653/v1/2022.acl-short.54) |  | 0 | This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is... | Benoît Crabbé, Bingzhi Li, Guillaume Wisniewski |  |
| 131 |  |  [Machine Translation for Livonian: Catering to 20 Speakers](https://doi.org/10.18653/v1/2022.acl-short.55) |  | 0 | Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other – enabling access to Livonian... | Marili Tomingas, Mark Fishel, Matiss Rikters, Tuuli Tuisk, Valts Ernstreits |  |
| 132 |  |  [Fire Burns, Sword Cuts: Commonsense Inductive Bias for Exploration in Text-based Games](https://doi.org/10.18653/v1/2022.acl-short.56) |  | 0 | Text-based games (TGs) are exciting testbeds for developing deep reinforcement learning techniques due to their partially observed environments and large action spaces. In these games, the agent learns to explore the environment via natural language interactions with the game simulator. A fundamental challenge in TGs is the efficient exploration of the large... | Dongwon Ryu, Ehsan Shareghi, Gholamreza Haffari, Meng Fang, Shirui Pan, Yunqiu Xu |  |
| 133 |  |  [A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-short.57) |  | 0 | Pre-trained language models (PLMs) cannot well recall rich factual knowledge of entities exhibited in large-scale corpora, especially those rare entities. In this paper, we propose to build a simple but effective Pluggable Entity Lookup Table (PELT) on demand by aggregating the entity’s output representations of multiple occurrences in the corpora. PELT can be... | Deming Ye, Maosong Sun, Peng Li, Yankai Lin, Zhiyuan Liu |  |
| 134 |  |  [S$^4$-Tuning: A Simple Cross-lingual Sub-network Tuning Method](https://doi.org/10.18653/v1/2022.acl-short.58) |  | 0 | The emergence of multilingual pre-trained language models makes it possible to adapt to target languages with only few labeled examples. However, vanilla fine-tuning tends to achieve degenerated and unstable results, owing to the Language Interference among different languages, and Parameter Overload under the few-sample transfer learning scenarios. To address... | Baobao Chang, Fei Huang, Fuli Luo, Runxin Xu, Songfang Huang |  |
| 135 |  |  [Region-dependent temperature scaling for certainty calibration and application to class-imbalanced token classification](https://doi.org/10.18653/v1/2022.acl-short.59) |  | 0 | Certainty calibration is an important goal on the path to interpretability and trustworthy AI. Particularly in the context of human-in-the-loop systems, high-quality low to mid-range certainty estimates are essential. In the presence of a dominant high-certainty class, for instance the non-entity class in NER problems, existing calibration error measures are... | Hillary Dawkins, Isar Nejadgholi |  |
| 136 |  |  [Developmental Negation Processing in Transformer Language Models](https://doi.org/10.18653/v1/2022.acl-short.60) |  | 0 | Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of... | Antonio Laverghetta Jr., John Licato |  |
| 137 |  |  [Canary Extraction in Natural Language Understanding Models](https://doi.org/10.18653/v1/2022.acl-short.61) |  | 0 | Natural Language Understanding (NLU) models can be trained on sensitive information such as phone numbers, zip-codes etc. Recent literature has focused on Model Inversion Attacks (ModIvA) that can extract training data from model parameters. In this work, we present a version of such an attack by extracting canaries inserted in NLU training data. In the attack,... | Christophe Dupuy, Rahil Parikh, Rahul Gupta |  |
| 138 |  |  [On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations](https://doi.org/10.18653/v1/2022.acl-short.62) |  | 0 | Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we... | Aram Galstyan, Jwala Dhamala, KaiWei Chang, Rahul Gupta, Varun Kumar, Yada Pruksachatkun, Yang Trista Cao |  |
| 139 |  |  [Sequence-to-sequence AMR Parsing with Ancestor Information](https://doi.org/10.18653/v1/2022.acl-short.63) |  | 0 | AMR parsing is the task that maps a sentence to an AMR semantic graph automatically. The difficulty comes from generating the complex graph structure. The previous state-of-the-art method translates the AMR graph into a sequence, then directly fine-tunes a pretrained sequence-to-sequence Transformer model (BART). However, purely treating the graph as a sequence... | Chen Yu, Daniel Gildea |  |
| 140 |  |  [Zero-Shot Dependency Parsing with Worst-Case Aware Automated Curriculum Learning](https://doi.org/10.18653/v1/2022.acl-short.64) |  | 0 | Large multilingual pretrained language models such as mBERT and XLM-RoBERTa have been found to be surprisingly effective for cross-lingual transfer of syntactic parsing models Wu and Dredze (2019), but only between related languages. However, source and training languages are rarely related, when parsing truly low-resource languages. To close this gap, we adopt... | Anders Søgaard, Miryam de Lhoneux, Sheng Zhang |  |
| 141 |  |  [PriMock57: A Dataset Of Primary Care Mock Consultations](https://doi.org/10.18653/v1/2022.acl-short.65) |  | 0 | Recent advances in Automatic Speech Recognition (ASR) have made it possible to reliably produce automatic transcripts of clinician-patient conversations. However, access to clinical datasets is heavily restricted due to patient privacy, thus slowing down normal research practices. We detail the development of a public access, high quality dataset comprising of... | Aleksandar Savkov, Alex PapadopoulosKorfiatis, Francesco Moramarco, Radmila Sarac |  |
| 142 |  |  [UniGDD: A Unified Generative Framework for Goal-Oriented Document-Grounded Dialogue](https://doi.org/10.18653/v1/2022.acl-short.66) |  | 0 | The goal-oriented document-grounded dialogue aims at responding to the user query based on the dialogue context and supporting document. Existing studies tackle this problem by decomposing it into two sub-tasks: knowledge identification and response generation. However, such pipeline methods would unavoidably suffer from the error propagation issue. This paper... | Chang Gao, Wai Lam, Wenxuan Zhang |  |
| 143 |  |  [DMix: Adaptive Distance-aware Interpolative Mixup](https://doi.org/10.18653/v1/2022.acl-short.67) |  | 0 | Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities. We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity... | Di Jin, Diyi Yang, Lucie Flek, Megh Thakkar, Ramit Sawhney, Ritesh Soun, Shrey Pandit |  |
| 144 |  |  [Sub-Word Alignment is Still Useful: A Vest-Pocket Method for Enhancing Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.68) |  | 0 | We leverage embedding duplication between aligned sub-words to extend the Parent-Child transfer learning method, so as to improve low-resource machine translation. We conduct experiments on benchmark datasets of My-En, Id-En and Tr-En translation scenarios. The test results show that our method produces substantial improvements, achieving the BLEU scores of... | Minhan Xu, Yu Hong |  |
| 145 |  |  [HYPHEN: Hyperbolic Hawkes Attention For Text Streams](https://doi.org/10.18653/v1/2022.acl-short.69) |  | 0 | Analyzing the temporal sequence of texts from sources such as social media, news, and parliamentary debates is a challenging problem as it exhibits time-varying scale-free properties and fine-grained timing irregularities. We propose a Hyperbolic Hawkes Attention Network (HYPHEN), which learns a data-driven hyperbolic space and models irregular powerlaw... | Ramit Sawhney, Ritesh Soun, Sanchit Ahuja, Shivam Agarwal, Sudheer Chava |  |
| 146 |  |  [A Risk-Averse Mechanism for Suicidality Assessment on Social Media](https://doi.org/10.18653/v1/2022.acl-short.70) |  | 0 | Recent studies have shown that social media has increasingly become a platform for users to express suicidal thoughts outside traditional clinical settings. With advances in Natural Language Processing strategies, it is now possible to design automated systems to assess suicide risk. However, such systems may generate uncertain predictions, leading to severe... | Atula Tejaswi Neerkaje, Manas Gaur, Ramit Sawhney |  |
| 147 |  |  [When classifying grammatical role, BERT doesn't care about word order... except when it matters](https://doi.org/10.18653/v1/2022.acl-short.71) |  | 0 | Because meaning can often be inferred from lexical semantics alone, word order is often a redundant cue in natural language. For example, the words chopped, chef, and onion are more likely used to convey “The chef chopped the onion,” not “The onion chopped the chef.” Recent work has shown large language models to be surprisingly word order invariant, but... | Isabel Papadimitriou, Kyle Mahowald, Richard Futrell |  |
| 148 |  |  [Triangular Transfer: Freezing the Pivot for Triangular Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.72) |  | 0 | Triangular machine translation is a special case of low-resource machine translation where the language pair of interest has limited parallel data, but both languages have abundant parallel data with a pivot language. Naturally, the key to triangular machine translation is the successful exploitation of such auxiliary data. In this work, we propose a... | Liangyou Li, Meng Zhang, Qun Liu |  |
| 149 |  |  [Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge](https://doi.org/10.18653/v1/2022.acl-short.73) |  | 0 | Cognitively plausible visual dialogue models should keep a mental scoreboard of shared established facts in the dialogue context. We propose a theory-based evaluation method for investigating to what degree models pretrained on the VisDial dataset incrementally build representations that appropriately do scorekeeping. Our conclusion is that the ability to make... | Brielen Madureira, David Schlangen |  |
| 150 |  |  [Focus on the Target's Vocabulary: Masked Label Smoothing for Machine Translation](https://doi.org/10.18653/v1/2022.acl-short.74) |  | 0 | Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language... | Baobao Chang, Liang Chen, Runxin Xu |  |
| 151 |  |  [Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification](https://doi.org/10.18653/v1/2022.acl-short.75) |  | 0 | Multi-Label Text Classification (MLTC) is a fundamental and challenging task in natural language processing. Previous studies mainly focus on learning text representation and modeling label correlation but neglect the rich knowledge from the existing similar instances when predicting labels of a specific text. To make up for this oversight, we propose a k... | Ran Wang, Xi'ao Su, Xinyu Dai |  |
| 152 |  |  [NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better](https://doi.org/10.18653/v1/2022.acl-short.76) |  | 0 | Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this... | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 153 |  |  [Adjusting the Precision-Recall Trade-Off with Align-and-Predict Decoding for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-short.77) |  | 0 | Modern writing assistance applications are always equipped with a Grammatical Error Correction (GEC) model to correct errors in user-entered sentences. Different scenarios have varying requirements for correction behavior, e.g., performing more precise corrections (high precision) or providing more candidates for users (high recall). However, previous works... | Houfeng Wang, Xin Sun |  |
| 154 |  |  [On the Effect of Isotropy on VAE Representations of Text](https://doi.org/10.18653/v1/2022.acl-short.78) |  | 0 | Injecting desired geometric properties into text representations has attracted a lot of attention. A property that has been argued for, due to its better utilisation of representation space, is isotropy. In parallel, VAEs have been successful in areas of NLP, but are known for their sub-optimal utilisation of the representation space. To address an aspect of... | Ehsan Shareghi, Lan Zhang, Wray L. Buntine |  |
| 155 |  |  [Efficient Classification of Long Documents Using Transformers](https://doi.org/10.18653/v1/2022.acl-short.79) |  | 0 | Several methods have been proposed for classifying long textual documents using Transformers. However, there is a lack of consensus on a benchmark to enable a fair comparison among different approaches. In this paper, we provide a comprehensive evaluation of the relative efficacy measured against various baselines and diverse datasets — both in terms of... | Hyunji Hayley Park, Kashif Shah, Yogarshi Vyas |  |
| 156 |  |  [Rewarding Semantic Similarity under Optimized Alignments for AMR-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-short.80) |  | 0 | A common way to combat exposure bias is by applying scores from evaluation metrics as rewards in reinforcement learning (RL). Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards. However, metrics such as BERTScore greedily align candidate and reference tokens, which can... | Daniel Gildea, Lisa Jin |  |
| 157 |  |  [An Analysis of Negation in Natural Language Understanding Corpora](https://doi.org/10.18653/v1/2022.acl-short.81) |  | 0 | This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show... | Dhivya Chinnappa, Eduardo Blanco, Md Mosharaf Hossain |  |
| 158 |  |  [Primum Non Nocere: Before working with Indigenous data, the ACL must confront ongoing colonialism](https://doi.org/10.18653/v1/2022.acl-short.82) |  | 0 | In this paper, we challenge the ACL community to reckon with historical and ongoing colonialism by adopting a set of ethical obligations and best practices drawn from the Indigenous studies literature. While the vast majority of NLP research focuses on a very small number of very high resource languages (English, Chinese, etc), some work has begun to engage... | Lane Schwartz |  |
| 159 |  |  [Unsupervised multiple-choice question generation for out-of-domain Q&A fine-tuning](https://doi.org/10.18653/v1/2022.acl-short.83) |  | 0 | Pre-trained models have shown very good performances on a number of question answering benchmarks especially when fine-tuned on multiple question answering datasets at once. In this work, we propose an approach for generating a fine-tuning dataset thanks to a rule-based algorithm that generates questions and answers from unannotated sentences. We show that the... | Christophe Cerisara, Guillaume Le Berre, Guy Lapalme, Philippe Langlais |  |
| 160 |  |  [Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models](https://doi.org/10.18653/v1/2022.acl-short.84) |  | 0 | Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has... | Ling Liu, Mans Hulden |  |
| 161 |  |  [Probing the Robustness of Trained Metrics for Conversational Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-short.85) |  | 0 | This paper introduces an adversarial method to stress-test trained metrics for the evaluation of conversational dialogue systems. The method leverages Reinforcement Learning to find response strategies that elicit optimal scores from the trained metrics. We apply our method to test recently proposed trained metrics. We find that they all are susceptible to... | Don Tuggener, Jan Deriu, Mark Cieliebak, Pius von Däniken |  |
| 162 |  |  [Rethinking and Refining the Distinct Metric](https://doi.org/10.18653/v1/2022.acl-short.86) |  | 0 | Distinct is a widely used automatic metric for evaluating diversity in language generation tasks. However, we observed that the original approach to calculating distinct scores has evident biases that tend to assign higher penalties to longer sequences. We refine the calculation of distinct scores by scaling the number of distinct tokens based on their... | Minlie Huang, Pei Ke, Sahand Sabour, Siyang Liu, Xiaoyan Zhu, Yinhe Zheng |  |
| 163 |  |  [How reparametrization trick broke differentially-private text representation learning](https://doi.org/10.18653/v1/2022.acl-short.87) |  | 0 | As privacy gains traction in the NLP community, researchers have started adopting various approaches to privacy-preserving methods. One of the favorite privacy frameworks, differential privacy (DP), is perhaps the most compelling thanks to its fundamental theoretical guarantees. Despite the apparent simplicity of the general concept of differential privacy, it... | Ivan Habernal |  |
| 164 |  |  [Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution](https://doi.org/10.18653/v1/2022.acl-short.88) |  | 0 | We consider the task of document-level entity linking (EL), where it is important to make consistent decisions for entity mentions over the full document jointly. We aim to leverage explicit “connections” among mentions within the document itself: we propose to join EL and coreference resolution (coref) in a single structured prediction task over directed trees... | Chris Develder, Johannes Deleu, Klim Zaporojets, Thomas Demeester, Yiwei Jiang |  |
| 165 |  |  [A Flexible Multi-Task Model for BERT Serving](https://doi.org/10.18653/v1/2022.acl-short.89) |  | 0 | We present an efficient BERT-based multi-task (MT) framework that is particularly suitable for iterative and incremental development of the tasks. The proposed framework is based on the idea of partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the other layers frozen. For each task, we train independently a single-task (ST) model using... | Jianwei Qi, Shenghuan He, Tianwen Wei |  |
| 166 |  |  [Understanding Game-Playing Agents with Natural Language Annotations](https://doi.org/10.18653/v1/2022.acl-short.90) |  | 0 | We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of... | Andre He, Dan Klein, Nicholas Tomlin |  |
| 167 |  |  [Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding](https://doi.org/10.18653/v1/2022.acl-short.91) |  | 0 | Automatic ICD coding is defined as assigning disease codes to electronic medical records (EMRs).Existing methods usually apply label attention with code representations to match related text snippets. Unlike these works that model the label with the code hierarchy or description, we argue that the code synonyms can provide more comprehensive knowledge based on... | Chuanqi Tan, Songfang Huang, Zheng Yuan |  |
| 168 |  |  [CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment](https://doi.org/10.18653/v1/2022.acl-short.92) |  | 0 | Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task... | Hinrich Schütze, Lütfi Kerem Senel, Timo Schick |  |
| 169 |  |  [On the Importance of Effectively Adapting Pretrained Language Models for Active Learning](https://doi.org/10.18653/v1/2022.acl-short.93) |  | 0 | Recent active learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing... | Katerina Margatina, Loïc Barrault, Nikolaos Aletras |  |
| 170 |  |  [A Recipe for Arbitrary Text Style Transfer with Large Language Models](https://doi.org/10.18653/v1/2022.acl-short.94) |  | 0 | In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot... | Andy Coenen, Ann Yuan, Chris CallisonBurch, Daphne Ippolito, Emily Reif, Jason Wei |  |
| 171 |  |  [DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.acl-short.95) |  | 0 | Our goal is to study the novel task of distant supervision for multilingual relation extraction (Multi DS-RE). Research in Multi DS-RE has remained limited due to the absence of a reliable benchmarking dataset. The only available dataset for this task, RELX-Distant (Köksal and Özgür, 2020), displays several unrealistic characteristics, leading to a systematic... | Abhyuday Bhartiya, Kartikeya Badola, Mausam |  |
| 172 |  |  [(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance](https://doi.org/10.18653/v1/2022.acl-short.96) |  | 0 | In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON’s shared-tasks. With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided. In this work, we... | David Guriel, Omer Goldman, Reut Tsarfaty |  |
| 173 |  |  [Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks](https://doi.org/10.18653/v1/2022.acl-short.97) |  | 0 | Before entering the neural network, a token needs to be converted to its one-hot representation, which is a discrete distribution of the vocabulary. Smoothed representation is the probability of candidate tokens obtained from the pre-trained masked language model, which can be seen as a more informative augmented substitution to the one-hot representation. We... | Chaochen Gao, Liangjun Zang, Meng Lin, Songlin Hu, Xing Wu |  |
| 174 |  |  [Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022](https://aclanthology.org/volumes/2022.findings-acl/) |  | 0 |  | Aline Villavicencio, Preslav Nakov, Smaranda Muresan |  |
| 175 |  |  [Frontmatter](https://aclanthology.org/2022.findings-acl.0) |  | 0 |  |  |  |
| 176 |  |  ["Is Whole Word Masking Always Better for Chinese BERT?": Probing on Chinese Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-acl.1) |  | 0 | Whole word masking (WWM), which masks all subwords corresponding to a word at once, makes a better English BERT model. For the Chinese language, however, there is no subword because each token is an atomic character. The meaning of a word in Chinese is different in that a word is a compositional unit consisting of multiple characters. Such difference motivates... | Cong Zhou, Duyu Tang, Enbo Zhao, Linyang Li, Piji Li, Xipeng Qiu, Yong Dai, Zhangyin Feng |  |
| 177 |  |  [Compilable Neural Code Generation with Compiler Feedback](https://doi.org/10.18653/v1/2022.findings-acl.2) |  | 0 | Automatically generating compilable programs with (or without) natural language descriptions has always been a touchstone problem for computational linguistics and automated software engineering. Existing deep-learning approaches model code generation as text generation, either constrained by grammar structures in decoder, or driven by pre-trained language... | Fei Mi, Hao Wu, Jin Liu, Pingyi Zhou, Qun Liu, Xin Jiang, Xin Wang, Yao Wan, Yasheng Wang, Yitong Li |  |
| 178 |  |  [Towards Unifying the Label Space for Aspect- and Sentence-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.3) |  | 0 | The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims to determine the sentiment polarity towards targeted aspect terms occurring in the sentence. The development of the ABSA task is very much hindered by the lack of annotated data. To tackle this, the prior works have studied the possibility of utilizing the sentiment analysis (SA)... | Junbo Zhao, Min Zhang, Sai Wu, Yiming Zhang |  |
| 179 |  |  [Input-specific Attention Subnetworks for Adversarial Detection](https://doi.org/10.18653/v1/2022.findings-acl.4) |  | 0 | Self-attention heads are characteristic of Transformer models and have been well studied for interpretability and pruning. In this work, we demonstrate an altogether different utility of attention heads, namely for adversarial detection. Specifically, we propose a method to construct input-specific attention subnetworks (IAS) from which we extract three... | Anirudh Sriram, Emil Biju, Mitesh M. Khapra, Pratyush Kumar |  |
| 180 |  |  [RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction](https://doi.org/10.18653/v1/2022.findings-acl.5) |  | 0 | Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet... | Lidong Bing, Luo Si, Soujanya Poria, Yew Ken Chia |  |
| 181 |  |  [Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?](https://doi.org/10.18653/v1/2022.findings-acl.6) |  | 0 | What can pre-trained multilingual sequence-to-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: (1) the amount of fine-tuning data, (2) the noise in the fine-tuning data, (3) the amount of pre-training data in the model, (4) the... | Arya McCarthy, David Ifeoluwa Adelani, EnShiun Annie Lee, Ruisi Su, Sarubi Thillainathan, Shravan Nayak, Surangika Ranathunga |  |
| 182 |  |  [Multi-Scale Distribution Deep Variational Autoencoder for Explanation Generation](https://doi.org/10.18653/v1/2022.findings-acl.7) |  | 0 | Generating explanations for recommender systems is essential for improving their transparency, as users often wish to understand the reason for receiving a specified recommendation. Previous methods mainly focus on improving the generation quality, but often produce generic explanations that fail to incorporate user and item specific details. To resolve this... | Fei Sun, Gerard de Melo, Liang He, Linlin Wang, Zefeng Cai |  |
| 183 |  |  [Dual Context-Guided Continuous Prompt Tuning for Few-Shot Learning](https://doi.org/10.18653/v1/2022.findings-acl.8) |  | 0 | Prompt-based paradigm has shown its competitive performance in many NLP tasks. However, its success heavily depends on prompt design, and the effectiveness varies upon the model and training data. In this paper, we propose a novel dual context-guided continuous prompt (DCCP) tuning method. To explore the rich contextual information in language structure and... | Houjin Yu, Hui Su, Jie Zhou, Jie Zhou, Le Tian, Xiao Zhou |  |
| 184 |  |  [Extract-Select: A Span Selection Framework for Nested Named Entity Recognition with Generative Adversarial Training](https://doi.org/10.18653/v1/2022.findings-acl.9) |  | 0 | Nested named entity recognition (NER) is a task in which named entities may overlap with each other. Span-based approaches regard nested NER as a two-stage span enumeration and classification task, thus having the innate ability to handle this task. However, they face the problems of error propagation, ignorance of span boundary, difficulty in long entity... | Minghao Hu, Peixin Huang, Weidong Xiao, Xiang Zhao, Xinyi Li, Yang Fang |  |
| 185 |  |  [Controlled Text Generation Using Dictionary Prior in Variational Autoencoders](https://doi.org/10.18653/v1/2022.findings-acl.10) |  | 0 | While variational autoencoders (VAEs) have been widely applied in text generation tasks, they are troubled by two challenges: insufficient representation capacity and poor controllability. The former results from the posterior collapse and restrictive assumption, which impede better representation learning. The latter arises as continuous latent variables in... | DitYan Yeung, Jian Li, Lifeng Shang, Qun Liu, Xianghong Fang, Xin Jiang |  |
| 186 |  |  [Challenges to Open-Domain Constituency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.11) |  | 0 | Neural constituency parsers have reached practical performance on news-domain benchmarks. However, their generalization ability to other domains remains weak. Existing findings on cross-domain constituency parsing are only made on a limited number of domains. Tracking this, we manually annotate a high-quality constituency treebank containing five domains. We... | Di Wu, Leyang Cui, Ruoxi Ning, Sen Yang, Yue Zhang |  |
| 187 |  |  [Going "Deeper": Structured Sememe Prediction via Transformer with Tree Attention](https://doi.org/10.18653/v1/2022.findings-acl.12) |  | 0 | Sememe knowledge bases (SKBs), which annotate words with the smallest semantic units (i.e., sememes), have proven beneficial to many NLP tasks. Building an SKB is very time-consuming and labor-intensive. Therefore, some studies have tried to automate the building process by predicting sememes for the unannotated words. However, all existing sememe prediction... | Fanchao Qi, Maosong Sun, Yining Ye, Zhiyuan Liu |  |
| 188 |  |  [Table-based Fact Verification with Self-adaptive Mixture of Experts](https://doi.org/10.18653/v1/2022.findings-acl.13) |  | 0 | The table-based fact verification task has recently gained widespread attention and yet remains to be a very challenging problem. It inherently requires informative reasoning over natural language together with different numerical and logical reasoning on tables (e.g., count, superlative, comparative). Considering that, we exploit mixture-of-experts and present... | Ji Wu, Kaiyin Zhou, Xien Liu, Yuxuan Zhou |  |
| 189 |  |  [Investigating Data Variance in Evaluations of Automatic Machine Translation Metrics](https://doi.org/10.18653/v1/2022.findings-acl.14) |  | 0 | Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year’s WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further... | Defu Lian, Guoping Huang, Huayang Li, Jiannan Xiang, Lemao Liu, Shuming Shi, Yahui Liu |  |
| 190 |  |  [Sememe Prediction for BabelNet Synsets using Multilingual and Multimodal Information](https://doi.org/10.18653/v1/2022.findings-acl.15) |  | 0 | In linguistics, a sememe is defined as the minimum semantic unit of languages. Sememe knowledge bases (KBs), which are built by manually annotating words with sememes, have been successfully applied to various NLP tasks. However, existing sememe KBs only cover a few languages, which hinders the wide utilization of sememes. To address this issue, the task of... | Chuancheng Lv, Fanchao Qi, HaiTao Zheng, Maosong Sun, Xiaojun Meng, Zhiyuan Liu |  |
| 191 |  |  [Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding](https://doi.org/10.18653/v1/2022.findings-acl.16) |  | 0 | Event extraction is typically modeled as a multi-class classification problem where event types and argument roles are treated as atomic symbols. These approaches are usually limited to a set of pre-defined types. We propose a novel event extraction framework that uses event types and argument roles as natural language queries to extract candidate triggers and... | Lichao Sun, Lifu Huang, Mo Yu, Shiyu Chang, Sijia Wang |  |
| 192 |  |  [LEVEN: A Large-Scale Chinese Legal Event Detection Dataset](https://doi.org/10.18653/v1/2022.findings-acl.17) |  | 0 | Recognizing facts is the most fundamental step in making judgments, hence detecting events in the legal documents is important to legal case analysis tasks. However, existing Legal Event Detection (LED) datasets only concern incomprehensive event types and have limited annotated data, which restricts the development of LED methods and their downstream... | Chaojun Xiao, Cunchao Tu, Feng Yao, Juanzi Li, Lei Hou, Maosong Sun, Weixing Shen, Xiaozhi Wang, Yun Liu, Zhiyuan Liu |  |
| 193 |  |  [Analyzing Dynamic Adversarial Training Data in the Limit](https://doi.org/10.18653/v1/2022.findings-acl.18) |  | 0 | To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown... | Adina Williams, Douwe Kiela, Eric Wallace, Robin Jia |  |
| 194 |  |  [AbductionRules: Training Transformers to Explain Unexpected Inputs](https://doi.org/10.18653/v1/2022.findings-acl.19) |  | 0 | Transformers have recently been shown to be capable of reliably performing logical reasoning over facts and rules expressed in natural language, but abductive reasoning - inference to the best explanation of an unexpected observation - has been underexplored despite significant applications to scientific discovery, common-sense reasoning, and model... | Joshua Bensemann, Michael Witbrock, Nathan Young, Qiming Bao |  |
| 195 |  |  [On the Importance of Data Size in Probing Fine-tuned Models](https://doi.org/10.18653/v1/2022.findings-acl.20) |  | 0 | Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of... | Houman Mehrafarin, Mohammad Taher Pilehvar, Sara Rajaee |  |
| 196 |  |  [RuCCoN: Clinical Concept Normalization in Russian](https://doi.org/10.18653/v1/2022.findings-acl.21) |  | 0 | We present RuCCoN, a new dataset for clinical concept normalization in Russian manually annotated by medical professionals. It contains over 16,028 entity mentions manually linked to over 2,409 unique concepts from the Russian language part of the UMLS ontology. We provide train/test splits for different settings (stratified, zero-shot, and CUI-less) and... | Alexandr Nesterov, Andrey Chertok, Anton Alekseev, Artem Shelmanov, Elena Tutubalina, Galina Zubkova, Manvel Avetisian, Sergey I. Nikolenko, Vladimir Kokh, Zulfat Miftahutdinov |  |
| 197 |  |  [A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings](https://doi.org/10.18653/v1/2022.findings-acl.22) |  | 0 | Contrastive learning has shown great potential in unsupervised sentence embedding tasks, e.g., SimCSE (CITATION).However, these existing solutions are heavily affected by superficial features like the length of sentences or syntactic structures. In this paper, we propose a semantic-aware contrastive learning framework for sentence embeddings, termed... | Han Wu, Haochen Tan, Ke Yang, Linqi Song, Wei Shao |  |
| 198 |  |  [Eider: Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion](https://doi.org/10.18653/v1/2022.findings-acl.23) |  | 0 | Document-level relation extraction (DocRE) aims to extract semantic relations among entity pairs in a document. Typical DocRE methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an... | Jiaming Shen, Jiawei Han, Sha Li, Yiqing Xie, Yuning Mao |  |
| 199 |  |  [Meta-XNLG: A Meta-Learning Approach Based on Language Clustering for Zero-Shot Cross-Lingual Transfer and Generation](https://doi.org/10.18653/v1/2022.findings-acl.24) |  | 0 | Recently, the NLP community has witnessed a rapid advancement in multilingual and cross-lingual transfer research where the supervision is transferred from high-resource languages (HRLs) to low-resource languages (LRLs). However, the cross-lingual transfer is not uniform across languages, particularly in the zero-shot setting. Towards this goal, one promising... | Kaushal Kumar Maurya, Maunendra Sankar Desarkar |  |
| 200 |  |  [MR-P: A Parallel Decoding Algorithm for Iterative Refinement Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.findings-acl.25) |  | 0 | Non-autoregressive translation (NAT) predicts all the target tokens in parallel and significantly speeds up the inference process. The Conditional Masked Language Model (CMLM) is a strong baseline of NAT. It decodes with the Mask-Predict algorithm which iteratively refines the output. Most works about CMLM focus on the model structure and the training... | Hao Cheng, Zhihua Zhang |  |
| 201 |  |  [Open Relation Modeling: Learning to Define Relations between Entities](https://doi.org/10.18653/v1/2022.findings-acl.26) |  | 0 | Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this... | Jie Huang, Jinjun Xiong, Kevin Chang, WenMei Hwu |  |
| 202 |  |  [A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots](https://doi.org/10.18653/v1/2022.findings-acl.27) |  | 0 | A slot value might be provided segment by segment over multiple-turn interactions in a dialog, especially for some important information such as phone numbers and names. It is a common phenomenon in daily life, but little attention has been paid to it in previous work. To fill the gap, this paper defines a new task named Sub-Slot based Task-Oriented Dialog... | Caixia Yuan, Jiaman Wu, Jian Sun, Sai Zhang, Xiaojie Wang, Yongbin Li, Yuchuan Wu, Yuwei Hu |  |
| 203 |  |  [Towards Transparent Interactive Semantic Parsing via Step-by-Step Correction](https://doi.org/10.18653/v1/2022.findings-acl.28) |  | 0 | Existing studies on semantic parsing focus on mapping a natural-language utterance to a logical form (LF) in one turn. However, because natural language may contain ambiguity and variability, this is a difficult challenge. In this work, we investigate an interactive semantic parsing framework that explains the predicted LF step by step in natural language and... | Ashley Lewis, Huan Sun, Lingbo Mo, Michael White |  |
| 204 |  |  [MINER: Multi-Interest Matching Network for News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.29) |  | 0 | Personalized news recommendation is an essential technique to help users find interested news. Accurately matching user’s interests and candidate news is the key to news recommendation. Most existing methods learn a single user embedding from user’s historical behaviors to represent the reading interest. However, user interest is usually diverse and may not be... | Guohao Cai, Jian Li, Jieming Zhu, Lifeng Shang, Qiwei Bi, Qun Liu, Xin Jiang, Zhenhua Dong |  |
| 205 |  |  [KSAM: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding](https://doi.org/10.18653/v1/2022.findings-acl.30) |  | 0 | Knowledge-enhanced methods have bridged the gap between human beings and machines in generating dialogue responses. However, most previous works solely seek knowledge from a single source, and thus they often fail to obtain available knowledge because of the insufficient coverage of a single knowledge source. To this end, infusing knowledge from multiple... | Dawei Zhang, Sixing Wu, Ying Li, Zhonghai Wu |  |
| 206 |  |  [Towards Responsible Natural Language Annotation for the Varieties of Arabic](https://doi.org/10.18653/v1/2022.findings-acl.31) |  | 0 | When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for... | A. Stevie Bergman, Mona T. Diab |  |
| 207 |  |  [Dynamically Refined Regularization for Improving Cross-corpora Hate Speech Detection](https://doi.org/10.18653/v1/2022.findings-acl.32) |  | 0 | Hate speech classifiers exhibit substantial performance degradation when evaluated on datasets different from the source. This is due to learning spurious correlations between words that are not necessarily relevant to hateful language, and hate speech labels from the training corpus. Previous work has attempted to mitigate this problem by regularizing specific... | Dominique Fohr, Irina Illina, Nikolaos Aletras, Tulika Bose |  |
| 208 |  |  [Towards Large-Scale Interpretable Knowledge Graph Reasoning for Dialogue Systems](https://doi.org/10.18653/v1/2022.findings-acl.33) |  | 0 | Users interacting with voice assistants today need to phrase their requests in a very specific manner to elicit an appropriate response. This limits the user experience, and is partly due to the lack of reasoning capabilities of dialogue platforms and the hand-crafted rules that require extensive labor. One possible solution to improve user experience and... | Alessandra Cervone, Maryam FazelZarandi, Qiaozi Gao, Sajjad Beygi, William Yang Wang, YiLin Tuan |  |
| 209 |  |  [MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction](https://doi.org/10.18653/v1/2022.findings-acl.34) |  | 0 | Keyphrase extraction (KPE) automatically extracts phrases in a document that provide a concise summary of the core content, which benefits downstream information retrieval and NLP tasks. Previous state-of-the-art methods select candidate keyphrases based on the similarity between learned representations of the candidates and the document. They suffer... | Bing Li, Chong Deng, Linhan Zhang, Qian Chen, Shiliang Zhang, Wei Wang, Wen Wang, Xin Cao |  |
| 210 |  |  [Visualizing the Relationship Between Encoded Linguistic Information and Task Performance](https://doi.org/10.18653/v1/2022.findings-acl.35) |  | 0 | Probing is popular to analyze whether linguistic information can be captured by a well-trained deep neural model, but it is hard to answer how the change of the encoded linguistic information will affect task performance. To this end, we study the dynamic relationship between the encoded linguistic information and task performance from the viewpoint of Pareto... | Defu Lian, Guoping Huang, Huayang Li, Jiannan Xiang, Lemao Liu, Taro Watanabe |  |
| 211 |  |  [Efficient Argument Structure Extraction with Transfer Learning and Active Learning](https://doi.org/10.18653/v1/2022.findings-acl.36) |  | 0 | The automation of extracting argument structures faces a pair of challenges on (1) encoding long-term contexts to facilitate comprehensive understanding, and (2) improving data efficiency since constructing high-quality argument structures is time-consuming. In this work, we propose a novel context-aware Transformer-based argument structure prediction model... | Lu Wang, Xinyu Hua |  |
| 212 |  |  [Plug-and-Play Adaptation for Continuously-updated QA](https://doi.org/10.18653/v1/2022.findings-acl.37) |  | 0 | Language models (LMs) have shown great potential as implicit knowledge bases (KBs). And for their practical use, knowledge in LMs need to be updated periodically. However, existing tasks to assess LMs’ efficacy as KBs do not adequately consider multiple large-scale updates. To this end, we first propose a novel task—Continuously-updated QA (CuQA)—in which... | Hwaran Lee, Joonsuk Park, Kyungjae Lee, SangWoo Lee, Seungwon Hwang, Wookje Han |  |
| 213 |  |  [Reinforced Cross-modal Alignment for Radiology Report Generation](https://doi.org/10.18653/v1/2022.findings-acl.38) |  | 0 | Medical images are widely used in clinical decision-making, where writing radiology reports is a potential application that can be enhanced by automatic solutions to alleviate physicians’ workload. In general, radiology report generation is an image-text task, where cross-modal mappings between images and texts play an important role in generating high-quality... | Han Qin, Yan Song |  |
| 214 |  |  [What Works and Doesn't Work, A Deep Decoder for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.39) |  | 0 | Deep learning has demonstrated performance advantages in a wide range of natural language processing tasks, including neural machine translation (NMT). Transformer NMT models are typically strengthened by deeper encoder layers, but deepening their decoder layers usually results in failure. In this paper, we first identify the cause of the failure of the deep... | Eiichiro Sumita, Hai Zhao, Masao Utiyama, Taro Watanabe, Yiran Wang, Zuchao Li |  |
| 215 |  |  [SyMCoM - Syntactic Measure of Code Mixing A Study Of English-Hindi Code-Mixing](https://doi.org/10.18653/v1/2022.findings-acl.40) |  | 0 | Code mixing is the linguistic phenomenon where bilingual speakers tend to switch between two or more languages in conversations. Recent work on code-mixing in computational settings has leveraged social media code mixed texts to train NLP models. For capturing the variety of code mixing in, and across corpus, Language ID (LID) tags based measures (CMI) have... | Anmol Goel, Manish Shrivastava, Monojit Choudhury, Ponnurangam Kumaraguru, Prashant Kodali |  |
| 216 |  |  [HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data](https://doi.org/10.18653/v1/2022.findings-acl.41) |  | 0 | A pressing challenge in current dialogue systems is to successfully converse with users on topics with information distributed across different modalities. Previous work in multiturn dialogue systems has primarily focused on either text or table information. In more realistic scenarios, having a joint understanding of both is critical as knowledge is typically... | Kai Nakamura, Sharon Levy, Wenhu Chen, William Yang Wang, YiLin Tuan |  |
| 217 |  |  [NEWTS: A Corpus for News Topic-Focused Summarization](https://doi.org/10.18653/v1/2022.findings-acl.42) |  | 0 | Text summarization models are approaching human levels of fidelity. Existing benchmarking corpora provide concordant pairs of full and abridged versions of Web, news or professional content. To date, all summarization datasets operate under a one-size-fits-all paradigm that may not reflect the full range of organic summarization needs. Several recently proposed... | Carsten Eickhoff, Seyed Ali Bahrainian, Sheridan Feucht |  |
| 218 |  |  [Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis](https://doi.org/10.18653/v1/2022.findings-acl.43) |  | 0 | Reddit is home to a broad spectrum of political activity, and users signal their political affiliations in multiple ways—from self-declarations to community participation. Frequently, computational studies have treated political users as a single bloc, both in developing models to infer political leaning and in studying political behavior. Here, we test this... | Bohan Zhang, David Jurgens, Kenan Alkiek |  |
| 219 |  |  [Toward More Meaningful Resources for Lower-resourced Languages](https://doi.org/10.18653/v1/2022.findings-acl.44) |  | 0 | In this position paper, we describe our perspective on how meaningful resources for lower-resourced languages should be developed in connection with the speakers of those languages. Before advancing that position, we first examine two massively multilingual resources used in language technology development, identifying shortcomings that limit their usefulness.... | Chester PalenMichel, Constantine Lignos, Jonne Sälevä, Nolan Holley |  |
| 220 |  |  [Better Quality Estimation for Low Resource Corpus Mining](https://doi.org/10.18653/v1/2022.findings-acl.45) |  | 0 | Quality Estimation (QE) models have the potential to change how we evaluate and maybe even train machine translation models. However, these models still lack the robustness to achieve general adoption. We show that Stateof-the-art QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform unexpectedly bad due to a lack of robustness to... | Derry Wijaya, Jiho Lee, Muhammed Yusuf Kocyigit |  |
| 221 |  |  [End-to-End Segmentation-based News Summarization](https://doi.org/10.18653/v1/2022.findings-acl.46) |  | 0 | In this paper, we bring a new way of digesting news content by introducing the task of segmenting a news article into multiple sections and generating the corresponding summary to each section. We make two contributions towards this new task. First, we create and make available a dataset, SegNews, consisting of 27k news articles with sections and aligned... | Chenguang Zhu, Michael Zeng, Yang Liu |  |
| 222 |  |  [Fast Nearest Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.47) |  | 0 | Though nearest neighbor Machine Translation (kNN-MT) (CITATION) has proved to introduce significant performance boosts over standard neural MT systems, it is prohibitively slow since it uses the entire reference corpus as the datastore for the nearest neighbor search. This means each step for each beam in the beam search has to search over the entire reference... | Fei Wu, Jiwei Li, Tianwei Zhang, Xiaofei Sun, Xiaoya Li, Xiayu Zheng, Yuxian Meng |  |
| 223 |  |  [Extracting Latent Steering Vectors from Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.48) |  | 0 | Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different... | Matthew E. Peters, Nishant Subramani, Nivedita Suresh |  |
| 224 |  |  [Domain Generalisation of NMT: Fusing Adapters with Leave-One-Domain-Out Training](https://doi.org/10.18653/v1/2022.findings-acl.49) |  | 0 | Generalising to unseen domains is under-explored and remains a challenge in neural machine translation. Inspired by recent research in parameter-efficient transfer learning from pretrained models, this paper proposes a fusion-based generalisation method that learns to combine domain-specific parameters. We propose a leave-one-domain-out training strategy to... | Dinh Q. Phung, Gholamreza Haffari, Shahram Khadivi, ThuyTrang Vu |  |
| 225 |  |  [Reframing Instructional Prompts to GPTk's Language](https://doi.org/10.18653/v1/2022.findings-acl.50) |  | 0 | What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples... | Chitta Baral, Daniel Khashabi, Hannaneh Hajishirzi, Yejin Choi |  |
| 226 |  |  [Read Top News First: A Document Reordering Approach for Multi-Document News Summarization](https://doi.org/10.18653/v1/2022.findings-acl.51) |  | 0 | A common method for extractive multi-document news summarization is to re-formulate it as a single-document summarization problem by concatenating all documents as a single meta-document. However, this method neglects the relative importance of documents. We propose a simple approach to reorder the documents according to their relative importance before... | Chao Zhao, Kathleen R. McKeown, Muthu Kumar Chandrasekaran, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury, Tenghao Huang |  |
| 227 |  |  [Human Language Modeling](https://doi.org/10.18653/v1/2022.findings-acl.52) |  | 0 | Natural language is generated by people, yet traditional language modeling views words or documents as if generated independently. Here, we propose human language modeling (HuLM), a hierarchical extension to the language modeling problem where by a human- level exists to connect sequences of documents (e.g. social media messages) and capture the notion that... | H. Andrew Schwartz, Matthew Matero, Nikita Soni, Niranjan Balasubramanian |  |
| 228 |  |  [Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging](https://doi.org/10.18653/v1/2022.findings-acl.53) |  | 0 | Prompting methods recently achieve impressive success in few-shot learning. These methods modify input samples with prompt sentence pieces, and decode label tokens to map samples to corresponding labels. However, such a paradigm is very inefficient for the task of slot tagging. Since slot tagging samples are multiple consecutive words in a sentence, the... | Bohan Li, Cheng Chen, Wanxiang Che, Xianzhen Luo, Yutai Hou |  |
| 229 |  |  [Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding](https://doi.org/10.18653/v1/2022.findings-acl.54) |  | 0 | Decoding language from non-invasive brain activity has attracted increasing attention from both researchers in neuroscience and natural language processing. Due to the noisy nature of brain recordings, existing work has simplified brain-to-word decoding as a binary classification task which is to discriminate a brain signal between its corresponding word and a... | Chengqing Zong, Jiajun Zhang, Shaonan Wang, Shuxian Zou |  |
| 230 |  |  [Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://doi.org/10.18653/v1/2022.findings-acl.55) |  | 0 | Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without... | Apurv Verma, Aram Galstyan, Greg Ver Steeg, Jwala Dhamala, KaiWei Chang, Rahul Gupta, Satyapriya Krishna, Umang Gupta, Varun Kumar, Yada Pruksachatkun |  |
| 231 |  |  [Domain Representative Keywords Selection: A Probabilistic Approach](https://doi.org/10.18653/v1/2022.findings-acl.56) |  | 0 | We propose a probabilistic approach to select a subset of a target domain representative keywords from a candidate set, contrasting with a context domain. Such a task is crucial for many downstream tasks in natural language processing. To contrast the target domain and the context domain, we adapt the two-component mixture model concept to generate a... | ChengXiang Zhai, Jie Huang, Kevin ChenChuan Chang, Lucian Popa, Pritom Saha Akash, Yunyao Li |  |
| 232 |  |  [Hierarchical Inductive Transfer for Continual Dialogue Learning](https://doi.org/10.18653/v1/2022.findings-acl.57) |  | 0 | Pre-trained models have achieved excellent performance on the dialogue task. However, for the continual increase of online chit-chat scenarios, directly fine-tuning these models for each of the new tasks not only explodes the capacity of the dialogue system on the embedded devices but also causes knowledge forgetting on pre-trained models and knowledge... | Kan Li, Shaoxiong Feng, Xu Sun, Xuancheng Ren |  |
| 233 |  |  [Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation](https://doi.org/10.18653/v1/2022.findings-acl.58) |  | 0 | Current language generation models suffer from issues such as repetition, incoherence, and hallucinations. An often-repeated hypothesis for this brittleness of generation models is that it is caused by the training and the generation procedure mismatch, also referred to as exposure bias. In this paper, we verify this hypothesis by analyzing exposure bias from... | Hareesh Bahuleyan, Jackie Chi Kit Cheung, Kushal Arora, Layla El Asri |  |
| 234 |  |  [Question Answering Infused Pre-training of General-Purpose Contextualized Representations](https://doi.org/10.18653/v1/2022.findings-acl.59) |  | 0 | We propose a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. To this end, we train a bi-encoder QA model, which independently encodes passages and... | Luke Zettlemoyer, Mike Lewis, Robin Jia |  |
| 235 |  |  [Automatic Song Translation for Tonal Languages](https://doi.org/10.18653/v1/2022.findings-acl.60) |  | 0 | This paper develops automatic song translation (AST) for tonal languages and addresses the unique challenge of aligning words’ tones with melody of a song in addition to conveying the original meaning. We propose three criteria for effective AST—preserving meaning, singability and intelligibility—and design metrics for these criteria. We develop a new benchmark... | Chen Zhang, Fenfei Guo, Jordan L. BoydGraber, Jun Xie, Kejun Zhang, Qixin He, Zhirui Zhang |  |
| 236 |  |  [Read before Generate! Faithful Long Form Question Answering with Machine Reading](https://doi.org/10.18653/v1/2022.findings-acl.61) |  | 0 | Long-form question answering (LFQA) aims to generate a paragraph-length answer for a given question. While current work on LFQA using large pre-trained model for generation are effective at producing fluent and somewhat relevant content, one primary challenge lies in how to generate a faithful answer that has less hallucinated content. We propose a new... | Dan Su, Jindi Zhang, Lifeng Shang, Pascale Fung, Qun Liu, Xiaoguang Li, Xin Jiang |  |
| 237 |  |  [A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.62) |  | 0 | Few-Shot Relation Extraction aims at predicting the relation for a pair of entities in a sentence by training with a few labelled examples in each relation. Some recent works have introduced relation information (i.e., relation labels or descriptions) to assist model learning based on Prototype Network. However, most of them constrain the prototypes of each... | Jinpeng Hu, TsungHui Chang, Xiang Wan, Yang Liu |  |
| 238 |  |  [MIMICause: Representation and automatic extraction of causal relation types from clinical notes](https://doi.org/10.18653/v1/2022.findings-acl.63) |  | 0 | Understanding causal narratives communicated in clinical notes can help make strides towards personalized healthcare. Extracted causal information from clinical notes can be combined with structured EHR data such as patients’ demographics, diagnoses, and medications. This will enhance healthcare providers’ ability to identify aspects of a patient’s story... | Andrew E. Fano, Bogdan Sacaleanu, Jessica Huber, Md Imbesat Hassan Rizvi, Paige Bartusiak, Vivek Khetan |  |
| 239 |  |  [Compressing Sentence Representation for Semantic Retrieval via Homomorphic Projective Distillation](https://doi.org/10.18653/v1/2022.findings-acl.64) |  | 0 | How to learn highly compact yet effective sentence representation? Pre-trained language models have been effective in many NLP tasks. However, these models are often huge and produce large sentence embeddings. Moreover, there is a big performance gap between large and small models. In this paper, we propose Homomorphic Projective Distillation (HPD) to learn... | Lei Li, Ming Wu, Xuandong Zhao, Zhiguo Yu |  |
| 240 |  |  [Debiasing Event Understanding for Visual Commonsense Tasks](https://doi.org/10.18653/v1/2022.findings-acl.65) |  | 0 | We study event understanding as a critical step towards visual commonsense tasks. Meanwhile, we argue that current object-based event understanding is purely likelihood-based, leading to incorrect event prediction, due to biased correlation between events and objects. We propose to mitigate such biases with do-calculus, proposed in causality research, but... | Bei Liu, Minji Seo, Seungtaek Choi, Seungwon Hwang, YeonJoon Jung |  |
| 241 |  |  [Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs](https://doi.org/10.18653/v1/2022.findings-acl.66) |  | 0 | Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless,... | Adam Jatowt, Hongru Liang, Peiyao Li, Yao Zhang, Zhenglu Yang |  |
| 242 |  |  [DeepStruct: Pretraining of Language Models for Structure Prediction](https://doi.org/10.18653/v1/2022.findings-acl.67) |  | 0 | We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned... | Chenguang Wang, Dawn Song, Haoyun Hong, Jie Tang, Xiao Liu, Zui Chen |  |
| 243 |  |  [The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error](https://doi.org/10.18653/v1/2022.findings-acl.68) |  | 0 | Discourse analysis allows us to attain inferences of a text document that extend beyond the sentence-level. The current performance of discourse models is very low on texts outside of the training distribution’s coverage, diminishing the practical utility of existing models. There is need for a measure that can inform us to what extent our model generalizes... | Anthony Sicilia, Katherine Atwell, Malihe Alikhani, Seong Jae Hwang |  |
| 244 |  |  [Mukayese: Turkish NLP Strikes Back](https://doi.org/10.18653/v1/2022.findings-acl.69) |  | 0 | Having sufficient resources for language X lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the Turkish language. We demonstrate that languages such as Turkish are left behind the state-of-the-art in NLP applications. As a... | Ali Safaya, Arda Göktogan, Deniz Yüret, Emirhan Kurtulus |  |
| 245 |  |  [Virtual Augmentation Supported Contrastive Learning of Sentence Representations](https://doi.org/10.18653/v1/2022.findings-acl.70) |  | 0 | Despite profound successes, contrastive representation learning relies on carefully designed data augmentations using domain-specific knowledge. This challenge is magnified in natural language processing, where no general rules exist for data augmentation due to the discrete nature of natural language. We tackle this challenge by presenting a Virtual... | Andrew O. Arnold, Dejiao Zhang, Henghui Zhu, Wei Xiao, Xiaofei Ma |  |
| 246 |  |  [MoEfication: Transformer Feed-forward Layers are Mixtures of Experts](https://doi.org/10.18653/v1/2022.findings-acl.71) |  | 0 | Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This... | Jie Zhou, Maosong Sun, Peng Li, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu |  |
| 247 |  |  [DS-TOD: Efficient Domain Specialization for Task-Oriented Dialog](https://doi.org/10.18653/v1/2022.findings-acl.72) |  | 0 | Recent work has shown that self-supervised dialog-specific pretraining on large conversational datasets yields substantial gains over traditional language modeling (LM) pretraining in downstream task-oriented dialog (TOD). These approaches, however, exploit general dialogic corpora (e.g., Reddit) and thus presumably fail to reliably embed domain-specific... | Anne Lauscher, ChiaChien Hung, Goran Glavas, Simone Paolo Ponzetto |  |
| 248 |  |  [Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-acl.73) |  | 0 | Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We... | Hai Zhao, Jiayi Wang, Rongzhou Bao, Zhuosheng Zhang |  |
| 249 |  |  [Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns](https://doi.org/10.18653/v1/2022.findings-acl.74) |  | 0 | We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied... | Ani Nenkova, Handong Zhao, Jason Kuen, Jingbo Shang, Jiuxiang Gu, Ruiyi Zhang, Tong Sun, Vlad I. Morariu, Zihan Wang |  |
| 250 |  |  [Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment](https://doi.org/10.18653/v1/2022.findings-acl.75) |  | 0 | Most research on question answering focuses on the pre-deployment stage; i.e., building an accurate model for deployment. In this paper, we ask the question: Can we improve QA systems further post-deployment based on user interactions? We focus on two kinds of improvements: 1) improving the QA system’s performance itself, and 2) providing the model with the... | Jackie Chi Kit Cheung, Prakhar Sharma, Siva Reddy, Xing Han Lù, Zichao Li |  |
| 251 |  |  [To be or not to be an Integer? Encoding Variables for Mathematical Text](https://doi.org/10.18653/v1/2022.findings-acl.76) |  | 0 | The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is... | André Freitas, Deborah Ferreira, Julia Rozanova, Marco Valentino, Mokanarangan Thayaparan |  |
| 252 |  |  [GRS: Combining Generation and Revision in Unsupervised Sentence Simplification](https://doi.org/10.18653/v1/2022.findings-acl.77) |  | 0 | We propose GRS: an unsupervised approach to sentence simplification that combines text generation and text revision. We start with an iterative framework in which an input sentence is revised using explicit edit operations, and add paraphrasing as a new edit operation. This allows us to combine the advantages of generative and revision-based approaches:... | Dhruv Kumar, Lukasz Golab, Mohammad Dehghan |  |
| 253 |  |  [BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages](https://doi.org/10.18653/v1/2022.findings-acl.78) |  | 0 | Morphologically-rich polysynthetic languages present a challenge for NLP systems due to data sparsity, and a common strategy to handle this issue is to apply subword segmentation. We investigate a wide variety of supervised and unsupervised morphological segmentation methods for four polysynthetic languages: Nahuatl, Raramuri, Shipibo-Konibo, and Wixarika.... | Arturo Oncevay, Elisabeth Mager, Katharina Kann, Manuel Mager, Ngoc Thang Vu |  |
| 254 |  |  [Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.79) |  | 0 | We introduce distributed NLI, a new NLU task with a goal to predict the distribution of human judgements for natural language inference. We show that by applying additional distribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation, models can capture human judgement distribution more... | Mohit Bansal, Xiang Zhou, Yixin Nie |  |
| 255 |  |  [Morphological Processing of Low-Resource Languages: Where We Are and What's Next](https://doi.org/10.18653/v1/2022.findings-acl.80) |  | 0 | Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no... | Adam Wiemerslage, Arya McCarthy, Changbing Yang, Eliana Colunga, Garrett Nicolai, Katharina Kann, Miikka Silfverberg |  |
| 256 |  |  [Learning and Evaluating Character Representations in Novels](https://doi.org/10.18653/v1/2022.findings-acl.81) |  | 0 | We address the problem of learning fixed-length vector representations of characters in novels. Recent advances in word embeddings have proven successful in learning entity representations from short texts, but fall short on longer documents because they do not capture full book-level information. To overcome the weakness of such text-based embeddings, we... | Allen Kim, Charuta Pethe, Naoya Inoue, Steven Skiena |  |
| 257 |  |  [Answer Uncertainty and Unanswerability in Multiple-Choice Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.findings-acl.82) |  | 0 | Machine reading comprehension (MRC) has drawn a lot of attention as an approach for assessing the ability of systems to understand natural language. Usually systems focus on selecting the correct answer to a question given a contextual paragraph. However, for many applications of multiple-choice MRC systems there are two additional considerations. For... | Mark J. F. Gales, Vatsal Raina |  |
| 258 |  |  [Measuring the Language of Self-Disclosure across Corpora](https://doi.org/10.18653/v1/2022.findings-acl.83) |  | 0 | Being able to reliably estimate self-disclosure – a key component of friendship and intimacy – from language is important for many psychology studies. We build single-task models on five self-disclosure corpora, but find that these models generalize poorly; the within-domain accuracy of predicted message-level self-disclosure of the best-performing model (mean... | AnnKatrin Reuel, Garrick Sherman, João Sedoc, Lyle H. Ungar, Sebastian Peralta |  |
| 259 |  |  [When Chosen Wisely, More Data Is What You Need: A Universal Sample-Efficient Strategy For Data Augmentation](https://doi.org/10.18653/v1/2022.findings-acl.84) |  | 0 | Data Augmentation (DA) is known to improve the generalizability of deep neural networks. Most existing DA techniques naively add a certain number of augmented samples without considering the quality and the added computational cost of these samples. To tackle this problem, a common strategy, adopted by several state-of-the-art DA methods, is to adaptively... | Ali Ghodsi, Ehsan Kamalloo, Mehdi Rezagholizadeh |  |
| 260 |  |  [Explaining Classes through Stable Word Attributions](https://doi.org/10.18653/v1/2022.findings-acl.85) |  | 0 | Input saliency methods have recently become a popular tool for explaining predictions of deep learning models in NLP. Nevertheless, there has been little work investigating methods for aggregating prediction-level explanations to the class level, nor has a framework for evaluating such class explanations been established. We explore explanations based on XLM-R... | AkiJuhani Kyröläinen, Amanda Myntti, Filip Ginter, Samuel Rönnqvist, Veronika Laippala |  |
| 261 |  |  [What to Learn, and How: Toward Effective Learning from Rationales](https://doi.org/10.18653/v1/2022.findings-acl.86) |  | 0 | Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1)... | Chenhao Tan, Samuel Carton, Surya Kanoria |  |
| 262 |  |  [Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments](https://doi.org/10.18653/v1/2022.findings-acl.87) |  | 0 | Building on current work on multilingual hate speech (e.g., Ousidhoum et al. (2019)) and hate speech reduction (e.g., Sap et al. (2020)), we present XTREMESPEECH, a new hate speech dataset containing 20,297 social media passages from Brazil, Germany, India and Kenya. The key novelty is that we directly involve the affected communities in collecting and... | Antonis Maronikolakis, Axel Wisiorek, Haris Jabbar, Hinrich Schütze, Leah Nann, Sahana Udupa |  |
| 263 |  |  [Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists](https://doi.org/10.18653/v1/2022.findings-acl.88) |  | 0 | Natural Language Processing (NLP) models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation... | Debora Nozza, Dirk Hovy, Elena Baralis, Giuseppe Attanasio |  |
| 264 |  |  [From BERT's Point of View: Revealing the Prevailing Contextual Differences](https://doi.org/10.18653/v1/2022.findings-acl.89) |  | 0 | Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high... | Carolin Schuster, Simon Hegelich |  |
| 265 |  |  [Learning Bias-reduced Word Embeddings Using Dictionary Definitions](https://doi.org/10.18653/v1/2022.findings-acl.90) |  | 0 | Pre-trained word embeddings, such as GloVe, have shown undesirable gender, racial, and religious biases. To address this problem, we propose DD-GloVe, a train-time debiasing algorithm to learn word embeddings by leveraging ̲dictionary ̲definitions. We introduce dictionary-guided loss functions that encourage word embeddings to be similar to their relatively... | Donald Zhang, Haozhe An, Xiaojiang Liu |  |
| 266 |  |  [Knowledge Graph Embedding by Adaptive Limit Scoring Loss Using Dynamic Weighting Strategy](https://doi.org/10.18653/v1/2022.findings-acl.91) |  | 0 | Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for predicting missing links in knowledge graphs. Designing a strong and effective loss framework is essential for knowledge graph embedding models to distinguish between correct and incorrect triplets. The classic margin-based ranking loss... | Bowei Xing, Jinfa Yang, Ruibin Wang, Taiyan Chen, Xianghua Ying, Xin Tong, Yongjie Shi |  |
| 267 |  |  [OCR Improves Machine Translation for Low-Resource Languages](https://doi.org/10.18653/v1/2022.findings-acl.92) |  | 0 | We aim to investigate the performance of current OCR systems on low resource languages and low resource scripts. We introduce and make publicly available a novel benchmark, OCR4MT, consisting of real and synthetic data, enriched with noise, for 60 low-resource languages in low resource scripts. We evaluate state-of-the-art OCR systems on our benchmark and... | Francisco Guzmán, Jean Maillard, Oana Ignat, Vishrav Chaudhary |  |
| 268 |  |  [CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations](https://doi.org/10.18653/v1/2022.findings-acl.93) |  | 0 | Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric... | Changlong Yu, Hongming Zhang, Wilfred Ng, Yangqiu Song |  |
| 269 |  |  [Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming](https://doi.org/10.18653/v1/2022.findings-acl.94) |  | 0 | A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a... | Ayush Maheshwari, Ganesh Ramakrishnan, KrishnaTeja Killamsetty, Lucian Popa, Marina Danilevsky, Rishabh K. Iyer |  |
| 270 |  |  [Multi-Granularity Semantic Aware Graph Model for Reducing Position Bias in Emotion Cause Pair Extraction](https://doi.org/10.18653/v1/2022.findings-acl.95) |  | 0 | The emotion cause pair extraction (ECPE) task aims to extract emotions and causes as pairs from documents. We observe that the relative distance distribution of emotions and causes is extremely imbalanced in the typical ECPE dataset. Existing methods have set a fixed size window to capture relations between neighboring clauses. However, they neglect the... | Lingwei Wei, Qianwen Ma, Songlin Hu, Wei Zhou, Yinan Bao |  |
| 271 |  |  [Cross-lingual Inference with A Chinese Entailment Graph](https://doi.org/10.18653/v1/2022.findings-acl.96) |  | 0 | Predicate entailment detection is a crucial task for question-answering from text, where previous work has explored unsupervised learning of entailment graphs from typed open relation triples. In this paper, we present the first pipeline for building Chinese entailment graphs, which involves a novel high-recall open relation extraction (ORE) method and the... | Liane Guillou, Mark Steedman, Mohammad Javad Hosseini, Sabine Weber, Tianyi Li |  |
| 272 |  |  [Multi-task Learning for Paraphrase Generation With Keyword and Part-of-Speech Reconstruction](https://doi.org/10.18653/v1/2022.findings-acl.97) |  | 0 | Paraphrase generation using deep learning has been a research hotspot of natural language processing in the past few years. While previous studies tackle the problem from different aspects, the essence of paraphrase generation is to retain the key semantics of the source sentence and rewrite the rest of the content. Inspired by this observation, we propose a... | Bei Chen, Xuesong Lu, Xuhang Xie |  |
| 273 |  |  [MDCSpell: A Multi-task Detector-Corrector Framework for Chinese Spelling Correction](https://doi.org/10.18653/v1/2022.findings-acl.98) |  | 0 | Chinese Spelling Correction (CSC) is a task to detect and correct misspelled characters in Chinese texts. CSC is challenging since many Chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use BERT-based language models to directly correct each character of the input sentence. However, these... | Boyu Zhang, Chenxi Zhu, Feng Mao, Ziqiang Ying |  |
| 274 |  |  [S²SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers](https://doi.org/10.18653/v1/2022.findings-acl.99) |  | 0 | The task of converting a natural language question into an executable SQL query, known as text-to-SQL, is an important branch of semantic parsing. The state-of-the-art graph-based encoder has been successfully used in this task but does not model the question syntax well. In this paper, we propose S2SQL, injecting Syntax to question-Schema graph encoder for... | Binyuan Hui, Bowen Li, Bowen Qin, Jian Sun, Lihan Wang, Ruiying Geng, Yanyang Li, Yongbin Li |  |
| 275 |  |  [Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers](https://doi.org/10.18653/v1/2022.findings-acl.100) |  | 0 | This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing re-ranking algorithm that improves overall test structure. Experiments using automatic and... | Mariano Felice, Paula Buttery, Shiva Taslimipoor |  |
| 276 |  |  [Co-training an Unsupervised Constituency Parser with Weak Supervision](https://doi.org/10.18653/v1/2022.findings-acl.101) |  | 0 | We introduce a method for unsupervised parsing that relies on bootstrapping classifiers to identify if a node dominates a specific span in a sentence. There are two types of classifiers, an inside classifier that acts on a span, and an outside classifier that acts on everything outside of a given span. Through self-training and co-training with the two... | Nickil Maveli, Shay B. Cohen |  |
| 277 |  |  [HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information](https://doi.org/10.18653/v1/2022.findings-acl.102) |  | 0 | Transformer-based language models usually treat texts as linear sequences. However, most texts also have an inherent hierarchical structure, i.e., parts of a text can be identified using their position in this hierarchy. In addition, section titles usually indicate the common topic of their respective sentences. We propose a novel approach to formulate,... | Georg Rehm, Malte Ostendorff, Qian Ruan |  |
| 278 |  |  [An Isotropy Analysis in the Multilingual BERT Embedding Space](https://doi.org/10.18653/v1/2022.findings-acl.103) |  | 0 | Several studies have explored various advantages of multilingual pre-trained models (such as multilingual BERT) in capturing shared linguistic knowledge. However, less attention has been paid to their limitations. In this paper, we investigate the multilingual BERT for two known issues of the monolingual models: anisotropic embedding space and outlier... | Mohammad Taher Pilehvar, Sara Rajaee |  |
| 279 |  |  [Multi-Stage Prompting for Knowledgeable Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-acl.104) |  | 0 | Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these... | Bryan Catanzaro, Mohammad Shoeybi, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Zihan Liu |  |
| 280 |  |  [DuReadervis: A Chinese Dataset for Open-domain Document Visual Question Answering](https://doi.org/10.18653/v1/2022.findings-acl.105) |  | 0 | Open-domain question answering has been used in a wide range of applications, such as web search and enterprise search, which usually takes clean texts extracted from various formats of documents (e.g., web pages, PDFs, or Word documents) as the information source. However, designing different text extraction approaches is time-consuming and not scalable. In... | Haifeng Wang, Hongyu Li, Hua Wu, Jing Liu, Le Qi, Qiaoqiao She, Shangwen Lv, Ting Liu, Yu Zhang |  |
| 281 |  |  [Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models](https://doi.org/10.18653/v1/2022.findings-acl.106) |  | 0 | Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions. However,... | Aaron Mueller, Luheng Wang, Robert Frank, Sebastian Schuster, Tal Linzen |  |
| 282 |  |  [C³KG: A Chinese Commonsense Conversation Knowledge Graph](https://doi.org/10.18653/v1/2022.findings-acl.107) |  | 0 | Existing commonsense knowledge bases often organize tuples in an isolated manner, which is deficient for commonsense conversational models to plan the next steps. To fill the gap, we curate a large-scale multi-turn human-written conversation corpus, and create the first Chinese commonsense conversation knowledge graph which incorporates both social commonsense... | Bin Wang, Chen Wei, Dawei Li, Jianwei Cui, Jiayi Zhang, Ke Li, Yanran Li |  |
| 283 |  |  [Graph Neural Networks for Multiparallel Word Alignment](https://doi.org/10.18653/v1/2022.findings-acl.108) |  | 0 | After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word... | Ayyoob Imani, François Yvon, Hinrich Schütze, Lütfi Kerem Senel, Masoud Jalili Sabet |  |
| 284 |  |  [Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors](https://doi.org/10.18653/v1/2022.findings-acl.109) |  | 0 | Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed. However, the performance of the state-of-the-art models decreases sharply when they are deployed in the real world. We find that the main reason is that real-world applications can only access the text outputs by the automatic speech recognition (ASR) models,... | Bing Qin, Hao Yang, Song Chen, Wenting Zhao, Xiaohuan Cao, Yang Wu, Yanyan Zhao |  |
| 285 |  |  [A Novel Framework Based on Medical Concept Driven Attention for Explainable Medical Code Prediction via External Knowledge](https://doi.org/10.18653/v1/2022.findings-acl.110) |  | 0 | Medical code prediction from clinical notes aims at automatically associating medical codes with the clinical notes. Rare code problem, the medical codes with low occurrences, is prominent in medical code prediction. Recent studies employ deep neural networks and the external knowledge to tackle it. However, such approaches lack interpretability which is a... | Chenchen Ye, Deyu Zhou, Junxi Liu, Linhai Zhang, Tao Wang |  |
| 286 |  |  [Effective Unsupervised Constrained Text Generation based on Perturbed Masking](https://doi.org/10.18653/v1/2022.findings-acl.111) |  | 0 | Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in... | Wenjie Ou, Yingwen Fu, Yue Lin, Zhou Yu |  |
| 287 |  |  [Combining (Second-Order) Graph-Based and Headed-Span-Based Projective Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.112) |  | 0 | Graph-based methods, which decompose the score of a dependency tree into scores of dependency arcs, are popular in dependency parsing for decades. Recently, (CITATION) propose a headed-span-based method that decomposes the score of a dependency tree into scores of headed spans. They show improvement over first-order graph-based methods. However, their method... | Kewei Tu, Songlin Yang |  |
| 288 |  |  [End-to-End Speech Translation for Code Switched Speech](https://doi.org/10.18653/v1/2022.findings-acl.113) |  | 0 | Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and... | Christian Gollan, Dominic Telaar, Hendra Setiawan, Matthias Paulik, Matthias Sperber, Orion Weller, Telmo Pires |  |
| 289 |  |  [A Transformational Biencoder with In-Domain Negative Sampling for Zero-Shot Entity Linking](https://doi.org/10.18653/v1/2022.findings-acl.114) |  | 0 | Recent interest in entity linking has focused in the zero-shot scenario, where at test time the entity mention to be labelled is never seen during training, or may belong to a different domain from the source domain. Current work leverage pre-trained BERT with the implicit assumption that it bridges the gap between the source and target domain distributions.... | Kai Sun, Richong Zhang, Samuel Mensah, Xudong Liu, Yongyi Mao |  |
| 290 |  |  [Finding the Dominant Winning Ticket in Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.115) |  | 0 | The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve... | Di He, Dongyan Zhao, JiRong Wen, Rui Yan, TieYan Liu, Weizhu Chen, Yelong Shen, Zhuocheng Gong |  |
| 291 |  |  [Thai Nested Named Entity Recognition Corpus](https://doi.org/10.18653/v1/2022.findings-acl.116) |  | 0 | This paper presents the first Thai Nested Named Entity Recognition (N-NER) dataset. Thai N-NER consists of 264,798 mentions, 104 classes, and a maximum depth of 8 layers obtained from 4,894 documents in the domains of news articles and restaurant reviews. Our work, to the best of our knowledge, presents the largest non-English N-NER dataset and the first... | Attapol Rutherford, Can Udomcharoenchaikit, Peerat Limkonchotiwat, Sarana Nutanong, Weerayut Buaphet |  |
| 292 |  |  [Two-Step Question Retrieval for Open-Domain QA](https://doi.org/10.18653/v1/2022.findings-acl.117) |  | 0 | The retriever-reader pipeline has shown promising performance in open-domain QA but suffers from a very slow inference speed. Recently proposed question retrieval models tackle this problem by indexing question-answer pairs and searching for similar questions. These models have shown a significant increase in inference speed, but at the cost of lower QA... | Alice Oh, JiHoon Kim, Jiho Jin, Juhee Son, JungWoo Ha, SangWoo Lee, Yeon Seonwoo |  |
| 293 |  |  [Semantically Distributed Robust Optimization for Vision-and-Language Inference](https://doi.org/10.18653/v1/2022.findings-acl.118) |  | 0 | Analysis of vision-and-language models has revealed their brittleness under linguistic phenomena such as paraphrasing, negation, textual entailment, and word substitutions with synonyms or antonyms. While data augmentation techniques have been designed to mitigate against these failure modes, methods that can integrate this knowledge into the training pipeline... | Abhishek Chaudhary, Chitta Baral, Pratyay Banerjee, Tejas Gokhale, Yezhou Yang |  |
| 294 |  |  [Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference](https://doi.org/10.18653/v1/2022.findings-acl.119) |  | 0 | Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of... | JoonYoung Choi, JunHyung Park, Junho Kim, KangMin Kim, Mingyu Lee, SangKeun Lee, YongHo Jung |  |
| 295 |  |  [Capture Human Disagreement Distributions by Calibrated Networks for Natural Language Inference](https://doi.org/10.18653/v1/2022.findings-acl.120) |  | 0 | Natural Language Inference (NLI) datasets contain examples with highly ambiguous labels due to its subjectivity. Several recent efforts have been made to acknowledge and embrace the existence of ambiguity, and explore how to capture the human disagreement distribution. In contrast with directly learning from gold ambiguity labels, relying on special resource,... | Chang Su, Hao Yang, Jiaxin Guo, Min Zhang, Minghan Wang, Shimin Tao, Yimeng Chen, Yuxia Wang |  |
| 296 |  |  [Efficient, Uncertainty-based Moderation of Neural Networks Text Classifiers](https://doi.org/10.18653/v1/2022.findings-acl.121) |  | 0 | To maximize the accuracy and increase the overall acceptance of text classifiers, we propose a framework for the efficient, in-operation moderation of classifiers’ output. Our framework focuses on use cases in which F1-scores of modern Neural Networks classifiers (ca. 90%) are still inapplicable in practice. We suggest a semi-automated approach that uses... | Jakob Smedegaard Andersen, Walid Maalej |  |
| 297 |  |  [Revisiting Automatic Evaluation of Extractive Summarization Task: Can We Do Better than ROUGE?](https://doi.org/10.18653/v1/2022.findings-acl.122) |  | 0 | It has been the norm for a long time to evaluate automated summarization tasks using the popular ROUGE metric. Although several studies in the past have highlighted the limitations of ROUGE, researchers have struggled to reach a consensus on a better alternative until today. One major limitation of the traditional ROUGE metric is the lack of semantic... | Mousumi Akter, Naman Bansal, Shubhra Kanti Karmaker Santu |  |
| 298 |  |  [Open Vocabulary Extreme Classification Using Generative Models](https://doi.org/10.18653/v1/2022.findings-acl.123) |  | 0 | The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set. The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. However in real world scenarios this label set, although large, is often incomplete and experts frequently need to... | Christina Du, Daniel Simig, Fabio Petroni, Kashyap Popat, Majid Yazdani, Pouya Yanki, Sebastian Riedel |  |
| 299 |  |  [Decomposed Meta-Learning for Few-Shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.124) |  | 0 | Few-shot named entity recognition (NER) systems aim at recognizing novel-class named entities based on only a few labeled examples. In this paper, we present a decomposed meta-learning approach which addresses the problem of few-shot NER by sequentially tackling few-shot span detection and few-shot entity typing using meta-learning. In particular, we take the... | ChinYew Lin, Huiqiang Jiang, Qianhui Wu, Tiejun Zhao, Tingting Ma |  |
| 300 |  |  [TegTok: Augmenting Text Generation via Task-specific and Open-world Knowledge](https://doi.org/10.18653/v1/2022.findings-acl.125) |  | 0 | Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this... | Can Xu, ChaoHong Tan, Chongyang Tao, Daxin Jiang, Huang Hu, JiaChen Gu, Xiubo Geng, ZhenHua Ling |  |
| 301 |  |  [EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition](https://doi.org/10.18653/v1/2022.findings-acl.126) |  | 0 | Emotion recognition in conversation (ERC) aims to analyze the speaker’s state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure... | Fengxiao Tang, Ming Zhao, Yusen Zhu, Zaijing Li |  |
| 302 |  |  [Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text](https://doi.org/10.18653/v1/2022.findings-acl.127) |  | 0 | Logical reasoning of text requires identifying critical logical structures in the text and performing inference over them. Existing methods for logical reasoning mainly focus on contextual semantics of text while struggling to explicitly model the logical inference process. In this paper, we not only put forward a logic-driven context extension framework but... | Daxin Jiang, Duyu Tang, Ming Zhou, Nan Duan, Siyuan Wang, Wanjun Zhong, Zhihao Fan, Zhongyu Wei |  |
| 303 |  |  [Transfer Learning and Prediction Consistency for Detecting Offensive Spans of Text](https://doi.org/10.18653/v1/2022.findings-acl.128) |  | 0 | Toxic span detection is the task of recognizing offensive spans in a text snippet. Although there has been prior work on classifying text snippets as offensive or not, the task of recognizing spans responsible for the toxicity of a text is not explored yet. In this work, we introduce a novel multi-task framework for toxic span detection in which the model seeks... | Amir Pouran Ben Veyseh, Franck Dernoncourt, Ning Xu, Quan Hung Tran, Thien Huu Nguyen, Varun Manjunatha |  |
| 304 |  |  [Learning Reasoning Patterns for Relational Triple Extraction with Mutual Generation of Text and Graph](https://doi.org/10.18653/v1/2022.findings-acl.129) |  | 0 | Relational triple extraction is a critical task for constructing knowledge graphs. Existing methods focused on learning text patterns from explicit relational mentions. However, they usually suffered from ignoring relational reasoning patterns, thus failed to extract the implicitly implied triples. Fortunately, the graph structure of a sentence’s relational... | Yongfeng Huang, Yubo Chen, Yunqi Zhang |  |
| 305 |  |  [Document-Level Event Argument Extraction via Optimal Transport](https://doi.org/10.18653/v1/2022.findings-acl.130) |  | 0 | Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for... | Amir Pouran Ben Veyseh, Bonan Min, Franck Dernoncourt, Minh Van Nguyen, Thien Huu Nguyen |  |
| 306 |  |  [N-Shot Learning for Augmenting Task-Oriented Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.131) |  | 0 | Augmentation of task-oriented dialogues has followed standard methods used for plain-text such as back-translation, word-level manipulation, and paraphrasing despite its richly annotated structure. In this work, we introduce an augmentation framework that utilizes belief state annotations to match turns from various dialogues and form new synthetic dialogues in... | Ibrahim Taha Aksu, MinYen Kan, Nancy F. Chen, Zhengyuan Liu |  |
| 307 |  |  [Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-acl.132) |  | 0 | Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs,... | Hwee Tou Ng, Lidong Bing, Qingyu Tan, Ruidan He |  |
| 308 |  |  [Calibration of Machine Reading Systems at Scale](https://doi.org/10.18653/v1/2022.findings-acl.133) |  | 0 | In typical machine learning systems, an estimate of the probability of the prediction is used to assess the system’s confidence in the prediction. This confidence measure is usually uncalibrated; i.e. the system’s confidence in the prediction does not match the true probability of the predicted output. In this paper, we present an investigation into calibrating... | Leonard Adolphs, Mrinmaya Sachan, Rajarshi Das, Shehzaad Dhuliawala |  |
| 309 |  |  [Towards Adversarially Robust Text Classifiers by Learning to Reweight Clean Examples](https://doi.org/10.18653/v1/2022.findings-acl.134) |  | 0 | Most of the existing defense methods improve the adversarial robustness by making the models adapt to the training set augmented with some adversarial examples. However, the augmented adversarial examples may not be natural, which might distort the training distribution, resulting in inferior performance both in clean accuracy and adversarial robustness. In... | Cenyuan Zhang, ChoJui Hsieh, Jianhan Xu, KaiWei Chang, Linyang Li, Xiaoqing Zheng, Xuanjing Huang |  |
| 310 |  |  [Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects](https://doi.org/10.18653/v1/2022.findings-acl.135) |  | 0 | We present state-of-the-art results on morphosyntactic tagging across different varieties of Arabic using fine-tuned pre-trained transformer language models. Our models consistently outperform existing systems in Modern Standard Arabic and all the Arabic dialects we study, achieving 2.6% absolute improvement over the previous state-of-the-art in Modern Standard... | Go Inoue, Nizar Habash, Salam Khalifa |  |
| 311 |  |  [How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis](https://doi.org/10.18653/v1/2022.findings-acl.136) |  | 0 | Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut... | Bingquan Liu, Chengjie Sun, Lifeng Shang, Qun Liu, Shaobo Li, Xiaoguang Li, Xin Jiang, Zhenhua Dong, Zhenzhou Ji |  |
| 312 |  |  [Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models](https://doi.org/10.18653/v1/2022.findings-acl.137) |  | 0 | Popular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a... | Christopher Ré, Enci Liu, Sen Wu, Simran Arora |  |
| 313 |  |  [Enhancing Natural Language Representation with Large-Scale Out-of-Domain Commonsense](https://doi.org/10.18653/v1/2022.findings-acl.138) |  | 0 | We study how to enhance text representation via textual commonsense. We point out that commonsense has the nature of domain discrepancy. Namely, commonsense has different data formats and is domain-independent from the downstream task. This nature brings challenges to introducing commonsense in general text understanding tasks. A typical method of introducing... | Wanyun Cui, Xingran Chen |  |
| 314 |  |  [Weighted self Distillation for Chinese word segmentation](https://doi.org/10.18653/v1/2022.findings-acl.139) |  | 0 | Recent researches show that multi-criteria resources and n-gram features are beneficial to Chinese Word Segmentation (CWS). However, these methods rely heavily on such additional information mentioned above and focus less on the model itself. We thus propose a novel neural framework, named Weighted self Distillation for Chinese word segmentation (WeiDC). The... | Jialei Zhang, Rian He, Shubin Cai, Zhong Ming |  |
| 315 |  |  [Sibylvariant Transformations for Robust Text Classification](https://doi.org/10.18653/v1/2022.findings-acl.140) |  | 0 | The vast majority of text transformation techniques in NLP are inherently limited in their ability to expand input space coverage due to an implicit constraint to preserve the original class label. In this work, we propose the notion of sibylvariance (SIB) to describe the broader set of transforms that relax the label-preserving constraint, knowably vary the... | Fabrice HarelCanada, Miryung Kim, Muhammad Ali Gulzar, Nanyun Peng |  |
| 316 |  |  [DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.141) |  | 0 | Domain Adaptation (DA) of Neural Machine Translation (NMT) model often relies on a pre-trained general NMT model which is adapted to the new domain on a sample of in-domain parallel data. Without parallel data, there is no way to estimate the potential benefit of DA, nor the amount of parallel samples it would require. It is however a desirable functionality... | Cheonbok Park, Hantae Kim, Hyunchang Cho, Ioan Calapodescu, Vassilina Nikoulina |  |
| 317 |  |  [Hey AI, Can You Solve Complex Tasks by Talking to Agents?](https://doi.org/10.18653/v1/2022.findings-acl.142) |  | 0 | Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks... | Ashish Sabharwal, Daniel Khashabi, Kyle Richardson, Tushar Khot |  |
| 318 |  |  [Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion](https://doi.org/10.18653/v1/2022.findings-acl.143) |  | 0 | In multimodal machine learning, additive late-fusion is a straightforward approach to combine the feature representations from different modalities, in which the final prediction can be formulated as the sum of unimodal predictions. While it has been found that certain late-fusion models can achieve competitive performance with lower computational costs... | Rada Mihalcea, Yiqun Yao |  |
| 319 |  |  [BiSyn-GAT+: Bi-Syntax Aware Graph Attention Network for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.144) |  | 0 | Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to align aspects and corresponding sentiments for aspect-specific sentiment polarity inference. It is challenging because a sentence may contain multiple aspects or complicated (e.g., conditional, coordinating, or adversative) relations. Recently, exploiting dependency... | Fei Wang, Shuo Liang, Wei Wei, XianLing Mao, Zhiyong He |  |
| 320 |  |  [IndicBART: A Pre-trained Model for Indic Natural Language Generation](https://doi.org/10.18653/v1/2022.findings-acl.145) |  | 0 | In this paper, we study pre-trained sequence-to-sequence models for a group of related languages, with a focus on Indic languages. We present IndicBART, a multilingual, sequence-to-sequence pre-trained model focusing on 11 Indic languages and English. IndicBART utilizes the orthographic similarity between Indic scripts to improve transfer learning between... | Anoop Kunchukuttan, Himani Shrotriya, Mitesh M. Khapra, Pratyush Kumar, Raj Dabre, Ratish Puduppully |  |
| 321 |  |  [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models](https://doi.org/10.18653/v1/2022.findings-acl.146) |  | 0 | We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from... | Daniel Cer, Gustavo Hernández Ábrego, Ji Ma, Jianmo Ni, Keith B. Hall, Noah Constant, Yinfei Yang |  |
| 322 |  |  [Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking](https://doi.org/10.18653/v1/2022.findings-acl.147) |  | 0 | Relation extraction (RE) is an important natural language processing task that predicts the relation between two given entities, where a good understanding of the contextual information is essential to achieve an outstanding model performance. Among different types of contextual information, the auto-generated syntactic information (namely, word dependencies)... | Fei Xia, Yan Song, Yuanhe Tian |  |
| 323 |  |  [Striking a Balance: Alleviating Inconsistency in Pre-trained Models for Symmetric Classification Tasks](https://doi.org/10.18653/v1/2022.findings-acl.148) |  | 0 | While fine-tuning pre-trained models for downstream classification is the conventional paradigm in NLP, often task-specific nuances may not get captured in the resultant models. Specifically, for tasks that take two inputs and require the output to be invariant of the order of the inputs, inconsistency is often observed in the predicted labels or confidence... | Aditya Joshi, Ashutosh Kumar |  |
| 324 |  |  [Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts](https://doi.org/10.18653/v1/2022.findings-acl.149) |  | 0 | Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative... | Chenguang Zhu, Lianhui Qin, Meng Jiang, Tong Zhao, Wenhao Yu, Zhihan Zhang |  |
| 325 |  |  [Dict-BERT: Enhancing Language Model Pre-training with Dictionary](https://doi.org/10.18653/v1/2022.findings-acl.150) |  | 0 | Pre-trained language models (PLMs) aim to learn universal language representations by conducting self-supervised training tasks on large-scale corpora. Since PLMs capture word semantics in different contexts, the quality of word representations highly depends on word frequency, which usually follows a heavy-tailed distributions in the pre-training corpus.... | Chenguang Zhu, Donghan Yu, Meng Jiang, Michael Zeng, Shuohang Wang, Wenhao Yu, Yichong Xu, Yuwei Fang |  |
| 326 |  |  [A Feasibility Study of Answer-Unaware Question Generation for Education](https://doi.org/10.18653/v1/2022.findings-acl.151) |  | 0 | We conduct a feasibility study into the applicability of answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or un-interpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written... | Chris CallisonBurch, Chuning Yuan, DaHyeon Choi, Eleni Miltsakaki, Etan Ginsberg, Hannah Gonzalez, Liam Dugan, Shriyash Upadhyay |  |
| 327 |  |  [Relevant CommonSense Subgraphs for "What if..." Procedural Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.152) |  | 0 | We study the challenge of learning causal reasoning over procedural text to answer “What if...” questions when external commonsense knowledge is required. We propose a novel multi-hop graph reasoning model to 1) efficiently extract a commonsense subgraph with the most relevant information from a large knowledge graph; 2) predict the causal answer by reasoning... | Chen Zheng, Parisa Kordjamshidi |  |
| 328 |  |  [Combining Feature and Instance Attribution to Detect Artifacts](https://doi.org/10.18653/v1/2022.findings-acl.153) |  | 0 | Training the deep neural networks that dominate NLP requires large datasets. These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts. By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes;... | Byron C. Wallace, Pouya Pezeshkpour, Sameer Singh, Sarthak Jain |  |
| 329 |  |  [Leveraging Expert Guided Adversarial Augmentation For Improving Generalization in Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.154) |  | 0 | Named Entity Recognition (NER) systems often demonstrate great performance on in-distribution data, but perform poorly on examples drawn from a shifted distribution. One way to evaluate the generalization ability of NER models is to use adversarial examples, on which the specific variations associated with named entities are rarely considered. To this end, we... | Aaron Reich, Aastha Agrawal, Diyi Yang, Jiaao Chen, Yanzhe Zhang |  |
| 330 |  |  [Label Semantics for Few Shot Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.155) |  | 0 | We study the problem of few shot learning for named entity recognition. Specifically, we leverage the semantic information in the names of the labels as a way of giving the model additional signal and enriched priors. We propose a neural architecture that consists of two BERT encoders, one to encode the document and its tokens and another one to encode each of... | Dan Roth, Jie Ma, Miguel Ballesteros, Rishita Anubhai, Srikanth Doss, Sunil Mallya, Yaser AlOnaizan |  |
| 331 |  |  [Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem](https://doi.org/10.18653/v1/2022.findings-acl.156) |  | 0 | We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We... | Hamed Firooz, Jiatao Gu, Khalil Mrini, Maziar Sanjabi, Shaoliang Nie, Sinong Wang |  |
| 332 |  |  [VISITRON: Visual Semantics-Aligned Interactively Trained Object-Navigator](https://doi.org/10.18653/v1/2022.findings-acl.157) |  | 0 | Interactive robots navigating photo-realistic environments need to be trained to effectively leverage and handle the dynamic nature of dialogue in addition to the challenges underlying vision-and-language navigation (VLN). In this paper, we present VISITRON, a multi-modal Transformer-based navigator better suited to the interactive regime inherent to... | Ayush Shrivastava, Devi Parikh, Dilek HakkaniTur, Gökhan Tür, Karthik Gopalakrishnan, Robinson Piramuthu, Yang Liu |  |
| 333 |  |  [Investigating Selective Prediction Approaches Across Several Tasks in IID, OOD, and Adversarial Settings](https://doi.org/10.18653/v1/2022.findings-acl.158) |  | 0 | In order to equip NLP systems with ‘selective prediction’ capability, several task-specific approaches have been proposed. However, which approaches work best across tasks or even if they consistently outperform the simplest baseline MaxProb remains to be explored. To this end, we systematically study selective prediction in a large-scale setup of 17 datasets... | Chitta Baral, Neeraj Varshney, Swaroop Mishra |  |
| 334 |  |  [Unsupervised Natural Language Inference Using PHL Triplet Generation](https://doi.org/10.18653/v1/2022.findings-acl.159) |  | 0 | Transformer-based models achieve impressive performance on numerous Natural Language Inference (NLI) benchmarks when trained on respective training datasets. However, in certain cases, training samples may not be available or collecting them could be time-consuming and resource-intensive. In this work, we address the above challenge and present an explorative... | Chitta Baral, Neeraj Varshney, Pratyay Banerjee, Tejas Gokhale |  |
| 335 |  |  [Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue](https://doi.org/10.18653/v1/2022.findings-acl.160) |  | 0 | Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources... | Anna Korhonen, Evgeniia Razumovskaia, Ivan Vulic |  |
| 336 |  |  [Ranking-Constrained Learning with Rationales for Text Classification](https://doi.org/10.18653/v1/2022.findings-acl.161) |  | 0 | We propose a novel approach that jointly utilizes the labels and elicited rationales for text classification to speed up the training of deep learning models with limited training data. We define and optimize a ranking-constrained loss function that combines cross-entropy loss with ranking losses as rationale constraints. We evaluate our proposed... | Juanyan Wang, Manali Sharma, Mustafa Bilgic |  |
| 337 |  |  [CaM-Gen: Causally Aware Metric-Guided Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.162) |  | 0 | Content is created for a well-defined purpose, often described by a metric or signal represented in the form of structured information. The relationship between the goal (metrics) of target content and the content itself is non-trivial. While large-scale language models show promising text generation capabilities, guiding the generated text with external... | Abhilasha Sancheti, Ayush Agarwal, Navita Goyal, Niyati Chhaya, Roodram Paneri, Udit Kalani |  |
| 338 |  |  [Training Dynamics for Text Summarization Models](https://doi.org/10.18653/v1/2022.findings-acl.163) |  | 0 | Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training... | Greg Durrett, Jiacheng Xu, Junyi Jessy Li, Tanya Goyal |  |
| 339 |  |  [Richer Countries and Richer Representations](https://doi.org/10.18653/v1/2022.findings-acl.164) |  | 0 | We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and... | Dan Jurafsky, Kaitlyn Zhou, Kawin Ethayarajh |  |
| 340 |  |  [BBQ: A hand-built bias benchmark for question answering](https://doi.org/10.18653/v1/2022.findings-acl.165) |  | 0 | It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected... | Alicia Parrish, Angelica Chen, Jana Thompson, Jason Phang, Nikita Nangia, Phu Mon Htut, Samuel R. Bowman, Vishakh Padmakumar |  |
| 341 |  |  [Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble](https://doi.org/10.18653/v1/2022.findings-acl.166) |  | 0 | Grapheme-to-Phoneme (G2P) has many applications in NLP and speech fields. Most existing work focuses heavily on languages with abundant training datasets, which limits the scope of target languages to less than 100 languages. This work attempts to apply zero-shot learning to approximate G2P models for all low-resource and endangered languages in Glottolog... | Alan W. Black, David R. Mortensen, Florian Metze, Shinji Watanabe, Xinjian Li |  |
| 342 |  |  [Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Under-Documented Languages](https://doi.org/10.18653/v1/2022.findings-acl.167) |  | 0 | Recent progress in NLP is driven by pretrained models leveraging massive datasets and has predominantly benefited the world’s political and economic superpowers. Technologically underserved languages are left behind because they lack such resources. Hundreds of underserved languages, nevertheless, have available data sources in the form of interlinear glossed... | Bruce Harold Oliver, Changbing Yang, Clarissa Forbes, Edith Coates, Farhan Samir, Garrett Nicolai, Miikka Silfverberg |  |
| 343 |  |  [Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask](https://doi.org/10.18653/v1/2022.findings-acl.168) |  | 0 | Reading is integral to everyday life, and yet learning to read is a struggle for many young learners. During lessons, teachers can use comprehension questions to increase engagement, test reading skills, and improve retention. Historically such questions were written by skilled teachers, but recently language models have been used to generate comprehension... | Alona Fyshe, Bilal Ghanem, Julia Rivard Dexter, Lauren Lutz Coleman, Spencer McIntosh von der Ohe |  |
| 344 |  |  [TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.169) |  | 0 | Entity retrieval—retrieving information about entity mentions in a query—is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help... | Christopher Ré, Daniel Y. Fu, Mayee F. Chen, Megan Leszczynski |  |
| 345 |  |  [Hierarchical Recurrent Aggregative Generation for Few-Shot NLG](https://doi.org/10.18653/v1/2022.findings-acl.170) |  | 0 | Large pretrained models enable transfer learning to low-resource domains for language generation tasks. However, previous end-to-end approaches do not account for the fact that some generation sub-tasks, specifically aggregation and lexicalisation, can benefit from transfer learning in different extents. To exploit these varying potentials for transfer... | Gerasimos Lampouras, Giulio Zhou, Ignacio Iacobacci |  |
| 346 |  |  [Training Text-to-Text Transformers with Privacy Guarantees](https://doi.org/10.18653/v1/2022.findings-acl.171) |  | 0 | Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information... | Jasmijn Bastings, Natalia Ponomareva, Sergei Vassilvitskii |  |
| 347 |  |  [Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers](https://doi.org/10.18653/v1/2022.findings-acl.172) |  | 0 | Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (“transformers”) became popular, despite its practical importance, comparably few papers have investigated how... | Andreas Niekler, Christopher Schröder, Martin Potthast |  |
| 348 |  |  [The impact of lexical and grammatical processing on generating code from natural language](https://doi.org/10.18653/v1/2022.findings-acl.173) |  | 0 | Considering the seq2seq architecture of Yin and Neubig (2018) for natural language to code translation, we identify four key components of importance: grammatical constraints, lexical preprocessing, input representations, and copy mechanisms. To study the impact of these components, we use a state-of-the-art architecture that relies on BERT encoder and a... | Benoît Crabbé, Nathanaël Beau |  |
| 349 |  |  [Seq2Path: Generating Sentiment Tuples as Paths of a Tree](https://doi.org/10.18653/v1/2022.findings-acl.174) |  | 0 | Aspect-based sentiment analysis (ABSA) tasks aim to extract sentiment tuples from a sentence. Recent generative methods such as Seq2Seq models have achieved good performance by formulating the output as a sequence of sentiment tuples. However, the orders between the sentiment tuples do not naturally exist and the generation of the current tuple should not... | Jingchao Yang, Longjun Cai, Xiaoying Zhu, Yi Shen, Yue Mao |  |
| 350 |  |  [Mitigating the Inconsistency Between Word Saliency and Model Confidence with Pathological Contrastive Training](https://doi.org/10.18653/v1/2022.findings-acl.175) |  | 0 | Neural networks are widely used in various NLP tasks for their remarkable performance. However, the complexity makes them difficult to interpret, i.e., they are not guaranteed right for the right reason. Besides the complexity, we reveal that the model pathology - the inconsistency between word saliency and model confidence, further hurts the interpretability.... | Liming Wang, Pengwei Zhan, Shaolei Zhou, Yang Wu, Yunjian Zhang |  |
| 351 |  |  [Your fairness may vary: Pretrained language model fairness in toxic text classification](https://doi.org/10.18653/v1/2022.findings-acl.176) |  | 0 | The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well.... | Dennis Wei, Ioana Baldini, Karthikeyan Natesan Ramamurthy, Mikhail Yurochkin, Moninder Singh |  |
| 352 |  |  [ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.177) |  | 0 | Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are... | Ahmed Masry, Do Xuan Long, Enamul Hoque, Jia Qing Tan, Shafiq R. Joty |  |
| 353 |  |  [A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification](https://doi.org/10.18653/v1/2022.findings-acl.178) |  | 0 | Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models’ complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel... | Dairui Liu, Derek Greene, Ruihai Dong |  |
| 354 |  |  [Learn and Review: Enhancing Continual Named Entity Recognition via Reviewing Synthetic Samples](https://doi.org/10.18653/v1/2022.findings-acl.179) |  | 0 | Traditional methods for named entity recognition (NER) classify mentions into a fixed set of pre-defined entity types. However, in many real-world scenarios, new entity types are incrementally involved. To investigate this problem, continual learning is introduced for NER. However, the existing method depends on the relevance between tasks and is prone to... | Dai Dai, Quan Wang, Sujian Li, Wenhao Wu, Yajuan Lyu, Yong Zhu, Yu Xia |  |
| 355 |  |  [Phoneme transcription of endangered languages: an evaluation of recent ASR architectures in the single speaker scenario](https://doi.org/10.18653/v1/2022.findings-acl.180) |  | 0 | Transcription is often reported as the bottleneck in endangered language documentation, requiring large efforts from scarce speakers and transcribers. In general, automatic speech recognition (ASR) can be accurate enough to accelerate transcription only if trained on large amounts of transcribed data. However, when a single speaker is involved, several studies... | Gilles Boulianne |  |
| 356 |  |  [Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task](https://doi.org/10.18653/v1/2022.findings-acl.181) |  | 0 | Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without... | Alessandro Lenci, Karim Lasri, Thierry Poibeau |  |
| 357 |  |  [Combining Static and Contextualised Multilingual Embeddings](https://doi.org/10.18653/v1/2022.findings-acl.182) |  | 0 | Static and contextual multilingual embeddings have complementary strengths. Static embeddings, while less expressive than contextual language models, can be more straightforwardly aligned across multiple languages. We combine the strengths of static and contextual models to improve multilingual representations. We extract static embeddings for 40 languages from... | Alexander Fraser, Jindrich Libovický, Katharina Hämmerl |  |
| 358 |  |  [An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection](https://doi.org/10.18653/v1/2022.findings-acl.183) |  | 0 | Knowledge graph integration typically suffers from the widely existing dangling entities that cannot find alignment cross knowledge graphs (KGs). The dangling entity set is unavailable in most real-world scenarios, and manually mining the entity pairs that consist of entities with the same meaning is labor-consuming. In this paper, we propose a novel accurate... | Sheng Yu, Shengxuan Luo |  |
| 359 |  |  [Square One Bias in NLP: Towards a Multi-Dimensional Exploration of the Research Manifold](https://doi.org/10.18653/v1/2022.findings-acl.184) |  | 0 | The prototypical NLP experiment trains a standard architecture on labeled English data and optimizes for accuracy, without accounting for other dimensions such as fairness, interpretability, or computational efficiency. We show through a manual classification of recent NLP research papers that this is indeed the case and refer to it as the square one... | Anders Søgaard, Ivan Vulic, Sebastian Ruder |  |
| 360 |  |  [Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective](https://doi.org/10.18653/v1/2022.findings-acl.185) |  | 0 | Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of... | André Freitas, Danilo S. Carvalho, Edoardo Manino, Julia Rozanova, Lucas C. Cordeiro |  |
| 361 |  |  [Improving Neural Political Statement Classification with Class Hierarchical Information](https://doi.org/10.18653/v1/2022.findings-acl.186) |  | 0 | Many tasks in text-based computational social science (CSS) involve the classification of political statements into categories based on a domain-specific codebook. In order to be useful for CSS analysis, these categories must be fine-grained. The typically skewed distribution of fine-grained categories, however, results in a challenging classification problem... | André Blessing, Erenay Dayanik, Gabriella Lapesa, Jonas Kuhn, Nico Blokker, Sebastian Haunss, Sebastian Padó |  |
| 362 |  |  [Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](https://doi.org/10.18653/v1/2022.findings-acl.187) |  | 0 | The recent large-scale vision-language pre-training (VLP) of dual-stream architectures (e.g., CLIP) with a tremendous amount of image-text pair data, has shown its superiority on various multimodal alignment tasks. Despite its success, the resulting models are not capable of multimodal generative tasks due to the weak text encoder. To tackle this problem, we... | Lifeng Shang, Lu Hou, Pascale Fung, Qun Liu, Wenliang Dai, Xin Jiang |  |
| 363 |  |  [Co-VQA : Answering by Interactive Sub Question Sequence](https://doi.org/10.18653/v1/2022.findings-acl.188) |  | 0 | Most existing approaches to Visual Question Answering (VQA) answer questions directly, however, people usually decompose a complex question into a sequence of simple sub questions and finally obtain the answer to the original question after answering the sub question sequence(SQS). By simulating the process, this paper proposes a conversation-based VQA (Co-VQA)... | Fangxiang Feng, Huixing Jiang, Ruonan Wang, Xiaojie Wang, Yuxi Qian |  |
| 364 |  |  [A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation](https://doi.org/10.18653/v1/2022.findings-acl.189) |  | 0 | Early exiting allows instances to exit at different layers according to the estimation of difficulty. Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more... | Guotong Xie, Lingling Wu, Tianxiang Sun, Wei Zhu, Xiangyang Liu, Xipeng Qiu, Xuanjing Huang, Yilong He, Yuan Ni, Zhichao Geng |  |
| 365 |  |  [Auxiliary tasks to boost Biaffine Semantic Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.190) |  | 0 | The biaffine parser of (CITATION) was successfully extended to semantic dependency parsing (SDP) (CITATION). Its performance on graphs is surprisingly high given that, without the constraint of producing a tree, all arcs for a given sentence are predicted independently from each other (modulo a shared representation of tokens).To circumvent such an independence... | Marie Candito |  |
| 366 |  |  [Syntax-guided Contrastive Learning for Pre-trained Language Model](https://doi.org/10.18653/v1/2022.findings-acl.191) |  | 0 | Syntactic information has been proved to be useful for transformer-based pre-trained language models. Previous studies often rely on additional syntax-guided attention components to enhance the transformer, which require more parameters and additional syntactic parsing in downstream tasks. This increase in complexity severely limits the application of... | Hua Wu, Lijie Wang, Shuai Zhang, Xinyan Xiao |  |
| 367 |  |  [Improved Multi-label Classification under Temporal Concept Drift: Rethinking Group-Robust Algorithms in a Label-Wise Setting](https://doi.org/10.18653/v1/2022.findings-acl.192) |  | 0 | In document classification for, e.g., legal and biomedical text, we often deal with hundreds of classes, including very infrequent ones, as well as temporal concept drift caused by the influence of real world events, e.g., policy changes, conflicts, or pandemics. Class imbalance and drift can sometimes be mitigated by resampling the training data to simulate... | Anders Søgaard, Ilias Chalkidis |  |
| 368 |  |  [ASCM: An Answer Space Clustered Prompting Method without Answer Engineering](https://doi.org/10.18653/v1/2022.findings-acl.193) |  | 0 | Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (NLI). Because of the diverse linguistic expression, there exist many answer tokens for the same... | Azmat Anwar, Bo Ma, Lei Wang, Rui Dong, Yating Yang, Zhen Wang, Zhou Xi |  |
| 369 |  |  [Why don't people use character-level machine translation?](https://doi.org/10.18653/v1/2022.findings-acl.194) |  | 0 | We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with... | Alexander Fraser, Helmut Schmid, Jindrich Libovický |  |
| 370 |  |  [Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems](https://doi.org/10.18653/v1/2022.findings-acl.195) |  | 0 | Math Word Problem (MWP) solving needs to discover the quantitative relationships over natural language narratives. Recent work shows that existing models memorize procedures from context and rely on shallow heuristics to solve MWPs. In this paper, we look at this issue and argue that the cause is a lack of overall understanding of MWP patterns. We first... | Chao Li, Chao Yan, Hongzhi Liu, Qingyu Zhou, Wenxuan Zhang, Yunbo Cao, Zhongli Li |  |
| 371 |  |  [xGQA: Cross-Lingual Visual Question Answering](https://doi.org/10.18653/v1/2022.findings-acl.196) |  | 0 | Recent advances in multimodal vision and language modeling have predominantly focused on the English language, mostly due to the lack of multilingual multimodal datasets to steer modeling efforts. In this work, we address this gap and provide xGQA, a new multilingual evaluation benchmark for the visual question answering task. We extend the established English... | Aishwarya Kamath, Gregor Geigle, Iryna Gurevych, Ivan Vulic, JanMartin O. Steitz, Jonas Pfeiffer, Stefan Roth |  |
| 372 |  |  [Automatic Speech Recognition and Query By Example for Creole Languages Documentation](https://doi.org/10.18653/v1/2022.findings-acl.197) |  | 0 | We investigate the exploitation of self-supervised models for two Creole languages with few resources: Gwadloupéyen and Morisien. Automatic language processing tools are almost non-existent for these two languages. We propose to use about one hour of annotated data to design an automatic speech recognition system for each language. We evaluate how much data is... | Benjamin Lecouteux, Cécile Macaire, Didier Schwab, Emmanuel Schang |  |
| 373 |  |  [MReD: A Meta-Review Dataset for Structure-Controllable Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.198) |  | 0 | When directly using existing text generation datasets for controllable generation, we are facing the problem of not having the domain knowledge and thus the aspects that could be controlled are limited. A typical example is when using CNN/Daily Mail dataset for controllable text summarization, there is no guided information on the emphasis of summary sentences.... | Chenhui Shen, Lidong Bing, Liying Cheng, Luo Si, Ran Zhou, Yang You |  |
| 374 |  |  [Single Model Ensemble for Subword Regularized Models in Low-Resource Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.199) |  | 0 | Subword regularizations use multiple subword segmentations during training to improve the robustness of neural machine translation models. In previous subword regularizations, we use multiple segmentations in the training process but use only one segmentation in the inference. In this study, we propose an inference strategy to address this discrepancy. The... | Naoaki Okazaki, Sho Takase, Tatsuya Hiraoka |  |
| 375 |  |  [Detecting Various Types of Noise for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.200) |  | 0 | The filtering and/or selection of training data is one of the core aspects to be considered when building a strong machine translation system. In their influential work, Khayrallah and Koehn (2018) investigated the impact of different types of noise on the performance of machine translation systems. In the same year the WMT introduced a shared task on parallel... | Christian Herold, Hermann Ney, Jan Rosendahl, Joris Vanvinckenroye |  |
| 376 |  |  [DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training](https://doi.org/10.18653/v1/2022.findings-acl.201) |  | 0 | Due to the limitations of the model structure and pre-training objectives, existing vision-and-language generation models cannot utilize pair-wise images and text through bi-directional generation. In this paper, we propose DU-VLG, a framework which unifies vision-and-language generation as sequence generation problems. DU-VLG is trained with novel dual... | Guocheng Niu, Hua Wu, Jiachen Liu, Luyang Huang, Xinyan Xiao |  |
| 377 |  |  [HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.202) |  | 0 | Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical... | Chengyu Wang, Dongyang Li, Nan Hu, Taolin Zhang, Xiaofeng He |  |
| 378 |  |  [Prompt-Driven Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.203) |  | 0 | Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose... | Jing Li, Yafu Li, Yongjing Yin, Yue Zhang |  |
| 379 |  |  [On Controlling Fallback Responses for Grounded Dialogue Generation](https://doi.org/10.18653/v1/2022.findings-acl.204) |  | 0 | Dialogue agents can leverage external textual knowledge to generate responses of a higher quality. To our best knowledge, most existing works on knowledge grounded dialogue settings assume that the user intention is always answerable. Unfortunately, this is impractical as there is no guarantee that the knowledge retrievers could always retrieve the desired... | Helen Meng, Hong Cheng, Hongyuan Lu, Wai Lam |  |
| 380 |  |  [CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions](https://doi.org/10.18653/v1/2022.findings-acl.205) |  | 0 | Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT, a new video question answering dataset that requires causal reasoning about physical forces and object... | Aykut Erdem, Cagatay Yigit, Deniz Yuret, Erkut Erdem, Ilker Kesen, Mert Kobas, Muhammed Samil Atesoglu, Tayfun Ates, Tilbe Göksun |  |
| 381 |  |  [A Graph Enhanced BERT Model for Event Prediction](https://doi.org/10.18653/v1/2022.findings-acl.206) |  | 0 | Predicting the subsequent event for an existing event context is an important but challenging task, as it requires understanding the underlying relationship between events. Previous methods propose to retrieve relational features from event graph to enhance the modeling of event correlation. However, the sparsity of event graph may restrict the acquisition of... | Bing Qin, Li Du, Ting Liu, Xiao Ding, Yue Zhang |  |
| 382 |  |  [Long Time No See! Open-Domain Conversation with Long-Term Persona Memory](https://doi.org/10.18653/v1/2022.findings-acl.207) |  | 0 | Most of the open-domain dialogue models tend to perform poorly in the setting of long-term human-bot conversations. The possible reason is that they lack the capability of understanding and memorizing long-term dialogue history information. To address this issue, we present a novel task of Long-term Memory Conversation (LeMon) and then build a new dialogue... | Haifeng Wang, Hua Wu, Shihang Wang, Wenquan Wu, Xinchao Xu, ZhengYu Niu, Zhibin Gou |  |
| 383 |  |  [Lacking the Embedding of a Word? Look it up into a Traditional Dictionary](https://doi.org/10.18653/v1/2022.findings-acl.208) |  | 0 | Word embeddings are powerful dictionaries, which may easily capture language variations. However, these dictionaries fail to give sense to rare words, which are surprisingly often covered by traditional dictionaries. In this paper, we propose to use definitions retrieved in traditional dictionaries to produce word embeddings for rare words. For this purpose, we... | Elena Sofia Ruzzetti, Fabio Massimo Zanzotto, Francesca Fallucchi, Leonardo Ranaldi, Michele Mastromattei, Noemi Scarpato |  |
| 384 |  |  [MTRec: Multi-Task Learning over BERT for News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.209) |  | 0 | Existing news recommendation methods usually learn news representations solely based on news titles. To sufficiently utilize other fields of news information such as category and entities, some methods treat each field as an additional feature and combine different feature vectors with attentive pooling. With the adoption of large pre-trained models like BERT... | Hanfang Yang, Jian Li, Lifeng Shang, Qiwei Bi, Qun Liu, Xin Jiang |  |
| 385 |  |  [Cross-domain Named Entity Recognition via Graph Matching](https://doi.org/10.18653/v1/2022.findings-acl.210) |  | 0 | Cross-domain NER is a practical yet challenging problem since the data scarcity in the real-world scenario. A common practice is first to learn a NER model in a rich-resource general domain and then adapt the model to specific domains. Due to the mismatch problem between entity types across domains, the wide knowledge in the general domain can not effectively... | Haibin Chen, Junhao Zheng, Qianli Ma |  |
| 386 |  |  [Assessing Multilingual Fairness in Pre-trained Multimodal Representations](https://doi.org/10.18653/v1/2022.findings-acl.211) |  | 0 | Recently pre-trained multimodal models, such as CLIP, have shown exceptional capabilities towards connecting images and natural language. The textual representations in English can be desirably transferred to multilingualism and support downstream multimodal tasks for different languages. Nevertheless, the principle of multilingual fairness is rarely... | Jialu Wang, Xin Eric Wang, Yang Liu |  |
| 387 |  |  [More Than Words: Collocation Retokenization for Latent Dirichlet Allocation Models](https://doi.org/10.18653/v1/2022.findings-acl.212) |  | 0 | Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a collection of documents to discover their latent topics using word-document co-occurrences. Previous studies show that representing bigrams collocations in the input can improve topic coherence in English. However, it is unclear how to achieve the best results for languages without marked word... | Alexandra Schofield, Attapol Rutherford, Jin Cheevaprawatdomrong |  |
| 388 |  |  [Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness](https://doi.org/10.18653/v1/2022.findings-acl.213) |  | 0 | Data modification, either via additional training datasets, data augmentation, debiasing, and dataset filtering, has been proposed as an effective solution for generalizing to out-of-domain (OOD) inputs, in both natural language processing and computer vision literature. However, the effect of data modification on adversarial robustness remains unclear. In this... | Bhavdeep Singh Sachdeva, Chitta Baral, Man Luo, Swaroop Mishra, Tejas Gokhale |  |
| 389 |  |  [ASSIST: Towards Label Noise-Robust Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.214) |  | 0 | The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking (DST). However, substantial noise has been discovered in its state annotations. Such noise brings about huge challenges for training DST models robustly. Although several refined versions, including MultiWOZ 2.1-2.4, have been published recently, there are still lots of noisy... | Emine Yilmaz, Fanghua Ye, Yue Feng |  |
| 390 |  |  [Graph Refinement for Coreference Resolution](https://doi.org/10.18653/v1/2022.findings-acl.215) |  | 0 | The state-of-the-art models for coreference resolution are based on independent mention pair-wise decisions. We propose a modelling approach that learns coreference at the document-level and takes global decisions. For this purpose, we model coreference links in a graph structure where the nodes are tokens in the text, and the edges represent the relationship... | James Henderson, Lesly Miculicich |  |
| 391 |  |  [ECO v1: Towards Event-Centric Opinion Mining](https://doi.org/10.18653/v1/2022.findings-acl.216) |  | 0 | Events are considered as the fundamental building blocks of the world. Mining event-centric opinions can benefit decision making, people communication, and social good. Unfortunately, there is little literature addressing event-centric opinion mining, although which significantly diverges from the well-studied entity-centric opinion mining in connotation,... | Hongyu Lin, Jin Xu, Le Sun, Meng Liao, Ruoxi Xu, Wei Tan, Xianpei Han, Yingfei Sun |  |
| 392 |  |  [Deep Reinforcement Learning for Entity Alignment](https://doi.org/10.18653/v1/2022.findings-acl.217) |  | 0 | Embedding-based methods have attracted increasing attention in recent entity alignment (EA) studies. Although great promise they can offer, there are still several limitations. The most notable is that they identify the aligned entities based on cosine similarity, ignoring the semantics underlying the embeddings themselves. Furthermore, these methods are... | Huajun Chen, Lingbing Guo, Qiang Zhang, Yuqiang Han |  |
| 393 |  |  [Breaking Down Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.218) |  | 0 | While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data... | Graham Neubig, TingRui Chiang, YiPei Chen, YiTing Yeh |  |
| 394 |  |  [Mitigating Contradictions in Dialogue Based on Contrastive Learning](https://doi.org/10.18653/v1/2022.findings-acl.219) |  | 0 | Chatbot models have achieved remarkable progress in recent years but tend to yield contradictory responses. In this paper, we exploit the advantage of contrastive learning technique to mitigate this issue. To endow the model with the ability of discriminating contradictory patterns, we minimize the similarity between the target response and contradiction... | Ben Liao, Junsheng Kong, Weizhao Li, Yi Cai |  |
| 395 |  |  [ELLE: Efficient Lifelong Pre-training for Emerging Data](https://doi.org/10.18653/v1/2022.findings-acl.220) |  | 0 | Current pre-trained language models (PLM) are typically trained with static data, ignoring that in real-world scenarios, streaming data of various sources may continuously grow. This requires PLMs to integrate the information from all the sources in a lifelong manner. Although this goal could be achieved by exhaustive pre-training on all the existing data, such... | Jiajie Zhang, Jie Zhou, Maosong Sun, Peng Li, Yankai Lin, Yujia Qin, Zhiyuan Liu |  |
| 396 |  |  [EnCBP: A New Benchmark Dataset for Finer-Grained Cultural Background Prediction in English](https://doi.org/10.18653/v1/2022.findings-acl.221) |  | 0 | While cultural backgrounds have been shown to affect linguistic expressions, existing natural language processing (NLP) research on culture modeling is overly coarse-grained and does not examine cultural differences among speakers of the same language. To address this problem and augment NLP models with cultural background features, we collect, annotate,... | Lili Wang, Samiha Datta, Soroush Vosoughi, Weicheng Ma |  |
| 397 |  |  [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://doi.org/10.18653/v1/2022.findings-acl.222) |  | 0 | Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor... | Eric Wallace, Fabio Petroni, Ivana Balazevic, Robert L. Logan IV, Sameer Singh, Sebastian Riedel |  |
| 398 |  |  [uFACT: Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.223) |  | 0 | We propose uFACT (Un-Faithful Alien Corpora Training), a training corpus construction method for data-to-text (d2t) generation models. We show that d2t models trained on uFACT datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. Our approach is to augment the... | Alexandru Coca, Bill Byrne, Tisha Anders |  |
| 399 |  |  [Good Night at 4 pm?! Time Expressions in Different Cultures](https://doi.org/10.18653/v1/2022.findings-acl.224) |  | 0 | We propose the task of culture-specific time expression grounding, i.e. mapping from expressions such as “morning” in English or “Manhã” in Portuguese to specific hours in the day. We propose 3 language-agnostic methods, one of which achieves promising results on gold standard annotations that we collected for a small number of languages. We then apply this... | Vered Shwartz |  |
| 400 |  |  [Extracting Person Names from User Generated Text: Named-Entity Recognition for Combating Human Trafficking](https://doi.org/10.18653/v1/2022.findings-acl.225) |  | 0 | Online escort advertisement websites are widely used for advertising victims of human trafficking. Domain experts agree that advertising multiple people in the same ad is a strong indicator of trafficking. Thus, extracting person names from the text of these ads can provide valuable clues for further analysis. However, Named-Entity Recognition (NER) on escort... | Kellin Pelrine, Pratheeksha Nair, Reihaneh Rabbany, Yifei Li |  |
| 401 |  |  [OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.226) |  | 0 | Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as Machine Translation. In this work, we present OneAligner, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource... | Caiming Xiong, Kazuma Hashimoto, Tong Niu, Yingbo Zhou |  |
| 402 |  |  [Suum Cuique: Studying Bias in Taboo Detection with a Community Perspective](https://doi.org/10.18653/v1/2022.findings-acl.227) |  | 0 | Prior research has discussed and illustrated the need to consider linguistic norms at the community level when studying taboo (hateful/offensive/toxic etc.) language. However, a methodology for doing so, that is firmly founded on community language norms is still largely absent. This can lead both to biases in taboo text classification and limitations in our... | Jonathan Rusert, Osama Khalid, Padmini Srinivasan |  |
| 403 |  |  [Modeling Intensification for Sign Language Generation: A Computational Approach](https://doi.org/10.18653/v1/2022.findings-acl.228) |  | 0 | End-to-end sign language generation models do not accurately represent the prosody in sign language. A lack of temporal and spatial variations leads to poor-quality generated presentations that confuse human interpreters. In this paper, we aim to improve the prosody in generated sign languages by modeling intensification in a data-driven manner. We present... | Lorna C. Quandt, Malihe Alikhani, Mert Inan, Sabit Hassan, Yang Zhong |  |
| 404 |  |  [Controllable Natural Language Generation with Contrastive Prefixes](https://doi.org/10.18653/v1/2022.findings-acl.229) |  | 0 | To guide the generation of large pretrained language models (LM), previous work has focused on directly fine-tuning the language model or utilizing an attribute discriminator. In this work, we propose a novel lightweight framework for controllable GPT2 generation, which utilizes a set of small attribute-specific vectors, called prefixes (Li and Liang, 2021), to... | Furu Wei, Jing Qian, Li Dong, Weizhu Chen, Yelong Shen |  |
| 405 |  |  [Revisiting the Effects of Leakage on Dependency Parsing](https://doi.org/10.18653/v1/2022.findings-acl.230) |  | 0 | Recent work by Søgaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed leakage) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings.... | Antonios Anastasopoulos, Miriam Wanner, Nathaniel Krasner |  |
| 406 |  |  [Learning to Describe Solutions for Bug Reports Based on Developer Discussions](https://doi.org/10.18653/v1/2022.findings-acl.231) |  | 0 | When a software bug is reported, developers engage in a discussion to collaboratively resolve it. While the solution is likely formulated within the discussion, it is often buried in a large amount of text, making it difficult to comprehend and delaying its implementation. To expedite bug resolution, we propose generating a concise natural language description... | Junyi Jessy Li, Milos Gligoric, Raymond J. Mooney, Sheena Panthaplackel |  |
| 407 |  |  [Perturbations in the Wild: Leveraging Human-Written Text Perturbations for Realistic Adversarial Attack and Defense](https://doi.org/10.18653/v1/2022.findings-acl.232) |  | 0 | We proposes a novel algorithm, ANTHRO, that inductively extracts over 600K human-written text perturbations in the wild and leverages them for realistic adversarial attack. Unlike existing character-based attacks which often deductively hypothesize a set of manipulation strategies, our work is grounded on actual observations from real-world texts. We find that... | Dongwon Lee, Jooyoung Lee, Kevin Yen, Thai Le, Yifan Hu |  |
| 408 |  |  [Improving Chinese Grammatical Error Detection via Data augmentation by Conditional Error Generation](https://doi.org/10.18653/v1/2022.findings-acl.233) |  | 0 | Chinese Grammatical Error Detection(CGED) aims at detecting grammatical errors in Chinese texts. One of the main challenges for CGED is the lack of annotated data. To alleviate this problem, previous studies proposed various methods to automatically generate more training samples, which can be roughly categorized into rule-based methods and model-based methods.... | Huihui Cai, Shengkang Song, Shulin Liu, Tao Yang, Tianchi Yue, Tinghao Yu |  |
| 409 |  |  [Modular and Parameter-Efficient Multimodal Fusion with Prompting](https://doi.org/10.18653/v1/2022.findings-acl.234) |  | 0 | Recent research has made impressive progress in large-scale multimodal pre-training. In the context of the rapid growth of model size, it is necessary to seek efficient and flexible methods other than finetuning. In this paper, we propose to use prompt vectors to align the modalities. Our method achieves comparable performance to several other multimodal fusion... | Hinrich Schütze, Mengjie Zhao, Sheng Liang |  |
| 410 |  |  [Synchronous Refinement for Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.235) |  | 0 | Machine translation typically adopts an encoder-to-decoder framework, in which the decoder generates the target sentence word-by-word in an auto-regressive manner. However, the auto-regressive decoder faces a deep-rooted one-pass issue whereby each generated word is considered as one element of the final output regardless of whether it is correct or not. These... | Eiichiro Sumita, Kehai Chen, Masao Utiyama, Min Zhang, Rui Wang |  |
| 411 |  |  [HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing](https://doi.org/10.18653/v1/2022.findings-acl.236) |  | 0 | Recently, context-dependent text-to-SQL semantic parsing which translates natural language into SQL in an interaction process has attracted a lot of attentions. Previous works leverage context dependence information either from interaction history utterances or previous predicted queries but fail in taking advantage of both of them since of the mismatch between... | Baohua Dong, Changshan Li, Haibin Wang, Xingjun Wang, Yanzhao Zheng |  |
| 412 |  |  [CRASpell: A Contextual Typo Robust Approach to Improve Chinese Spelling Correction](https://doi.org/10.18653/v1/2022.findings-acl.237) |  | 0 | Recently, Bert-based models have dominated the research of Chinese spelling correction (CSC). These methods have two limitations: (1) they have poor performance on multi-typo texts. In such texts, the context of each typo contains at least one misspelled character, which brings noise information. Such noisy context leads to the declining performance on... | Huihui Cai, Shengkang Song, Shengli Sun, Shulin Liu, Tao Yang, Tianchi Yue, Tinghao Yu |  |
| 413 |  |  [Gaussian Multi-head Attention for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.238) |  | 0 | Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency,... | Shaolei Zhang, Yang Feng |  |
| 414 |  |  [Composing Structure-Aware Batches for Pairwise Sentence Classification](https://doi.org/10.18653/v1/2022.findings-acl.239) |  | 0 | Identifying the relation between two sentences requires datasets with pairwise annotations. In many cases, these datasets contain instances that are annotated multiple times as part of different pairs. They constitute a structure that contains additional helpful information about the inter-relatedness of the text instances based on the annotations. This paper... | Andreas Waldis, Iryna Gurevych, Tilman Beck |  |
| 415 |  |  [Factual Consistency of Multilingual Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.240) |  | 0 | Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they... | Anders Søgaard, Constanza Fierro |  |
| 416 |  |  [Selecting Stickers in Open-Domain Dialogue through Multitask Learning](https://doi.org/10.18653/v1/2022.findings-acl.241) |  | 0 | With the increasing popularity of online chatting, stickers are becoming important in our online communication. Selecting appropriate stickers in open-domain dialogue requires a comprehensive understanding of both dialogues and stickers, as well as the relationship between the two types of modalities. To tackle these challenges, we propose a multitask learning... | Jie Zhou, Jinchao Zhang, Yeshuang Zhu, Zhengcong Fei, Zhexin Zhang |  |
| 417 |  |  [ZiNet: Linking Chinese Characters Spanning Three Thousand Years](https://doi.org/10.18653/v1/2022.findings-acl.242) |  | 0 | Modern Chinese characters evolved from 3,000 years ago. Up to now, tens of thousands of glyphs of ancient characters have been discovered, which must be deciphered by experts to interpret unearthed documents. Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods. However, it is inevitably... | Chuntao Li, Daqian Shi, Fausto Giunchiglia, Hao Xu, Xiaolei Diao, Yang Chi |  |
| 418 |  |  [How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?](https://doi.org/10.18653/v1/2022.findings-acl.243) |  | 0 | Cross-lingual Entity Typing (CLET) aims at improving the quality of entity type prediction by transferring semantic knowledge learned from rich-resourced languages to low-resourced languages. In this paper, by utilizing multilingual transfer learning via the mixture-of-experts approach, our model dynamically capture the relationship between target language and... | Hailong Jin, Hui Chen, Juanzi Li, Lei Hou, Tiansi Dong, Yincen Qu, Zelin Dai |  |
| 419 |  |  [AMR-DA: Data Augmentation by Abstract Meaning Representation](https://doi.org/10.18653/v1/2022.findings-acl.244) |  | 0 | Abstract Meaning Representation (AMR) is a semantic representation for NLP/NLU. In this paper, we propose to use it for data augmentation in NLP. Our proposed data augmentation technique, called AMR-DA, converts a sample sentence to an AMR graph, modifies the graph according to various data augmentation policies, and then generates augmentations from graphs.... | Fangzhen Lin, Yuxin Jiang, Ziyi Shou |  |
| 420 |  |  [Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study](https://doi.org/10.18653/v1/2022.findings-acl.245) |  | 0 | In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most... | Helena Bonaldi, Marco Guerini, Margherita Fanton, Serra Sinem Tekiroglu |  |
| 421 |  |  [Improving Robustness of Language Models from a Geometry-aware Perspective](https://doi.org/10.18653/v1/2022.findings-acl.246) |  | 0 | Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean... | Bin Zhu, Jinyin Chen, Le Wang, Qi Xuan, Zhaoquan Gu |  |
| 422 |  |  [Task-guided Disentangled Tuning for Pretrained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.247) |  | 0 | Pretrained language models (PLMs) trained on large-scale unlabeled corpus are typically fine-tuned on task-specific downstream datasets, which have produced state-of-the-art results on various NLP tasks. However, the data discrepancy issue in domain and scale makes fine-tuning fail to efficiently capture task-specific patterns, especially in low data regime. To... | Jiali Zeng, Mu Li, Shuangzhi Wu, Yongjing Yin, Yufan Jiang |  |
| 423 |  |  [Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding](https://doi.org/10.18653/v1/2022.findings-acl.248) |  | 0 | Contrastive learning is emerging as a powerful technique for extracting knowledge from unlabeled data. This technique requires a balanced mixture of two ingredients: positive (similar) and negative (dissimilar) samples. This is typically achieved by maintaining a queue of negative samples during training. Prior works in the area typically uses a fixed-length... | Jie Ren, Jie Zheng, Ling Gao, Rui Cao, Yihao Wang, Yuxin Liang, Zheng Wang |  |
| 424 |  |  [The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through](https://doi.org/10.18653/v1/2022.findings-acl.249) |  | 0 | Language models are increasingly becoming popular in AI-powered scientific IR systems. This paper evaluates popular scientific language models in handling (i) short-query texts and (ii) textual neighbors. Our experiments showcase the inability to retrieve relevant documents for a short-query text even under the most relaxed conditions. Additionally, we leverage... | Mayank Singh, Shruti Singh |  |
| 425 |  |  [Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition](https://doi.org/10.18653/v1/2022.findings-acl.250) |  | 0 | Nested entities are observed in many domains due to their compositionality, which cannot be easily recognized by the widely-used sequence labeling framework.A natural solution is to treat the task as a span classification problem. To learn better span representation and increase classification performance, it is crucial to effectively integrate heterogeneous... | Chuanqi Tan, Fei Huang, Songfang Huang, Zheng Yuan |  |
| 426 |  |  [UNIMO-2: End-to-End Unified Vision-Language Grounded Learning](https://doi.org/10.18653/v1/2022.findings-acl.251) |  | 0 | Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end unified-modal pre-training... | Can Gao, Guocheng Niu, Haifeng Wang, Hao Liu, Hua Wu, Jiachen Liu, Wei Li, Xinyan Xiao |  |
| 427 |  |  [The Past Mistake is the Future Wisdom: Error-driven Contrastive Probability Optimization for Chinese Spell Checking](https://doi.org/10.18653/v1/2022.findings-acl.252) |  | 0 | Chinese Spell Checking (CSC) aims to detect and correct Chinese spelling errors, which are mainly caused by the phonological or visual similarity. Recently, pre-trained language models (PLMs) promote the progress of CSC task. However, there exists a gap between the learned knowledge of PLMs and the goal of CSC task. PLMs focus on the semantics in text and tend... | Chao Li, HaiTao Zheng, Qingyu Zhou, Rongyi Sun, Ruiyang Liu, Yangning Li, Yinghui Li, Yunbo Cao, Zhongli Li, Zizhen Wang |  |
| 428 |  |  [XFUND: A Benchmark Dataset for Multilingual Visually Rich Form Understanding](https://doi.org/10.18653/v1/2022.findings-acl.253) |  | 0 | Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually rich document understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. However, the existed research work has focused only on the English domain while neglecting the importance of multilingual... | Cha Zhang, Dinei A. F. Florêncio, Furu Wei, Guoxin Wang, Lei Cui, Tengchao Lv, Yiheng Xu, Yijuan Lu |  |
| 429 |  |  [Type-Driven Multi-Turn Corrections for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.findings-acl.254) |  | 0 | Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors. In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference. Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks.... | Chao Li, Jiali Zeng, Jinsong Su, Qingyu Zhou, Shaopeng Lai, Yunbo Cao, Zhongli Li |  |
| 430 |  |  [Leveraging Knowledge in Multilingual Commonsense Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.255) |  | 0 | Commonsense reasoning (CSR) requires models to be equipped with general world knowledge. While CSR is a language-agnostic process, most comprehensive knowledge sources are restricted to a small number of languages, especially English. Thus, it remains unclear how to effectively conduct multilingual commonsense reasoning (XCSR) for various languages. In this... | Chenguang Zhu, Michael Zeng, Ruochen Xu, Shuohang Wang, Siqi Sun, Yichong Xu, Yuwei Fang |  |
| 431 |  |  [Encoding and Fusing Semantic Connection and Linguistic Evidence for Implicit Discourse Relation Recognition](https://doi.org/10.18653/v1/2022.findings-acl.256) |  | 0 | Prior studies use one attention mechanism to improve contextual semantic representation learning for implicit discourse relation recognition (IDRR). However, diverse relation senses may benefit from different attention mechanisms. We also argue that some linguistic relation in between two words can be further exploited for IDRR. This paper proposes a... | Bang Wang, Lu Dai, Wei Xiang, Yijun Mo |  |
| 432 |  |  [One Agent To Rule Them All: Towards Multi-agent Conversational AI](https://doi.org/10.18653/v1/2022.findings-acl.257) |  | 0 | The increasing volume of commercially available conversational agents (CAs) on the market has resulted in users being burdened with learning and adopting multiple agents to accomplish their tasks. Though prior work has explored supporting a multitude of domains within the design of a single agent, the interaction experience suffers due to the large action space... | Christopher Clarke, Jason Mars, Joseph Peper, Karthik Krishnamurthy, Kevin Leach, Lingjia Tang, Walter S. Lasecki, Walter Talamonti, Yiping Kang |  |
| 433 |  |  [Word-level Perturbation Considering Word Length and Compositional Subwords](https://doi.org/10.18653/v1/2022.findings-acl.258) |  | 0 | We present two simple modifications for word-level perturbation: Word Replacement considering Length (WR-L) and Compositional Word Replacement (CWR).In conventional word replacement, a word in an input is replaced with a word sampled from the entire vocabulary, regardless of the length and context of the target word.WR-L considers the length of a target word by... | Atsushi Keyaki, Kei Uchiumi, Naoaki Okazaki, Sho Takase, Tatsuya Hiraoka |  |
| 434 |  |  [Bridging Pre-trained Language Models and Hand-crafted Features for Unsupervised POS Tagging](https://doi.org/10.18653/v1/2022.findings-acl.259) |  | 0 | In recent years, large-scale pre-trained language models (PLMs) have made extraordinary progress in most NLP tasks. But, in the unsupervised POS tagging task, works utilizing PLMs are few and fail to achieve state-of-the-art (SOTA) performance. The recent SOTA performance is yielded by a Guassian HMM variant proposed by He et al. (2018). However, as a... | Houquan Zhou, Min Zhang, Yang Li, Zhenghua Li |  |
| 435 |  |  [Controlling the Focus of Pretrained Language Generation Models](https://doi.org/10.18653/v1/2022.findings-acl.260) |  | 0 | The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model’s focus. This work aims to develop a control mechanism by which a user can select spans of... | James R. Glass, Jiabao Ji, Tianxing He, Yoon Kim |  |
| 436 |  |  [Comparative Opinion Summarization via Collaborative Decoding](https://doi.org/10.18653/v1/2022.findings-acl.261) |  | 0 | Opinion summarization focuses on generating summaries that reflect popular subjective information expressed in multiple online reviews. While generated summaries offer general and concise information about a particular hotel or product, the information may be insufficient to help the user compare multiple different choices. Thus, the user may still struggle... | Hayate Iso, Stefanos Angelidis, Xiaolan Wang, Yoshihiko Suhara |  |
| 437 |  |  [IsoScore: Measuring the Uniformity of Embedding Space Utilization](https://doi.org/10.18653/v1/2022.findings-acl.262) |  | 0 | The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine... | Carsten Eickhoff, Nate Gillman, Taylor Rayne, William Rudman |  |
| 438 |  |  [A Natural Diet: Towards Improving Naturalness of Machine Translation Output](https://doi.org/10.18653/v1/2022.findings-acl.263) |  | 0 | Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower... | Colin Cherry, David Grangier, David Vilar, George F. Foster, Markus Freitag |  |
| 439 |  |  [From Stance to Concern: Adaptation of Propositional Analysis to New Tasks and Domains](https://doi.org/10.18653/v1/2022.findings-acl.264) |  | 0 | We present a generalized paradigm for adaptation of propositional analysis (predicate-argument pairs) to new tasks and domains. We leverage an analogy between stances (belief-driven sentiment) and concerns (topical issues with moral dimensions/endorsements) to produce an explanatory representation. A key contribution is the combination of semi-automatic... | Adam Dalton, Bonnie J. Dorr, Brodie Mather, Owen Rambow, Sonja SchmerGalunder, William de Beaumont |  |
| 440 |  |  [CUE Vectors: Modular Training of Language Models Conditioned on Diverse Contextual Signals](https://doi.org/10.18653/v1/2022.findings-acl.265) |  | 0 | We propose a framework to modularize the training of neural language models that use diverse forms of context by eliminating the need to jointly train context and within-sentence encoders. Our approach, contextual universal embeddings (CUE), trains LMs on one type of contextual data and adapts to novel context types. The model consists of a pretrained neural... | Andreas Stolcke, Scott Novotney, Sreeparna Mukherjee, Zeeshan Ahmed |  |
| 441 |  |  [Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning](https://doi.org/10.18653/v1/2022.findings-acl.266) |  | 0 | We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our cross-lingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a fine-tuned... | Michael Elhadad, Rina Galperin, Shachar Schnapp |  |
| 442 |  |  [Aligned Weight Regularizers for Pruning Pretrained Neural Networks](https://doi.org/10.18653/v1/2022.findings-acl.267) |  | 0 | Pruning aims to reduce the number of parameters while maintaining performance close to the original network. This work proposes a novel self-distillation based pruning strategy, whereby the representational similarity between the pruned and unpruned versions of the same network is maximized. Unlike previous approaches that treat distillation and pruning... | Haytham Assem, James O'Neill, Sourav Dutta |  |
| 443 |  |  [Consistent Representation Learning for Continual Relation Extraction](https://doi.org/10.18653/v1/2022.findings-acl.268) |  | 0 | Continual relation extraction (CRE) aims to continuously train a model on data with new relations while avoiding forgetting old ones. Some previous work has proved that storing a few typical samples of old relations and replaying them when learning new relations can effectively avoid forgetting. However, these memory-based methods tend to overfit the memory... | Hua Xu, Jiangong Yang, Kai Gao, Kang Zhao |  |
| 444 |  |  [Event Transition Planning for Open-ended Text Generation](https://doi.org/10.18653/v1/2022.findings-acl.269) |  | 0 | Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context. The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. Despite these neural models are good at producing human-like text, it is... | Lingpeng Kong, Piji Li, Qintong Li, Wei Bi, Yuxuan Lai, Zhaochun Ren |  |
| 445 |  |  [Comprehensive Multi-Modal Interactions for Referring Image Segmentation](https://doi.org/10.18653/v1/2022.findings-acl.270) |  | 0 | We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the natural language description. Addressing RIS efficiently requires considering the interactions happening across visual and linguistic modalities and the interactions within each modality. Existing methods are limited because they either compute different... | Kanishk Jain, Vineet Gandhi |  |
| 446 |  |  [MetaWeighting: Learning to Weight Tasks in Multi-Task Learning](https://doi.org/10.18653/v1/2022.findings-acl.271) |  | 0 | Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization... | Pengtao Xie, Weiwei Liu, Xuemin Lin, Yuren Mao, Zekai Wang |  |
| 447 |  |  [Improving Controllable Text Generation with Position-Aware Weighted Decoding](https://doi.org/10.18653/v1/2022.findings-acl.272) |  | 0 | Weighted decoding methods composed of the pretrained language model (LM) and the controller have achieved promising results for controllable text generation. However, these models often suffer from a control strength/fluency trade-off problem as higher control strength is more likely to generate incoherent and repetitive text. In this paper, we illustrate this... | Bing Qin, Heng Gong, Jiaming Wu, Sicheng Ma, Xiaocheng Feng, Yuxuan Gu |  |
| 448 |  |  [Prompt Tuning for Discriminative Pre-trained Language Models](https://doi.org/10.18653/v1/2022.findings-acl.273) |  | 0 | Recent works have shown promising results of prompt tuning in stimulating pre-trained language models (PLMs) for natural language processing (NLP) tasks. However, to the best of our knowledge, existing works focus on prompt-tuning generative PLMs that are pre-trained to generate target tokens, such as BERT. It is still unknown whether and how discriminative... | Ao Zhang, Bowen Dong, Jianyong Wang, Leyu Lin, Maosong Sun, Ruobing Xie, Yuan Yao, Zhengyan Zhang, Zhiyuan Liu |  |
| 449 |  |  [Two Birds with One Stone: Unified Model Learning for Both Recall and Ranking in News Recommendation](https://doi.org/10.18653/v1/2022.findings-acl.274) |  | 0 | Recall and ranking are two critical steps in personalized news recommendation. Most existing news recommender systems conduct personalized news recall and ranking separately with different models. However, maintaining multiple models leads to high computational cost and poses great challenges to meeting the online latency requirement of news recommender... | Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang |  |
| 450 |  |  [What does it take to bake a cake? The RecipeRef corpus and anaphora resolution in procedural text](https://doi.org/10.18653/v1/2022.findings-acl.275) |  | 0 | Procedural text contains rich anaphoric phenomena, yet has not received much attention in NLP. To fill this gap, we investigate the textual properties of two types of procedural text, recipes and chemical patents, and generalize an anaphora annotation framework developed for the chemical domain for modeling anaphoric phenomena in recipes. We apply this... | Biaoyan Fang, Karin Verspoor, Timothy Baldwin |  |
| 451 |  |  [MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.276) |  | 0 | Logical reasoning is of vital importance to natural language understanding. Previous studies either employ graph-based models to incorporate prior knowledge about logical relations, or introduce symbolic logic into neural models through data augmentation. These methods, however, heavily depend on annotated training data, and thus suffer from over-fitting and... | Fangkai Jiao, Liqiang Nie, Xuemeng Song, Yangyang Guo |  |
| 452 |  |  [THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption](https://doi.org/10.18653/v1/2022.findings-acl.277) |  | 0 | As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only... | Binxing Jiao, Daxin Jiang, Furu Wei, Hangbo Bao, Haoyi Zhou, Jianxin Li, Li Dong, Shaohan Huang, Tianyu Chen |  |
| 453 |  |  [HLDC: Hindi Legal Documents Corpus](https://doi.org/10.18653/v1/2022.findings-acl.278) |  | 0 | Many populous countries including India are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the... | Akshala Bhatnagar, Amul Agrawal, Anmol Goel, Arnab Bhattacharya, Arnav Kapoor, Ashutosh Modi, Mudit Dhawan, Ponnurangam Kumaraguru, T. H. Arjun, Vibhu Agrawal |  |
| 454 |  |  [Rethinking Document-level Neural Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.279) |  | 0 | This paper does not aim at introducing a novel model for document-level neural machine translation. Instead, we head back to the original Transformer model and hope to answer the following question: Is the capacity of current models strong enough for document-level translation? Interestingly, we observe that the original Transformer with appropriate training... | Chengqi Zhao, Hao Zhou, Jiajun Chen, Lei Li, Mingxuan Wang, Shujian Huang, Zewei Sun |  |
| 455 |  |  [Incremental Intent Detection for Medical Domain with Contrast Replay Networks](https://doi.org/10.18653/v1/2022.findings-acl.280) |  | 0 | Conventional approaches to medical intent detection require fixed pre-defined intent categories. However, due to the incessant emergence of new medical intents in the real world, such requirement is not practical. Considering that it is computationally expensive to store and re-train the whole data every time new data and intents come in, we propose to... | Guirong Bai, Jun Zhao, Kang Liu, Shizhu He |  |
| 456 |  |  [LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval](https://doi.org/10.18653/v1/2022.findings-acl.281) |  | 0 | In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever that does not require any supervised data for training. Specifically, we first present Iterative Contrastive Learning (ICoL) that iteratively trains the query and document encoders with a cache mechanism. ICoL not only enlarges the number of negative instances but also keeps... | Canwen Xu, Daya Guo, Julian J. McAuley, Nan Duan |  |
| 457 |  |  [Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach](https://doi.org/10.18653/v1/2022.findings-acl.282) |  | 0 | In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1)... | Jie Zhou, Juanzi Li, Lei Hou, Peng Li, Xin Lv, Yankai Lin, Yixin Cao, Zhiyuan Liu |  |
| 458 |  |  [EICO: Improving Few-Shot Text Classification via Explicit and Implicit Consistency Regularization](https://doi.org/10.18653/v1/2022.findings-acl.283) |  | 0 | While the prompt-based fine-tuning methods had advanced few-shot natural language understanding tasks, self-training methods are also being explored. This work revisits the consistency regularization in self-training and presents explicit and implicit consistency regularization enhanced language model (EICO). By employing both explicit and implicit consistency... | Cheng Yao, Lei Zhao |  |
| 459 |  |  [Improving the Adversarial Robustness of NLP Models by Information Bottleneck](https://doi.org/10.18653/v1/2022.findings-acl.284) |  | 0 | Existing studies have demonstrated that adversarial examples can be directly attributed to the presence of non-robust features, which are highly predictive, but can be easily manipulated by adversaries to fool NLP models. In this study, we explore the feasibility of capturing task-specific robust features, while eliminating the non-robust ones by using the... | Cenyuan Zhang, ChoJui Hsieh, KaiWei Chang, Xiang Zhou, Xiaoqing Zheng, Yixin Wan |  |
| 460 |  |  [Incorporating Dynamic Semantics into Pre-Trained Language Model for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.findings-acl.285) |  | 0 | Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a specific aspect in the given sentence. While pre-trained language models such as BERT have achieved great success, incorporating dynamic semantic changes into ABSA remains challenging. To this end, in this paper, we propose to address this problem by Dynamic Re-weighting BERT... | Enhong Chen, Hongke Zhao, Kai Zhang, Kun Zhang, Mengdi Zhang, Qi Liu, Wei Wu |  |
| 461 |  |  [DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition](https://doi.org/10.18653/v1/2022.findings-acl.286) |  | 0 | The task of joint dialog sentiment classification (DSC) and act recognition (DAR) aims to simultaneously predict the sentiment label and act label for each utterance in a dialog. In this paper, we put forward a new framework which models the explicit dependencies via integrating prediction-level interactions other than semantics-level interactions, more... | Bowen Xing, Ivor W. Tsang |  |
| 462 |  |  [Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents](https://doi.org/10.18653/v1/2022.findings-acl.287) |  | 0 | Text semantic matching is a fundamental task that has been widely used in various scenarios, such as community question answering, information retrieval, and recommendation. Most state-of-the-art matching models, e.g., BERT, directly perform text comparison by processing each word uniformly. However, a query sentence generally comprises content that calls for... | Daniel Wang, Haixiang Li, Hongwei Liu, Junzhe Wang, Meng Tang, Qi Zhang, Tao Gui, Yicheng Zou |  |
| 463 |  |  [Modular Domain Adaptation](https://doi.org/10.18653/v1/2022.findings-acl.288) |  | 0 | Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment. However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model... | Dallas Card, Dan Jurafsky, Junshen K. Chen |  |
| 464 |  |  [Detection of Adversarial Examples in Text Classification: Benchmark and Baseline via Robust Density Estimation](https://doi.org/10.18653/v1/2022.findings-acl.289) |  | 0 | Word-level adversarial attacks have shown success in NLP models, drastically decreasing the performance of transformer-based models in recent years. As a countermeasure, adversarial defense has been explored, but relatively few efforts have been made to detect adversarial examples. However, detecting adversarial examples may be crucial for automated tasks (e.g.... | Jangho Kim, Jiho Jang, KiYoon Yoo, Nojun Kwak |  |
| 465 |  |  [Platt-Bin: Efficient Posterior Calibrated Training for NLP Classifiers](https://doi.org/10.18653/v1/2022.findings-acl.290) |  | 0 | Modern NLP classifiers are known to return uncalibrated estimations of class posteriors. Existing methods for posterior calibration rescale the predicted probabilities but often have an adverse impact on final classification accuracy, thus leading to poorer generalization. We propose an end-to-end trained calibrator, Platt-Binning, that directly optimizes the... | Rishabh Singh, Shirin Goshtasbpour |  |
| 466 |  |  [Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation](https://doi.org/10.18653/v1/2022.findings-acl.291) |  | 0 | We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: (1) lack of similar datasets/models from a related domain, (2) inability to sample useful logical forms directly from a grammar, and (3) privacy requirements for unlabeled natural utterances. Our goal is to... | Benjamin Van Durme, Charles Chen, Kevin Yang, Olivia Deng, Richard Shin, Subhro Roy |  |
| 467 |  |  [Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking](https://doi.org/10.18653/v1/2022.findings-acl.292) |  | 0 | Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB). Many previous studies focus on Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB. The scale of Wikidata can open up many new real-world applications, but its massive number of... | ChengXiang Zhai, Heng Ji, Tuan Manh Lai |  |
| 468 |  |  [Local Structure Matters Most: Perturbation Study in NLU](https://doi.org/10.18653/v1/2022.findings-acl.293) |  | 0 | Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words. In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural... | Amal Zouaq, Louis Clouâtre, Prasanna Parthasarathi, Sarath Chandar |  |
| 469 |  |  [Probing Factually Grounded Content Transfer with Factual Ablation](https://doi.org/10.18653/v1/2022.findings-acl.294) |  | 0 | Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality–it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual... | Chris Quirk, Michel Galley, Peter West, Yejin Choi |  |
| 470 |  |  [ED2LM: Encoder-Decoder to Language Model for Faster Document Re-ranking Inference](https://doi.org/10.18653/v1/2022.findings-acl.295) |  | 0 | State-of-the-art neural models typically encode document-query pairs using cross-attention for re-ranking. To this end, models generally utilize an encoder-only (like BERT) paradigm or an encoder-decoder (like T5) approach. These paradigms, however, are not without flaws, i.e., running the model on all query-document pairs at inference-time incurs a significant... | Cícero Nogueira dos Santos, Dara Bahri, Donald Metzler, Honglei Zhuang, Jai Prakash Gupta, Ji Ma, Jing Lu, Kai Hui, Tao Chen, Yi Tay, Zhen Qin |  |
| 471 |  |  [Benchmarking Answer Verification Methods for Question Answering-Based Summarization Evaluation Metrics](https://doi.org/10.18653/v1/2022.findings-acl.296) |  | 0 | Question answering-based summarization evaluation metrics must automatically determine whether the QA model’s prediction is correct or not, a task known as answer verification. In this work, we benchmark the lexical answer verification methods which have been used by current QA-based metrics as well as two more sophisticated text comparison methods, BERTScore... | Dan Roth, Daniel Deutsch |  |
| 472 |  |  [Prior Knowledge and Memory Enriched Transformer for Sign Language Translation](https://doi.org/10.18653/v1/2022.findings-acl.297) |  | 0 | This paper attacks the challenging problem of sign language translation (SLT), which involves not only visual and textual understanding but also additional prior knowledge learning (i.e. performing style, syntax). However, the majority of existing methods with vanilla encoder-decoder structures fail to sufficiently explore all of them. Based on this concern, we... | Meng Zhang, Tao Jin, Xingshan Zeng, Zhou Zhao |  |
| 473 |  |  [Discontinuous Constituency and BERT: A Case Study of Dutch](https://doi.org/10.18653/v1/2022.findings-acl.298) |  | 0 | In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small... | Gijs Wijnholds, Konstantinos Kogkalidis |  |
| 474 |  |  [Probing Multilingual Cognate Prediction Models](https://doi.org/10.18653/v1/2022.findings-acl.299) |  | 0 | Character-based neural machine translation models have become the reference models for cognate prediction, a historical linguistics task. So far, all linguistic interpretations about latent information captured by such models have been based on external analysis (accuracy, raw results, errors). In this paper, we investigate what probing can tell us about both... | Benoît Sagot, Clémentine Fourrier |  |
| 475 |  |  [A Neural Pairwise Ranking Model for Readability Assessment](https://doi.org/10.18653/v1/2022.findings-acl.300) |  | 0 | Automatic Readability Assessment (ARA), the task of assigning a reading level to a text, is traditionally treated as a classification problem in NLP research. In this paper, we propose the first neural, pairwise ranking approach to ARA and compare it with existing classification, regression, and (non-neural) ranking methods. We establish the performance of our... | Justin Lee, Sowmya Vajjala |  |
| 476 |  |  [First the Worst: Finding Better Gender Translations During Beam Search](https://doi.org/10.18653/v1/2022.findings-acl.301) |  | 0 | Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or... | Bill Byrne, Danielle Saunders, Rosie Sallis |  |
| 477 |  |  [Dialogue Summaries as Dialogue States (DS2), Template-Guided Summarization for Few-shot Dialogue State Tracking](https://doi.org/10.18653/v1/2022.findings-acl.302) |  | 0 | Annotating task-oriented dialogues is notorious for the expensive and difficult data collection process. Few-shot dialogue state tracking (DST) is a realistic solution to this problem. In this paper, we hypothesize that dialogue summaries are essentially unstructured dialogue states; hence, we propose to reformulate dialogue state tracking as a dialogue... | Andrea Madotto, Hangyeol Yu, Hyeongdon Moon, Jamin Shin, Juneyoung Park |  |
| 478 |  |  [Unsupervised Preference-Aware Language Identification](https://doi.org/10.18653/v1/2022.findings-acl.303) |  | 0 | Recognizing the language of ambiguous texts has become a main challenge in language identification (LID). When using multilingual applications, users have their own language preferences, which can be regarded as external knowledge for LID. Nevertheless, current studies do not consider the inter-personal variations due to the lack of user annotated training... | Baosong Yang, Dayiheng Liu, Haibo Zhang, Jun Xie, Liang Yao, Xiaoyu Lv, Xingzhang Ren |  |
| 479 |  |  [Using NLP to quantify the environmental cost and diversity benefits of in-person NLP conferences](https://doi.org/10.18653/v1/2022.findings-acl.304) |  | 0 | The environmental costs of research are progressively important to the NLP community and their associated challenges are increasingly debated. In this work, we analyse the carbon cost (measured as CO2-equivalent) associated with journeys made by researchers attending in-person NLP conferences. We obtain the necessary data by text-mining all publications from... | Matthew Shardlow, Piotr Przybyla |  |
| 480 |  |  [Interpretable Research Replication Prediction via Variational Contextual Consistency Sentence Masking](https://doi.org/10.18653/v1/2022.findings-acl.305) |  | 0 | Research Replication Prediction (RRP) is the task of predicting whether a published research result can be replicated or not. Building an interpretable neural text classifier for RRP promotes the understanding of why a research paper is predicted as replicable or non-replicable and therefore makes its real-world application more reliable and trustworthy.... | Rui Meng, Tianyi Luo, Xin Wang, Yang Liu |  |
| 481 |  |  [Chinese Synesthesia Detection: New Dataset and Models](https://doi.org/10.18653/v1/2022.findings-acl.306) |  | 0 | In this paper, we introduce a new task called synesthesia detection, which aims to extract the sensory word of a sentence, and to predict the original and synesthetic sensory modalities of the corresponding sensory word. Synesthesia refers to the description of perceptions in one sensory modality through concepts from other modalities. It involves not only a... | Qingqing Zhao, Xiaotong Jiang, Yunfei Long, Zhongqing Wang |  |
| 482 |  |  [Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem](https://doi.org/10.18653/v1/2022.findings-acl.307) |  | 0 | We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and release SLIGHT, a dataset to support research on this task.... | Jason Naradowsky, Qiang Zhang, Yusuke Miyao |  |
| 483 |  |  [On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark](https://doi.org/10.18653/v1/2022.findings-acl.308) |  | 0 | Dialogue safety problems severely limit the real-world deployment of neural conversational models and have attracted great research interests recently. However, dialogue safety problems remain under-defined and the corresponding dataset is scarce. We propose a taxonomy for dialogue safety specifically designed to capture unsafe behaviors in human-bot dialogue... | Chujie Zheng, Guangxuan Xu, Hao Sun, Hao Zhou, Jiale Cheng, Jiawen Deng, Minlie Huang, Nanyun Peng, Xiaoyan Zhu |  |
| 484 |  |  [Word Segmentation by Separation Inference for East Asian Languages](https://doi.org/10.18653/v1/2022.findings-acl.309) |  | 0 | Chinese Word Segmentation (CWS) intends to divide a raw sentence into words through sequence labeling. Thinking in reverse, CWS can also be viewed as a process of grouping a sequence of characters into a sequence of words. In such a way, CWS is reformed as a separation inference task in every adjacent character pair. Since every character is either connected or... | Ge Chen, Guokai Zheng, Jingzhi Guo, Jizhe Zhou, Yu Tong |  |
| 485 |  |  [Unsupervised Chinese Word Segmentation with BERT Oriented Probing and Transformation](https://doi.org/10.18653/v1/2022.findings-acl.310) |  | 0 | Word Segmentation is a fundamental step for understanding Chinese language. Previous neural approaches for unsupervised Chinese Word Segmentation (CWS) only exploits shallow semantic information, which can miss important context. Large scale Pre-trained language models (PLM) have achieved great success in many areas because of its ability to capture the deep... | Qi Su, Wei Li, Yanqiu Shao, Yuhan Song |  |
| 486 |  |  [E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning](https://doi.org/10.18653/v1/2022.findings-acl.311) |  | 0 | The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical... | Changzhi Sun, Hao Zhou, Jiangjie Chen, Lei Li, Rui Xu, Wei Shi, Xinbo Zhang, Yanghua Xiao, Zhongqiao Li, Ziquan Fu |  |
| 487 |  |  [Implicit Relation Linking for Question Answering over Knowledge Graph](https://doi.org/10.18653/v1/2022.findings-acl.312) |  | 0 | Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of... | Chengfu Huo, Jiacheng Huang, Qijin Chen, Wei Hu, Weijun Ren, Xiaoxia Qiu, Yao Zhao |  |
| 488 |  |  [Attention Mechanism with Energy-Friendly Operations](https://doi.org/10.18653/v1/2022.findings-acl.313) |  | 0 | Attention mechanism has become the dominant module in natural language processing models. It is computationally intensive and depends on massive power-hungry multiplications. In this paper, we rethink variants of attention mechanism from the energy consumption aspects. After reaching the conclusion that the energy costs of several energy-friendly operations are... | Baosong Yang, Boxing Chen, Dayiheng Liu, Derek F. Wong, Haibo Zhang, Lidia S. Chao, Rong Xiao, Yu Wan |  |
| 489 |  |  [Probing BERT's priors with serial reproduction chains](https://doi.org/10.18653/v1/2022.findings-acl.314) |  | 0 | Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive... | Robert D. Hawkins, Takateru Yamakoshi, Thomas L. Griffiths |  |
| 490 |  |  [Interpreting the Robustness of Neural NLP Models to Textual Perturbations](https://doi.org/10.18653/v1/2022.findings-acl.315) |  | 0 | Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen... | Liangming Pan, MinYen Kan, Samson Tan, Yunxiang Zhang |  |
| 491 |  |  [Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations](https://doi.org/10.18653/v1/2022.findings-acl.316) |  | 0 | Dense retrieval (DR) methods conduct text retrieval by first encoding texts in the embedding space and then matching them by nearest neighbor search. This requires strong locality properties from the representation space, e.g., close allocations of each small group of relevant texts, which are hard to generalize to domains without sufficient training data. In... | Ankita Sharma, Ashwin Srinivasan, Chenyan Xiong, Damien Jose, Ji Xin, Paul Bennett |  |
| 492 |  |  [A Few-Shot Semantic Parser for Wizard-of-Oz Dialogues with the Precise ThingTalk Representation](https://doi.org/10.18653/v1/2022.findings-acl.317) |  | 0 | Previous attempts to build effective semantic parsers for Wizard-of-Oz (WOZ) conversations suffer from the difficulty in acquiring a high-quality, manually annotated training set. Approaches based only on dialogue synthesis are insufficient, as dialogues generated from state-machine based models are poor approximations of real-life conversations. Furthermore,... | Giovanni Campagna, Lucas Jun Koba Sato, Monica Lam, Ryan Kearns, Silei Xu, Sina J. Semnani |  |
| 493 |  |  [GCPG: A General Framework for Controllable Paraphrase Generation](https://doi.org/10.18653/v1/2022.findings-acl.318) |  | 0 | Controllable paraphrase generation (CPG) incorporates various external conditions to obtain desirable paraphrases. However, existing works only highlight a special condition under two indispensable aspects of CPG (i.e., lexically and syntactically CPG) individually, lacking a unified circumstance to explore and analyze their effectiveness. In this paper, we... | Baosong Yang, Boxing Chen, Dayiheng Liu, Haibo Zhang, Kexin Yang, Wenqiang Lei, Wenqing Yao, Xue Zhao |  |
| 494 |  |  [CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding](https://doi.org/10.18653/v1/2022.findings-acl.319) |  | 0 | Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as... | Ignacio Iacobacci, Milan Gritta, Ruoyu Hu |  |
| 495 |  |  [Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer](https://doi.org/10.18653/v1/2022.findings-acl.320) |  | 0 | We explore how a multi-modal transformer trained for generation of longer image descriptions learns syntactic and semantic representations about entities and relations grounded in objects at the level of masked self-attention (text generation) and cross-modal attention (information fusion). We observe that cross-attention learns the visual grounding of noun... | Nikolai Ilinykh, Simon Dobnik |  |
| 496 |  |  [Improving Zero-Shot Cross-lingual Transfer Between Closely Related Languages by Injecting Character-Level Noise](https://doi.org/10.18653/v1/2022.findings-acl.321) |  | 0 | Cross-lingual transfer between a high-resource language and its dialects or closely related language varieties should be facilitated by their similarity. However, current approaches that operate in the embedding space do not take surface similarity into account. This work presents a simple yet effective strategy to improve cross-lingual transfer between closely... | Noëmi Aepli, Rico Sennrich |  |
| 497 |  |  [Structural Supervision for Word Alignment and Machine Translation](https://doi.org/10.18653/v1/2022.findings-acl.322) |  | 0 | Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the... | Chun Yuan, Hongjia Li, Kai Fan, Lei Li |  |
| 498 |  |  [Focus on the Action: Learning to Highlight and Summarize Jointly for Email To-Do Items Summarization](https://doi.org/10.18653/v1/2022.findings-acl.323) |  | 0 | Automatic email to-do item generation is the task of generating to-do items from a given email to help people overview emails and schedule daily work. Different from prior research on email summarization, to-do item generation focuses on generating action mentions to provide more structured summaries of email text. Prior work either requires large amount of... | Diyi Yang, Jiaao Chen, Kexun Zhang |  |
| 499 |  |  [Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors](https://doi.org/10.18653/v1/2022.findings-acl.324) |  | 0 | In this paper, we explore the capacity of a language model-based method for grammatical error detection in detail. We first show that 5 to 10% of training data are enough for a BERT-based error detection method to achieve performance equivalent to what a non-language model-based method can achieve with the full training data; recall improves much faster with... | Kazuaki Hanawa, Manabu Kimura, Ryo Nagata |  |
| 500 |  |  [Should We Trust This Summary? Bayesian Abstractive Summarization to The Rescue](https://doi.org/10.18653/v1/2022.findings-acl.325) |  | 0 | We explore the notion of uncertainty in the context of modern abstractive summarization models, using the tools of Bayesian Deep Learning. Our approach approximates Bayesian inference by first extending state-of-the-art summarization models with Monte Carlo dropout and then using them to perform multiple stochastic forward passes. Based on Bayesian inference we... | Alexios Gidiotis, Grigorios Tsoumakas |  |
| 501 |  |  [On the data requirements of probing](https://doi.org/10.18653/v1/2022.findings-acl.326) |  | 0 | As large and powerful neural language models are developed, researchers have been increasingly interested in developing diagnostic tools to probe them. There are many papers with conclusions of the form “observation X is found in model Y”, using their own datasets with varying sizes. Larger probing datasets bring more reliability, but are also expensive to... | Bai Li, Frank Rudzicz, Jixuan Wang, Zining Zhu |  |
| 502 |  |  [Translation Error Detection as Rationale Extraction](https://doi.org/10.18653/v1/2022.findings-acl.327) |  | 0 | Recent Quality Estimation (QE) models based on multilingual pre-trained representations have achieved very competitive results in predicting the overall quality of translated sentences. However, detecting specifically which translated words are incorrect is a more challenging task, especially when dealing with limited amounts of training data. We hypothesize... | Lucia Specia, Marina Fomicheva, Nikolaos Aletras |  |
| 503 |  |  [Towards Collaborative Neural-Symbolic Graph Semantic Parsing via Uncertainty](https://doi.org/10.18653/v1/2022.findings-acl.328) |  | 0 | Recent work in task-independent graph semantic parsing has shifted from grammar-based symbolic approaches to neural models, showing strong performance on different types of meaning representations. However, it is still unclear that what are the limitations of these neural parsers, and whether these limitations can be compensated by incorporating symbolic... | Jeremiah Zhe Liu, Jingbo Shang, Zi Lin |  |
| 504 |  |  [Towards Few-shot Entity Recognition in Document Images: A Label-aware Sequence-to-Sequence Framework](https://doi.org/10.18653/v1/2022.findings-acl.329) |  | 0 | Entity recognition is a fundamental task in understanding document images. Traditional sequence labeling frameworks treat the entity types as class IDs and rely on extensive data and high-quality annotations to learn semantics which are typically expensive in practice. In this paper, we aim to build an entity recognition model requiring only a few shots of... | Jingbo Shang, Zilong Wang |  |
| 505 |  |  [On Length Divergence Bias in Textual Matching Models](https://doi.org/10.18653/v1/2022.findings-acl.330) |  | 0 | Despite the remarkable success deep models have achieved in Textual Matching (TM) tasks, it still remains unclear whether they truly understand language or measure the semantic similarity of texts by exploiting statistical bias in datasets. In this work, we provide a new perspective to study this issue — via the length divergence bias. We find the length... | Chong Meng, Dawei Yin, Lan Jiang, Tianshu Lyu, Xiaoyong Lyu, Yankai Lin |  |
| 506 |  |  [What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation](https://doi.org/10.18653/v1/2022.findings-acl.331) |  | 0 | Accurate automatic evaluation metrics for open-domain dialogs are in high demand. Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect. In this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user... | Alexandros Papangelis, Behnam Hedayatnia, Dilek HakkaniTur, Sarik Ghazarian, Yang Liu |  |
| 507 |  |  [Frontmatter](https://aclanthology.org/2022.acl-long.0) |  | 0 |  |  |  |
| 508 |  |  [AdapLeR: Speeding up Inference by Adaptive Length Reduction](https://doi.org/10.18653/v1/2022.acl-long.1) |  | 0 | Pre-trained language models have shown stellar performance in various downstream tasks. But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings. In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance. Our method dynamically... | Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar |  |
| 509 |  |  [Quantified Reproducibility Assessment of NLP Results](https://doi.org/10.18653/v1/2022.acl-long.2) |  | 0 | This paper describes and tests a method for carrying out quantified reproducibility assessment (QRA) that is based on concepts and definitions from metrology. QRA produces a single score estimating the degree of reproducibility of a given system and evaluation measure, on the basis of the scores from, and differences between, different reproductions. We test... | Anya Belz, Maja Popovic, Simon Mille |  |
| 510 |  |  [Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings](https://doi.org/10.18653/v1/2022.acl-long.3) |  | 0 | Recent studies have determined that the learned token embeddings of large-scale neural language models are degenerated to be anisotropic with a narrow-cone shape. This phenomenon, called the representation degeneration problem, facilitates an increase in the overall similarity between token embeddings that negatively affect the performance of the models.... | Heeseung Kim, Jongyoon Song, Sangwon Yu, Seongmin Lee, Sungroh Yoon, WooJong Ryu |  |
| 511 |  |  [AlephBERT: Language Model Pre-training and Evaluation from Sub-Word to Sentence Level](https://doi.org/10.18653/v1/2022.acl-long.4) |  | 0 | Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs for Hebrew are few and far between. The problem is twofold. First, so far, Hebrew... | Amit Seker, Dan Bareket, Elron Bandel, Idan Brusilovsky, Refael Shaked Greenfeld, Reut Tsarfaty |  |
| 512 |  |  [Learning to Imagine: Integrating Counterfactual Thinking in Neural Discrete Reasoning](https://doi.org/10.18653/v1/2022.acl-long.5) |  | 0 | Neural discrete reasoning (NDR) has shown remarkable progress in combining deep models with discrete reasoning. However, we find that existing NDR solution suffers from large performance drop on hypothetical questions, e.g. “what the annualized rate of return would be if the revenue in 2020 was doubled”. The key to hypothetical question answering (HQA) is... | Fengbin Zhu, Fuli Feng, Hanwang Zhang, Moxin Li, TatSeng Chua, Xiangnan He |  |
| 513 |  |  [Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification](https://doi.org/10.18653/v1/2022.acl-long.6) |  | 0 | Complex word identification (CWI) is a cornerstone process towards proper text simplification. CWI is highly dependent on context, whereas its difficulty is augmented by the scarcity of available datasets which vary greatly in terms of domains and languages. As such, it becomes increasingly more difficult to develop a robust model that generalizes across a wide... | DumitruClementin Cercel, GeorgeEduard Zaharia, Mihai Dascalu, RazvanAlexandru Smadu |  |
| 514 |  |  [JointCL: A Joint Contrastive Learning Framework for Zero-Shot Stance Detection](https://doi.org/10.18653/v1/2022.acl-long.7) |  | 0 | Zero-shot stance detection (ZSSD) aims to detect the stance for an unseen target during the inference stage. In this paper, we propose a joint contrastive learning (JointCL) framework, which consists of stance contrastive learning and target-aware prototypical graph contrastive learning. Specifically, a stance contrastive learning strategy is employed to better... | Bin Liang, Lin Gui, Min Yang, Qinglin Zhu, Ruifeng Xu, Xiang Li, Yulan He |  |
| 515 |  |  [[CASPI] Causal-aware Safe Policy Improvement for Task-oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-long.8) |  | 0 | The recent success of reinforcement learning (RL) in solving complex tasks is often attributed to its capacity to explore and exploit an environment. Sample efficiency is usually not an issue for tasks with cheap simulators to sample data online. On the other hand, Task-oriented Dialogues (ToD) are usually learnt from offline data collected using human... | Caiming Xiong, Govardana Sachithanandam Ramachandran, Kazuma Hashimoto |  |
| 516 |  |  [UniTranSeR: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System](https://doi.org/10.18653/v1/2022.acl-long.9) |  | 0 | As a more natural and intelligent interaction manner, multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved. Nevertheless, almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature... | Guohui Li, Jianjun Li, Yongjing Cheng, Zhiyuan Ma |  |
| 517 |  |  [Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-long.10) |  | 0 | Dialogue State Tracking (DST) aims to keep track of users’ intentions during the course of a conversation. In DST, modelling the relations among domains and slots is still an under-studied problem. Existing approaches that have considered such relations generally fall short in: (1) fusing prior slot-domain membership relations and dialogue-aware dynamic slot... | Aldo Lipani, Emine Yilmaz, Fanghua Ye, Qiang Zhang, Yue Feng |  |
| 518 |  |  [Attention Temperature Matters in Abstractive Summarization Distillation](https://doi.org/10.18653/v1/2022.acl-long.11) |  | 0 | Recent progress of abstractive text summarization largely relies on large pre-trained sequence-to-sequence Transformer models, which are computationally expensive. This paper aims to distill these large models into smaller ones for faster inference and with minimal performance loss. Pseudo-labeling based methods are popular in sequence-to-sequence model... | Furu Wei, Hangbo Bao, Shengqiang Zhang, Xingxing Zhang |  |
| 519 |  |  [Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.12) |  | 0 | This paper demonstrates that multilingual pretraining and multilingual fine-tuning are both critical for facilitating cross-lingual transfer in zero-shot translation, where the neural machine translation (NMT) model is tested on source languages unseen during supervised training. Following this idea, we present SixT+, a strong many-to-English NMT model that... | Dongdong Zhang, Furu Wei, Guanhua Chen, Jia Pan, Shuming Ma, Wenping Wang, Yun Chen |  |
| 520 |  |  [TopWORDS-Seg: Simultaneous Text Segmentation and Word Discovery for Open-Domain Chinese Texts via Bayesian Inference](https://doi.org/10.18653/v1/2022.acl-long.13) |  | 0 | Processing open-domain Chinese texts has been a critical bottleneck in computational linguistics for decades, partially because text segmentation and word discovery often entangle with each other in this challenging scenario. No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain. This study fills in... | Changzai Pan, Ke Deng, Maosong Sun |  |
| 521 |  |  [An Unsupervised Multiple-Task and Multiple-Teacher Model for Cross-lingual Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.14) |  | 0 | Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages. Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer. However, existing cross-lingual distillation models... | Chunming Hu, Junfan Chen, Richong Zhang, Wenyi Qin, Xiaohui Guo, Zhuoran Li |  |
| 522 |  |  [Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature](https://doi.org/10.18653/v1/2022.acl-long.15) |  | 0 | Although current state-of-the-art Transformer-based solutions succeeded in a wide range for single-document NLP tasks, they still struggle to address multi-input tasks such as multi-document summarization. Many solutions truncate the inputs, thus ignoring potential summary-relevant contents, which is unacceptable in the medical domain where each information can... | Davide Freddi, Gianluca Moro, Lorenzo Valgimigli, Luca Ragazzi |  |
| 523 |  |  [Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm](https://doi.org/10.18653/v1/2022.acl-long.16) |  | 0 | Conventional wisdom in pruning Transformer-based language models is that pruning reduces the model expressiveness and thus is more likely to underfit rather than overfit. However, under the trending pretrain-and-finetune paradigm, we postulate a counter-traditional hypothesis, that is: pruning increases the risk of overfitting when performed at the fine-tuning... | Bingbing Li, Caiwen Ding, Dongkuan Xu, Hang Liu, Ian EnHsu Yen, Mimi Xie, Sanguthevar Rajasekaran, Shaoyi Huang, Shiyang Chen, SungEn Chang, Yijue Wang |  |
| 524 |  |  [CipherDAug: Ciphertext based Data Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.17) |  | 0 | We propose a novel data-augmentation technique for neural machine translation based on ROT-k ciphertexts. ROT-k is a simple letter substitution cipher that replaces a letter in the plaintext with the kth letter after it in the alphabet. We first generate multiple ROT-k ciphertexts using different values of k for the plaintext which is the source side of the... | Anoop Sarkar, Logan Born, Nishant Kambhatla |  |
| 525 |  |  [Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages](https://doi.org/10.18653/v1/2022.acl-long.18) |  | 0 | Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL). However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of... | Partha P. Talukdar, Sunita Sarawagi, Vaidehi Patil |  |
| 526 |  |  [Long-range Sequence Modeling with Predictable Sparse Attention](https://doi.org/10.18653/v1/2022.acl-long.19) |  | 0 | Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier... | Jing Zhang, Mei Tu, Yimeng Zhuang |  |
| 527 |  |  [Improving Personalized Explanation Generation through Visualization](https://doi.org/10.18653/v1/2022.acl-long.20) |  | 0 | In modern recommender systems, there are usually comments or reviews from users that justify their ratings for different items. Trained on such textual corpus, explainable recommendation models learn to discover user interests and generate personalized explanations. Though able to provide plausible explanations, existing models tend to generate repeated... | Gerard de Melo, Lei Li, Shijie Geng, Yingqiang Ge, Yongfeng Zhang, Zuohui Fu |  |
| 528 |  |  [New Intent Discovery with Pre-training and Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.21) |  | 0 | New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite its importance, this problem remains under-explored in the literature. Existing approaches typically rely on a large amount of... | Albert Y. S. Lam, Haode Zhang, LiMing Zhan, XiaoMing Wu, Yuwei Zhang |  |
| 529 |  |  [Modeling U.S. State-Level Policies by Extracting Winners and Losers from Legislative Texts](https://doi.org/10.18653/v1/2022.acl-long.22) |  | 0 | Decisions on state-level policies have a deep effect on many aspects of our everyday life, such as health-care and education access. However, there is little understanding of how these policies and decisions are being formed in the legislative process. We take a data-driven approach by decoding the impact of legislation on relevant stakeholders (e.g., teachers... | Dan Goldwasser, Eric Waltenburg, Maryam Davoodi |  |
| 530 |  |  [Structural Characterization for Dialogue Disentanglement](https://doi.org/10.18653/v1/2022.acl-long.23) |  | 0 | Tangled multi-party dialogue contexts lead to challenges for dialogue reading comprehension, where multiple dialogue threads flow simultaneously within a common dialogue record, increasing difficulties in understanding the dialogue history for both human and machine. Previous studies mainly focus on utterance encoding methods with carefully designed features... | Hai Zhao, Xinbei Ma, Zhuosheng Zhang |  |
| 531 |  |  [Multi-Party Empathetic Dialogue Generation: A New Task for Dialog Systems](https://doi.org/10.18653/v1/2022.acl-long.24) |  | 0 | Empathetic dialogue assembles emotion understanding, feeling projection, and appropriate response generation. Existing work for empathetic dialogue generation concentrates on the two-party conversation scenario. Multi-party dialogues, however, are pervasive in reality. Furthermore, emotion and sensibility are typically confused; a refined empathy analysis is... | Haiying Wu, Hongbin Wang, Jun Wang, Lingyu Zhu, Zhengkun Zhang, Zhenglu Yang |  |
| 532 |  |  [MISC: A Mixed Strategy-Aware Model integrating COMET for Emotional Support Conversation](https://doi.org/10.18653/v1/2022.acl-long.25) |  | 0 | Applying existing methods to emotional support conversation—which provides valuable assistance to people who are in need—has two major limitations: (a) they generally employ a conversation-level emotion label, which is too coarse-grained to capture user’s instant mental state; (b) most of them focus on expressing empathy in the response(s) rather than gradually... | Bin Wang, JiRong Wen, Jianwei Cui, Quan Tu, Rui Yan, Yanran Li |  |
| 533 |  |  [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://doi.org/10.18653/v1/2022.acl-long.26) |  | 0 | There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and... | Jie Tang, Jiezhong Qiu, Ming Ding, Xiao Liu, Yujie Qian, Zhengxiao Du, Zhilin Yang |  |
| 534 |  |  [QuoteR: A Benchmark of Quote Recommendation for Writing](https://doi.org/10.18653/v1/2022.acl-long.27) |  | 0 | It is very common to use quotations (quotes) to make our writings more elegant or convincing. To help people find appropriate quotes efficiently, the task of quote recommendation is presented, aiming to recommend quotes that fit the current context of writing. There have been various quote recommendation approaches, but they are evaluated on different... | Fanchao Qi, Jing Yi, Maosong Sun, Yanhui Yang, Zhili Cheng, Zhiyuan Liu |  |
| 535 |  |  [Towards Comprehensive Patent Approval Predictions: Beyond Traditional Document Classification](https://doi.org/10.18653/v1/2022.acl-long.28) |  | 0 | Predicting the approval chance of a patent application is a challenging problem involving multiple facets. The most crucial facet is arguably the novelty — 35 U.S. Code § 102 rejects more recent applications that have very similar prior arts. Such novelty evaluations differ the patent approval prediction from conventional document classification — Successful... | Beilei He, Jingbo Shang, Kewen Zhao, Vish Krishnan, Xiaochen Gao, Yifei Ning, Zhaoyi Hou |  |
| 536 |  |  [Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering](https://doi.org/10.18653/v1/2022.acl-long.29) |  | 0 | Knowledge-based visual question answering (QA) aims to answer a question which requires visually-grounded external knowledge beyond image content itself. Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i) no supervision is given to the reasoning process and ii) high-order semantics... | ByoungTak Zhang, EunSol Kim, Woo Suk Choi, YuJung Heo |  |
| 537 |  |  [Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech](https://doi.org/10.18653/v1/2022.acl-long.30) |  | 0 | Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and... | Cheng Yu, Fanglei Sun, Guangzhi Sun, Hua Jiang, Jun Wang, Weiqin Zu, Yang Li, Yang Yang, Ying Wen |  |
| 538 |  |  [Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models](https://doi.org/10.18653/v1/2022.acl-long.31) |  | 0 | Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that... | Fatemehsadat Mireshghallah, Kartik Goyal, Taylor BergKirkpatrick |  |
| 539 |  |  [So Different Yet So Alike! Constrained Unsupervised Text Style Transfer](https://doi.org/10.18653/v1/2022.acl-long.32) |  | 0 | Automatic transfer of text between domains has become popular in recent times. One of its aims is to preserve the semantic content while adapting to the target domain. However, it does not explicitly maintain other attributes between the source and translated text: e.g., text length and descriptiveness. Maintaining constraints in transfer has several downstream... | Abhinav Ramesh Kashyap, Devamanyu Hazarika, MinYen Kan, Roger Zimmermann, Soujanya Poria |  |
| 540 |  |  [e-CARE: a New Dataset for Exploring Explainable Causal Reasoning](https://doi.org/10.18653/v1/2022.acl-long.33) |  | 0 | Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning... | Bing Qin, Kai Xiong, Li Du, Ting Liu, Xiao Ding |  |
| 541 |  |  [Fantastic Questions and Where to Find Them: FairytaleQA - An Authentic Dataset for Narrative Comprehension](https://doi.org/10.18653/v1/2022.acl-long.34) |  | 0 | Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of... | Bingsheng Yao, Branda Sun, Dakuo Wang, Daniel Ritchie, Diyi Yang, Mark Warschauer, Mo Yu, Nanyun Peng, Nora Bradford, Toby JiaJun Li, Tongshuang Wu, Tran Bao Hoang, Xiaojuan Ma, Ying Xu, Yisi Sang, Yufang Hou, Zheng Zhang, Zhou Yu |  |
| 542 |  |  [KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base](https://doi.org/10.18653/v1/2022.acl-long.35) |  | 0 | In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios. (2) Knowledge base information is not well exploited and incorporated into semantic parsing. To mitigate the... | Deyi Xiong, Junzhuo Li |  |
| 543 |  |  [Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment](https://doi.org/10.18653/v1/2022.acl-long.36) |  | 0 | Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs are far from complete. Due to labor-intensive human labeling, this phenomenon deteriorates when handling knowledge represented in various languages. In this paper, we explore multilingual KG completion, which leverages limited seed alignment as a bridge, to embrace the collective... | Bing Yin, Hanqing Lu, Haoming Jiang, Karthik Subbian, Tianyu Cao, Wei Wang, Yizhou Sun, Zheng Li, Zijie Huang |  |
| 544 |  |  [Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization](https://doi.org/10.18653/v1/2022.acl-long.37) |  | 0 | Automatic code summarization, which aims to describe the source code in natural language, has become an essential task in software maintenance. Our fellow researchers have attempted to achieve such a purpose through various machine learning-based approaches. One key challenge keeping these approaches from being practical lies in the lacking of retaining the... | Jin Liu, Juncai Guo, Li Li, Pingyi Zhou, Yao Wan |  |
| 545 |  |  [FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.38) |  | 0 | The few-shot natural language understanding (NLU) task has attracted much recent attention. However, prior methods have been evaluated under a disparate set of protocols, which hinders fair comparison and measuring the progress of the field. To address this issue, we introduce an evaluation framework that improves previous evaluation procedures in three key... | Chonghua Liao, Jie Tang, Jing Zhou, Li Jian, Ming Ding, Ruslan Salakhutdinov, Sebastian Ruder, Yanan Zheng, Yujie Qian, Zhilin Yang |  |
| 546 |  |  [Learn to Adapt for Generalized Zero-Shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.39) |  | 0 | Generalized zero-shot text classification aims to classify textual instances from both previously seen classes and incrementally emerging unseen classes. Most existing methods generalize poorly since the learned parameters are only optimal for seen classes rather than for both classes, and the parameters keep stationary in predicting procedures. To address... | Caixia Yuan, Xiaojie Wang, Yiwen Zhang, Yongbin Liu, Ziwei Bai |  |
| 547 |  |  [TableFormer: Robust Transformer Modeling for Table-Text Encoding](https://doi.org/10.18653/v1/2022.acl-long.40) |  | 0 | Understanding tables is an important aspect of natural language understanding. Existing models for table understanding require linearization of the table structure, where row or column order is encoded as an unwanted bias. Such spurious biases make the model vulnerable to row and column order perturbations. Additionally, prior work has not thoroughly modeled... | Aditya Gupta, Jingfeng Yang, Luheng He, Rahul Goel, Shachi Paul, Shyam Upadhyay |  |
| 548 |  |  [Perceiving the World: Question-guided Reinforcement Learning for Text-based Games](https://doi.org/10.18653/v1/2022.acl-long.41) |  | 0 | Text-based games provide an interactive way to study natural language processing. While deep reinforcement learning has shown effectiveness in developing the game playing agent, the low sample efficiency and the large action space remain to be the two major challenges that hinder the DRL from being applied in the real world. In this paper, we address the... | Chengqi Zhang, Joey Tianyi Zhou, Ling Chen, Meng Fang, Yali Du, Yunqiu Xu |  |
| 549 |  |  [Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.42) |  | 0 | In zero-shot multilingual extractive text summarization, a model is typically trained on English summarization dataset and then applied on summarization datasets of other languages. Given English gold summaries and documents, sentence-level labels for extractive summarization are usually generated using heuristics. However, these monolingual labels created on... | Furu Wei, Ruipeng Jia, Shi Wang, Xingxing Zhang, Yanan Cao, Zheng Lin |  |
| 550 |  |  [Few-Shot Class-Incremental Learning for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.43) |  | 0 | Previous work of class-incremental learning for Named Entity Recognition (NER) relies on the assumption that there exists abundance of labeled data for the training of new classes. In this work, we study a more challenging but practical problem, i.e., few-shot class-incremental learning for NER, where an NER model is trained with only few labeled samples of the... | Handong Zhao, Ricardo Henao, Rui Wang, Ruiyi Zhang, Subrata Mitra, Sungchul Kim, Tong Yu |  |
| 551 |  |  [Improving Meta-learning for Low-resource Text Classification and Generation via Memory Imitation](https://doi.org/10.18653/v1/2022.acl-long.44) |  | 0 | Building models of natural language processing (NLP) is challenging in low-resource scenarios where limited data are available. Optimization-based meta-learning algorithms achieve promising results in low-resource scenarios by adapting a well-generalized model initialization to handle new tasks. Nonetheless, these approaches suffer from the memorization... | Dongkyu Lee, Huaxiu Yao, Jian Sun, Nevin L. Zhang, Yingxiu Zhao, Yinhe Zheng, Yiping Song, Zhiliang Tian |  |
| 552 |  |  [Quality Controlled Paraphrase Generation](https://doi.org/10.18653/v1/2022.acl-long.45) |  | 0 | Paraphrase generation has been widely used in various downstream tasks. Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence. Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity... | Elron Bandel, Ilya Shnayderman, Liat EinDor, Michal ShmueliScheuer, Noam Slonim, Ranit Aharonov |  |
| 553 |  |  [Controllable Dictionary Example Generation: Generating Example Sentences for Specific Targeted Audiences](https://doi.org/10.18653/v1/2022.acl-long.46) |  | 0 | Example sentences for targeted words in a dictionary play an important role to help readers understand the usage of words. Traditionally, example sentences in a dictionary are usually created by linguistics experts, which are labor-intensive and knowledge-intensive. In this paper, we introduce the problem of dictionary example sentence generation, aiming to... | SiuMing Yiu, Xingwei He |  |
| 554 |  |  [AraT5: Text-to-Text Transformers for Arabic Language Generation](https://doi.org/10.18653/v1/2022.acl-long.47) |  | 0 | Transfer learning with a unified Transformer framework (T5) that converts all language problems into a text-to-text format was recently proposed as a simple and effective transfer learning approach. Although a multilingual version of the T5 model (mT5) was also introduced, it is not clear how well it can fare on non-English tasks involving diverse data. To... | AbdelRahim A. Elmadany, El Moatez Billah Nagoudi, Muhammad AbdulMageed |  |
| 555 |  |  [Legal Judgment Prediction via Event Extraction with Constraints](https://doi.org/10.18653/v1/2022.acl-long.48) |  | 0 | While significant progress has been made on the task of Legal Judgment Prediction (LJP) in recent years, the incorrect predictions made by SOTA LJP models can be attributed in part to their failure to (1) locate the key event information that determines the judgment, and (2) exploit the cross-task consistency constraints that exist among the subtasks of LJP. To... | Chuanyi Li, Vincent Ng, Yi Feng |  |
| 556 |  |  [Answer-level Calibration for Free-form Multiple Choice Question Answering](https://doi.org/10.18653/v1/2022.acl-long.49) |  | 0 | Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While... | Sawan Kumar |  |
| 557 |  |  [Learning When to Translate for Streaming Speech](https://doi.org/10.18653/v1/2022.acl-long.50) |  | 0 | How to find proper moments to generate partial sentence translation given a streaming speech input? Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech, since the boundaries between acoustic units in speech are not even. In this paper, we propose MoSST, a simple yet effective method for translating streaming... | Lei Li, Mingxuan Wang, Qian Dong, Yaoming Zhu |  |
| 558 |  |  [Compact Token Representations with Contextual Quantization for Efficient Document Re-ranking](https://doi.org/10.18653/v1/2022.acl-long.51) |  | 0 | Transformer based re-ranking models can achieve high search relevance through context- aware soft matching of query tokens with document tokens. To alleviate runtime complexity of such inference, previous work has adopted a late interaction architecture with pre-computed contextual token representations at the cost of a large online storage. This paper proposes... | Tao Yang, Yifan Qiao, Yingrui Yang |  |
| 559 |  |  [Early Stopping Based on Unlabeled Samples in Text Classification](https://doi.org/10.18653/v1/2022.acl-long.52) |  | 0 | Early stopping, which is widely used to prevent overfitting, is generally based on a separate validation set. However, in low resource settings, validation-based stopping can be risky because a small validation set may not be sufficiently representative, and the reduction in the number of samples by validation split may result in insufficient samples for... | Dongha Choi, Hongseok Choi, Hyunju Lee |  |
| 560 |  |  [Meta-learning via Language Model In-context Tuning](https://doi.org/10.18653/v1/2022.acl-long.53) |  | 0 | The goal of meta-learning is to learn to adapt to a new task with only a few labeled examples. Inspired by the recent progress in large language models, we propose in-context tuning (ICT), which recasts task adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, labeled in-context... | George Karypis, He He, Ruiqi Zhong, Sheng Zha, Yanda Chen |  |
| 561 |  |  [It is AI's Turn to Ask Humans a Question: Question-Answer Pair Generation for Children's Story Books](https://doi.org/10.18653/v1/2022.acl-long.54) |  | 0 | Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education... | Bingsheng Yao, Dakuo Wang, Mo Yu, Toby JiaJun Li, Tongshuang Wu, Ying Xu, Zheng Zhang |  |
| 562 |  |  [Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning](https://doi.org/10.18653/v1/2022.acl-long.55) |  | 0 | Weakly-supervised learning (WSL) has shown promising results in addressing label scarcity on many NLP tasks, but manually designing a comprehensive, high-quality labeling rule set is tedious and difficult. We study interactive weakly-supervised learning—the problem of iteratively and automatically discovering novel labeling rules from data to improve the WSL... | Chao Zhang, Le Song, Pranav Shetty, Rongzhi Zhang, Yue Yu |  |
| 563 |  |  [Constrained Multi-Task Learning for Bridging Resolution](https://doi.org/10.18653/v1/2022.acl-long.56) |  | 0 | We examine the extent to which supervised bridging resolvers can be improved without employing additional labeled bridging data by proposing a novel constrained multi-task learning framework for bridging resolution, within which we (1) design cross-task consistency constraints to guide the learning process; (2) pre-train the entity coreference model in the... | Hideo Kobayashi, Vincent Ng, Yufang Hou |  |
| 564 |  |  [DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations](https://doi.org/10.18653/v1/2022.acl-long.57) |  | 0 | Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models. Although recently proposed trainable conversation-level metrics have shown encouraging results, the quality of the metrics is strongly dependent on the quality of training data. Prior works... | Aram Galstyan, Nanyun Peng, Nuan Wen, Sarik Ghazarian |  |
| 565 |  |  [HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.58) |  | 0 | Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern Transformer architecture. In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task, hierarchical question-summary... | Lu Wang, Shuyang Cao |  |
| 566 |  |  [De-Bias for Generative Extraction in Unified NER Task](https://doi.org/10.18653/v1/2022.acl-long.59) |  | 0 | Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three... | Shuai Zhang, Weiming Lu, Yiquan Wu, Yongliang Shen, Zeqi Tan |  |
| 567 |  |  [An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels](https://doi.org/10.18653/v1/2022.acl-long.60) |  | 0 | Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method... | Alexander Glenn Shaw, Alexia Pauline Delorey, Christopher Michael Rytting, David Wingate, Joshua Robinson, Kyle Jeffrey Rogers, Mahmoud Khalil, Nancy Fulda, Taylor Sorensen |  |
| 568 |  |  [Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation](https://doi.org/10.18653/v1/2022.acl-long.61) |  | 0 | The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world’s languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented... | Graham Neubig, Sebastian Ruder, Xinyi Wang |  |
| 569 |  |  [Language-agnostic BERT Sentence Embedding](https://doi.org/10.18653/v1/2022.acl-long.62) |  | 0 | While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and... | Daniel Cer, Fangxiaoyu Feng, Naveen Arivazhagan, Wei Wang, Yinfei Yang |  |
| 570 |  |  [Nested Named Entity Recognition with Span-level Graphs](https://doi.org/10.18653/v1/2022.acl-long.63) |  | 0 | Span-based methods with the neural networks backbone have great potential for the nested named entity recognition (NER) problem. However, they face problems such as degenerating when positive instances and negative instances largely overlap. Besides, the generalization ability matters a lot in nested NER, as a large proportion of entities in the test set hardly... | Dongyu Ru, Juncheng Wan, Weinan Zhang, Yong Yu |  |
| 571 |  |  [CogTaskonomy: Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP](https://doi.org/10.18653/v1/2022.acl-long.64) |  | 0 | Is there a principle to guide transfer learning across tasks in natural language processing (NLP)? Taxonomy (Zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them. In this paper, we propose a cognitively inspired framework, CogTaskonomy, to learn taxonomy for NLP tasks. The framework consists... | Deyi Xiong, Minghui Xu, Yifei Luo |  |
| 572 |  |  [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://doi.org/10.18653/v1/2022.acl-long.65) |  | 0 | Large-scale pretrained language models have achieved SOTA results on NLP tasks. However, they have been shown vulnerable to adversarial attacks especially for logographic languages like Chinese. In this work, we propose RoCBert: a pretrained Chinese Bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc. It is... | Hui Su, Jiarui Fang, Jie Zhou, Tuo Ji, Weiwei Shi, Xiao Zhou, Xiaoyu Shen |  |
| 573 |  |  [Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues](https://doi.org/10.18653/v1/2022.acl-long.66) |  | 0 | It is a common practice for recent works in vision language cross-modal reasoning to adopt a binary or multi-choice classification formulation taking as input a set of source image(s) and textual query. In this work, we take a sober look at such an “unconditional” formulation in the sense that no prior knowledge is specified with respect to the source image(s).... | Baobao Chang, Haoran Meng, Heming Xia, Lin Xu, Qingxiu Dong, Shoujie Tong, Sujian Li, Tian Feng, Tianyu Liu, Weidong Zhan, Zhifang Sui, Zhongyu Wei, Ziwei Qin |  |
| 574 |  |  [Parallel Instance Query Network for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.67) |  | 0 | Named entity recognition (NER) is a fundamental task in natural language processing. Recent works treat named entity recognition as a reading comprehension task, constructing type-specific queries manually to extract entities. This paradigm suffers from three issues. First, type-specific queries can only extract one type of entities per inference, which is... | Fei Huang, Guangwei Xu, Pengjun Xie, Weiming Lu, Xiaobin Wang, Yongliang Shen, Yueting Zhuang, Zeqi Tan |  |
| 575 |  |  [ProphetChat: Enhancing Dialogue Generation with Simulation of Future Conversation](https://doi.org/10.18653/v1/2022.acl-long.68) |  | 0 | Typical generative dialogue models utilize the dialogue history to generate the response. However, since one dialogue utterance can often be appropriately answered by multiple distinct responses, generating a desired response solely based on the historical information is not easy. Intuitively, if the chatbot can foresee in advance what the user would talk about... | Chang Liu, Chongyang Tao, Dongyan Zhao, Rui Yan, TieYan Liu, Xu Tan, Zhenxin Fu |  |
| 576 |  |  [Modeling Multi-hop Question Answering as Single Sequence Prediction](https://doi.org/10.18653/v1/2022.acl-long.69) |  | 0 | Fusion-in-decoder (Fid) (Izacard and Grave, 2020) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA. However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach. In this work, we propose a simple generative approach... | Caiming Xiong, Kazuma Hashimoto, Nitish Shirish Keskar, Semih Yavuz, Yingbo Zhou |  |
| 577 |  |  [Learning Disentangled Semantic Representations for Zero-Shot Cross-Lingual Transfer in Multilingual Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.70) |  | 0 | Multilingual pre-trained models are able to zero-shot transfer knowledge from rich-resource to low-resource languages in machine reading comprehension (MRC). However, inherent linguistic discrepancies in different languages could make answer spans predicted by zero-shot transfer violate syntactic constraints of the target language. In this paper, we propose a... | Deyi Xiong, Linjuan Wu, Shaojuan Wu, Shizhan Chen, Xiaowang Zhang, Zhiqiang Zhuang, Zhiyong Feng |  |
| 578 |  |  [Multi-Granularity Structural Knowledge Distillation for Language Model Compression](https://doi.org/10.18653/v1/2022.acl-long.71) |  | 0 | Transferring the knowledge to a small model through distillation has raised great interest in recent years. Prevailing methods transfer the knowledge derived from mono-granularity language units (e.g., token-level or sample-level), which is not enough to represent the rich semantics of a text and may lose some vital knowledge. Besides, these methods form the... | Chang Liu, Chongyang Tao, Dongyan Zhao, Jiazhan Feng |  |
| 579 |  |  [Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts](https://doi.org/10.18653/v1/2022.acl-long.72) |  | 0 | Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing... | Ahmed Abbasi, Yi Yang, Yue Guo |  |
| 580 |  |  [Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals](https://doi.org/10.18653/v1/2022.acl-long.73) |  | 0 | Most dialog systems posit that users have figured out clear and specific goals before starting an interaction. For example, users have determined the departure, the destination, and the travel time for booking a flight. However, in many scenarios, limited by experience and knowledge, users may know what they need, but still struggle to figure out clear and... | Haifeng Wang, Hua Wu, Jun Xu, Zeming Liu, Zeyang Lei, ZhengYu Niu |  |
| 581 |  |  [Semi-supervised Domain Adaptation for Dependency Parsing with Dynamic Matching Network](https://doi.org/10.18653/v1/2022.acl-long.74) |  | 0 | Supervised parsing models have achieved impressive results on in-domain texts. However, their performances drop drastically on out-of-domain texts due to the data distribution shift. The shared-private model has shown its promising advantages for alleviating this problem via feature separation, whereas prior works pay more attention to enhance shared features... | Min Zhang, Shuaike Li, Ying Li |  |
| 582 |  |  [A Closer Look at How Fine-tuning Changes BERT](https://doi.org/10.18653/v1/2022.acl-long.75) |  | 0 | Given the prevalence of pre-trained contextualized representations in today’s NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space... | Vivek Srikumar, Yichu Zhou |  |
| 583 |  |  [Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.76) |  | 0 | Training dense passage representations via contrastive learning has been shown effective for Open-Domain Passage Retrieval (ODPR). Existing studies focus on further optimizing by improving negative sampling strategy or extra pretraining. However, these studies keep unknown in capturing passage with internal representation conflicts from improper modeling... | Hai Zhao, Jinyuan Wang, Wu Hong, Zhuosheng Zhang |  |
| 584 |  |  [FaiRR: Faithful and Robust Deductive Reasoning over Natural Language](https://doi.org/10.18653/v1/2022.acl-long.77) |  | 0 | Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in natural language. Recent works show that such models can also produce the reasoning steps (i.e., the proof graph) that emulate the model’s logical reasoning process. Currently, these black-box models generate both the proof... | Harman Singh, Soumya Sanyal, Xiang Ren |  |
| 585 |  |  [HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.78) |  | 0 | Tables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge numerical reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and... | Dongmei Zhang, Haoyu Dong, JianGuang Lou, Jiaqi Guo, Ran Jia, Shi Han, Yan Gao, Zhiruo Wang, Zhoujun Cheng |  |
| 586 |  |  [Doctor Recommendation in Online Health Forums via Expertise Learning](https://doi.org/10.18653/v1/2022.acl-long.79) |  | 0 | Huge volumes of patient queries are daily generated on online health forums, rendering manual doctor allocation a labor-intensive task. To better help patients, this paper studies a novel task of doctor recommendation to enable automatic pairing of a patient to a doctor with relevant expertise. While most prior work in recommendation focuses on modeling target... | Jing Li, Shi Zong, Xiaoxin Lu, Yubo Zhang |  |
| 587 |  |  [Continual Prompt Tuning for Dialog State Tracking](https://doi.org/10.18653/v1/2022.acl-long.80) |  | 0 | A desirable dialog system should be able to continually learn new skills without forgetting old ones, and thereby adapt to new domains or tasks in its life cycle. However, continually training a model often leads to a well-known catastrophic forgetting issue. In this paper, we present Continual Prompt Tuning, a parameter-efficient framework that not only avoids... | Bing Li, Fei Mi, Minlie Huang, Qi Zhu, Xiaoyan Zhu |  |
| 588 |  |  [There's a Time and Place for Reasoning Beyond the Image](https://doi.org/10.18653/v1/2022.acl-long.81) |  | 0 | Images are often more significant than only the pixels to human eyes, as we can infer, associate, and reason with contextual information from other sources to establish a more complete picture. For example, in Figure 1, we can find a way to identify the news articles related to the picture through segment-wise understandings of the signs, the buildings, the... | Ben Zhou, Carl Vondrick, Dan Roth, Ishaan Preetam Chandratreya, Xingyu Fu |  |
| 589 |  |  [FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining](https://doi.org/10.18653/v1/2022.acl-long.82) |  | 0 | Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we... | Dongmei Zhang, Fan Cheng, Haoyu Dong, Pengfei Wu, Ran Jia, Shi Han, Zhoujun Cheng |  |
| 590 |  |  [Multimodal fusion via cortical network inspired losses](https://doi.org/10.18653/v1/2022.acl-long.83) |  | 0 | Information integration from different modalities is an active area of research. Human beings and, in general, biological neural systems are quite adept at using a multitude of signals from different sensory perceptive fields to interact with the environment and each other. Recent work in deep fusion models via neural networks has led to substantial... | Shiv Shankar |  |
| 591 |  |  [Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension](https://doi.org/10.18653/v1/2022.acl-long.84) |  | 0 | Procedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step. Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M3C). In this study, we approach Procedural M3C at a fine-grained level (compared with existing... | Huibin Zhang, Jun Wang, Ning Jiang, Xin Wei, Yao Zhang, Yufan Li, Zhengkun Zhang, Zhenglu Yang |  |
| 592 |  |  [Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.85) |  | 0 | Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task,... | Mohit Bansal, Prateek Yadav, Swarnadeep Saha |  |
| 593 |  |  [Unsupervised Extractive Opinion Summarization Using Sparse Coding](https://doi.org/10.18653/v1/2022.acl-long.86) |  | 0 | Opinion summarization is the task of automatically generating summaries that encapsulate information expressed in multiple user reviews. We present Semantic Autoencoder (SemAE) to perform extractive opinion summarization in an unsupervised manner. SemAE uses dictionary learning to implicitly capture semantic information from the review text and learns a latent... | Chao Zhao, Snigdha Chaturvedi, Somnath Basu Roy Chowdhury |  |
| 594 |  |  [LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution](https://doi.org/10.18653/v1/2022.acl-long.87) |  | 0 | Lexical substitution is the task of generating meaningful substitutes for a word in a given textual context. Contextual word embedding models have achieved state-of-the-art results in the lexical substitution task by relying on contextual information extracted from the replaced word within the sentence. However, such models do not take into account structured... | Alexander Wong, George Michalopoulos, Helen H. Chen, Ian McKillop |  |
| 595 |  |  [Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation](https://doi.org/10.18653/v1/2022.acl-long.88) |  | 0 | Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this... | Behnam Hedayatnia, Dilek HakkaniTur, Jay Pujara, Karthik Gopalakrishnan, Pei Zhou, Seokhwan Kim, Xiang Ren, Yang Liu |  |
| 596 |  |  [Flow-Adapter Architecture for Unsupervised Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.89) |  | 0 | In this work, we propose a flow-adapter architecture for unsupervised NMT. It leverages normalizing flows to explicitly model the distributions of sentence-level latent representations, which are subsequently used in conjunction with the attention mechanism for the translation task. The primary novelties of our model are: (a) capturing language-specific... | Haris Jabbar, Hinrich Schütze, Yihong Liu |  |
| 597 |  |  [Efficient Unsupervised Sentence Compression by Fine-tuning Transformers with Reinforcement Learning](https://doi.org/10.18653/v1/2022.acl-long.90) |  | 0 | Sentence compression reduces the length of text by removing non-essential content while preserving important facts and grammaticality. Unsupervised objective driven methods for sentence compression can be used to create customized models without the need for ground-truth training data, while allowing flexibility in the objective function(s) that are used for... | Chris Hokamp, Demian Gholipour Ghalandari, Georgiana Ifrim |  |
| 598 |  |  [Tracing Origins: Coreference-aware Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.91) |  | 0 | Machine reading comprehension is a heavily-studied research and test field for evaluating new pre-trained language models (PrLMs) and fine-tuning strategies, and recent studies have enriched the pre-trained language models with syntactic, semantic and other linguistic information to improve the performance of the models. In this paper, we imitate the human... | Hai Zhao, Zhuosheng Zhang |  |
| 599 |  |  [WatClaimCheck: A new Dataset for Claim Entailment and Inference](https://doi.org/10.18653/v1/2022.acl-long.92) |  | 0 | We contribute a new dataset for the task of automated fact checking and an evaluation of state of the art algorithms. The dataset includes claims (from speeches, interviews, social media and news articles), review articles published by professional fact checkers and premise articles used by those professional fact checkers to support their review and verify the... | Kashif Khan, Pascal Poupart, Ruizhe Wang |  |
| 600 |  |  [FrugalScore: Learning Cheaper, Lighter and Faster Evaluation Metrics for Automatic Text Generation](https://doi.org/10.18653/v1/2022.acl-long.93) |  | 0 | Fast and reliable evaluation metrics are key to R&D progress. While traditional natural language generation metrics are fast, they are not very reliable. Conversely, new metrics based on large pretrained language models are much more reliable, but require significant computational resources. In this paper, we propose FrugalScore, an approach to learn a fixed,... | Antoine J.P. Tixier, Guokan Shang, Michalis Vazirgiannis, Moussa Kamal Eddine |  |
| 601 |  |  [A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation](https://doi.org/10.18653/v1/2022.acl-long.94) |  | 0 | We propose Composition Sampling, a simple but effective method to generate diverse outputs for conditional generation of higher quality compared to previous stochastic decoding strategies. It builds on recently proposed plan-based neural generation models (FROST, Narayan et al, 2021) that are trained to first create a composition of the output and then generate... | Dipanjan Das, Gonçalo Simões, Joshua Maynez, Michael Collins, Mirella Lapata, Shashi Narayan, Yao Zhao |  |
| 602 |  |  [Synthetic Question Value Estimation for Domain Adaptation of Question Answering](https://doi.org/10.18653/v1/2022.acl-long.95) |  | 0 | Synthesizing QA pairs with a question generator (QG) on the target domain has become a popular approach for domain adaptation of question answering (QA) models. Since synthetic questions are often noisy in practice, existing work adapts scores from a pretrained QA (or QG) model as criteria to select high-quality questions. However, these scores do not directly... | Huan Sun, Xiang Yue, Ziyu Yao |  |
| 603 |  |  [Better Language Model with Hypernym Class Prediction](https://doi.org/10.18653/v1/2022.acl-long.96) |  | 0 | Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs. In this study, we revisit this approach in the context of neural LMs. We hypothesize that class-based prediction leads to an implicit context aggregation for similar words and thus can improve generalization for rare words. We map words that have a common WordNet... | Alessandro Sordoni, He Bai, Peng Shi, Tong Wang |  |
| 604 |  |  [Tackling Fake News Detection by Continually Improving Social Context Representations using Graph Neural Networks](https://doi.org/10.18653/v1/2022.acl-long.97) |  | 0 | Easy access, variety of content, and fast widespread interactions are some of the reasons making social media increasingly popular. However, this rise has also enabled the propagation of fake news, text published by news sources with an intent to spread misinformation and sway beliefs. Detecting it is an important and challenging problem to prevent large scale... | Dan Goldwasser, Maria Leonor Pacheco, Nikhil Mehta |  |
| 605 |  |  [Understanding Gender Bias in Knowledge Base Embeddings](https://doi.org/10.18653/v1/2022.acl-long.98) |  | 0 | Knowledge base (KB) embeddings have been shown to contain gender biases. In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is... | Man Lan, Meirong Ma, Qi Zheng, Yan Yang, Yuanbin Wu, Yupei Du |  |
| 606 |  |  [Computational Historical Linguistics and Language Diversity in South Asia](https://doi.org/10.18653/v1/2022.acl-long.99) |  | 0 | South Asia is home to a plethora of languages, many of which severely lack access to new language technologies. This linguistic diversity also results in a research environment conducive to the study of comparative, contact, and historical linguistics–fields which necessitate the gathering of extensive data from many languages. We claim that data scatteredness... | Adam Farris, Aryaman Arora, Samopriya Basu, Suresh Kolichala |  |
| 607 |  |  [Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.100) |  | 0 | Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive.... | Claire Cardie, Esin Durmus, Faisal Ladhak, He He, Kathleen R. McKeown |  |
| 608 |  |  [Slangvolution: A Causal Analysis of Semantic Change and Frequency Dynamics in Slang](https://doi.org/10.18653/v1/2022.acl-long.101) |  | 0 | Languages are continuously undergoing changes, and the mechanisms that underlie these changes are still a matter of debate. In this work, we approach language evolution through the lens of causality in order to model not only how various distributional factors associate with language change, but how they causally affect it. In particular, we study slang, which... | Andreas Opedal, Daphna Keidar, Mrinmaya Sachan, Zhijing Jin |  |
| 609 |  |  [Spurious Correlations in Reference-Free Evaluation of Text Generation](https://doi.org/10.18653/v1/2022.acl-long.102) |  | 0 | Model-based, reference-free evaluation metricshave been proposed as a fast and cost-effectiveapproach to evaluate Natural Language Generation(NLG) systems. Despite promising recentresults, we find evidence that reference-freeevaluation metrics of summarization and dialoggeneration may be relying on spuriouscorrelations with measures such as word... | Esin Durmus, Faisal Ladhak, Tatsunori Hashimoto |  |
| 610 |  |  [On The Ingredients of an Effective Zero-shot Semantic Parser](https://doi.org/10.18653/v1/2022.acl-long.103) |  | 0 | Semantic parsers map natural language utterances into meaning representations (e.g., programs). Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts. Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar, and... | Avirup Sil, Graham Neubig, John Wieting, Pengcheng Yin |  |
| 611 |  |  [Bias Mitigation in Machine Translation Quality Estimation](https://doi.org/10.18653/v1/2022.acl-long.104) |  | 0 | Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations. While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation. In particular, there... | Hanna Behnke, Lucia Specia, Marina Fomicheva |  |
| 612 |  |  [Unified Speech-Text Pre-training for Speech Translation and Recognition](https://doi.org/10.18653/v1/2022.acl-long.105) |  | 0 | In this work, we describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method utilizes multi-task learning to integrate four self-supervised and supervised subtasks for cross modality learning. A self-supervised speech subtask, which leverages unlabelled speech data,... | Abdelrahman Mohamed, Alexei Baevski, Changhan Wang, Hongyu Gong, Jiatao Gu, Juan Miguel Pino, Michael Auli, Ning Dong, WeiNing Hsu, Xian Li, Yun Tang |  |
| 613 |  |  [Match the Script, Adapt if Multilingual: Analyzing the Effect of Multilingual Pretraining on Cross-lingual Transferability](https://doi.org/10.18653/v1/2022.acl-long.106) |  | 0 | Pretrained multilingual models enable zero-shot learning even for unseen languages, and that performance can be further improved via adaptation prior to finetuning. However, it is unclear how the number of pretraining languages influences a model’s zero-shot learning for languages unseen during pretraining. To fill this gap, we ask the following research... | Jordan L. BoydGraber, Katharina Kann, Yoshinari Fujinuma |  |
| 614 |  |  [Structured Pruning Learns Compact and Accurate Models](https://doi.org/10.18653/v1/2022.acl-long.107) |  | 0 | The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large... | Danqi Chen, Mengzhou Xia, Zexuan Zhong |  |
| 615 |  |  [How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language](https://doi.org/10.18653/v1/2022.acl-long.108) |  | 0 | More than 43% of the languages spoken in the world are endangered, and language loss currently occurs at an accelerated rate because of globalization and neocolonialism. Saving and revitalizing endangered languages has become very important for maintaining the cultural diversity on our planet. In this work, we focus on discussing how NLP can help revitalize... | Benjamin Frey, Mohit Bansal, Shiyue Zhang |  |
| 616 |  |  [Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization](https://doi.org/10.18653/v1/2022.acl-long.109) |  | 0 | The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist’s reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks... | Hinrich Schütze, Ning Liu, Oladimeji Farri, Sanjeev Kumar Karn |  |
| 617 |  |  [Online Semantic Parsing for Latency Reduction in Task-Oriented Dialogue](https://doi.org/10.18653/v1/2022.acl-long.110) |  | 0 | Standard conversational semantic parsing maps a complete user utterance into an executable program, after which the program is executed to respond to the user. This could be slow when the program contains expensive function calls. We investigate the opportunity to reduce latency by predicting and executing function calls while the user is still speaking. We... | Emmanouil Antonios Platanios, Jason Eisner, Jiawei Zhou, Michael Newman, Sam Thomson |  |
| 618 |  |  [Few-Shot Tabular Data Enrichment Using Fine-Tuned Transformer Architectures](https://doi.org/10.18653/v1/2022.acl-long.111) |  | 0 | The enrichment of tabular datasets using external sources has gained significant attention in recent years. Existing solutions, however, either ignore external unstructured data completely or devise dataset-specific solutions. In this study we proposed Few-Shot Transformer based Enrichment (FeSTE), a generic and robust framework for the enrichment of tabular... | Asaf Harari, Gilad Katz |  |
| 619 |  |  [SummN: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents](https://doi.org/10.18653/v1/2022.acl-long.112) |  | 0 | Text summarization helps readers capture salient information from documents, news, interviews, and meetings. However, most state-of-the-art pretrained language models (LM) are unable to efficiently process long text for many summarization tasks. In this paper, we propose SummN, a simple, flexible, and effective multi-stage framework for input texts that are... | Ahmed Hassan Awadallah, Ansong Ni, Budhaditya Deb, Chen Henry Wu, Chenguang Zhu, Dragomir R. Radev, Rui Zhang, Yusen Zhang, Ziming Mao |  |
| 620 |  |  [Open Domain Question Answering with A Unified Knowledge Interface](https://doi.org/10.18653/v1/2022.acl-long.113) |  | 0 | The retriever-reader framework is popular for open-domain question answering (ODQA) due to its ability to use explicit knowledge. Although prior work has sought to increase the knowledge coverage by incorporating structured knowledge beyond text, accessing heterogeneous knowledge sources through a unified interface remains an open question. While data-to-text... | Eric Nyberg, Hao Cheng, Jianfeng Gao, Kaixin Ma, Xiaodong Liu |  |
| 621 |  |  [Principled Paraphrase Generation with Parallel Corpora](https://doi.org/10.18653/v1/2022.acl-long.114) |  | 0 | Round-trip Machine Translation (MT) is a popular choice for paraphrase generation, which leverages readily available parallel corpora for supervision. In this paper, we formalize the implicit similarity function induced by this approach, and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation. Based on these insights, we... | Aitor Ormazabal, Aitor Soroa, Eneko Agirre, Gorka Labaka, Mikel Artetxe |  |
| 622 |  |  [GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.115) |  | 0 | Over the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (ToD) systems that can serve people speaking different languages. However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in... | Bosheng Ding, Chunyan Miao, Junjie Hu, Lidong Bing, Luo Si, Shafiq R. Joty, Sharifah Mahani Aljunied |  |
| 623 |  |  [Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation](https://doi.org/10.18653/v1/2022.acl-long.116) |  | 0 | Since the development and wide use of pretrained language models (PLMs), several approaches have been applied to boost their performance on downstream tasks in specific domains, such as biomedical or scientific domains. Additional pre-training with in-domain texts is the most common approach for providing domain-specific knowledge to PLMs. However, these... | Dongha Choi, Hongseok Choi, Hyunju Lee |  |
| 624 |  |  [Retrieval-guided Counterfactual Generation for QA](https://doi.org/10.18653/v1/2022.acl-long.117) |  | 0 | Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals — i.e. minimally perturbed inputs — can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity,... | Bhargavi Paranjape, Ian Tenney, Matthew Lamm |  |
| 625 |  |  [DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization](https://doi.org/10.18653/v1/2022.acl-long.118) |  | 0 | Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present DYLE, a novel dynamic latent extraction approach for abstractive long-input summarization. DYLE jointly trains an extractor and a generator and treats the extracted text snippets... | Ahmed Hassan Awadallah, Ansong Ni, Budhaditya Deb, Chen Henry Wu, Chenguang Zhu, Dragomir R. Radev, Rui Zhang, Tao Yu, Yusen Zhang, Ziming Mao |  |
| 626 |  |  [Searching for fingerspelled content in American Sign Language](https://doi.org/10.18653/v1/2022.acl-long.119) |  | 0 | Natural language processing for sign language video—including tasks like recognition, translation, and search—is crucial for making artificial intelligence technologies accessible to deaf individuals, and is gaining research interest in recent years. In this paper, we address the problem of searching for fingerspelled keywords or key phrases in raw sign... | Bowen Shi, Diane Brentari, Greg Shakhnarovich, Karen Livescu |  |
| 627 |  |  [Skill Induction and Planning with Latent Language](https://doi.org/10.18653/v1/2022.acl-long.120) |  | 0 | We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of... | Antonio Torralba, Jacob Andreas, Pratyusha Sharma |  |
| 628 |  |  [Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation](https://doi.org/10.18653/v1/2022.acl-long.121) |  | 0 | A language-independent representation of meaning is one of the most coveted dreams in Natural Language Understanding. With this goal in mind, several formalisms have been proposed as frameworks for meaning representation in Semantic Parsing. And yet, the dependencies these formalisms share with respect to language-specific repositories of knowledge make the... | Abelardo Carlos Martinez Lorenzo, Marco Maru, Roberto Navigli |  |
| 629 |  |  [Leveraging Similar Users for Personalized Language Modeling with Limited Data](https://doi.org/10.18653/v1/2022.acl-long.122) |  | 0 | Personalized language models are designed and trained to capture language patterns specific to individual users. This makes them more accurate at predicting what a user will write. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. We propose a solution for this problem,... | Charles Welch, Chenxi Gu, Jonathan K. Kummerfeld, Rada Mihalcea, Verónica PérezRosas |  |
| 630 |  |  [DEEP: DEnoising Entity Pre-training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.123) |  | 0 | It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we... | Graham Neubig, Hiroaki Hayashi, Junjie Hu, Kyunghyun Cho |  |
| 631 |  |  [Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network](https://doi.org/10.18653/v1/2022.acl-long.124) |  | 0 | With the increasing popularity of posting multimodal messages online, many recent studies have been carried out utilizing both textual and visual information for multi-modal sarcasm detection. In this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic... | Bin Liang, Chenwei Lou, Lin Gui, Min Yang, Ruifeng Xu, Wenjie Pei, Xiang Li, Yulan He |  |
| 632 |  |  [Composable Sparse Fine-Tuning for Cross-Lingual Transfer](https://doi.org/10.18653/v1/2022.acl-long.125) |  | 0 | Fine-tuning the entire set of parameters of a large pretrained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different... | Alan Ansell, Anna Korhonen, Edoardo Maria Ponti, Ivan Vulic |  |
| 633 |  |  [Toward Annotator Group Bias in Crowdsourcing](https://doi.org/10.18653/v1/2022.acl-long.126) |  | 0 | Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models. However, annotator bias can lead to defective annotations. Though there are a few works investigating individual annotator bias, the group effects in annotators are largely overlooked. In this work, we reveal that annotators within the same... | Da Tang, Haochen Liu, Hui Liu, Ji Yang, Jiliang Tang, Joseph Thekinen, Sinem Mollaoglu, Youlong Cheng |  |
| 634 |  |  [Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation](https://doi.org/10.18653/v1/2022.acl-long.127) |  | 0 | Gender bias is largely recognized as a problematic phenomenon affecting language technologies, with recent studies underscoring that it might surface differently across languages. However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions. Such protocols overlook key features of... | Beatrice Savoldi, Luisa Bentivogli, Marco Gaido, Marco Turchi, Matteo Negri |  |
| 635 |  |  [Answering Open-Domain Multi-Answer Questions via a Recall-then-Verify Framework](https://doi.org/10.18653/v1/2022.acl-long.128) |  | 0 | Open-domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers. Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. According to our empirical analysis, this framework faces three problems: first, to leverage a large reader under a memory... | Minlie Huang, Zhihong Shao |  |
| 636 |  |  [Probing as Quantifying Inductive Bias](https://doi.org/10.18653/v1/2022.acl-long.129) |  | 0 | Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks. Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations. In general, researchers quantify the amount of linguistic information through probing, an endeavor... | Alexander Immer, Lucas Torroba Hennigen, Ryan Cotterell, Vincent Fortuin |  |
| 637 |  |  [Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency](https://doi.org/10.18653/v1/2022.acl-long.130) |  | 0 | Structured pruning has been extensively studied on monolingual pre-trained language models and is yet to be fully evaluated on their multilingual counterparts. This work investigates three aspects of structured pruning on multilingual pre-trained language models: settings, algorithms, and efficiency. Experiments on nine downstream tasks show several... | Fei Huang, Fuli Luo, Liwei Wang, Runxin Xu, Songfang Huang, Yanyang Li |  |
| 638 |  |  [GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.131) |  | 0 | Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer’s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are... | Changye Li, David S. Knopman, Serguei Pakhomov, Trevor Cohen, Weizhe Xu |  |
| 639 |  |  [An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.132) |  | 0 | Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative... | Elinor PooleDayan, Nicholas Meade, Siva Reddy |  |
| 640 |  |  [Exploring and Adapting Chinese GPT to Pinyin Input Method](https://doi.org/10.18653/v1/2022.acl-long.133) |  | 0 | While GPT has become the de-facto method for text generation tasks, its application to pinyin input method remains unexplored. In this work, we make the first exploration to leverage Chinese GPT for pinyin input method. We find that a frozen GPT achieves state-of-the-art performance on perfect pinyin. However, the performance drops dramatically when the input... | Duyu Tang, Guoping Huang, Jing Jiang, Jiwei Li, Minghuan Tan, Shuming Shi, Yong Dai, Zhangyin Feng |  |
| 641 |  |  [Enhancing Cross-lingual Natural Language Inference by Prompt-learning from Cross-lingual Templates](https://doi.org/10.18653/v1/2022.acl-long.134) |  | 0 | Cross-lingual natural language inference (XNLI) is a fundamental task in cross-lingual natural language understanding. Recently this task is commonly addressed by pre-trained cross-lingual language models. Existing methods usually enhance pre-trained language models with additional data, such as annotated parallel corpora. These additional data, however, are... | Hai Wan, Haolan Chen, Jianfeng Du, Kunxun Qi |  |
| 642 |  |  [Sense Embeddings are also Biased - Evaluating Social Biases in Static and Contextualised Sense Embeddings](https://doi.org/10.18653/v1/2022.acl-long.135) |  | 0 | Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word. One sense of an ambiguous word might be socially biased while its other senses remain unbiased. In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively... | Danushka Bollegala, Masahiro Kaneko, Yi Zhou |  |
| 643 |  |  [Hybrid Semantics for Goal-Directed Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.136) |  | 0 | We consider the problem of generating natural language given a communicative goal and a world description. We ask the question: is it possible to combine complementary meaning representations to scale a goal-directed NLG system without losing expressiveness? In particular, we consider using two meaning representations, one based on logical semantics and the... | Connor Baumler, Soumya Ray |  |
| 644 |  |  [Predicting Intervention Approval in Clinical Trials through Multi-Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.137) |  | 0 | Clinical trials offer a fundamental opportunity to discover new treatments and advance the medical knowledge. However, the uncertainty of the outcome of a trial can lead to unforeseen costs and setbacks. In this study, we propose a new method to predict the effectiveness of an intervention in a clinical trial. Our method relies on generating an informative... | Georgios Katsimpras, Georgios Paliouras |  |
| 645 |  |  [BiTIIMT: A Bilingual Text-infilling Method for Interactive Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.138) |  | 0 | Interactive neural machine translation (INMT) is able to guarantee high-quality translations by taking human interactions into account. Existing IMT systems relying on lexical constrained decoding (LCD) enable humans to translate in a flexible translation order beyond the left-to-right. However, they typically suffer from two significant limitations in... | Guoping Huang, Jiajun Chen, Lemao Liu, Qu Cui, Shujian Huang, Shuming Shi, Yanling Xiao |  |
| 646 |  |  [Distributionally Robust Finetuning BERT for Covariate Drift in Spoken Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.139) |  | 0 | In this study, we investigate robustness against covariate drift in spoken language understanding (SLU). Covariate drift can occur in SLUwhen there is a drift between training and testing regarding what users request or how they request it. To study this we propose a method that exploits natural variations in data to create a covariate drift in SLU datasets.... | Judith Gaspers, Quynh Do, Samuel Broscheit |  |
| 647 |  |  [Enhancing Chinese Pre-trained Language Model via Heterogeneous Linguistics Graph](https://doi.org/10.18653/v1/2022.acl-long.140) |  | 0 | Chinese pre-trained language models usually exploit contextual character information to learn representations, while ignoring the linguistics knowledge, e.g., word and sentence information. Hence, we propose a task-free enhancement module termed as Heterogeneous Linguistics Graph (HLG) to enhance Chinese pre-trained language models by integrating linguistics... | Bowen Yu, Hongsong Zhu, Jiangxia Cao, Tingwen Liu, Xin Cong, Yanzeng Li, Zhenyu Zhang |  |
| 648 |  |  [Divide and Denoise: Learning from Noisy Labels in Fine-Grained Entity Typing with Cluster-Wise Loss Correction](https://doi.org/10.18653/v1/2022.acl-long.141) |  | 0 | Fine-grained Entity Typing (FET) has made great progress based on distant supervision but still suffers from label noise. Existing FET noise learning methods rely on prediction distributions in an instance-independent manner, which causes the problem of confirmation bias. In this work, we propose a clustering-based loss correction framework named Feature... | Haoyu Zhang, Jie Zhou, Kunyuan Pang, Ting Wang |  |
| 649 |  |  [Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation](https://doi.org/10.18653/v1/2022.acl-long.142) |  | 0 | The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications. Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables. Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new... | Bing Wang, JianGuang Lou, Jiaqi Guo, Xinyu Pi, Yan Gao, Zhoujun Li |  |
| 650 |  |  [Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.143) |  | 0 | Neural networks tend to gradually forget the previously learned knowledge when learning multiple tasks sequentially from dynamic data distributions. This problem is called catastrophic forgetting, which is a fundamental challenge in the continual learning of neural networks. In this work, we observe that catastrophic forgetting not only occurs in continual... | Chenze Shao, Yang Feng |  |
| 651 |  |  [Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages](https://doi.org/10.18653/v1/2022.acl-long.144) |  | 0 | Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity... | Ehsan Aghazadeh, Mohsen Fayyaz, Yadollah Yaghoobzadeh |  |
| 652 |  |  [Discrete Opinion Tree Induction for Aspect-based Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.145) |  | 0 | Dependency trees have been intensively used with graph neural networks for aspect-based sentiment classification. Though being effective, such methods rely on external dependency parsers, which can be unavailable for low-resource languages or perform worse in low-resource domains. In addition, dependency trees are also not optimized for aspect-based sentiment... | Chenhua Chen, Yue Zhang, Zhiyang Teng, Zhongqing Wang |  |
| 653 |  |  [Investigating Non-local Features for Neural Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.146) |  | 0 | Thanks to the strong representation power of neural encoders, neural chart-based parsers have achieved highly competitive performance by using local features. Recently, it has been shown that non-local features in CRF structures lead to improvements. In this paper, we investigate injecting non-local features into the training process of a local span-based... | Leyang Cui, Sen Yang, Yue Zhang |  |
| 654 |  |  [Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing](https://doi.org/10.18653/v1/2022.acl-long.147) |  | 0 | In this paper, we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts, which consequently limits their overall typing performance. To this end, we propose to exploit sibling mentions for enhancing the mention representations. Specifically, we present two different metrics for sibling selection and... | Haisong Zhang, Haiyun Jiang, Jiayang Cheng, Lemao Liu, Ruifeng Xu, Shuming Shi, Yi Chen |  |
| 655 |  |  [A Variational Hierarchical Model for Neural Cross-Lingual Summarization](https://doi.org/10.18653/v1/2022.acl-long.148) |  | 0 | The goal of the cross-lingual summarization (CLS) is to convert a document in one language (e.g., English) to a summary in another one (e.g., Chinese). The CLS task is essentially the combination of machine translation (MT) and monolingual summarization (MS), and thus there exists the hierarchical relationship between MT&MS and CLS. Existing studies on CLS... | Chulun Zhou, Fandong Meng, Jie Zhou, Jinan Xu, Jinsong Su, Yufeng Chen, Yunlong Liang |  |
| 656 |  |  [On the Robustness of Question Rewriting Systems to Questions of Varying Hardness](https://doi.org/10.18653/v1/2022.acl-long.149) |  | 0 | In conversational question answering (CQA), the task of question rewriting (QR) in context aims to rewrite a context-dependent question into an equivalent self-contained question that gives the same answer. In this paper, we are interested in the robustness of a QR system to questions varying in rewriting hardness or difficulty. Since there is a lack of... | Hai Ye, Hwee Tou Ng, Wenjuan Han |  |
| 657 |  |  [OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages](https://doi.org/10.18653/v1/2022.acl-long.150) |  | 0 | AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign... | Gokul N. C., Mitesh M. Khapra, Pratyush Kumar, Prem Selvaraj |  |
| 658 |  |  [bert2BERT: Towards Reusable Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.151) |  | 0 | In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources, and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. In this paper, we propose bert2BERT, which... | Cheng Chen, Fengyu Wang, Lifeng Shang, Qun Liu, Xiao Chen, Xin Jiang, Yichun Yin, Yujia Qin, Zhi Wang, Zhiyuan Liu |  |
| 659 |  |  [Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.152) |  | 0 | As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment Analysis (MABSA) has attracted increasing attention inrecent years. However, previous approaches either (i) use separately pre-trained visual and textual models, which ignore the crossmodalalignment or (ii) use vision-language models pre-trained with general pre-training tasks, which... | Jianfei Yu, Rui Xia, Yan Ling |  |
| 660 |  |  ["You might think about slightly revising the title": Identifying Hedges in Peer-tutoring Interactions](https://doi.org/10.18653/v1/2022.acl-long.153) |  | 0 | Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring... | Chloé Clavel, Justine Cassell, Yann Raphalen |  |
| 661 |  |  [Efficient Cluster-Based k-Nearest-Neighbor Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.154) |  | 0 | k-Nearest-Neighbor Machine Translation (kNN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed... | Boxing Chen, Dexin Wang, Deyi Xiong, Kai Fan |  |
| 662 |  |  [Headed-Span-Based Projective Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.155) |  | 0 | We propose a new method for projective dependency parsing based on headed spans. In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order. We call such a span marked by a root word headed span. A projective dependency tree can be represented as a collection of headed spans. We... | Kewei Tu, Songlin Yang |  |
| 663 |  |  [Decoding Part-of-Speech from Human EEG Signals](https://doi.org/10.18653/v1/2022.acl-long.156) |  | 0 | This work explores techniques to predict Part-of-Speech (PoS) tags from neural signals measured at millisecond resolution with electroencephalography (EEG) during text reading. We first show that information about word length, frequency and word class is encoded by the brain at different post-stimulus latencies. We then demonstrate that pre-training on averaged... | Alex Murphy, Bernd Bohnet, Ryan T. McDonald, Uta Noppeney |  |
| 664 |  |  [Robust Lottery Tickets for Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.157) |  | 0 | Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this... | Bao Rong, Di Liang, Qi Zhang, Rui Zheng, Sirui Wang, Tao Gui, Wei Wu, Xuanjing Huang, Yuhao Zhou |  |
| 665 |  |  [Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification](https://doi.org/10.18653/v1/2022.acl-long.158) |  | 0 | Tuning pre-trained language models (PLMs) with task-specific prompts has been a promising approach for text classification. Particularly, previous studies suggest that prompt-tuning has remarkable superiority in the low-data scenario over the generic fine-tuning methods with extra classifiers. The core idea of prompt-tuning is to insert text pieces, i.e.,... | Huadong Wang, Jingang Wang, Juanzi Li, Maosong Sun, Ning Ding, Shengding Hu, Wei Wu, Zhiyuan Liu |  |
| 666 |  |  [Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages](https://doi.org/10.18653/v1/2022.acl-long.159) |  | 0 | Fine-grained entity typing (FGET) aims to classify named entity mentions into fine-grained entity types, which is meaningful for entity-related NLP tasks. For FGET, a key challenge is the low-resource problem — the complex entity type hierarchy makes it difficult to manually label data. Especially for those languages other than English, human-labeled data is... | Botong Zhou, Fei Hao, Maosong Sun, Suncong Zheng, Weize Chen, Xu Han, Yuqi Luo, Zhiyuan Liu |  |
| 667 |  |  [MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER](https://doi.org/10.18653/v1/2022.acl-long.160) |  | 0 | Data augmentation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as NER, data augmentation methods often suffer from token-label misalignment, which leads to unsatsifactory performance. In this work, we propose Masked Entity Language Modeling (MELM) as a novel data augmentation framework for... | Chunyan Miao, Erik Cambria, Lidong Bing, Luo Si, Ran Zhou, Ruidan He, Xin Li |  |
| 668 |  |  [Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings](https://doi.org/10.18653/v1/2022.acl-long.161) |  | 0 | Learning representations of words in a continuous space is perhaps the most fundamental task in NLP, however words interact in ways much richer than vector dot product similarity can provide. Many relationships between words can be expressed set-theoretically, for example, adjective-noun compounds (eg. “red cars”⊆“cars”) and homographs (eg. “tongue”∩“body”... | Andrew McCallum, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta, Shriya Atmakuri, Siddhartha Mishra, Xiang Li |  |
| 669 |  |  [IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks](https://doi.org/10.18653/v1/2022.acl-long.162) |  | 0 | Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the AI debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating... | Lidong Bing, Liying Cheng, Luo Si, Qian Yu, Ruidan He, Yan Zhang |  |
| 670 |  |  [PLANET: Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation](https://doi.org/10.18653/v1/2022.acl-long.163) |  | 0 | Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose PLANET, a novel generation framework leveraging autoregressive... | Hou Pong Chan, Hua Wu, Jiachen Liu, Lifu Huang, Xinyan Xiao, Zhe Hu |  |
| 671 |  |  [CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation](https://doi.org/10.18653/v1/2022.acl-long.164) |  | 0 | Existing reference-free metrics have obvious limitations for evaluating controlled text generation models. Unsupervised metrics can only provide a task-agnostic evaluation result which correlates weakly with human judgments, whereas supervised ones may overfit task-specific data with poor generalization ability to other datasets. In this paper, we propose an... | Hao Zhou, Jie Zhou, Minlie Huang, Pei Ke, Peng Li, Xiaoyan Zhu, Yankai Lin |  |
| 672 |  |  [Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking](https://doi.org/10.18653/v1/2022.acl-long.165) |  | 0 | In dialogue state tracking, dialogue history is a crucial material, and its utilization varies between different models. However, no matter how the dialogue history is used, each existing model uses its own consistent dialogue history during the entire state tracking process, regardless of which slot is updated. Apparently, it requires different dialogue... | Jijie Li, Jinyu Guo, Kai Shuang, Yixuan Liu, Zihan Wang |  |
| 673 |  |  [Are Prompt-based Models Clueless?](https://doi.org/10.18653/v1/2022.acl-long.166) |  | 0 | Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.... | Pride Kavumba, Ryo Takahashi, Yusuke Oda |  |
| 674 |  |  [Learning Confidence for Transformer-based Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.167) |  | 0 | Confidence estimation aims to quantify the confidence of the model prediction, providing an expectation of success. A well-calibrated confidence estimate enables accurate failure prediction and proper risk measurement when given noisy samples and out-of-distribution data in real-world settings. However, this task remains a severe challenge for neural machine... | Jiajun Zhang, Jiali Zeng, Mu Li, Shuangzhi Wu, Yu Lu |  |
| 675 |  |  [Things not Written in Text: Exploring Spatial Commonsense from Visual Signals](https://doi.org/10.18653/v1/2022.acl-long.168) |  | 0 | Spatial commonsense, the knowledge about spatial position and relationship between objects (like the relative size of a lion and a girl, and the position of a boy relative to a bicycle when cycling), is an important part of commonsense knowledge. Although pretrained language models (PLMs) succeed in many NLP tasks, they are shown to be ineffective in spatial... | Da Yin, Dongyan Zhao, Xiao Liu, Yansong Feng |  |
| 676 |  |  [Conditional Bilingual Mutual Information Based Adaptive Training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.169) |  | 0 | Token-level adaptive training approaches can alleviate the token imbalance problem and thus improve neural machine translation, through re-weighting the losses of different target tokens based on specific statistical metrics (e.g., token frequency or mutual information). Given that standard translation models make predictions on the condition of previous target... | Fandong Meng, Jian Liu, Jie Zhou, Jinan Xu, Songming Zhang, Yijin Liu, Yufeng Chen |  |
| 677 |  |  [ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer](https://doi.org/10.18653/v1/2022.acl-long.170) |  | 0 | Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to... | Guobing Gan, Ningning Wang, Peng Zhang, Qun Liu, Shuai Zhang, Victor Junqiu Wei, Xin Jiang |  |
| 678 |  |  [Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks](https://doi.org/10.18653/v1/2022.acl-long.171) |  | 0 | Constituency parsing and nested named entity recognition (NER) are similar tasks since they both aim to predict a collection of nested and non-crossing spans. In this work, we cast nested NER to constituency parsing and propose a novel pointing mechanism for bottom-up parsing to tackle both tasks. The key idea is based on the observation that if we traverse a... | Kewei Tu, Songlin Yang |  |
| 679 |  |  [Redistributing Low-Frequency Words: Making the Most of Monolingual Data in Non-Autoregressive Translation](https://doi.org/10.18653/v1/2022.acl-long.172) |  | 0 | Knowledge distillation (KD) is the preliminary step for training non-autoregressive translation (NAT) models, which eases the training of NAT models at the cost of losing important information for translating low-frequency words. In this work, we provide an appealing alternative for NAT – monolingual KD, which trains NAT student on external monolingual data... | Dacheng Tao, Liang Ding, Longyue Wang, Shuming Shi, Zhaopeng Tu |  |
| 680 |  |  [Dependency Parsing as MRC-based Span-Span Prediction](https://doi.org/10.18653/v1/2022.acl-long.173) |  | 0 | Higher-order methods for dependency parsing can partially but not fully address the issue that edges in dependency trees should be constructed at the text span/subtree level rather than word level. In this paper, we propose a new method for dependency parsing to address this issue. The proposed method constructs dependency trees by directly modeling span-span... | Chun Fan, Fei Wu, Jiwei Li, Kun Kuang, Leilei Gan, Xiaofei Sun, Yuxian Meng |  |
| 681 |  |  [Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.174) |  | 0 | Cross-domain sentiment analysis has achieved promising results with the help of pre-trained language models. As GPT-3 appears, prompt tuning has been widely explored to enable better semantic modeling in many natural language processing tasks. However, directly using a fixed predefined template for cross-domain research cannot model different distributions of... | Hui Wu, Xiaodong Shi |  |
| 682 |  |  [Generating Scientific Claims for Zero-Shot Scientific Fact Checking](https://doi.org/10.18653/v1/2022.acl-long.175) |  | 0 | Automated scientific fact checking is difficult due to the complexity of scientific language and a lack of significant amounts of training data, as annotation requires domain expertise. To address this challenge, we propose scientific claim generation, the task of generating one or more atomic and verifiable claims from scientific sentences, and demonstrate its... | Arman Cohan, Bailey Kuehl, David Wadden, Dustin Wright, Isabelle Augenstein, Kyle Lo, Lucy Lu Wang |  |
| 683 |  |  [Modeling Dual Read/Write Paths for Simultaneous Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.176) |  | 0 | Simultaneous machine translation (SiMT) outputs translation while reading source sentence and hence requires a policy to decide whether to wait for the next source word (READ) or generate a target word (WRITE), the actions of which form a read/write path. Although the read/write path is essential to SiMT performance, no direct supervision is given to the path... | Shaolei Zhang, Yang Feng |  |
| 684 |  |  [ExtEnD: Extractive Entity Disambiguation](https://doi.org/10.18653/v1/2022.acl-long.177) |  | 0 | Local models for Entity Disambiguation (ED) have today become extremely powerful, in most part thanks to the advent of large pre-trained language models. However, despite their significant performance achievements, most of these approaches frame ED through classification formulations that have intrinsic limitations, both computationally and from a modeling... | Edoardo Barba, Luigi Procopio, Roberto Navigli |  |
| 685 |  |  [Hierarchical Sketch Induction for Paraphrase Generation](https://doi.org/10.18653/v1/2022.acl-long.178) |  | 0 | We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch. We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of... | Hao Tang, Mirella Lapata, Tom Hosking |  |
| 686 |  |  [Alignment-Augmented Consistent Translation for Multilingual Open Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.179) |  | 0 | Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages. In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages. We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to... | Keshav Kolluru, Mausam, Muqeeth Mohammed, Shubham Mittal, Soumen Chakrabarti |  |
| 687 |  |  [Text-to-Table: A New Way of Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.180) |  | 0 | We study a new problem setting of information extraction (IE), referred to as text-to-table. In text-to-table, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for IE. First, the extraction can be... | Hang Li, Jiacheng Zhang, Xueqing Wu |  |
| 688 |  |  [Accelerating Code Search with Deep Hashing and Code Classification](https://doi.org/10.18653/v1/2022.acl-long.181) |  | 0 | Code search is to search reusable code snippets from source code corpus based on natural languages queries. Deep learning-based methods on code search have shown promising results. However, previous methods focus on retrieval accuracy, but lacked attention to the efficiency of the retrieval process. We propose a novel method CoSHC to accelerate code search with... | Dongmei Zhang, Hongyu Zhang, Lun Du, Michael R. Lyu, Shi Han, Wenchao Gu, Yanlin Wang |  |
| 689 |  |  [Other Roles Matter! Enhancing Role-Oriented Dialogue Summarization via Role Interactions](https://doi.org/10.18653/v1/2022.acl-long.182) |  | 0 | Role-oriented dialogue summarization is to generate summaries for different roles in the dialogue, e.g., merchants and consumers. Existing methods handle this task by summarizing each role’s content separately and thus are prone to ignore the information from other roles. However, we believe that other roles’ content could benefit the quality of summaries, such... | Chengqing Zong, Haitao Lin, Jiajun Zhang, Junnan Zhu, Lu Xiang, Yu Zhou |  |
| 690 |  |  [ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification](https://doi.org/10.18653/v1/2022.acl-long.183) |  | 0 | Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks. Existing works either limit their scope to specific scenarios or overlook event-level correlations. In this paper, we propose to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning. To achieve... | Daxin Jiang, Guodong Long, Tao Shen, Xiubo Geng, Yucheng Zhou |  |
| 691 |  |  [Measuring and Mitigating Name Biases in Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.184) |  | 0 | Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender. In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names. To correctly translate such sentences, a NMT... | Benjamin I. P. Rubinstein, Jun Wang, Trevor Cohn |  |
| 692 |  |  [Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.185) |  | 0 | In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation (NMT). We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT. By carefully... | Michael R. Lyu, Shuming Shi, Wenxiang Jiao, Wenxuan Wang, Xing Wang, Yongchang Hao, Zhaopeng Tu |  |
| 693 |  |  [MSCTD: A Multimodal Sentiment Chat Translation Dataset](https://doi.org/10.18653/v1/2022.acl-long.186) |  | 0 | Multimodal machine translation and textual chat translation have received considerable attention in recent years. Although the conversation in its natural form is usually multimodal, there still lacks work on multimodal machine translation in conversations. In this work, we introduce a new task named Multimodal Chat Translation (MCT), aiming to generate more... | Fandong Meng, Jie Zhou, Jinan Xu, Yufeng Chen, Yunlong Liang |  |
| 694 |  |  [Learning Disentangled Textual Representations via Statistical Measures of Similarity](https://doi.org/10.18653/v1/2022.acl-long.187) |  | 0 | When working with textual data, a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased (or influenced) by sensible attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely... | Guillaume Staerman, Nathan Noiry, Pablo Piantanida, Pierre Colombo |  |
| 695 |  |  [On the Sensitivity and Stability of Model Interpretations in NLP](https://doi.org/10.18653/v1/2022.acl-long.188) |  | 0 | Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretation methods, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent interpretations... | ChoJui Hsieh, Fan Yin, KaiWei Chang, Zhouxing Shi |  |
| 696 |  |  [Down and Across: Introducing Crossword-Solving as a New NLP Benchmark](https://doi.org/10.18653/v1/2022.acl-long.189) |  | 0 | Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle. In this work, we introduce solving crossword puzzles as a new natural language understanding task. We release a corpus of crossword puzzles... | Anna Rumshisky, Namrata Shivagunde, Olga Kovaleva, Saurabh Kulshreshtha |  |
| 697 |  |  [Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets](https://doi.org/10.18653/v1/2022.acl-long.190) |  | 0 | Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train... | Matt Gardner, Pontus Stenetorp, Pradeep Dasigi, Yuxiang Wu |  |
| 698 |  |  [GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.191) |  | 0 | Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (SLU) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present Global-Local Contrastive Learning Framework... | JianGuang Lou, Libo Qin, MinYen Kan, Qiguang Chen, Qixin Li, Tianbao Xie, Wanxiang Che |  |
| 699 |  |  [Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER](https://doi.org/10.18653/v1/2022.acl-long.192) |  | 0 | Recent advances in prompt-based learning have shown strong results on few-shot text classification by using cloze-style templates. Similar attempts have been made on named entity recognition (NER) which manually design templates to predict entity types for every text span in a sentence. However, such methods may suffer from error propagation induced by entity... | Akshen Kadakia, DongHo Lee, Jay Pujara, Kangmin Tan, Mahak Agarwal, Ryosuke Mitani, Takashi Shibuya, Toshiyuki Sekiya, Xiang Ren, Xinyu Feng |  |
| 700 |  |  [Contextual Representation Learning beyond Masked Language Modeling](https://doi.org/10.18653/v1/2022.acl-long.193) |  | 0 | Currently, masked language modeling (e.g., BERT) is the prime choice to learn contextualized representations. Due to the pervasiveness, it naturally raises an interesting question: how do masked language models (MLMs) learn contextual representations? In this work, we analyze the learning dynamics of MLMs and find that it adopts sampled embeddings as anchors to... | Hao Zhou, Jingjing Xu, Lei Li, Wangchunshu Zhou, Zhiyi Fu |  |
| 701 |  |  [Efficient Hyper-parameter Search for Knowledge Graph Embedding](https://doi.org/10.18653/v1/2022.acl-long.194) |  | 0 | While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which... | Quanming Yao, Yong Li, Yongqi Zhang, Zhanke Zhou |  |
| 702 |  |  [A Meta-framework for Spatiotemporal Quantity Extraction from Text](https://doi.org/10.18653/v1/2022.acl-long.195) |  | 0 | News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the... | Ben Zhou, Chuchu Fan, Hao Wu, Haoruo Peng, Matt Gardner, Qiang Ning |  |
| 703 |  |  [Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer](https://doi.org/10.18653/v1/2022.acl-long.196) |  | 0 | Pre-trained language models are still far from human performance in tasks that need understanding of properties (e.g. appearance, measurable quantity) and affordances of everyday objects in the real world since the text lacks such information due to reporting bias. In this work, we study whether integrating visual knowledge into a language model can fill the... | Chenguang Zhu, DongHo Lee, Jay Pujara, Woojeong Jin, Xiang Ren |  |
| 704 |  |  [A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models](https://doi.org/10.18653/v1/2022.acl-long.197) |  | 0 | Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning. However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed. To solve this limitation, we study prompt-based low-resource learning of VL tasks... | Weizhu Chen, Woojeong Jin, Xiang Ren, Yelong Shen, Yu Cheng |  |
| 705 |  |  [Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation](https://doi.org/10.18653/v1/2022.acl-long.198) |  | 0 | Existing continual relation learning (CRL) methods rely on plenty of labeled training data for learning a new task, which can be hard to acquire in real scenario as getting large and representative labeled data is often expensive and time-consuming. It is therefore necessary for the model to learn novel relational patterns with very few labeled data while... | Chengwei Qin, Shafiq R. Joty |  |
| 706 |  |  [Variational Graph Autoencoding as Cheap Supervision for AMR Coreference Resolution](https://doi.org/10.18653/v1/2022.acl-long.199) |  | 0 | Coreference resolution over semantic graphs like AMRs aims to group the graph nodes that represent the same entity. This is a crucial step for making document-level formal semantic representations. With annotated data on AMR coreference resolution, deep learning approaches have recently shown great potential for this task, yet they are usually data hunger and... | Dong Yu, Irene Li, Kun Xu, Linfeng Song |  |
| 707 |  |  [Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations](https://doi.org/10.18653/v1/2022.acl-long.200) |  | 0 | Recent works of opinion expression identification (OEI) rely heavily on the quality and scale of the manually-constructed training corpus, which could be extremely difficult to satisfy. Crowdsourcing is one practical solution for this problem, aiming to create a large-scale but quality-unguaranteed corpus. In this work, we investigate Chinese OEI with... | Guangwei Xu, Meishan Zhang, Min Zhang, Xiaobin Wang, Xin Zhang, Yueheng Sun |  |
| 708 |  |  [Sequence-to-Sequence Knowledge Graph Completion and Question Answering](https://doi.org/10.18653/v1/2022.acl-long.201) |  | 0 | Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors. These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA). KGEs typically create an embedding for each entity in the graph, which results in large model sizes on... | Adrian Kochsiek, Apoorv Saxena, Rainer Gemulla |  |
| 709 |  |  [Learning to Mediate Disparities Towards Pragmatic Communication](https://doi.org/10.18653/v1/2022.acl-long.202) |  | 0 | Human communication is a collaborative process. Speakers, on top of conveying their own intent, adjust the content and language expressions by taking the listeners into account, including their knowledge background, personalities, and physical capabilities. Towards building AI agents with similar abilities in language communication, we propose a novel rational... | Joyce Chai, Sayan Ghosh, Yuwei Bao |  |
| 710 |  |  [Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.203) |  | 0 | Recent research demonstrates the effectiveness of using fine-tuned language models (LM) for dense retrieval. However, dense retrievers are hard to train, typically requiring heavily engineered fine-tuning pipelines to realize their full potential. In this paper, we identify and address two underlying problems of dense retrievers: i) fragility to training data... | Jamie Callan, Luyu Gao |  |
| 711 |  |  [Multimodal Dialogue Response Generation](https://doi.org/10.18653/v1/2022.acl-long.204) |  | 0 | Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG) -... | Can Xu, Daxin Jiang, Fei Xu, Huang Hu, Jessica Zhang, Kai Zheng, Qingfeng Sun, Xiubo Geng, Yaming Yang, Yujing Wang |  |
| 712 |  |  [CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.acl-long.205) |  | 0 | Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (KGE) techniques suffer from invalid negative... | Bo Li, Guanglin Niu, Shiliang Pu, Yongfei Zhang |  |
| 713 |  |  [Confidence Based Bidirectional Global Context Aware Training Framework for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.206) |  | 0 | Most dominant neural machine translation (NMT) models are restricted to make predictions only according to the local context of preceding words in a left-to-right manner. Although many previous studies try to incorporate global information into NMT models, there still exist limitations on how to effectively exploit bidirectional global context. In this paper,... | Chulun Zhou, Fandong Meng, Hongji Wang, Jie Zhou, Jinsong Su, Min Zhang |  |
| 714 |  |  [BRIO: Bringing Order to Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.207) |  | 0 | Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several... | Dragomir R. Radev, Graham Neubig, Pengfei Liu, Yixin Liu |  |
| 715 |  |  [Leveraging Relaxed Equilibrium by Lazy Transition for Sequence Modeling](https://doi.org/10.18653/v1/2022.acl-long.208) |  | 0 | In sequence modeling, certain tokens are usually less ambiguous than others, and representations of these tokens require fewer refinements for disambiguation. However, given the nature of attention-based models like Transformer and UT (universal transformer), all tokens are equally processed towards depth. Inspired by the equilibrium phenomenon, we present a... | Bin Fang, Xi Ai |  |
| 716 |  |  [FIBER: Fill-in-the-Blanks as a Challenging Video Understanding Evaluation Framework](https://doi.org/10.18653/v1/2022.acl-long.209) |  | 0 | We propose fill-in-the-blanks as a video understanding evaluation framework and introduce FIBER – a novel dataset consisting of 28,000 videos and descriptions in support of this evaluation framework. The fill-in-the-blanks setting tests a model’s understanding of a video by requiring it to predict a masked noun phrase in the caption of the video, given the... | Ian Stewart, Jonathan C. Stroud, Nan Liu, Oana Ignat, Pingxuan Huang, Rada Mihalcea, Ruoyao Wang, Santiago Castro |  |
| 717 |  |  [KenMeSH: Knowledge-enhanced End-to-end Biomedical Text Labelling](https://doi.org/10.18653/v1/2022.acl-long.210) |  | 0 | Currently, Medical Subject Headings (MeSH) are manually assigned to every biomedical article published and subsequently recorded in the PubMed database to facilitate retrieving relevant information. With the rapid growth of the PubMed database, large-scale biomedical document indexing becomes increasingly important. MeSH indexing is a challenging task for... | Frank Rudzicz, Robert E. Mercer, Xindi Wang |  |
| 718 |  |  [A Taxonomy of Empathetic Questions in Social Dialogs](https://doi.org/10.18653/v1/2022.acl-long.211) |  | 0 | Effective question-asking is a crucial component of a successful conversational chatbot. It could help the bots manifest empathy and render the interaction more engaging by demonstrating attention to the speaker’s emotions. However, current dialog generation approaches do not model this subtle emotion regulation technique due to the lack of a taxonomy of... | Anuradha Welivita, Ekaterina Svikhnushina, Iuliana Voinea, Pearl Pu |  |
| 719 |  |  [Enhanced Multi-Channel Graph Convolutional Network for Aspect Sentiment Triplet Extraction](https://doi.org/10.18653/v1/2022.acl-long.212) |  | 0 | Aspect Sentiment Triplet Extraction (ASTE) is an emerging sentiment analysis task. Most of the existing studies focus on devising a new tagging scheme that enables the model to extract the sentiment triplets in an end-to-end fashion. However, these methods ignore the relations between words for ASTE task. In this paper, we propose an Enhanced Multi-Channel... | Fangxiang Feng, Hao Chen, Ruifan Li, Xiaojie Wang, Zepeng Zhai |  |
| 720 |  |  [ProtoTEx: Explaining Model Decisions with Prototype Tensors](https://doi.org/10.18653/v1/2022.acl-long.213) |  | 0 | We present ProtoTEx, a novel white-box NLP classification architecture based on prototype networks (Li et al., 2018). ProtoTEx faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors,... | Anubrata Das, Chitrank Gupta, Junyi Jessy Li, Matthew Lease, Venelin Kovatchev |  |
| 721 |  |  [Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data](https://doi.org/10.18653/v1/2022.acl-long.214) |  | 0 | Procedures are inherently hierarchical. To “make videos”, one may need to “purchase a camera”, which in turn may require one to “set a budget”. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation. In this work, we attempt... | Chris CallisonBurch, Graham Neubig, Li Zhang, Pengcheng Yin, Qing Lyu, Shuyan Zhou, Yue Yang |  |
| 722 |  |  [Cross-Modal Discrete Representation Learning](https://doi.org/10.18653/v1/2022.acl-long.215) |  | 0 | In contrast to recent advances focusing on high-level representation learning across modalities, in this work we present a self-supervised learning framework that is able to learn a representation that captures finer levels of granularity across different modalities such as concepts or events represented by visual objects or spoken words. Our framework relies... | Alexander H. Liu, Andrew Rouditchenko, Aude Oliva, ChengI Lai, James R. Glass, SouYoung Jin |  |
| 723 |  |  [Improving Event Representation via Simultaneous Weakly Supervised Contrastive Learning and Clustering](https://doi.org/10.18653/v1/2022.acl-long.216) |  | 0 | Representations of events described in text are important for various tasks. In this work, we present SWCC: a Simultaneous Weakly supervised Contrastive learning and Clustering framework for event representation learning. SWCC learns event representations by making better use of co-occurrence information of events. Specifically, we introduce a weakly supervised... | Changlong Yu, Huan Zhao, Jun Gao, Ruifeng Xu, Wei Wang, Wilfred Ng |  |
| 724 |  |  [Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations](https://doi.org/10.18653/v1/2022.acl-long.217) |  | 0 | We examine the effects of contrastive visual semantic pretraining by comparing the geometry and semantic properties of contextualized English language representations formed by GPT-2 and CLIP, a zero-shot multimodal image classifier which adapts the GPT-2 architecture to encode image captions. We find that contrastive visual semantic pretraining significantly... | Aylin Caliskan, Robert Wolfe |  |
| 725 |  |  [ConTinTin: Continual Learning from Task Instructions](https://doi.org/10.18653/v1/2022.acl-long.218) |  | 0 | The mainstream machine learning paradigms for NLP often work with two underlying presumptions. First, the target task is predefined and static; a system merely needs to learn to solve it exclusively. Second, the supervision of a task mainly comes from a set of labeled examples. A question arises: how to build a system that can keep learning new tasks from their... | Caiming Xiong, Jia Li, Wenpeng Yin |  |
| 726 |  |  [Automated Crossword Solving](https://doi.org/10.18653/v1/2022.acl-long.219) |  | 0 | We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system... | Albert Xu, Dan Klein, Eric Wallace, Eshaan Pathak, Kevin Yang, Matthew L. Ginsberg, Nicholas Tomlin |  |
| 727 |  |  [Learned Incremental Representations for Parsing](https://doi.org/10.18653/v1/2022.acl-long.220) |  | 0 | We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to... | Dan Klein, Nikita Kitaev, Thomas Lu |  |
| 728 |  |  [Knowledge Enhanced Reflection Generation for Counseling Dialogues](https://doi.org/10.18653/v1/2022.acl-long.221) |  | 0 | In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration. We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domain-specific and commonsense knowledge bases improves the... | Charles Welch, Rada Mihalcea, Siqi Shen, Soujanya Poria, Verónica PérezRosas |  |
| 729 |  |  [Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines](https://doi.org/10.18653/v1/2022.acl-long.222) |  | 0 | Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer’s intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news. We... | Eunsol Choi, Franziska Roesner, Maarten Sap, Pemi Nguyen, Saadia Gabriel, Skyler Hallinan, Yejin Choi |  |
| 730 |  |  [On Continual Model Refinement in Out-of-Distribution Data Streams](https://doi.org/10.18653/v1/2022.acl-long.223) |  | 0 | Real-world natural language processing (NLP) models need to be continually updated to fix the prediction errors in out-of-distribution (OOD) data streams while overcoming catastrophic forgetting. However, existing continual learning (CL) problem setups cannot cover such a realistic and complex scenario. In response to this, we propose a new CL problem... | Bill Yuchen Lin, Lin Xiao, Robin Jia, Scott Yih, Sida Wang, Xi Victoria Lin, Xiang Ren |  |
| 731 |  |  [Achieving Conversational Goals with Unsupervised Post-hoc Knowledge Injection](https://doi.org/10.18653/v1/2022.acl-long.224) |  | 0 | A limitation of current neural dialog models is that they tend to suffer from a lack of specificity and informativeness in generated responses, primarily due to dependence on training data that covers a limited variety of scenarios and conveys limited knowledge. One way to alleviate this issue is to extract relevant knowledge from external sources at decoding... | Bodhisattwa Prasad Majumder, Harsh Jhamtani, Julian J. McAuley, Taylor BergKirkpatrick |  |
| 732 |  |  [Generated Knowledge Prompting for Commonsense Reasoning](https://doi.org/10.18653/v1/2022.acl-long.225) |  | 0 | It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering... | Alisa Liu, Hannaneh Hajishirzi, Jiacheng Liu, Peter West, Ronan Le Bras, Sean Welleck, Ximing Lu, Yejin Choi |  |
| 733 |  |  [Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data](https://doi.org/10.18653/v1/2022.acl-long.226) |  | 0 | Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled... | Chenguang Zhu, Michael Zeng, Ruochen Xu, Shuohang Wang, Siqi Sun, Yang Liu, Yichong Xu, Yuwei Fang |  |
| 734 |  |  [Life after BERT: What do Other Muppets Understand about Language?](https://doi.org/10.18653/v1/2022.acl-long.227) |  | 0 | Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the... | Anna Rumshisky, Kevin Zhao, Namrata Shivagunde, Vladislav Lialin |  |
| 735 |  |  [Tailor: Generating and Perturbing Text with Semantic Controls](https://doi.org/10.18653/v1/2022.acl-long.228) |  | 0 | Controlled text perturbation is useful for evaluating and improving model generalizability. However, current techniques rely on training a model for every target perturbation, which is expensive and hard to generalize. We present Tailor, a semantically-controlled text generation system. Tailor builds on a pretrained seq2seq model and produces textual outputs... | Alexis Ross, Hao Peng, Matt Gardner, Matthew E. Peters, Tongshuang Wu |  |
| 736 |  |  [TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://doi.org/10.18653/v1/2022.acl-long.229) |  | 0 | We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating... | Jacob Hilton, Owain Evans, Stephanie Lin |  |
| 737 |  |  [Adaptive Testing and Debugging of NLP Models](https://doi.org/10.18653/v1/2022.acl-long.230) |  | 0 | Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are... | Marco Túlio Ribeiro, Scott M. Lundberg |  |
| 738 |  |  [Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning](https://doi.org/10.18653/v1/2022.acl-long.231) |  | 0 | When pre-trained contextualized embedding-based models developed for unstructured data are adapted for structured tabular data, they perform admirably. However, recent probing studies show that these models use spurious correlations, and often predict inference labels by focusing on false evidence or ignoring it altogether. To study this issue, we introduce the... | Alakananda Vempala, Shuo Zhang, Temma Choji, Vivek Gupta, Vivek Srikumar, Yujie He |  |
| 739 |  |  [Interactive Word Completion for Plains Cree](https://doi.org/10.18653/v1/2022.acl-long.232) |  | 0 | The composition of richly-inflected words in morphologically complex languages can be a challenge for language learners developing literacy. Accordingly, Lane and Bird (2020) proposed a finite state approach which maps prefixes in a language to a set of possible completions up to the next morpheme boundary, for the incremental building of complex words. In this... | Antti Arppe, Atticus Harrigan, William Lane |  |
| 740 |  |  [LAGr: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.233) |  | 0 | Semantic parsing is the task of producing structured meaning representations for natural language sentences. Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings. In this work, we show that... | Dora Jambor, Dzmitry Bahdanau |  |
| 741 |  |  [ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://doi.org/10.18653/v1/2022.acl-long.234) |  | 0 | Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and... | Dipankar Ray, Ece Kamar, Hamid Palangi, Maarten Sap, Saadia Gabriel, Thomas Hartvigsen |  |
| 742 |  |  [Direct Speech-to-Speech Translation With Discrete Units](https://doi.org/10.18653/v1/2022.acl-long.235) |  | 0 | We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT)... | Adam Polyak, Ann Lee, Changhan Wang, Jiatao Gu, Juan Pino, PengJen Chen, Qing He, Sravya Popuri, WeiNing Hsu, Xutai Ma, Yossi Adi, Yun Tang |  |
| 743 |  |  [Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.236) |  | 0 | State-of-the-art abstractive summarization systems often generate hallucinations; i.e., content that is not directly inferable from the source text. Despite being assumed to be incorrect, we find that much hallucinated content is actually consistent with world knowledge, which we call factual hallucinations. Including these factual hallucinations in a summary... | Jackie Chi Kit Cheung, Meng Cao, Yue Dong |  |
| 744 |  |  [EntSUM: A Data Set for Entity-Centric Extractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.237) |  | 0 | Controllable summarization aims to provide summaries that take into account user-specified aspects and preferences to better assist them with their information need, as opposed to the standard summarization setup which build a single generic summary of a document. We introduce a human-annotated data set EntSUM for controllable summarization with a focus on... | Daniel PreotiucPietro, Mayank Kulkarni, Mounica Maddela |  |
| 745 |  |  [Sentence-level Privacy for Document Embeddings](https://doi.org/10.18653/v1/2022.acl-long.238) |  | 0 | User language data can contain highly sensitive personal content. As such, it is imperative to offer users a strong and interpretable privacy guarantee when learning from their data. In this work we propose SentDP, pure local differential privacy at the sentence level for a single user document. We propose a novel technique, DeepCandidate, that combines... | Casey Meehan, Kamalika Chaudhuri, Khalil Mrini |  |
| 746 |  |  [Dataset Geography: Mapping Language Data to Language Users](https://doi.org/10.18653/v1/2022.acl-long.239) |  | 0 | As language technologies become more ubiquitous, there are increasing efforts towards expanding the language diversity and coverage of natural language processing (NLP) systems. Arguably, the most important factor influencing the quality of modern NLP systems is data availability. In this work, we study the geographical representativeness of NLP datasets,... | Antonios Anastasopoulos, Fahim Faisal, Yinkai Wang |  |
| 747 |  |  [ILDAE: Instance-Level Difficulty Analysis of Evaluation Data](https://doi.org/10.18653/v1/2022.acl-long.240) |  | 0 | Knowledge of difficulty level of questions helps a teacher in several ways, such as estimating students’ potential quickly by asking carefully selected questions and improving quality of examination by modifying trivial and hard questions. Can we extract such benefits of instance difficulty in Natural Language Processing? To this end, we conduct Instance-Level... | Chitta Baral, Neeraj Varshney, Swaroop Mishra |  |
| 748 |  |  [Image Retrieval from Contextual Descriptions](https://doi.org/10.18653/v1/2022.acl-long.241) |  | 0 | The ability to integrate context, including perceptual and temporal cues, plays a pivotal role in grounding the meaning of a linguistic utterance. In order to measure to what extent current vision-and-language models master this ability, we devise a new multimodal challenge, Image Retrieval from Contextual Descriptions (ImageCoDe). In particular, models are... | Benno Krojer, Edoardo Maria Ponti, Siva Reddy, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal |  |
| 749 |  |  [Multilingual Molecular Representation Learning via Contrastive Pre-training](https://doi.org/10.18653/v1/2022.acl-long.242) |  | 0 | Molecular representation learning plays an essential role in cheminformatics. Recently, language model-based approaches have gained popularity as an alternative to traditional expert-designed features to encode molecules. However, these approaches only utilize a single molecular language for representation learning. Motivated by the fact that a given molecule... | Andy Martinez, Liang Du, Pramod Kumar Sharma, Robin Abraham, Zhihui Guo |  |
| 750 |  |  [Investigating Failures of Automatic Translationin the Case of Unambiguous Gender](https://doi.org/10.18653/v1/2022.acl-long.243) |  | 0 | Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn’t mark gender on nouns into... | Adi Renduchintala, Adina Williams |  |
| 751 |  |  [Cross-Task Generalization via Natural Language Crowdsourcing Instructions](https://doi.org/10.18653/v1/2022.acl-long.244) |  | 0 | Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve... | Chitta Baral, Daniel Khashabi, Hannaneh Hajishirzi, Swaroop Mishra |  |
| 752 |  |  [Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost](https://doi.org/10.18653/v1/2022.acl-long.245) |  | 0 | State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words. To address this issue, we follow the principle of mimick-like models to generate vectors for unseen words, by learning the behavior of pre-trained embeddings using only the surface form of words. We present a simple... | Fabian M. Suchanek, Gaël Varoquaux, Lihu Chen |  |
| 753 |  |  [NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks](https://doi.org/10.18653/v1/2022.acl-long.246) |  | 0 | Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing... | Arindam Mitra, Ashwin Kalyan, Bhavdeep Singh Sachdeva, Chitta Baral, Neeraj Varshney, Peter Clark, Swaroop Mishra |  |
| 754 |  |  [Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.247) |  | 0 | A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific... | Ari Kobren, Michael L. Wick, Ryan Steed, Swetasudha Panda |  |
| 755 |  |  [Improving Multi-label Malevolence Detection in Dialogues through Multi-faceted Label Correlation Enhancement](https://doi.org/10.18653/v1/2022.acl-long.248) |  | 0 | A dialogue response is malevolent if it is grounded in negative emotions, inappropriate behavior, or an unethical value basis in terms of content and dialogue acts. The detection of malevolent dialogue responses is attracting growing interest. Current research on detecting dialogue malevolence has limitations in terms of datasets and methods. First, available... | Maarten de Rijke, Pengjie Ren, Wentao Deng, Yangjun Zhang, Zhumin Chen |  |
| 756 |  |  [How Do We Answer Complex Questions: Discourse Structure of Long-form Answers](https://doi.org/10.18653/v1/2022.acl-long.249) |  | 0 | Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize... | Eunsol Choi, Fangyuan Xu, Junyi Jessy Li |  |
| 757 |  |  [Understanding Iterative Revision from Human-Written Text](https://doi.org/10.18653/v1/2022.acl-long.250) |  | 0 | Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits,... | Dhruv Kumar, Dongyeop Kang, Melissa Lopez, Vipul Raheja, Wanyu Du, Zae Myung Kim |  |
| 758 |  |  [Making Transformers Solve Compositional Tasks](https://doi.org/10.18653/v1/2022.acl-long.251) |  | 0 | Several studies have reported the inability of Transformer models to generalize compositionally, a key type of generalization in many NLP tasks such as semantic parsing. In this paper we explore the design space of Transformer models showing that the inductive biases given to the model by several design decisions significantly impact compositional... | Joshua Ainslie, Santiago Ontañón, Vaclav Cvicek, Zachary Fisher |  |
| 759 |  |  [Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.252) |  | 0 | Unlike literal expressions, idioms’ meanings do not directly follow from their parts, posing a challenge for neural machine translation (NMT). NMT models are often unable to translate idioms accurately and over-generate compositional, literal translations. In this work, we investigate whether the non-compositionality of idioms is reflected in the mechanics of... | Christopher G. Lucas, Ivan Titov, Verna Dankers |  |
| 760 |  |  [ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers](https://doi.org/10.18653/v1/2022.acl-long.253) |  | 0 | We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features:(1) long context documents with information that is related in logically complex ways;(2)... | Haitian Sun, Ruslan Salakhutdinov, William W. Cohen |  |
| 761 |  |  [Prompt-free and Efficient Few-shot Learning with Language Models](https://doi.org/10.18653/v1/2022.acl-long.254) |  | 0 | Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose Perfect, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting,... | James Henderson, Lambert Mathias, Luke Zettlemoyer, Majid Yazdani, Marzieh Saeidi, Rabeeh Karimi Mahabadi, Veselin Stoyanov |  |
| 762 |  |  [Continual Sequence Generation with Adaptive Compositional Modules](https://doi.org/10.18653/v1/2022.acl-long.255) |  | 0 | Continual learning is essential for real-world deployment when there is a need to quickly adapt the model to new tasks without forgetting knowledge of old tasks. Existing work on continual sequence generation either always reuses existing parameters to learn new tasks, which is vulnerable to catastrophic forgetting on dissimilar tasks, or blindly adds new... | Diyi Yang, Xuezhi Wang, Yanzhe Zhang |  |
| 763 |  |  [An Investigation of the (In)effectiveness of Counterfactually Augmented Data](https://doi.org/10.18653/v1/2022.acl-long.256) |  | 0 | While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD)—data generated by minimally perturbing examples to flip the ground-truth label—to... | He He, Nitish Joshi |  |
| 764 |  |  [Inducing Positive Perspectives with Text Reframing](https://doi.org/10.18653/v1/2022.acl-long.257) |  | 0 | Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the... | Anthony Zhang, Caleb Ziems, Diyi Yang, Minzhi Li |  |
| 765 |  |  [VALUE: Understanding Dialect Disparity in NLU](https://doi.org/10.18653/v1/2022.acl-long.258) |  | 0 | English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE. However, these benchmarks contain only textbook Standard American English (SAE). Other dialects have been largely overlooked in the NLP community. This leads to biased and inequitable NLU systems that serve... | Caleb Ziems, Camille Harris, Diyi Yang, Jessica Anderson, Jiaao Chen |  |
| 766 |  |  [From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer](https://doi.org/10.18653/v1/2022.acl-long.259) |  | 0 | We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible. We introduce a dataset for this task, ToxicSpans, which we release publicly. By experimenting with several methods, we show that sequence labeling models perform best, but methods that add generic rationale... | Alexandros Xenos, Ion Androutsopoulos, Jeffrey Sorensen, John Pavlopoulos, Léo Laugier |  |
| 767 |  |  [FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.260) |  | 0 | Sequence modeling has demonstrated state-of-the-art performance on natural language and document understanding tasks. However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms. First,... | ChenYu Lee, ChunLiang Li, Guolong Su, Joshua Ainslie, Nan Hua, Renshen Wang, Timothy Dozat, Tomas Pfister, Vincent Perot, Yasuhisa Fujii |  |
| 768 |  |  [The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.261) |  | 0 | Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be... | Alon Y. Halevy, Caleb Ziems, Diyi Yang, Jane A. Yu, YiChia Wang |  |
| 769 |  |  [Token Dropping for Efficient BERT Pretraining](https://doi.org/10.18653/v1/2022.acl-long.262) |  | 0 | Transformer-based models generally allocate the same amount of computation for each token in a given sequence. We develop a simple but effective “token dropping” method to accelerate the pretraining of transformer models, such as BERT, without degrading its performance on downstream tasks. In particular, we drop unimportant tokens starting from an intermediate... | Denny Zhou, Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Xiaodan Song, Xinying Song, Yuexin Wu |  |
| 770 |  |  [DialFact: A Benchmark for Fact-Checking in Dialogue](https://doi.org/10.18653/v1/2022.acl-long.263) |  | 0 | Fact-checking is an essential tool to mitigate the spread of misinformation and disinformation. We introduce the task of fact-checking in dialogue, which is a relatively unexplored area. We construct DialFact, a testing benchmark dataset of 22,245 annotated conversational claims, paired with pieces of evidence from Wikipedia. There are three sub-tasks in... | Caiming Xiong, ChienSheng Wu, Prakhar Gupta, Wenhao Liu |  |
| 771 |  |  [The Trade-offs of Domain Adaptation for Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.264) |  | 0 | This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large out-of-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain... | Dan Iter, David Grangier |  |
| 772 |  |  [Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go](https://doi.org/10.18653/v1/2022.acl-long.265) |  | 0 | Aligning with ACL 2022 special Theme on “Language Diversity: from Low Resource to Endangered Languages”, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To... | Ife Adebara, Muhammad AbdulMageed |  |
| 773 |  |  [Ensembling and Knowledge Distilling of Large Sequence Taggers for Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-long.266) |  | 0 | In this paper, we investigate improvements to the GEC sequence tagging architecture with a focus on ensembling of recent cutting-edge Transformer-based encoders in Large configurations. We encourage ensembling models by majority votes on span-level edits because this approach is tolerant to the model architecture and vocabulary size. Our best ensemble achieves... | Artem N. Chernodub, Kostiantyn Omelianchuk, Maksym Tarnavskyi |  |
| 774 |  |  [Speaker Information Can Guide Models to Better Inductive Biases: A Case Study On Predicting Code-Switching](https://doi.org/10.18653/v1/2022.acl-long.267) |  | 0 | Natural language processing (NLP) models trained on people-generated data can be unreliable because, without any constraints, they can learn from spurious correlations that are not relevant to the task. We hypothesize that enriching models with speaker information in a controlled, educated way can guide them to pick up on relevant inductive biases. For the... | Alissa Ostapenko, Melinda Fricke, Shuly Wintner, Yulia Tsvetkov |  |
| 775 |  |  [Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling](https://doi.org/10.18653/v1/2022.acl-long.268) |  | 0 | This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task. We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings—words from one language that are introduced into another without orthographic adaptation—and use it to evaluate how several... | Constantine Lignos, Elena Álvarez Mellado |  |
| 776 |  |  [Is Attention Explanation? An Introduction to the Debate](https://doi.org/10.18653/v1/2022.acl-long.269) |  | 0 | The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of... | Adrien Bibal, David Alfter, Patrick Watrin, Rodrigo Wilkens, Rémi Cardon, Thomas François, Xiaoou Wang |  |
| 777 |  |  [There Are a Thousand Hamlets in a Thousand People's Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory](https://doi.org/10.18653/v1/2022.acl-long.270) |  | 0 | Knowledge-grounded conversation (KGC) shows great potential in building an engaging and knowledgeable chatbot, and knowledge selection is a key ingredient in it. However, previous methods for knowledge selection only concentrate on the relevance between knowledge and dialogue context, ignoring the fact that age, hobby, education and life experience of an... | Chongyang Tao, JiRong Wen, Rui Yan, Tingchen Fu, Xueliang Zhao |  |
| 778 |  |  [Neural Pipeline for Zero-Shot Data-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-long.271) |  | 0 | In data-to-text (D2T) generation, training on in-domain data leads to overfitting to the data representation and repeating training data noise. We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs. Inspired by pipeline approaches, we propose to... | Ondrej Dusek, Zdenek Kasner |  |
| 779 |  |  [Not always about you: Prioritizing community needs when developing endangered language technology](https://doi.org/10.18653/v1/2022.acl-long.272) |  | 0 | Languages are classified as low-resource when they lack the quantity of data necessary for training statistical and machine learning tools and models. Causes of resource scarcity vary but can include poor access to technology for developing these resources, a relatively small population of speakers, or a lack of urgency for collecting such resources in... | Crystal Richardson, Emily Prud'hommeaux, Richard J. Hatcher, Zoey Liu |  |
| 780 |  |  [Automatic Identification and Classification of Bragging in Social Media](https://doi.org/10.18653/v1/2022.acl-long.273) |  | 0 | Bragging is a speech act employed with the goal of constructing a favorable self-image through positive statements about oneself. It is widespread in daily communication and especially popular in social media, where users aim to build a positive image of their persona directly or indirectly. In this paper, we present the first large scale study of bragging in... | A. Seza Dogruöz, Daniel PreotiucPietro, Mali Jin, Nikolaos Aletras |  |
| 781 |  |  [Automatic Error Analysis for Document-level Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.274) |  | 0 | Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported... | Aliva Das, Barry Wang, Claire Cardie, Jiayuan Gu, Kejian Shi, Thomas Porter, Xinya Du |  |
| 782 |  |  [Learning Functional Distributional Semantics with Visual Data](https://doi.org/10.18653/v1/2022.acl-long.275) |  | 0 | Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability. It models the meaning of a word as a binary classifier rather than a numerical vector. In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data. We train it... | Guy Emerson, Yinhong Liu |  |
| 783 |  |  [ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding](https://doi.org/10.18653/v1/2022.acl-long.276) |  | 0 | While large language models have shown exciting progress on several NLP benchmarks, evaluating their ability for complex analogical reasoning remains under-explored. Here, we introduce a high-quality crowdsourced dataset of narratives for employing proverbs in context as a benchmark for abstract language understanding. The dataset provides fine-grained... | Sayan Ghosh, Shashank Srivastava |  |
| 784 |  |  [Chart-to-Text: A Large-Scale Benchmark for Chart Summarization](https://doi.org/10.18653/v1/2022.acl-long.277) |  | 0 | Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide... | Ahmed Masry, Enamul Hoque, Megh Thakkar, Rixie Tiffany Ko Leong, Shafiq R. Joty, Shankar Kantharaj, Xiang Lin |  |
| 785 |  |  [Characterizing Idioms: Conventionality and Contingency](https://doi.org/10.18653/v1/2022.acl-long.278) |  | 0 | Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to... | Jackie Chi Kit Cheung, Michael Wagner, Michaela Socolof, Timothy J. O'Donnell |  |
| 786 |  |  [Bag-of-Words vs. Graph vs. Sequence in Text Classification: Questioning the Necessity of Text-Graphs and the Surprising Strength of a Wide MLP](https://doi.org/10.18653/v1/2022.acl-long.279) |  | 0 | Graph neural networks have triggered a resurgence of graph-based text classification methods, defining today’s state of the art. We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and HeteGCN in an inductive text classification setting and is comparable with HyperGAT. Moreover, we... | Ansgar Scherp, Lukas Galke |  |
| 787 |  |  [Generative Pretraining for Paraphrase Evaluation](https://doi.org/10.18653/v1/2022.acl-long.280) |  | 0 | We introduce ParaBLEU, a paraphrase representation learning model and evaluation metric for text generation. Unlike previous approaches, ParaBLEU learns to understand paraphrasis using generative conditioning as a pretraining objective. ParaBLEU correlates more strongly with human judgements than existing metrics, obtaining new state-of-the-art results on the... | Emil Fristed, Jack Weston, Raphael Lenain, Udeepa Meepegama |  |
| 788 |  |  [Incorporating Stock Market Signals for Twitter Stance Detection](https://doi.org/10.18653/v1/2022.acl-long.281) |  | 0 | Research in stance detection has so far focused on models which leverage purely textual input. In this paper, we investigate the integration of textual and financial signals for stance detection in the financial domain. Specifically, we propose a robust multi-task neural architecture that combines textual input with high-frequency intra-day time series from... | Chryssi Giannitsarou, Costanza Conforti, Flavio Toxvaerd, Jakob Berndt, Mohammad Taher Pilehvar, Nigel Collier |  |
| 789 |  |  [Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.282) |  | 0 | Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. The dominant inductive bias applied to these models is a shared vocabulary and a shared set of parameters across languages; the inputs and labels corresponding to examples drawn from different language pairs might still... | Ankur Bapna, Orhan Firat, Pidong Wang, Wolfgang Macherey, Yong Cheng, Yuan Cao |  |
| 790 |  |  [Word Segmentation as Unsupervised Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.283) |  | 0 | Word identification from continuous input is typically viewed as a segmentation task. Experiments with human adults suggest that familiarity with syntactic structures in their native language also influences word identification in artificial languages; however, the relation between syntactic processing and word identification is yet unclear. This work takes one... | Raquel G. Alhama |  |
| 791 |  |  [SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems](https://doi.org/10.18653/v1/2022.acl-long.284) |  | 0 | The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically... | A. Stevie Bergman, Dirk Hovy, Emily Dinan, Gavin Abercrombie, Shannon L. Spruit, Verena Rieser, YLan Boureau |  |
| 792 |  |  [Zero-Shot Cross-lingual Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.285) |  | 0 | Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e.,... | Mirella Lapata, Tom Sherborne |  |
| 793 |  |  [The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study](https://doi.org/10.18653/v1/2022.acl-long.286) |  | 0 | Obtaining human-like performance in NLP is often argued to require compositional generalisation. Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data. However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial... | Dieuwke Hupkes, Elia Bruni, Verna Dankers |  |
| 794 |  |  [Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents](https://doi.org/10.18653/v1/2022.acl-long.287) |  | 0 | Document-level neural machine translation (DocNMT) achieves coherent translations by incorporating cross-sentence context. However, for most language pairs there’s a shortage of parallel documents, although parallel sentences are readily available. In this paper, we study whether and how contextual modeling in DocNMT is transferable via multilingual modeling.... | Ali Dabirmoghaddam, Ankur Bapna, Biao Zhang, Melvin Johnson, Naveen Arivazhagan, Orhan Firat |  |
| 795 |  |  [Cross-Lingual Phrase Retrieval](https://doi.org/10.18653/v1/2022.acl-long.288) |  | 0 | Cross-lingual retrieval aims to retrieve relevant text across languages. Current methods typically achieve cross-lingual retrieval by learning language-agnostic text representations in word or sentence level. However, how to learn phrase representations for cross-lingual phrase retrieval is still an open problem. In this paper, we propose , a cross-lingual... | Heqi Zheng, Heyan Huang, Tian Lan, Wei Wei, XianLing Mao, Xiao Zhang, Yan Tan, Zewen Chi |  |
| 796 |  |  [Improving Compositional Generalization with Self-Training for Data-to-Text Generation](https://doi.org/10.18653/v1/2022.acl-long.289) |  | 0 | Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations (MRs). Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata, thereby necessitating few-shot generalization to novel MRs. In this work, we systematically study the... | Ankur Parikh, Emma Strubell, Jinfeng Rao, Mihir Kale, Sanket Vaibhav Mehta, Yi Tay |  |
| 797 |  |  [MMCoQA: Conversational Question Answering over Text, Tables, and Images](https://doi.org/10.18653/v1/2022.acl-long.290) |  | 0 | The rapid development of conversational assistants accelerates the study on conversational question answering (QA). However, the existing conversational QA systems usually answer users’ questions with a single knowledge source, e.g., paragraphs or a knowledge graph, but overlook the important visual cues, let alone multiple knowledge sources of different... | Liqiang Nie, Wenjie Li, Yongqi Li |  |
| 798 |  |  [Effective Token Graph Modeling using a Novel Labeling Strategy for Structured Sentiment Analysis](https://doi.org/10.18653/v1/2022.acl-long.291) |  | 0 | The state-of-the-art model for structured sentiment analysis casts the task as a dependency parsing problem, which has some limitations: (1) The label proportions for span prediction and span relation prediction are imbalanced. (2) The span lengths of sentiment tuple components may be very large in this task, which will further exacerbates the imbalance... | Donghong Ji, Fei Li, Hao Fei, Jingye Li, Wenxuan Shi |  |
| 799 |  |  [PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks](https://doi.org/10.18653/v1/2022.acl-long.292) |  | 0 | This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and... | Can Xu, Chongyang Tao, Daxin Jiang, Huang Hu, Qingfeng Sun, Xiubo Geng, Yufei Wang |  |
| 800 |  |  [Disentangled Sequence to Sequence Learning for Compositional Generalization](https://doi.org/10.18653/v1/2022.acl-long.293) |  | 0 | There is mounting evidence that existing neural network models, in particular the very popular sequence-to-sequence architecture, struggle to systematically generalize to unseen compositions of seen components. We demonstrate that one of the reasons hindering compositional generalization relates to representations being entangled. We propose an extension to... | Hao Zheng, Mirella Lapata |  |
| 801 |  |  [RST Discourse Parsing with Second-Stage EDU-Level Pre-training](https://doi.org/10.18653/v1/2022.acl-long.294) |  | 0 | Pre-trained language models (PLMs) have shown great potentials in natural language processing (NLP) including rhetorical structure theory (RST) discourse parsing. Current PLMs are obtained by sentence-level pre-training, which is different from the basic processing unit, i.e. element discourse unit (EDU).To this end, we propose a second-stage EDU-level... | Guohong Fu, Meishan Zhang, Min Zhang, Nan Yu |  |
| 802 |  |  [SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.295) |  | 0 | Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like... | Jingming Liu, Liang Wang, Wei Zhao, Zhuoyu Wei |  |
| 803 |  |  [Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?](https://doi.org/10.18653/v1/2022.acl-long.296) |  | 0 | Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific... | Anders Søgaard, Jonas Pilot, Oliver Eberle, Stephanie Brandl |  |
| 804 |  |  [LexGLUE: A Benchmark Dataset for Legal Language Understanding in English](https://doi.org/10.18653/v1/2022.acl-long.297) |  | 0 | Laws and their interpretations, legal arguments and agreements are typically expressed in writing, leading to the production of vast corpora of legal text. Their analysis, which is at the center of legal practice, becomes increasingly elaborate as these collections grow in size. Natural language understanding (NLU) technologies can be a valuable tool to support... | Abhik Jana, Daniel Martin Katz, Dirk Hartung, Ilias Chalkidis, Ion Androutsopoulos, Michael J. Bommarito II, Nikolaos Aletras |  |
| 805 |  |  [DiBiMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.298) |  | 0 | Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation. Over the last few decades, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words. Within this body of research, some studies have posited that models pick up semantic biases existing in the training data,... | Federico Martelli, Francesco Saina, Niccolò Campolungo, Roberto Navigli |  |
| 806 |  |  [Improving Word Translation via Two-Stage Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.299) |  | 0 | Word translation or bilingual lexicon induction (BLI) is a key cross-lingual task, aiming to bridge the lexical gap between different languages. In this work, we propose a robust and effective two-stage contrastive learning framework for the BLI task. At Stage C1, we propose to refine standard cross-lingual linear maps between static word embeddings (WEs) via a... | Anna Korhonen, Fangyu Liu, Ivan Vulic, Nigel Collier, Yaoyiran Li |  |
| 807 |  |  [Scheduled Multi-task Learning for Neural Chat Translation](https://doi.org/10.18653/v1/2022.acl-long.300) |  | 0 | Neural Chat Translation (NCT) aims to translate conversational text into different languages. Existing methods mainly focus on modeling the bilingual dialogue characteristics (e.g., coherence) to improve chat translation via multi-task learning on small-scale chat translation data. Although the NCT models have achieved impressive success, it is still far from... | Fandong Meng, Jie Zhou, Jinan Xu, Yufeng Chen, Yunlong Liang |  |
| 808 |  |  [FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing](https://doi.org/10.18653/v1/2022.acl-long.301) |  | 0 | We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (European Council, USA, Switzerland, and China), five languages (English, German, French, Italian and Chinese) and fairness across five attributes... | Anders Søgaard, Ilias Chalkidis, Letizia Tomada, Sebastian Felix Schwemer, Sheng Zhang, Tommaso Pasini |  |
| 809 |  |  [Towards Abstractive Grounded Summarization of Podcast Transcripts](https://doi.org/10.18653/v1/2022.acl-long.302) |  | 0 | Podcasts have shown a recent rise in popularity. Summarization of podcasts is of practical benefit to both content providers and consumers. It helps people quickly decide whether they will listen to a podcast and/or reduces the cognitive load of content providers to write summaries. Nevertheless, podcast summarization faces significant challenges including... | Chen Li, Dong Yu, Fei Liu, Kaiqiang Song, Xiaoyang Wang |  |
| 810 |  |  [FiNER: Financial Numeric Entity Recognition for XBRL Tagging](https://doi.org/10.18653/v1/2022.acl-long.303) |  | 0 | Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike... | Eirini Spyropoulou, Georgios Paliouras, Ilias Chalkidis, Ion Androutsopoulos, Lefteris Loukas, Manos Fergadiotis, Prodromos Malakasiotis |  |
| 811 |  |  [Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation](https://doi.org/10.18653/v1/2022.acl-long.304) |  | 0 | Contrastive learning has achieved impressive success in generation tasks to militate the “exposure bias” problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the... | Dongyan Zhao, Feng Wang, Jinxiong Chang, Mingzhe Li, Qishen Zhang, Rui Yan, Taifeng Wang, Wei Chu, Xiexiong Lin, Xiuying Chen, Zhongyi Liu |  |
| 812 |  |  [EPT-X: An Expression-Pointer Transformer model that generates eXplanations for numbers](https://doi.org/10.18653/v1/2022.acl-long.305) |  | 0 | In this paper, we propose a neural model EPT-X (Expression-Pointer Transformer with Explanations), which utilizes natural language explanations to solve an algebraic word problem. To enhance the explainability of the encoding process of a neural model, EPT-X adopts the concepts of plausibility and faithfulness which are drawn from math word problem solving... | Bugeun Kim, Gahgene Gweon, Kyung Seo Ki, Sangkyu Rhim |  |
| 813 |  |  [Identifying the Human Values behind Arguments](https://doi.org/10.18653/v1/2022.acl-long.306) |  | 0 | This paper studies the (often implicit) human values behind natural language arguments, such as to have freedom of thought or to be broadminded. Values are commonly accepted answers to why some option is desirable in the ethical sense and are thus essential both in real-world argumentation and theoretical argumentation frameworks. However, their large variety... | Benno Stein, Henning Wachsmuth, Johannes Kiesel, Milad Alshomary, Nicolas Handke, Xiaoni Cai |  |
| 814 |  |  [BenchIE: A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation](https://doi.org/10.18653/v1/2022.acl-long.307) |  | 0 | Intrinsic evaluations of OIE systems are carried out either manually—with human evaluators judging the correctness of extractions—or automatically, on standardized benchmarks. The latter, while much more cost-effective, is less reliable, primarily because of the incompleteness of the existing OIE benchmarks: the ground truth extractions do not include all... | Bhushan Kotnis, Carolin Lawrence, Goran Glavas, Kiril Gashteovski, Mathias Niepert, Mingying Yu |  |
| 815 |  |  [Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition](https://doi.org/10.18653/v1/2022.acl-long.308) |  | 0 | Training Transformer-based models demands a large amount of data, while obtaining aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled unimodal data. On the other side, although the effectiveness of large-scale self-supervised learning... | Helong Zhou, Peiyu Chen, Xichen Pan, Xinbing Wang, Yichen Gong, Zhouhan Lin |  |
| 816 |  |  [SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization](https://doi.org/10.18653/v1/2022.acl-long.309) |  | 0 | Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is... | Mathieu Ravaut, Nancy F. Chen, Shafiq R. Joty |  |
| 817 |  |  [Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals](https://doi.org/10.18653/v1/2022.acl-long.310) |  | 0 | The ability to sequence unordered events is evidence of comprehension and reasoning about real world tasks/procedures. It is essential for applications such as task planning and multi-source instruction summarization. It often requires thorough understanding of temporal common sense and multimodal information, since these procedures are often conveyed by a... | Alexander Spangher, Marjorie Freedman, Nanyun Peng, Pegah Alipoormolabashi, Ralph M. Weischedel, TeLin Wu |  |
| 818 |  |  [Zoom Out and Observe: News Environment Perception for Fake News Detection](https://doi.org/10.18653/v1/2022.acl-long.311) |  | 0 | Fake news detection is crucial for preventing the dissemination of misinformation on social media. To differentiate fake news from real ones, existing methods observe the language patterns of the news post and “zoom in” to verify its content with knowledge sources or check its readers’ replies. However, these methods neglect the information in the external news... | Danding Wang, Juan Cao, Qiang Sheng, Rundong Li, Xueyao Zhang, Yongchun Zhu |  |
| 819 |  |  [Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models](https://doi.org/10.18653/v1/2022.acl-long.312) |  | 0 | Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training... | Laurent Besacier, Lorenzo Lupo, Marco Dinarelli |  |
| 820 |  |  [Saliency as Evidence: Event Detection with Trigger Saliency Attribution](https://doi.org/10.18653/v1/2022.acl-long.313) |  | 0 | Event detection (ED) is a critical subtask of event extraction that seeks to identify event triggers of certain types in texts. Despite significant advances in ED, existing methods typically follow a “one model fits all types” approach, which sees no differences between event types and often results in a quite skewed performance. Finding the causes of skewed... | Jian Liu, Jinan Xu, Yufeng Chen |  |
| 821 |  |  [SRL4E - Semantic Role Labeling for Emotions: A Unified Evaluation Framework](https://doi.org/10.18653/v1/2022.acl-long.314) |  | 0 | In the field of sentiment analysis, several studies have highlighted that a single sentence may express multiple, sometimes contrasting, sentiments and emotions, each with its own experiencer, target and/or cause. To this end, over the past few years researchers have started to collect and annotate data manually, in order to investigate the capabilities of... | Cesare Campagnano, Roberto Navigli, Simone Conia |  |
| 822 |  |  [Context Matters: A Pragmatic Study of PLMs' Negation Understanding](https://doi.org/10.18653/v1/2022.acl-long.315) |  | 0 | In linguistics, there are two main perspectives on negation: a semantic and a pragmatic view. So far, research in NLP on negation has almost exclusively adhered to the semantic view. In this article, we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs. Our results differ from previous, semantics-based... | Reto Gubelmann, Siegfried Handschuh |  |
| 823 |  |  [Probing for Predicate Argument Structures in Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.316) |  | 0 | Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of... | Roberto Navigli, Simone Conia |  |
| 824 |  |  [Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.317) |  | 0 | We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument... | IHung Hsu, KaiWei Chang, KuanHao Huang, Nanyun Peng, Prem Natarajan |  |
| 825 |  |  [Identifying Moments of Change from Longitudinal User Text](https://doi.org/10.18653/v1/2022.acl-long.318) |  | 0 | Identifying changes in individuals’ behaviour and mood, as observed via content shared on online platforms, is increasingly gaining importance. Most research to-date on this topic focuses on either: (a) identifying individuals at risk or with a certain mental health condition given a batch of posts or (b) providing equivalent labels at the post level. A... | Adam Tsakalidis, Anthony Hills, Federico Nanni, Jenny Chim, Jiayu Song, Maria Liakata |  |
| 826 |  |  [Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System](https://doi.org/10.18653/v1/2022.acl-long.319) |  | 0 | Pre-trained language models have been recently shown to benefit task-oriented dialogue (TOD) systems. Despite their success, existing methods often formulate this task as a cascaded generation problem which can lead to error accumulation across different sub-tasks and greater data annotation overhead. In this study, we present PPTOD, a unified plug-and-play... | Arshit Gupta, Deng Cai, Elman Mansimov, Lei Shu, Yi Zhang, YiAn Lai, Yixuan Su |  |
| 827 |  |  [Graph Enhanced Contrastive Learning for Radiology Findings Summarization](https://doi.org/10.18653/v1/2022.acl-long.320) |  | 0 | The impression section of a radiology report summarizes the most prominent observation from the findings section and is the most important section for radiologists to communicate to physicians. Summarizing findings is time-consuming and can be prone to error for inexperienced radiologists, and thus automatic impression generation has attracted substantial... | Jinpeng Hu, TsungHui Chang, Xiang Wan, Zhen Li, Zhihong Chen, Zhuo Li |  |
| 828 |  |  [Semi-Supervised Formality Style Transfer with Consistency Training](https://doi.org/10.18653/v1/2022.acl-long.321) |  | 0 | Formality style transfer (FST) is a task that involves paraphrasing an informal sentence into a formal one without altering its meaning. To address the data-scarcity problem of existing parallel datasets, previous studies tend to adopt a cycle-reconstruction scheme to utilize additional unlabeled data, where the FST model mainly benefits from target-side... | An Wang, Ao Liu, Naoaki Okazaki |  |
| 829 |  |  [Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure](https://doi.org/10.18653/v1/2022.acl-long.322) |  | 0 | Multilingual pre-trained language models, such as mBERT and XLM-R, have shown impressive cross-lingual ability. Surprisingly, both of them use multilingual masked language model (MLM) without any cross-lingual supervision or aligned data. Despite the encouraging results, we still lack a clear understanding of why cross-lingual ability could emerge from... | Nan Duan, Yaobo Liang, Yuan Chai |  |
| 830 |  |  [Rare and Zero-shot Word Sense Disambiguation using Z-Reweighting](https://doi.org/10.18653/v1/2022.acl-long.323) |  | 0 | Word sense disambiguation (WSD) is a crucial problem in the natural language processing (NLP) community. Current methods achieve decent performance by utilizing supervised learning and large pre-trained language models. However, the imbalanced training dataset leads to poor performance on rare senses and zero-shot senses. There are more training instances and... | Hongming Zhang, Tong Zhang, Yangqiu Song, Ying Su |  |
| 831 |  |  [Nibbling at the Hard Core of Word Sense Disambiguation](https://doi.org/10.18653/v1/2022.acl-long.324) |  | 0 | With state-of-the-art systems having finally attained estimated human performance, Word Sense Disambiguation (WSD) has now joined the array of Natural Language Processing tasks that have seemingly been solved, thanks to the vast amounts of knowledge encoded into Transformer-based pre-trained language models. And yet, if we look below the surface of raw figures,... | Marco Maru, Michele Bevilacqua, Roberto Navigli, Simone Conia |  |
| 832 |  |  [Large Scale Substitution-based Word Sense Induction](https://doi.org/10.18653/v1/2022.acl-long.325) |  | 0 | We present a word-sense induction method based on pre-trained masked language models (MLMs), which can cheaply scale to large vocabularies and large corpora. The result is a corpus which is sense-tagged according to a corpus-derived sense inventory and where each sense is associated with indicative words. Evaluation on English Wikipedia that was sense-tagged... | Hillel TaubTabib, Matan Eyal, Shoval Sadde, Yoav Goldberg |  |
| 833 |  |  [Can Synthetic Translations Improve Bitext Quality?](https://doi.org/10.18653/v1/2022.acl-long.326) |  | 0 | Synthetic translations have been used for a wide range of NLP tasks primarily as a means of data augmentation. This work explores, instead, how synthetic translations can be used to revise potentially imperfect reference translations in mined bitext. We find that synthetic samples can improve bitext quality without any additional bilingual supervision when they... | Eleftheria Briakou, Marine Carpuat |  |
| 834 |  |  [Unsupervised Dependency Graph Network](https://doi.org/10.18653/v1/2022.acl-long.327) |  | 0 | Recent work has identified properties of pretrained self-attention models that mirror those of dependency parse structures. In particular, some self-attention heads correspond well to individual dependency types. Inspired by these developments, we propose a new competitive mechanism that encourages these attention heads to model different dependency relations.... | Aaron C. Courville, Alessandro Sordoni, Jie Zhou, Peng Li, Shawn Tan, Yikang Shen |  |
| 835 |  |  [WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types](https://doi.org/10.18653/v1/2022.acl-long.328) |  | 0 | Multimodal Entity Linking (MEL) which aims at linking mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia), is an essential task for many multimodal applications. Although much attention has been paid to MEL, the shortcomings of existing MEL datasets including limited contextual topics and entity types, simplified... | Junfeng Tian, Lihan Chen, Min Gui, Ming Yan, Rui Wang, Xuwu Wang, Yanghua Xiao, Zhixu Li |  |
| 836 |  |  [Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.329) |  | 0 | Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as the biomedical domain are vastly under-explored. To facilitate this, we release a well-curated biomedical knowledge probing... | Charlotte Collins, Ehsan Shareghi, Fangyu Liu, Nigel Collier, Yixuan Su, Zaiqiao Meng |  |
| 837 |  |  [Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT](https://doi.org/10.18653/v1/2022.acl-long.330) |  | 0 | Transformer-based pre-trained models, such as BERT, have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, deploying these models can be prohibitively costly, as the standard self-attention mechanism of the Transformer suffers from quadratic computational cost in the input sequence... | Jing Zhao, Junwei Bao, Xiaodong He, Yifan Wang, Youzheng Wu |  |
| 838 |  |  [Compression of Generative Pre-trained Language Models via Quantization](https://doi.org/10.18653/v1/2022.acl-long.331) |  | 0 | The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that... | Chaofan Tao, Lifeng Shang, Lu Hou, Ngai Wong, Ping Luo, Qun Liu, Wei Zhang, Xin Jiang |  |
| 839 |  |  [Visual-Language Navigation Pretraining via Prompt-based Environmental Self-exploration](https://doi.org/10.18653/v1/2022.acl-long.332) |  | 0 | Vision-language navigation (VLN) is a challenging task due to its large searching space in the environment. To address this problem, previous works have proposed some methods of fine-tuning a large model that pretrained on large-scale datasets. However, the conventional fine-tuning methods require extra human-labeled navigation data and lack self-exploration... | Fengda Zhu, Hang Xu, Lingling Li, Xiaodan Liang, Xiwen Liang |  |
| 840 |  |  [DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation](https://doi.org/10.18653/v1/2022.acl-long.333) |  | 0 | Dialog response generation in open domain is an important research topic where the main challenge is to generate relevant and diverse responses. In this paper, we propose a new dialog pre-training framework called DialogVED, which introduces continuous latent variables into the enhanced encoder-decoder pre-training framework to increase the relevance and... | Bartuer Zhou, Biao Cheng, Bolun Yao, Nan Duan, Song Wang, Wei Chen, Weizhen Qi, Weizhu Chen, Xiaowu Hu, Yeyun Gong, Yi Mao, Zhongyu Wei |  |
| 841 |  |  [Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations](https://doi.org/10.18653/v1/2022.acl-long.334) |  | 0 | We study the problem of coarse-grained response selection in retrieval-based dialogue systems. The problem is equally important with fine-grained response selection, but is less explored in existing literature. In this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained response selection in open-domain conversations. In our... | Bartuer Zhou, Biao Cheng, Bolun Yao, Can Xu, Daxin Jiang, Huang Hu, Nan Duan, Wei Chen, Xiaowu Hu, Yeyun Gong, Zhihao Fan, Zhongyu Wei |  |
| 842 |  |  [Textomics: A Dataset for Genomics Data Summary Generation](https://doi.org/10.18653/v1/2022.acl-long.335) |  | 0 | Summarizing biomedical discovery from genomics data using natural languages is an essential step in biomedical research but is mostly done manually. Here, we introduce Textomics, a novel dataset of genomics data description, which contains 22,273 pairs of genomics data matrices and their summaries. Each summary is written by the researchers who generated the... | MuChun Wang, Sheng Wang, Zixuan Liu |  |
| 843 |  |  [A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space](https://doi.org/10.18653/v1/2022.acl-long.336) |  | 0 | Learning high-quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks. Though the BERT-like pre-trained language models have achieved great success, using their sentence representations directly often results in poor performance on the semantic textual similarity task.... | Binqiang Zhao, Hongji Zhu, Nan Xu, Xiaobo Li, Yongliang Wang, Yuhao Zhang |  |
| 844 |  |  [Packed Levitated Marker for Entity and Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.337) |  | 0 | Recent entity and relation extraction works focus on investigating how to obtain a better span representation from the pre-trained encoder. However, a major limitation of existing works is that they ignore the interrelation between spans (pairs). In this work, we propose a novel span representation approach, named Packed Levitated Markers (PL-Marker), to... | Deming Ye, Maosong Sun, Peng Li, Yankai Lin |  |
| 845 |  |  [An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.338) |  | 0 | We study the interpretability issue of task-oriented dialogue systems in this paper. Previously, most neural-based task-oriented dialogue systems employ an implicit reasoning strategy that makes the model predictions uninterpretable to humans. To obtain a transparent reasoning process, we introduce neuro-symbolic to perform explicit reasoning that justifies... | Jey Han Lau, Rui Zhang, Sarah M. Erfani, Shiquan Yang |  |
| 846 |  |  [Impact of Evaluation Methodologies on Code Summarization](https://doi.org/10.18653/v1/2022.acl-long.339) |  | 0 | There has been a growing interest in developing machine learning (ML) models for code summarization tasks, e.g., comment generation and method naming. Despite substantial increase in the effectiveness of ML models, the evaluation methodologies, i.e., the way people split datasets into training, validation, and test sets, were not well studied. Specifically, no... | Jiyang Zhang, Junyi Jessy Li, Milos Gligoric, Pengyu Nie, Raymond J. Mooney |  |
| 847 |  |  [KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering](https://doi.org/10.18653/v1/2022.acl-long.340) |  | 0 | Current Open-Domain Question Answering (ODQA) models typically include a retrieving module and a reading module, where the retriever selects potentially relevant passages from open-source documents for a given question, and the reader produces an answer based on the retrieved passages. The recently proposed Fusion-in-Decoder (FiD) framework is a representative... | Chenguang Zhu, Donghan Yu, Michael Zeng, Shuohang Wang, Wenhao Yu, Xiang Ren, Yichong Xu, Yiming Yang, Yuwei Fang |  |
| 848 |  |  [Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media](https://doi.org/10.18653/v1/2022.acl-long.341) |  | 0 | Social media is a breeding ground for threat narratives and related conspiracy theories. In these, an outside group threatens the integrity of an inside group, leading to the emergence of sharply defined group identities: Insiders – agents with whom the authors identify and Outsiders – agents who threaten the insiders. Inferring the members of these groups... | Pavan Holur, Shadi Shahsavari, Tianyi Wang, Timothy R. Tangherlini, Vwani P. Roychowdhury |  |
| 849 |  |  [Learning From Failure: Data Capture in an Australian Aboriginal Community](https://doi.org/10.18653/v1/2022.acl-long.342) |  | 0 | Most low resource language technology development is premised on the need to collect data for training statistical models. When we follow the typical process of recording and transcribing text for small Indigenous languages, we hit up against the so-called “transcription bottleneck.” Therefore it is worth exploring new ways of engaging with speakers which... | Laurent Besacier, Steven Bird, Éric Le Ferrand |  |
| 850 |  |  [Deep Inductive Logic Reasoning for Multi-Hop Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.343) |  | 0 | Multi-hop reading comprehension requires an ability to reason across multiple documents. On the one hand, deep learning approaches only implicitly encode query-related information into distributed embeddings which fail to uncover the discrete relational reasoning process to infer the correct answer. On the other hand, logic-based approaches provide... | Sinno Jialin Pan, Wenya Wang |  |
| 851 |  |  [CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues](https://doi.org/10.18653/v1/2022.acl-long.344) |  | 0 | This paper addresses the problem of dialogue reasoning with contextualized commonsense inference. We curate CICERO, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences: cause, subsequent event, prerequisite, motivation, and emotional reaction. The dataset contains 53,105 of such inferences from 5,672 dialogues. We use... | Deepanway Ghosal, Navonil Majumder, Rada Mihalcea, Siqi Shen, Soujanya Poria |  |
| 852 |  |  [A Comparative Study of Faithfulness Metrics for Model Interpretability Methods](https://doi.org/10.18653/v1/2022.acl-long.345) |  | 0 | Interpretable methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years. To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms, various faithfulness evaluation metrics have been proposed. However, we find that different... | Chun Sik Chan, Guanqing Liang, Huanqi Kong |  |
| 853 |  |  [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer](https://doi.org/10.18653/v1/2022.acl-long.346) |  | 0 | There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called... | Brian Lester, Daniel Cer, Noah Constant, Rami AlRfou', Tu Vu |  |
| 854 |  |  [Pass off Fish Eyes for Pearls: Attacking Model Selection of Pre-trained Models](https://doi.org/10.18653/v1/2022.acl-long.347) |  | 0 | Selecting an appropriate pre-trained model (PTM) for a specific downstream task typically requires significant efforts of fine-tuning. To accelerate this process, researchers propose feature-based model selection (FMS) methods, which assess PTMs’ transferability to a specific task in a fast way without fine-tuning. In this work, we argue that current FMS... | Biru Zhu, Fanchao Qi, Maosong Sun, Ming Gu, Yangdong Deng, Yujia Qin, Zhiyuan Liu |  |
| 855 |  |  [Educational Question Generation of Children Storybooks via Question Type Distribution Learning and Event-centric Summarization](https://doi.org/10.18653/v1/2022.acl-long.348) |  | 0 | Generating educational questions of fairytales or storybooks is vital for improving children’s literacy ability. However, it is challenging to generate questions that capture the interesting aspects of a fairytale story with educational meaningfulness. In this paper, we propose a novel question generation method that first learns the question type distribution... | Chengzhong Liu, Dakuo Wang, Mo Yu, Xiaojuan Ma, Yufang Hou, Zhenjie Zhao |  |
| 856 |  |  [HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations](https://doi.org/10.18653/v1/2022.acl-long.349) |  | 0 | Recently, various response generation models for two-party conversations have achieved impressive improvements, but less effort has been paid to multi-party conversations (MPCs) which are more practical and complicated. Compared with a two-party conversation where a dialogue context is a sequence of utterances, building a response generation model for MPCs is... | ChaoHong Tan, Chongyang Tao, Daxin Jiang, Huang Hu, JiaChen Gu, Xiubo Geng, ZhenHua Ling |  |
| 857 |  |  [The patient is more dead than alive: exploring the current state of the multi-document summarisation of the biomedical literature](https://doi.org/10.18653/v1/2022.acl-long.350) |  | 0 | Although multi-document summarisation (MDS) of the biomedical literature is a highly valuable task that has recently attracted substantial interest, evaluation of the quality of biomedical summaries lacks consistency and transparency. In this paper, we examine the summaries generated by two current models in order to understand the deficiencies of existing... | Jey Han Lau, Karin Verspoor, Timothy Baldwin, Yulia Otmakhova |  |
| 858 |  |  [A Multi-Document Coverage Reward for RELAXed Multi-Document Summarization](https://doi.org/10.18653/v1/2022.acl-long.351) |  | 0 | Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks,... | Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi |  |
| 859 |  |  [KNN-Contrastive Learning for Out-of-Domain Intent Classification](https://doi.org/10.18653/v1/2022.acl-long.352) |  | 0 | The Out-of-Domain (OOD) intent classification is a basic and challenging task for dialogue systems. Previous methods commonly restrict the region (in feature space) of In-domain (IND) intent features to be compact or simply-connected implicitly, which assumes no OOD intents reside, to learn discriminative semantic features. Then the distribution of the IND... | Peiju Liu, Xipeng Qiu, Yunhua Zhou |  |
| 860 |  |  [A Neural Network Architecture for Program Understanding Inspired by Human Behaviors](https://doi.org/10.18653/v1/2022.acl-long.353) |  | 0 | Program understanding is a fundamental task in program language processing. Despite the success, existing works fail to take human behaviors as reference in understanding programs. In this paper, we consider human behaviors and propose the PGNN-EK model that consists of two main components. On the one hand, inspired by the “divide-and-conquer” reading behaviors... | Lei Yuan, Ming Gao, Renyu Zhu, Wenyuan Cai, Xiang Li |  |
| 861 |  |  [FaVIQ: FAct Verification from Information-seeking Questions](https://doi.org/10.18653/v1/2022.acl-long.354) |  | 0 | Despite significant interest in developing general purpose fact checking models, it is challenging to construct a large-scale fact verification dataset with realistic real-world claims. Existing claims are either authored by crowdworkers, thereby introducing subtle biases thatare difficult to control for, or manually verified by professional fact checkers,... | Hannaneh Hajishirzi, Jaewoo Kang, Jungsoo Park, Luke Zettlemoyer, Sewon Min |  |
| 862 |  |  [Simulating Bandit Learning from User Feedback for Extractive Question Answering](https://doi.org/10.18653/v1/2022.acl-long.355) |  | 0 | We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given... | Eunsol Choi, Ge Gao, Yoav Artzi |  |
| 863 |  |  [Beyond Goldfish Memory: Long-Term Open-Domain Conversation](https://doi.org/10.18653/v1/2022.acl-long.356) |  | 0 | Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about... | Arthur Szlam, Jason Weston, Jing Xu |  |
| 864 |  |  [ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension](https://doi.org/10.18653/v1/2022.acl-long.357) |  | 0 | Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more... | Anna Rohrbach, Matt Gardner, Sameer Singh, Sanjay Subramanian, Trevor Darrell, William Merrill |  |
| 865 |  |  [Dynamic Prefix-Tuning for Generative Template-based Event Extraction](https://doi.org/10.18653/v1/2022.acl-long.358) |  | 0 | We consider event extraction in a generative manner with template-based conditional generation. Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information. In this... | Bo Wang, Ge Shi, Heyan Huang, Xiao Liu |  |
| 866 |  |  [E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models](https://doi.org/10.18653/v1/2022.acl-long.359) |  | 0 | Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of... | Amin BanitalebiDehkordi, Mohammad Akbari, Yong Zhang |  |
| 867 |  |  [PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization](https://doi.org/10.18653/v1/2022.acl-long.360) |  | 0 | We introduce PRIMERA, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. PRIMERA uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses... | Arman Cohan, Giuseppe Carenini, Iz Beltagy, Wen Xiao |  |
| 868 |  |  [Dynamic Global Memory for Document-level Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.361) |  | 0 | Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted... | Heng Ji, Sha Li, Xinya Du |  |
| 869 |  |  [Measuring the Impact of (Psycho-)Linguistic and Readability Features and Their Spill Over Effects on the Prediction of Eye Movement Patterns](https://doi.org/10.18653/v1/2022.acl-long.362) |  | 0 | There is a growing interest in the combined use of NLP and machine learning methods to predict gaze patterns during naturalistic reading. While promising results have been obtained through the use of transformer-based language models, little work has been undertaken to relate the performance of such models to general text characteristics. In this paper we... | Daniel Wiechmann, Elma Kerz |  |
| 870 |  |  [Alternative Input Signals Ease Transfer in Multilingual Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.363) |  | 0 | Recent work in multilingual machine translation (MMT) has focused on the potential of positive transfer between languages, particularly cases where higher-resourced languages can benefit lower-resourced ones. While training an MMT model, the supervision signals learned from one language pair can be transferred to the other via the tokens shared by multiple... | Angela Fan, Chau Tran, Francisco Guzmán, James Cross, Philipp Koehn, Simeng Sun, Vishrav Chaudhary |  |
| 871 |  |  [Phone-ing it in: Towards Flexible Multi-Modal Language Model Training by Phonetic Representations of Data](https://doi.org/10.18653/v1/2022.acl-long.364) |  | 0 | Multi-modal techniques offer significant untapped potential to unlock improved NLP technology for local languages. However, many advances in language model pre-training are focused on text, a fact that only increases systematic inequalities in the performance of NLP tasks across the world’s languages. In this work, we propose a multi-modal approach to train... | Colin Leong, Daniel Whitenack |  |
| 872 |  |  [Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.365) |  | 0 | We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for... | Hannaneh Hajishirzi, Luke Zettlemoyer, Mike Lewis, Sewon Min |  |
| 873 |  |  [Multilingual unsupervised sequence segmentation transfers to extremely low-resource languages](https://doi.org/10.18653/v1/2022.acl-long.366) |  | 0 | We show that unsupervised sequence-segmentation performance can be transferred to extremely low-resource languages by pre-training a Masked Segmental Language Model (Downey et al., 2021) multilingually. Further, we show that this transfer can be achieved by training over a collection of low-resource languages that are typologically similar (but phylogenetically... | C. M. Downey, Levon Haroutunian, Shannon Drizin, Shivin Thukral |  |
| 874 |  |  [KinyaBERT: a Morphology-aware Kinyarwanda Language Model](https://doi.org/10.18653/v1/2022.acl-long.367) |  | 0 | Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding - BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes... | Andre Niyongabo Rubungo, Antoine Nzeyimana |  |
| 875 |  |  [On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency](https://doi.org/10.18653/v1/2022.acl-long.368) |  | 0 | A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU)... | Cornelia Caragea, Seoyeon Park |  |
| 876 |  |  [IMPLI: Investigating NLI Models' Performance on Figurative Language](https://doi.org/10.18653/v1/2022.acl-long.369) |  | 0 | Natural language inference (NLI) has been widely used as a task to train and evaluate models for language understanding. However, the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the IMPLI (Idiomatic and Metaphoric Paired Language Inference) dataset, an... | Iryna Gurevych, Kevin Stowe, Prasetya Ajie Utama |  |
| 877 |  |  [QAConv: Question Answering on Informative Conversations](https://doi.org/10.18653/v1/2022.acl-long.370) |  | 0 | This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations, including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain... | Andrea Madotto, Caiming Xiong, ChienSheng Wu, Pascale Fung, Wenhao Liu |  |
| 878 |  |  [Prix-LM: Pretraining for Multilingual Knowledge Base Construction](https://doi.org/10.18653/v1/2022.acl-long.371) |  | 0 | Knowledge bases (KBs) contain plenty of structured world and commonsense knowledge. As such, they often complement distributional text-based information and facilitate various downstream tasks. Since their manual construction is resource- and time-intensive, recent efforts have tried leveraging large pretrained language models (PLMs) to generate additional... | Fangyu Liu, Ivan Vulic, Muhao Chen, Nigel Collier, Wenxuan Zhou |  |
| 879 |  |  [Semantic Composition with PSHRG for Derivation Tree Reconstruction from Graph-Based Meaning Representations](https://doi.org/10.18653/v1/2022.acl-long.372) |  | 0 | We introduce a data-driven approach to generating derivation trees from meaning representation graphs with probabilistic synchronous hyperedge replacement grammar (PSHRG). SHRG has been used to produce meaning representation graphs from texts and syntax trees, but little is known about its viability on the reverse. In particular, we experiment on Dependency... | Chun Hei Lo, Hong Cheng, Wai Lam |  |
| 880 |  |  [HOLM: Hallucinating Objects with Language Models for Referring Expression Recognition in Partially-Observed Scenes](https://doi.org/10.18653/v1/2022.acl-long.373) |  | 0 | AI systems embodied in the physical world face a fundamental challenge of partial observability; operating with only a limited view and knowledge of the environment. This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not... | LouisPhilippe Morency, Taylor BergKirkpatrick, Volkan Cirik |  |
| 881 |  |  [Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models](https://doi.org/10.18653/v1/2022.acl-long.374) |  | 0 | Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a... | Kabir Ahuja, Monojit Choudhury, Sandipan Dandapat, Shanu Kumar |  |
| 882 |  |  [∞-former: Infinite Memory Transformer](https://doi.org/10.18653/v1/2022.acl-long.375) |  | 0 | Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the ∞-former, which extends the vanilla... | André F. T. Martins, Pedro Henrique Martins, Zita Marinho |  |
| 883 |  |  [Systematic Inequalities in Language Technology Performance across the World's Languages](https://doi.org/10.18653/v1/2022.acl-long.376) |  | 0 | Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world’s ≈6,500 languages. We... | Antonios Anastasopoulos, Damián E. Blasi, Graham Neubig |  |
| 884 |  |  [CaMEL: Case Marker Extraction without Labels](https://doi.org/10.18653/v1/2022.acl-long.377) |  | 0 | We introduce CaMEL (Case Marker Extraction without Labels), a novel and challenging task in computational morphology that is especially relevant for low-resource languages. We propose a first model for CaMEL that uses a massively multilingual corpus to extract case markers in 83 languages based only on a noun phrase chunker and an alignment system. To evaluate... | Hinrich Schütze, Leonie Weissweiler, Masoud Jalili Sabet, Valentin Hofmann |  |
| 885 |  |  [Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors](https://doi.org/10.18653/v1/2022.acl-long.378) |  | 0 | Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation. New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to... | Isar Nejadgholi, Kathleen C. Fraser, Svetlana Kiritchenko |  |
| 886 |  |  [Reports of personal experiences and stories in argumentation: datasets and analysis](https://doi.org/10.18653/v1/2022.acl-long.379) |  | 0 | Reports of personal experiences or stories can play a crucial role in argumentation, as they represent an immediate and (often) relatable way to back up one’s position with respect to a given topic. They are easy to understand and increase empathy: this makes them powerful in argumentation. The impact of personal reports and stories in argumentation has been... | Gabriella Lapesa, Neele Falk |  |
| 887 |  |  [Non-neural Models Matter: a Re-evaluation of Neural Referring Expression Generation Systems](https://doi.org/10.18653/v1/2022.acl-long.380) |  | 0 | In recent years, neural models have often outperformed rule-based and classic Machine Learning approaches in NLG. These classic approaches are now often disregarded, for example when new neural models are evaluated. We argue that they should not be overlooked, since, for some tasks, well-designed non-neural approaches achieve better performance than neural... | Fahime Same, Guanyi Chen, Kees van Deemter |  |
| 888 |  |  [Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion](https://doi.org/10.18653/v1/2022.acl-long.381) |  | 0 | Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers, and are typically evaluated on large-scale datasets like Spider (Yu et al., 2018). We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain... | Adam Pauls, Chen Zhao, Emmanouil Antonios Platanios, Yu Su |  |
| 889 |  |  [Predicate-Argument Based Bi-Encoder for Paraphrase Identification](https://doi.org/10.18653/v1/2022.acl-long.382) |  | 0 | Paraphrase identification involves identifying whether a pair of sentences express the same or similar meanings. While cross-encoders have achieved high performances across several benchmarks, bi-encoders such as SBERT have been widely applied to sentence pair tasks. They exhibit substantially lower computation complexity and are better suited to symmetric... | David J. Weir, Julie Weeds, Qiwei Peng, Yekun Chai |  |
| 890 |  |  [MINER: Improving Out-of-Vocabulary Named Entity Recognition from an Information Theoretic Perspective](https://doi.org/10.18653/v1/2022.acl-long.383) |  | 0 | NER model has achieved promising performance on standard NER benchmarks. However, recent studies show that previous approaches may over-rely on entity mention information, resulting in poor performance on out-of-vocabulary(OOV) entity recognition. In this work, we propose MINER, a novel NER learning framework, to remedy this issue from an information-theoretic... | Liang Qiao, Limao Xiong, Qi Zhang, Shihan Dou, Tao Gui, Xiao Wang, Xuanjing Huang, Yicheng Zou, Zhanzhan Cheng |  |
| 891 |  |  [Leveraging Wikipedia article evolution for promotional tone detection](https://doi.org/10.18653/v1/2022.acl-long.384) |  | 0 | Detecting biased language is useful for a variety of applications, such as identifying hyperpartisan news sources or flagging one-sided rhetoric. In this work we introduce WikiEvolve, a dataset for document-level promotional tone detection. Unlike previously proposed datasets, WikiEvolve contains seven versions of the same article from Wikipedia, from different... | Andreas Vlachos, Christine de Kock |  |
| 892 |  |  [From text to talk: Harnessing conversational corpora for humane and diversity-aware language technology](https://doi.org/10.18653/v1/2022.acl-long.385) |  | 0 | Informal social interaction is the primordial home of human language. Linguistically diverse conversational corpora are an important and largely untapped resource for computational linguistics and language technology. Through the efforts of a worldwide language documentation movement, such corpora are increasingly becoming available. We show how interactional... | Andreas Liesenfeld, Mark Dingemanse |  |
| 893 |  |  [Flooding-X: Improving BERT's Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning](https://doi.org/10.18653/v1/2022.acl-long.386) |  | 0 | Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training. However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples. To address... | Bao Rong, Jingyi Liu, Liang Qiao, Qi Zhang, Qin Liu, Rui Zheng, Tao Gui, Xuanjing Huang, Zhanzhan Cheng, Zhihua Liu |  |
| 894 |  |  [RoMe: A Robust Metric for Evaluating Natural Language Generation](https://doi.org/10.18653/v1/2022.acl-long.387) |  | 0 | Evaluating Natural Language Generation (NLG) systems is a challenging task. Firstly, the metric should ensure that the generated hypothesis reflects the reference’s semantics. Secondly, it should consider the grammatical quality of the generated sentence. Thirdly, it should be robust enough to handle various surface forms of the generated sentence. Thus, an... | Debanjan Chaudhuri, Jens Lehmann, Liubov Kovriguina, Md. Rashad Al Hasan Rony, Ricardo Usbeck |  |
| 895 |  |  [Finding Structural Knowledge in Multimodal-BERT](https://doi.org/10.18653/v1/2022.acl-long.388) |  | 0 | In this work, we investigate the knowledge learned in the embeddings of multimodal-BERT models. More specifically, we probe their capabilities of storing the grammatical structure of linguistic data and the structure learned over objects in visual data. To reach that goal, we first make the inherent structure of language and visuals explicit by a dependency... | MarieFrancine Moens, Miryam de Lhoneux, Victor Milewski |  |
| 896 |  |  [Fully Hyperbolic Neural Networks](https://doi.org/10.18653/v1/2022.acl-long.389) |  | 0 | Hyperbolic neural networks have shown great potential for modeling complex data. However, existing hyperbolic networks are not completely hyperbolic, as they encode features in the hyperbolic space yet formalize most of their operations in the tangent space (a Euclidean subspace) at the origin of the hyperbolic model. This hybrid method greatly limits the... | Hexu Zhao, Jie Zhou, Maosong Sun, Peng Li, Weize Chen, Xu Han, Yankai Lin, Zhiyuan Liu |  |
| 897 |  |  [Neural Machine Translation with Phrase-Level Universal Visual Representations](https://doi.org/10.18653/v1/2022.acl-long.390) |  | 0 | Multimodal machine translation (MMT) aims to improve neural machine translation (NMT) with additional visual information, but most existing MMT methods require paired input of source sentence and image, which makes them suffer from shortage of sentence-image pairs. In this paper, we propose a phrase-level retrieval-based method for MMT to get visual information... | Qingkai Fang, Yang Feng |  |
| 898 |  |  [M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database](https://doi.org/10.18653/v1/2022.acl-long.391) |  | 0 | The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label... | Haizhou Li, Jingwen Hu, Jinming Zhao, Qin Jin, Tenggan Zhang, Xinchao Wang, Yuchen Liu |  |
| 899 |  |  [Few-shot Named Entity Recognition with Self-describing Networks](https://doi.org/10.18653/v1/2022.acl-long.392) |  | 0 | Few-shot NER needs to effectively capture information from limited instances and transfer useful knowledge from external resources. In this paper, we propose a self-describing mechanism for few-shot NER, which can effectively leverage illustrative instances and precisely transfer knowledge from external resources by describing both entity types and mentions... | Hongyu Lin, Jiawei Chen, Le Sun, Qing Liu, Xianpei Han |  |
| 900 |  |  [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://doi.org/10.18653/v1/2022.acl-long.393) |  | 0 | Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific... | Chengyi Wang, Furu Wei, Jinyu Li, Junyi Ao, Long Zhou, Qing Li, Rui Wang, Shujie Liu, Shuo Ren, Tom Ko, Yao Qian, Yu Wu, Yu Zhang, Zhihua Wei |  |
| 901 |  |  [Human Evaluation and Correlation with Automatic Metrics in Consultation Note Generation](https://doi.org/10.18653/v1/2022.acl-long.394) |  | 0 | In recent years, machine learning models have rapidly become better at generating clinical consultation notes; yet, there is little work on how to properly evaluate the generated consultation notes to understand the impact they may have on both the clinician using them and the patient’s clinical safety. To address this we present an extensive human evaluation... | Aleksandar Savkov, Alex PapadopoulosKorfiatis, Anya Belz, Damir Juric, Ehud Reiter, Francesco Moramarco, Jack Flann, Mark Perera |  |
| 902 |  |  [Unified Structure Generation for Universal Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.395) |  | 0 | Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge... | Dai Dai, Hongyu Lin, Hua Wu, Le Sun, Qing Liu, Xianpei Han, Xinyan Xiao, Yaojie Lu |  |
| 903 |  |  [Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.acl-long.396) |  | 0 | Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the... | Cuiping Li, Hong Chen, Jian Tang, Jie Tang, Jifan Yu, Jing Zhang, Xiaokang Zhang |  |
| 904 |  |  [Pre-training to Match for Unified Low-shot Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.397) |  | 0 | Low-shot relation extraction (RE) aims to recognize novel relations with very few or even no samples, which is critical in real scenario application. Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem to be with similar target but require totally different underlying abilities. In this paper, we propose Multi-Choice Matching Networks... | Boxi Cao, Fangchao Liu, Hongyu Lin, Le Sun, Xianpei Han |  |
| 905 |  |  [Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View](https://doi.org/10.18653/v1/2022.acl-long.398) |  | 0 | Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks... | Boxi Cao, Fangchao Liu, Hongyu Lin, Le Sun, Xianpei Han |  |
| 906 |  |  [Evaluating Extreme Hierarchical Multi-label Classification](https://doi.org/10.18653/v1/2022.acl-long.399) |  | 0 | Several natural language processing (NLP) tasks are defined as a classification problem in its most complex form: Multi-label Hierarchical Extreme classification, in which items may be associated with multiple classes from a set of thousands of possible classes organized in a hierarchy and with a highly unbalanced distribution both in terms of class frequency... | Agustín D. Delgado, Enrique Amigó |  |
| 907 |  |  [What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels](https://doi.org/10.18653/v1/2022.acl-long.400) |  | 0 | We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances. Our model is divided into three independent components: extracting direct-speech, compiling a list of characters, and attributing those characters to their utterances. Although we find that existing systems can perform the first two tasks accurately,... | Animesh Prasad, Carolina CuestaLázaro, Trevor Wood |  |
| 908 |  |  [Measuring Fairness of Text Classifiers via Prediction Sensitivity](https://doi.org/10.18653/v1/2022.acl-long.401) |  | 0 | With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation –... | Apurv Verma, Jwala Dhamala, KaiWei Chang, Rahul Gupta, Satyapriya Krishna, Yada Pruksachatkun |  |
| 909 |  |  [RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion](https://doi.org/10.18653/v1/2022.acl-long.402) |  | 0 | Temporal factors are tied to the growth of facts in realistic applications, such as the progress of diseases and the development of political situation, therefore, research on Temporal Knowledge Graph (TKG) attracks much attention. In TKG, relation patterns inherent with temporality are required to be studied for representation learning and reasoning across... | Aiping Li, Kai Chen, Ye Wang, Yitong Li |  |
| 910 |  |  [Feeding What You Need by Understanding What You Learned](https://doi.org/10.18653/v1/2022.acl-long.403) |  | 0 | Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM) and F1. However, such a paradigm lacks sufficient interpretation to model capability and... | Bang Liu, Bo Long, Fangli Xu, Lingfei Wu, Siliang Tang, Xiaoqiang Wang |  |
| 911 |  |  [Probing Simile Knowledge from Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.404) |  | 0 | Simile interpretation (SI) and simile generation (SG) are challenging tasks for NLP because models require adequate world knowledge to produce predictions. Previous works have employed many hand-crafted resources to bring knowledge-related into models, which is time-consuming and labor-intensive. In recent years, pre-trained language models (PLMs) based... | Chang Su, Guandan Chen, Jiashu Pu, Le Zhang, Rongsheng Zhang, Weijie Chen, Yadong Xi, Yijiang Chen, Yongzhu Chang |  |
| 912 |  |  [An Effective and Efficient Entity Alignment Decoding Algorithm via Third-Order Tensor Isomorphism](https://doi.org/10.18653/v1/2022.acl-long.405) |  | 0 | Entity alignment (EA) aims to discover the equivalent entity pairs between KGs, which is a crucial step for integrating multi-source KGs.For a long time, most researchers have regarded EA as a pure graph representation learning task and focused on improving graph encoders while paying little attention to the decoding process. In this paper, we propose an... | Hao Yuan, Jianchao Zhu, Man Lan, Meirong Ma, Rui Xie, Wei Wu, Xin Mao, Zongyu Wang |  |
| 913 |  |  [Entailment Graph Learning with Textual Entailment and Soft Transitivity](https://doi.org/10.18653/v1/2022.acl-long.406) |  | 0 | Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes. The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity. We propose a two-stage method, Entailment Graph with Textual Entailment and Transitivity (EGT2).... | Dongyan Zhao, Yansong Feng, Zhibin Chen |  |
| 914 |  |  [Logic Traps in Evaluating Attribution Scores](https://doi.org/10.18653/v1/2022.acl-long.407) |  | 0 | Modern deep learning models are notoriously opaque, which has motivated the development of methods for interpreting how deep models predict. This goal is usually approached with attribution method, which assesses the influence of features on model predictions. As an explanation method, the evaluation criteria of attribution methods is how accurately it reflects... | Jun Zhao, Kang Liu, Yiming Ju, Yuanzhe Zhang, Zhao Yang, Zhongtao Jiang |  |
| 915 |  |  [Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network](https://doi.org/10.18653/v1/2022.acl-long.408) |  | 0 | In this paper, we study how to continually pre-train language models for improving the understanding of math problems. Specifically, we focus on solving a fundamental challenge in modeling math problems, how to fuse the semantics of textual description and formulas, which are highly different in essence. To address this issue, we propose a new approach called... | JiRong Wen, Jing Sha, Kun Zhou, Shijin Wang, Wayne Xin Zhao, Zheng Gong |  |
| 916 |  |  [Multitasking Framework for Unsupervised Simple Definition Generation](https://doi.org/10.18653/v1/2022.acl-long.409) |  | 0 | The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner’s dictionaries in many... | Cunliang Kong, Erhong Yang, Hengyuan Zhang, Liner Yang, Yun Chen |  |
| 917 |  |  [Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction](https://doi.org/10.18653/v1/2022.acl-long.410) |  | 0 | Solving math word problems requires deductive reasoning over the quantities in the text. Various recent research efforts mostly relied on sequence-to-sequence or sequence-to-tree models to generate mathematical expressions without explicitly performing relational reasoning between quantities in the given context. While empirically effective, such approaches... | Jierui Li, Wei Lu, Zhanming Jie |  |
| 918 |  |  [When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues](https://doi.org/10.18653/v1/2022.acl-long.411) |  | 0 | Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue... | Atharva Kulkarni, Md. Shad Akhtar, Shivani Kumar, Tanmoy Chakraborty |  |
| 919 |  |  [Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning](https://doi.org/10.18653/v1/2022.acl-long.412) |  | 0 | Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the... | Dongha Lee, Hwanjo Yu, Seongbo Jang, Seonghyeon Lee |  |
| 920 |  |  [Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge](https://doi.org/10.18653/v1/2022.acl-long.413) |  | 0 | Recent years have witnessed growing interests in incorporating external knowledge such as pre-trained word embeddings (PWEs) or pre-trained language models (PLMs) into neural topic modeling. However, we found that employing PWEs and PLMs for topic modeling only achieved limited performance improvements but with huge computational overhead. In this paper, we... | Boyu Wang, Deyu Zhou, Linhai Zhang, QianWen Zhang, Xuemeng Hu, Yunbo Cao |  |
| 921 |  |  [Multi-View Document Representation Learning for Open-Domain Dense Retrieval](https://doi.org/10.18653/v1/2022.acl-long.414) |  | 0 | Dense retrieval has achieved impressive advances in first-stage retrieval from a large-scale document collection, which is built on bi-encoder architecture to produce single vector representation of query and document. However, a document can usually answer multiple potential queries from different views. So the single vector representation of a document is... | Daxin Jiang, Ming Gong, Nan Duan, Shunyu Zhang, Yaobo Liang |  |
| 922 |  |  [Graph Pre-training for AMR Parsing and Generation](https://doi.org/10.18653/v1/2022.acl-long.415) |  | 0 | Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we... | Xuefeng Bai, Yue Zhang, Yulong Chen |  |
| 923 |  |  [Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills](https://doi.org/10.18653/v1/2022.acl-long.416) |  | 0 | Models pre-trained with a language modeling objective possess ample world knowledge and language skills, but are known to struggle in tasks that require reasoning. In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the... | Alon Talmor, Jonathan Berant, Ori Yoran |  |
| 924 |  |  [RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering](https://doi.org/10.18653/v1/2022.acl-long.417) |  | 0 | Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage... | Caiming Xiong, Kazuma Hashimoto, Semih Yavuz, Xi Ye, Yingbo Zhou |  |
| 925 |  |  [Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling](https://doi.org/10.18653/v1/2022.acl-long.418) |  | 0 | Given the claims of improved text generation quality across various pre-trained neural models, we consider the coherence evaluation of machine generated text to be one of the principal applications of coherence models that needs to be investigated. Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the... | Prathyusha Jwalapuram, Shafiq R. Joty, Xiang Lin |  |
| 926 |  |  [Just Rank: Rethinking Evaluation with Word and Sentence Similarities](https://doi.org/10.18653/v1/2022.acl-long.419) |  | 0 | Word and sentence embeddings are useful feature representations in natural language processing. However, intrinsic evaluation for embeddings lags far behind, and there has been no significant update since the past decade. Word and sentence similarity tasks have become the de facto evaluation method. It leads models to overfit to such evaluations, negatively... | Bin Wang, C.C. Jay Kuo, Haizhou Li |  |
| 927 |  |  [MarkupLM: Pre-training of Text and Markup Language for Visually Rich Document Understanding](https://doi.org/10.18653/v1/2022.acl-long.420) |  | 0 | Multimodal pre-training with text, layout, and image has made significant progress for Visually Rich Document Understanding (VRDU), especially the fixed-layout documents such as scanned document images. While, there are still a large number of digital documents where the layout information is not fixed and needs to be interactively and dynamically rendered for... | Furu Wei, Junlong Li, Lei Cui, Yiheng Xu |  |
| 928 |  |  [CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment](https://doi.org/10.18653/v1/2022.acl-long.421) |  | 0 | CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pre-trained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we... | Furu Wei, Haoyu Song, Li Dong, Ting Liu, Weinan Zhang |  |
| 929 |  |  [KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base](https://doi.org/10.18653/v1/2022.acl-long.422) |  | 0 | Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning... | Bin He, Hanwang Zhang, Jiaxin Shi, Juanzi Li, Lei Hou, Liangming Pan, Lunyiu Nie, Shulin Cao, Yutong Xiang |  |
| 930 |  |  [Debiased Contrastive Learning of Unsupervised Sentence Representations](https://doi.org/10.18653/v1/2022.acl-long.423) |  | 0 | Recently, contrastive learning has been shown to be effective in improving pre-trained language models (PLM) to derive high-quality sentence representations. It aims to pull close positive examples to enhance the alignment while push apart irrelevant negatives for the uniformity of the whole representation space. However, previous works mostly adopt in-batch... | Beichen Zhang, JiRong Wen, Kun Zhou, Wayne Xin Zhao |  |
| 931 |  |  [MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators](https://doi.org/10.18653/v1/2022.acl-long.424) |  | 0 | Prompting has recently been shown as a promising approach for applying pre-trained language models to perform downstream tasks. We present Multi-Stage Prompting, a simple and automatic approach for leveraging pre-trained language models to translation tasks. To better mitigate the discrepancy between pre-training and translation, MSP divides the translation... | Shuo Wang, Xiangwen Zhang, Yang Liu, Zhixing Tan |  |
| 932 |  |  [SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues](https://doi.org/10.18653/v1/2022.acl-long.425) |  | 0 | Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a... | Maolin Li, Ssu Chiu, YenTing Lin, YunNung Chen |  |
| 933 |  |  [UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining](https://doi.org/10.18653/v1/2022.acl-long.426) |  | 0 | High-quality phrase representations are essential to finding topics and related terms in documents (a.k.a. topic mining). Existing phrase representation learning methods either simply combine unigram representations in a context-free manner or rely on extensive annotations to learn context-aware knowledge. In this paper, we propose UCTopic, a novel unsupervised... | Jiacheng Li, Jingbo Shang, Julian J. McAuley |  |
| 934 |  |  [XLM-E: Cross-lingual Language Model Pre-training via ELECTRA](https://doi.org/10.18653/v1/2022.acl-long.427) |  | 0 | In this paper, we introduce ELECTRA-style tasks to cross-lingual language model pre-training. Specifically, we present two pre-training tasks, namely multilingual replaced token detection, and translation replaced token detection. Besides, we pretrain the model, named as XLM-E, on both multilingual and parallel corpora. Our model outperforms the baseline models... | Bo Zheng, Furu Wei, Heyan Huang, Li Dong, Payal Bajaj, Saksham Singhal, Shaohan Huang, Shuming Ma, Xia Song, XianLing Mao, Zewen Chi |  |
| 935 |  |  [Nested Named Entity Recognition as Latent Lexicalized Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.428) |  | 0 | Nested named entity recognition (NER) has been receiving increasing attention. Recently, Fu et al. (2020) adapt a span-based constituency parser to tackle nested NER. They treat nested entities as partially-observed constituency trees and propose the masked inside algorithm for partial marginalization. However, their method cannot leverage entity heads, which... | Chao Lou, Kewei Tu, Songlin Yang |  |
| 936 |  |  [Can Explanations Be Useful for Calibrating Black Box Models?](https://doi.org/10.18653/v1/2022.acl-long.429) |  | 0 | NLP practitioners often want to take existing trained models and apply them to data from new domains. While fine-tuning or few-shot learning can be used to adapt a base model, there is no single recipe for making these techniques work; moreover, one may not have access to the original model weights if it is deployed as a black box. We study how to improve a... | Greg Durrett, Xi Ye |  |
| 937 |  |  [OIE@OIA: an Adaptable and Efficient Open Information Extraction Framework](https://doi.org/10.18653/v1/2022.acl-long.430) |  | 0 | Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements. This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution. OIE@OIA follows the... | Mingming Sun, Minlong Peng, Ping Li, Xin Wang |  |
| 938 |  |  [ReACC: A Retrieval-Augmented Code Completion Framework](https://doi.org/10.18653/v1/2022.acl-long.431) |  | 0 | Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current... | Alexey Svyatkovskiy, Daya Guo, Hojae Han, Nan Duan, Seungwon Hwang, Shuai Lu |  |
| 939 |  |  [Does Recommend-Revise Produce Reliable Annotations? An Analysis on Missing Instances in DocRED](https://doi.org/10.18653/v1/2022.acl-long.432) |  | 0 | DocRED is a widely used dataset for document-level relation extraction. In the large-scale annotation, a recommend-revise scheme is adopted to reduce the workload. Within this scheme, annotators are provided with candidate relation instances from distant supervision, and they then manually supplement and remove relational facts based on the recommendations.... | Dongyan Zhao, Quzhe Huang, Shengqi Zhu, Shibo Hao, Yansong Feng, Yuan Ye |  |
| 940 |  |  [UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning](https://doi.org/10.18653/v1/2022.acl-long.433) |  | 0 | Recent parameter-efficient language model tuning (PELT) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different PELT methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific... | Amjad Almahairi, Hao Ma, Jiawei Han, Lambert Mathias, Madian Khabsa, Rui Hou, Scott Yih, Yuning Mao |  |
| 941 |  |  [An Empirical Study of Memorization in NLP](https://doi.org/10.18653/v1/2022.acl-long.434) |  | 0 | A recent study by Feldman (2020) proposed a long-tail theory to explain the memorization behavior of deep learning models. However, memorization has not been empirically verified in the context of NLP, a gap addressed by this work. In this paper, we use three different NLP tasks to check if the long-tail theory holds. Our experiments demonstrate that top-ranked... | Jing Jiang, Xiaosen Zheng |  |
| 942 |  |  [AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages](https://doi.org/10.18653/v1/2022.acl-long.435) |  | 0 | Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining. However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen... | Abteen Ebrahimi, Alexis Palmer, Angela Fan, Annette Rios, Arturo Oncevay, Elisabeth Mager, Graham Neubig, Gustavo Giménez Lugo, Iván Vladimir Meza Ruíz, John E. Ortega, Katharina Kann, Luis Chiruzzo, Manuel Mager, Ngoc Thang Vu, Ricardo Ramos, Rolando CotoSolano, Vishrav Chaudhary |  |
| 943 |  |  [Towards Learning (Dis)-Similarity of Source Code from Program Contrasts](https://doi.org/10.18653/v1/2022.acl-long.436) |  | 0 | Understanding the functional (dis)-similarity of source code is significant for code modeling tasks such as software vulnerability and code clone detection. We present DISCO (DIS-similarity of COde), a novel self-supervised model focusing on identifying (dis)similar functionalities of source code. Different from existing works, our approach does not require a... | Alessandro Morari, Baishakhi Ray, Luca Buratti, Saikat Chakraborty, Saurabh Pujar, Yangruibo Ding |  |
| 944 |  |  [Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News](https://doi.org/10.18653/v1/2022.acl-long.437) |  | 0 | Most works on financial forecasting use information directly associated with individual companies (e.g., stock prices, news on the company) to predict stock returns for trading. We refer to such company-specific information as local information. Stock returns may also be influenced by global information (e.g., news on the economy in general), and inter-company... | EePeng Lim, Gary Ang |  |
| 945 |  |  [On Vision Features in Multimodal Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.438) |  | 0 | Previous work on multimodal machine translation (MMT) has focused on the way of incorporating vision features into translation but little attention is on the quality of vision models. In this work, we investigate the impact of vision models on MMT. Given the fact that Transformer is becoming popular in computer vision, we experiment with various strong models... | Anxiang Ma, Bei Li, Chuanhao Lv, Jingbo Zhu, Tao Zhou, Tong Xiao, Zefan Zhou |  |
| 946 |  |  [CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning](https://doi.org/10.18653/v1/2022.acl-long.439) |  | 0 | Named Entity Recognition (NER) in Few-Shot setting is imperative for entity tagging in low resource domains. Existing approaches only learn class-specific semantic features and intermediate representations from source domains. This affects generalizability to unseen target domains, resulting in suboptimal performances. To this end, we present CONTaiNER, a novel... | Arzoo Katiyar, Rebecca J. Passonneau, Rui Zhang, Sarkar Snigdha Sarathi Das |  |
| 947 |  |  [Cree Corpus: A Collection of nêhiyawêwin Resources](https://doi.org/10.18653/v1/2022.acl-long.440) |  | 0 | Plains Cree (nêhiyawêwin) is an Indigenous language that is spoken in Canada and the USA. It is the most widely spoken dialect of Cree and a morphologically complex language that is polysynthetic, highly inflective, and agglutinative. It is an extremely low resource language, with no existing corpus that is both available and prepared for supporting the... | Carrie Demmans Epp, Daniela Teodorescu, Delaney Lothian, Denilson Barbosa, Josie Matalski |  |
| 948 |  |  [Learning to Rank Visual Stories From Human Ranking Data](https://doi.org/10.18653/v1/2022.acl-long.441) |  | 0 | Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first... | Chacha Chen, ChiYang Hsu, KuanChieh Lo, LunWei Ku, TingHao (Kenneth) Huang, Vincent Chen, YunWei Chu |  |
| 949 |  |  [Universal Conditional Masked Language Pre-training for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.442) |  | 0 | Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive... | Liangyou Li, Meng Zhang, Minghao Wu, Pengfei Li, Qun Liu |  |
| 950 |  |  [CARETS: A Consistency And Robustness Evaluative Test Suite for VQA](https://doi.org/10.18653/v1/2022.acl-long.443) |  | 0 | We introduce CARETS, a systematic test suite to measure consistency and robustness of modern VQA models through a series of six fine-grained capability tests. In contrast to existing VQA test sets, CARETS features balanced question generation to create pairs of instances to test models, with each pair focusing on a specific capability such as rephrasing,... | Carlos E. Jimenez, Karthik Narasimhan, Olga Russakovsky |  |
| 951 |  |  [Phrase-aware Unsupervised Constituency Parsing](https://doi.org/10.18653/v1/2022.acl-long.444) |  | 0 | Recent studies have achieved inspiring success in unsupervised grammar induction using masked language modeling (MLM) as the proxy task. Despite their high accuracy in identifying low-level structures, prior arts tend to struggle in capturing high-level structures like clauses, since the MLM task usually only requires information from local context. In this... | Jiaming Shen, Jiawei Han, Jingbo Shang, Xiaotao Gu, Yikang Shen |  |
| 952 |  |  [Achieving Reliable Human Assessment of Open-Domain Dialogue Systems](https://doi.org/10.18653/v1/2022.acl-long.445) |  | 0 | Evaluation of open-domain dialogue systems is highly challenging and development of better techniques is highlighted time and again as desperately needed. Despite substantial efforts to carry out reliable live evaluation of systems in recent competitions, annotations have been abandoned and reported as too unreliable to yield sensible results. This is a serious... | Chenyang Lyu, Gareth J. F. Jones, Qun Liu, Tianbo Ji, Yvette Graham |  |
| 953 |  |  [Updated Headline Generation: Creating Updated Summaries for Evolving News Stories](https://doi.org/10.18653/v1/2022.acl-long.446) |  | 0 | We propose the task of updated headline generation, in which a system generates a headline for an updated article, considering both the previous article and headline. The system must identify the novel information in the article update, and modify the existing headline accordingly. We create data for this task using the NewsEdits corpus by automatically... | Adrian Benton, Mark Dredze, Sheena Panthaplackel |  |
| 954 |  |  [SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures](https://doi.org/10.18653/v1/2022.acl-long.447) |  | 0 | Current open-domain conversational models can easily be made to talk in inadequate ways. Online learning from conversational feedback given by the conversation partner is a promising avenue for a model to improve and adapt, so as to generate fewer of these safety failures. However, current state-of-the-art models tend to react to feedback with defensive or... | Jing Xu, Megan Ung, YLan Boureau |  |
| 955 |  |  [Compositional Generalization in Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.448) |  | 0 | Compositionality— the ability to combine familiar units like words into novel phrases and sentences— has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the... | Dzmitry Bahdanau, Emily Goodwin, Siva Reddy, Timothy J. O'Donnell |  |
| 956 |  |  [ASPECTNEWS: Aspect-Oriented Summarization of News Documents](https://doi.org/10.18653/v1/2022.acl-long.449) |  | 0 | Generic summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users’ needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents. In this paper, we collect a dataset of realistic aspect-oriented summaries, AspectNews, which... | Akshay Gupta, Greg Durrett, Jiacheng Xu, Kevin Horecka, Ojas Ahuja |  |
| 957 |  |  [MemSum: Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes](https://doi.org/10.18653/v1/2022.acl-long.450) |  | 0 | We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in... | Elliott Ash, Nianlong Gu, Richard H. R. Hahnloser |  |
| 958 |  |  [CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations](https://doi.org/10.18653/v1/2022.acl-long.451) |  | 0 | Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, a hallmark of human intelligence is the ability to learn new concepts purely from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier... | Rakesh R. Menon, Sayan Ghosh, Shashank Srivastava |  |
| 959 |  |  [Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.452) |  | 0 | We present substructure distribution projection (SubDP), a technique that projects a distribution over structures in one domain to another, by projecting substructure distributions separately. Models for the target domain can then be trained, using the projected distributions as soft silver labels. We evaluate SubDP on zero shot cross-lingual dependency... | Freda Shi, Karen Livescu, Kevin Gimpel |  |
| 960 |  |  [Multilingual Detection of Personal Employment Status on Twitter](https://doi.org/10.18653/v1/2022.acl-long.453) |  | 0 | Detecting disclosures of individuals’ employment status on social media can provide valuable information to match job seekers with suitable vacancies, offer social protection, or measure labor market flows. However, identifying such personal disclosures is a challenging task due to their rarity in a sea of social media content and the variety of linguistic... | Dhaval Adjodah, João Palotti, Manuel Tonneau, Nir Grinberg, Samuel Fraiberger |  |
| 961 |  |  [MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data](https://doi.org/10.18653/v1/2022.acl-long.454) |  | 0 | Numerical reasoning over hybrid data containing both textual and tabular content (e.g., financial reports) has recently attracted much attention in the NLP community. However, existing question answering (QA) benchmarks over hybrid data only include a single flat table in each document and thus lack examples of multi-step numerical reasoning across multiple... | Chenying Li, Rui Zhang, Yilun Zhao, Yunxiang Li |  |
| 962 |  |  [Transformers in the loop: Polarity in neural models of language](https://doi.org/10.18653/v1/2022.acl-long.455) |  | 0 | Representation of linguistic phenomena in computational language models is typically assessed against the predictions of existing linguistic theories of these phenomena. Using the notion of polarity as a case study, we show that this is not always the most adequate set-up. We probe polarity via so-called ‘negative polarity items’ (in particular, English ‘any’)... | Alexey Tikhonov, Lisa Bylinina |  |
| 963 |  |  [Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.456) |  | 0 | Back-translation is a critical component of Unsupervised Neural Machine Translation (UNMT), which generates pseudo parallel data from target monolingual data. A UNMT model is trained on the pseudo parallel data with translated source, and translates natural source sentences in inference. The source discrepancy between training and inference hinders the... | Rui Wang, Shuming Shi, Xing Wang, Zhaopeng Tu, Zhiwei He |  |
| 964 |  |  [SDR: Efficient Neural Re-ranking using Succinct Document Representation](https://doi.org/10.18653/v1/2022.acl-long.457) |  | 0 | BERT based ranking models have achieved superior performance on various information retrieval tasks. However, the large number of parameters and complex self-attention operations come at a significant latency overhead. To remedy this, recent works propose late-interaction architectures, which allow pre-computation of intermediate document representations, thus... | Amir Ingber, Amit Portnoy, Besnik Fetahu, Nachshon Cohen |  |
| 965 |  |  [The AI Doctor Is In: A Survey of Task-Oriented Dialogue Systems for Healthcare Applications](https://doi.org/10.18653/v1/2022.acl-long.458) |  | 0 | Task-oriented dialogue systems are increasingly prevalent in healthcare settings, and have been characterized by a diverse range of architectures and objectives. Although these systems have been surveyed in the medical community from a non-technical perspective, a systematic review from a rigorous computational perspective has to date remained noticeably... | Mina Valizadeh, Natalie Parde |  |
| 966 |  |  [SHIELD: Defending Textual Neural Networks against Multiple Black-Box Adversarial Attacks with Stochastic Multi-Expert Patcher](https://doi.org/10.18653/v1/2022.acl-long.459) |  | 0 | Even though several methods have proposed to defend textual neural network (NN) models against black-box adversarial attacks, they often defend against a specific text perturbation strategy and/or require re-training the models from scratch. This leads to a lack of generalization in practice and redundant computation. In particular, the state-of-the-art... | Dongwon Lee, Noseong Park, Thai Le |  |
| 967 |  |  [Accurate Online Posterior Alignments for Principled Lexically-Constrained Decoding](https://doi.org/10.18653/v1/2022.acl-long.460) |  | 0 | Online alignment in machine translation refers to the task of aligning a target word to a source word when the target sequence has only been partially decoded. Good online alignments facilitate important applications such as lexically constrained translation where user-defined dictionaries are used to inject lexical constraints into the translation model. We... | Preethi Jyothi, Soumya Chatterjee, Sunita Sarawagi |  |
| 968 |  |  [Leveraging Task Transferability to Meta-learning for Clinical Section Classification with Limited Data](https://doi.org/10.18653/v1/2022.acl-long.461) |  | 0 | Identifying sections is one of the critical components of understanding medical information from unstructured clinical notes and developing assistive technologies for clinical note-writing tasks. Most state-of-the-art text classification systems require thousands of in-domain text data to achieve high performance. However, collecting in-domain and recent... | Jangwon Kim, Mustafa Y. Sir, Ram Bhakta, Zhuohao Chen |  |
| 969 |  |  [Reinforcement Guided Multi-Task Learning Framework for Low-Resource Stereotype Detection](https://doi.org/10.18653/v1/2022.acl-long.462) |  | 0 | As large Pre-trained Language Models (PLMs) trained on large amounts of data in an unsupervised manner become more ubiquitous, identifying various types of bias in the text has come into sharp focus. Existing ‘Stereotype Detection’ datasets mainly adopt a diagnostic approach toward large PLMs. Blodgett et. al. (2021) show that there are significant reliability... | Elnaz Nouri, Erik Oveson, Priyanka Kulkarni, Rajkumar Pujari |  |
| 970 |  |  [Letters From the Past: Modeling Historical Sound Change Through Diachronic Character Embeddings](https://doi.org/10.18653/v1/2022.acl-long.463) |  | 0 | While a great deal of work has been done on NLP approaches to lexical semantic change detection, other aspects of language change have received less attention from the NLP community. In this paper, we address the detection of sound change through historical spelling. We propose that a sound change can be captured by comparing the relative distance through time... | Patrizia Paggio, Sidsel Boldsen |  |
| 971 |  |  [A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation](https://doi.org/10.18653/v1/2022.acl-long.464) |  | 0 | Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily... | Bill Dolan, Chris Brockett, Tianyu Liu, Weizhu Chen, Yi Mao, Yizhe Zhang, Zhifang Sui |  |
| 972 |  |  [Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice](https://doi.org/10.18653/v1/2022.acl-long.465) |  | 0 | Classifiers in natural language processing (NLP) often have a large number of output classes. For example, neural language models (LMs) and machine translation (MT) models both predict tokens from a vocabulary of thousands. The Softmax output layer of these models typically receives as input a dense feature representation, which has much lower dimensionality... | Adam Lopez, Andreas Grivas, Nikolay Bogoychev |  |
| 973 |  |  [Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction](https://doi.org/10.18653/v1/2022.acl-long.466) |  | 0 | In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two... | Jing Shao, Kun Wang, Meiqi Chen, Mukai Li, Yixin Cao, Yubo Ma, Zehao Wang |  |
| 974 |  |  [Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework](https://doi.org/10.18653/v1/2022.acl-long.467) |  | 0 | Simultaneous machine translation (SiMT) starts translating while receiving the streaming source inputs, and hence the source sentence is always incomplete during translating. Different from the full-sentence MT using the conventional seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture, which forces each target word to only align with a... | Shaolei Zhang, Yang Feng |  |
| 975 |  |  [A Statutory Article Retrieval Dataset in French](https://doi.org/10.18653/v1/2022.acl-long.468) |  | 0 | Statutory article retrieval is the task of automatically retrieving law articles relevant to a legal question. While recent advances in natural language processing have sparked considerable interest in many legal tasks, statutory article retrieval remains primarily untouched due to the scarcity of large-scale and high-quality annotated datasets. To address this... | Antoine Louis, Gerasimos Spanakis |  |
| 976 |  |  [ParaDetox: Detoxification with Parallel Data](https://doi.org/10.18653/v1/2022.acl-long.469) |  | 0 | We present a novel pipeline for the collection of parallel data for the detoxification task. We collect non-toxic paraphrases for over 10,000 English toxic sentences. We also show that this pipeline can be used to distill a large existing corpus of paraphrases to get toxic-neutral sentence pairs. We release two parallel corpora which can be used for the... | Alexander Panchenko, Daniil Moskovskiy, Daryna Dementieva, David Dale, Irina Krotova, Nikita Semenov, Sergey Ustyantsev, Varvara Logacheva |  |
| 977 |  |  [Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color](https://doi.org/10.18653/v1/2022.acl-long.470) |  | 0 | Character-level information is included in many NLP models, but evaluating the information encoded in character representations is an open issue. We leverage perceptual representations in the form of shape, sound, and color embeddings and perform a representational similarity analysis to evaluate their correlation with textual representations in five languages.... | Manex Agirrezabal, Nora Hollenstein, Sidsel Boldsen |  |
| 978 |  |  [Fine-Grained Controllable Text Generation Using Non-Residual Prompting](https://doi.org/10.18653/v1/2022.acl-long.471) |  | 0 | The introduction of immensely large Causal Language Models (CLMs) has rejuvenated the interest in open-ended text generation. However, controlling the generative process for these Transformer-based models is at large an unsolved problem. Earlier work has explored either plug-and-play decoding strategies, or more powerful but blunt approaches such as prompting.... | Fangyu Liu, Fredrik Carlsson, Joakim Nivre, Joey Öhman, Magnus Sahlgren, Severine Verlinden |  |
| 979 |  |  [Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features](https://doi.org/10.18653/v1/2022.acl-long.472) |  | 0 | While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme... | Florian Lux, Ngoc Thang Vu |  |
| 980 |  |  [TwittIrish: A Universal Dependencies Treebank of Tweets in Modern Irish](https://doi.org/10.18653/v1/2022.acl-long.473) |  | 0 | Modern Irish is a minority language lacking sufficient computational resources for the task of accurate automatic syntactic parsing of user-generated content such as tweets. Although language technology for the Irish language has been developing in recent years, these tools tend to perform poorly on user-generated content. As with other languages, the... | James Barry, Jennifer Foster, Lauren Cassidy, Teresa Lynn |  |
| 981 |  |  [Length Control in Abstractive Summarization by Pretraining Information Selection](https://doi.org/10.18653/v1/2022.acl-long.474) |  | 0 | Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to... | Kenny Q. Zhu, Qi Jia, Yizhu Liu |  |
| 982 |  |  [CQG: A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation](https://doi.org/10.18653/v1/2022.acl-long.475) |  | 0 | Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage. Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers. However, most models can not ensure the complexity of generated questions, so they... | Di Liang, Qi Zhang, Sirui Wang, Tao Gui, Wei Wu, Xuanjing Huang, Zichu Fei |  |
| 983 |  |  [Word Order Does Matter and Shuffled Language Models Know It](https://doi.org/10.18653/v1/2022.acl-long.476) |  | 0 | Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models’ good performance with... | Anders Søgaard, Artur Kulmizev, Mostafa Abdou, Vinit Ravishankar |  |
| 984 |  |  [An Empirical Study on Explanations in Out-of-Domain Settings](https://doi.org/10.18653/v1/2022.acl-long.477) |  | 0 | Recent work in Natural Language Processing has focused on developing approaches that extract faithful explanations, either via identifying the most important tokens in the input (i.e. post-hoc explanations) or by designing inherently faithful models that first select the most important tokens and then use them to predict the correct label (i.e.... | George Chrysostomou, Nikolaos Aletras |  |
| 985 |  |  [MILIE: Modular & Iterative Multilingual Open Information Extraction](https://doi.org/10.18653/v1/2022.acl-long.478) |  | 0 | Open Information Extraction (OpenIE) is the task of extracting (subject, predicate, object) triples from natural language sentences. Current OpenIE systems extract all triple slots independently. In contrast, we explore the hypothesis that it may be beneficial to extract triple slots iteratively: first extract easy slots, followed by the difficult ones by... | Ammar Shaker, Bhushan Kotnis, Carolin Lawrence, Daniel OñoroRubio, Kiril Gashteovski, Makoto Takamoto, Mathias Niepert, Vanesa RodriguezTembras |  |
| 986 |  |  [What Makes Reading Comprehension Questions Difficult?](https://doi.org/10.18653/v1/2022.acl-long.479) |  | 0 | For a natural language understanding benchmark to be useful in research, it has to consist of examples that are diverse and difficult enough to discriminate among current and near-future state-of-the-art systems. However, we do not yet know how best to select text sources to collect a variety of challenging examples. In this study, we crowdsource... | Alex Warstadt, Nikita Nangia, Saku Sugawara, Samuel R. Bowman |  |
| 987 |  |  [From Simultaneous to Streaming Machine Translation by Leveraging Streaming History](https://doi.org/10.18653/v1/2022.acl-long.480) |  | 0 | Simultaneous Machine Translation is the task of incrementally translating an input sentence before it is fully available. Currently, simultaneous translation is carried out by translating each sentence independently of the previously translated text. More generally, Streaming MT can be understood as an extension of Simultaneous MT to the incremental translation... | Alfons JuanCíscar, Javier IranzoSánchez, Jorge Civera |  |
| 988 |  |  [A Rationale-Centric Framework for Human-in-the-loop Machine Learning](https://doi.org/10.18653/v1/2022.acl-long.481) |  | 0 | We present a novel rational-centric framework with human-in-the-loop – Rationales-centric Double-robustness Learning (RDL) – to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible “inductive bias”, exploits rationales (i.e.... | Brian MacNamee, Jinghui Lu, Linyi Yang, Yue Zhang |  |
| 989 |  |  [Challenges and Strategies in Cross-Cultural NLP](https://doi.org/10.18653/v1/2022.acl-long.482) |  | 0 | Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked,... | Anders Søgaard, Constanza Fierro, Daniel Hershcovich, Emanuele Bugliarello, Heather C. Lent, Ilias Chalkidis, Katerina Margatina, Laura Cabello Piqueras, Miryam de Lhoneux, Mostafa Abdou, Phillip Rust, Ruixiang Cui, Stella Frank, Stephanie Brandl |  |
| 990 |  |  [Prototypical Verbalizer for Prompt-based Few-shot Tuning](https://doi.org/10.18653/v1/2022.acl-long.483) |  | 0 | Prompt-based tuning for pre-trained language models (PLMs) has shown its effectiveness in few-shot learning. Typically, prompt-based tuning wraps the input text into a cloze question. To make predictions, the model maps the output words to labels via a verbalizer, which is either manually designed or automatically built. However, manual verbalizers heavily... | Ganqu Cui, Longtao Huang, Ning Ding, Shengding Hu, Zhiyuan Liu |  |
| 991 |  |  [Clickbait Spoiling via Question Answering and Passage Retrieval](https://doi.org/10.18653/v1/2022.acl-long.484) |  | 0 | We introduce and study the task of clickbait spoiling: generating a short text that satisfies the curiosity induced by a clickbait post. Clickbait links to a web page and advertises its contents by arousing curiosity instead of providing an informative summary. Our contributions are approaches to classify the type of spoiler needed (i.e., a phrase or a... | Artur Jurk, Maik Fröbe, Martin Potthast, Matthias Hagen |  |
| 992 |  |  [BERT Learns to Teach: Knowledge Distillation with Meta Learning](https://doi.org/10.18653/v1/2022.acl-long.485) |  | 0 | We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of... | Canwen Xu, Julian J. McAuley, Wangchunshu Zhou |  |
| 993 |  |  [STEMM: Self-learning with Speech-text Manifold Mixup for Speech Translation](https://doi.org/10.18653/v1/2022.acl-long.486) |  | 0 | How to learn a better speech representation for end-to-end speech-to-text translation (ST) with limited labeled data? Existing techniques often attempt to transfer powerful machine translation (MT) capabilities to ST, but neglect the representation discrepancy across modalities. In this paper, we propose the Speech-TExt Manifold Mixup (STEMM) method to... | Lei Li, Mingxuan Wang, Qingkai Fang, Rong Ye, Yang Feng |  |
| 994 |  |  [Integrating Vectorized Lexical Constraints for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.487) |  | 0 | Lexically constrained neural machine translation (NMT), which controls the generation of NMT models with pre-specified constraints, is important in many practical scenarios. Due to the representation gap between discrete constraints and continuous vectors in NMT models, most existing works choose to construct synthetic data or modify the decoding algorithm to... | Shuo Wang, Yang Liu, Zhixing Tan |  |
| 995 |  |  [MPII: Multi-Level Mutual Promotion for Inference and Interpretation](https://doi.org/10.18653/v1/2022.acl-long.488) |  | 0 | In order to better understand the rationale behind model behavior, recent works have exploited providing interpretation to support the inference prediction. However, existing methods tend to provide human-unfriendly interpretation, and are prone to sub-optimal performance due to one-side promotion, i.e. either inference promotion with interpretation or vice... | Qi Dai, Sanyuan Chen, Yan Liu, Yazheng Yang |  |
| 996 |  |  [StableMoE: Stable Routing Strategy for Mixture of Experts](https://doi.org/10.18653/v1/2022.acl-long.489) |  | 0 | The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during... | Baobao Chang, Bo Zheng, Damai Dai, Furu Wei, Li Dong, Shuming Ma, Zhifang Sui |  |
| 997 |  |  [Boundary Smoothing for Named Entity Recognition](https://doi.org/10.18653/v1/2022.acl-long.490) |  | 0 | Neural named entity recognition (NER) models may easily encounter the over-confidence issue, which degrades the performance and calibration. Inspired by label smoothing and driven by the ambiguity of boundary annotation in NER engineering, we propose boundary smoothing as a regularization technique for span-based neural NER models. It re-assigns entity... | Enwei Zhu, Jinpeng Li |  |
| 998 |  |  [Incorporating Hierarchy into Text Encoder: a Contrastive Learning Approach for Hierarchical Text Classification](https://doi.org/10.18653/v1/2022.acl-long.491) |  | 0 | Hierarchical text classification is a challenging subtask of multi-label classification due to its complex label hierarchy. Existing methods encode text and label hierarchy separately and mix their representations for classification, where the hierarchy remains unchanged for all input text. Instead of modeling them separately, in this work, we propose... | Houfeng Wang, Lianzhe Huang, Peiyi Wang, Xin Sun, Zihan Wang |  |
| 999 |  |  [Signal in Noise: Exploring Meaning Encoded in Random Character Sequences with Character-Aware Language Models](https://doi.org/10.18653/v1/2022.acl-long.492) |  | 0 | Natural language processing models learn word representations based on the distributional hypothesis, which asserts that word context (e.g., co-occurrence) correlates with meaning. We propose that n-grams composed of random character sequences, or garble, provide a novel context for studying word meaning both within and beyond extant language. In particular,... | Bhargav Srinivasa Desikan, Donald Ruggiero Lo Sardo, Douglas Guilbeault, Elise DarraghFord, Ethan O. Nadler, Mark Chu |  |
| 1000 |  |  [Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering](https://doi.org/10.18653/v1/2022.acl-long.493) |  | 0 | To alleviate the data scarcity problem in training question answering systems, recent works propose additional intermediate pre-training for dense passage retrieval (DPR). However, there still remains a large discrepancy between the provided upstream signals and the downstream question-passage relevance, which leads to less improvement. To bridge this gap, we... | Enrui Hu, Fan Yu, Hao Jiang, Jiawei Zhou, Ke Zhan, Lan Luo, Lei Chen, Lifeng Shang, Qun Liu, Xiaoguang Li, Xin Jiang, Xinyu Zhang, Zhao Cao |  |
| 1001 |  |  [AdaLoGN: Adaptive Logic Graph Network for Reasoning-Based Machine Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.494) |  | 0 | Recent machine reading comprehension datasets such as ReClor and LogiQA require performing logical reasoning over text. Conventional neural models are insufficient for logical reasoning, while symbolic reasoners cannot directly apply to text. To meet the challenge, we present a neural-symbolic approach which, to predict an answer, passes messages over a graph... | Gong Cheng, Xiao Li, Yawei Sun, Yuzhong Qu, Ziheng Chen |  |
| 1002 |  |  [CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing](https://doi.org/10.18653/v1/2022.acl-long.495) |  | 0 | Model ensemble is a popular approach to produce a low-variance and well-generalized model. However, it induces large memory and inference costs, which is often not affordable for real-world deployment. Existing work has resorted to sharing weights among models. However, when increasing the proportion of the shared weights, the resulting models tend to be... | Chen Liang, Pengcheng He, Tuo Zhao, Weizhu Chen, Yelong Shen |  |
| 1003 |  |  [Interpretability for Language Learners Using Example-Based Grammatical Error Correction](https://doi.org/10.18653/v1/2022.acl-long.496) |  | 0 | Grammatical Error Correction (GEC) should not focus only on high accuracy of corrections but also on interpretability for language learning. However, existing neural-based GEC models mainly aim at improving accuracy, and their interpretability has not been explored.A promising approach for improving interpretability is an example-based method, which uses... | Ayana Niwa, Masahiro Kaneko, Naoaki Okazaki, Sho Takase |  |
| 1004 |  |  [Rethinking Negative Sampling for Handling Missing Entity Annotations](https://doi.org/10.18653/v1/2022.acl-long.497) |  | 0 | Negative sampling is highly effective in handling missing annotations for named entity recognition (NER). One of our contributions is an analysis on how it makes sense through introducing two insightful concepts: missampling and uncertainty. Empirical studies show low missampling rate and high uncertainty are both essential for achieving promising performances... | Lemao Liu, Shuming Shi, Yangming Li |  |
| 1005 |  |  [Distantly Supervised Named Entity Recognition via Confidence-Based Multi-Class Positive and Unlabeled Learning](https://doi.org/10.18653/v1/2022.acl-long.498) |  | 0 | In this paper, we study the named entity recognition (NER) problem under distant supervision. Due to the incompleteness of the external dictionaries and/or knowledge bases, such distantly annotated training data usually suffer from a high false negative rate. To this end, we formulate the Distantly Supervised NER (DS-NER) problem via Multi-class Positive and... | Kang Zhou, Qi Li, Yuepei Li |  |
| 1006 |  |  [UniXcoder: Unified Cross-Modal Pre-training for Code Representation](https://doi.org/10.18653/v1/2022.acl-long.499) |  | 0 | Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a... | Daya Guo, Jian Yin, Ming Zhou, Nan Duan, Shuai Lu, Yanlin Wang |  |
| 1007 |  |  [One Country, 700+ Languages: NLP Challenges for Underrepresented Languages and Dialects in Indonesia](https://doi.org/10.18653/v1/2022.acl-long.500) |  | 0 | NLP research is impeded by a lack of resources and awareness of the challenges presented by underrepresented languages and dialects. Focusing on the languages spoken in Indonesia, the second most linguistically diverse and the fourth most populous nation of the world, we provide an overview of the current state of NLP research for Indonesia’s 700+ languages. We... | Ade Romadhony, Alham Fikri Aji, David Moeljadi, Fajri Koto, Genta Indra Winata, Jey Han Lau, Kemal Kurniawan, Radityo Eko Prasojo, Rahmad Mahendra, Samuel Cahyawijaya, Sebastian Ruder, Timothy Baldwin |  |
| 1008 |  |  [Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text](https://doi.org/10.18653/v1/2022.acl-long.501) |  | 0 | Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses... | Maxwell Forbes, Noah A. Smith, Rik KoncelKedziorski, Yao Dou, Yejin Choi |  |
| 1009 |  |  [Transkimmer: Transformer Learns to Layer-wise Skim](https://doi.org/10.18653/v1/2022.acl-long.502) |  | 0 | Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior... | Jingwen Leng, Minyi Guo, Yue Guan, Zhengyi Li, Zhouhan Lin |  |
| 1010 |  |  [SkipBERT: Efficient Inference with Shallow Layer Skipping](https://doi.org/10.18653/v1/2022.acl-long.503) |  | 0 | In this paper, we propose SkipBERT to accelerate BERT inference by skipping the computation of shallow layers. To achieve this, our approach encodes small text chunks into independent representations, which are then materialized to approximate the shallow representation of BERT. Since the use of such approximation is inexpensive compared with transformer... | Gang Chen, Jue Wang, Julian J. McAuley, Ke Chen, Lidan Shou |  |
| 1011 |  |  [Pretraining with Artificial Language: Studying Transferable Knowledge in Language Models](https://doi.org/10.18653/v1/2022.acl-long.504) |  | 0 | We investigate what kind of structural knowledge learned in neural network encoders is transferable to processing natural language. We design artificial languages with structural properties that mimic natural language, pretrain encoders on the data, and see how much performance the encoder exhibits on downstream tasks in natural language.Our experimental... | Ryokan Ri, Yoshimasa Tsuruoka |  |
| 1012 |  |  [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://doi.org/10.18653/v1/2022.acl-long.505) |  | 0 | Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity... | Ikuya Yamada, Ryokan Ri, Yoshimasa Tsuruoka |  |
| 1013 |  |  [Evaluating Factuality in Text Simplification](https://doi.org/10.18653/v1/2022.acl-long.506) |  | 0 | Automated simplification models aim to make input texts more readable. Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader. However, such models risk introducing errors into automatically simplified texts, for... | Ashwin Devaraj, Byron C. Wallace, Junyi Jessy Li, William Sheffield |  |
| 1014 |  |  [Requirements and Motivations of Low-Resource Speech Synthesis for Language Revitalization](https://doi.org/10.18653/v1/2022.acl-long.507) |  | 0 | This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien’kéha, Gitksan & SENĆOŦEN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring... | Aidan Pine, Dan Wells, Korin Richmond, Nathan Thanyehténhas Brinklow, Patrick Littell |  |
| 1015 |  |  [Sharpness-Aware Minimization Improves Language Model Generalization](https://doi.org/10.18653/v1/2022.acl-long.508) |  | 0 | The allure of superhuman-level capabilities has led to considerable interest in language models like GPT-3 and T5, wherein the research has, by and large, revolved around new model architectures, training tasks, and loss objectives, along with substantial engineering efforts to scale up model capacity and dataset size. Comparatively little work has been done to... | Dara Bahri, Hossein Mobahi, Yi Tay |  |
| 1016 |  |  [Adversarial Authorship Attribution for Deobfuscation](https://doi.org/10.18653/v1/2022.acl-long.509) |  | 0 | Recent advances in natural language processing have enabled powerful privacy-invasive authorship attribution. To counter authorship attribution, researchers have proposed a variety of rule-based and learning-based text obfuscation approaches. However, existing authorship obfuscation approaches do not consider the adversarial threat model. Specifically, they are... | Jonathan Rusert, Padmini Srinivasan, Wanyue Zhai, Zubair Shafiq |  |
| 1017 |  |  [Weakly Supervised Word Segmentation for Computational Language Documentation](https://doi.org/10.18653/v1/2022.acl-long.510) |  | 0 | Word and morpheme segmentation are fundamental steps of language documentation as they allow to discover lexical units in a language for which the lexicon is unknown. However, in most language documentation scenarios, linguists do not start from a blank page: they may already have a pre-existing dictionary or have initiated manual segmentation of a small part... | François Yvon, Laurent Besacier, Shu Okabe |  |
| 1018 |  |  [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://doi.org/10.18653/v1/2022.acl-long.511) |  | 0 | Existing Natural Language Inference (NLI) datasets, while being instrumental in the advancement of Natural Language Understanding (NLU) research, are not related to scientific text. In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP... | Cornelia Caragea, Mobashir Sadat |  |
| 1019 |  |  [Neural reality of argument structure constructions](https://doi.org/10.18653/v1/2022.acl-long.512) |  | 0 | In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Two decades of... | Bai Li, Frank Rudzicz, Guillaume Thomas, Yang Xu, Zining Zhu |  |
| 1020 |  |  [On the Robustness of Offensive Language Classifiers](https://doi.org/10.18653/v1/2022.acl-long.513) |  | 0 | Social media platforms are deploying machine learning based offensive language classification systems to combat hateful, racist, and other forms of offensive speech at scale. However, despite their real-world deployment, we do not yet comprehensively understand the extent to which offensive language classifiers are robust against adversarial attacks. Prior work... | Jonathan Rusert, Padmini Srinivasan, Zubair Shafiq |  |
| 1021 |  |  [Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings](https://doi.org/10.18653/v1/2022.acl-long.514) |  | 0 | Style transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al. 2021) has attempted “few-shot” style transfer using only 3-10 sentences at inference for style extraction. In this work we study a relevant... | Bidisha Samanta, Deepak Nathani, Kalpesh Krishna, Partha Talukdar, Xavier Garcia |  |
| 1022 |  |  [ABC: Attention with Bounded-memory Control](https://doi.org/10.18653/v1/2022.acl-long.515) |  | 0 | Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token... | Dani Yogatama, Hao Peng, Jungo Kasai, Lingpeng Kong, Nikolaos Pappas, Noah A. Smith, Roy Schwartz, Zhaofeng Wu |  |
| 1023 |  |  [The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail](https://doi.org/10.18653/v1/2022.acl-long.516) |  | 0 | Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field’s successes, often in response to the field’s widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility... | Samuel R. Bowman |  |
| 1024 |  |  [RELiC: Retrieving Evidence for Literary Claims](https://doi.org/10.18653/v1/2022.acl-long.517) |  | 0 | Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an... | Kalpesh Krishna, Katherine Thai, Mohit Iyyer, Yapei Chang |  |
| 1025 |  |  [Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas](https://doi.org/10.18653/v1/2022.acl-long.518) |  | 0 | Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were... | Raphael Schumann, Stefan Riezler |  |
| 1026 |  |  [Adapting Coreference Resolution Models through Active Learning](https://doi.org/10.18653/v1/2022.acl-long.519) |  | 0 | Neural coreference resolution models trained on one dataset may not transfer to new, low-resource domains. Active learning mitigates this problem by sampling a small subset of data for annotators to label. While active learning is well-defined for classification tasks, its application to coreference resolution is neither well-defined nor fully understood. This... | Benjamin Van Durme, Chandler May, Jordan L. BoydGraber, Michelle Yuan, Patrick Xia |  |
| 1027 |  |  [An Imitation Learning Curriculum for Text Editing with Non-Autoregressive Models](https://doi.org/10.18653/v1/2022.acl-long.520) |  | 0 | We propose a framework for training non-autoregressive sequence-to-sequence models for editing tasks, where the original input sequence is iteratively edited to produce the output. We show that the imitation learning algorithms designed to train such models for machine translation introduces mismatches between training and inference that lead to undertraining... | Marine Carpuat, Sweta Agrawal |  |
| 1028 |  |  [Memorisation versus Generalisation in Pre-trained Language Models](https://doi.org/10.18653/v1/2022.acl-long.521) |  | 0 | State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label... | Marek Rei, Michael Tänzer, Sebastian Ruder |  |
| 1029 |  |  [ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments](https://doi.org/10.18653/v1/2022.acl-long.522) |  | 0 | Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth, which is hard to obtain, and requires access to the models of the bots as a form of “white-box testing”. Interactive evaluation mitigates this problem but requires human involvement. In our work, we propose an interactive chatbot evaluation framework in which... | Haifeng Tang, Kenny Q. Zhu, Ruolan Yang, Zitong Li |  |
| 1030 |  |  [Do self-supervised speech models develop human-like perception biases?](https://doi.org/10.18653/v1/2022.acl-long.523) |  | 0 | Self-supervised models for speech processing form representational spaces without using any external labels. Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages. But what kind of representational spaces do these models construct?Human perception... | Ewan Dunbar, Juliette Millet |  |
| 1031 |  |  [Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions](https://doi.org/10.18653/v1/2022.acl-long.524) |  | 0 | A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer... | Eliana Stefani, Jesse Thomason, Jing Gu, Qi Wu, Xin Wang |  |
| 1032 |  |  [Learning to Generate Programs for Table Fact Verification via Structure-Aware Semantic Parsing](https://doi.org/10.18653/v1/2022.acl-long.525) |  | 0 | Table fact verification aims to check the correctness of textual statements based on given semi-structured data. Most existing methods are devoted to better comprehending logical operations and tables, but they hardly study generating latent programs from statements, with which we can not only retrieve evidences efficiently but also explain reasons behind... | Suixin Ou, Yongmei Liu |  |
| 1033 |  |  [Cluster & Tune: Boost Cold Start Performance in Text Classification](https://doi.org/10.18653/v1/2022.acl-long.526) |  | 0 | In real-world scenarios, a text classification task often begins with a cold start, when labeled data is scarce. In such cases, the common practice of fine-tuning pre-trained models, such as BERT, for a target classification task, is prone to produce poor performance. We suggest a method to boost the performance of such models by adding an intermediate... | Alon Halfon, Ariel Gera, Eyal Shnarch, Lena Dankin, Leshem Choshen, Noam Slonim, Ranit Aharonov |  |
| 1034 |  |  [Overcoming a Theoretical Limitation of Self-Attention](https://doi.org/10.18653/v1/2022.acl-long.527) |  | 0 | Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer’s classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get... | David Chiang, Peter Cholak |  |
| 1035 |  |  [Prediction Difference Regularization against Perturbation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.528) |  | 0 | Regularization methods applying input perturbation have drawn considerable attention and have been frequently explored for NMT tasks in recent years. Despite their simplicity and effectiveness, we argue that these methods are limited by the under-fitting of training data. In this paper, we utilize prediction difference for ground-truth tokens to analyze the... | Dengji Guo, Min Zhang, Yang Feng, Zhengrui Ma |  |
| 1036 |  |  [Make the Best of Cross-lingual Transfer: Evidence from POS Tagging with over 100 Languages](https://doi.org/10.18653/v1/2022.acl-long.529) |  | 0 | Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more... | Malvina Nissim, Martijn Wieling, Wietse de Vries |  |
| 1037 |  |  [Should a Chatbot be Sarcastic? Understanding User Preferences Towards Sarcasm Generation](https://doi.org/10.18653/v1/2022.acl-long.530) |  | 0 | Previous sarcasm generation research has focused on how to generate text that people perceive as sarcastic to create more human-like interactions. In this paper, we argue that we should first turn our attention to the question of when sarcasm should be generated, finding that humans consider sarcastic responses inappropriate to many input utterances. Next, we... | Silviu Vlad Oprea, Steven R. Wilson, Walid Magdy |  |
| 1038 |  |  [How Do Seq2Seq Models Perform on End-to-End Data-to-Text Generation?](https://doi.org/10.18653/v1/2022.acl-long.531) |  | 0 | With the rapid development of deep learning, Seq2Seq paradigm has become prevalent for end-to-end data-to-text generation, and the BLEU scores have been increasing in recent years. However, it is widely recognized that there is still a gap between the quality of the texts generated by models and the texts written by human. In order to better understand the... | Xiaojun Wan, Xunjian Yin |  |
| 1039 |  |  [Probing for Labeled Dependency Trees](https://doi.org/10.18653/v1/2022.acl-long.532) |  | 0 | Probing has become an important tool for analyzing representations in Natural Language Processing (NLP). For graphical NLP tasks such as dependency parsing, linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task. This work introduces DepProbe, a linear probe which can extract labeled and directed... | Barbara Plank, Max MüllerEberstein, Rob van der Goot |  |
| 1040 |  |  [DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation](https://doi.org/10.18653/v1/2022.acl-long.533) |  | 0 | Natural language processing (NLP) algorithms have become very successful, but they still struggle when applied to out-of-distribution examples. In this paper we propose a controllable generation approach in order to deal with this domain adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm generates a domain-counterfactual textual... | Amir Feder, Eyal BenDavid, Nitay Calderon, Roi Reichart |  |
| 1041 |  |  [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://doi.org/10.18653/v1/2022.acl-long.534) |  | 0 | Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To... | Jiapeng Wang, Kai Ding, Lianwen Jin |  |
| 1042 |  |  [Dependency-based Mixture Language Models](https://doi.org/10.18653/v1/2022.acl-long.535) |  | 0 | Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and... | Xiaojun Wan, Zhixian Yang |  |
| 1043 |  |  [Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?](https://doi.org/10.18653/v1/2022.acl-long.536) |  | 0 | Identifying argument components from unstructured texts and predicting the relationships expressed among them are two primary steps of argument mining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the... | Dipankar Das, Jeevesh Juneja, Subhabrata Dutta, Tanmoy Chakraborty |  |
| 1044 |  |  [Entity-based Neural Local Coherence Modeling](https://doi.org/10.18653/v1/2022.acl-long.537) |  | 0 | In this paper, we propose an entity-based neural local coherence model which is linguistically more sound than previously proposed neural coherence models. Recent neural coherence models encode the input document using large-scale pretrained language models. Hence their basis for computing local coherence are words and even sub-words. The analysis of their... | Michael Strube, Sungho Jeon |  |
| 1045 |  |  ["That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks](https://doi.org/10.18653/v1/2022.acl-long.538) |  | 0 | Adversarial attacks are a major challenge faced by current machine learning research. These purposely crafted inputs fool even the most advanced models, precluding their deployment in safety-critical applications. Extensive research in computer vision has been carried to develop reliable defense strategies. However, the same issue remains less explored in... | Edoardo Mosca, Georg Groh, Javier RandoRamirez, Shreyash Agarwal |  |
| 1046 |  |  [Local Languages, Third Spaces, and other High-Resource Scenarios](https://doi.org/10.18653/v1/2022.acl-long.539) |  | 0 | How can language technology address the diverse situations of the world’s languages? In one view, languages exist on a resource continuum and the challenge is to scale existing solutions, bringing under-resourced languages into the high-resource world. In another view, presented here, the world’s language ecology includes standardised languages, local... | Steven Bird |  |
| 1047 |  |  [That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory](https://doi.org/10.18653/v1/2022.acl-long.540) |  | 0 | The evolution of language follows the rule of gradual change. Grammar, vocabulary, and lexical semantic shifts take place over time, resulting in a diachronic linguistic gap. As such, a considerable amount of texts are written in languages of different eras, which creates obstacles for natural language processing tasks, such as word segmentation and machine... | Qi Su, Xuemei Tang |  |
| 1048 |  |  [Fair and Argumentative Language Modeling for Computational Argumentation](https://doi.org/10.18653/v1/2022.acl-long.541) |  | 0 | Although much work in NLP has focused on measuring and mitigating stereotypical bias in semantic spaces, research addressing bias in computational argumentation is still in its infancy. In this paper, we address this research gap and conduct a thorough investigation of bias in argumentative language models. To this end, we introduce ABBA, a novel resource for... | Anne Lauscher, Carolin Holtermann, Simone Paolo Ponzetto |  |
| 1049 |  |  [Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation](https://doi.org/10.18653/v1/2022.acl-long.542) |  | 0 | End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is... | Haifeng Wang, Hua Wu, Ruiqing Zhang, Zhongjun He |  |
| 1050 |  |  [Can Pre-trained Language Models Interpret Similes as Smart as Human?](https://doi.org/10.18653/v1/2022.acl-long.543) |  | 0 | Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named... | Qianyu He, Rui Xie, Sijie Cheng, Yanghua Xiao, Zhixu Li |  |
| 1051 |  |  [CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark](https://doi.org/10.18653/v1/2022.acl-long.544) |  | 0 | Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to... | Baobao Chang, Buzhou Tang, Chuanqi Tan, Fei Huang, Guotong Xie, Hongying Zan, Hui Zong, Jian Xu, Jun Yan, Kangping Yin, Kunli Zhang, Lei Li, Linfeng Li, Luo Si, Mosha Chen, Ningyu Zhang, Qingcai Chen, Xiaozhuan Liang, Xin Shang, Yuan Ni, Zhen Bi, Zheng Yuan, Zhifang Sui |  |
| 1052 |  |  [Learning Non-Autoregressive Models from Search for Unsupervised Sentence Summarization](https://doi.org/10.18653/v1/2022.acl-long.545) |  | 0 | Text summarization aims to generate a short summary for an input text. In this work, we propose a Non-Autoregressive Unsupervised Summarization (NAUS) approach, which does not require parallel data for training. Our NAUS first performs edit-based search towards a heuristically defined score, and generates a summary as pseudo-groundtruth. Then, we train an... | Chenyang Huang, Lili Mou, Puyuan Liu |  |
| 1053 |  |  [Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.546) |  | 0 | The principal task in supervised neural machine translation (NMT) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs, and thus produce a model capable of generalizing to unseen instances. However, it is commonly observed that the generalization performance of the model is highly influenced by the... | Heng Yu, Rong Jin, Rongxiang Weng, Weihua Luo, Xiangpeng Wei, Yue Hu |  |
| 1054 |  |  [Lexical Knowledge Internalization for Neural Dialog Generation](https://doi.org/10.18653/v1/2022.acl-long.547) |  | 0 | We propose knowledge internalization (KI), which aims to complement the lexical knowledge into neural dialog models. Instead of further conditioning the knowledge-grounded dialog (KGD) models on externally retrieved knowledge, we seek to integrate knowledge about each input token internally into the model’s parameters. To tackle the challenge due to the large... | Ben Kao, Lingpeng Kong, Wei Bi, Xiang Li, Zhiyong Wu |  |
| 1055 |  |  [Modeling Syntactic-Semantic Dependency Correlations in Semantic Role Labeling Using Mixture Models](https://doi.org/10.18653/v1/2022.acl-long.548) |  | 0 | In this paper, we propose a mixture model-based end-to-end method to model the syntactic-semantic dependency correlation in Semantic Role Labeling (SRL). Semantic dependencies in SRL are modeled as a distribution over semantic dependency labels conditioned on a predicate and an argument word. The semantic label distribution varies depending on Shortest... | Junjie Chen, Xiangheng He, Yusuke Miyao |  |
| 1056 |  |  [Learning the Beauty in Songs: Neural Singing Voice Beautifier](https://doi.org/10.18653/v1/2022.acl-long.549) |  | 0 | We are interested in a novel task, singing voice beautification (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic... | Chengxi Li, Jinglin Liu, Yi Ren, Zhiying Zhu, Zhou Zhao |  |
| 1057 |  |  [A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.550) |  | 0 | Towards building intelligent dialogue agents, there has been a growing interest in introducing explicit personas in generation models. However, with limited persona-based dialogue data at hand, it may be difficult to train a dialogue generation model well. We point out that the data challenges of this generation task lie in two aspects: first, it is expensive... | Dacheng Tao, Meng Fang, Shuming Shi, Wei Bi, Yu Cao |  |
| 1058 |  |  [LinkBERT: Pretraining Language Models with Document Links](https://doi.org/10.18653/v1/2022.acl-long.551) |  | 0 | Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks.... | Jure Leskovec, Michihiro Yasunaga, Percy Liang |  |
| 1059 |  |  [Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs](https://doi.org/10.18653/v1/2022.acl-long.552) |  | 0 | Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., “Who was the president of the US before Obama?”). These questions often involve three time-related challenges that previous work fail to adequately... | Chao Shang, Guangtao Wang, Jing Huang, Peng Qi |  |
| 1060 |  |  [Self-supervised Semantic-driven Phoneme Discovery for Zero-resource Speech Recognition](https://doi.org/10.18653/v1/2022.acl-long.553) |  | 0 | Phonemes are defined by their relationship to words: changing a phoneme changes the word. Learning a phoneme inventory with little supervision has been a longstanding challenge with important applications to under-resourced speech technology. In this paper, we bridge the gap between the linguistic and statistical definition of phonemes and propose a novel... | Chang Dong Yoo, Liming Wang, Mark HasegawaJohnson, Siyuan Feng |  |
| 1061 |  |  [Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions](https://doi.org/10.18653/v1/2022.acl-long.554) |  | 0 | Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability... | Andrew McCallum, HawShiuan Chang |  |
| 1062 |  |  [Ditch the Gold Standard: Re-evaluating Conversational Question Answering](https://doi.org/10.18653/v1/2022.acl-long.555) |  | 0 | Conversational question answering aims to provide natural-language answers to users in information-seeking conversations. Existing conversational QA benchmarks compare models with pre-collected human-human conversations, using ground-truth answers provided in conversational history. It remains unclear whether we can rely on this static evaluation for model... | Danqi Chen, Huihan Li, Manan Goenka, Tianyu Gao |  |
| 1063 |  |  [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://doi.org/10.18653/v1/2022.acl-long.556) |  | 0 | When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess... | Alastair Moore, Max Bartolo, Pontus Stenetorp, Sebastian Riedel, Yao Lu |  |
| 1064 |  |  [Situated Dialogue Learning through Procedural Environment Generation](https://doi.org/10.18653/v1/2022.acl-long.557) |  | 0 | We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of... | Mark O. Riedl, Prithviraj Ammanabrolu, Renee Jia |  |
| 1065 |  |  [UniTE: Unified Translation Evaluation](https://doi.org/10.18653/v1/2022.acl-long.558) |  | 0 | Translation quality evaluation plays a crucial role in machine translation. According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of them. This limits the convenience of these methods,... | Baosong Yang, Boxing Chen, Dayiheng Liu, Derek F. Wong, Haibo Zhang, Lidia S. Chao, Yu Wan |  |
| 1066 |  |  [Program Transfer for Answering Complex Questions over Knowledge Bases](https://doi.org/10.18653/v1/2022.acl-long.559) |  | 0 | Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually... | Jiaxin Shi, Jifan Yu, Jinghui Xiao, Juanzi Li, Lei Hou, Shulin Cao, Xin Lv, Zhiyuan Liu, Zijun Yao |  |
| 1067 |  |  [EAG: Extract and Generate Multi-way Aligned Corpus for Complete Multi-lingual Neural Machine Translation](https://doi.org/10.18653/v1/2022.acl-long.560) |  | 0 | Complete Multi-lingual Neural Machine Translation (C-MNMT) achieves superior performance against the conventional MNMT by constructing multi-way aligned corpus, i.e., aligning bilingual training examples from different language pairs when either their source or target sides are identical. However, since exactly identical sentences from different language pairs... | Fandong Meng, Jie Zhou, Yulin Xu, Zhen Yang |  |
| 1068 |  |  [Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings](https://doi.org/10.18653/v1/2022.acl-long.561) |  | 0 | Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1)... | Ge Wang, Guojiang Zhao, Jiangbin Zheng, Jun Xia, Stan Z. Li, Yile Wang, Yue Zhang, Yufei Huang |  |
| 1069 |  |  [Multimodal Sarcasm Target Identification in Tweets](https://doi.org/10.18653/v1/2022.acl-long.562) |  | 0 | Sarcasm is important to sentiment analysis on social media. Sarcasm Target Identification (STI) deserves further study to understand sarcasm in depth. However, text lacking context or missing sarcasm target makes target identification very difficult. In this paper, we introduce multimodality to STI and present Multimodal Sarcasm Target Identification (MSTI)... | Jiquan Wang, Lin Sun, Meizhi Shao, Yi Liu, Zengwei Zheng |  |
| 1070 |  |  [Flexible Generation from Fragmentary Linguistic Input](https://doi.org/10.18653/v1/2022.acl-long.563) |  | 0 | The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models. But does direct specialization capture how humans approach novel language tasks? We hypothesize that human performance is better characterized by flexible inference through... | Peng Qian, Roger Levy |  |
| 1071 |  |  [Revisiting Over-Smoothness in Text to Speech](https://doi.org/10.18653/v1/2022.acl-long.564) |  | 0 | Non-autoregressive text to speech (NAR-TTS) models have attracted much attention from both academia and industry due to their fast generation speed. One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms, and thus cause blurry and over-smoothed results. In this work, we revisit... | Tao Qin, TieYan Liu, Xu Tan, Yi Ren, Zhou Zhao |  |
| 1072 |  |  [Coherence boosting: When your pretrained language model is not paying enough attention](https://doi.org/10.18653/v1/2022.acl-long.565) |  | 0 | Long-range semantic coherence remains a challenge in automatic language generation and understanding. We demonstrate that large language models have insufficiently learned the effect of distant words on next-token prediction. We present coherence boosting, an inference procedure that increases a LM’s focus on a long context. We show the benefits of coherence... | Nebojsa Jojic, Nikolay Malkin, Zhen Wang |  |
| 1073 |  |  [Uncertainty Estimation of Transformer Predictions for Misclassification Detection](https://doi.org/10.18653/v1/2022.acl-long.566) |  | 0 | Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been... | Akim Tsvigun, Alexander Panchenko, Artem Shelmanov, Artem Vazhentsev, Evgenii Tsymbalov, Gleb Gusev, Gleb Kuzmin, Kirill Fedyanin, Leonid Zhukov, Manvel Avetisian, Maxim Panov, Mikhail Burtsev |  |
| 1074 |  |  [VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena](https://doi.org/10.18653/v1/2022.acl-long.567) |  | 0 | We propose VALSE (Vision And Language Structured Evaluation), a novel benchmark designed for testing general-purpose pretrained vision and language (V&L) models for their visio-linguistic grounding capabilities on specific linguistic phenomena. VALSE offers a suite of six tests covering various linguistic constructs. Solving these requires models to ground... | Albert Gatt, Anette Frank, Iacer Calixto, Letitia Parcalabescu, Lilitta Muradjan, Michele Cafagna |  |
| 1075 |  |  [The Grammar-Learning Trajectories of Neural Language Models](https://doi.org/10.18653/v1/2022.acl-long.568) |  | 0 | The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make.... | Daphna Weinshall, Guy Hacohen, Leshem Choshen, Omri Abend |  |
| 1076 |  |  [Generating Scientific Definitions with Controllable Complexity](https://doi.org/10.18653/v1/2022.acl-long.569) |  | 0 | Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of generated definitions as a way of adapting to a specific... | Katharina Reinecke, Noah A. Smith, Tal August |  |
| 1077 |  |  [Label Semantic Aware Pre-training for Few-shot Text Classification](https://doi.org/10.18653/v1/2022.acl-long.570) |  | 0 | In text classification tasks, useful information is encoded in the label names. Label semantic aware systems have leveraged this information for improved text classification performance during fine-tuning and prediction. However, use of label-semantics during pre-training has not been extensively explored. We therefore propose Label Semantic Aware Pre-training... | Aaron Mueller, Dan Roth, Elman Mansimov, Jason Krone, Saab Mansour, Salvatore Romeo, Yi Zhang |  |
| 1078 |  |  [ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation](https://doi.org/10.18653/v1/2022.acl-long.571) |  | 0 | Residual networks are an Euler discretization of solutions to Ordinary Differential Equations (ODE). This paper explores a deeper relationship between Transformer and numerical ODE methods. We first show that a residual block of layers in Transformer can be described as a higher-order solution to ODE. Inspired by this, we design a new architecture, ODE... | Bei Li, Jingbo Zhu, Min Zhang, Quan Du, Shuhan Zhou, Tao Zhou, Tong Xiao, Xin Zeng, Xuebo Liu, Yi Jing |  |
| 1079 |  |  [A Comparison of Strategies for Source-Free Domain Adaptation](https://doi.org/10.18653/v1/2022.acl-long.572) |  | 0 | Data sharing restrictions are common in NLP, especially in the clinical domain, but there is limited research on adapting models to new domains without access to the original training data, a setting known as source-free domain adaptation. We take algorithms that traditionally assume access to the source-domain training data—active learning, self-training, and... | Steven Bethard, Xin Su, Yiyun Zhao |  |
| 1080 |  |  [Ethics Sheets for AI Tasks](https://doi.org/10.18653/v1/2022.acl-long.573) |  | 0 | Several high-profile events, such as the mass testing of emotion recognition systems on vulnerable sub-populations and using question answering systems to make moral judgments, have highlighted how technology will often lead to more adverse outcomes for those that are already marginalized. At issue here are not just individual systems and datasets, but also the... | Saif M. Mohammad |  |
| 1081 |  |  [Learning Disentangled Representations of Negation and Uncertainty](https://doi.org/10.18653/v1/2022.acl-long.574) |  | 0 | Negation and uncertainty modeling are long-standing tasks in natural language processing. Linguistic theory postulates that expressions of negation and uncertainty are semantically independent from each other and the content they modify. However, previous works on representation learning do not explicitly model this independence. We therefore attempt to... | Chrysoula Zerva, Jake Vasilakes, Makoto Miwa, Sophia Ananiadou |  |
| 1082 |  |  [latent-GLAT: Glancing at Latent Variables for Parallel Text Generation](https://doi.org/10.18653/v1/2022.acl-long.575) |  | 0 | Recently, parallel text generation has received widespread attention due to its success in generation efficiency. Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In... | Dongqi Wang, Hao Zhou, Jiajun Chen, Lei Li, Lihua Qian, Shujian Huang, Xinyu Dai, Yu Bao |  |
| 1083 |  |  [PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://doi.org/10.18653/v1/2022.acl-long.576) |  | 0 | Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting large-scale PLMs to downstream tasks. However, prompt tuning is... | Minlie Huang, Xu Han, Yuxian Gu, Zhiyuan Liu |  |
| 1084 |  |  [Deduplicating Training Data Makes Language Models Better](https://doi.org/10.18653/v1/2022.acl-long.577) |  | 0 | We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61... | Andrew Nystrom, Chiyuan Zhang, Chris CallisonBurch, Daphne Ippolito, Douglas Eck, Katherine Lee, Nicholas Carlini |  |
| 1085 |  |  [Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires](https://doi.org/10.18653/v1/2022.acl-long.578) |  | 0 | Automated methods have been widely used to identify and analyze mental health conditions (e.g., depression) from various sources of information, including social media. Yet, deployment of such models in real-world healthcare applications faces challenges including poor out-of-domain generalization and lack of trust in black box models. In this work, we propose... | Andrew Yates, Arman Cohan, Ayah Zirikly, Bart Desmet, Thong Nguyen |  |
| 1086 |  |  [Internet-Augmented Dialogue Generation](https://doi.org/10.18653/v1/2022.acl-long.579) |  | 0 | The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al.,... | Jason Weston, Kurt Shuster, Mojtaba Komeili |  |
| 1087 |  |  [SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities](https://doi.org/10.18653/v1/2022.acl-long.580) |  | 0 | Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years. In speech, a model pre-trained by self-supervised learning transfers remarkably well on multiple tasks. However, the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such... | Abdelrahman Mohamed, Andy T. Liu, ChengI Lai, HengJui Chang, HsiangSheng Tsai, HsuanJui Chen, Hungyi Lee, Jiatong Shi, Kushal Lakhotia, Phil Hall, ShangWen Li, Shinji Watanabe, ShuWen Yang, Shuyan Dong, WenChin Huang, Xuankai Chang, Zili Huang |  |
| 1088 |  |  [Knowledge Neurons in Pretrained Transformers](https://doi.org/10.18653/v1/2022.acl-long.581) |  | 0 | Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a... | Baobao Chang, Damai Dai, Furu Wei, Li Dong, Yaru Hao, Zhifang Sui |  |
| 1089 |  |  [Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing](https://doi.org/10.18653/v1/2022.acl-long.582) |  | 0 | Meta-learning, or learning to learn, is a technique that can help to overcome resource scarcity in cross-lingual NLP problems, by enabling fast adaptation to new tasks. We apply model-agnostic meta-learning (MAML) to the task of cross-lingual dependency parsing. We train our model on a diverse set of languages to learn a parameter initialization that can adapt... | Anna Langedijk, Bryan Cardenas Guevara, Ekaterina Shutova, Helen Yannakoudakis, Phillip Lippe, Sander Bos, Verna Dankers |  |
| 1090 |  |  [French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English](https://doi.org/10.18653/v1/2022.acl-long.583) |  | 0 | Warning: This paper contains explicit statements of offensive stereotypes which may be upsetting. Much work on biases in natural language processing has addressed biases linked to the social and cultural experience of English speaking individuals in the United States. We seek to widen the scope of bias studies by creating material to measure social bias in... | Aurélie Névéol, Julien Bezançon, Karën Fort, Yoann Dupont |  |
| 1091 |  |  [Few-Shot Learning with Siamese Networks and Label Tuning](https://doi.org/10.18653/v1/2022.acl-long.584) |  | 0 | We study the problem of building text classifiers with little or no training data, commonly known as zero and few-shot text classification. In recent years, an approach based on neural textual entailment models has been found to give strong results on a diverse range of tasks. In this work, we show that with proper pre-training, Siamese Networks that embed... | Guillermo PérezTorró, Marc FrancoSalvador, Thomas Müller |  |
| 1092 |  |  [Inferring Rewards from Language in Context](https://doi.org/10.18653/v1/2022.acl-long.585) |  | 0 | In classic instruction following, language like “I’d like the JetBlue flight” maps to actions (e.g., selecting that flight). However, language also conveys information about a user’s underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards... | Anca D. Dragan, Dan Klein, Daniel Fried, Jessy Lin |  |
| 1093 |  |  [Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies](https://doi.org/10.18653/v1/2022.acl-long.586) |  | 0 | Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting... | Angela Fan, Claire Gardent |  |
| 1094 |  |  [Your Answer is Incorrect... Would you like to know why? Introducing a Bilingual Short Answer Feedback Dataset](https://doi.org/10.18653/v1/2022.acl-long.587) |  | 0 | Handing in a paper or exercise and merely receiving “bad” or “incorrect” as feedback is not very helpful when the goal is to improve. Unfortunately, this is currently the kind of feedback given by Automatic Short Answer Grading (ASAG) systems. One of the reasons for this is a lack of content-focused elaborated feedback datasets. To encourage research on... | Anna Filighera, Sebastian Ochs, Siddharth Parihar, Tim Steuer, Tobias Meuser |  |
| 1095 |  |  [Towards Better Characterization of Paraphrases](https://doi.org/10.18653/v1/2022.acl-long.588) |  | 0 | To effectively characterize the nature of paraphrase pairs without expert human annotation, we proposes two new metrics: word position deviation (WPD) and lexical deviation (LD). WPD measures the degree of structural alteration, while LD measures the difference in vocabulary used. We apply these metrics to better understand the commonly-used MRPC dataset and... | De Wen Soh, Timothy Liu |  |
| 1096 |  |  [SummScreen: A Dataset for Abstractive Screenplay Summarization](https://doi.org/10.18653/v1/2022.acl-long.589) |  | 0 | We introduce SummScreen, a summarization dataset comprised of pairs of TV series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be... | Kevin Gimpel, Mingda Chen, Sam Wiseman, Zewei Chu |  |
| 1097 |  |  [Sparsifying Transformer Models with Trainable Representation Pooling](https://doi.org/10.18653/v1/2022.acl-long.590) |  | 0 | We propose a novel method to sparsify attention in the Transformer model by learning to select the most-informative token representations during the training process, thus focusing on the task-specific parts of an input. A reduction of quadratic time and memory complexity to sublinear was achieved due to a robust trainable top-k operator.Our experiments on a... | Lukasz Borchmann, Lukasz Garncarek, Michal Pietruszka |  |
| 1098 |  |  [Uncertainty Determines the Adequacy of the Mode and the Tractability of Decoding in Sequence-to-Sequence Models](https://doi.org/10.18653/v1/2022.acl-long.591) |  | 0 | In many natural language processing (NLP) tasks the same input (e.g. source sentence) can have multiple possible outputs (e.g. translations). To analyze how this ambiguity (also known as intrinsic uncertainty) shapes the distribution learned by neural sequence models we measure sentence-level uncertainty by computing the degree of overlap between references in... | Felix Stahlberg, Ilia Kulikov, Shankar Kumar |  |
| 1099 |  |  [FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning](https://doi.org/10.18653/v1/2022.acl-long.592) |  | 0 | Most previous methods for text data augmentation are limited to simple tasks and weak baselines. We explore data augmentation on hard tasks (i.e., few-shot natural language understanding) and strong baselines (i.e., pretrained models with over one billion parameters). Under this setting, we reproduced a large number of previous augmentation methods and found... | Jie Tang, Jing Zhou, Li Jian, Yanan Zheng, Zhilin Yang |  |
| 1100 |  |  [Text-Free Prosody-Aware Generative Spoken Language Modeling](https://doi.org/10.18653/v1/2022.acl-long.593) |  | 0 | Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a... | Abdelrahman Mohamed, Adam Polyak, Ann Lee, Emmanuel Dupoux, Eugene Kharitonov, Jade Copet, Kushal Lakhotia, Morgane Rivière, Tu Anh Nguyen, WeiNing Hsu, Yossi Adi |  |
| 1101 |  |  [Lite Unified Modeling for Discriminative Reading Comprehension](https://doi.org/10.18653/v1/2022.acl-long.594) |  | 0 | As a broad and major category in machine reading comprehension (MRC), the generalized goal of discriminative MRC is answer prediction from the given materials. However, the focuses of various discriminative MRC tasks may be diverse enough: multi-choice MRC requires model to highlight and integrate all potential critical evidence globally; while extractive MRC... | Hai Zhao, Libin Shen, Yilin Zhao, Yinggong Zhao |  |
| 1102 |  |  [Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining](https://doi.org/10.18653/v1/2022.acl-long.595) |  | 0 | This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingual texts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. We thus introduce dual-pivot transfer: training on one... | Chihchan Tien, Shane SteinertThrelkeld |  |
| 1103 |  |  [End-to-End Modeling via Information Tree for One-Shot Natural Language Spatial Video Grounding](https://doi.org/10.18653/v1/2022.acl-long.596) |  | 0 | Natural language spatial video grounding aims to detect the relevant objects in video frames with descriptive sentences as the query. In spite of the great advances, most existing methods rely on dense video frame annotations, which require a tremendous amount of human effort. To achieve effective grounding under a limited annotation budget, we investigate... | Fei Wu, Haoyu Zhang, Jiaxu Miao, Jin Wang, Mengze Li, Peng Wang, Shengyu Zhang, Shiliang Pu, Tianbao Wang, Wenming Tan, Wenqiao Zhang, Zhou Zhao |  |
| 1104 |  |  [RNSum: A Large-Scale Dataset for Automatic Release Note Generation via Commit Logs Summarization](https://doi.org/10.18653/v1/2022.acl-long.597) |  | 0 | A release note is a technical document that describes the latest changes to a software product and is crucial in open source software development. However, it still remains challenging to generate release notes automatically. In this paper, we present a new dataset called RNSum, which contains approximately 82,000 English release notes and the associated commit... | Hideki Nakayama, Hisashi Kamezawa, Nobuyuki Shimizu, Noriki Nishida, Takashi Miyazaki |  |
| 1105 |  |  [Improving Machine Reading Comprehension with Contextualized Commonsense Knowledge](https://doi.org/10.18653/v1/2022.acl-long.598) |  | 0 | To perform well on a machine reading comprehension (MRC) task, machine readers usually require commonsense knowledge that is not explicitly mentioned in the given documents. This paper aims to extract a new kind of structured knowledge from scripts and use it to improve MRC. We focus on scripts as they contain rich verbal and nonverbal messages, and two... | Claire Cardie, Dian Yu, Dong Yu, Jianshu Chen, Kai Sun |  |
| 1106 |  |  [Modeling Persuasive Discourse to Adaptively Support Students' Argumentative Writing](https://doi.org/10.18653/v1/2022.acl-long.599) |  | 0 | We introduce an argumentation annotation approach to model the structure of argumentative discourse in student-written business model pitches. Additionally, the annotation scheme captures a series of persuasiveness scores such as the specificity, strength, evidence, and relevance of the pitch and the individual components. Based on this scheme, we annotated a... | Christina Niklaus, Thiemo Wambsganss |  |
| 1107 |  |  [Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons](https://doi.org/10.18653/v1/2022.acl-long.600) |  | 0 | Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given k systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all k \choose 2 pairs of systems. However, this can be very expensive as the number of human annotations... | Akash Kumar Mohankumar, Mitesh M. Khapra |  |
| 1108 |  |  [The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments](https://doi.org/10.18653/v1/2022.acl-long.601) |  | 0 | An audience’s prior beliefs and morals are strong indicators of how likely they will be affected by a given argument. Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement. In argumentation technology, however, this is barely exploited so far. This paper studies the feasibility of automatically generating... | Henning Wachsmuth, Milad Alshomary, Roxanne El Baff, Timon Gurcke |  |
| 1109 |  |  [Pyramid-BERT: Reducing Complexity via Successive Core-set based Token Selection](https://doi.org/10.18653/v1/2022.acl-long.602) |  | 0 | Transformer-based language models such as BERT (CITATION) have achieved the state-of-the-art performance on various NLP tasks, but are computationally prohibitive. A recent line of works use various heuristics to successively shorten sequence length while transforming tokens through encoders, in tasks such as classification and ranking that require a single... | Ashish Khetan, Rene Bidart, Xin Huang, Zohar Karnin |  |
| 1110 |  |  [Probing for the Usage of Grammatical Number](https://doi.org/10.18653/v1/2022.acl-long.603) |  | 0 | A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious—i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a... | Alessandro Lenci, Karim Lasri, Ryan Cotterell, Thierry Poibeau, Tiago Pimentel |  |
