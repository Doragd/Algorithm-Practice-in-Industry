# KDD2024

## 会议论文列表

本会议共有 620 篇论文

| 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- |
|  |  [Cross-Domain LifeLong Sequential Modeling for Online Click-Through Rate Prediction](https://doi.org/10.1145/3637528.3671601) |  | 0 | Lifelong sequential modeling (LSM) has significantly advanced recommendation systems on social media platforms. Diverging from single-domain LSM, cross-domain LSM involves modeling lifelong behavior sequences from a source domain to a different target domain. In this paper, we propose the Lifelong Cross Network (LCN), a novel approach for cross-domain LSM. LCN features a Cross Representation Production (CRP) module that utilizes contrastive loss to improve the learning of item embeddings, effectively bridging items across domains. This is important for enhancing the retrieval of relevant items in cross-domain lifelong sequences. Furthermore, we propose the Lifelong Attention Pyramid (LAP) module, which contains three cascading attention levels. By adding an intermediate level and integrating the results from all three levels, the LAP module can capture a broad spectrum of user interests and ensure gradient propagation throughout the sequence. The proposed LAP can also achieve remarkable consistency across attention levels, making it possible to further narrow the candidate item pool of the top level. This allows for the use of advanced attention techniques to effectively mitigate the impact of the noise in cross-domain sequences and improve the non-linearity of the representation, all while maintaining computational efficiency. Extensive experiments conducted on both a public dataset and an industrial dataset from the WeChat Channels platform reveal that the LCN outperforms current methods in terms of prediction accuracy and online performance metrics. | Ruijie Hou, Zhaoyang Yang, Ming Yu, Hongyu Lu, Zhuobin Zheng, Yu Chen, Qinsong Zeng, Ming Chen | Wechat, Tencent, Beijing, China; Wechat, Tencent, Guangzhou, China |
|  |  [Mitigating Pooling Bias in E-commerce Search via False Negative Estimation](https://doi.org/10.1145/3637528.3671630) |  | 0 | Efficient and accurate product relevance assessment is critical for user experiences and business success. Training a proficient relevance assessment model requires high-quality query-product pairs, often obtained through negative sampling strategies. Unfortunately, current methods introduce pooling bias by mistakenly sampling false negatives, diminishing performance and business impact. To address this, we present Bias-mitigating Hard Negative Sampling (BHNS), a novel negative sampling strategy tailored to identify and adjust for false negatives, building upon our original False Negative Estimation algorithm. Our experiments in the Instacart search setting confirm BHNS as effective for practical e-commerce use. Furthermore, comparative analyses on public dataset showcase its domain-agnostic potential for diverse applications. | Xiaochen Wang, Xiao Xiao, Ruhan Zhang, Xuan Zhang, Taesik Na, Tejaswi Tenneti, Haixun Wang, Fenglong Ma | The Pennsylvania State University, University Park, PA, USA; Instacart, San Francisco, CA, USA |
|  |  [Automatic Multi-Task Learning Framework with Neural Architecture Search in Recommendations](https://doi.org/10.1145/3637528.3671715) |  | 0 | Multi-task learning (MTL), which aims to make full use of knowledge contained in multiple tasks to enhance overall performance and efficiency, has been broadly applied in recommendations. The main challenge for MTL models is negative transfer. Existing MTL models, mainly built on the Mixture-of-Experts (MoE) structure, seek enhancements in performance through feature selection and specific expert sharing mode design. However, one expert sharing mode may not be universally applicable due to the complex correlations and diverse demands among various tasks. Additionally, homogeneous expert architectures in such models further limit their performance. To address these issues, in this paper, we propose an innovative automatic MTL framework, AutoMTL, leveraging neural architecture search (NAS) to design optimal expert architectures and sharing modes. The Dual-level Expert Sharing mode and Architecture Navigator (DESAN) search space of AutoMTL can not only efficiently explore expert sharing modes and feature selection schemes but also focus on the architectures of expert subnetworks. Along with this, we introduce an efficient Progressively Discretizing Differentiable Architecture Search (PD-DARTS) algorithm for search space exploration. Extensive experiments demonstrate that AutoMTL can consistently outperform state-of-the-art, human-crafted MTL models. Moreover, the insights obtained from the discovered architectures provide valuable guidance for building new multi-task recommendation models. | Shen Jiang, Guanghui Zhu, Yue Wang, Chunfeng Yuan, Yihua Huang | State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China |
|  |  [CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation](https://doi.org/10.1145/3637528.3671901) |  | 0 | The long-tail recommendation is a challenging task for traditionalrecommender systems, due to data sparsity and data imbalance issues. The recentdevelopment of large language models (LLMs) has shown their abilities incomplex reasoning, which can help to deduce users' preferences based on veryfew previous interactions. However, since most LLM-based systems rely on items'semantic meaning as the sole evidence for reasoning, the collaborativeinformation of user-item interactions is neglected, which can cause the LLM'sreasoning to be misaligned with task-specific collaborative information of thedataset. To further align LLMs' reasoning to task-specific user-iteminteraction knowledge, we introduce collaborative retrieval-augmented LLMs,CoRAL, which directly incorporate collaborative evidence into the prompts.Based on the retrieved user-item interactions, the LLM can analyze shared anddistinct preferences among users, and summarize the patterns indicating whichtypes of users would be attracted by certain items. The retrieved collaborativeevidence prompts the LLM to align its reasoning with the user-item interactionpatterns in the dataset. However, since the capacity of the input prompt islimited, finding the minimally-sufficient collaborative information forrecommendation tasks can be challenging. We propose to find the optimalinteraction set through a sequential decision-making process and develop aretrieval policy learned through a reinforcement learning (RL) framework,CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'reasoning abilities on specific recommendation tasks. Our analysis also revealsthat CoRAL can more efficiently explore collaborative information throughreinforcement learning. | Junda Wu, ChengChun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng Hou, Julian J. McAuley | Adobe Research, San Jose, CA, USA; University of California San Diego, La Jolla, CA, USA; Columbia University, New York, NY, USA |
|  |  [Text Matching Indexers in Taobao Search](https://doi.org/10.1145/3637528.3671654) |  | 0 | Product search is an important service on Taobao, the largest e-commerce platform in China. Through this service, users can easily find products relevant to their specific needs. Coping with billion-size query loads, Taobao product search has traditionally relied on classical term-based retrieval models due to their powerful and interpretable indexes. In essence, efficient retrieval hinges on the proper storage of the inverted index. Recent successes involve reducing the size (pruning) of the inverted index but the construction and deployment of lossless static index pruning in practical product search still pose non-trivial challenges. In this work, we introduce a novel SM art INDexing (SMIND) solution in Taobao product search. SMIND is designed to reduce information loss during the static pruning process by incorporating user search preferences. Specifically, we first construct "user-query-item'' hypergraphs for four different search preferences, namely purchase, click, exposure, and relevance. Then, we develop an efficient TermRank algorithm applied to these hypergraphs, to preserve relevant items based on specific user preferences during the pruning of the inverted indexer. Our approach offers fresh insights into the field of product search, emphasizing that term dependencies in user search preferences go beyond mere text relevance. Moreover, to address the vocabulary mismatch problem inherent in term-based models, we also incorporate an multi-granularity semantic retrieval model to facilitate semantic matching. Empirical results from both offline evaluation and online A/B tests showcase the superiority of SMIND over state-of-the-art methods, especially in commerce metrics with significant improvements of 1.34% in Pay Order Count and 1.50% in Gross Merchandise Value. Besides, SMIND effectively mitigates the Matthew effect of user queries and has been in service for hundreds of millions of daily users since November 2022. | Sen Li, Fuyu Lv, Ruqing Zhang, Dan Ou, Zhixuan Zhang, Maarten de Rijke | CAS Key Lab of Network Data Science and Technology, ICT, CAS, Beijing, China; Alibaba Group, Hangzhou, China; University of Amsterdam, Amsterdam, Netherlands |
|  |  [Unified Low-rank Compression Framework for Click-through Rate Prediction](https://doi.org/10.1145/3637528.3671520) |  | 0 | Deep Click-Through Rate (CTR) prediction models play an important role in modern industrial recommendation scenarios. However, high memory overhead and computational costs limit their deployment in resource-constrained environments. Low-rank approximation is an effective method for computer vision and natural language processing models, but its application in compressing CTR prediction models has been less explored. Due to the limited memory and computing resources, compression of CTR prediction models often confronts three fundamental challenges, i.e., (1). How to reduce the model sizes to adapt to edge devices? (2). How to speed up CTR prediction model inference? (3). How to retain the capabilities of original models after compression? Previous low-rank compression research mostly uses tensor decomposition, which can achieve a high parameter compression ratio, but brings in AUC degradation and additional computing overhead. To address these challenges, we propose a unified low-rank decomposition framework for compressing CTR prediction models. We find that even with the most classic matrix decomposition SVD method, our framework can achieve better performance than the original model. To further improve the effectiveness of our framework, we locally compress the output features instead of compressing the model weights. Our unified low-rank compression framework can be applied to embedding tables and MLP layers in various CTR prediction models. Extensive experiments on two academic datasets and one real industrial benchmark demonstrate that, with 3--5× model size reduction, our compressed models can achieve both faster inference and higher AUC than the uncompressed original models. Our code is at https://github.com/yuhao318/Atomic_Feature_Mimicking. | Hao Yu, Minghao Fu, Jiandong Ding, Yusheng Zhou, Jianxin Wu | Researcher, Shanghai, China; Nanjing University, Nanjing, Jiangsu, China |
|  |  [Optimizing Smartphone App Usage Prediction: A Click-Through Rate Ranking Approach](https://doi.org/10.1145/3637528.3671567) |  | 0 | Over the past decade, smartphones have become indispensable personal mobile devices, experiencing a remarkable surge in software apps. These apps empower users to seamlessly connect with various internet services, such as social communication and online shopping. Accurately predicting smartphone app usage can effectively improve user experience and optimize resource utilization. However, existing models often treat app usage prediction as a classification problem, which suffers from issues of app usage imbalance and out-of-distribution (OOD) during deployment. To address these challenges, this paper proposes a novel click-through rate (CTR) ranking-based method for predicting app usage. By transforming the classification problem into a CTR problem, we can eliminate the negative impact of the app usage imbalance issue. To address the OOD issue during deployment, we generate the app click sequence and three types of discriminative features, which enable generalization on unseen apps. The app click sequence and the three types of features serve as inputs for training a CTR estimation model in the cloud, and the trained model is then deployed on the user's smartphone to predict the CTR for each installed app. The decision-making process involves ranking these CTR values and selecting the app with the highest CTR as the final prediction. Our method has been extensively tested with large-scale app usage data. The results demonstrate that our approach is able to outperform state-of-the-art methods, with improvements over 4.93% in top-3 accuracy and 6.64% in top-5 accuracy. It achieves approximately twice the accuracy in predicting apps with low usage frequencies in comparison to baseline methods. Our method has been successfully deployed on the app recommendation system of a leading smartphone manufacturer. | Yuqi Zhang, Meiying Kang, Xiucheng Li, Yu Qiu, Zhijun Li | Independent, Chengdu, China; Harbin Institute of Technology, Harbin, China; Soochow University, Suzhou, China; Harbin Institute of Technology, Shenzhen, China |
|  |  [Relevance Meets Diversity: A User-Centric Framework for Knowledge Exploration Through Recommendations](https://doi.org/10.1145/3637528.3671949) |  | 0 | Providing recommendations that are both relevant and diverse is a key consideration of modern recommender systems. Optimizing both of these measures presents a fundamental trade-off, as higher diversity typically comes at the cost of relevance, resulting in lower user engagement. Existing recommendation algorithms try to resolve this trade-off by combining the two measures, relevance and diversity, into one aim and then seeking recommendations that optimize the combined objective, for a given number of items. Traditional approaches, however, do not consider the user interaction with the suggested items. In this paper, we put the user at the central stage, and build on the interplay between relevance, diversity, and user behavior. In contrast to applications where the goal is solely to maximize engagement, we focus on scenarios aiming at maximizing the total amount of knowledge encountered by the user. We use diversity as a surrogate for the amount of knowledge obtained by the user while interacting with the system, and we seek to maximize diversity. We propose a probabilistic user-behavior model in which users keep interacting with the recommender system as long as they receive relevant suggestions, but they may stop if the relevance of the recommended items drops. Thus, for a recommender system to achieve a high-diversity measure, it will need to produce recommendations that are both relevant and diverse. Finally, we propose a novel recommendation strategy that combines relevance and diversity by a copula function. We conduct an extensive evaluation of the proposed methodology over multiple datasets, and we show that our strategy outperforms several state-of-the-art competitors. Our implementation is publicly available at https://github.com/EricaCoppolillo/EXPLORE. | Erica Coppolillo, Giuseppe Manco, Aristides Gionis | ICAR-CNR, Rende, Italy; Division of Theoretical Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, University of Calabria & ICAR-CNR, Rende, Italy |
|  |  [Understanding the Ranking Loss for Recommendation with Sparse User Feedback](https://doi.org/10.1145/3637528.3671565) |  | 0 | Click-through rate (CTR) prediction is a crucial area of research in online advertising. While binary cross entropy (BCE) has been widely used as the optimization objective for treating CTR prediction as a binary classification problem, recent advancements have shown that combining BCE loss with an auxiliary ranking loss can significantly improve performance. However, the full effectiveness of this combination loss is not yet fully understood. In this paper, we uncover a new challenge associated with the BCE loss in scenarios where positive feedback is sparse: the issue of gradient vanishing for negative samples. We introduce a novel perspective on the effectiveness of the auxiliary ranking loss in CTR prediction: it generates larger gradients on negative samples, thereby mitigating the optimization difficulties when using the BCE loss only and resulting in improved classification ability. To validate our perspective, we conduct theoretical analysis and extensive empirical evaluations on public datasets. Additionally, we successfully integrate the ranking loss into Tencent's online advertising system, achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for two main scenarios. The code is openly accessible at: https://github.com/SkylerLinn/Understanding-the-Ranking-Loss. | Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang | Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China; Tencent Inc., Shenzhen, China |
|  |  [Multi-Task Neural Linear Bandit for Exploration in Recommender Systems](https://doi.org/10.1145/3637528.3671649) |  | 0 | Exposure bias and its induced feedback loop effect are well-known problems in recommender systems. Exploration is believed to be the key to break such feedback loops. While classical contextual bandit algorithms such as Upper-Confidence-Bound and Thompson Sampling have been successful in addressing the exploration-exploitation trade-off in the single-task settings with one clear reward signal, modern recommender systems often leverage multiple rich sources of feedback such as clicks, likes, dislikes, shares, satisfaction survey responses, and employ multi-task learning in practice. It is unclear how one can incorporate exploration in the multi-task setup with different objectives. In this paper, we study an efficient bandit algorithm tailored to multi-task recommender systems, named Multi-task Neural Linear Bandit (mtNLB). In particular, we investigate efficient feature embeddings in the multi-task setups that could be used as contextual features in the Neural Linear Bandit, a contextual bandit algorithm that nicely combines the representation power from DNN and simplicity in uncertainty calculation from linear models. We further study cost-effective approximations of the uncertainty estimate and principled ways to incorporate uncertainty into the multi-task scoring of items. To showcase the efficacy of our proposed method, we conduct live experiments on a large-scale commercial recommendation platform that serves billions of users. We evaluate the quality of the uncertainty estimate and demonstrate its ability to improve exploration across the different dimensions of the reward signals in comparison to baseline approaches. | Yi Su, Haokai Lu, Yuening Li, Liang Liu, Shuchao Bi, Ed H. Chi, Minmin Chen | Google Deepmind, Mountain View, CA, USA; Google, Mountain View, CA, USA |
|  |  [Enhancing Pre-Ranking Performance: Tackling Intermediary Challenges in Multi-Stage Cascading Recommendation Systems](https://doi.org/10.1145/3637528.3671580) |  | 0 | Large-scale search engines and recommendation systems utilize a three-stage cascading architecture-recall, pre-ranking, and ranking-to deliver relevant results within stringent latency limits. The pre-ranking stage is crucial for filtering a large number of recalled items into a manageable set for the ranking stage, greatly affecting the system's performance. Pre-ranking faces two intermediary challenges: Sample Selection Bias (SSB) arises when training is based on ranking stage feedback but the evaluation is on a broader recall dataset. Also, compared to the ranking stage, simpler pre-rank models may perform worse and less consistently. Traditional methods to tackle SSB issues include using all recall results and treating unexposed portions as negatives for training, which can be costly and noisy. To boost performance and consistency, some pre-ranking feature interaction enhancers don't fully fix consistency issues, while methods like knowledge distillation in ranking models ignore exposure bias. Our proposed framework targets these issues with three integral modules: Sample Selection, Domain Adaptation, and Unbiased Distillation. Sample Selection filters recall results to mitigate SSB and compute costs. Domain Adaptation enhances model robustness by assigning pseudo-labels to unexposed samples. Unbiased Distillation uses exposure-independent scores from Domain Adaptation to implement unbiased distillation for the pre-ranking model. The framework focuses on optimizing pre-ranking while maintaining training efficiency. We introduce new metrics for pre-ranking evaluation, while experiments confirm the effectiveness of our framework. Our framework is also deployed in real industrial systems. | Jianping Wei, Yujie Zhou, Zhengwei Wu, Ziqi Liu | Ant Group, HangZhou, China |
|  |  [Explicit and Implicit Modeling via Dual-Path Transformer for Behavior Set-informed Sequential Recommendation](https://doi.org/10.1145/3637528.3671755) |  | 0 | Sequential recommendation (SR) and multi-behavior sequential recommendation (MBSR) both come from real-world scenarios. Compared with SR, MBSR takes into account the dependencies of different behaviors. We find that most existing works on MBSR are studied in the context of e-commerce scenarios. In terms of the data format of the behavior types, we observe that the conventional label-formatted data carries limited information and is inadequate for scenarios like social media. With this observation, we introducebehavior set and extend MBSR to behavior set-informed sequential recommendation (BSSR). In BSSR, behavior dependencies become more complex and personalized, and user interest arousal may lack explicit contextual associations. To delve into the dynamics inhered within a behavior set and adaptively tailor recommendation lists upon its variability, we propose a novel solution called Explicit and Implicit modeling via Dual-Path Transformer (EIDP) for BSSR. Our EIDP adopts a dual-path architecture, distinguishing between explicit modeling path (EMP) and implicit modeling path (IMP) based on whether to directly incorporate the behavior representations. EMP features the personalized behavior set-wise transition pattern extractor (PBS-TPE) as its core component. It couples behavioral representations with both the items and positions to explore intra-behavior dynamics within a behavior set at a fine granularity. IMP utilizes light multi-head self-attention blocks (L-MSAB) as encoders under specific behavior types. The obtained multi-view representations are then aggregated by cross-behavior attention fusion (CBAF), using the behavior set of the next time step as a guidance to extract collaborative semantics at the behavioral level. Extensive experiments on two real-world datasets demonstrate the effectiveness of our EIDP. We release the implementation code at: https://github.com/OshiNoCSMA/EIDP. | Ming Chen, Weike Pan, Zhong Ming | Shenzhen University & Shenzhen Technology University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China |
|  |  [Disentangled Multi-interest Representation Learning for Sequential Recommendation](https://doi.org/10.1145/3637528.3671800) |  | 0 | Recently, much effort has been devoted to modeling users' multi-interests (aka multi-faceted preferences) based on their behaviors, aiming to accurately capture users' complex preferences. Existing methods attempt to model each interest of users through a distinct representation, but these multi-interest representations easily collapse into similar ones due to a lack of effective guidance. In this paper, we propose a generic multi-interest method for sequential recommendation, achieving disentangled representation learning of diverse interests technically and theoretically. To alleviate the collapse issue of multi-interests, we propose to conduct item partition guided by their likelihood of being co-purchased in a global view. It can encourage items in each group to focus on a discriminated interest, thus achieving effective disentangled learning of multi-interests. Specifically, we first prove the theoretical connection between item partition and spectral clustering, demonstrating its effectiveness in alleviating item-level and facet-level collapse issues that hinder existing disentangled methods. To efficiently optimize this problem, we then propose a Markov Random Field (MRF)-based method that samples small-scale sub-graphs from two separate MRFs, thus it can be approximated with a cross-entropy loss and optimized through contrastive learning. Finally, we perform multi-task learning to seamlessly align item partition learning with multi-interest modeling for more accurate recommendation. Experiments on three real-world datasets show that our method significantly outperforms state-of-the-art methods and can flexibly integrate with existing multi-interest models as a plugin to enhance their performances. | Yingpeng Du, Ziyan Wang, Zhu Sun, Yining Ma, Hongzhi Liu, Jie Zhang | Singapore University of Technology and Design, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore; Peking University, Beijing, China |
|  |  [Continual Collaborative Distillation for Recommender System](https://doi.org/10.1145/3637528.3671924) |  | 0 | Knowledge distillation (KD) has emerged as a promising technique foraddressing the computational challenges associated with deploying large-scalerecommender systems. KD transfers the knowledge of a massive teacher system toa compact student model, to reduce the huge computational burdens for inferencewhile retaining high accuracy. The existing KD studies primarily focus onone-time distillation in static environments, leaving a substantial gap intheir applicability to real-world scenarios dealing with continuously incomingusers, items, and their interactions. In this work, we delve into a systematicapproach to operating the teacher-student KD in a non-stationary data stream.Our goal is to enable efficient deployment through a compact student, whichpreserves the high performance of the massive teacher, while effectivelyadapting to continuously incoming data. We propose Continual CollaborativeDistillation (CCD) framework, where both the teacher and the studentcontinually and collaboratively evolve along the data stream. CCD facilitatesthe student in effectively adapting to new data, while also enabling theteacher to fully leverage accumulated knowledge. We validate the effectivenessof CCD through extensive quantitative, ablative, and exploratory experiments ontwo real-world datasets. We expect this research direction to contribute tonarrowing the gap between existing KD studies and practical applications,thereby enhancing the applicability of KD in real-world systems. | Gyuseok Lee, SeongKu Kang, Wonbin Kweon, Hwanjo Yu | University of Illinois Urbana-Champaign, Champaign, Illinois, USA; Pohang University of Science and Technology, Pohang, Gyeongsangbuk-do, Republic of Korea |
|  |  [Mitigating Negative Transfer in Cross-Domain Recommendation via Knowledge Transferability Enhancement](https://doi.org/10.1145/3637528.3671799) |  | 0 | Cross-Domain Recommendation (CDR) is a promising technique to alleviate data sparsity by transferring knowledge across domains. However, the negative transfer issue in the presence of numerous domains has received limited attention. Most existing methods transfer all information from source domains to the target domain without distinction. This introduces harmful noise and irrelevant features, resulting in suboptimal performance. Although some methods decompose user features into domain-specific and domain-shared components, they fail to consider other causes of negative transfer. Worse still, we argue that simple feature decomposition is insufficient for multi-domain scenarios. To bridge this gap, we propose TrineCDR, the TRIple-level kNowledge transferability Enhanced model for multi-target CDR. Unlike previous methods, TrineCDR captures single domain and targeted cross-domain embeddings to serve multi-domain recommendation. For the latter, we identify three fundamental causes of negative transfer, ranging from micro to macro perspectives, and correspondingly enhance knowledge transferability at three different levels: the feature level, the interaction level, and the domain level. Through these efforts, TrineCDR effectively filters out noise and irrelevant information from source domains, leading to more comprehensive and accurate representations in the target domain. We extensively evaluate the proposed model on real-world datasets, sampled from Amazon and Douban, under both dual-target and multi-target scenarios. The experimental results demonstrate the superiority of TrineCDR over state-of-the-art cross-domain recommendation methods. | Zijian Song, Wenhan Zhang, Lifang Deng, Jiandong Zhang, Zhihua Wu, Kaigui Bian, Bin Cui | Lazada Group, Beijing, China |
|  |  [Controllable Multi-Behavior Recommendation for In-Game Skins with Large Sequential Model](https://doi.org/10.1145/3637528.3671572) |  | 0 | Online games often house virtual shops where players can acquire character skins. Our task is centered on tailoring skin recommendations across diverse scenarios by analyzing historical interactions such as clicks, usage, and purchases. Traditional multi-behavior recommendation models employed for this task are limited. They either only predict skins based on a single type of behavior or merely recommend skins for target behavior type/task. These models lack the ability to control predictions of skins that are associated with different scenarios and behaviors. To overcome these limitations, we utilize the pretraining capabilities of Large Sequential Models (LSMs) coupled with a novel stimulus prompt mechanism and build a controllable multi-behavior recommendation (CMBR) model. In our approach, the pretraining ability is used to encapsulate users' multi-behavioral sequences into the representation of users' general interests. Subsequently, our designed stimulus prompt mechanism stimulates the model to extract scenario-related interests, thus generating potential skin purchases (or clicks and other interactions) for users. To the best of our knowledge, this is the first work to provide controlled multi-behavior recommendations, and also the first to apply the pretraining capabilities of LSMs in game domain. Through offline experiments and online A/B tests, we validate our method significantly outperforms baseline models, exhibiting about a tenfold improvement on various metrics during the offline test. | Yanjie Gou, Yuanzhou Yao, Zhao Zhang, Yiqing Wu, Yi Hu, Fuzhen Zhuang, Jiangming Liu, Yongjun Xu | Common Data Platform, Tencent, Shenzhen, China; School of Information Science and Engineering, Yunnan University, Kunming, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China |
|  |  [Multi-objective Learning to Rank by Model Distillation](https://doi.org/10.1145/3637528.3671597) |  | 0 | In online marketplaces, search ranking's objective is not only to purchase or conversion (primary objective), but to also the purchase outcomes(secondary objectives), e.g. order cancellation(or return), review rating, customer service inquiries, platform long term growth. Multi-objective learning to rank has been widely studied to balance primary and secondary objectives. But traditional approaches in industry face some challenges including expensive parameter tuning leads to sub-optimal solution, suffering from imbalanced data sparsity issue, and being not compatible with ad-hoc objective. In this paper, we propose a distillation-based ranking solution for multi-objective ranking, which optimizes the end-to-end ranking system at Airbnb across multiple ranking models on different objectives along with various considerations to optimize training and serving efficiency to meet industry standards. We found it performs much better than traditional approaches, it doesn't only significantly increases primary objective by a large margin but also meet secondary objectives constraints and improve model stability. We also demonstrated the proposed system could be further simplified by model self-distillation. Besides this, we did additional simulations to show that this approach could also help us efficiently inject ad-hoc non-differentiable business objective into the ranking system while enabling us to balance our optimization objectives. | Jie Tang, Huiji Gao, Liwei He, Sanjeev Katariya | Airbnb, San Francisco, CA, USA |
|  |  [Unified Dual-Intent Translation for Joint Modeling of Search and Recommendation](https://doi.org/10.1145/3637528.3671519) |  | 0 | Recommendation systems, which assist users in discovering their preferred items among numerous options, have served billions of users across various online platforms. Intuitively, users' interactions with items are highly driven by their unchanging inherent intents (e.g., always preferring high-quality items) and changing demand intents (e.g., wanting a T-shirt in summer but a down jacket in winter). However, both types of intents are implicitly expressed in recommendation scenario, posing challenges in leveraging them for accurate intent-aware recommendations. Fortunately, in search scenario, often found alongside recommendation on the same online platform, users express their demand intents explicitly through their query words. Intuitively, in both scenarios, a user shares the same inherent intent and his/her interactions may be influenced by the same demand intent. It is therefore feasible to utilize the interaction data from both scenarios to reinforce the dual intents for joint intent-aware modeling. But the joint modeling should deal with two problems: (1) accurately modeling users' implicit demand intents in recommendation; (2) modeling the relation between the dual intents and the interactive items. To address these problems, we propose a novel model named Unified Dual-Intents Translation for joint modeling of Search and Recommendation (UDITSR). To accurately simulate users' demand intents in recommendation, we utilize real queries from search data as supervision information to guide its generation. To explicitly model the relation among the triplet , we propose a dual-intent translation propagation mechanism to learn the triplet in the same semantic space via embedding translations. Extensive experiments demonstrate that UDITSR outperforms SOTA baselines both in search and recommendation tasks. Moreover, our model has been deployed online on Meituan Waimai platform, leading to an average improvement in GMV (Gross Merchandise Value) of 1.46% and CTR(Click-Through Rate) of 0.77% over one month. | Yuting Zhang, Yiqing Wu, Ruidong Han, Ying Sun, Yongchun Zhu, Xiang Li, Wei Lin, Fuzhen Zhuang, Zhulin An, Yongjun Xu | Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China; Meituan, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China |
|  |  [Shopping Trajectory Representation Learning with Pre-training for E-commerce Customer Understanding and Recommendation](https://doi.org/10.1145/3637528.3671747) |  | 0 | Understanding customer behavior is crucial for improving service quality in large-scale E-commerce. This paper proposes C-STAR, a new framework that learns compact representations from customer shopping journeys, with good versatility to fuel multiple downstream customer-centric tasks. We define the notion of shopping trajectory that encompasses customer interactions at the level of product categories, capturing the overall flow of their browsing and purchase activities. C-STAR excels at modeling both inter-trajectory distribution similarity-the structural similarities between different trajectories, and intra-trajectory semantic correlation-the semantic relationships within individual ones. This coarse-to-fine approach ensures informative trajectory embeddings for representing customers. To enhance embedding quality, we introduce a pre-training strategy that captures two intrinsic properties within the pre-training data. Extensive evaluation on large-scale industrial and public datasets demonstrates the effectiveness of C-STAR across three diverse customer-centric tasks. These tasks empower customer profiling and recommendation services for enhancing personalized shopping experiences on our E-commerce platform. | Yankai Chen, QuocTuan Truong, Xin Shen, Jin Li, Irwin King | The Chinese University of Hong Kong, Hong Kong, China; Amazon, Seattle, WA, USA |
|  |  [DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation](https://doi.org/10.1145/3637528.3671669) |  | 0 | Due to the continuously improving capabilities of mobile edges, recommendersystems start to deploy models on edges to alleviate network congestion causedby frequent mobile requests. Several studies have leveraged the proximity ofedge-side to real-time data, fine-tuning them to create edge-specific models.Despite their significant progress, these methods require substantial on-edgecomputational resources and frequent network transfers to keep the model up todate. The former may disrupt other processes on the edge to acquirecomputational resources, while the latter consumes network bandwidth, leadingto a decrease in user satisfaction. In response to these challenges, we proposea customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploysthe same generic backbone (potentially incompatible for a specific edge) to alldevices. To minimize frequent bandwidth usage and storage consumption inpersonalization, DIET tailors specific subnets for each edge based on its pastinteractions, learning to generate slimming subnets(diets) within incompatiblenetworks for efficient transfer. It also takes the inter-layer relationshipsinto account, empirically reducing inference time while obtaining more suitablediets. We further explore the repeated modules within networks and propose amore storage-efficient framework, DIETING, which utilizes a single layer ofparameters to represent the entire network, achieving comparably excellentperformance. The experiments across four state-of-the-art datasets and twowidely used models demonstrate the superior accuracy in recommendation andefficiency in transmission and storage of our framework. | Kairui Fu, Shengyu Zhang, Zheqi Lv, Jingyuan Chen, Jiwei Li | Zhejiang University, Hangzhou, China; Zhejiang University & Shanghai Institute for Advanced Study of Zhejiang University, Hangzhou, China |
|  |  [Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System](https://doi.org/10.1145/3637528.3671931) |  | 0 | Collaborative filtering recommender systems (CF-RecSys) have shown successiveresults in enhancing the user experience on social media and e-commerceplatforms. However, as CF-RecSys struggles under cold scenarios with sparseuser-item interactions, recent strategies have focused on leveraging modalityinformation of user/items (e.g., text or images) based on pre-trained modalityencoders and Large Language Models (LLMs). Despite their effectiveness undercold scenarios, we observe that they underperform simple traditionalcollaborative filtering models under warm scenarios due to the lack ofcollaborative knowledge. In this work, we propose an efficient All-roundLLM-based Recommender system, called A-LLMRec, that excels not only in the coldscenario but also in the warm scenario. Our main idea is to enable an LLM todirectly leverage the collaborative knowledge contained in a pre-trainedstate-of-the-art CF-RecSys so that the emergent ability of the LLM as well asthe high-quality user/item embeddings that are already trained by thestate-of-the-art CF-RecSys can be jointly exploited. This approach yields twoadvantages: (1) model-agnostic, allowing for integration with various existingCF-RecSys, and (2) efficiency, eliminating the extensive fine-tuning typicallyrequired for LLM-based recommenders. Our extensive experiments on variousreal-world datasets demonstrate the superiority of A-LLMRec in variousscenarios, including cold/warm, few-shot, cold user, and cross-domainscenarios. Beyond the recommendation task, we also show the potential ofA-LLMRec in generating natural language outputs based on the understanding ofthe collaborative knowledge by performing a favorite genre prediction task. Ourcode is available at https://github.com/ghdtjr/A-LLMRec . | Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, MinChul Yang, Chanyoung Park | NAVER Corporation, Seongnam, Republic of Korea; KAIST, Daejeon, Republic of Korea |
|  |  [Probabilistic Attention for Sequential Recommendation](https://doi.org/10.1145/3637528.3671733) |  | 0 | Sequential Recommendation (SR) navigates users' dynamic preferences through modeling their historical interactions. The incorporation of the popular Transformer framework, which captures long relationships through pairwise dot products, has notably benefited SR. However, prevailing research in this domain faces three significant challenges: (i) Existing studies directly adopt the primary component of Transformer (i.e., the self-attention mechanism), without a clear explanation or tailored definition for its specific role in SR; (ii) The predominant focus on pairwise computations overlooks the global context or relative prevalence of item pairs within the overall sequence; (iii) Transformer primarily pursues relevance-dominated relationships, neglecting another essential objective in recommendation, i.e., diversity. In response, this work introduces a fresh perspective to elucidate the attention mechanism in SR. Here, attention is defined as dependency interactions among items, quantitatively determined under a global probabilistic model by observing the probabilities of corresponding item subsets. This viewpoint offers a precise and context-specific definition of attention, leading to the design of a distinctive attention mechanism tailored for SR. Specifically, we transmute the well-formulated global, repulsive interactions in Determinantal Point Processes (DPPs) to effectively model dependency interactions. Guided by the repulsive interactions, a theoretically and practically feasible DPP kernel is designed, enabling our attention mechanism to directly consider category/topic distribution for enhancing diversity. Consequently, the Probabilistic Attention mechanism (PAtt) for sequential recommendation is developed. Experimental results demonstrate the excellent scalability and adaptability of our attention mechanism, which significantly improves recommendation performance in terms of both relevance and diversity. | Yuli Liu, Christian Walder, Lexing Xie, Yiqun Liu | Google Research, Brain Team, Montreal, Canada; Australian National University & Data61 CSIRO, Canberra, Australia |
|  |  [Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POI Recommendations](https://doi.org/10.1145/3637528.3671743) |  | 0 | The rapid expansion of Location-Based Social Networks (LBSNs) has highlightedthe importance of effective next Point-of-Interest (POI) recommendations, whichleverage historical check-in data to predict users' next POIs to visit.Traditional centralized deep neural networks (DNNs) offer impressive POIrecommendation performance but face challenges due to privacy concerns andlimited timeliness. In response, on-device POI recommendations have beenintroduced, utilizing federated learning (FL) and decentralized approaches toensure privacy and recommendation timeliness. However, these methods oftensuffer from computational strain on devices and struggle to adapt to new usersand regions. This paper introduces a novel collaborative learning framework,Diffusion-Based Cloud-Edge-Device Collaborative Learning for Next POIRecommendations (DCPR), leveraging the diffusion model known for its successacross various domains. DCPR operates with a cloud-edge-device architecture tooffer region-specific and highly personalized POI recommendations whilereducing on-device computational burdens. DCPR minimizes on-devicecomputational demands through a unique blend of global and local learningprocesses. Our evaluation with two real-world datasets demonstrates DCPR'ssuperior performance in recommendation accuracy, efficiency, and adaptabilityto new users and regions, marking a significant step forward in on-device POIrecommendation technology. | Jing Long, Guanhua Ye, Tong Chen, Yang Wang, Meng Wang, Hongzhi Yin | Hefei University of Technology, Hefei, China; Beijing University of Posts and Telecommunications, BeiJing, China; The University of Queensland, Brisbane, Australia |
|  |  [Certified Robustness on Visual Graph Matching via Searching Optimal Smoothing Range](https://doi.org/10.1145/3637528.3671852) |  | 0 | Deep visual graph matching (GM) is a challenging combinatorial task that involves finding a permutation matrix that indicates the correspondence between keypoints from a pair of images. Like many learning systems, empirical studies have shown that visual GM is susceptible to adversarial attacks, with reliability issues in downstream applications. To the best of our knowledge, certifying robustness for deep visual GM remains an open challenge with two main difficulties: how to handle the paired inputs together with the heavily non-linear permutation output space (especially at large scale), and how to balance the trade-off between certified robustness and matching performance. Inspired by the randomized smoothing (RS) technique, we propose the Certified Robustness based on the Optimal Smoothing Range Search (CR-OSRS) technique to fulfill the robustness guarantee for deep visual GM. First, unlike conventional RS methods that use isotropic Gaussian distributions for smoothing, we build the smoothed model with paired joint Gaussian distributions, which capture the structural information among keypoints, and mitigate the performance degradation caused by smoothing. For the vast space of the permutation output, we devise a similarity-based partitioning method that can lower the computational complexity and certification difficulty. We then derive a stringent robustness guarantee that links the certified space of inputs to their corresponding fixed outputs. Second, we design a global optimization method to search for optimal joint Gaussian distributions and facilitate a larger certified space and better performance. Third, we apply data augmentation and a similarity-based regularizer in training to enhance smoothed model performance. Lastly, for the high-dimensional and multivariable nature of the certified space, we propose two methods (sampling and marginal radii) to evaluate it. Experimental results on public benchmarks show that our method achieves state-of-the-art certified robustness. | Huaqing Shao, Lanjun Wang, Yongwei Wang, Qibing Ren, Junchi Yan | School of AI and Department of CSE, Shanghai Jiao Tong University, Shanghai, China; Department of CSE, Shanghai Jiao Tong University, Shanghai, China; SIAS and College of Computer Science, Zhejiang University, Hangzhou, China; SNMC, Tianjin University, Tianjin, China; Department of CSE and MoE Key Lab of AI, Shanghai Jiao Tong University, Shanghai, China |
|  |  [Pre-Training with Transferable Attention for Addressing Market Shifts in Cross-Market Sequential Recommendation](https://doi.org/10.1145/3637528.3671698) |  | 0 | Cross-market recommendation (CMR) involves selling the same set of items across multiple nations or regions within a transfer learning framework. However, CMR's distinctive characteristics, including limited data sharing due to privacy policies, absence of user overlap, and a shared item set between markets present challenges for traditional recommendation methods. Moreover, CMR experiences market shifts, leading to differences in item popularity and user preferences among different markets. This study focuses on cross-market sequential recommendation (CMSR) and proposes the Cross-market Attention Transferring with Sequential Recommendation (CAT-SR) framework to address these challenges and market shifts. CAT-SR incorporates a pre-training strategy emphasizing item-item correlation, selective self-attention transferring for effective transfer learning, and query and key adapters for market-specific user preferences. Experimental results on real-world cross-market datasets demonstrate the superiority of CAT-SR, and ablation studies validate the benefits of its components across different geographical continents. CAT-SR offers a robust and adaptable solution for cross-market sequential recommendation. The code is available at https://github.com/ChenMetanoia/CATSR-KDD/. | Chen Wang, Ziwei Fan, Liangwei Yang, Mingdai Yang, Xiaolong Liu, Zhiwei Liu, Philip S. Yu | Tsinghua University, Beijing, China; The University of Chicago, Chicago, IL, USA; University of Illinois Chicago, Chicago, IL, USA; Salesforce AI Research, Palo Alto, CA, USA; Amazon, Santa Clara, CA, USA |
|  |  [Dataset Regeneration for Sequential Recommendation](https://doi.org/10.1145/3637528.3671841) |  | 0 | The sequential recommender (SR) system is a crucial component of modern recommender systems, as it aims to capture the evolving preferences of users. Significant efforts have been made to enhance the capabilities of SR systems. These methods typically follow the model-centric paradigm, which involves developing effective models based on fixed datasets. However, this approach often overlooks potential quality issues and flaws inherent in the data. Driven by the potential of data-centric AI, we propose a novel data-centric paradigm for developing an ideal training dataset using a model-agnostic dataset regeneration framework called DR4SR. This framework enables the regeneration of a dataset with exceptional cross-architecture generalizability. Additionally, we introduce the DR4SR+ framework, which incorporates a model-aware dataset personalizer to tailor the regenerated dataset specifically for a target model. To demonstrate the effectiveness of the data-centric paradigm, we integrate our framework with various model-centric methods and observe significant performance improvements across four widely adopted datasets. Furthermore, we conduct in-depth analyses to explore the potential of the data-centric paradigm and provide valuable insights. The code can be found at https://github.com/USTC-StarTeam/DR4SR. | Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, Enhong Chen | Huawei Singapore Research Center, Singapore, Singapore |
|  |  [GPFedRec: Graph-Guided Personalization for Federated Recommendation](https://doi.org/10.1145/3637528.3671702) |  | 0 | The federated recommendation system is an emerging AI service architecture that provides recommendation services in a privacy-preserving manner. Using user-relation graphs to enhance federated recommendations is a promising topic. However, it is still an open challenge to construct the user-relation graph while preserving data locality-based privacy protection in federated settings. Inspired by a simple motivation, similar users share a similar vision (embeddings) to the same item set, this paper proposes a novel Graph-guided Personalization for Federated Recommendation (GPFedRec). The proposed method constructs a user-relation graph from user-specific personalized item embeddings at the server without accessing the users' interaction records. The personalized item embedding is locally fine-tuned on each device, and then a user-relation graph will be constructed by measuring the similarity among client-specific item embeddings. Without accessing users' historical interactions, we embody the data locality-based privacy protection of vanilla federated learning. Furthermore, a graph-guided aggregation mechanism is designed to leverage the user-relation graph and federated optimization framework simultaneously. Extensive experiments on five benchmark datasets demonstrate GPFedRec's superior performance. The in-depth study validates that GPFedRec can generally improve existing federated recommendation methods as a plugin while keeping user privacy safe. Code is available https://github.com/Zhangcx19/GPFedRec | Chunxu Zhang, Guodong Long, Tianyi Zhou, Zijian Zhang, Peng Yan, Bo Yang | Computer Science and UMIACS, University of Maryland, Maryland, USA |
|  |  [GradCraft: Elevating Multi-task Recommendations through Holistic Gradient Crafting](https://doi.org/10.1145/3637528.3671585) |  | 0 | Recommender systems require the simultaneous optimization of multiple objectives to accurately model user interests, necessitating the application of multi-task learning methods. However, existing multi-task learning methods in recommendations overlook the specific characteristics of recommendation scenarios, falling short in achieving proper gradient balance. To address this challenge, we set the target of multi-task learning as attaining the appropriate magnitude balance and the global direction balance, and propose an innovative methodology named GradCraft in response. GradCraft dynamically adjusts gradient magnitudes to align with the maximum gradient norm, mitigating interference from gradient magnitudes for subsequent manipulation. It then employs projections to eliminate gradient conflicts in directions while considering all conflicting tasks simultaneously, theoretically guaranteeing the global resolution of direction conflicts. GradCraft ensures the concurrent achievement of appropriate magnitude balance and global direction balance, aligning with the inherent characteristics of recommendation scenarios. Both offline and online experiments attest to the efficacy of GradCraft in enhancing multi-task performance in recommendations. The source code for GradCraft can be accessed at https://github.com/baiyimeng/GradCraft. | Yimeng Bai, Yang Zhang, Fuli Feng, Jing Lu, Xiaoxue Zang, Chenyi Lei, Yang Song | University of Science and Technology of China & USTC Beijing Research Institute, Hefei, China; University of Science and Technology of China, Hefei, China; Kuaishou Technology, Beijing, China |
|  |  [NudgeRank: Digital Algorithmic Nudging for Personalized Health](https://doi.org/10.1145/3637528.3671562) |  | 0 | In this paper we describe NudgeRankTM, an innovative digital algorithmic nudging system designed to foster positive health behaviors on a population-wide scale. Utilizing a novel combination of Graph Neural Networks augmented with an extensible Knowledge Graph, this Recommender System is operational in production, delivering personalized and context-aware nudges to over 1.1 million care recipients daily. This enterprise deployment marks one of the largest AI-driven health behavior change initiatives, accommodating diverse health conditions and wearable devices. Rigorous evaluation reveals statistically significant improvements in health outcomes, including a 6.17% increase in daily steps and 7.61% more exercise minutes. Moreover, user engagement and program enrollment surged, with a 13.1% open rate compared to baseline systems' 4%. Demonstrating scalability and reliability, NudgeRankTM operates efficiently on commodity compute resources while maintaining automation and observability standards essential for production systems. | Jodi Chiam, Aloysius Lim, Ankur Teredesai | CueZen, Inc. & University of Washington, Seattle, WA, USA; CueZen, Inc., Singapore, Singapore |
|  |  [Achieving a Better Tradeoff in Multi-stage Recommender Systems through Personalization](https://doi.org/10.1145/3637528.3671593) |  | 0 | Recommender systems in social media websites provide value to their communities by recommending engaging content and meaningful connections. Scaling high-quality recommendations to billions of users in real-time requires sophisticated ranking models operating on a vast number of potential items to recommend, becoming prohibitively expensive computationally. A common technique "funnels'' these items through progressively complex models ("multi-stage''), each ranking fewer items but at higher computational cost for greater accuracy. This architecture introduces a trade-off between the cost of ranking items and providing users with the best recommendations. A key observation we make in this paper is that, all else equal, ranking more items indeed improves the overall objective but has diminishing returns. Following this observation, we provide a rigorous formulation through the framework of DR-submodularity, and argue that for a certain class of objectives (reward functions), it is possible to improve the trade-off between performance and computational cost in multi-stage ranking systems with strong theoretical guarantees. We show that this class of reward functions that provide this guarantee is large and robust to various noise models. Finally, we describe extensive experimentation of our method on three real-world recommender systems in Facebook, achieving 8.8% reduction in overall compute resources with no significant impact on recommendation quality, compared to a 0.8% quality loss in a non-personalized budget allocation. | Ariel Evnine, Stratis Ioannidis, Dimitris Kalimeris, Shankar Kalyanaraman, Weiwei Li, Israel Nir, Wei Sun, Udi Weinsberg | Meta, Menlo Park, CA, USA; Northeastern University, Boston, MA, USA |
|  |  [Residual Multi-Task Learner for Applied Ranking](https://doi.org/10.1145/3637528.3671523) |  | 0 | Modern e-commerce platforms rely heavily on modeling diverse user feedback to provide personalized services. Consequently, multi-task learning has become an integral part of their ranking systems. However, existing multi-task learning methods encounter two main challenges: some lack explicit modeling of task relationships, resulting in inferior performance, while others have limited applicability due to being computationally intensive, having scalability issues, or relying on strong assumptions. To address these limitations and better fit our real-world scenario, pre-rank in Shopee Search, we introduce in this paper ResFlow, a lightweight multi-task learning framework that enables efficient cross-task information sharing via residual connections between corresponding layers of task networks. Extensive experiments on datasets from various scenarios and modalities demonstrate its superior performance and adaptability over state-of-the-art methods. The online A/B tests in Shopee Search showcase its practical value in large-scale industrial applications, evidenced by a 1.29% increase in OPU (order-per-user) without additional system latency. ResFlow is now fully deployed in the pre-rank module of Shopee Search. To facilitate efficient online deployment, we propose a novel offline metric Weighted Recall@K, which aligns well with our online metric OPU, addressing the longstanding online-offline metric misalignment issue. Besides, we propose to fuse scores from the multiple tasks additively when ranking items, which outperforms traditional multiplicative fusion. | Cong Fu, Kun Wang, Jiahua Wu, Yizhou Chen, Guangda Huzhang, Yabo Ni, Anxiang Zeng, Zhiming Zhou | Shopee Pte. Ltd., Singapore, Singapore; SCSE, Nanyang Technological University, Singapore, Singapore; ECONCS, Shanghai University of Finance and Economics, Shanghai, China; Shopee Pte. Ltd., Shanghai, China; Nanyang Technological University, Singapore, Singapore |
|  |  [Multi-task Conditional Attention Network for Conversion Prediction in Logistics Advertising](https://doi.org/10.1145/3637528.3671549) |  | 0 | Logistics advertising is an emerging task in online-to-offline logistics systems, where logistics companies expand parcel shipping services to new users through advertisements on shopping websites. Compared to existing online e-commerce advertising, logistics advertising has two significant new characteristics: (i) the complex factors in logistics advertising considering both users' offline logistics preference and online purchasing profiles; and (ii) data sparsity and mutual relations among multiple steps due to longer advertising conversion processes. To address these challenges, we design MCAC, a Multi-task Conditional Attention network-based logistics advertising Conversion prediction framework, which consists of (i) an offline shipping preference extraction model to extract the user's offline logistics preference from historical shipping records, and (ii) a multi-task conditional attention-based conversion rate prediction module to model mutual relations among multiple steps in logistics advertising conversion processes. We evaluate and deploy MCAC on one of the largest e-commerce platforms in China for logistics advertising. Extensive offline experiments show that our method outperforms state-of-the-art baselines in various metrics. Moreover, the conversion rate prediction results of large-scale online A/B testing show that MCAC achieves a 15.22% improvement compared to existing industrial practices, which demonstrates the effectiveness of the proposed framework. | Baoshen Guo, Xining Song, Shuai Wang, Wei Gong, Tian He, Xue Liu | University of Science and Technology of China, Hefei, China; McGill University, Montréal, Canada; Southeast University, Nanjing, China; JD Logistics, Beijing, China; Southeast University & JD Logistics, Nanjing, China |
|  |  [Learning to Rank for Maps at Airbnb](https://doi.org/10.1145/3637528.3671648) |  | 0 | As a two-sided marketplace, Airbnb brings together hosts who own listings for rent with prospective guests from around the globe. Results from a guest's search for listings are displayed primarily through two interfaces: (1) as a list of rectangular cards that contain on them the listing image, price, rating, and other details, referred to as list-results (2) as oval pins on a map showing the listing price, called map-results. Both these interfaces, since their inception, have used the same ranking algorithm that orders listings by their booking probabilities and selects the top listings for display. But some of the basic assumptions underlying ranking, built for a world where search results are presented as lists, simply break down for maps. This paper describes how we rebuilt ranking for maps by revising the mathematical foundations of how users interact with search results. Our iterative and experiment-driven approach led us through a path full of twists and turns, ending in a unified theory for the two interfaces. Our journey shows how assumptions taken for granted when designing machine learning algorithms may not apply equally across all user interfaces, and how they can be adapted. The net impact was one of the largest improvements in user experience for Airbnb which we discuss as a series of experimental validations. | Malay Haldar, Hongwei Zhang, Kedar Bellare, Sherry Chen, Soumyadip Banerjee, Xiaotang Wang, Mustafa Abdool, Huiji Gao, Pavan Tapadia, Liwei He, Sanjeev Katariya | Airbnb, Inc., San Francisco, CA, USA; Airbnb, Inc., San Francisco, WA, USA |
|  |  [Deep Bag-of-Words Model: An Efficient and Interpretable Relevance Architecture for Chinese E-Commerce](https://doi.org/10.1145/3637528.3671559) |  | 0 | Text relevance or text matching of query and product is an essential technique for the e-commerce search system to ensure that the displayed products can match the intent of the query. Many studies focus on improving the performance of the relevance model in search system. Recently, pre-trained language models like BERT have achieved promising performance on the text relevance task. While these models perform well on the offline test dataset, there are still obstacles to deploy the pre-trained language model to the online system as their high latency. The two-tower model is extensively employed in industrial scenarios, owing to its ability to harmonize performance with computational efficiency. Regrettably, such models present an opaque ''black box'' nature, which prevents developers from making special optimizations. In this paper, we raise deep Bag-o f-Words (DeepBoW) model, an efficient and interpretable relevance architecture for Chinese e-commerce. Our approach proposes to encode the query and the product into the sparse BoW representation, which is a set of word-weight pairs. The weight means the important or the relevant score between the corresponding word and the raw text. The relevance score is measured by the accumulation of the matched word between the sparse BoW representation of the query and the product. Compared to popular dense distributed representation that usually suffers from the drawback of black-box, the most advantage of the proposed representation model is highly explainable and interventionable, which is a superior advantage to the deployment and operation of online search engines. Moreover, the online efficiency of the proposed model is even better than the most efficient inner product form of dense representation. The proposed model is experimented on three different datasets for learning the sparse BoW representations, including the human-annotation set, the search-log set and the click-through set. Then the models are evaluated by experienced human annotators. Both the auto metrics and the online evaluations show our DeepBoW model achieves competitive performance while the online inference is much more efficient than the other models. Our DeepBoW model has already deployed to the biggest Chinese e-commerce search engine Taobao and served the entire search traffic for over 6 months. | Zhe Lin, Jiwei Tan, Dan Ou, Xi Chen, Shaowei Yao, Bo Zheng | Alibaba Group, HangZhou, China |
|  |  [GRAM: Generative Retrieval Augmented Matching of Data Schemas in the Context of Data Security](https://doi.org/10.1145/3637528.3671602) |  | 0 | Schema matching constitutes a pivotal phase in the data ingestion process forcontemporary database systems. Its objective is to discern pairwisesimilarities between two sets of attributes, each associated with a distinctdata table. This challenge emerges at the initial stages of data analytics,such as when incorporating a third-party table into existing databases toinform business insights. Given its significance in the realm of databasesystems, schema matching has been under investigation since the 2000s. Thisstudy revisits this foundational problem within the context of large languagemodels. Adhering to increasingly stringent data security policies, our focuslies on the zero-shot and few-shot scenarios: the model should analyze only aminimal amount of customer data to execute the matching task, contrasting withthe conventional approach of scrutinizing the entire data table. We emphasizethat the zero-shot or few-shot assumption is imperative to safeguard theidentity and privacy of customer data, even at the potential cost of accuracy.The capability to accurately match attributes under such stringent requirementsdistinguishes our work from previous literature in this domain. | Xuanqing Liu, Runhui Wang, Yang Song, Luyang Kong | Amazon Web Services, Seattle, WA, USA |
|  |  [Ads Recommendation in a Collapsed and Entangled World](https://doi.org/10.1145/3637528.3671607) |  | 0 | We present Tencent's ads recommendation system and examine the challenges and practices of learning appropriate recommendation representations. Our study begins by showcasing our approaches to preserving prior knowledge when encoding features of diverse types into embedding representations. We specifically address sequence features, numeric features, and pre-trained embedding features. Subsequently, we delve into two crucial challenges related to feature representation: the dimensional collapse of embeddings and the interest entanglement across different tasks or scenarios. We propose several practical approaches to address these challenges that result in robust and disentangled recommendation representations. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Additionally, we introduce three analysis tools that enable us to study feature correlation, dimensional collapse, and interest entanglement. This work builds upon the continuous efforts of Tencent's ads recommendation team over the past decade. It summarizes general design principles and presents a series of readily applicable solutions and analysis tools. The reported performance is based on our online advertising platform, which handles hundreds of billions of requests daily and serves millions of ads to billions of users. | Junwei Pan, Wei Xue, Ximei Wang, Haibin Yu, Xun Liu, Shijie Quan, Xueming Qiu, Dapeng Liu, Lei Xiao, Jie Jiang | Tencent Inc., Shenzhen, China |
|  |  [Non-autoregressive Generative Models for Reranking Recommendation](https://doi.org/10.1145/3637528.3671645) |  | 0 | Contemporary recommendation systems are designed to meet users' needs by delivering tailored lists of items that align with their specific demands or interests. In a multi-stage recommendation system, reranking plays a crucial role by modeling the intra-list correlations among items. The key challenge of reranking lies in the exploration of optimal sequences within the combinatorial space of permutations. Recent research proposes a generator-evaluator learning paradigm, where the generator generates multiple feasible sequences and the evaluator picks out the best sequence based on the estimated listwise score. The generator is of vital importance, and generative models are well-suited for the generator function. Current generative models employ an autoregressive strategy for sequence generation. However, deploying autoregressive models in real-time industrial systems is challenging. Firstly, the generator can only generate the target items one by one and hence suffers from slow inference. Secondly, the discrepancy between training and inference brings an error accumulation. Lastly, the left-to-right generation overlooks information from succeeding items, leading to suboptimal performance. To address these issues, we propose a Non-AutoRegressive generative model for reranking Recommendation (NAR4Rec) designed to enhance efficiency and effectiveness. To tackle challenges such as sparse training samples and dynamic candidates, we introduce a matching model. Considering the diverse nature of user feedback, we employ a sequence-level unlikelihood training objective to differentiate feasible sequences from unfeasible ones. Additionally, to overcome the lack of dependency modeling in non-autoregressive models regarding target items, we introduce contrastive decoding to capture correlations among these items. Extensive offline experiments validate the superior performance of NAR4Rec over state-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec significantly enhances the user experience. Furthermore, NAR4Rec has been fully deployed in a popular video app Kuaishou with over 300 million daily active users. | Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang | Kuaishou Technology, Beijing, China; Tsinghua University, Beijing, China; Peking University, Beijing, China |
|  |  [Chaining Text-to-Image and Large Language Model: A Novel Approach for Generating Personalized e-commerce Banners](https://doi.org/10.1145/3637528.3671636) |  | 0 | Text-to-image models such as stable diffusion have opened a plethora ofopportunities for generating art. Recent literature has surveyed the use oftext-to-image models for enhancing the work of many creative artists. Manye-commerce platforms employ a manual process to generate the banners, which istime-consuming and has limitations of scalability. In this work, we demonstratethe use of text-to-image models for generating personalized web banners withdynamic content for online shoppers based on their interactions. The novelty inthis approach lies in converting users' interaction data to meaningful promptswithout human intervention. To this end, we utilize a large language model(LLM) to systematically extract a tuple of attributes from itemmeta-information. The attributes are then passed to a text-to-image model viaprompt engineering to generate images for the banner. Our results show that theproposed approach can create high-quality personalized banners for users. | Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, Kannan Achan | Walmart Global Tech, Sunnyvale, CA, USA |
|  |  [LiMAML: Personalization of Deep Recommender Models via Meta Learning](https://doi.org/10.1145/3637528.3671599) |  | 0 | In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in production, which involves transforming them into fixed-sized vectors, termed meta embeddings, thereby enabling the seamless deployment of models with hundreds of billions of parameters for online serving. Through extensive experimentation on production data drawn from various applications at LinkedIn, we demonstrate that the proposed solution consistently outperforms the best performing baseline models of those applications, including strong baselines such as using wide-and-deep ID based personalization approach. Our approach has enabled the deployment of a range of highly personalized AI models across diverse LinkedIn applications, leading to substantial improvements in business metrics as well as refreshed experience for our members. | Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang, Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya Venugopalan, Aman Gupta, Fedor Borisyuk, S. Sathiya Keerthi, Ajith Muralidharan | LinkedIn Corporation, Sunnyvale, CA, USA; Aliveo AI Corp, Sunnyvale, CA, USA |
|  |  [Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking](https://doi.org/10.1145/3637528.3671517) |  | 0 | This paper addresses the challenge of the semantic gap between user queries and web content, commonly referred to as asymmetric text matching, within the domain of web search. By leveraging BERT for reading comprehension, current algorithms enable significant advancements in query understanding, but still encounter limitations in effectively resolving the asymmetrical ranking problem due to model comprehension and summarization constraints. To tackle this issue, we propose the QAGR (Question-Answer Generation and Ranking) method, comprising an offline module called QAGeneration and an online module called QARanking. The QAGeneration module utilizes large language models (LLMs) to generate high-quality question-answering pairs for each web page. This process involves two steps: generating question-answer pairs and performing verification to eliminate irrelevant questions, resulting in high-quality questions associated with their respective documents. The QARanking module combines and ranks the generated questions and web page content. To ensure efficient online inference, we design the QARanking model as a homogeneous dual-tower model, incorporating query intent to drive score fusion while balancing keyword matching and asymmetric matching. Additionally, we conduct a preliminary screening of questions for each document, selecting only the top-N relevant questions for further relevance calculation. Empirical results demonstrate the substantial performance improvement of our proposed method in web search. We achieve over 8.7% relative offline relevance improvement and over 8.5% online engagement gain compared to the state-of-the-art web search system. Furthermore, we deploy QAGR to online web search engines and share our deployment experience, including production considerations and ablation experiments. This research contributes to advancing the field of asymmetric web search and provides valuable insights for enhancing search engine performance. | Dezhi Ye, Jie Liu, Jiabin Fan, Bowen Tian, Tianhua Zhou, Xiang Chen, Jin Ma | Tencent PCG, Beijing, China |
|  |  [Unsupervised Ranking Ensemble Model for Recommendation](https://doi.org/10.1145/3637528.3671598) |  | 0 | When visiting an online platform, a user generates various actions, such as clicks, long views, likes, comments, etc. To capture user preferences in these aspects, we learn these objectives and return multiple rankings of candidate items for each user. We need to aggregate them into one to truncate the candidate set, and ranking ensemble model is proposed for this task. However, there is a critical issue: though we input abundant information, what model learns depends on the supervision. Unfortunately, the existing supervision is poorly designed, leading to serious information loss issue. To address this issue, we designed an unsupervised loss to compel the ranking ensemble model to learn all information of input rankings, including sequential and numerical information. (1) For sequential information, we design a distance measure between two rankings, and train the ensemble ranking to have similar order with all input rankings by minimizing the distance. (2) For numerical information, we design a decoder to reconstruct values of original rankings from the hidden layer of the model, to guarantee that the model captures as much input information as possible. Our unsupervised loss is compatible with all ranking ensemble models. We optimize several widely-used structures to propose unsupervised ranking ensemble models. We devise comprehensive experiments on two real-world datasets to demonstrate the effectiveness of the proposed models. We also apply our model in a short video platform with billions of users, and achieve significant improvement. | Wenhui Yu, Bingqi Liu, Bin Xia, Xiaoxiao Xu, Ying Chen, Yongchang Li, Lantao Hu | Kuaishou Technology, Beijing, China |
|  |  [Adapting Job Recommendations to User Preference Drift with Behavioral-Semantic Fusion Learning](https://doi.org/10.1145/3637528.3671759) |  | 0 | Job recommender systems are crucial for aligning job opportunities with job-seekers in online job-seeking. However, users tend to adjust their job preferences to secure employment opportunities continually, which limits the performance of job recommendations. The inherent frequency of preference drift poses a challenge to promptly and precisely capture user preferences. To address this issue, we propose a novel session-based framework, BISTRO, to timely model user preference through fusion learning of semantic and behavioral information. Specifically, BISTRO is composed of three stages: 1) coarse-grained semantic clustering, 2) fine-grained job preference extraction, and 3) personalized top-k job recommendation. Initially, BISTRO segments the user interaction sequence into sessions and leverages session-based semantic clustering to achieve broad identification of person-job matching. Subsequently, we design a hypergraph wavelet learning method to capture the nuanced job preference drift. To mitigate the effect of noise in interactions caused by frequent preference drift, we innovatively propose an adaptive wavelet filtering technique to remove noisy interaction. Finally, a recurrent neural network is utilized to analyze session-based interaction for inferring personalized preferences. Extensive experiments on three real-world offline recruitment datasets demonstrate the significant performances of our framework. Significantly, BISTRO also excels in online experiments, affirming its effectiveness in live recruitment settings. This dual success underscores the robustness and adaptability of BISTRO. The source code is available at https://github.com/Applied-Machine-Learning-Lab/BISTRO. | Xiao Han, Chen Zhu, Xiao Hu, Chuan Qin, Xiangyu Zhao, Hengshu Zhu | Career Science Lab, BOSS Zhipin, Beijing, China; Career Science Lab, BOSS Zhipin, University of Science and Technology of China, Beijing, China; City University of Hong Kong, Hong Kong, China |
|  |  [EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration](https://doi.org/10.1145/3637528.3671775) |  | 0 | Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods. Our source code will be publicly available on PapersWithCode.com. | Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan Li, Linjun Li, Yan Xia, Zhou Zhao, Zhenhua Dong | Zhejiang University, Hangzhou, China; Huawei Noah's Ark Lab, Shenzhen, China; Zhejiang University, Hangzhou, Zhejiang, China |
|  |  [Learning to Bid the Interest Rate in Online Unsecured Personal Loans](https://doi.org/10.1145/3637528.3671584) |  | 0 | The unsecured personal loan (UPL) market is a multi-billion dollar market where numerous financial institutions compete. Due to the development of online banking, loan applicants start to compare numerous loan products. They aim for high loan limits and low interest rates. Since loan applicants have a desired loan amount, institutions instead focus on adjusting interest rates. Despite the importance of determining optimal interest strategies, institutions have traditionally relied on heuristic methods by human experts to set interest rates. This is done by adding a target return on assets (ROA) to the applicant's expected default probability predicted by a credit scoring system (CSS) such as the FICO score. We conceptualize the UPL market dynamics as a repeated auction scenario, where loan applicants (akin to sellers) seek the lowest interest rates, while financial institutions (akin to bidders) aim to maximize profits through higher interest rates. To the best of our knowledge, this is the first time anyone has approached the UPL market through the viewpoint of a repeated auction. While there are several research done in learning to bid in repeated auctions, those works cannot be directly applied to the UPL market due to the lack of any feedback about other bidders' strategies and the need to satisfy the bidder's target loan volume and profit variance. We present an algorithm named AutoInterest, which is a modification of the dual gradient descent algorithm. In addition, we provide a framework to evaluate interest rate bidding strategies on a benchmark dataset and the credit bureau dataset of actual loan applicants in South Korea. We evaluate AutoInterest on this framework and show higher cumulative profit compared to other common online algorithms and the current fixed strategy used by real institutions. | Dong Jun Jee, Seung Jung Jin, JiHoon Yoo, Byunggyu Ahn | PFC Technologies, Seoul, Republic of Korea |
|  |  [Debiased Recommendation with Noisy Feedback](https://doi.org/10.1145/3637528.3671915) |  | 0 | Ratings of a user to most items in recommender systems are usually missing not at random (MNAR), largely because users are free to choose which items to rate. To achieve unbiased learning of the prediction model under MNAR data, three typical solutions have been proposed, including error-imputation-based (EIB), inverse-propensity-scoring (IPS), and doubly robust (DR) methods. However, these methods ignore an alternative form of bias caused by the inconsistency between the observed ratings and the users' true preferences, also known as noisy feedback or outcome measurement errors (OME), e.g., due to public opinion or low-quality data collection process. In this work, we study intersectional threats to the unbiased learning of the prediction model from data MNAR and OME in the collected data. First, we design OME-EIB, OME-IPS, and OME-DR estimators, which largely extend the existing estimators to combat OME in real-world recommendation scenarios. Next, we theoretically prove the unbiasedness and generalization bound of the proposed estimators. We further propose an alternate denoising training approach to achieve unbiased learning of the prediction model under MNAR data with OME. Extensive experiments are conducted on three real-world datasets and one semi-synthetic dataset to show the effectiveness of our proposed approaches. The code is available at https://github.com/haoxuanli-pku/KDD24-OME-DR. | Haoxuan Li, Chunyuan Zheng, Wenjie Wang, Hao Wang, Fuli Feng, XiaoHua Zhou | Zhejiang University, Hangzhou, China; University of Science and Technology of China, Hefei, China; University of California, San Diego, Beijing, China; National University of Singapore, Singapore, Singapore; Peking University, Beijing, China |
|  |  [A Hierarchical and Disentangling Interest Learning Framework for Unbiased and True News Recommendation](https://doi.org/10.1145/3637528.3671944) |  | 0 | In the era of information explosion, news recommender systems are crucial for users to effectively and efficiently discover their interested news. However, most of the existing news recommender systems face two major issues, hampering recommendation quality. Firstly, they often oversimplify users' reading interests, neglecting their hierarchical nature, spanning from high-level event (e.g., US Election) related interests to low-level news article-specifc interests. Secondly, existing work often assumes a simplistic context, disregarding the prevalence of fake news and political bias under the real-world context. This oversight leads to recommendations of biased or fake news, posing risks to individuals and society. To this end, this paper addresses these gaps by introducing a novel framework, the Hierarchical and Disentangling Interest learning framework (HDInt). HDInt incorporates a hierarchical interest learning module and a disentangling interest learning module. The former captures users' high- and low-level interests, enhancing next-news recommendation accuracy. The latter effectively separates polarity and veracity information from news contents and model them more specifcally, promoting fairness- and truth-aware reading interest learning for unbiased and true news recommendations. Extensive experiments on two real-world datasets demonstrate HDInt's superiority over state-of-the-art news recommender systems in delivering accurate, unbiased, and true news recommendations. | Shoujin Wang, Wentao Wang, Xiuzhen Zhang, Yan Wang, Huan Liu, Fang Chen | RMIT University, Melbourne, Australia; University of Technology Sydney, Sydney, Australia; Arizona State University, Tempe, USA; Macquarie University, Sydney, Australia |
|  |  [Warming Up Cold-Start CTR Prediction by Learning Item-Specific Feature Interactions](https://doi.org/10.1145/3637528.3671784) |  | 0 | In recommendation systems, new items are continuously introduced, initially lacking interaction records but gradually accumulating them over time. Accurately predicting the click-through rate (CTR) for these items is crucial for enhancing both revenue and user experience. While existing methods focus on enhancing item ID embeddings for new items within general CTR models, they tend to adopt a global feature interaction approach, often overshadowing new items with sparse data by those with abundant interactions. Addressing this, our work introduces EmerG, a novel approach that warms up cold-start CTR prediction by learning item-specific feature interaction patterns. EmerG utilizes hypernetworks to generate an item-specific feature graph based on item characteristics, which is then processed by a Graph Neural Network (GNN). This GNN is specially tailored to provably capture feature interactions at any order through a customized message passing mechanism. We further design a meta learning strategy that optimizes parameters of hypernetworks and GNN across various item CTR prediction tasks, while only adjusting a minimal set of item-specific parameters within each task. This strategy effectively reduces the risk of overfitting when dealing with limited data. Extensive experiments on benchmark datasets validate that EmerG consistently performs the best given no, a few and sufficient instances of new items. | Yaqing Wang, Hongming Piao, Daxiang Dong, Quanming Yao, Jingbo Zhou | Department of Electronic Engineering, Tsinghua University, Beijing, China; Baidu Research, Baidu Inc., Beijing, China; Baidu AI Cloud, Baidu Inc., Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong |
|  |  [Improving Multi-modal Recommender Systems by Denoising and Aligning Multi-modal Content and User Feedback](https://doi.org/10.1145/3637528.3671703) |  | 0 | Multi-modal recommender systems (MRSs) are pivotal in diverse online web platforms and have garnered considerable attention in recent years. However, previous studies overlook the challenges of (1)noisy multi-modal content, (2) noisy user feedback, and (3) aligning multi-modal content and user feedback. To tackle these challenges, we propose Denoising and Aligning Multi-modal Recommender System (DA-MRS). To mitigate noise in multi-modal content, DA-MRS first constructs item-item graphs determined by consistent content similarity across modalities. To denoise user feedback, DA-MRS associates the probability of observed feedback with multi-modal content and devises a denoised BPR loss. Furthermore, DA-MRS implements Alignment guided by User preference to enhance task-specific item representation and Alignment guided by graded Item relations to provide finer-grained alignment. Extensive experiments verify that DA-MRS is a plug-and-play framework and achieves significant and consistent improvements across various datasets, backbone models, and noisy scenarios. | Guipeng Xv, Xinyu Li, Ruobing Xie, Chen Lin, Chong Liu, Feng Xia, Zhanhui Kang, Leyu Lin | Tencent, Beijing, China; School of Informatics, Xiamen University, Xiamen, Fujian, China |
|  |  [DDCDR: A Disentangle-based Distillation Framework for Cross-Domain Recommendation](https://doi.org/10.1145/3637528.3671605) |  | 0 | Modern recommendation platforms frequently encompass multiple domains to cater to the varied preferences of users. Recently, cross-domain learning has gained traction as a significant paradigm within the context of recommendation systems, enabling the leveraging of rich information from a well-endowed source domain to enhance a target domain, often limited by inadequate data resources. A primary concern in cross-domain recommendation is the mitigation of negative transfer-ensuring the selective transference of pertinent knowledge from the source (domain-shared knowledge) while maintaining the integrity of domain-unique insights within the target domain (domain-specific knowledge). In this paper, we propose a novel Disentangle-based Distillation Framework for Cross-Domain Recommendation (DDCDR), designed to operate at the representational level and rooted in the established teacher-student knowledge distillation paradigm. Our methodology begins with the development of a cross-domain teacher model, trained adversarially alongside a domain discriminator. This is followed by the creation of a target domain-specific student model. By employing the trained domain discriminator, we successfully segregate domain-shared from domain-specific representations. The teacher model guides the learning of domain-shared features, while domain-specific features are enhanced via contrastive learning methods. Experiments conducted on both public datasets and an industrial dataset demonstrate DDCDR achieves a new state-of-the-art performance. The implementation within Ant Group's platform further confirms its online efficacy, manifesting relative improvements of 0.33% and 0.45% in Unique Visitor Click-Through Rate (UVCTR) across two distinct recommendation scenarios, compared to baseline performances. | Zhicheng An, Zhexu Gu, Li Yu, Ke Tu, Zhengwei Wu, Binbin Hu, Zhiqiang Zhang, Lihong Gu, Jinjie Gu | Ant Group, Hangzhou, Zhejiang, China |
|  |  [Rankability-enhanced Revenue Uplift Modeling Framework for Online Marketing](https://doi.org/10.1145/3637528.3671516) |  | 0 | Uplift modeling has been widely employed in online marketing by predicting the response difference between the treatment and control groups, so as to identify the sensitive individuals toward interventions like coupons or discounts. Compared with traditional conversion uplift modeling,revenue uplift modeling exhibits higher potential due to its direct connection with the corporate income. However, previous works can hardly handle the continuous long-tail response distribution in revenue uplift modeling. Moreover, they have neglected to optimize the uplift ranking among different individuals, which is actually the core of uplift modeling. To address such issues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss to regress the responses and customize the corresponding modeling network, which can be adapted to different existing uplift models. Then, we study the ranking-related uplift modeling error from the theoretical perspective and propose two tighter error bounds as the additional loss terms to the conventional response regression loss. Finally, we directly model the uplift ranking error for the entire population with a listwise uplift ranking loss. The experiment results on offline public and industrial datasets validate the effectiveness of our method for revenue uplift modeling. Furthermore, we conduct large-scale experiments on a prominent online fintech marketing platform, Tencent FiT, which further demonstrates the superiority of our method in real-world applications. | Bowei He, Yunpeng Weng, Xing Tang, Ziqiang Cui, Zexu Sun, Liang Chen, Xiuqiang He, Chen Ma | Renmin University of China, Beijing, China; City University of Hong Kong, Hong Kong, Hong Kong; FiT, Tencent, Shenzhen, China |
|  |  [Personalized Product Assortment with Real-time 3D Perception and Bayesian Payoff Estimation](https://doi.org/10.1145/3637528.3671518) |  | 0 | Product assortment selection is a critical challenge facing physical retailers. Effectively aligning inventory with the preferences of shoppers can increase sales and decrease out-of-stocks. However, in real-world settings the problem is challenging due to the combinatorial explosion of product assortment possibilities. Consumer preferences are typically heterogeneous across space and time, making inventory-preference alignment challenging. Additionally, existing strategies rely on syndicated data, which tends to be aggregated, low resolution, and suffer from high latency. To solve these challenges, we introduce a real-time recommendation system, which we call EdgeRec3D. Our system utilizes recent advances in 3D computer vision for perception and automatic, fine grained sales estimation. These perceptual components run on the edge of the network and facilitate real-time reward signals. Additionally, we develop a Bayesian payoff model to account for noisy estimates from 3D LIDAR data. We rely on spatial clustering to allow the system to adapt to heterogeneous consumer preferences, and a graph-based candidate generation algorithm to address the combinatorial search problem. We test our system in real-world stores across two, 6-8 week A/B tests with beverage products and demonstrate a 35% and 27% increase in sales respectively. Finally, we monitor the deployed system for a period of 28 weeks with an observational study and show a 9.4% increase in sales. | Porter Jenkins, Michael Selander, J. Stockton Jenkins, Andrew Merrill, Kyle Armstrong | Delicious AI, Lehi, UT, USA; Department of Computer Science, Brigham Young University, Provo, UT, USA |
|  |  [Where Have You Been? A Study of Privacy Risk for Point-of-Interest Recommendation](https://doi.org/10.1145/3637528.3671758) |  | 0 | As location-based services (LBS) have grown in popularity, more human mobility data has been collected. The collected data can be used to build machine learning (ML) models for LBS to enhance their performance and improve overall experience for users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges. | Kunlin Cai, Jinghuai Zhang, Zhiqing Hong, William Shand, Guang Wang, Desheng Zhang, Jianfeng Chi, Yuan Tian | Florida State University, Tallahassee, FL, USA; Meta, New York, NY, USA; Rutgers University, New Brunswick, NJ, USA; University of California, Los Angeles, Los Angeles, CA, USA |
|  |  [Harm Mitigation in Recommender Systems under User Preference Dynamics](https://doi.org/10.1145/3637528.3671925) |  | 0 | We consider a recommender system that takes into account the interplaybetween recommendations, the evolution of user interests, and harmful content.We model the impact of recommendations on user behavior, particularly thetendency to consume harmful content. We seek recommendation policies thatestablish a tradeoff between maximizing click-through rate (CTR) and mitigatingharm. We establish conditions under which the user profile dynamics have astationary point, and propose algorithms for finding an optimal recommendationpolicy at stationarity. We experiment on a semi-synthetic movie recommendationsetting initialized with real data and observe that our policies outperformbaselines at simultaneously maximizing CTR and mitigating harm. | Jerry Chee, Shankar Kalyanaraman, Sindhu Kiranmai Ernala, Udi Weinsberg, Sarah Dean, Stratis Ioannidis | Northeastern University, Boston, MA, USA; Meta, Menlo Park, CA, USA; Cornell University, Ithaca, NY, USA |
|  |  [Neural Retrievers are Biased Towards LLM-Generated Content](https://doi.org/10.1145/3637528.3671882) |  | 0 | Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search, by generating vast amounts of human-like texts on the Internet. As a result, IR systems in the LLM era are facing a new challenge: the indexed documents are now not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrievers towards the LLM-generated content as the source bias. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, in-depth analyses from the perspective of text compression indicate that LLM-generated texts exhibit more focused semantics with less noise, making it easier for neural retrieval models to semantic match. To mitigate the source bias, we also propose a plug-and-play debiased constraint for the optimization objective, and experimental results show its effectiveness. Finally, we discuss the potential severe concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks are available at https://github.com/KID-22/Source-Bias. | Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Jun Xu | Noah's Ark Lab, Huawei, Shenzhen, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation](https://doi.org/10.1145/3637528.3672008) |  | 0 | Recommender systems play important roles in various applications such ase-commerce, social media, etc. Conventional recommendation methods usuallymodel the collaborative signals within the tabular representation space.Despite the personalization modeling and the efficiency, the latent semanticdependencies are omitted. Methods that introduce semantics into recommendationthen emerge, injecting knowledge from the semantic representation space wherethe general language understanding are compressed. However, existingsemantic-enhanced recommendation methods focus on aligning the two spaces,during which the representations of the two spaces tend to get close while theunique patterns are discarded and not well explored. In this paper, we proposeDisCo to Disentangle the unique patterns from the two representation spaces andCollaborate the two spaces for recommendation enhancement, where both thespecificity and the consistency of the two spaces are captured. Concretely, wepropose 1) a dual-side attentive network to capture the intra-domain patternsand the inter-domain patterns, 2) a sufficiency constraint to preserve thetask-relevant information of each representation space and filter out thenoise, and 3) a disentanglement constraint to avoid the model from discardingthe unique information. These modules strike a balance between disentanglementand collaboration of the two representation spaces to produce informativepattern vectors, which could serve as extra features and be appended toarbitrary recommendation backbones for enhancement. Experiment results validatethe superiority of our method against different models and the compatibility ofDisCo over different backbones. Various ablation studies and efficiencyanalysis are also conducted to justify each model component. | Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang, Xinyi Dai, Bo Chen, Ruiming Tang, Weinan Zhang | Huawei Noah's Ark Lab, Shanghai, China; Huawei Noah's Ark Lab, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China |
|  |  [Label Shift Correction via Bidirectional Marginal Distribution Matching](https://doi.org/10.1145/3637528.3671867) |  | 0 | Due to the timeliness and uncertainty of data acquisition, label shift, which assumes that the source (training) and target (test) label distributions differ, occurs with the changing environment and reduces the generalization ability of traditional models. To correct the label shift, existing methods estimate the true label distribution by prediction of target data from a source classifier, which results in high variance, especially with large label shift. In this paper, we tackle this problem by proposing a novel approach termed as Label Shift Correction via Bidirectional Marginal Distribution Matching (BMDM). Our approach matchs the label and feature marginal distributions simultaneously to ensure the stability of estimated class proportions. We prove theoretically that there is a unique optimal solution, i.e., true target label distribution, for our approach under mild conditions, and an efficient optimization strategy is also proposed. On this basis, in multi-shot scenario where label distribution changes continuously, we extend BMDM by designing a new distribution matching mechanism and constructing a regularization term that constrains the direction of label distribution change. Extensive experimental results validate the effectiveness of our approach over existing state-of-the-arts methods. | Ruidong Fan, Xiao Ouyang, Hong Tao, Chenping Hou | National University of Defense Technology, Changsha, Hunan, China |
|  |  [On (Normalised) Discounted Cumulative Gain as an Off-Policy Evaluation Metric for Top-n Recommendation](https://doi.org/10.1145/3637528.3671687) |  | 0 | Approaches to recommendation are typically evaluated in one of two ways: (1)via a (simulated) online experiment, often seen as the gold standard, or (2)via some offline evaluation procedure, where the goal is to approximate theoutcome of an online experiment. Several offline evaluation metrics have beenadopted in the literature, inspired by ranking metrics prevalent in the fieldof Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is onesuch metric that has seen widespread adoption in empirical studies, and higher(n)DCG values have been used to present new methods as the state-of-the-art intop-n recommendation for many years. Our work takes a critical look at this approach, and investigates when we canexpect such metrics to approximate the gold standard outcome of an onlineexperiment. We formally present the assumptions that are necessary to considerDCG an unbiased estimator of online reward and provide a derivation for thismetric from first principles, highlighting where we deviate from itstraditional uses in IR. Importantly, we show that normalising the metricrenders it inconsistent, in that even when DCG is unbiased, ranking competingmethods by their normalised DCG can invert their relative order. Through acorrelation analysis between off- and on-line experiments conducted on alarge-scale recommendation platform, we show that our unbiased DCG estimatesstrongly correlate with online reward, even when some of the metric's inherentassumptions are violated. This statement no longer holds for its normalisedvariant, suggesting that nDCG's practical utility may be limited. | Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko | ShareChat, London, United Kingdom; ShareChat, Edinburgh, United Kingdom |
|  |  [FairMatch: Promoting Partial Label Learning by Unlabeled Samples](https://doi.org/10.1145/3637528.3671685) |  | 0 | This paper studies the semi-supervised partial label learning (SSPLL) problem, which aims to improve the partial label learning (PLL) by leveraging unlabeled samples. Both the existing SSPLL methods and the semi-supervised learning methods exploit the information in unlabeled samples by selecting high-confidence unlabeled samples as the pseudo labels based on the maximum value of the model output. However, the scarcity of labeled samples and the ambiguity from partial labels skew this strategy towards an unfair selection of high-confidence samples on each class, most notably during the initial phases of training, resulting in slower training and performance degradation. In this paper, we propose a novel method FairMatch, which adopts a learning state aware self-adaptive threshold for selecting the same number of high-confidence samples on each class, and uses augmentation consistency to incorporate the unlabeled samples to promote PLL. In addition, we adopt the candidate label disambiguation to utilize the partial labeled samples and mix up the partial labeled samples and the selected high-confidence unlabeled samples to prevent the model from overfitting on partial label samples. FairMatch can achieve maximum accuracy improvements of 9.53%, 4.9%, and 16.45% on CIFAR-10, CIFAR-100, and CIFAR-100H, respectively. The codes can be found at https://github.com/jhjiangSEU/FairMatch. | Jiahao Jiang, Yuheng Jia, Hui Liu, Junhui Hou | Department of Computer Science, City University of Hong Kong, HongKong, China; School of Computing & Information Sciences, Saint Francis University, HongKong, China; College of Software Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China |
|  |  [Privileged Knowledge State Distillation for Reinforcement Learning-based Educational Path Recommendation](https://doi.org/10.1145/3637528.3671872) |  | 0 | Educational recommendation seeks to suggest knowledge concepts that match a learner's ability, thus facilitating a personalized learning experience. In recent years, reinforcement learning (RL) methods have achieved considerable results by taking the encoding of the learner's exercise log as the state and employing an RL-based agent to make suitable recommendations. However, these approaches suffer from handling the diverse and dynamic learner's knowledge states. In this paper, we introduce the privileged feature distillation technique and propose the P rivileged K nowledge S tate D istillation (PKSD ) framework, allowing the RL agent to leverage the "actual'' knowledge state as privileged information in the state encoding to help tailor recommendations to meet individual needs. Concretely, our PKSD takes the privileged knowledge states together with the representations of the exercise log for the state representations during training. And through distillation, we transfer the ability to adapt to learners to aknowledge state adapter. During inference, theknowledge state adapter would serve as the estimated privileged knowledge states instead of the real one since it is not accessible. Considering that there are strong connections among the knowledge concepts in education, we further propose to collaborate the graph structure learning for concepts into our PKSD framework. This new approach is termed GEPKSD (Graph-Enhanced PKSD). As our method is model-agnostic, we evaluate PKSD and GEPKSD by integrating them with five different RL bases on four public simulators, respectively. Our results verify that PKSD can consistently improve the recommendation performance with various RL methods, and our GEPKSD could further enhance the effectiveness of PKSD in all the simulations. | Qingyao Li, Wei Xia, Li'ang Yin, Jiarui Jin, Yong Yu | Huawei Noah's Ark Lab, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China |
|  |  [Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach](https://doi.org/10.1145/3637528.3671848) |  | 0 | Recent studies successfully learned static graph embeddings that arestructurally fair by preventing the effectiveness disparity of high- andlow-degree vertex groups in downstream graph mining tasks. However, achievingstructure fairness in dynamic graph embedding remains an open problem.Neglecting degree changes in dynamic graphs will significantly impair embeddingeffectiveness without notably improving structure fairness. This is because theembedding performance of high-degree and low-to-high-degree vertices willsignificantly drop close to the generally poorer embedding performance of mostslightly changed vertices in the long-tail part of the power-law distribution.We first identify biased structural evolutions in a dynamic graph based on theevolving trend of vertex degree and then propose FairDGE, the firststructurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biasedstructural evolutions by jointly embedding the connection changes amongvertices and the long-short-term evolutionary trend of vertex degrees.Furthermore, a novel dual debiasing approach is devised to encode fairembeddings contrastively, customizing debiasing strategies for different biasedstructural evolutions. This innovative debiasing strategy breaks theeffectiveness bottleneck of embeddings without notable fairness loss. Extensiveexperiments demonstrate that FairDGE achieves simultaneous improvement in theeffectiveness and fairness of embeddings. | Yicong Li, Yu Yang, Jiannong Cao, Shuaiqi Liu, Haoran Tang, Guandong Xu | The Education University of Hong Kong & University of Technology Sydney, Hong Kong, Hong Kong; The Hong Kong Polytechnic University, Hong Kong, Hong Kong |
|  |  [Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation](https://doi.org/10.1145/3637528.3671884) |  | 0 | Harnessing Large Language Models (LLMs) for recommendation is rapidly emerging, which relies on two fundamental steps to bridge the recommendation item space and the language space: 1) item indexing utilizes identifiers to represent items in the language space, and 2) generation grounding associates LLMs' generated token sequences to in-corpus items. However, previous methods exhibit inherent limitations in the two steps. Existing ID-based identifiers (e.g., numeric IDs) and description-based identifiers (e.g., titles) either lose semantics or lack adequate distinctiveness. Moreover, prior generation grounding methods might generate invalid identifiers, thus misaligning with in-corpus items. To address these issues, we propose a novel Transition paradigm for LLM-based Recommender (named TransRec) to bridge items and language. Specifically, TransRec presents multi-facet identifiers, which simultaneously incorporate ID, title, and attribute for item indexing to pursue both distinctiveness and semantics. Additionally, we introduce a specialized data structure for TransRec to ensure generating valid identifiers only and utilize substring indexing to encourage LLMs to generate from any position of identifiers. Lastly, TransRec presents an aggregated grounding module to leverage generated multi-facet identifiers to rank in-corpus items efficiently. We instantiate TransRec on two backbone models, BART-large and LLaMA-7B. Extensive results on three real-world datasets under diverse settings validate the superiority of TransRec. | Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, SeeKiong Ng, TatSeng Chua | The Hong Kong Polytechnic University, Hong Kong SAR, China; University of Science and Technology of China, Hefei, China; National University of Singapore, Singapore, Singapore |
|  |  [BadSampler: Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning](https://doi.org/10.1145/3637528.3671879) |  | 0 | Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model's generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-k sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks. | Yi Liu, Cong Wang, Xingliang Yuan | The University of Melbourne, Melbourne, Australia; City University of Hong Kong, Hong Kong, China |
|  |  [Dataset Condensation for Time Series Classification via Dual Domain Matching](https://doi.org/10.1145/3637528.3671675) |  | 0 | Time series data has been demonstrated to be crucial in various researchfields. The management of large quantities of time series data presentschallenges in terms of deep learning tasks, particularly for training a deepneural network. Recently, a technique named Dataset Condensation hasemerged as a solution to this problem. This technique generates a smallersynthetic dataset that has comparable performance to the full real dataset indownstream tasks such as classification. However, previous methods areprimarily designed for image and graph datasets, and directly adapting them tothe time series dataset leads to suboptimal performance due to their inabilityto effectively leverage the rich information inherent in time series data,particularly in the frequency domain. In this paper, we propose a novelframework named Dataset Condensation forTime SeriesClassification via Dual Domain Matching (CondTSC)which focuses on the time series classification dataset condensation task.Different from previous methods, our proposed framework aims to generate acondensed dataset that matches the surrogate objectives in both the time andfrequency domains. Specifically, CondTSC incorporates multi-view dataaugmentation, dual domain training, and dual surrogate objectives to enhancethe dataset condensation process in the time and frequency domains. Throughextensive experiments, we demonstrate the effectiveness of our proposedframework, which outperforms other baselines and learns a condensed syntheticdataset that exhibits desirable characteristics such as conforming to thedistribution of the original data. | Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu | Shanghai Jiao Tong University, Shanghai, China; Ocean University of China, Qingdao, China |
|  |  [Self-Supervised Denoising through Independent Cascade Graph Augmentation for Robust Social Recommendation](https://doi.org/10.1145/3637528.3671958) |  | 0 | Social Recommendation (SR) typically exploits neighborhood influence in the social network to enhance user preference modeling. However, users' intricate social behaviors may introduce noisy social connections for user modeling and harm the models' robustness. Existing solutions to alleviate social noise either filter out the noisy connections or generate new potential social connections. Due to the absence of labels, the former approaches may retain uncertain connections for user preference modeling while the latter methods may introduce additional social noise. Through data analysis, we discover that (1) social noise likely comes from the connected users with low preference similarity; and (2) Opinion Leaders (OLs) play a pivotal role in influence dissemination, surpassing high-similarity neighbors, regardless of their preference similarity with trusting peers. Guided by these observations, we propose a novel Self-Supervised Denoising approach through Independent Cascade Graph Augmentation, for more robust SR. Specifically, we employ the independent cascade diffusion model to generate an augmented graph view, which traverses the social graph and activates the edges in sequence to simulate the cascading influence spread. To steer the augmentation towards a denoised social graph, we (1) introduce a hierarchical contrastive loss to prioritize the activation of OLs first, followed by high-similarity neighbors, while weakening the low-similarity neighbors; and (2) integrate an information bottleneck based contrastive loss, aiming to minimize mutual information between original and augmented graphs yet preserve sufficient information for improved SR. Experiments conducted on two public datasets demonstrate that our model outperforms the state-of-the-art while also exhibiting higher robustness to different extents of social noise. | Youchen Sun, Zhu Sun, Yingpeng Du, Jie Zhang, Yew Soon Ong | ASTAR Centre for Frontier AI Research & Nanyang Technological University, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore |
|  |  [Revisiting Local PageRank Estimation on Undirected Graphs: Simple and Optimal](https://doi.org/10.1145/3637528.3671820) |  | 0 | We propose a simple and optimal algorithm, BackMC, for local PageRank estimation in undirected graphs: given an arbitrary target node t in an undirected graph G comprising n nodes and m edges, BackMC accurately estimates the PageRank score of node t while assuring a small relative error and a high success probability. The worst-case computational complexity of BackMC is upper bounded by O(1/dmin ⋅ min(dt, m1/2)), where dmin denotes the minimum degree of G, and dt denotes the degree of t, respectively. Compared to the previously best upper bound of O(log n ⋅ min(dt, m1/2)) (VLDB '23), which is derived from a significantly more complex algorithm and analysis, our BackMC improves the computational complexity for this problem by a factor of Θ(log n/dmin) with a much simpler algorithm. Furthermore, we establish a matching lower bound of Ω(1/dmin ⋅ min(dt, m1/2)) for any algorithm that attempts to solve the problem of local PageRank estimation, demonstrating the theoretical optimality of our BackMC. We conduct extensive experiments on various large-scale real-world and synthetic graphs, where BackMC consistently shows superior performance. | Hanzhi Wang | Renmin University of China, Beijing, China |
|  |  [Performative Debias with Fair-exposure Optimization Driven by Strategic Agents in Recommender Systems](https://doi.org/10.1145/3637528.3671786) |  | 0 | Data bias, e.g., popularity impairs the dynamics of two-sided markets within recommender systems. This overshadows the less visible but potentially intriguing long-tail items that could capture user interest. Despite the abundance of research surrounding this issue, it still poses challenges and remains a hot topic in academic circles. Along this line, in this paper, we developed a re-ranking approach in dynamic settings with fair-exposure optimization driven by strategic agents. Designed for the producer side, the execution of agents assumes content creators can modify item features based on strategic incentives to maximize their exposure. This iterative process entails an end-to-end optimization, employing differentiable ranking operators that simultaneously target accuracy and fairness. Joint objectives ensure the performance of recommendations while enhancing the visibility of tail items. We also leveraged the performativity nature of predictions to illustrate how strategic learning influences content creators to shift towards fairness efficiently, thereby incentivizing features of tail items. Through comprehensive experiments on both public and industrial datasets, we have substantiated the effectiveness and dominance of the proposed method especially on unveiling the potential of tail items. | Zhichen Xiang, Hongke Zhao, Chuang Zhao, Ming He, Jianping Fan | AI Lab at Lenovo Research, Beijing, China; The Hong Kong University of Science and Technology, Hong Kong, China |
|  |  [Preventing Strategic Behaviors in Collaborative Inference for Vertical Federated Learning](https://doi.org/10.1145/3637528.3671663) |  | 0 | Vertical federated learning (VFL) is an emerging collaborative machine learning paradigm to facilitate the utilization of private features distributed across multiple parties. During the inference process of VFL, the involved parties need to upload their local embeddings to be aggregated for the final prediction. Despite its remarkable performances, the inference process of the current VFL system is vulnerable to the strategic behavior of involved parties, as they could easily change the uploaded local embeddings to exert direct influences on the prediction result. In a representative case study of federated recommendation, we find the allocation of display opportunities to be severely disrupted due to the parties' preferences in display content. In order to elicit the true local embeddings for VFL system, we propose a distribution-based penalty mechanism to detect and penalize the strategic behaviors in collaborative inference. As the key motivation of our design, we theoretically prove the power of constraining the distribution of uploaded embeddings in preventing the dishonest parties from achieving higher utility. Our mechanism leverages statistical two-sample tests to distinguish whether the distribution of uploaded embeddings is reasonable, and penalize the dishonest party through deactivating her uploaded embeddings. The resulted mechanism could be shown to admit truth-telling to converge to a Bayesian Nash equilibrium asymptotically under mild conditions. The experimental results further demonstrate the effectiveness of the proposed mechanism to reduce the dishonest utility increase of strategic behaviors and promote the truthful uploading of local embeddings in inferences. | Yidan Xing, Zhenzhe Zheng, Fan Wu | Shanghai Jiao Tong University, Shanghai, China |
|  |  [Extreme Meta-Classification for Large-Scale Zero-Shot Retrieval](https://doi.org/10.1145/3637528.3672046) |  | 0 | We develop accurate and efficient solutions for large-scale retrieval tasks where novel (zero-shot) items can arrive continuously at a rapid pace. Conventional Siamese-style approaches embed both queries and items through a small encoder and retrieve the items lying closest to the query. While this approach allows efficient addition and retrieval of novel items, the small encoder lacks sufficient capacity for the necessary world knowledge in complex retrieval tasks. The extreme classification approaches have addressed this by learning a separate classifier for each item observed in the training set which significantly increases the representation capacity of the model. Such classifiers outperform Siamese approaches on observed items, but cannot be trained for novel items due to data and latency constraints. To bridge these gaps, this paper develops: (1) A new algorithmic framework, EMMETT, which efficiently synthesizes classifiers on-the-fly for novel items, by relying on the readily available classifiers for observed items; (2) A new algorithm, IRENE, which is a simple and effective instance of EMMETT that is specifically suited for large-scale deployments, and (3) A new theoretical framework for analyzing the generalization performance in large-scale zero-shot retrieval which guides our algorithm and training related design decisions. Comprehensive experiments are conducted on a wide range of retrieval tasks which demonstrate that IRENE improves the zero-shot retrieval accuracy by up to 15% points in Recall@10 when added on top of leading encoders. Additionally, on an online A/B test in a large-scale ad retrieval task in a major search engine, IRENE improved the ad click-through rate by 4.2%. Lastly, we validate our design choices through extensive ablative experiments. The source code for IRENE is available at https://aka.ms/irene. | Sachin Yadav, Deepak Saini, Anirudh Buvanesh, Bhawna Paliwal, Kunal Dahiya, Siddarth Asokan, Yashoteja Prabhu, Jian Jiao, Manik Varma | Microsoft Research, Bangalore, India; Indian Institute of Technology, Delhi, India; Microsoft, Redmond, WA, USA |
|  |  [Conversational Dueling Bandits in Generalized Linear Models](https://doi.org/10.1145/3637528.3671892) |  | 0 | Conversational recommendation systems elicit user preferences by interacting with users to obtain their feedback on recommended commodities. Such systems utilize a multi-armed bandit framework to learn user preferences in an online manner and have received great success in recent years. However, existing conversational bandit methods have several limitations. First, they only enable users to provide explicit binary feedback on the recommended items or categories, leading to ambiguity in interpretation. In practice, users are usually faced with more than one choice. Relative feedback, known for its informativeness, has gained increasing popularity in recommendation system design. Moreover, current contextual bandit methods mainly work under linear reward assumptions, ignoring practical non-linear reward structures in generalized linear models. Therefore, in this paper, we introduce relative feedback-based conversations into conversational recommendation systems through the integration of dueling bandits in generalized linear models (GLM) and propose a novel conversational dueling bandit algorithm called ConDuel. Theoretical analyses of regret upper bounds and empirical validations on synthetic and real-world data underscore ConDuel's efficacy. We also demonstrate the potential to extend our algorithm to multinomial logit bandits with theoretical and experimental guarantees, which further proves the applicability of the proposed framework. | Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang, Huazheng Wang | ByteDance, Beijing, China; University of Science and Technology of China, Hefei, China; Princeton University, Princeton, NJ, USA; Oregon State University, Corvallis, OR, USA |
|  |  [User Welfare Optimization in Recommender Systems with Competing Content Creators](https://doi.org/10.1145/3637528.3672021) |  | 0 | Driven by the new economic opportunities created by the creator economy, an increasing number of content creators rely on and compete for revenue generated from online content recommendation platforms. This burgeoning competition reshapes the dynamics of content distribution and profoundly impacts long-term user welfare on the platform. However, the absence of a comprehensive picture of global user preference distribution often traps the competition, especially the creators, in states that yield sub-optimal user welfare. To encourage creators to best serve a broad user population with relevant content, it becomes the platform's responsibility to leverage its information advantage regarding user preference distribution to accurately signal creators. In this study, we perform system-side user welfare optimization under a competitive game setting among content creators. We propose an algorithmic solution for the platform, which dynamically computes a sequence of weights for each user based on their satisfaction of the recommended content. These weights are then utilized to design mechanisms that adjust the recommendation policy or the post-recommendation rewards, thereby influencing creators' content production strategies. To validate the effectiveness of our proposed method, we report our findings from a series of experiments, including: 1. a proof-of-concept negative example illustrating how creators' strategies converge towards sub-optimal states without platform intervention; 2. offline experiments employing our proposed intervention mechanisms on diverse datasets; and 3. results from a three-week online experiment conducted on Instagram Reels short-video recommendation platform. | Fan Yao, Yiming Liao, Mingzhe Wu, Chuanhao Li, Yan Zhu, James Yang, Jingzhou Liu, Qifan Wang, Haifeng Xu, Hongning Wang | University of Southern California, Los Angeles, USA; Google, Mountain View, USA; Yale University, New Haven, USA; University of Chicago, Chicago, USA; Meta Platforms, Inc., Menlo Park, USA; University of Virginia, Charlottesville, USA; Meta Platforms, Inc., New York, USA |
|  |  [Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity](https://doi.org/10.1145/3637528.3671941) |  | 0 | Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models. | Kyuhwan Yeom, Hyeongjun Yang, Gayeon Park, Myeongheon Jeon, Yunjeong Ko, Byungkook Oh, KyongHo Lee | Computer Science, Yonsei University, Seoul, Republic of Korea; Computer Science and Engineering, Konkuk University, Seoul, Republic of Korea; Artificial Intelligence, Yonsei University, Seoul, Republic of Korea |
|  |  [Item-Difficulty-Aware Learning Path Recommendation: From a Real Walking Perspective](https://doi.org/10.1145/3637528.3671947) |  | 0 | Learning path recommendation aims to provide learners with a reasonable order of items to achieve their learning goals. Intuitively, the learning process on the learning path can be metaphorically likened to walking. Despite extensive efforts in this area, most previous methods mainly focus on the relationship among items but overlook the difficulty of items, which may raise two issues from a real walking perspective: (1) The path may be rough: When learners tread the path without considering item difficulty, it's akin to walking a dark, uneven road, making learning harder and dampening interest. (2) The path may be inefficient: Allowing learners only a few attempts on very challenging items before switching, or persisting with a difficult item despite numerous attempts without mastery, can result in inefficiencies in the learning journey. To conquer the above limitations, we propose a novel method named Difficulty-constrained Learning Path Recommendation (DLPR), which is aware of item difficulty. Specifically, we first explicitly categorize items into learning items and practice items, then construct a hierarchical graph to model and leverage item difficulty adequately. Then we design a Difficulty-driven Hierarchical Reinforcement Learning (DHRL) framework to facilitate learning paths with efficiency and smoothness. Finally, extensive experiments on three different simulators demonstrate our framework achieves state-of-the-art performance. | Haotian Zhang, Shuanghong Shen, Bihan Xu, Zhenya Huang, Jinze Wu, Jing Sha, Shijin Wang | State Key Laboratory of Cognitive Intelligence & iFLYTEK AI Research, Hefei, China; iFLYTEK AI Research, Hefei, China |
|  |  [Optimized Cost Per Click in Online Advertising: A Theoretical Analysis](https://doi.org/10.1145/3637528.3671767) |  | 0 | In recent years, Optimized Cost Per Click (OCPC) and Optimized Cost Per Mille(OCPM) have emerged as the most widely adopted pricing models in the onlineadvertising industry. However, the existing literature has yet to identify thespecific conditions under which these models outperform traditional pricingmodels like Cost Per Click (CPC) and Cost Per Action (CPA). To fill the gap,this paper builds an economic model that compares OCPC with CPC and CPAtheoretically, which incorporates out-site scenarios and outside options as twokey factors. Our analysis reveals that OCPC can effectively replace CPA bytackling the problem of advertisers strategically manipulating conversionreporting in out-site scenarios where conversions occur outside the advertisingplatform. Furthermore, OCPC exhibits the potential to surpass CPC in platformpayoffs by providing higher advertiser payoffs and consequently attracting moreadvertisers. However, if advertisers have less competitive outside options andconsistently stay in the focal platform, the platform may achieve higherpayoffs using CPC. Our findings deliver valuable insights for onlineadvertising platforms in selecting optimal pricing models, and providerecommendations for further enhancing their payoffs. To the best of ourknowledge, this is the first study to analyze OCPC from an economicperspective. Moreover, our analysis can be applied to the OCPM model as well. | Kaichen Zhang, Zixuan Yuan, Hui Xiong |  |
|  |  [Counteracting Duration Bias in Video Recommendation via Counterfactual Watch Time](https://doi.org/10.1145/3637528.3671817) |  | 0 | In video recommendation, an ongoing effort is to satisfy users' personalizedinformation needs by leveraging their logged watch time. However, watch timeprediction suffers from duration bias, hindering its ability to reflect users'interests accurately. Existing label-correction approaches attempt to uncoveruser interests through grouping and normalizing observed watch time accordingto video duration. Although effective to some extent, we found that theseapproaches regard completely played records (i.e., a user watches the entirevideo) as equally high interest, which deviates from what we observed on realdatasets: users have varied explicit feedback proportion when completelyplaying videos. In this paper, we introduce the counterfactual watch time(CWT),the potential watch time a user would spend on the video if its duration issufficiently long. Analysis shows that the duration bias is caused by thetruncation of CWT due to the video duration limitation, which usually occurs onthose completely played records. Besides, a Counterfactual Watch Model (CWM) isproposed, revealing that CWT equals the time users get the maximum benefit fromvideo recommender systems. Moreover, a cost-based transform function is definedto transform the CWT into the estimation of user interest, and the model can belearned by optimizing a counterfactual likelihood function defined overobserved user watch times. Extensive experiments on three real videorecommendation datasets and online A/B testing demonstrated that CWMeffectively enhanced video recommendation accuracy and counteracted theduration bias. | Haiyuan Zhao, Guohao Cai, Jieming Zhu, Zhenhua Dong, Jun Xu, JiRong Wen | School of Information, Renmin University of China, Beijing, China; Noah's Ark Lab, Huawei, Shenzhen, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [MMBee: Live Streaming Gift-Sending Recommendations via Multi-Modal Fusion and Behaviour Expansion](https://doi.org/10.1145/3637528.3671511) |  | 0 | Live streaming services are becoming increasingly popular due to real-time interactions and entertainment. Viewers can chat and send comments or virtual gifts to express their preferences for the streamers. Accurately modeling the gifting interaction not only enhances users' experience but also increases streamers' revenue. Previous studies on live streaming gifting prediction treat this task as a conventional recommendation problem, and model users' preferences using categorical data and observed historical behaviors. However, it is challenging to precisely describe the real-time content changes in live streaming using limited categorical information. Moreover, due to the sparsity of gifting behaviors, capturing the preferences and intentions of users is quite difficult. In this work, we propose MMBee based on real-time Multi-Modal Fusion and Behaviour Expansion to address these issues. Specifically, we first present a Multi-modal Fusion Module with Learnable Query (MFQ) to perceive the dynamic content of streaming segments and process complex multi-modal interactions, including images, text comments and speech. To alleviate the sparsity issue of gifting behaviors, we present a novel Graph-guided Interest Expansion (GIE) approach that learns both user and streamer representations on large-scale gifting graphs with multi-modal attributes. It consists of two main parts: graph node representations pre-training and metapath-based behavior expansion, all of which help model jump out of the specific historical gifting behaviors for exploration and largely enrich the behavior representations. Comprehensive experiment results show that MMBee achieves significant performance improvements on both public datasets and Kuaishou real-world streaming datasets and the effectiveness has been further validated through online A/B experiments. MMBee has been deployed and is serving hundreds of millions of users at Kuaishou. | Jiaxin Deng, Shiyao Wang, Yuchen Wang, Jiansong Qi, Liqin Zhao, Guorui Zhou, Gaofeng Meng | Institute of Automation, Beijing, China; KuaiShou Inc., Beijing, China |
|  |  [Contextual Distillation Model for Diversified Recommendation](https://doi.org/10.1145/3637528.3671514) |  | 0 | The diversity of recommendation is equally crucial as accuracy in improvinguser experience. Existing studies, e.g., Determinantal Point Process (DPP) andMaximal Marginal Relevance (MMR), employ a greedy paradigm to iterativelyselect items that optimize both accuracy and diversity. However, prior methodstypically exhibit quadratic complexity, limiting their applications to there-ranking stage and are not applicable to other recommendation stages with alarger pool of candidate items, such as the pre-ranking and ranking stages. Inthis paper, we propose Contextual Distillation Model (CDM), an efficientrecommendation model that addresses diversification, suitable for thedeployment in all stages of industrial recommendation pipelines. Specifically,CDM utilizes the candidate items in the same user request as context to enhancethe diversification of the results. We propose a contrastive context encoderthat employs attention mechanisms to model both positive and negative contexts.For the training of CDM, we compare each target item with its context embeddingand utilize the knowledge distillation framework to learn the win probabilityof each target item under the MMR algorithm, where the teacher is derived fromMMR outputs. During inference, ranking is performed through a linearcombination of the recommendation and student model scores, ensuring bothdiversity and efficiency. We perform offline evaluations on two industrialdatasets and conduct online A/B test of CDM on the short-video platformKuaiShou. The considerable enhancements observed in both recommendation qualityand diversity, as shown by metrics, provide strong superiority for theeffectiveness of CDM. | Fan Li, Xu Si, Shisong Tang, Dingmin Wang, Kunyan Han, Bing Han, Guorui Zhou, Yang Song, Hechang Chen | Tsinghua University, Beijing, China; Jilin University, Changchun, China; University of Science and Technology of China, Hefei, China; Kuaishou Inc., Beijing, China; Kuaishou Inc. & Tsinghua University, Beijing, China; University of Oxford, Oxford, United Kingdom |
|  |  [MGMatch: Fast Matchmaking with Nonlinear Objective and Constraints via Multimodal Deep Graph Learning](https://doi.org/10.1145/3637528.3671553) |  | 0 | As a core problem of online games, matchmaking is to assign players into multiple teams to maximize their gaming experience. With the rapid development of game industry, it is increasingly difficulty to explicitly model players' experiences as linear functions. Instead, it is often modeled in a data-driven way by training a neural network. Meanwhile, complex rules must be satisfied to ensure the robustness of matchmaking, which are often described using logical operators. Therefore, matchmaking in practical scenarios is a challenging combinatorial optimization problem with nonlinear objective, linear constraints and logical constraints, which receives much less attention in previous research. In this paper, we propose a novel deep learning method for high-quality matchmaking in real-time. We first cast the problem as standard mixed-integer programming (MIP) by linearizing ReLU networks and logical constraints. Then, based on supervised learning, we design and train a multi-modal graph learning architecture to predict optimal solutions end-to-end from instance data, and solve a surrogate problem to efficiently obtain feasible solutions. Evaluation results on real industry datasets show that our method can deliver near-optimal solutions within 100ms. | Yu Sun, Kai Wang, Zhipeng Hu, Runze Wu, Yaoxin Wu, Wen Song, Xudong Shen, Tangjie Lv, Changjie Fan | Shandong University, Qingdao, Shandong, China; Fuxi AI Lab, NetEase Inc., Hangzhou, Zhejiang, China; Eindhoven University of Technology, Eindhoven, Netherlands |
|  |  [R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models](https://doi.org/10.1145/3637528.3671564) |  | 0 | Large language models have achieved remarkable success on general NLP tasks,but they may fall short for domain-specific problems. Recently, variousRetrieval-Augmented Large Language Models (RALLMs) are proposed to address thisshortcoming. However, existing evaluation tools only provide a few baselinesand evaluate them on various domains without mining the depth of domainknowledge. In this paper, we address the challenges of evaluating RALLMs byintroducing the R-Eval toolkit, a Python toolkit designed to streamline theevaluation of different RAG workflows in conjunction with LLMs. Our toolkit,which supports popular built-in RAG workflows and allows for the incorporationof customized testing data on the specific domain, is designed to beuser-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMsacross three task levels and two representative domains, revealing significantvariations in the effectiveness of RALLMs across different tasks and domains.Our analysis emphasizes the importance of considering both task and domainrequirements when choosing a RAG workflow and LLM combination. We are committedto continuously maintaining our platform at https://github.com/THU-KEG/R-Evalto facilitate both the industry and the researchers. | Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li | BNRist, DCST, Tsinghua University, Beijing, China; SIOE, Beihang University, Beijing, China; DCST, Tsinghua University, Beijing, China; SoI, Renmin University of China, Beijing, China |
|  |  [ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising](https://doi.org/10.1145/3637528.3671612) |  | 0 | Advertising platforms have evolved in estimating Lifetime Value (LTV) to better align with advertisers' true performance metric which considers cumulative sum of purchases a customer contributes over a period. Accurate LTV estimation is crucial for the precision of the advertising system and the effectiveness of advertisements. However, the sparsity of real-world LTV data presents a significant challenge to LTV predictive model(i.e., pLTV), severely limiting the their capabilities. Therefore, we propose to utilize external data, in addition to the internal data of advertising platform, to expand the size of purchase samples and enhance the LTV prediction model of the advertising platform. To tackle the issue of data distribution shift between internal and external platforms, we introduce an Adaptive Difference Siamese Network (ADSNet), which employs cross-domain transfer learning to prevent negative transfer. Specifically, ADSNet is designed to learn information that is beneficial to the target domain. We introduce a gain evaluation strategy to calculate information gain, aiding the model in learning helpful information for the target domain and providing the ability to reject noisy samples, thus avoiding negative transfer. Additionally, we also design a Domain Adaptation Module as a bridge to connect different domains, reduce the distribution distance between them, and enhance the consistency of representation space distribution. We conduct extensive offline experiments and online A/B tests on a real advertising platform. Our proposed ADSNet method outperforms other methods, improving GINI by 2%. The ablation study highlights the importance of the gain evaluation strategy in negative gain sample rejection and improving model performance. Additionally, ADSNet significantly improves long-tail prediction. The online A/B tests confirm ADSNet's efficacy, increasing online LTV by 3.47% and GMV by 3.89%. | Ruize Wang, Hui Xu, Ying Cheng, Qi He, Xing Zhou, Rui Feng, Wei Xu, Lei Huang, Jie Jiang | School of Computer Science, Fudan University, Shanghai, China; Tencent Inc., Shanghai, China; Tencent Inc., Shenzhen, China |
|  |  [Multi-Behavior Collaborative Filtering with Partial Order Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671569) |  | 0 | Representing information of multiple behaviors in the single graph collaborative filtering (CF) vector has been a long-standing challenge. This is because different behaviors naturally form separate behavior graphs and learn separate CF embeddings. Existing models merge the separate embeddings by appointing the CF embeddings for some behaviors as the primary embedding and utilizing other auxiliaries to enhance the primary embedding. However, this approach often results in the joint embedding performing well on the main tasks but poorly on the auxiliary ones. To address the problem arising from the separate behavior graphs, we propose the concept of Partial Order Recommendation Graphs (POG). POG defines the partial order relation of multiple behaviors and models behavior combinations as weighted edges to merge separate behavior graphs into a joint POG. Theoretical proof verifies that POG can be generalized to any given set of multiple behaviors. Based on POG, we propose the tailored Partial Order Graph Convolutional Networks (POGCN) that convolute neighbors' information while considering the behavior relations between users and items. POGCN also introduces a partial-order BPR sampling strategy for efficient and effective multiple-behavior CF training. POGCN has been successfully deployed on the homepage of Alibaba for two months, providing recommendation services for over one billion users. Extensive offline experiments conducted on three public benchmark datasets demonstrate that POGCN outperforms state-of-the-art multi-behavior baselines across all types of behaviors. Furthermore, online A/B tests confirm the superiority of POGCN in billion-scale recommender systems. | Yijie Zhang, Yuanchen Bei, Hao Chen, Qijie Shen, Zheng Yuan, Huan Gong, Senzhang Wang, Feiran Huang, Xiao Huang | National University of Defense Technology, Changsha, China; Jinan University, Guangzhou, China; Alibaba Group, Zhejiang, Hangzhou, China; Central South University, Hunan, Changsha, China; Zhejiang University, Zhejiang, Hangzhou, China; The Hong Kong Polytechnic University, Hong Kong, China |
|  |  [Inductive Modeling for Realtime Cold Start Recommendations](https://doi.org/10.1145/3637528.3671588) |  | 0 | In recommendation systems, the timely delivery of new content to their relevant audiences is critical for generating a growing and high quality collection of content for all users. The nature of this problem requires retrieval models to be able to make inferences in real time and with high relevance. There are two specific challenges for cold start contents. First, the information loss problem in a standard Two Tower model, due to the limited feature interactions between the user and item towers, is exacerbated for cold start items due to training data sparsity. Second, the huge volume of user-generated content in industry applications today poses a big bottleneck in the end-to-end latency of recommending new content. To overcome the two challenges, we propose a novel architecture, the Item History Model (IHM). IHM directly injects user-interaction information into the item tower to overcome information loss. In addition, IHM incorporates an inductive structure using attention-based pooling to eliminate the need for recurring training, a key bottleneck for the real-timeness. On both public and industry datasets, we demonstrate that IHM can not only outperform baselines in recommending cold start contents, but also achieves SoTA real-timeness in industry applications. | Chandler Zuo, Jonathan Castaldo, Hanqing Zhu, Haoyu Zhang, Ji Liu, Yangpeng Ou, Xiao Kong | Meta, Menlo Park, CA, USA |
|  |  [Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era](https://doi.org/10.1145/3637528.3671458) |  | 0 | With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey. | Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu | Huawei Noah's Ark Lab, Shenzhen, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Approximating Memorization Using Loss Surface Geometry for Dataset Pruning and Summarization](https://doi.org/10.1145/3637528.3671985) |  | 0 | The sustainable training of modern neural network models represents an open challenge. Several existing methods approach this issue by identifying a subset of relevant data samples from the full training data to be used in model optimization with the goal of matching the performance of the full data training with that of the subset data training. Our work explores using memorization scores to find representative and atypical samples. We demonstrate that memorization-aware dataset summarization improves the subset construction performance. However, computing memorization scores is notably resource-intensive. To this end, we propose a novel method that leverages the discrepancy between sharpness-aware minimization and stochastic gradient descent to capture data points atypicality. We evaluate our metric over several efficient approximation functions for memorization scores - namely proxies -, empirically showing superior correlation and effectiveness. We explore the causes behind our approximation quality, highlighting how typical data points trigger a flatter loss landscape compared to atypical ones. Extensive experiments confirm the effectiveness of our proxy for dataset pruning and summarization tasks, surpassing state-of-the-art approaches both on canonical setups - where atypical data points benefit performance - and few-shot learning scenarios-where atypical data points can be detrimental. | Andrea Agiollo, Young In Kim, Rajiv Khanna | Purdue University, West Lafayette, IN, USA; University of Bologna, Bologna, Italy |
|  |  [Evading Community Detection via Counterfactual Neighborhood Search](https://doi.org/10.1145/3637528.3671896) |  | 0 | Community detection techniques are useful for social media platforms todiscover tightly connected groups of users who share common interests. However,this functionality often comes at the expense of potentially exposingindividuals to privacy breaches by inadvertently revealing their tastes orpreferences. Therefore, some users may wish to preserve their anonymity and optout of community detection for various reasons, such as affiliation withpolitical or religious organizations, without leaving the platform. In thisstudy, we address the challenge of community membership hiding, which involvesstrategically altering the structural properties of a network graph to preventone or more nodes from being identified by a given community detectionalgorithm. We tackle this problem by formulating it as a constrainedcounterfactual graph objective, and we solve it via deep reinforcementlearning. Extensive experiments demonstrate that our method outperformsexisting baselines, striking the best balance between accuracy and cost. | Andrea Bernini, Fabrizio Silvestri, Gabriele Tolomei | Sapienza University of Rome, Rome, Italy |
|  |  [FoRAG: Factuality-optimized Retrieval Augmented Generation for Web-enhanced Long-form Question Answering](https://doi.org/10.1145/3637528.3672065) |  | 0 | Retrieval Augmented Generation (RAG) has become prevalent in question-answering (QA) tasks due to its ability of utilizing search engine to enhance the quality of long-form question-answering (LFQA). Despite the emergence of various open source methods and web-enhanced commercial systems such as Bing Chat, two critical problems remain unsolved, i.e., the lack of factuality and clear logic in the generated long-form answers. In this paper, we remedy these issues via a systematic study on answer generation in web-enhanced LFQA. Specifically, we first propose a novel outline-enhanced generator to achieve clear logic in the generation of multifaceted answers and construct two datasets accordingly. Then we propose a factuality optimization method based on a carefully designed doubly fine-grained RLHF framework, which contains automatic evaluation and reward modeling in different levels of granularity. Our generic framework comprises conventional fine-grained RLHF methods as special cases. Extensive experiments verify the superiority of our proposed Factuality-optimized RAG (FoRAG) method on both English and Chinese benchmarks. In particular, when applying our method to Llama2-7B-chat, the derived model FoRAG-L-7B outperforms WebGPT-175B in terms of three commonly used metrics (i.e., coherence, helpfulness, and factuality), while the number of parameters is much smaller (only 1/24 of that of WebGPT-175B). Our datasets and models are made publicly available for better reproducibility.https://huggingface.co/forag łabelfootnote_dataset_url | Tianchi Cai, Zhiwen Tan, Xierui Song, Tao Sun, Jiyan Jiang, Yunqi Xu, Yinger Zhang, Jinjie Gu | Tsinghua University, Beijing, China; Ant Group, Hangzhou, China |
|  |  [A Hierarchical Context Augmentation Method to Improve Retrieval-Augmented LLMs on Scientific Papers](https://doi.org/10.1145/3637528.3671847) |  | 0 | Scientific papers of a large scale on the Internet encompass a wealth of data and knowledge, attracting the attention of numerous researchers. To fully utilize these knowledge, Retrieval-Augmented Large Language Models (LLMs) usually leverage large-scale scientific corpus to train and then retrieve relevant passages from external memory to improve generation, which have demonstrated outstanding performance. However, existing methods can only capture one-dimension fragmented textual information without incorporating hierarchical structural knowledge, eg. the deduction relationship of abstract and main body, which makes it difficult to grasp the central thought of papers. To tackle this problem, we propose a hierarchical context augmentation method, which helps Retrieval-Augmented LLMs to autoregressively learn the structure knowledge of scientific papers. Specifically, we utilize the document tree to represent the hierarchical relationship of a paper and enhance the structure information of scientific context from three aspects: scale, format and global information. First, we think each top-bottom path of document tree is a logical independent context, which can be used to largely increase the scale of extracted structural corpus. Second, we propose a novel label-based format to represent the structure of context in textual sequences, unified between training and inference. Third, we introduce the global information of retrieved passages to further enhance the structure of context. Extensive experiments on three scientific tasks show that the proposed method significantly improves the performance of Retrieval-Augmented LLMs on all tasks. Besides, our method achieves start-of-art performance in Question Answer task and outperforms ChatGPT. Moreover, it also brings considerate gains with irrelevant retrieval passages, illustrating its effectiveness on practical application scenarios. | TianYi Che, XianLing Mao, Tian Lan, Heyan Huang | Beijing Institute of Technology, Beijing, China |
|  |  [Maximum-Entropy Regularized Decision Transformer with Reward Relabelling for Dynamic Recommendation](https://doi.org/10.1145/3637528.3671750) |  | 0 | Reinforcement learning-based recommender systems have recently gainedpopularity. However, due to the typical limitations of simulation environments(e.g., data inefficiency), most of the work cannot be broadly applied in alldomains. To counter these challenges, recent advancements have leveragedoffline reinforcement learning methods, notable for their data-driven approachutilizing offline datasets. A prominent example of this is the DecisionTransformer. Despite its popularity, the Decision Transformer approach hasinherent drawbacks, particularly evident in recommendation methods based on it.This paper identifies two key shortcomings in existing DecisionTransformer-based methods: a lack of stitching capability and limitedeffectiveness in online adoption. In response, we introduce a novel methodologynamed Max-Entropy enhanced Decision Transformer with Reward Relabeling forOffline RLRS (EDT4Rec). Our approach begins with a max entropy perspective,leading to the development of a max entropy enhanced exploration strategy. Thisstrategy is designed to facilitate more effective exploration in onlineenvironments. Additionally, to augment the model's capability to stitchsub-optimal trajectories, we incorporate a unique reward relabeling technique.To validate the effectiveness and superiority of EDT4Rec, we have conductedcomprehensive experiments across six real-world offline datasets and in anonline simulator. | Xiaocong Chen, Siyu Wang, Lina Yao | Data 61, CSIRO & The University of New South Wales, Eveleigh, Australia; Data 61, CSIRO, Eveleigh, Australia; The University of New South Wales, Sydney, Australia |
|  |  [Retrieval-Augmented Hypergraph for Multimodal Social Media Popularity Prediction](https://doi.org/10.1145/3637528.3672041) |  | 0 | Accurately predicting the popularity of multimodal user-generated content (UGC) is fundamental for many real-world applications such as online advertising and recommendation. Existing approaches generally focus on limited contextual information within individual UGCs, yet overlook the potential benefit of exploiting meaningful knowledge in relevant UGCs. In this work, we propose RAGTrans, an aspect-aware retrieval-augmented multi-modal hypergraph transformer that retrieves pertinent knowledge from a multi-modal memory bank and enhances UGC representations via neighborhood knowledge aggregation on multi-model hypergraphs. In particular, we initially retrieve relevant multimedia instances from a large corpus of UGCs via the aspect information and construct a knowledge-enhanced hypergraph based on retrieved relevant instances. This allows capturing meaningful contextual information across the data. We then design a novel bootstrapping hypergraph transformer on multimodal hypergraphs to strengthen UGC representations across modalities via customizing a propagation algorithm to effectively diffuse information across nodes and edges. Additionally, we propose a user-aware attention-based fusion module to comprise the enriched UGC representations for popularity prediction. Extensive experiments on real-world social media datasets demonstrate that RAGTrans outperforms state-of-the-art popularity prediction models across settings. | Zhangtao Cheng, Jienan Zhang, Xovee Xu, Goce Trajcevski, Ting Zhong, Fan Zhou | University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Iowa State University, Ames, Iowa, USA |
|  |  [ROTAN: A Rotation-based Temporal Attention Network for Time-Specific Next POI Recommendation](https://doi.org/10.1145/3637528.3671809) |  | 0 | The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users' subsequent movements. The main challenge is how to effectively capture users' personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model. | Shanshan Feng, Feiyu Meng, Lisi Chen, Shuo Shang, Yew Soon Ong | University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Centre for Frontier AI Research, ASTAR & Nanyang Technological University, Singapore, Singapore; University of Electronic Science and Technology of China, Chengdu, China |
|  |  [AutoXPCR: Automated Multi-Objective Model Selection for Time Series Forecasting](https://doi.org/10.1145/3637528.3672057) |  | 0 | Automated machine learning (AutoML) streamlines the creation of ML models, but few specialized methods have approached the challenging domain of time series forecasting. Deep neural networks (DNNs) often deliver state-of-the-art predictive performance for forecasting data, however these models are also criticized for being computationally intensive black boxes. As a result, when searching for the "best" model, it is crucial to also acknowledge other aspects, such as interpretability and resource consumption. In this paper, we propose AutoXPCR - a novel method that produces DNNs for forecasting under consideration of multiple objectives in an automated and explainable fashion. Our approach leverages meta-learning to estimate any model's performance along PCR criteria, which encompass (P)redictive error, (C)omplexity, and (R)esource demand. Explainability is addressed on multiple levels, as AutoXPCR pro-vides by-product explanations of recommendations and allows to interactively control the desired PCR criteria importance and trade-offs. We demonstrate the practical feasibility AutoXPCR across 108 forecasting data sets from various domains. Notably, our method outperforms competing AutoML approaches - on average, it only requires 20% of computation costs for recommending highly efficient models with 85% of the empirical best quality. | Raphael Fischer, Amal Saadallah |  |
|  |  [Topology-Driven Multi-View Clustering via Tensorial Refined Sigmoid Rank Minimization](https://doi.org/10.1145/3637528.3672070) |  | 0 | Benefiting from the effective exploitation of the high-order correlations across multiple views, tensor-based multi-view clustering (TMVC) has garnered considerable attention in recent years. Nevertheless, prior TMVC techniques commonly involve assembling multiple view-specific spatial similarity graphs into a three-dimensional tensor, overlooking the intrinsic topological structure essential for precise clustering of data within a manifold. Additionally, mainstream techniques are constrained by equally shrinking all singular values to recover a low-rank tensor, limiting their capacity to distinguish significant variations among different singular values. In this investigation, we present an innovative TMVC framework termed toPology-driven multi-view clustering viA refined teNsorial sigmoiD rAnk minimization (PANDA ). Specifically, PANDA extracts view-specific topological structures from Euclidean graphs and intricately integrates them into a low-rank three-dimensional tensor, facilitating the concurrent utilization of intra-view topological connectivity and inter-view high-order correlations. Moreover, we develop a refined sigmoid function as the tighter surrogate to tensor rank, enabling the exploration of significant information of heterogeneous singular values. Meanwhile, the topological structures are merged into a unified structure with varying weights, associated with a connectivity constraint, empowering the significant divergence among views and the explicit cluster structure of the target graph are simultaneously leveraged. Extensive experiments demonstrate the superiority of PANDA, outperforming SOTA methods. | Zhibin Gu, Zhendong Li, Songhe Feng |  |
|  |  [Ranking with Slot Constraints](https://doi.org/10.1145/3637528.3672000) |  | 0 | We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, empirical evaluations show that MatchRank can provide substantial improvements over a range of synthetic and real-world tasks. | Wentao Guo, Andrew Wang, Bradon Thymes, Thorsten Joachims |  |
|  |  [Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations](https://doi.org/10.1145/3637528.3672056) |  | 0 | Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the long-standing cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation (CDR). This approach leverages two novel meta-path-based metrics-consistency and discrepancy-to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks (GCN) layers under a multi-objective optimization framework, using the limit theory of GCN. Additionally, we introduce a novel Contrastive Divergence (CD) loss, which can seamlessly integrate the consistency and discrepancy metrics into the contrastive objective as the positive and contrastive supervision signals to learn node representations, enhancing the pairwise ranking of recommended objects and proving particularly valuable in severe cold-start scenarios. Extensive experiments demonstrate the effectiveness of the proposed CDR. The code is released at https://github.com/foodfaust/CDR. | Linxin Guo, Yaochen Zhu, Min Gao, Yinghui Tao, Junliang Yu, Chen Chen | the University of Queensland, Queensland, Australia; Institute of Guizhou Aerospace Measuring and Testing Technology, Guiyang, China; University of Virginia, Charlottesville, VA, USA; Chongqing University, Chongqing, China |
|  |  [Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors](https://doi.org/10.1145/3637528.3671793) |  | 0 | For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70% higher F1-score than the second best method (98.6% vs 29%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training. | Croix Gyurek, Niloy Talukder, Mohammad Al Hasan | University of Waterloo, Waterloo, Ontario, Canada; Indiana University at Indianapolis, Indianapolis, IN, USA |
|  |  [An Efficient Local Search Algorithm for Large GD Advertising Inventory Allocation with Multilinear Constraints](https://doi.org/10.1145/3637528.3671811) |  | 0 | The Guaranteed Delivery (GD) advertising is a crucial component of the online advertising industry, and the allocation of inventory in GD advertising is an important procedure that influences directly the ability of the publisher to fulfill the requirements and increase its revenues. Nowadays, as the requirements of advertisers become more and more diverse and fine-grained, the focus ratio requirement, which states that the portion of allocated impressions of a designated contract on focus media among all possible media should be greater than another contract, often appears in business scenarios. However, taking these requirements into account brings hardness for the GD advertising inventory allocation as the focus ratio requirements involve non-convex multilinear constraints. Existing methods which rely on the convex properties are not suitable for processing this problem, while mathematical programming or constraint-based heuristic solvers are unable to produce high-quality solutions within the time limit. Therefore, we propose a local search framework to address this challenge. It incorporates four new operators designed for handling multilinear constraints and a two-mode algorithmic architecture. Experimental results demonstrate that our algorithm is able to compute high-quality allocations with better business metrics compared to the state-of-the-art mathematical programming or constraint based heuristic solvers. Moreover, our algorithm is able to handle the general multilinear constraints and we hope it could be used to solve other problems in GD advertising with similar requirements. | Xiang He, Wuyang Mao, Zhenghang Xu, Yuanzhe Gu, Yundu Huang, Zhonglin Zu, Liang Wang, Mengyu Zhao, Mengchuan Zou | Alibaba Group, Hangzhou, China; Alibaba Group, Beijing, China |
|  |  [Double Correction Framework for Denoising Recommendation](https://doi.org/10.1145/3637528.3671692) |  | 0 | As its availability and generality in online services, implicit feedback ismore commonly used in recommender systems. However, implicit feedback usuallypresents noisy samples in real-world recommendation scenarios (such asmisclicks or non-preferential behaviors), which will affect precise userpreference learning. To overcome the noisy samples problem, a popular solutionis based on dropping noisy samples in the model training phase, which followsthe observation that noisy samples have higher training losses than cleansamples. Despite the effectiveness, we argue that this solution still haslimits. (1) High training losses can result from model optimization instabilityor hard samples, not just noisy samples. (2) Completely dropping of noisysamples will aggravate the data sparsity, which lacks full data exploitation.To tackle the above limitations, we propose a Double Correction Framework forDenoising Recommendation (DCF), which contains two correction components fromviews of more precise sample dropping and avoiding more sparse data. In thesample dropping correction component, we use the loss value of the samples overtime to determine whether it is noise or not, increasing dropping stability.Instead of averaging directly, we use the damping function to reduce the biaseffect of outliers. Furthermore, due to the higher variance exhibited by hardsamples, we derive a lower bound for the loss through concentration inequalityto identify and reuse hard samples. In progressive label correction, weiteratively re-label highly deterministic noisy samples and retrain them tofurther improve performance. Finally, extensive experimental results on threedatasets and four backbones demonstrate the effectiveness and generalization ofour proposed framework. | Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai, Jinqi Gong, Richang Hong, Min Zhang | Hefei University of Technology, Hefei, China; Tsinghua University, Beijing, China; University of Macau, Macau, China |
|  |  [Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models](https://doi.org/10.1145/3637528.3671932) |  | 0 | The robustness of large language models (LLMs) becomes increasingly importantas their use rapidly grows in a wide range of domains. Retrieval-AugmentedGeneration (RAG) is considered as a means to improve the trustworthiness oftext generation from LLMs. However, how the outputs from RAG-based LLMs areaffected by slightly different inputs is not well studied. In this work, wefind that the insertion of even a short prefix to the prompt leads to thegeneration of outputs far away from factually correct answers. Wesystematically evaluate the effect of such prefixes on RAG by introducing anovel optimization technique called Gradient Guided Prompt Perturbation (GGPP).GGPP achieves a high success rate in steering outputs of RAG-based LLMs totargeted wrong answers. It can also cope with instructions in the promptsrequesting to ignore irrelevant context. We also exploit LLMs' neuronactivation difference between prompts with and without GGPP perturbations togive a method that improves the robustness of RAG-based LLMs through a highlyeffective detector trained on neuron activation triggered by GGPP generatedprompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness ofour methods. | Zhibo Hu, Chen Wang, Yanfeng Shu, HyeYoung Paik, Liming Zhu | University of New South Wales, Sydney, NSW, Australia; CSIRO Data61 & The University of New South Wales, Sydney, NSW, Australia; The University of New South Wales & CSIRO Data61, Sydney, NSW, Australia; CSIRO Data61, Hobart, Tasmania, Australia; CSIRO Data61 & University of New South Wales, Sydney, NSW, Australia |
|  |  [Dynamic Neural Dowker Network: Approximating Persistent Homology in Dynamic Directed Graphs](https://doi.org/10.1145/3637528.3671980) |  | 0 | Persistent homology, a fundamental technique within Topological Data Analysis (TDA), captures structural and shape characteristics of graphs, yet encounters computational difficulties when applied to dynamic directed graphs. This paper introduces the Dynamic Neural Dowker Network (DNDN), a novel framework specifically designed to approximate the results of dynamic Dowker filtration, aiming to capture the high-order topological features of dynamic directed graphs. Our approach creatively uses line graph transformations to produce both source and sink line graphs, highlighting the shared neighbor structures that Dowker complexes focus on. The DNDN incorporates a Source-Sink Line Graph Neural Network (SSLGNN) layer to effectively capture the neighborhood relationships among dynamic edges. Additionally, we introduce an innovative duality edge fusion mechanism, ensuring that the results for both the sink and source line graphs adhere to the duality principle intrinsic to Dowker complexes. Our approach is validated through comprehensive experiments on real-world datasets, demonstrating DNDN's capability not only to effectively approximate dynamic Dowker filtration results but also to perform exceptionally in dynamic graph classification tasks. | Hao Li, Hao Jiang, Jiajun Fan, Dongsheng Ye, Liang Du | Electronic Information School, Wuhan University, Wuhan, Hubei, China |
|  |  [RecExplainer: Aligning Large Language Models for Explaining Recommendation Models](https://doi.org/10.1145/3637528.3671802) |  | 0 | Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI. | Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie | Microsoft Research Asia, Beijing, China; University of Science and Technology of China, Hefei, China |
|  |  [Customizing Graph Neural Network for CAD Assembly Recommendation](https://doi.org/10.1145/3637528.3671788) |  | 0 | CAD assembly modeling, which refers to using CAD software to design new products from a catalog of existing machine components, is important in the industrial field. The graph neural network (GNN) based recommender system for CAD assembly modeling can help designers make decisions and speed up the design process by recommending the next required component based on the existing components in CAD software. These components can be represented as a graph naturally. However, present recommender systems for CAD assembly modeling adopt fixed GNN architectures, which may be sub-optimal for different manufacturers with different data distribution. Therefore, to customize a well-suited recommender system for different manufacturers, we propose a novel neural architecture search (NAS) framework, dubbed CusGNN, which can design data-specific GNN automatically. Specifically, we design a search space from three dimensions (i.e., aggregation, fusion, and readout functions), which contains a wide variety of GNN architectures. Then, we develop an effective differentiable search algorithm to search high-performing GNN from the search space. Experimental results show that the customized GNNs achieve 1.5-5.1% higher top-10 accuracy compared to previous manual designed methods, demonstrating the superiority of the proposed approach. Code and data are available at https://github.com/BUPT-GAMMA/CusGNN. | Fengqi Liang, Huan Zhao, Yuhan Quan, Wei Fang, Chuan Shi | 4Paradigm Inc., Beijing, China; Beijing University of Post and Telecommunication, Beijing, China |
|  |  [When Box Meets Graph Neural Network in Tag-aware Recommendation](https://doi.org/10.1145/3637528.3671973) |  | 0 | Last year has witnessed the re-flourishment of tag-aware recommender systems supported by the LLM-enriched tags. Unfortunately, though large efforts have been made, current solutions may fail to describe the diversity and uncertainty inherent in user preferences with only tag-driven profiles. Recently, with the development of geometry-based techniques, e.g., box embeddings, the diversity of user preferences now could be fully modeled as the range within a box in high dimension space. However, defect still exists as these approaches are incapable of capturing high-order neighbor signals, i.e., semantic-rich multi-hop relations within the user-tag-item tripartite graph, which severely limits the effectiveness of user modeling. To deal with this challenge, in this paper, we propose a novel framework, called BoxGNN, to perform message aggregation via combinations of logical operations, thereby incorporating high-order signals. Specifically, we first embed users, items, and tags as hyper-boxes rather than simple points in the representation space, and define two logical operations, i.e., union and intersection, to facilitate the subsequent process. Next, we perform the message aggregation mechanism via the combination of logical operations, to obtain the corresponding high-order box representations. Finally, we adopt a volume-based learning objective with Gumbel smoothing techniques to refine the representation of boxes. Extensive experiments on two publicly available datasets and one LLM-enhanced e-commerce dataset have validated the superiority of BoxGNN compared with various state-of-the-art baselines. The code is released online: https://github.com/critical88/BoxGNN. | Fake Lin, Ziwei Zhao, Xi Zhu, Da Zhang, Shitian Shen, Xueying Li, Tong Xu, Suojuan Zhang, Enhong Chen | Alibaba Group, Hangzhou, NC, USA; Army Engineering University of PLA, Nanjing, China; University of Science and Technology of China, Hefei, China; Alibaba Group, Hangzhou, China |
|  |  [On the Convergence of Zeroth-Order Federated Tuning for Large Language Models](https://doi.org/10.1145/3637528.3671865) |  | 0 | The confluence of Federated Learning (FL) and Large Language Models (LLMs) isushering in a new era in privacy-preserving natural language processing.However, the intensive memory requirements for fine-tuning LLMs posesignificant challenges, especially when deploying on clients with limitedcomputational resources. To circumvent this, we explore the novel integrationof Memory-efficient Zeroth-Order Optimization within a federated setting, asynergy we term as FedMeZO. Our study is the first to examine the theoreticalunderpinnings of FedMeZO in the context of LLMs, tackling key questionsregarding the influence of large parameter spaces on optimization behavior, theestablishment of convergence properties, and the identification of criticalparameters for convergence to inform personalized federated strategies. Ourextensive empirical evidence supports the theory, showing that FedMeZO not onlyconverges faster than traditional first-order methods such as FedAvg but alsosignificantly reduces GPU memory usage during training to levels comparable tothose during inference. Moreover, the proposed personalized FL strategy that isbuilt upon the theoretical insights to customize the client-wise learning ratecan effectively accelerate loss reduction. We hope our work can help to bridgetheoretical and practical aspects of federated fine-tuning for LLMs, therebystimulating further advancements and research in this area. | Zhenqing Ling, Daoyuan Chen, Liuyi Yao, Yaliang Li, Ying Shen | Alibaba Group, Hangzhou, Zhejiang, China; Sun Yat-sen University & Pazhou Lab, Shenzhen, Guangdong, China; Sun Yat-sen University, Shenzhen, Guangdong, China; Alibaba Group, Bellevue, Washington, USA |
|  |  [Fast Query of Biharmonic Distance in Networks](https://doi.org/10.1145/3637528.3671856) |  | 0 | Thebiharmonic distance (BD) is a fundamental metric that measures the distance of two nodes in a graph. It has found applications in network coherence, machine learning, and computational graphics, among others. In spite of BD's importance, efficient algorithms for the exact computation or approximation of this metric on large graphs remain notably absent. In this work, we provide several algorithms to estimate BD, building on a novel formulation of this metric. These algorithms enjoy locality property (that is, they only read a small portion of the input graph) and at the same time possess provable performance guarantees. In particular, our main algorithms approximate the BD between any node pair with an arbitrarily small additive error ε in time O(1/ε2 poly(log n/ε)). Furthermore, we perform an extensive empirical study on several benchmark networks, validating the performance and accuracy of our algorithms. | Changan Liu, Ahad N. Zehmakan, Zhongzhi Zhang | Australian National University, Canberra, Australia; Fudan University, Shanghai, China |
|  |  [Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization](https://doi.org/10.1145/3637528.3672040) |  | 0 | Vehicle routing problems (VRP) are very important in many real-world applications and has been studied for several decades. Recently, neural combinatorial optimization (NCO) has attracted growing research effort. NCO is to train a neural network model to solve an optimization problem in question. However, existing NCO methods often build a different model for each routing problem, which significantly hinders their application in some areas where there are many different VRP variants to solve. In this work, we make a first attempt to tackle the crucial challenge of cross-problem generalization in NCO. We formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization manner. In our experiments, the neural model is trained on five VRP variants and its performance is tested on eleven VRP variants. The experimental results show that the model demonstrates superior performance on these eleven VRP variants, reducing the average gap to around 5% from over 20% and achieving a notable performance boost on both benchmark datasets and real-world logistics scenarios. | Fei Liu, Xi Lin, Zhenkun Wang, Qingfu Zhang, Tong Xialiang, Mingxuan Yuan | Huawei Technologies Ltd., Shenzhen, China; Huawei Technologies Ltd., Hong Kong, China; Southern University of Science and Technology, Shenzhen, China; City University of Hong Kong, Hong Kong, China |
|  |  [Low Rank Multi-Dictionary Selection at Scale](https://doi.org/10.1145/3637528.3671723) |  | 0 | The sparse dictionary coding framework represents signals as a linear combination of a few predefined dictionary atoms. It has been employed for images, time series, graph signals and recently for 2-way (or 2D) spatio-temporal data employing jointly temporal and spatial dictionaries. Large and over-complete dictionaries enable high-quality models, but also pose scalability challenges which are exacerbated in multi-dictionary settings. Hence, an important problem that we address in this paper is: How to scale multi-dictionary coding for large dictionaries and datasets? We propose a multi-dictionary atom selection technique for low-rank sparse coding named LRMDS. To enable scalability to large dictionaries and datasets, it progressively selects groups of row-column atom pairs based on their alignment with the data and performs convex relaxation coding via the corresponding sub-dictionaries. We demonstrate both theoretically and experimentally that when the data has a low-rank encoding with a sparse subset of the atoms, LRMDS is able to select them with strong guarantees under mild assumptions. Furthermore, we demonstrate the scalability and quality of LRMDS in both synthetic and real-world datasets and for a range of coding dictionaries. It achieves 3 times to 10 times speed-up compared to baselines, while obtaining up to two orders of magnitude improvement in representation quality on some of the real world datasets given a fixed target number of atoms. | Boya Ma, Maxwell McNeil, Abram Magner, Petko Bogdanov | Department of Computer Science, University at Albany, State University of New York, Albany, NY, USA |
|  |  [ImputeFormer: Low Rankness-Induced Transformers for Generalizable Spatiotemporal Imputation](https://doi.org/10.1145/3637528.3671751) |  | 0 | Missing data is a pervasive issue in both scientific and engineering tasks, especially for the modeling of spatiotemporal data. Existing imputation solutions mainly include low-rank models and deep learning models. The former assumes general structural priors but has limited model capacity. The latter possesses salient expressivity, but lacks prior knowledge of the underlying spatiotemporal structures. Leveraging the strengths of both two paradigms, we demonstrate a low rankness-induced Transformer to achieve a balance between strong inductive bias and high expressivity. The exploitation of the inherent structures of spatiotemporal data enables our model to learn balanced signal-noise representations, making it generalizable for a variety of imputation tasks. We demonstrate its superiority in terms of accuracy, efficiency, and versatility in heterogeneous datasets, including traffic flow, solar energy, smart meters, and air quality. Promising empirical results provide strong conviction that incorporating time series primitives, such as low-rankness, can substantially facilitate the development of a generalizable model to approach a wide range of spatiotemporal imputation problems. | Tong Nie, Guoyang Qin, Wei Ma, Yuewen Mei, Jian Sun | Tongji University, Shanghai, China; The Hong Kong Polytechnic University, Hong Kong SAR, China |
|  |  [Improving the Consistency in Cross-Lingual Cross-Modal Retrieval with 1-to-K Contrastive Learning](https://doi.org/10.1145/3637528.3671787) |  | 0 | Cross-lingual Cross-modal Retrieval (CCR) is an essential task in web search, which aims to break the barriers between modality and language simultaneously and achieves image-text retrieval in the multi-lingual scenario with a single model. In recent years, excellent progress has been made based on cross-lingual cross-modal pre-training; particularly, the methods based on contrastive learning on large-scale data have significantly improved retrieval tasks. However, these methods directly follow the existing pre-training methods in the cross-lingual or cross-modal domain, leading to two problems of inconsistency in CCR: The methods with cross-lingual style suffer from the intra-modal error propagation, resulting in inconsistent recall performance across languages in the whole dataset. The methods with cross-modal style suffer from the inter-modal optimization direction bias, resulting in inconsistent rank across languages within each instance, which cannot be reflected by Recall@K. To solve these problems, we propose a simple but effective 1-to-K contrastive learning method, which treats each language equally and eliminates error propagation and optimization bias. In addition, we propose a new evaluation metric, Mean Rank Variance (MRV), to reflect the rank inconsistency across languages within each instance. Extensive experiments on four CCR datasets show that our method improves both recall rates and MRV with smaller-scale pre-trained data, achieving the new state-of-art. | Zhijie Nie, Richong Zhang, Zhangchi Feng, Hailang Huang, Xudong Liu | CCSE, Beihang University, Beijing, China |
|  |  [CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent](https://doi.org/10.1145/3637528.3671837) |  | 0 | Recently, Large Language Model (LLM)-empowered recommender systems (RecSys) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of LLM-empowered RecSys still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box RecSys, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning (RL) agents are not effective for attacking LLM-empowered RecSys due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, LLMs provide unprecedented opportunities to serve as attack agents to attack RecSys because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called CheatAgent by harnessing the human-like capabilities of LLMs, where an LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the LLM agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim RecSys iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method. | LiangBo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang | The Hong Kong Polytechnic University, Hong Kong, China; Jinan University, Guangzhou, China |
|  |  [Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I](https://doi.org/10.1145/3637528.3671883) |  | 0 | The traditional evaluation of information retrieval (IR) systems is generally very costly as it requires manual relevance annotation from human experts. Recent advancements in generative artificial intelligence -specifically large language models (LLMs)- can generate relevance annotations at an enormous scale with relatively small computational costs. Potentially, this could alleviate the costs traditionally associated with IR evaluation and make it applicable to numerous low-resource applications. However, generated relevance annotations are not immune to (systematic) errors, and as a result, directly using them for evaluation produces unreliable results. In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence intervals (CIs) around IR evaluation metrics. Our proposed methods require a small number of reliable annotations from which the methods can statistically analyze the errors in the generated annotations. Using this information, we can place CIs around evaluation metrics with strong theoretical guarantees. Unlike existing approaches, our conformal risk control method is specifically designed for ranking metrics and can vary its CIs per query and document. Our experimental results show that our CIs accurately capture both the variance and bias in evaluation based on LLM annotations, better than the typical empirical bootstrapping estimates. We hope our contributions bring reliable evaluation to the many IR applications where this was traditionally infeasible. | Harrie Oosterhuis, Rolf Jagerman, Zhen Qin, Xuanhui Wang, Michael Bendersky | Google Research, New York City, USA; Google Research & Radboud University, Amsterdam, Netherlands; Google Research, Mountain View, USA; Google Research, Amsterdam, Netherlands |
|  |  [How Powerful is Graph Filtering for Recommendation](https://doi.org/10.1145/3637528.3671789) |  | 0 | It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering. Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training. However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality. Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training. (2) Lack of expressive power. We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable. To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training. Based on this observation, we propose a generalized graph normalization (G2N) with hyperparameters adjusting the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training. As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings. By simplifying LGCN, we further propose a simplified graph filtering for CF (SGFCF) which only requires the top-K singular values for recommendation. Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods. | Shaowen Peng, Xin Liu, Kazunari Sugiyama, Tsunenori Mine | Osaka Seikei University Osaka, Japan; National Institute of Advanced Industrial Science and Technology Tokyo, Japan; NARA Institute of Science and Technology, Nara, Japan; Kyushu University Fukuoka, Japan |
|  |  [STEMO: Early Spatio-temporal Forecasting with Multi-Objective Reinforcement Learning](https://doi.org/10.1145/3637528.3671922) |  | 0 | Accuracy and timeliness are indeed often conflicting goals in predictiontasks. Premature predictions may yield a higher rate of false alarms, whereasdelaying predictions to gather more information can render them too late to beuseful. In applications such as wildfires, crimes, and traffic jams, timelyforecasting are vital for safeguarding human life and property. Consequently,finding a balance between accuracy and timeliness is crucial. In this paper, wepropose an early spatio-temporal forecasting model based on Multi-Objectivereinforcement learning that can either implement an optimal policy given apreference or infer the preference based on a small number of samples. Themodel addresses two primary challenges: 1) enhancing the accuracy of earlyforecasting and 2) providing the optimal policy for determining the mostsuitable prediction time for each area. Our method demonstrates superiorperformance on three large-scale real-world datasets, surpassing existingmethods in early spatio-temporal forecasting tasks. | Wei Shao, Yufan Kang, Ziyan Peng, Xiao Xiao, Lei Wang, Yuhui Yang, Flora D. Salim | Zhejiang University, Hangzhou, China; Data61, CSIRO, Clayton, Victoria, Australia; University of New South Wales, Sydney, Australia; RMIT University, Melbourne, Victoria, Australia; Xidian University, Xi'an, China |
|  |  [Marrying Dialogue Systems with Data Visualization: Interactive Data Visualization Generation from Natural Language Conversations](https://doi.org/10.1145/3637528.3671935) |  | 0 | Data visualization (DV) has become the prevailing tool in the market due to its effectiveness into illustrating insights in vast amounts of data. To lower the barrier of using DVs, automatic DV tasks, such as natural language question (NLQ) to visualization translation (formally called text-to-vis), have been investigated in the research community. However, text-to-vis assumes the NLQ to be well-organized and expressed in a single sentence. However, in real-world settings, complex DV is needed through consecutive exchanges between the DV system and the users. In this paper, we propose a new task named CoVis, short for Conversational text-to-Visualization, aiming at constructing DVs through a series of interactions between users and the system. Since it is the task which has not been studied in the literature, we first build a benchmark dataset named Dial-NVBench, including dialogue sessions with a sequence of queries from a user and responses from the system. The ultimate goal of each dialogue session is to create a suitable DV. However, this process can contain diverse dialogue queries, such as seeking information about the dataset, manipulating parts of the data, and visualizing the data. Then, we propose a multi-modal neural network named MMCoVisNet to answer these DV-related queries. In particular, MMCoVisNet first fully understands the dialogue context and determines the corresponding responses. Then, it uses adaptive decoders to provide the appropriate replies: (i) a straightforward text decoder is used to produce general responses, (ii) an SQL-form decoder is applied to synthesize data querying responses, and (iii) a DV-form decoder tries to construct the appropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over our proposed benchmark dataset. Experimental results validate that MMCoVisNet performs better than existing baselines and achieves a state-of-the-art performance. | Yuanfeng Song, Xuefang Zhao, Raymond ChiWing Wong | AI Group, WeBank Co., Ltd., Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China |
|  |  [Towards Robust Recommendation via Decision Boundary-aware Graph Contrastive Learning](https://doi.org/10.1145/3637528.3671661) |  | 0 | In recent years, graph contrastive learning (GCL) has received increasing attention in recommender systems due to its effectiveness in reducing bias caused by data sparsity. However, most existing GCL models rely on heuristic approaches and usually assume entity independence when constructing contrastive views. We argue that these methods struggle to strike a balance between semantic invariance and view hardness across the dynamic training process, both of which are critical factors in graph contrastive learning. To address the above issues, we propose a novel GCL-based recommendation framework RGCL, which effectively maintains the semantic invariance of contrastive pairs and dynamically adapts as the model capability evolves through the training process. Specifically, RGCL first introduces decision boundary-aware adversarial perturbations to constrain the exploration space of contrastive augmented views, avoiding the decrease of task-specific information. Furthermore, to incorporate global user-user and item-item collaboration relationships for guiding on the generation of hard contrastive views, we propose an adversarial-contrastive learning objective to construct a relation-aware view-generator. Besides, considering that unsupervised GCL could potentially narrower margins between data points and the decision boundary, resulting in decreased model robustness, we introduce the adversarial examples based on maximum perturbations to achieve margin maximization. We also provide theoretical analyses on the effectiveness of our designs. Through extensive experiments on five public datasets, we demonstrate the superiority of RGCL compared against twelve baseline models. | Jiakai Tang, Sunhao Dai, Zexu Sun, Xu Chen, Jun Xu, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li | Kuaishou Technology, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Reinforced Compressive Neural Architecture Search for Versatile Adversarial Robustness](https://doi.org/10.1145/3637528.3672009) |  | 0 | Prior research on neural architecture search (NAS) for adversarial robustness has revealed that a lightweight and adversarially robust sub-network could exist in a non-robust large teacher network. Such a sub-network is generally discovered based on heuristic rules to perform neural architecture search. However, heuristic rules are inadequate to handle diverse adversarial attacks and different "teacher" network capacity. To address this key challenge, we propose Reinforced Compressive Neural Architecture Search (RC-NAS), aiming to achieve Versatile Adversarial Robustness. Specifically, we define novel task settings that compose datasets, adversarial attacks, and teacher network configuration. Given diverse tasks, we develop an innovative dual-level training paradigm that consists of a meta-training and a fine-tuning phase to effectively expose the RL agent to diverse attack scenarios (in meta-training), and make it adapt quickly to locate an optimal sub-network (in fine-tuning) for previously unseen scenarios. Experiments show that our framework could achieve adaptive compression towards different initial teacher networks, datasets, and adversarial attacks, resulting in more lightweight and adversarially robust architectures. We also provide a theoretical analysis to explain why the reinforcement learning (RL)-guided adversarial architectural search helps adversarial robustness over standard adversarial training methods. | Dingrong Wang, Hitesh Sapkota, Zhiqiang Tao, Qi Yu | Rochester Institute of Technology, Rochester, NY, USA; Amazon Inc., Sunnyvale, CA, USA |
|  |  [Routing Evidence for Unseen Actions in Video Moment Retrieval](https://doi.org/10.1145/3637528.3671693) |  | 0 | Video moment retrieval (VMR) is a cutting-edge vision-language task locating a segment in a video according to the query. Though the methods have achieved significant performance, they assume that training and testing samples share the same action types, hindering real-world application. In this paper, we specifically consider a new problem: video moment retrieval by queries with unseen actions. We propose a plug-and-play structure, Routing Evidence (RE), with multiple evidence-learning heads and dynamically route one to locate a sentence with an unseen action. Each evidence-learning head estimates the uncertainty while regressing timestamps. We formulate the evidence distribution by a Normal-Inverse Gamma function and design a router to select the most appropriate distribution for a sample. Empirically, we study the efficacy of RE on three updated databases where training and testing samples contain different action types. We find that RE outperforms other state-of-the-art methods with a more robust predictor. Code and data will be available at https://github.com/dieuroi/Routing-Evidence. | Guolong Wang, Xun Wu, Zheng Qin, Liangliang Shi | Tsinghua University, Beijing, China |
|  |  [Unveiling Vulnerabilities of Contrastive Recommender Systems to Poisoning Attacks](https://doi.org/10.1145/3637528.3671795) |  | 0 | Contrastive learning (CL) has recently gained prominence in the domain of recommender systems due to its great ability to enhance recommendation accuracy and improve model robustness. Despite its advantages, this paper identifies a vulnerability of CL-based recommender systems that they are more susceptible to poisoning attacks aiming to promote individual items. Our analysis indicates that this vulnerability is attributed to the uniform spread of representations caused by the InfoNCE loss. Furthermore, theoretical and empirical evidence shows that optimizing this loss favors smooth spectral values of representations. This finding suggests that attackers could facilitate this optimization process of CL by encouraging a more uniform distribution of spectral values, thereby enhancing the degree of representation dispersion. With these insights, we attempt to reveal a potential poisoning attack against CL-based recommender systems, which encompasses a dual-objective framework: one that induces a smoother spectral value distribution to amplify the InfoNCE loss's inherent dispersion effect, named dispersion promotion; and the other that directly elevates the visibility of target items, named rank promotion. We validate the threats of our attack model through extensive experimentation on four datasets. By shedding light on these vulnerabilities, our goal is to advance the development of more robust CL-based recommender systems. The code is available at https://github.com/CoderWZW/ARLib. | Zongwei Wang, Junliang Yu, Min Gao, Hongzhi Yin, Bin Cui, Shazia W. Sadiq | Peking University, Beijing, China; The University of Queensland, Brisbane, Australia; Chongqing University, Chongqing, China |
|  |  [FedSAC: Dynamic Submodel Allocation for Collaborative Fairness in Federated Learning](https://doi.org/10.1145/3637528.3671748) |  | 0 | Collaborative fairness stands as an essential element in federated learningto encourage client participation by equitably distributing rewards based onindividual contributions. Existing methods primarily focus on adjustinggradient allocations among clients to achieve collaborative fairness. However,they frequently overlook crucial factors such as maintaining consistency acrosslocal models and catering to the diverse requirements of high-contributingclients. This oversight inevitably decreases both fairness and model accuracyin practice. To address these issues, we propose FedSAC, a novel Federatedlearning framework with dynamic Submodel Allocation for Collaborative fairness,backed by a theoretical convergence guarantee. First, we present the concept of"bounded collaborative fairness (BCF)", which ensures fairness by tailoringrewards to individual clients based on their contributions. Second, toimplement the BCF, we design a submodel allocation module with a theoreticalguarantee of fairness. This module incentivizes high-contributing clients withhigh-performance submodels containing a diverse range of crucial neurons,thereby preserving consistency across local models. Third, we further develop adynamic aggregation module to adaptively aggregate submodels, ensuring theequitable treatment of low-frequency neurons and consequently enhancing overallmodel accuracy. Extensive experiments conducted on three public benchmarksdemonstrate that FedSAC outperforms all baseline methods in both fairness andmodel accuracy. We see this work as a significant step towards incentivizingbroader client participation in federated learning. The source code isavailable at https://github.com/wangzihuixmu/FedSAC. | Zihui Wang, Zheng Wang, Lingjuan Lyu, Zhaopeng Peng, Zhicheng Yang, Chenglu Wen, Rongshan Yu, Cheng Wang, Xiaoliang Fan | Sony AI, Zurich, Swaziland |
|  |  [Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering](https://doi.org/10.1145/3637528.3671840) |  | 0 | Graph-based models and contrastive learning have emerged as prominent methods in Collaborative Filtering (CF). While many existing models in CF incorporate these methods in their design, there seems to be a limited depth of analysis regarding the foundational principles behind them. This paper bridges graph convolution, a pivotal element of graph-based models, with contrastive learning through a theoretical framework. By examining the learning dynamics and equilibrium of the contrastive loss, we offer a fresh lens to understand contrastive learning via graph theory, emphasizing its capability to capture high-order connectivity. Building on this analysis, we further show that the graph convolutional layers often used in graph-based models are not essential for high-order connectivity modeling and might contribute to the risk of oversmoothing. Stemming from our findings, we introduce Simple Contrastive Collaborative Filtering (SCCF), a simple and effective algorithm based on a naive embedding model and a modified contrastive loss. The efficacy of the algorithm is demonstrated through extensive experiments across four public datasets. The experiment code is available at https://github.com/wu1hong/SCCF. | Yihong Wu, Le Zhang, Fengran Mo, Tianyu Zhu, Weizhi Ma, JianYun Nie | Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China; MIIT Key Laboratory of Data Intelligence and Management, Beihang University, Beijing, China; Université de Montréal, Montréal, Canada; Mila - Quebec AI Institute, Montréal, Canada |
|  |  [Towards Lightweight Graph Neural Network Search with Curriculum Graph Sparsification](https://doi.org/10.1145/3637528.3671706) |  | 0 | Graph Neural Architecture Search (GNAS) has achieved superior performance on various graph-structured tasks. However, existing GNAS studies overlook the applications of GNAS in resource-constrained scenarios. This paper proposes to design a joint graph data and architecture mechanism, which identifies important sub-architectures via the valuable graph data. To search for optimal lightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural Architecture Search with Graph SparsIfication and Network Pruning (GASSIP) method. In particular, GASSIP comprises an operation-pruned architecture search module to enable efficient lightweight GNN search. Meanwhile, we design a novel curriculum graph data sparsification module with an architecture-aware edge-removing difficulty measurement to help select optimal sub-architectures. With the aid of two differentiable masks, we iteratively optimize these two modules to efficiently search for the optimal lightweight architecture. Extensive experiments on five benchmarks demonstrate the effectiveness of GASSIP. Particularly, our method achieves on-par or even higher node classification performance with half or fewer model parameters of searched GNNs and a sparser graph. | Beini Xie, Heng Chang, Ziwei Zhang, Zeyang Zhang, Simin Wu, Xin Wang, Yuan Meng, Wenwu Zhu | DCST, BNRist, Tsinghua University, Beijing, China; DCST, Tsinghua University, Beijing, China; Lanzhou University, Lanzhou, China |
|  |  [Revisiting Reciprocal Recommender Systems: Metrics, Formulation, and Method](https://doi.org/10.1145/3637528.3671734) |  | 0 | Reciprocal recommender systems (RRS), conducting bilateral recommendations between two involved parties, have gained increasing attention for enhancing matching efficiency. However, the majority of existing methods in the literature still reuse conventional ranking metrics to separately assess the performance on each side of the recommendation process. These methods overlook the fact that the ranking outcomes of both sides collectively influence the effectiveness of the RRS, neglecting the necessity of a more holistic evaluation and a capable systemic solution. In this paper, we systemically revisit the task of reciprocal recommendation, by introducing the new metrics, formulation, and method. Firstly, we propose five new evaluation metrics that comprehensively and accurately assess the performance of RRS from three distinct perspectives: overall coverage, bilateral stability, and balanced ranking. These metrics provide a more holistic understanding of the system's effectiveness and enable a comprehensive evaluation. Furthermore, we formulate the RRS from a causal perspective, formulating recommendations as bilateral interventions, which can better model the decoupled effects of potential influencing factors. By utilizing the potential outcome framework, we further develop a model-agnostic causal reciprocal recommendation method that considers the causal effects of recommendations. Additionally, we introduce a reranking strategy to maximize matching outcomes, as measured by the proposed metrics. Extensive experiments on two real-world datasets from recruitment and dating scenarios demonstrate the effectiveness of our proposed metrics and approach. The code and dataset are available at: https://github.com/RUCAIBox/CRRS. | Chen Yang, Sunhao Dai, Yupeng Hou, Wayne Xin Zhao, Jun Xu, Yang Song, Hengshu Zhu | Nanbeige Lab, BOSS Zhipin, Beijing, China; Career Science Lab, BOSS Zhipin, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China; University of California, San Diego, La Jolla, USA |
|  |  [Graph Bottlenecked Social Recommendation](https://doi.org/10.1145/3637528.3671807) |  | 0 | With the emergence of social networks, social recommendation has become an essential technique for personalized services. Recently, graph-based social recommendations have shown promising results by capturing the high-order social influence. Most empirical studies of graph-based social recommendations directly take the observed social networks into formulation, and produce user preferences based on social homogeneity. Despite the effectiveness, we argue that social networks in the real-world are inevitably noisy~(existing redundant social relations), which may obstruct precise user preference characterization. Nevertheless, identifying and removing redundant social relations is challenging due to a lack of labels. In this paper, we focus on learning the denoised social structure to facilitate recommendation tasks from an information bottleneck perspective. Specifically, we propose a novel Graph Bottlenecked Social Recommendation (GBSR) framework to tackle the social noise issue. GBSR is a model-agnostic social denoising framework, that aims to maximize the mutual information between the denoised social graph and recommendation labels, meanwhile minimizing it between the denoised social graph and the original one. This enables GBSR to learn the minimal yet sufficient social structure, effectively reducing redundant social relations and enhancing social recommendations. Technically, GBSR consists of two elaborate components, preference-guided social graph refinement, and HSIC-based bottleneck learning. Extensive experimental results demonstrate the superiority of the proposed GBSR, including high performances and good generality combined with various backbones. Our code is available at: https://github.com/yimutianyang/KDD24-GBSR. | Yonghui Yang, Le Wu, Zihan Wang, Zhuangzhuang He, Richang Hong, Meng Wang | Hefei University of Technology, Hefei, China |
|  |  [Efficient and Effective Anchored Densest Subgraph Search: A Convex-programming based Approach](https://doi.org/10.1145/3637528.3671727) |  | 0 | The quest to identify local dense communities closely connected to predetermined seed nodes is vital across numerous applications. Given the seed nodes R, the R-subgraph density of a subgraph S is defined as traditional graph density of S with penalties on the nodes in S / R. The state-of-the-art (SOTA) anchored densest subgraph model, which is based on R-subgraph density, is designed to address the community search problem. However, it often struggles to efficiently uncover truly dense communities. To eliminate this issue, we propose a novel NR-subgraph density metric, a nuanced measure that identifies communities intimately linked to seed nodes and also exhibiting overall high graph density. We redefine the anchored densest subgraph search problem through the lens of NR-subgraph density and cast it as a Linear Programming (LP) problem. This allows us to transition into a dual problem, tapping into the efficiency and effectiveness of convex programming-based iterative algorithm. To solve this redefined problem, we propose two algorithms: FDP, an iterative method that swiftly attains near-optimal solutions, and FDPE, an exact approach that ensures full convergence. We perform extensive experiments on 12 real-world networks. The results show that our proposed algorithms not only outperform the SOTA methods by 3.6~14.1 times in terms of running time, but also produce subgraphs with superior internal quality. | Xiaowei Ye, RongHua Li, Lei Liang, Zhizhen Liu, Longlong Lin, Guoren Wang | Ant Group, Hangzhou, China; Southwest University, Chongqing, China; Beijing Institute of Technology, Beijing, China |
|  |  [Approximate Matrix Multiplication over Sliding Windows](https://doi.org/10.1145/3637528.3671819) |  | 0 | Large-scale streaming matrix multiplication is very common in various applications, sparking significant interest in develop efficient algorithms for approximate matrix multiplication (AMM) over streams. In addition, many practical scenarios require to process time-sensitive data and aim to compute matrix multiplication for most recent columns of the data matrices rather than the entire matrices, which motivated us to study efficient AMM algorithms over sliding windows. In this paper, we present two novel deterministic algorithms for this problem and provide corresponding error guarantees. We further reduce the space and time costs of our methods for sparse matrices by performing an approximate singular value decomposition which can utilize the sparsity of matrices. Extensive experimental results on both synthetic and real-world datasets validate our theoretical analysis and highlight the efficiency of our methods. | Ziqi Yao, Lianzhi Li, Mingsong Chen, Xian Wei, Cheng Chen | East China Normal University, Shanghai, China |
|  |  [Unsupervised Generative Feature Transformation via Graph Contrastive Pre-training and Multi-objective Fine-tuning](https://doi.org/10.1145/3637528.3672015) |  | 0 | Feature transformation is to derive a new feature set from original features to augment the AI power of data. In many science domains such as material performance screening, while feature transformation can model material formula interactions and compositions and discover performance drivers, supervised labels are collected from expensive and lengthy experiments. This issue motivates an Unsupervised Feature Transformation Learning (UFTL) problem. Prior literature, such as manual transformation, supervised feedback guided search, and PCA, either relies on domain knowledge or expensive supervised feedback, or suffers from large search space, or overlooks non-linear feature-feature interactions. UFTL imposes a major challenge on existing methods: how to design a new unsupervised paradigm that captures complex feature interactions and avoids large search space? To fill this gap, we connect graph, contrastive, and generative learning to develop a measurement-pretrain-finetune paradigm for UFTL. For unsupervised feature set utility measurement, we propose a feature value consistency preservation perspective and develop a mean discounted cumulative gain like unsupervised metric to evaluate feature set utility. For unsupervised feature set representation pretraining, we regard a feature set as a feature-feature interaction graph, and develop an unsupervised graph contrastive learning encoder to embed feature sets into vectors. For generative transformation finetuning, we regard a feature set as a feature cross sequence and feature transformation as sequential generation. We develop a deep generative feature transformation model that coordinates the pretrained feature set encoder and the gradient information extracted from a feature set utility evaluator to optimize a transformed feature generator. Finally, we conduct extensive experiments to demonstrate the effectiveness, efficiency, traceability, and explicitness of our framework. | Wangyang Ying, Dongjie Wang, Xuanming Hu, Yuanchun Zhou, Charu C. Aggarwal, Yanjie Fu | Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; International Business Machines T. J. Watson Research Center, Yorktown Heights, USA; The University of Kansas, Lawrence, KS, USA; Arizona State University, Tempe, AZ, USA |
|  |  [Personalized Federated Continual Learning via Multi-Granularity Prompt](https://doi.org/10.1145/3637528.3671948) |  | 0 | Personalized Federated Continual Learning (PFCL) is a new practical scenario that poses greater challenges in sharing and personalizing knowledge. PFCL not only relies on knowledge fusion for server aggregation at the global spatial-temporal perspective but also needs model improvement for each client according to the local requirements. Existing methods, whether in Personalized Federated Learning (PFL) or Federated Continual Learning (FCL), have overlooked the multi-granularity representation of knowledge, which can be utilized to overcome Spatial-Temporal Catastrophic Forgetting (STCF) and adopt generalized knowledge to itself by coarse-to-fine human cognitive mechanisms. Moreover, it allows more effectively to personalized shared knowledge, thus serving its own purpose. To this end, we propose a novel concept called multi-granularity prompt, i.e., coarse-grained global prompt acquired through the common model learning process, and fine-grained local prompt used to personalize the generalized representation. The former focuses on efficiently transferring shared global knowledge without spatial forgetting, and the latter emphasizes specific learning of personalized local knowledge to overcome temporal forgetting. In addition, we design a selective prompt fusion mechanism for aggregating knowledge of global prompts distilled from different clients. By the exclusive fusion of coarse-grained knowledge, we achieve the transmission and refinement of common knowledge among clients, further enhancing the performance of personalization. Extensive experiments demonstrate the effectiveness of the proposed method in addressing STCF as well as improving personalized performance. | Hao Yu, Xin Yang, Xin Gao, Yan Kang, Hao Wang, Junbo Zhang, Tianrui Li | JD Intelligent Cities Research & JD iCity, JD Technology, Beijing, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; Webank, Shenzhen, China; College of Computer Science, Sichuan University, Chengdu, China |
|  |  [DipDNN: Preserving Inverse Consistency and Approximation Efficiency for Invertible Learning](https://doi.org/10.1145/3637528.3672036) |  | 0 | Consistent bi-directional inferences are the key for many machine learning applications. Without consistency, inverse learning-based inferences can cause fuzzy images, erroneous control signals, and cascading failure in SCADA systems. Since standard deep neural networks (DNNs) are not inherently invertible to offer consistency, some past methods reconstruct DNN architecture analytically for one-to-one correspondence but compromise key features such as universal approximation. Other work maintains the capability of universal approximation in DNNs via iterative numerical approximation. However, these methods limit their applications significantly due to Lipschitz conditions and issues of numerical convergence. The dilemma of the analytical and numerical methods is the incompatibility between nonlinear layer compositions and bijective function construction for inverse modeling. Based on the observation, we propose decomposed-invertible-pathway DNNs (DipDNN). It relaxes the redundant reconstruction of nested DNN in the former methods and eases the Lipschitz constraint. As a result, we strictly guarantee the consistency of global inverse modeling without harming DNN's capability for universal approximation. As numerical stability and generalizability are keys for controlling critical infrastructures, we integrate contractive property with a parallel structure for inductive biases, leading to stable performance. Numerical results show that DipDNN performs significantly better than past methods, thanks to its enforcement of inverse consistency, numerical stability, and physical regularization. | Jingyi Yuan, Yang Weng, Erik Blasch | Air Force Research Lab, Arlington, VA, USA; Arizona State University, Tempe, AZ, USA |
|  |  [Conditional Logical Message Passing Transformer for Complex Query Answering](https://doi.org/10.1145/3637528.3671869) |  | 0 | Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model. https://github.com/qianlima-lab/CLMPT. | Chongzhi Zhang, Zhiping Peng, Junhao Zheng, Qianli Ma | Guangdong University of Petrochemical Technology & Jiangmen Polytechnic, Maoming, China; South China University of Technology, Guangzhou, China |
|  |  [Natural Language Explainable Recommendation with Robustness Enhancement](https://doi.org/10.1145/3637528.3671781) |  | 0 | Natural language explainable recommendation has become a promising direction to facilitate more efficient and informed user decisions. Previous models mostly focus on how to enhance the explanation accuracy. However, the robustness problem has been largely ignored, which requires the explanations generated for similar user-item pairs should not be too much different. Different from traditional classification problems, improving the robustness of natural languages has two unique characteristics: (1) Different token importances, that is, different tokens play various roles in representing the complete sentence, and the robustness requirements for predicting them should also be different. (2) Continuous token semantics, that is, the similarity of the output should be judged based on semantics, and the sequences without any token-level overlap may also be highly similar. Based on these characteristics, we formulate and solve a novel problem in the recommendation domain, that is, robust natural language explainable recommendation. To the best of our knowledge, it is the first time in this field. Specifically, we base our modeling on adversarial robust optimization and design four types of heuristic methods to modify the adversarial outputs with weighted token probabilities and synonym replacements. Furthermore, to consider the mutual influence between the above characteristics, we regard language generation as a decision-making problem and design a dual-policy reinforcement learning framework to improve the robustness of the generated languages. We conduct extensive experiments to demonstrate the effectiveness of our framework. | Jingsen Zhang, Jiakai Tang, Xu Chen, Wenhui Yu, Lantao Hu, Peng Jiang, Han Li | Kuaishou Technology, Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Enabling Collaborative Test-Time Adaptation in Dynamic Environment via Federated Learning](https://doi.org/10.1145/3637528.3671908) |  | 0 | Deep learning models often suffer performance degradation when test data diverges from training data. Test-Time Adaptation (TTA) aims to adapt a trained model to the test data distribution using unlabeled test data streams. In many real-world applications, it is quite common for the trained model to be deployed across multiple devices simultaneously. Although each device can execute TTA independently, it fails to leverage information from the test data of other devices. To address this problem, we introduce Federated Learning (FL) to TTA to facilitate on-the-fly collaboration among devices during test time. The workflow involves clients (i.e., the devices) executing TTA locally, uploading their updated models to a central server for aggregation, and downloading the aggregated model for inference. However, implementing FL in TTA presents many challenges, especially in establishing inter-client collaboration in dynamic environment, where the test data distribution on different clients changes over time in different manners. To tackle these challenges, we propose a server-side Temporal-Spatial Aggregation (TSA) method. TSA utilizes a temporal-spatial attention module to capture intra-client temporal correlations and inter-client spatial correlations. To further improve robustness against temporal-spatial heterogeneity, we propose a heterogeneity-aware augmentation method and optimize the module using a self-supervised approach. More importantly, TSA can be implemented as a plug-in to TTA methods in distributed environments. Experiments on multiple datasets demonstrate that TSA outperforms existing methods and exhibits robustness across various levels of heterogeneity. The code is available at https://github.com/ZhangJiayuan-BUAA/FedTSA. | Jiayuan Zhang, Xuefeng Liu, Yukang Zhang, Guogang Zhu, Jianwei Niu, Shaojie Tang | Jindal School of Management, The University of Texas at Dallas, Richardson, TX, USA; Shenzhen International Graduate School, Tsinghua University, Beijing, China |
|  |  [Topology-aware Embedding Memory for Continual Learning on Expanding Networks](https://doi.org/10.1145/3637528.3671732) |  | 0 | Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding networks, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework,i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from O (ndL) to O (n)1: memory budget, d: average node degree, L: the radius of the GNN receptive field, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subnetwork viaTopology-aware Embeddings (TEs), which compress ego-subnetworks into compact vectors (i.e., TEs) to reduce the memory consumption. Based on this framework, we discover a unique pseudo-training effect in continual learning on expanding networks and this effect motivates us to develop a novel coverage maximization sampling strategy that can enhance the performance with a tight memory budget. Thorough empirical studies demonstrate that, by tackling the memory explosion problem and incorporating topological information into memory replay, PDGNNs with TEM significantly outperform state-of-the-art techniques, especially in the challenging class-incremental setting. | Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao | The University of Sydney, Sydney, NSW, Australia; Washington University, Saint Louis, St. Louis, MO, USA; University of Connecticut, Storrs, CT, USA |
|  |  [Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing](https://doi.org/10.1145/3637528.3671823) |  | 0 | Enhancing diverse human decision-making processes in an urban environment is a critical issue across various applications, including ride-sharing vehicle dispatching, public transportation management, and autonomous driving. Offline reinforcement learning (RL) is a promising approach to learn and optimize human urban strategies (or policies) from pre-collected human-generated spatial-temporal urban data. However, standard offline RL faces two significant challenges: (1) data scarcity and data heterogeneity, and (2) distributional shift. In this paper, we introduce MODA - a Multi-Task Offline Reinforcement Learning with Contrastive Data Sharing approach. MODA addresses the challenges of data scarcity and heterogeneity in a multi-task urban setting through Contrastive Data Sharing among tasks. This technique involves extracting latent representations of human behaviors by contrasting positive and negative data pairs. It then shares data presenting similar representations with the target task, facilitating data augmentation for each task. Moreover, MODA develops a novel model-based multi-task offline RL algorithm. This algorithm constructs a robust Markov Decision Process (MDP) by integrating a dynamics model with a Generative Adversarial Network (GAN). Once the robust MDP is established, any online RL or planning algorithm can be applied. Extensive experiments conducted in a real-world multi-task urban setting validate the effectiveness of MODA. The results demonstrate that MODA exhibits significant improvements compared to state-of-the-art baselines, showcasing its capability in advancing urban decision-making processes. We also made our code available to the research community. | Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, Yanhua Li, Jun Luo | University of Maryland, College Park, College Park, MD, USA; Worcester Polytechnic Institute, Worcester, MA, USA; Lehigh University, Bethlehem, PA, USA; San Diego State University, San Diego, CA, USA; Binghamton University, Binghamton, NY, USA; Logistics and Supply Chain MultiTech R&D Centre, Hong Kong, Hong Kong |
|  |  [Synthesizing Multimodal Electronic Health Records via Predictive Diffusion Models](https://doi.org/10.1145/3637528.3671836) |  | 0 | Synthesizing electronic health records (EHR) data has become a preferredstrategy to address data scarcity, improve data quality, and model fairness inhealthcare. However, existing approaches for EHR data generation predominantlyrely on state-of-the-art generative techniques like generative adversarialnetworks, variational autoencoders, and language models. These methodstypically replicate input visits, resulting in inadequate modeling of temporaldependencies between visits and overlooking the generation of time information,a crucial element in EHR data. Moreover, their ability to learn visitrepresentations is limited due to simple linear mapping functions, thuscompromising generation quality. To address these limitations, we propose anovel EHR data generation model called EHRPD. It is a diffusion-based modeldesigned to predict the next visit based on the current one while alsoincorporating time interval estimation. To enhance generation quality anddiversity, we introduce a novel time-aware visit embedding module and apioneering predictive denoising diffusion probabilistic model (PDDPM).Additionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM.Weconduct experiments on two public datasets and evaluate EHRPD from fidelity,privacy, and utility perspectives. The experimental results demonstrate theefficacy and utility of the proposed EHRPD in addressing the aforementionedlimitations and advancing EHR data generation. | Yuan Zhong, Xiaochen Wang, Jiaqi Wang, Xiaokun Zhang, Yaqing Wang, Mengdi Huai, Cao Xiao, Fenglong Ma | Purdue University, West Lafayette, IN, USA; GE Healthcare, Seattle, WA, USA; The Penn State University, University Park, PA, USA; Iowa State University, Ames, IA, USA; Dalian University of Technology, Dalian, Liaoning, China; The Pennsylvania State University, University Park, PA, USA |
|  |  [Generative AI in E-Commerce: What Can We Expect?](https://doi.org/10.1145/3637528.3672503) |  | 0 | The impact of generative AI on e-commerce is profound. It has significantly improved the understanding of user intent and serves as a comprehensive product knowledge graph. However, the most substantial disruptions are yet to come, partic- ularly through the rise of autonomous agents. In this talk, I will outline a tentative path toward a future where e-commerce not only offers an unparalleled customer experience but also thrives in a world dominated by generative AI and autonomous agents. | Haixun Wang | Instacart, San Francisco, CA, USA |
|  |  [LiRank: Industrial Large Scale Ranking Models at LinkedIn](https://doi.org/10.1145/3637528.3671561) |  | 0 | We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. These ideas have contributed to relative metrics improvements across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76% qualified job applications for Jobs search and recommendations, and +4.3% for Ads CTR. We hope this work can provide practical insights and solutions for practitioners interested in leveraging large-scale deep ranking systems. | Fedor Borisyuk, Mingzhou Zhou, Qingquan Song, Siyu Zhu, Birjodh Tiwana, Ganesh Parameswaran, Siddharth Dangi, Lars Hertel, Qiang Charles Xiao, Xiaochen Hou, Yunbo Ouyang, Aman Gupta, Sheallika Singh, Dan Liu, Hailing Cheng, Lei Le, Jonathan Hung, Sathiya Keerthi, Ruoyan Wang, Fengyu Zhang, Mohit Kothari, Chen Zhu, Daqi Sun, Yun Dai, Xun Luan, Sirou Zhu, Zhiwei Wang, Neil Daftary, Qianqi Shen, Chengming Jiang, Haichao Wei, Maneesh Varshney, Amol Ghoting, Souvik Ghosh | LinkedIn, Mountain View, CA, USA |
|  |  [Enhancing Multi-field B2B Cloud Solution Matching via Contrastive Pre-training](https://doi.org/10.1145/3637528.3671513) |  | 0 | Cloud solutions have gained significant popularity in the technology industry as they offer a combination of services and tools to tackle specific problems. However, despite their widespread use, the task of identifying appropriate company customers for a specific target solution to the sales team of a solution provider remains a complex business problem that existing matching systems have yet to adequately address. In this work, we study the B2B solution matching problem and identify two main challenges of this scenario: (1) the modeling of complex multi-field features and (2) the limited, incomplete, and sparse transaction data. To tackle these challenges, we propose a framework CAMA, which is built with a hierarchical multi-field matching structure as its backbone and supplemented by three data augmentation strategies and a contrastive pre-training objective to compensate for the imperfections in the available data. Through extensive experiments on a real-world dataset, we demonstrate that CAMA outperforms several strong baseline matching models significantly. Furthermore, we have deployed our matching framework on a system of Huawei Cloud. Our observations indicate an improvement of about 30% compared to the previous online model in terms of Conversion Rate (CVR), which demonstrates its great business value. | Haonan Chen, Zhicheng Dou, Xuetong Hao, Yunhao Tao, Shiren Song, Zhenli Sheng | Renmin University of China, Beijing, China; Huawei Cloud Computing, Hangzhou, China |
|  |  [GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants](https://doi.org/10.1145/3637528.3671622) |  | 0 | We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures. | Sophie Fischer, Carlos Gemmell, Niklas Tecklenburg, Iain Mackie, Federico Rossetto, Jeffrey Dalton | University of Edinburgh, Edinburgh, United Kingdom; University of Glasgow, Glasgow, United Kingdom |
|  |  [Enhancing E-commerce Spelling Correction with Fine-Tuned Transformer Models](https://doi.org/10.1145/3637528.3671625) |  | 0 | In the realm of e-commerce, the process of search stands as the primary point of interaction for users, wielding a profound influence on the platform's revenue generation. Notably, spelling correction assumes a pivotal role in shaping the user's search experience by rectifying erroneous query inputs, thus facilitating more accurate retrieval outcomes. Within the scope of this research paper, our aim is to enhance the existing state-of-the-art discriminative model performance with generative modelling strategies while concurrently addressing the engineering concerns associated with real-time online latency, inherent to models of this category. We endeavor to refine LSTM-based classification models for spelling correction through a generative fine-tuning approach hinged upon pre-trained language models. Our comprehensive offline assessments have yielded compelling results, showcasing that transformer-based architectures, such as BART (developed by Facebook) and T5 (a product of Google), have achieved a 4% enhancement in F1 score compared to baseline models for the English language sites. Furthermore, to mitigate the challenges posed by latency, we have incorporated model pruning techniques like no-teacher distillation. We have undertaken the deployment of our model (English only) as an A/B test candidate for real-time e-commerce traffic, encompassing customers from the US and the UK. The model attest to a 100% successful request service rate within real-time scenarios, with median, 90th percentile, and 99th percentile (p90/p99) latencies comfortably falling below production service level agreements. Notably, these achievements are further reinforced by positive customer engagement, transactional and search page metrics, including a significant reduction in instances of search results page with low or almost zero recall. Moreover, we have also extended our efforts into fine-tuning a multilingual model, which, notably, exhibits substantial accuracy enhancements, amounting to a minimum of 16%, across four distinct European languages and English. | Arnab Dutta, Gleb Polushin, Xiaoshuang Zhang, Daniel Stein | eBay GmbH, Aachen, Germany; eBay Inc., Shanghai, China; eBay GmbH, Dreilinden, Germany |
|  |  [Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information](https://doi.org/10.1145/3637528.3671652) |  | 0 | Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer-based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. Code for our method is available at https://github.com/CDAL-SOC/PREDICT-AI. We also present the design of a treatment recommendation system (TRS), which is currently deployed at the National University Hospital, Singapore and is being evaluated in a clinical trial. We discuss why the recommended drugs and their predicted scores alone, obtained from DRP models, are insufficient for treatment planning. Treatment planning for complex cancer cases, in the face of limited clinical validation, requires assessment of many other factors, including several indirect sources of evidence on drug efficacy. We discuss key lessons learnt on model validation and use of indirect supporting evidence to build clinicians' trust and aid their decision making. | Aishwarya Jayagopal, Hansheng Xue, Ziyang He, Robert J. Walsh, Krishna Kumar Hariprasannan, David Shao Peng Tan, Tuan Zea Tan, Jason J. Pitt, Anand D. Jeyasekharan, Vaibhav Rajan | Cancer Science Institute of Singapore, Singapore, Singapore; National University Cancer Institute, Singapore, Singapore; National University of Singapore, Singapore, Singapore |
|  |  [ERASE: Benchmarking Feature Selection Methods for Deep Recommender Systems](https://doi.org/10.1145/3637528.3671571) |  | 0 | Deep Recommender Systems (DRS) are increasingly dependent on a large number of feature fields for more precise recommendations. Effective feature selection methods are consequently becoming critical for further enhancing the accuracy and optimizing storage efficiencies to align with the deployment demands. This research area, particularly in the context of DRS, is nascent and faces three core challenges. Firstly, variant experimental setups across research papers often yield unfair comparisons, obscuring practical insights. Secondly, the existing literature's lack of detailed analysis on selection attributes, based on large-scale datasets and a thorough comparison among selection techniques and DRS backbones, restricts the generalizability of findings and impedes deployment on DRS. Lastly, research often focuses on comparing the peak performance achievable by feature selection methods. This approach is typically computationally infeasible for identifying the optimal hyperparameters and overlooks evaluating the robustness and stability of these methods. To bridge these gaps, this paper presents ERASE, a comprehensive bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation of eleven feature selection methods, covering both traditional and deep learning approaches, across four public datasets, private industrial datasets, and a real-world commercial platform, achieving significant enhancement. Our code is available online for ease of reproduction. | Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang | Huawei Noah's Ark Lab, Shenzhen, China; City University of Hong Kong, Hong Kong, China |
|  |  [Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration](https://doi.org/10.1145/3637528.3671577) |  | 0 | Predicting click-through rate (CTR) is a critical task in recommendation systems, where the models are optimized with pointwise loss to infer the probability of items being clicked. In industrial practice, applications also require ranking items based on these probabilities. Existing solutions primarily combine the ranking-based loss, i.e., pairwise and listwise loss, with CTR prediction. However, they can hardly calibrate or generalize well in CTR scenarios where the clicks reflect the binary preference. This is because the binary click feedback leads to a large number of ties, which renders high data sparsity. In this paper, we propose an effective data augmentation strategy, named Beyond Binary Preference (BBP) training framework, to address this problem. Our key idea is to break the ties by leveraging Bayesian approaches, where the beta distribution models click behavior as probability distributions in the training data that naturally break ties. Therefore, we can obtain an auxiliary training label that generates more comparable pairs and improves the ranking performance. Besides, BBP formulates ranking and calibration as a multi-task framework to optimize both objectives simultaneously. Through extensive offline experiments and online tests on various datasets, we demonstrate that BBP significantly outperforms state-of-the-art methods in both ranking and calibration capabilities, showcasing its effectiveness in addressing the limitations of existing methods. Our code is available at https://github.com/AlvinIsonomia/BBP. | Chang Liu, Qiwei Wang, Wenqing Lin, Yue Ding, Hongtao Lu | Shanghai Jiao Tong University, Shanghai, China; Tencent, Shenzhen, China |
|  |  [Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning](https://doi.org/10.1145/3637528.3671618) |  | 0 | Given an input query, a recommendation model is trained using user feedback data (e.g., click data) to output a ranked list of items. In real-world systems, besides accuracy, an important consideration for a new model is novelty of its top-k recommendations w.r.t. an existing deployed model. However, novelty of top-k items is a difficult goal to optimize a model for, since it involves a non-differentiable sorting operation on the model's predictions. Moreover, novel items, by definition, do not have any user feedback data. Given the semantic capabilities of large language models, we address these problems using a reinforcement learning (RL) formulation where large language models provide feedback for the novel items. However, given millions of candidate items, the sample complexity of a standard RL algorithm can be prohibitively high. To reduce sample complexity, we reduce the top-k list reward to a set of item-wise rewards and reformulate the state space to consist of tuples such that the action space is reduced to a binary decision; and show that this reformulation results in a significantly lower complexity when the number of items is large. We evaluate the proposed algorithm on improving novelty for a query-ad recommendation task on a large-scale search engine. Compared to supervised finetuning on recent pairs, the proposed RL-based algorithm leads to significant novelty gains with minimal loss in recall. We obtain similar results on the ORCAS query-webpage matching dataset and a product recommendation dataset based on Amazon reviews. | Amit Sharma, Hua Li, Xue Li, Jian Jiao | Microsoft Bing Ads, Redmond, USA; Microsoft Bing Ads, Mountain View, USA; Microsoft Research, Bengaluru, India |
|  |  [Measuring an LLM's Proficiency at using APIs: A Query Generation Strategy](https://doi.org/10.1145/3637528.3671592) |  | 0 | Connecting Large Language Models (LLMs) with the ability to leverage APIs (Web Search, Charting, Calculators, Calendar, Flight Search, Hotel Search, Data Lookup, etc. ) is likely to allow us to solve a variety of new hard problems. Several research efforts have made this observation and suggested recipes for LLMs to emit API calls, and proposed mechanisms by which they can generate additional text conditioned on the output for the API call. However, in practice, the focus has been on relatively simple slot-filling tasks that make an API call rather unlocking novel capabilities by combining different tools, reasoning over the response from a tool, making multiple invocations, or complex planning. In this paper, we pose the following question: what does it mean to say that an LLM is proficient at using a set of APIs? We answer this question in the context of structured APIs by defining seven capabilities for API-use. We provide an approach for generating synthetic tasks that exercise each of these capabilities given only the description of an API. We argue that this provides practitioners with a principled way to construct a dataset to evaluate an LLM's ability to use a given set of APIs. Through human evaluations, we show that our approach produces high-quality tasks for each of the seven capabilities. We also describe how we used this approach to on-board new API and create principled evaluation sets for multiple LLM-based products. | Ying Sheng, Sudeep Gandhe, Bhargav Kanagal, Nick Edmonds, Zachary Fisher, Sandeep Tata, Aarush Selvan | Google Research, Mountain View, CA, USA; Google, Mountain View, CA, USA |
|  |  [PEMBOT: Pareto-Ensembled Multi-task Boosted Trees](https://doi.org/10.1145/3637528.3671619) |  | 0 | Multi-task problems frequently arise in machine learning when there are multiple target variables, which share a common synergy while being sufficiently different that optimizing on any of the task does not necessarily imply an optimum for the others. In this work, we develop PEMBOT, a novel Pareto-based multi-task classification framework using a gradient boosted tree architecture. The proposed methodology involves a) generating multiple instances of Pareto optimal trees, b) diverse subset selection using a determinantal point process (DPP) model, and c) ensembling of diverse Pareto optimal trees to yield the final output. We tested our framework on a problem from an e-commerce domain wherein the task is to predict at order placement time the different adverse scenarios in the order shipment journey such as the package getting lost or damaged during shipment. This model enables us to take preemptive measures to prevent these scenarios from happening resulting in significant operational cost savings. Further, to show the generality of our approach, we demonstrate the performance of our algorithm on a publicly available wine quality prediction dataset and compare against state-of-the-art baselines. | Gokul Swamy, Anoop Saladi, Arunita Das, Shobhit Niranjan | Amazon, Bengaluru, KA, India; Amazon, Seattle, WA, USA |
|  |  [Enhancing Personalized Headline Generation via Offline Goal-conditioned Reinforcement Learning with Large Language Models](https://doi.org/10.1145/3637528.3671638) |  | 0 | Recently, significant advancements have been made in Large Language Models (LLMs) through the implementation of various alignment techniques. These techniques enable LLMs to generate highly tailored content in response to diverse user instructions. Consequently, LLMs have the potential to serve as robust, customizable recommendation systems in the field of content recommendation. However, using LLMs with user individual information and online exploration remains a challenge, which are important perspectives in developing personalized news headline generation algorithms. In this paper, we propose a novel framework to generate personalized news headlines using LLMs with extensive online exploration. The proposed approach involves initially training an offline goal-conditioned policy using supervised learning. Subsequently, online exploration is employed to collect new data for the next training iteration. Results from simulations, experiments, and real-word scenario demonstrate that our framework achieves outstanding performance on established benchmarks and can effectively generate personalized headlines under different reward settings. By treating the LLM as a goal-conditioned agent, the model can perform online exploration by modifying the goals without frequently retraining the model. To the best of our knowledge, this work represents the first investigation into the capability of LLMs to generate customized news headlines with goal-conditioned reinforcement learning via supervised learning within LLMs. | Xiaoyu Tan, Leijun Cheng, Xihe Qiu, Shaojie Shi, Yuan Cheng, Wei Chu, Yinghui Xu, Yuan Qi | AI3 Institute, Fudan University, Shanghai, China; INF Technology (Shanghai) Co., Ltd., Shanghai, China |
|  |  [Future Impact Decomposition in Request-level Recommendations](https://doi.org/10.1145/3637528.3671506) |  | 0 | In recommender systems, reinforcement learning solutions have shown promisingresults in optimizing the interaction sequence between users and the systemover the long-term performance. For practical reasons, the policy's actions aretypically designed as recommending a list of items to handle users' frequentand continuous browsing requests more efficiently. In this list-wiserecommendation scenario, the user state is updated upon every request in thecorresponding MDP formulation. However, this request-level formulation isessentially inconsistent with the user's item-level behavior. In this study, wedemonstrate that an item-level optimization approach can better utilize itemcharacteristics and optimize the policy's performance even under therequest-level MDP. We support this claim by comparing the performance ofstandard request-level methods with the proposed item-level actor-criticframework in both simulation and online experiments. Furthermore, we show thata reward-based future decomposition strategy can better express the item-wisefuture impact and improve the recommendation accuracy in the long term. Toachieve a more thorough understanding of the decomposition strategy, we proposea model-based re-weighting framework with adversarial learning that furtherboost the performance and investigate its correlation with the reward-basedstrategy. | Xiaobei Wang, Shuchang Liu, Xueliang Wang, Qingpeng Cai, Lantao Hu, Han Li, Peng Jiang, Kun Gai, Guangming Xie | Kuaishou Technology, Beijing, China; Unaffiliated, Beijing, China; Peking University, Beijing, China |
|  |  [Face4Rag: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese](https://doi.org/10.1145/3637528.3671656) |  | 0 | The prevailing issue of factual inconsistency errors in conventional Retrieval Augmented Generation (RAG) motivates the study of Factual Consistency Evaluation (FCE). Despite the various FCE methods proposed earlier, these methods are evaluated on datasets generated by specific Large Language Models (LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE methods perform on other LLMs with different error distributions or even unseen error types, as these methods may fail to detect the error types generated by other LLMs. To fill this gap, in this paper, we propose the first comprehensive FCE benchmark Face4RAG for RAG independent of the underlying LLM. Our benchmark consists of a synthetic dataset built upon a carefully designed typology for factuality inconsistency error and a real-world dataset constructed from six commonly used LLMs, enabling evaluation of FCE methods on specific error types or real-world error distributions. On the proposed benchmark, we discover the failure of existing FCE methods to detect the logical fallacy, which refers to a mismatch of logic structures between the answer and the retrieved reference. To fix this issue, we further propose a new method called L-Face4RAG with two novel designs of logic-preserving answer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG substantially outperforms previous methods for factual inconsistency detection on a wide range of tasks, notably beyond the RAG task from which it is originally motivated. Both the benchmark and our proposed method are publicly available. https://huggingface.co/datasets/yq27/Face4RAG | Yunqi Xu, Tianchi Cai, Jiyan Jiang, Xierui Song | Tsinghua University, Beijing, China; Ant Group, Hangzhou, China; Ant Group, Shanghai, China |
|  |  [A Self-boosted Framework for Calibrated Ranking](https://doi.org/10.1145/3637528.3671570) |  | 0 | Scale-calibrated ranking systems are ubiquitous in real-world applications nowadays, which pursue accurate ranking quality and calibrated probabilistic predictions simultaneously. For instance, in the advertising ranking system, the predicted click-through rate (CTR) is utilized for ranking and required to be calibrated for the downstream cost-per-click ads bidding. Recently, multi-objective based methods have been wildly adopted as a standard approach for Calibrated Ranking, which incorporates the combination of two loss functions: a pointwise loss that focuses on calibrated absolute values and a ranking loss that emphasizes relative orderings. However, when applied to industrial online applications, existing multi-objective CR approaches still suffer from two crucial limitations First, previous methods need to aggregate the full candidate list within a single mini-batch to compute the ranking loss. Such aggregation strategy violates extensive data shuffling which has long been proven beneficial for preventing overfitting, and thus degrades the training effectiveness. Second, existing multi-objective methods apply the two inherently conflicting loss functions on a single probabilistic prediction, which results in a sub-optimal trade-off between calibration and ranking. To tackle the two limitations, we propose a Self-Boosted framework for Calibrated Ranking (SBCR). In SBCR, the predicted ranking scores by the online deployed model are dumped into context features. With these additional context features, each single item can perceive the overall distribution of scores in the whole ranking list, so that the ranking loss can be constructed without the need for sample aggregation. As the deployed model is a few versions older than the training model, the dumped predictions reveal what was failed to learn and keep boosting the model to correct previously mis-predicted items. Moreover, a calibration module is introduced to decouple the point loss and ranking loss. The two losses are applied before and after the calibration module separately, which elegantly addresses the sub-optimal trade-off problem. We conduct comprehensive experiments on industrial scale datasets and online A/B tests, demonstrating that SBCR can achieve advanced performance on both calibration and ranking. Our method has been deployed on the video search system of Kuaishou, and results in significant performance improvements on CTR and the total amount of time users spend on Kuaishou. | Shunyu Zhang, Hu Liu, Wentian Bao, Enyun Yu, Yang Song | Kuaishou Technology, Beijing, China; Columbia University, Beijing, China; Northeasten University, Beijing, China |
|  |  [Bringing Multimodality to Amazon Visual Search System](https://doi.org/10.1145/3637528.3671640) |  | 0 | Image to image matching has been well studied in the computer vision community. Previous studies mainly focus on training a deep metric learning model matching visual patterns between the query image and gallery images. In this study, we show that pure image-to- image matching suffers from false positives caused by matching to local visual patterns. To alleviate this issue, we propose to leverage recent advances in vision-language pretraining research. Specifically, we introduce additional image-text alignment losses into deep metric learning, which serve as constraints to the image-to-image matching loss. With additional alignments between the text (e.g., product title) and image pairs, the model can learn concepts from both modalities explicitly, which avoids matching low-level visual features. We progressively develop two variants, a 3-tower and a 4-tower model, where the latter takes one more short text query input. Through extensive experiments, we show that this change leads to a substantial improvement to the image to image matching problem. We further leveraged this model for multimodal search, which takes both image and reformulation text queries to improve search quality. Both offline and online experiments show strong improvements on the main metrics. Specifically, we see 4.95% relative improvement on image matching click through rate with the 3-tower model and 1.13% further improvement from the 4-tower model. | Xinliang Zhu, ShengWei Huang, Han Ding, Jinyu Yang, Kelvin Chen, Tao Zhou, Tal Neiman, Ouye Xie, Son Tran, Benjamin Z. Yao, Douglas Gray, Anuj Bindal, Arnab Dhua | Amazon.com, Seattle, WA, USA; Amazon.com, New York, New York, USA; Amazon.com, Santa Clara, CA, USA; Amazon.com, Palo Alto, CA, USA |
|  |  [A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)](https://doi.org/10.1145/3637528.3671474) |  | 0 | Traditional recommender systems typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a "tutorial" presented at ACM KDD'24, with supporting materials provided at: https://encr.pw/vDhLq. | Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, Silvia Milano | Bespoke Labs, Santa Clara, CA, USA; Amazon, Palo Alto, CA, USA; University of Edinburgh, Edinburgh, UK; University of California, La Jolla, CA, USA; University of Toronto, Toronto, ON, Canada; Polytechnic University of Bari, Bari, Italy; University of Exeter and LMU Munich, Munich, Germany |
|  |  [A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models](https://doi.org/10.1145/3637528.3671470) |  | 0 | As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/ | Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, TatSeng Chua, Qing Li | The Hong Kong Polytechnic University, Hong Kong, China; Baidu Inc., Beijing, CN; National university of Singapore, Singapore, SG |
|  |  [Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey](https://doi.org/10.1145/3637528.3671473) |  | 0 | Personalized recommendation serves as a ubiquitous channel for users to discover information tailored to their interests. However, traditional recommendation models primarily rely on unique IDs and categorical features for user-item matching, potentially overlooking the nuanced essence of raw item contents across multiple modalities such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, especially in multimedia services like news, music, and short-video platforms. The recent advancements in large multimodal models offer new opportunities and challenges in developing content-aware recommender systems. This survey seeks to provide a comprehensive exploration of the latest advancements and future trajectories in multimodal pretraining, adaptation, and generation techniques, as well as their applications in enhancing recommender systems. Furthermore, we discuss current open challenges and opportunities for future research in this dynamic domain. We believe that this survey, alongside the curated resources, will provide valuable insights to inspire further advancements in this evolving landscape. | Qijiong Liu, Jieming Zhu, Yanting Yang, Quanyu Dai, Zhaocheng Du, XiaoMing Wu, Zhou Zhao, Rui Zhang, Zhenhua Dong | Zhejiang University, Hangzhou, China; Huawei Noah Ark Lab, Shenzhen, China; Huazhong University of Science and Technology & ruizhang.info, Shenzhen, China; The HK PolyU, Hong Kong, China; Huawei Noah's Ark Lab, Shenzhen, China |
|  |  [AI for Education (AI4EDU): Advancing Personalized Education with LLM and Adaptive Learning](https://doi.org/10.1145/3637528.3671498) |  | 0 | Recent advanced AI technologies, especially large language models (LLMs) like GPTs, have significantly advanced the field of data mining and led to the development of various LLM-based applications. AI for education (AI4EDU) is a vibrant multi-disciplinary field of data mining, machine learning, and education, with increasing importance and extraordinary potential. In this field, LLM and adaptive learning-based models can be utilized as interfaces in human-in-the-loop education systems, where the model serves as a mediator among the teacher, students, and machine capabilities, including its own. This perspective has several benefits, including the ability to personalize interactions, allow unprecedented flexibility and adaptivity for human-AI collaboration and improve the user experience. However, several challenges still exist, including the need for more robust and efficient algorithms, designing effective user interfaces, and ensuring ethical considerations are addressed. This workshop aims to bring together researchers and practitioners from academia and industry to explore cutting-edge AI technologies for personalized education, especially the potential of LLMs and adaptive learning technologies. | Qingsong Wen, Jing Liang, Carles Sierra, Rose Luckin, Richard Jiarui Tong, Zitao Liu, Peng Cui, Jiliang Tang | Squirrel Ai Learning, Shanghai, China; Squirrel Ai Learning, Bellevue, USA; Tsinghua University, Beijing, China; Jinan University, Guangzhou, China; Michigan State University, East Lansing, USA; IIIA of the Spanish National Research Council, Barcelona, Spain; University College London, London, United Kingdom |
|  |  [Understanding Inter-Session Intentions via Complex Logical Reasoning](https://doi.org/10.1145/3637528.3671808) |  | 0 | Understanding user intentions is essential for improving product recommendations, navigation suggestions, and query reformulations. However, user intentions can be intricate, involving multiple sessions and attribute requirements connected by logical operators such as And, Or, and Not. For instance, a user may search for Nike or Adidas running shoes across various sessions, with a preference for purple. In another example, a user may have purchased a mattress in a previous session and is now looking for a matching bed frame without intending to buy another mattress. Existing research on session understanding has not adequately addressed making product or attribute recommendations for such complex intentions. In this paper, we present the task of logical session complex query answering (LS-CQA), where sessions are treated as hyperedges of items, and we frame the problem of complex intention understanding as an LS-CQA task on an aggregated hypergraph of sessions, items, and attributes. This is a unique complex query answering task with sessions as ordered hyperedges. We also introduce a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure. We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators. By evaluating LSGT on three datasets, we demonstrate that it achieves state-of-the-art results. | Jiaxin Bai, Chen Luo, Zheng Li, Qingyu Yin, Yangqiu Song | Department of CSE, HKUST, Hong Kong, China; Amazon.com Inc, Palo Alto, USA |
|  |  [Online Preference Weight Estimation Algorithm with Vanishing Regret for Car-Hailing in Road Network](https://doi.org/10.1145/3637528.3671664) |  | 0 | Car-hailing services play an important role in the modern transportation system, and the utilities of the service providers highly depend on the efficiency of route planning algorithms. A widely adopted route planning framework is to assign weights to roads and compute the routes with the shortest path algorithms. Existing techniques of weight-assigning often focus on the traveling time and length of the roads, but cannot incorporate with the preferences of the passengers (users). In this paper, a set of preference weight estimation models is employed to capture the users' preferences over paths in car-hailing with their historical choices. Since the user preferences may vary dynamically over time, it is a challenging task to make real-time decisions over the models. The main technical contribution of this paper is to propose an online learning-based preference weight chasing (PWC) algorithm to solve this problem. The worst-case performance of PWC is analyzed with the metric regret, and it is proved that PWC has a vanishing regret, which means that the time-averaged loss concerning the fixed in-hindsight best model tends to zero. Experiments based on real-world datasets are conducted to verify the effectiveness and efficiency of our algorithm. The code is available at https://github.com/GaoYucen/PWC. | Yucen Gao, Zhehao Zhu, Mingqian Ma, Fei Gao, Hui Gao, Yangguang Shi, Xiaofeng Gao | Shandong University, Qingdao, China; Didi Global Inc., Beijing, China; Shanghai Jiao Tong University, Shanghai, China; Northwestern University, Evanston, IL, USA |
|  |  [A Population-to-individual Tuning Framework for Adapting Pretrained LM to On-device User Intent Prediction](https://doi.org/10.1145/3637528.3671984) |  | 0 | Mobile devices, especially smartphones, can support rich functions and have developed into indispensable tools in daily life. With the rise of generative AI services, smartphones can potentially transform into personalized assistants, anticipating user needs and scheduling services accordingly. Predicting user intents on smartphones, and reflecting anticipated activities based on past interactions and context, remains a pivotal step towards this vision. Existing research predominantly focuses on specific domains, neglecting the challenge of modeling diverse event sequences across dynamic contexts. Leveraging pre-trained language models (PLMs) offers a promising avenue, yet adapting PLMs to on-device user intent prediction presents significant challenges. To address these challenges, we propose PITuning, a Population-to-Individual Tuning framework. PITuning enhances common pattern extraction through dynamic event-to-intent transition modeling and addresses long-tailed preferences via adaptive unlearning strategies. Experimental results on real-world datasets demonstrate PITuning's superior intent prediction performance, highlighting its ability to capture long-tailed preferences and its practicality for on-device prediction scenarios. | Jiahui Gong, Jingtao Ding, Fanjin Meng, Guilong Chen, Hong Chen, Shen Zhao, Haisheng Lu, Yong Li | Honor Device Co., Ltd., Shenzhen, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China |
|  |  [Robust Auto-Bidding Strategies for Online Advertising](https://doi.org/10.1145/3637528.3671729) |  | 0 | In online advertising, existing auto-bidding strategies for bid shading mainly adopt the approach of first predicting the winning price distribution and then calculating the optimal bid. However, the winning price information available to the Demand Side Platforms (DSPs) is extremely limited, and the associated uncertainties make it challenging for DSPs to accurately estimate winning price distribution. To address this challenge, we conducted a comprehensive analysis of the process by which DSPs obtain winning price information, and abstracted two types of uncertainties from it: known uncertainty and unknown uncertainty. Based on these uncertainties, we proposed two levels of robust bidding strategies: Robust Bidding for Censorship (RBC) and Robust Bidding for Distribution Shift (RBDS), which offer guarantees for the surplus in the worst-case scenarios under uncertain conditions. Experimental results on public datasets demonstrate that our robust bidding strategies consistently enable DSPs to achieve superior surpluses, both on test sets and under worst-case conditions. | Qilong Lin, Zhenzhe Zheng, Fan Wu | Shanghai Jiao Tong University, Shanghai, China |
|  |  [QSketch: An Efficient Sketch for Weighted Cardinality Estimation in Streams](https://doi.org/10.1145/3637528.3671695) |  | 0 | Estimating cardinality, i.e., the number of distinct elements, of a data stream is a fundamental problem in areas like databases, computer networks, and information retrieval. This study delves into a broader scenario where each element carries a positive weight. Unlike traditional cardinality estimation, limited research exists on weighted cardinality, with current methods requiring substantial memory and computational resources, challenging for devices with limited capabilities and real-time applications like anomaly detection. To address these issues, we propose QSketch, a memory-efficient sketch method for estimating weighted cardinality in streams. QSketch uses a quantization technique to condense continuous variables into a compact set of integer variables, with each variable requiring only 8 bits, making it 8 times smaller than previous methods. Furthermore, we leverage dynamic properties during QSketch generation to significantly enhance estimation accuracy and achieve a lower time complexity of O(1) for updating estimations upon encountering a new element. Experimental results on synthetic and real-world datasets show that QSketch is approximately 30% more accurate and two orders of magnitude faster than the state-of-the-art, using only 1/8 of the memory. | Yiyan Qi, Rundong Li, Pinghui Wang, Yufang Sun, Rui Xing | MOE KLINNS Lab, Shaanxi Normal University, Xi'an, China; MOE KLINNS Lab, Xi'an Jiaotong University, Xi'an, China; International Digital Economy Academy (IDEA), Shenzhen, China |
|  |  [Make Your Home Safe: Time-aware Unsupervised User Behavior Anomaly Detection in Smart Homes via Loss-guided Mask](https://doi.org/10.1145/3637528.3671708) |  | 0 | Smart homes, powered by the Internet of Things, offer great convenience but also pose security concerns due to abnormal behaviors, such as improper operations of users and potential attacks from malicious attackers. Several behavior modeling methods have been proposed to identify abnormal behaviors and mitigate potential risks. However, their performance often falls short because they do not effectively learn less frequent behaviors, consider temporal context, or account for the impact of noise in human behaviors. In this paper, we propose SmartGuard, an autoencoder-based unsupervised user behavior anomaly detection framework. First, we design a Loss-guided Dynamic Mask Strategy (LDMS) to encourage the model to learn less frequent behaviors, which are often overlooked during learning. Second, we propose a Three-level Time-aware Position Embedding (TTPE) to incorporate temporal information into positional embedding to detect temporal context anomaly. Third, we propose a Noise-aware Weighted Reconstruction Loss (NWRL) that assigns different weights for routine behaviors and noise behaviors to mitigate the interference of noise behaviors during inference. Comprehensive experiments demonstrate that SmartGuard consistently outperforms state-of-the-art baselines and also offers highly interpretable results. | Jingyu Xiao, Zhiyao Xu, Qingsong Zou, Qing Li, Dan Zhao, Dong Fang, Ruoyu Li, Wenxin Tang, Kang Li, Xudong Zuo, Penghui Hu, Yong Jiang, Zixuan Weng, Michael R. Lyu | Peng Cheng Laborotary, Shenzhen, China; Tsinghua University, Beijing, China; Tencent, Shenzhen, China; Peng Cheng Laboratory & Tsinghua Shenzhen International Graduate School, Shenzhen, China; Tsinghua Shenzhen International Graduate School & Peng Cheng Laboratory, Shenzhen, China; Beijing Jiaotong University, Beijing, China; Tsinghua Shenzhen International Graduate School, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong, China; Peng Cheng Laboratory, Shenzhen, China; Xi'an University of Electronic Science and Technology, Xi'an, China |
|  |  [Top-Down Bayesian Posterior Sampling for Sum-Product Networks](https://doi.org/10.1145/3637528.3671876) |  | 0 | Sum-product networks (SPNs) are probabilistic models characterized by exactand fast evaluation of fundamental probabilistic operations. Its superiorcomputational tractability has led to applications in many fields, such asmachine learning with time constraints or accuracy requirements and real-timesystems. The structural constraints of SPNs supporting fast inference, however,lead to increased learning-time complexity and can be an obstacle to buildinghighly expressive SPNs. This study aimed to develop a Bayesian learningapproach that can be efficiently implemented on large-scale SPNs. We derived anew full conditional probability of Gibbs sampling by marginalizing multiplerandom variables to expeditiously obtain the posterior distribution. Thecomplexity analysis revealed that our sampling algorithm works efficiently evenfor the largest possible SPN. Furthermore, we proposed a hyperparameter tuningmethod that balances the diversity of the prior distribution and optimizationefficiency in large-scale SPNs. Our method has improved learning-timecomplexity and demonstrated computational speed tens to more than one hundredtimes faster and superior predictive performance in numerical experiments onmore than 20 datasets. | Soma Yokoi, Issei Sato | The University of Tokyo, Tokyo, Japan |
|  |  [CompanyKG: A Large-Scale Heterogeneous Graph for Company Similarity Quantification](https://doi.org/10.1145/3637528.3671515) |  | 0 | In the investment industry, it is often essential to carry out fine-grainedcompany similarity quantification for a range of purposes, including marketmapping, competitor analysis, and mergers and acquisitions. We propose andpublish a knowledge graph, named CompanyKG, to represent and learn diversecompany features and relations. Specifically, 1.17 million companies arerepresented as nodes enriched with company description embeddings; and 15different inter-company relations result in 51.06 million weighted edges. Toenable a comprehensive assessment of methods for company similarityquantification, we have devised and compiled three evaluation tasks withannotated test sets: similarity prediction, competitor retrieval and similarityranking. We present extensive benchmarking results for 11 reproduciblepredictive methods categorized into three groups: node-only, edge-only, andnode+edge. To the best of our knowledge, CompanyKG is the first large-scaleheterogeneous graph dataset originating from a real-world investment platform,tailored for quantifying inter-company similarity. | Lele Cao, Vilhelm von Ehrenheim, Mark GranrothWilding, Richard Anselmo Stahl, Andrew McCornack, Armin Catovic, Dhiana Deva Cavalcanti Rocha | Motherbrain, EQT Group & Silo AI, Stockholm, Sweden; Motherbrain, EQT Group & QA.tech, Stockholm, Sweden; Motherbrain, EQT Group, Stockholm, Sweden |
|  |  [CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning](https://doi.org/10.1145/3637528.3671538) |  | 0 | Advances in graph machine learning (ML) have been driven by applications in chemistry, as graphs have remained the most expressive representations of molecules. This has led to progress within both fields, as challenging chemical data has helped improve existing methods and to develop new ones. While early graph ML methods focused primarily on small organic molecules, more recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to immediately address. Moving to inorganic nanomaterials further increases complexity as the scale of number of nodes within each graph can be broad (10 to 100k). In addition, the bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. The most exciting applications of graph ML will be in their generative capabilities, in order to explore the vast chemical space from a data-driven perspective. Currently, generative modelling of graphs is not at par with other domains such as images or text, as generating chemically valid molecules and materials of varying properties is not straightforward. In this work, we invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets. These datasets contain nanomaterials of different scales and properties represented as graphs of varying sizes. The first dataset is a medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K). This dataset has a narrower chemical scope focused on an interesting part of chemical space with a lot of active research. The second is a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). The crystal structures used in CHILI-100K are obtained from a curated subset of the Crystallography Open Database (COD) and has a broader chemical scope covering database entries for 68 metals and 11 non-metals. We define 11 property prediction tasks covering node-, edge-, and graph- level tasks that span classification and regression. In addition we also define structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods starting with simple baselines to multiple off-the-shelf graph neural networks. Based on these benchmarking results, we highlight areas which need future work to achieve useful performance for applications in (nano) materials chemistry. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale - both on the individual graph level and of the dataset as a whole - and the only nanomaterials datasets with high structural and elemental diversity. | Ulrik FriisJensen, Frederik L. Johansen, Andy S. Anker, Erik B. Dam, Kirsten M. Ø. Jensen, Raghavendra Selvan | Department of Chemistry, University of Copenhagen, Copenhagen, Denmark; Department of Computer Science, University of Copenhagen, Copenhagen, Denmark |
|  |  [Offline Reinforcement Learning for Optimizing Production Bidding Policies](https://doi.org/10.1145/3637528.3671555) |  | 0 | The online advertising market, with its thousands of auctions run per second, presents a daunting challenge for advertisers who wish to optimize their spend under a budget constraint. Thus, advertising platforms typically provide automated agents to their customers, which act on their behalf to bid for impression opportunities in real time at scale. Because these proxy agents are owned by the platform but use advertiser funds to operate, there is a strong practical need to balance reliability and explainability of the agent with optimizing power. We propose a generalizable approach to optimizing bidding policies in production environments by learning from real data using offline reinforcement learning. This approach can be used to optimize any differentiable base policy (practically, a heuristic policy based on principles which the advertiser can easily understand), and only requires data generated by the base policy itself. We use a hybrid agent architecture that combines arbitrary base policies with deep neural networks, where only the optimized base policy parameters are eventually deployed, and the neural network part is discarded after training. We demonstrate that such an architecture achieves statistically significant performance gains in both simulated and at-scale production bidding environments. Our approach does not incur additional infrastructure, safety, or explainability costs, as it directly optimizes parameters of existing production routines without replacing them with black box-style models like neural networks. | Dmytro Korenkevych, Frank Cheng, Artsiom Balakir, Alex Nikulkov, Lingnan Gao, Zhihao Cen, Zuobing Xu, Zheqing Zhu | AI at Meta, Bellevue, USA; AI at Meta, Menlo Park, USA; Meta Platform Inc., Menlo Park, USA; AI at Meta, Sunnyvale, USA |
|  |  [Spending Programmed Bidding: Privacy-friendly Bid Optimization with ROI Constraint in Online Advertising](https://doi.org/10.1145/3637528.3671540) |  | 0 | Privacy policies have disrupted the multi-billion dollar online advertising market by making real-time and precise user data untraceable, which poses significant challenges to the optimization of Return-On-Investment (ROI) constrained products in the online advertising industry. Privacy protection strategies, including event aggregation and reporting delays, hinder access to detailed and instantaneous feedback data, thus incapacitating traditional identity-revealing attribution techniques. In this paper, we introduces a novel Spending Programmed Bidding (SPB) framework to navigate these challenges. SPB is a two-stage framework that separates long horizon delivery spend planning (the macro stage) and short horizon bidding execution (the micro stage). The macro stage models the target ROI to achieve maximum utility and derives the expected spend, whereas the micro stage optimizes the bid price given the expected spend. We further extend our framework to the cross-channel scenario where the agent bids in both privacy-constrained and identity-revealing attribution channels. We find that when privacy-constrained channels are present, SPB is superior to state-of-the-art bidding methods in both offline datasets and online experiments on a large ad platform. | Yumin Su, Min Xiang, Yifei Chen, Yanbiao Li, Tian Qin, Hongyi Zhang, Yasong Li, Xiaobing Liu | ByteDance Inc., San Jose, CA, USA; ByteDance Inc., Beijing, China; ByteDance Inc., Singapore |
|  |  [Know in AdVance: Linear-Complexity Forecasting of Ad Campaign Performance with Evolving User Interest](https://doi.org/10.1145/3637528.3671528) |  | 0 | Real-time Bidding (RTB) advertisers wish to know in advance theexpected cost and yield of ad campaigns to avoid trial-and-error expenses.However, Campaign Performance Forecasting (CPF), a sequence modeling taskinvolving tens of thousands of ad auctions, poses challenges of evolving userinterest, auction representation, and long context, making coarse-grained andstatic-modeling methods sub-optimal. We propose AdVance, a time-awareframework that integrates local auction-level and global campaign-levelmodeling. User preference and fatigue are disentangled using a time-positionedsequence of clicked items and a concise vector of all displayed items.Cross-attention, conditioned on the fatigue vector, captures the dynamics ofuser interest toward each candidate ad. Bidders compete with each other,presenting a complete graph similar to the self-attention mechanism. Hence, weemploy a Transformer Encoder to compress each auction into embedding by solvingauxiliary tasks. These sequential embeddings are then summarized by aconditional state space model (SSM) to comprehend long-range dependencies whilemaintaining global linear complexity. Considering the irregular time intervalsbetween auctions, we make SSM's parameters dependent on the current auctionembedding and the time interval. We further condition SSM's global predictionson the accumulation of local results. Extensive evaluations and ablationstudies demonstrate its superiority over state-of-the-art methods. AdVance hasbeen deployed on the Tencent Advertising platform, and A/B tests show aremarkable 4.5% uplift in Average Revenue per User (ARPU). | Xiaoyu Wang, Yonghui Guo, Hui Sheng, Peili Lv, Chi Zhou, Wei Huang, Shiqin Ta, Dongbo Huang, Xiujin Yang, Lan Xu, Hao Zhou, Yusheng Ji | Tencent Advertising, Shanghai, China; University of Science and Technology of China & National Institute of Informatics, Hefei, China |
|  |  [Trinity: Syncretizing Multi-/Long-Tail/Long-Term Interests All in One](https://doi.org/10.1145/3637528.3671651) |  | 0 | Interest modeling in recommender system has been a constant topic for improving user experience, and typical interest modeling tasks (e.g. multi-interest, long-tail interest and long-term interest) have been investigated in many existing works. However, most of them only consider one interest in isolation, while neglecting their interrelationships. In this paper, we argue that these tasks suffer from a common "interest amnesia" problem, and a solution exists to mitigate it simultaneously. We propose a novel and unified framework in the retrieval stage, "Trinity", to solve interest amnesia problem and improve multiple interest modeling tasks. We construct a real-time clustering system that enables us to project items into enumerable clusters, and calculate statistical interest histograms over these clusters. Based on these histograms, Trinity recognizes underdelivered themes and remains stable when facing emerging hot topics. Its derived retrievers have been deployed on the recommender system of Douyin, significantly improving user experience and retention. We believe that such practical experience can be well generalized to other scenarios. | Jing Yan, Liu Jiang, Jianfei Cui, Zhichen Zhao, Xingyan Bin, Feng Zhang, Zuotao Liu | ByteDance Inc., Shanghai, China; ByteDance Inc., Beijing, China |
|  |  [Temporal Uplift Modeling for Online Marketing](https://doi.org/10.1145/3637528.3671560) |  | 0 | In recent years, uplift modeling, also known as individual treatment effect (ITE) estimation, has seen wide applications in online marketing, such as delivering one-time issuance of coupons or discounts to motivate users' purchases. However, complex yet more realistic scenarios involving multiple interventions over time on users are still rarely explored. The challenges include handling the bias from time-varying confounders, determining optimal treatment timing, and selecting among numerous treatments. In this paper, to tackle the aforementioned challenges, we present a temporal point process-based uplift model (TPPUM) that utilizes users' temporal event sequences to estimate treatment effects via counterfactual analysis and temporal point processes. In this model, marketing actions are considered as treatments, user purchases as outcome events, and how treatments alter the future conditional intensity function of generating outcome events as the uplift. Empirical evaluations demonstrate that our method outperforms existing baselines on both real-world and synthetic datasets. In the online experiment conducted in a discounted bundle recommendation scenario involving an average of 3 to 4 interventions per day and hundreds of treatment candidates, we demonstrate how our model outperforms current state-of-the-art methods in selecting the appropriate treatment and timing of treatment, resulting in a 3.6% increase in application-level revenue. | Xin Zhang, Kai Wang, Zengmao Wang, Bo Du, Shiwei Zhao, Runze Wu, Xudong Shen, Tangjie Lv, Changjie Fan | Netease Fuxi AI Lab, Hangzhou, China; Wuhan University, Wuhan, China; NetEase Fuxi AI Lab, Hangzhou, China |
|  |  [STATE: A Robust ATE Estimator of Heavy-Tailed Metrics for Variance Reduction in Online Controlled Experiments](https://doi.org/10.1145/3637528.3672352) |  | 0 | Online controlled experiments play a crucial role in enabling data-driven decisions across a wide range of companies. Variance reduction is an effective technique to improve the sensitivity of experiments, achieving higher statistical power while using fewer samples and shorter experimental periods. However, typical variance reduction methods (e.g., regression-adjusted estimators) are built upon the intuitional assumption of Gaussian distributions and cannot properly characterize the real business metrics with heavy-tailed distributions. Furthermore, outliers diminish the correlation between pre-experiment covariates and outcome metrics, greatly limiting the effectiveness of variance reduction. In this paper, we develop a novel framework that integrates the Student's t-distribution with machine learning tools to fit heavy-tailed metrics and construct a robust average treatment effect estimator in online controlled experiments, which we call STATE. By adopting a variational EM method to optimize the loglikehood function, we can infer a robust solution that greatly eliminates the negative impact of outliers and achieves significant variance reduction. Moreover, we extend the STATE method from count metrics to ratio metrics by utilizing linear transformation that preserves unbiased estimation, whose variance reduction is more complex but less investigated in existing works. Finally, both simulations on synthetic data and long-term empirical results on Meituan experiment platform demonstrate the effectiveness of our method. Compared with the state-of-the-art estimators (CUPAC/MLRATE), STATE achieves over 50% variance reduction, indicating it can reach the same statistical power with only half of the observations, or half the experimental duration. | Hao Zhou, Kun Sun, Shaoming Li, Yangfeng Fan, Guibin Jiang, Jiaqi Zheng, Tao Li | Meituan, Beijing, China; State Key Laboratory for Novel Software Technology, Nanjing University & Meituan, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China |
|  |  [Practical Machine Learning for Streaming Data](https://doi.org/10.1145/3637528.3671442) |  | 0 | Machine Learning for Data Streams has been an important area of research since the late 1990s, and its use in industry has grown significantly over the last few years. However, there is still a gap between the cutting-edge research and the tools that are readily available, which makes it challenging for practitioners, including experienced data scientists, to implement and evaluate these methods in this complex domain. Our tutorial aims to bridge this gap with a dual focus. We will discuss important research topics, such as partially delayed labeled streams, while providing practical demonstrations of their implementation and assessment using CapyMOA, an open-source library that provides efficient algorithm implementations through a high-level Python API. Source code is available in https://github.com/adaptive-machine-learning/CapyMOA while the accompanying tutorials and installation guide are available in https://capymoa.org/. | Heitor Murilo Gomes, Albert Bifet | AI Institute, University of Waikato & LTCI, Télécom Paris, IP Paris, Waikato, New Zealand |
|  |  [Empower an End-to-end Scalable and Interpretable Data Science Ecosystem using Statistics, AI and Domain Science](https://doi.org/10.1145/3637528.3672194) |  | 0 | The data science ecosystem encompasses data fairness, statistical, ML and AI methods and tools, interpretable data analysis and results, and trustworthy decision-making. Rapid advancements in AI have revolutionized data utilization and enabled machines to learn from data more effectively. Statistics, as the science of learning from data while accounting for uncertainty, plays a pivotal role in addressing complex real-world problems and facilitating trustworthy decision-making. In this talk, I will discuss the challenges and opportunities involved in building an end-to-end scalable and interpretable data science ecosystem using the analysis of whole genome sequencing studies and biobanks that integrates statistics, ML/AI, and genomic and health science as an example. Biobanks collect whole genome data, electronic health records and epidemiological data. I will illustrate key points using the analysis of multi-ancestry whole genome sequencing studies and biobanks by discussing a few scalable and interpretable statistical and ML/AI methods, tools and data science resources. Specifically, first, data fairness and diversity is a critical pillar of a trustworthy data science ecosystem. About 85+% of genome wide association study samples in the last 15 years are European, resulting in disparity in genetic research. I will discuss the community effort on improving diversity in genetic studies in the last 10 years. I will present trans-ancestry polygenic risk scores (PRS) using millions of common genetic variants across the genome by leveraging large GWAS sample sizes of European and smaller sample sizes of under-represented populations for predicting disease risk using transfer learning and genetic association summary statistics. The performance of deep learning methods for PRS will also be discussed. Second, scalability in cloud platforms is critical for large scale affordable analysis for multi-ancestry biobanks and whole genome studies. I will discuss improving scalability in cloud-computing using interpretable sparsity via FastSparseGRM. To build an interpretable and powerful end-to-end ecosystem of rare variant analysis of large scale whole genome sequencing studies and biobanks, I will first introduce FAVOR, a multi-faceted variant functional annotation database and portal of all possible 9 billions of variants across the whole genome. I will discuss FAVOR-GPT, a LLM interface of the FAVOR functional annotation database to improve user experience for navigating FAVOR and performing variant functional annotation query and variant functional summary statistics calculations. I will also discuss FAVORannotator which can be used to functionally annotate any whole genome sequencing studies. I will also discuss STAAR and STAAR and STAARpipeline, the WGS rare variant analysis pipeline that boosts the power of WGS rare variant association analysis by dynamically incorporating multi-faceted variant functional annotations. Extension of incorporating single-cell data in WGS analysis will also be discussed. I will also discuss ensemble methods that improve the power of rare variant association tests. Cloud-deployment of these resources and tools in several ecosystems will be presented, such as RAP for the UK biobank, AnVIL for the NHGRI Genome Sequencing Program and All of Us, and BioData Catalyst for the NHLBI Trans-omics Precision Medine Program (TOPMed). This talk aims to ignite proactive and thought-provoking discussions, foster collaboration, and cultivate open-minded approaches to advance scientific discovery. | Xihong Lin | Harvard University, Boston, MA, USA |
|  |  [Semi-Supervised Learning for Time Series Collected at a Low Sampling Rate](https://doi.org/10.1145/3637528.3672033) |  | 0 | Although time-series classification has many applications in healthcare and manufacturing, the high cost of data collection and labeling hinders its widespread use. To reduce data collection and labeling costs while maintaining high classification accuracy, we propose a novel problem setting, called semi-supervised learning with low-sampling-rate time series, in which the majority of time series are collected at a low sampling rate and are unlabeled whereas the minority of time series are collected at a high sampling rate and are labeled. For this novel problem scenario, we develop the SemiTSR framework equipped with the super-resolution module and the semi-supervised learning module. Here, low-sampling-rate time series are upsampled precisely, taking periodicity and trend at each timestamp into account, and both labeled and unlabeled high-sampling-rate time series are utilized for training. In particular, consistency regularization between artificially downsampled time series derived from an original high-sampling-rate time series is effective at overcoming limited sampling rates. We demonstrate that SemiTSR significantly outperforms conventional semi-supervised learning techniques by assuring high classification accuracy with low-sampling-rate time series. | Minyoung Bae, Yooju Shin, Youngeun Nam, Youngseop Lee, JaeGil Lee | Samsung Electronics Co., Ltd., Suwon-si, Republic of Korea; KAIST, Daejeon, Republic of Korea |
|  |  [Meta Clustering of Neural Bandits](https://doi.org/10.1145/3637528.3671691) |  | 0 | The contextual bandit has been identified as a powerful framework to formulate the recommendation process as a sequential decision-making process, where each item is regarded as an arm and the objective is to minimize the regret of T rounds. In this paper, we study a new problem, Clustering of Neural Bandits, by extending previous work to the arbitrary reward function, to strike a balance between user heterogeneity and user correlations in the recommender system. To solve this problem, we propose a novel algorithm called M-CNB, which utilizes a meta-learner to represent and rapidly adapt to dynamic clusters, along with an informative Upper Confidence Bound (UCB)-based exploration strategy. We provide an instance-dependent performance guarantee for the proposed algorithm that withstands the adversarial context, and we further prove the guarantee is at least as good as state-of-the-art (SOTA) approaches under the same assumptions. In extensive experiments conducted in both recommendation and online classification scenarios, M-CNB outperforms SOTA baselines. This shows the effectiveness of the proposed approach in improving online recommendation and online classification performance. | Yikun Ban, Yunzhe Qi, Tianxin Wei, Lihui Liu, Jingrui He | University of Illinois, Urbana Champaign, Champaign, IL, USA; University of Illinois, Urbana-Champaign, Champaign, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA |
|  |  [Popularity-Aware Alignment and Contrast for Mitigating Popularity Bias](https://doi.org/10.1145/3637528.3671824) |  | 0 | Collaborative Filtering (CF) typically suffers from the significant challengeof popularity bias due to the uneven distribution of items in real-worlddatasets. This bias leads to a significant accuracy gap between popular andunpopular items. It not only hinders accurate user preference understanding butalso exacerbates the Matthew effect in recommendation systems. To alleviatepopularity bias, existing efforts focus on emphasizing unpopular items orseparating the correlation between item representations and their popularity.Despite the effectiveness, existing works still face two persistent challenges:(1) how to extract common supervision signals from popular items to improve theunpopular item representations, and (2) how to alleviate the representationseparation caused by popularity bias. In this work, we conduct an empiricalanalysis of popularity bias and propose Popularity-Aware Alignment and Contrast(PAAC) to address two challenges. Specifically, we use the common supervisorysignals modeled in popular item representations and propose a novelpopularity-aware supervised alignment module to learn unpopular itemrepresentations. Additionally, we suggest re-weighting the contrastive learningloss to mitigate the representation separation from a popularity-centricperspective. Finally, we validate the effectiveness and rationale of PAAC inmitigating popularity bias through extensive experiments on three real-worlddatasets. Our code is available athttps://github.com/miaomiao-cai2/KDD2024-PAAC. | Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, Min Zhang, Meng Wang | Hefei University of Technology, Hefei, China; Tsinghua University, Beijing, China; DCST, Tsinghua University & Quan Cheng Laboratory, Beijing, China; DCST, Tsinghua University, Beijing, China |
|  |  [Enhancing Contrastive Learning on Graphs with Node Similarity](https://doi.org/10.1145/3637528.3671898) |  | 0 | Graph Neural Networks (GNNs) have achieved great success in learning graph representations and thus facilitating various graph-related tasks. However, most GNN methods adopt a supervised learning setting, which is not always feasible in real-world applications due to the difficulty to obtain labeled data. Hence, graph self-supervised learning has been attracting increasing attention. Graph contrastive learning (GCL) is a representative framework for self-supervised learning. In general, GCL learns node representations by contrasting semantically similar nodes (positive samples) and dissimilar nodes (negative samples) with anchor nodes. Without access to labels, positive samples are typically generated by data augmentation, and negative samples are uniformly sampled from the entire graph, which leads to a sub-optimal objective. Specifically, data augmentation naturally limits the number of positive samples that involve in the process (typically only one positive sample is adopted). On the other hand, the random sampling process would inevitably select false-negative samples (samples sharing the same semantics with the anchor). These issues limit the learning capability of GCL. In this work, we propose an enhanced objective that addresses the aforementioned issues. We first introduce an unachievable ideal objective that contains all positive samples and no false-negative samples. This ideal objective is then transformed into a probabilistic form based on the distributions for sampling positive and negative samples. We then model these distributions with node similarity and derive the enhanced objective. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed enhanced objective under different settings. | Hongliang Chi, Yao Ma | Rensselaer Polytechnic Institute, Troy, NY, USA |
|  |  [Efficient Exploration of the Rashomon Set of Rule-Set Models](https://doi.org/10.1145/3637528.3671818) |  | 0 | Today, as increasingly complex predictive models are developed, simple rule sets remain a crucial tool to obtain interpretable predictions and drive high-stakes decision making. However, a single rule set provides a partial representation of a learning task. An emerging paradigm in interpretable machine learning aims at exploring the Rashomon set of all models exhibiting near-optimal performance. Existing work on Rashomon-set exploration focuses on exhaustive search of the Rashomon set for particular classes of models, which can be a computationally challenging task. On the other hand, exhaustive enumeration leads to redundancy that often is not necessary, and a representative sample or an estimate of the size of the Rashomon set is sufficient for many applications. In this work, we propose, for the first time, efficient methods to explore the Rashomon set of rule set models with or without exhaustive search. Extensive experiments demonstrate the effectiveness of the proposed methods in a variety of scenarios. | Martino Ciaperoni, Han Xiao, Aristides Gionis | The Upright Project, Helsinki, Uusimaa, Finland; Aalto University, Espoo, Uusimaa, Finland; KTH Royal Institute of Technology, Stockholm, Sweden |
|  |  [Fairness in Streaming Submodular Maximization Subject to a Knapsack Constraint](https://doi.org/10.1145/3637528.3671778) |  | 0 | Submodular optimization has been identified as a powerful tool for many data mining applications, where a representative subset of moderate size needs to be extracted from a large-scale dataset. In scenarios where data points possess sensitive attributes such as age, gender, or race, it becomes imperative to integrate fairness measures into submodular optimization to mitigate bias and discrimination. In this paper, we study the fundamental problem of fair submodular maximization subject to a knapsack constraint and propose the first streaming algorithm for it with provable performance guarantees for both monotone and non-monotone submodular functions. As a byproduct, we also propose a streaming algorithm for submodular maximization subject to a partition matroid and a knapsack constraint, significantly improving the performance bounds achieved by previous work. We conduct extensive experiments on real-world applications such as movie recommendation, image summarization, and maximum coverage in social networks. The experimental results strongly demonstrate the superiority of our proposed algorithms in terms of both fairness and utility. | Shuang Cui, Kai Han, Shaojie Tang, Feng Li, Jun Luo | Nanyang Technological University, Singapore, Singapore; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China; The University of Texas at Dallas, Richardson, TX, USA |
|  |  [AGS-GNN: Attribute-guided Sampling for Graph Neural Networks](https://doi.org/10.1145/3637528.3671940) |  | 0 | We propose AGS-GNN, a novel attribute-guided sampling algorithm for Graph Neural Networks (GNNs). AGS-GNN exploits the node features and the connectivity structure of a graph while simultaneously adapting for both homophily and heterophily in graphs. In homophilic graphs, vertices of the same class are more likely to be adjacent, but vertices of different classes tend to be adjacent in heterophilic graphs. GNNs have been successfully applied to homophilic graphs, but their utility to heterophilic graphs remains challenging. The state-of-the-art GNNs for heterophilic graphs use the full neighborhood of a node instead of sampling it, and hence do not scale to large graphs and are not inductive. We develop dual-channel sampling techniques based on feature-similarity and feature-diversity to select subsets of neighbors for a node that capture adaptive information from homophilic and heterophilic neighborhoods. Currently, AGS-GNN is the only algorithm that explicitly controls homophily in the sampled subgraph through similar and diverse neighborhood samples. For diverse neighborhood sampling, we employ submodularity, a novel contribution in this context. We pre-compute the sampling distribution in parallel, achieving the desired scalability. Using an extensive dataset consisting of 35 small (< 100K nodes) and large (- 100K nodes) homophilic and heterophilic graphs, we demonstrate the superiority of AGS-GNN compared to the state-of-the-art approaches. AGS-GNN achieves test accuracy comparable to the best-performing heterophilic GNNs, even outperforming methods that use the entire graph for node classification. AGS-GNN converges faster than methods that sample neighborhoods randomly, and can be incorporated into existing GNN models that employ node or graph sampling. | Siddhartha Shankar Das, S. M. Ferdous, Mahantesh M. Halappanavar, Edoardo Serra, Alex Pothen | Purdue University, West Lafayette, IN, USA; Pacific Northwest National Lab., Richland, WA, USA; Boise State University, Boise, ID, USA |
|  |  [Estimated Judge Reliabilities for Weighted Bradley-Terry-Luce Are Not Reliable](https://doi.org/10.1145/3637528.3671907) |  | 0 | There are many applications for which we want to learn a latent scale for subjective properties, such as the excitement of a photo or the legibility of a font; however, obtaining human-labeled data is costly and time-consuming. One oft-used method for acquiring these labels, despite the cost being quadratic in the number of items, is the method of pairwise comparisons since this method minimizes the effect of biases and generally can be used effectively outside of a controlled environment. Crowdsourcing appears to be a panacea since online platforms provide affordable access to numerous people, but these participants, judges, vary in diligence and expertise. Several methods have been proposed to assign weights to judges based on their responses relative to everyone else, the goal being to reduce exposure to poor performers, hopefully upgrading the quality of the data. Our research focuses on two natural extensions to the Bradley-Terry-Luce formulation of scaling that jointly optimize for both scale value and judge weights. While both methods appear to perform at least as well as the unweighted formulation on average with well-behaved judges, we report a previously unknown flaw, revealing that the resultant judge weights should not be interpreted as reliabilities. Consequently, these values should not be leveraged for decisions about the judges, such as for active sampling or to validate the participant pool. | Andrew F. Dreher, Etienne Vouga, Donald S. Fussell | The University of Texas at Austin, Austin, TX, USA |
|  |  [Influence Maximization via Graph Neural Bandits](https://doi.org/10.1145/3637528.3671983) |  | 0 | We consider a ubiquitous scenario in the study of Influence Maximization(IM), in which there is limited knowledge about the topology of the diffusionnetwork. We set the IM problem in a multi-round diffusion campaign, aiming tomaximize the number of distinct users that are influenced. Leveraging thecapability of bandit algorithms to effectively balance the objectives ofexploration and exploitation, as well as the expressivity of neural networks,our study explores the application of neural bandit algorithms to the IMproblem. We propose the framework IM-GNB (Influence Maximization with GraphNeural Bandits), where we provide an estimate of the users' probabilities ofbeing influenced by influencers (also known as diffusion seeds). This initialestimate forms the basis for constructing both an exploitation graph and anexploration one. Subsequently, IM-GNB handles the exploration-exploitationtradeoff, by selecting seed nodes in real-time using Graph ConvolutionalNetworks (GCN), in which the pre-estimated graphs are employed to refine theinfluencers' estimated rewards in each contextual setting. Through extensiveexperiments on two large real-world datasets, we demonstrate the effectivenessof IM-GNB compared with other baseline methods, significantly improving thespread outcome of such diffusion campaigns, when the underlying network isunknown. | Yuting Feng, Vincent Y. F. Tan, Bogdan Cautis | Department of Mathematics, Department of ECE, National University of Singapore, Singapore, Singapore; CNRS LISN, University of Paris-Saclay, Orsay, France |
|  |  [A Unified Core Structure in Multiplex Networks: From Finding the Densest Subgraph to Modeling User Engagement](https://doi.org/10.1145/3637528.3672011) |  | 0 | In many complex systems, the interactions between objects span multipleaspects. Multiplex networks are accurate paradigms to model such systems, whereeach edge is associated with a type. A key graph mining primitive is extractingdense subgraphs, and this has led to interesting notions such as K-cores, knownas building blocks of complex networks. Despite recent attempts to extend thenotion of core to multiplex networks, existing studies suffer from a subset ofthe following limitations: They 1) force all nodes to exhibit their high degreein the same set of relation types while in multiplex networks some connectiontypes can be noisy for some nodes, 2) either require high computational cost ormiss the complex information of multiplex networks, and 3) assume the sameimportance for all relation types. We introduce S-core, a novel and unifyingfamily of dense structures in multiplex networks that uses a function S(.) tosummarize the degree vector of each node. We then discuss how one can choose aproper S(.) from the data. To demonstrate the usefulness of S-cores, we focuson finding the densest subgraph as well as modeling user engagement inmultiplex networks. We present a new density measure in multiplex networks anddiscuss its advantages over existing density measures. We show that the problemof finding the densest subgraph in multiplex networks is NP-hard and design anefficient approximation algorithm based on S-cores. Finally, we present a newmathematical model of user engagement in the presence of different relationtypes. Our experiments shows the efficiency and effectiveness of our algorithmsand supports the proposed mathematical model of user engagement. | Farnoosh Hashemi, Ali Behrouz | Cornell University, Ithaca, NY, USA |
|  |  [Budgeted Multi-Armed Bandits with Asymmetric Confidence Intervals](https://doi.org/10.1145/3637528.3671833) |  | 0 | We study the stochastic Budgeted Multi-Armed Bandit (MAB) problem, where a player chooses from K arms with unknown expected rewards and costs. The goal is to maximize the total reward under a budget constraint. A player thus seeks to choose the arm with the highest reward-cost ratio as often as possible. Current approaches for this problem have several issues, which we illustrate. To overcome them, we propose a new upper confidence bound (UCB) sampling policy, ømega-UCB, that uses asymmetric confidence intervals. These intervals scale with the distance between the sample mean and the bounds of a random variable, yielding a more accurate and tight estimation of the reward-cost ratio compared to our competitors. We show that our approach has sublinear instance-dependent regret in general and logarithmic regret for parameter ρ ≥ 1, and that it outperforms existing policies consistently in synthetic and real settings. | Marco Heyden, Vadim Arzamasov, Edouard Fouché, Klemens Böhm | Karlsruhe Institute of Technology, Karlsruhe, Germany |
|  |  [Can Modifying Data Address Graph Domain Adaptation?](https://doi.org/10.1145/3637528.3672058) |  | 0 | Graph neural networks (GNNs) have demonstrated remarkable success in numerous graph analytical tasks. Yet, their effectiveness is often compromised in real-world scenarios due to distribution shifts, limiting their capacity for knowledge transfer across changing environments or domains. Recently, Unsupervised Graph Domain Adaptation (UGDA) has been introduced to resolve this issue. UGDA aims to facilitate knowledge transfer from a labeled source graph to an unlabeled target graph. Current UGDA efforts primarily focus on model-centric methods, such as employing domain invariant learning strategies and designing model architectures. However, our critical examination reveals the limitations inherent to these model-centric methods, while a data-centric method allowed to modify the source graph provably demonstrates considerable potential. This insight motivates us to explore UGDA from a data-centric perspective. By revisiting the theoretical generalization bound for UGDA, we identify two data-centric principles for UGDA: alignment principle and rescaling principle. Guided by these principles, we propose GraphAlign, a novel UGDA method that generates a small yet transferable graph. By exclusively training a GNN on this new graph with classic Empirical Risk Minimization (ERM), GraphAlign attains exceptional performance on the target graph. Extensive experiments under various transfer scenarios demonstrate the GraphAlign outperforms the best baselines by an average of 2.16%, training on the generated graph as small as 0.25~1% of the original training graph. | Renhong Huang, Jiarong Xu, Xin Jiang, Ruichuan An, Yang Yang | Zhejiang University, Hangzhou, China; Lehigh University, Bethlehem, PA, USA; Fudan University, Shanghai, China; Xi'an Jiaotong University, Xi'an, China; Zhejiang University & Fudan University, Hangzhou, China |
|  |  [Uplift Modelling via Gradient Boosting](https://doi.org/10.1145/3637528.3672019) |  | 0 | The Gradient Boosting machine learning ensemble algorithm, well-known for its proficiency and superior performance in intricate machine learning tasks, has encountered limited success in the realm of uplift modeling. Uplift modeling is a challenging task that necessitates a known target for the precise computation of the training gradient. The prevailing two-model strategies, which separately model treatment and control outcomes, are encumbered with limitations as they fail to directly tackle the uplift problem. This paper presents an innovative approach to uplift modeling that employs Gradient Boosting. Unlike previous works, our algorithm utilizes multioutput boosting model and calculates the uplift gradient based on intermediate surrogate predictions and directly models the concealed target. This method circumvents the requirement for a known target and addresses the uplift problem more effectively than existing solutions. Moreover, we broaden the scope of this solution to encompass multitreatment settings, thereby enhancing its applicability. This novel approach not only overcomes the limitations of the traditional two-model strategies but also paves the way for more effective and efficient uplift modeling using Gradient Boosting. | Bulat Ibragimov, Anton Vakhrushev | Sber AI Lab, Moscow, Russian Federation |
|  |  [Mutual Distillation Extracting Spatial-temporal Knowledge for Lightweight Multi-channel Sleep Stage Classification](https://doi.org/10.1145/3637528.3671981) |  | 0 | Sleep stage classification has important clinical significance for the diagnosis of sleep-related diseases. To pursue more accurate sleep stage classification, multi-channel sleep signals are widely used due to the rich spatial-temporal information contained. However, it leads to a great increment in the size and computational costs, which constrain the application of multi-channel sleep models on hardware devices. Knowledge distillation is an effective way to compress models, yet existing knowledge distillation methods cannot fully extract and transfer the spatial-temporal knowledge in the multi-channel sleep signals. To solve the problem, we propose a general knowledge distillation framework for multi-channel sleep stage classification called spatial-temporal mutual distillation. Based on the spatial relationship of human body and the temporal transition rules of sleep signals, the spatial and temporal modules are designed to extract the spatial-temporal knowledge, thus help the lightweight student model learn the rich spatial-temporal knowledge from large-scale teacher model. The mutual distillation framework transfers the spatial-temporal knowledge mutually. Teacher model and student model can learn from each other, further improving the student model. The results on the ISRUC-III and MASS-SS3 datasets show that our proposed framework compresses the sleep models effectively with minimal performance loss and achieves the state-of-the-art performance compared to the baseline methods. | Ziyu Jia, Haichao Wang, Yucheng Liu, Tianzi Jiang | University of Southern California, Los Angeles, USA; Institute of Automation, Chinese Academy of Science, Beijing, China; Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China |
|  |  [Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain](https://doi.org/10.1145/3637528.3672069) |  | 0 | Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. Our method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy. Code available at: https://github.com/aminK8/Masked-LoGoNet. | Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath | Department of Ophthalmology and Visual Science, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [Fast and Accurate Domain Adaptation for Irregular Tensor Decomposition](https://doi.org/10.1145/3637528.3671670) |  | 0 | Given an irregular tensor from a newly emerging domain, how can we quickly and accurately capture its patterns utilizing existing irregular tensors in multiple domains? The problem is of great importance for various tasks such as finding patterns of a new disease using pre-existing diseases data. This is challenging as new target tensors have limited information due to their recent emergence. Thus, carefully utilizing the existing source tensors for analyzing the target tensor is helpful. PARAFAC2 decomposition is a strong tool for finding the patterns of irregular tensors, and the patterns are used in many applications such as missing value prediction and anomaly detection. However, previous PARAFAC2-based works cannot adaptably handle newly emerging target tensors utilizing the source tensors. In this work, we propose Meta-P2, a fast and accurate domain adaptation method for irregular tensor decomposition. Meta-P2 generates a meta factor matrix from the multiple source domains, by domain adaptation and meta-update steps. Meta-P2 quickly and accurately finds the patterns of the new irregular tensor utilizing the meta factor matrix. Extensive experiments on real-world datasets show that Meta-P2 achieves the best performance in various downstream tasks including missing value prediction and anomaly detection tasks. | Junghun Kim, Ka Hyun Park, JunGi Jang, U Kang | University of Illinois at Urbana-Champaign, Illinois, IL, USA; Seoul National University, Seoul, Republic of Korea |
|  |  [SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning](https://doi.org/10.1145/3637528.3671845) |  | 0 | To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term ones. Failure in these tasks for a node signals its deviation from the norm. Notably, the neural network and tasks are carefully designed so that all required operations can be performed in constant time (w.r.t. the graph size) in response to each new edge in the input stream. In dynamic anomaly detection across four real-world datasets, SLADE outperforms nine competing methods, even those leveraging label supervision. Our code and datasets are available at https://github.com/jhsk777/SLADE. | Jongha Lee, Sunwoo Kim, Kijung Shin | KAIST, Seoul, Republic of Korea |
|  |  [Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity](https://doi.org/10.1145/3637528.3671835) |  | 0 | Multitask learning is a widely used paradigm for training models on diverse tasks, with applications ranging from graph neural networks to language model fine-tuning. Since tasks may interfere with each other, a key notion for modeling their relationships is task affinity. This includes pairwise task affinity, computed among pairs of tasks, and higher-order affinity, computed among subsets of tasks. Naively computing either of them requires repeatedly training on data pooled from various task combinations, which is computationally intensive. We present a new algorithm Grad-TAG that can estimate task affinities without this repeated training. The key idea of Grad-TAG is to train a "base" model for all tasks and then use a linearization technique to estimate the loss of any other model with a specific task combination. The linearization works by computing a gradient-based first-order approximation of the loss, using low-dimensional projections of gradients as features in a logistic regression trained to predict labels for the specific task combination. We show theoretically that the linearized model can provably approximate the loss when the gradient-based approximation is accurate, and also empirically verify that on several large models. Then, given the estimated task affinity matrix, we design a semi-definite program for clustering to group similar tasks that maximize the average density of clusters. We evaluate Grad-TAG's performance across seven datasets, including multi-label classification on graphs, and instruction fine-tuning of language models. Our results show that our task affinity estimates are within 2.7% distance of the true affinities while needing only 3% of FLOPs compared to full training. On our largest graph with 21M edges and 500 labeling tasks, our algorithm delivers an estimate accurate to within 5% of the true affinities, while using only 112.3 GPU hours. Our results show that Grad-TAG achieves excellent performance and runtime tradeoffs compared to existing approaches. | Dongyue Li, Aneesh Sharma, Hongyang R. Zhang | Google, Mountain View, USA; Northeastern University, Boston, USA |
|  |  [Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions](https://doi.org/10.1145/3637528.3671813) |  | 0 | Online advertising platforms leverage a two-stage auction architecture to deliver personalized ads to users with low latency. The first stage efficiently selects a small subset of promising candidates out of the complete pool of ads. In the second stage, an auction is conducted within the subset to determine the winning ad for display, using click-through-rate predictions from the second-stage machine learning model. In this work, we investigate the online learning process of the first-stage subset selection policy, while ensuring game-theoretic properties in repeated two-stage ad auctions. Specifically, we model the problem as designing a combinatorial bandit mechanism with a general reward function, as well as additional requirements of truthfulness and individual rationality (IR). We establish an O(T) regret lower bound for truthful bandit mechanisms, which demonstrates the challenge of simultaneously achieving allocation efficiency and truthfulness. To circumvent this impossibility result, we introduce truthful α-approximation oracles and evaluate the bandit mechanism through α-approximation regret. Two mechanisms are proposed, both of which are ex-post truthful and ex-post IR. The first mechanism is an explore-then-commit mechanism with regret O(T2/3 ), and the second mechanism achieves an improved O(log T /ΔΦ2) regret where ΔΦ is a distribution-dependent gap, but requires additional assumptions on the oracles and information about the strategic bidders. | Haoming Li, Yumou Liu, Zhenzhe Zheng, Zhilin Zhang, Jian Xu, Fan Wu | Shanghai Jiao Tong University, Shanghai, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Alibaba Group, Beijing, China |
|  |  [Self-Distilled Disentangled Learning for Counterfactual Prediction](https://doi.org/10.1145/3637528.3671782) |  | 0 | The advancements in disentangled representation learning significantlyenhance the accuracy of counterfactual predictions by granting precise controlover instrumental variables, confounders, and adjustable variables. Anappealing method for achieving the independent separation of these factors ismutual information minimization, a task that presents challenges in numerousmachine learning scenarios, especially within high-dimensional spaces. Tocircumvent this challenge, we propose the Self-Distilled Disentanglementframework, referred to as SD^2. Grounded in information theory, it ensurestheoretically sound independent disentangled representations without intricatemutual information estimator designs for high-dimensional representations. Ourcomprehensive experiments, conducted on both synthetic and real-world datasets,confirms the effectiveness of our approach in facilitating counterfactualinference in the presence of both observed and unobserved confounders. | Xinshu Li, Mingming Gong, Lina Yao | The University of Melbourne & MBZUAI, Melbourne, Australia; CSIRO's Data61 & The University of New South Wales, Sydney, Australia; The University of New South Wales, Sydney, Australia |
|  |  [Predicting Long-term Dynamics of Complex Networks via Identifying Skeleton in Hyperbolic Space](https://doi.org/10.1145/3637528.3671968) |  | 0 | Learning complex network dynamics is fundamental for understanding, modeling, and controlling real-world complex systems. Though great efforts have been made to predict the future states of nodes on networks, the capability of capturing long-term dynamics remains largely limited. This is because they overlook the fact that long-term dynamics in complex network are predominantly governed by their inherent low-dimensional manifolds, i.e., skeletons. Therefore, we propose the Dynamics-Invariant Skeleton Neural Network (DiskNet), which identifies skeletons of complex networks based on the renormalization group structure in hyperbolic space to preserve both topological and dynamics properties. Specifically, we first condense complex networks with various dynamics into simple skeletons through physics-informed hyperbolic embeddings. Further, we design graph neural ordinary differential equations to capture the condensed dynamics on the skeletons. Finally, we recover the skeleton networks and dynamics to the original ones using a degree-based super-resolution module. Extensive experiments across three representative dynamics as well as five real-world and two synthetic networks demonstrate the superior performances of the proposed DiskNet, which outperforms the state-of-the-art baselines by an average of 10.18% in terms of long-term prediction accuracy. Code for reproduction is available at: https://github.com/tsinghua-fib-lab/DiskNet. | Ruikun Li, Huandong Wang, Jinghua Piao, Qingmin Liao, Yong Li | Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering BNRist, Tsinghua University, Beijing, China |
|  |  [Image Similarity Using an Ensemble of Context-Sensitive Models](https://doi.org/10.1145/3637528.3672004) |  | 0 | Image similarity has been extensively studied in computer vision. In recent years, machine-learned models have shown their ability to encode more semantics than traditional multivariate metrics. However, in labelling semantic similarity, assigning a numerical score to a pair of images is impractical, making the improvement and comparisons on the task difficult. In this work, we present a more intuitive approach to build and compare image similarity models based on labelled data in the form of A:R vs B:R, i.e., determining if an image A is closer to a reference image R than another image B. We address the challenges of sparse sampling in the image space (R, A, B) and biases in the models trained with context-based data by using an ensemble model. Our testing results show that the ensemble model constructed performs ~5% better than the best individual context-sensitive models. They also performed better than the models that were directly fine-tuned using mixed imagery data as well as existing deep embeddings, e.g., CLIP [30] and DINO [3]. This work demonstrates that context-based labelling and model training can be effective when an appropriate ensemble approach is used to alleviate the limitation due to sparse sampling. | Zukang Liao, Min Chen | University of Oxford, Oxford, United Kingdom |
|  |  [Neural Collapse Inspired Debiased Representation Learning for Min-max Fairness](https://doi.org/10.1145/3637528.3671902) |  | 0 | Although machine learning algorithms demonstrate impressive performance, their trustworthiness remains a critical issue, particularly concerning fairness when implemented in real-world applications. Many notions of group fairness aim to minimize disparities in performance across protected groups. However, it can inadvertently reduce performance in certain groups, leading to sub-optimal outcomes. In contrast, Min-max group fairness notion prioritizes the improvement for the worst-performing group, thereby advocating a utility-promoting approach to fairness. However, it has been proven that existing efforts to achieve Min-max fairness exhibit limited effectiveness. In response to this challenge, we leverage the recently proposed "Neural Collapse'' framework to re-examine Empirical Risk Minimization (ERM) training, specifically investigating the root causes of poor performance in minority groups. The layer-peeled model is employed to decompose a network into two parts: an encoder to learn latent representation, and a subsequent classifier, with a systematic characterization of their training behaviors being conducted. Our analysis reveals that while classifiers achieve maximum separation, the separability of representations is insufficient, particularly for minority groups. This indicates the sub-optimal performance in minority groups stems from less separable representations, rather than classifiers. To tackle this issue, we introduce a novel strategy that incorporates a frozen classifier to directly enhance representation. Furthermore, we introduce two easily implemented loss functions to guide the learning process. The experimental assessments carried out on real-world benchmark datasets spanning the domains of Computer Vision, Natural Language Processing, and Tabular data demonstrate that our approach outperforms existing state-of-the-art methods in promoting the Min-max fairness notion. | Shenyu Lu, Junyi Chai, Xiaoqian Wang | Purdue University, West Lafayette, IN, USA |
|  |  [AdaGMLP: AdaBoosting GNN-to-MLP Knowledge Distillation](https://doi.org/10.1145/3637528.3671699) |  | 0 | Graph Neural Networks (GNNs) have revolutionized graph-based machinelearning, but their heavy computational demands pose challenges forlatency-sensitive edge devices in practical industrial applications. Inresponse, a new wave of methods, collectively known as GNN-to-MLP KnowledgeDistillation, has emerged. They aim to transfer GNN-learned knowledge to a moreefficient MLP student, which offers faster, resource-efficient inference whilemaintaining competitive performance compared to GNNs. However, these methodsface significant challenges in situations with insufficient training data andincomplete test data, limiting their applicability in real-world applications.To address these challenges, we propose AdaGMLP, an AdaBoosting GNN-to-MLPKnowledge Distillation framework. It leverages an ensemble of diverse MLPstudents trained on different subsets of labeled nodes, addressing the issue ofinsufficient training data. Additionally, it incorporates a Node Alignmenttechnique for robust predictions on test data with missing or incompletefeatures. Our experiments on seven benchmark datasets with different settingsdemonstrate that AdaGMLP outperforms existing G2M methods, making it suitablefor a wide range of latency-sensitive real-world applications. We havesubmitted our code to the GitHub repository(https://github.com/WeigangLu/AdaGMLP-KDD24). | Weigang Lu, Ziyu Guan, Wei Zhao, Yaming Yang | Xidian University, Xi'an, Shaanxi, China; Xidian University, Xi'an, Shannxi, China |
|  |  [Handling Varied Objectives by Online Decision Making](https://doi.org/10.1145/3637528.3671812) |  | 0 | Conventional machine learning typically assume a fixed learning objective throughout the learning process. However, for real-world tasks in open and dynamic environments, objectives can change frequently. For example, in autonomous driving, a car has several default modes, but a user's concern for speed and fuel consumption varies depending on road conditions and personal needs. We formulate this problem as learning with varied objectives (LVO), where the goal is to optimize a dynamic weighted combination of multiple sub-objectives by sequentially selecting actions that incur different losses on these sub-objectives. We propose the VaRons algorithm, which estimates the action-wise performance on each sub-objective and adaptively selects decisions according to the dynamic requirements on different sub-objectives. Further, we extend our approach to cases involving contextual representations and propose the ConVaRons algorithm, assuming parameterized linear structure that links contextual features to the main objective. Both the VaRons and ConVaRons are provably minimax optimal with respect to the time horizon T, with ConVaRons showing better dependency with the number of sub-objectives K. Experiments on dynamic classifier and real-world cluster service allocation tasks validate the effectiveness of our methods and support our theoretical findings. | Lanjihong Ma, ZhenYu Zhang, YaoXiang Ding, ZhiHua Zhou | State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China; Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan |
|  |  [Quantifying and Estimating the Predictability Upper Bound of Univariate Numeric Time Series](https://doi.org/10.1145/3637528.3671995) |  | 0 | The intrinsic predictability of a given time series indicates how well an (ideal) algorithm could potentially predict it when trained on the time series data. Being able to compute the intrinsic predictability helps the developers of prediction algorithms immensely in deciding whether there is further optimization potential, as it tells them how close they are to what is (theoretically) achievable. We call the intrinsic predictability the predictability upper bound ¶imax and propose a novel method for quantifying and estimating it for univariate numeric time series. So far, this has only been done for symbolic time series, even though most real-world time series are numeric by nature. We base our technique on the close relationship between entropy and predictability, utilizing the entropy rate of a time series to compute ¶imax . Since existing entropy rate estimators, such as those based on the Lempel-Ziv compression algorithm, only work for symbolic data, we develop new estimators using tolerance thresholds for matching numeric values. We demonstrate that ¶imax is an effective upper bound that characterizes the intrinsic predictability of a time series. We give formal proofs and we validate our arguments experimentally by comparing ¶imax with the prediction accuracy of different state-of-the-art models on various real-world datasets from different domains. | Jamal Mohammed, Michael H. Böhlen, Sven Helmer | University of Zurich, Zurich, Switzerland |
|  |  [Scalable Rule Lists Learning with Sampling](https://doi.org/10.1145/3637528.3671989) |  | 0 | Learning interpretable models has become a major focus of machine learningresearch, given the increasing prominence of machine learning in sociallyimportant decision-making. Among interpretable models, rule lists are among thebest-known and easily interpretable ones. However, finding optimal rule listsis computationally challenging, and current approaches are impractical forlarge datasets. We present a novel and scalable approach to learn nearly optimal rule listsfrom large datasets. Our algorithm uses sampling to efficiently obtain anapproximation of the optimal rule list with rigorous guarantees on the qualityof the approximation. In particular, our algorithm guarantees to find a rulelist with accuracy very close to the optimal rule list when a rule list withhigh accuracy exists. Our algorithm builds on the VC-dimension of rule lists,for which we prove novel upper and lower bounds. Our experimental evaluation onlarge datasets shows that our algorithm identifies nearly optimal rule listswith a speed-up up to two orders of magnitude over state-of-the-art exactapproaches. Moreover, our algorithm is as fast as, and sometimes faster than,recent heuristic approaches, while reporting higher quality rule lists. Inaddition, the rules reported by our algorithm are more similar to the rules inthe optimal rule list than the rules from heuristic approaches. | Leonardo Pellegrina, Fabio Vandin | Dept. of Information Engineering, University of Padova, Padova, Italy |
|  |  [Fredformer: Frequency Debiased Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671928) |  | 0 | The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertake empirical analyses to understand this bias and discover that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer | Xihao Piao, Zheng Chen, Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai | SANKEN, Osaka University, Osaka, Japan |
|  |  [ORCDF: An Oversmoothing-Resistant Cognitive Diagnosis Framework for Student Learning in Online Education Systems](https://doi.org/10.1145/3637528.3671988) |  | 0 | Cognitive diagnosis models (CDMs) are designed to learn students' mastery levels using their response logs. CDMs play a fundamental role in online education systems since they significantly influence downstream applications such as teachers' guidance and computerized adaptive testing. Despite the success achieved by existing CDMs, we find that they suffer from a thorny issue that the learned students' mastery levels are too similar. This issue, which we refer to as oversmoothing, could diminish the CDMs' effectiveness in downstream tasks. CDMs comprise two core parts: learning students' mastery levels and assessing mastery levels by fitting the response logs. This paper contends that the oversmoothing issue arises from that existing CDMs seldom utilize response signals on exercises in the learning part but only use them as labels in the assessing part. To this end, this paper proposes an oversmoothing-resistant cognitive diagnosis framework (ORCDF) to enhance existing CDMs by utilizing response signals in the learning part. Specifically, ORCDF introduces a novel response graph to inherently incorporate response signals as types of edges. Then, ORCDF designs a tailored response-aware graph convolution network (RGC) that effectively captures the crucial response signals within the response graph. Via ORCDF, existing CDMs are enhanced by replacing the input embeddings with the outcome of RGC, allowing for the consideration of response signals on exercises in the learning part. Extensive experiments on real-world datasets show that ORCDF not only helps existing CDMs alleviate the oversmoothing issue but also significantly enhances the models' prediction and interpretability performance. Moreover, the effectiveness of ORCDF is validated in the downstream task of computerized adaptive testing. | Hong Qian, Shuo Liu, Mingjia Li, Bingdong Li, Zhi Liu, Aimin Zhou | School of Computer Science and Technology, East China Normal University, Shanghai, China |
|  |  [LARP: Language Audio Relational Pre-training for Cold-Start Playlist Continuation](https://doi.org/10.1145/3637528.3671772) |  | 0 | As online music consumption increasingly shifts towards playlist-based listening, the task of playlist continuation, in which an algorithm suggests songs to extend a playlist in a personalized and musically cohesive manner, has become vital to the success of music streaming services. Currently, many existing playlist continuation approaches rely on collaborative filtering methods to perform their recommendations. However, such methods will struggle to recommend songs that lack interaction data, an issue known as the cold-start problem. Current approaches to this challenge design complex mechanisms for extracting relational signals from sparse collaborative signals and integrating them into content representations. However, these approaches leave content representation learning out of scope and utilize frozen, pre-trained content models that may not be aligned with the distribution or format of a specific musical setting. Furthermore, even the musical state-of-the-art content modules are either (1) incompatible with the cold-start setting or (2) unable to effectively integrate cross-modal and relational signals. In this paper, we introduce LARP, a multi-modal cold-start playlist continuation model, to effectively overcome these limitations. LARP is a three-stage contrastive learning framework that integrates both multi-modal and relational signals into its learned representations. Our framework uses increasing stages of task-specific abstraction: within-track (language-audio) contrastive loss, track-track contrastive loss, and track-playlist contrastive loss. Experimental results on two publicly available datasets demonstrate the efficacy of LARP over uni-modal and multi-modal models for playlist continuation in a cold-start setting. Finally, this work pioneers the perspective of addressing cold-start recommendation via relational representation learning. Code and dataset are released at: https://github.com/Rsalganik1123/LARP/ | Rebecca Salganik, Xiaohao Liu, Yunshan Ma, Jian Kang, TatSeng Chua | University of Rochester, Rochester, NY, USA; National University of Singapore, Singapore, Singapore |
|  |  [CrossLight: Offline-to-Online Reinforcement Learning for Cross-City Traffic Signal Control](https://doi.org/10.1145/3637528.3671927) |  | 0 | The recent advancements in Traffic Signal Control (TSC) have highlighted the potential of Reinforcement Learning (RL) as a promising solution to alleviate traffic congestion. Current research in this area primarily concentrates on either online or offline learning strategies, aiming to create optimized policies for specific cities. Nevertheless, the transferability of these policies to new cities is impeded by constraints such as the limited availability of high-quality data and the expensive and risky exploration process. To this end, in this paper, we present an innovative cross-city Traffic Signal Control (TSC) paradigm called CrossLight. Our approach involves meta training using offline data from source cities and adaptively fine-tuning in the target city. This novel methodology aims to address the challenges of transferring TSC policies across different cities effectively. In our proposed approach, we start by acquiring meta-decision pattern knowledge through trajectory dynamics reconstruction via pre-training in source cities. To address disparities in road network topologies between cities, we dynamically construct city topological structures based on the extracted meta-knowledge during the offline meta-training phase. These structures are then used to distill pattern-structure aware representations of decision trajectories from the source cities. To identify effective initial parameters for the learnable components, we employ the Model-Agnostic Meta-Learning (MAML) framework, a popular meta-learning approach. During adaptive fine-tuning in the target city, we introduce a replay buffer that is iteratively updated using online interactions with a rank and filter mechanism. This mechanism, along with a carefully designed exploration strategy, ensures a balance between exploitation and exploration, thereby fostering both the diversity and quality of the trajectories for fine-tuning. Finally, extensive experiments across four cities validate that CrossLight achieves comparable performance in new cities with minimal fine-tuning iterations, surpassing both existing online and offline methods. This success underscores that our CrossLight framework emerges as a groundbreaking and potent paradigm, offering a feasible and effective solution to the intelligent transportation community. | Qian Sun, Rui Zha, Le Zhang, Jingbo Zhou, Yu Mei, Zhiling Li, Hui Xiong | Department of Intelligent Transportation System, Baidu Inc., Beijing, China; Baidu Research, Baidu Inc., Beijing, China; School of Computer Science, University of Science and Technology of China, Hefei, China; Department of Intelligent Driving Group Business Management, Baidu Inc., Beijing, China |
|  |  [Going Where, by Whom, and at What Time: Next Location Prediction Considering User Preference and Temporal Regularity](https://doi.org/10.1145/3637528.3671916) |  | 0 | Next location prediction is a crucial task in human mobility modeling, and is pivotal for many downstream applications like location-based recommendation and transportation planning. Although there has been a large body of research tackling this problem, the usefulness of user preference and temporal regularity remains underrepresented. Specifically, previous studies usually neglect the explicit user preference information entailed from human trajectories and fall short in utilizing the arrival time of next location, as a key determinant on next location. To address these limitations, we propose a Multi-Context aware Location Prediction model (MCLP) to predict next locations for individuals, where it explicitly models user preference and the next arrival time as context. First, we utilize a topic model to extract user preferences for different types of locations from historical human trajectories. Second, we develop an arrival time estimator to construct a robust arrival time embedding based on the multi-head attention mechanism. The two components provide pivotal contextual information for the subsequent prediction. Finally, we utilize the Transformer architecture to mine sequential patterns and integrate multiple contextual information to predict the next locations. Experimental results on two real-world mobility datasets show that our proposed MCLP outperforms baseline methods. | Tianao Sun, Ke Fu, Weiming Huang, Kai Zhao, Yongshun Gong, Meng Chen | Robinson College of Business, Georgia State University, Atlanta, GA, USA; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Software, Shandong University, Jinan, China |
|  |  [EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://doi.org/10.1145/3637528.3672068) |  | 0 | Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall value of the data can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as aproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. The code is available at https://github.com/respai-lab/ecoval. | Ayush K. Tarun, Vikram S. Chundawat, Murari Mandal, Hong Ming Tan, Bowei Chen, Mohan S. Kankanhalli | National University of Singapore, Singapore, Singapore; RespAI Lab, Bhubaneswar, India; Adam Smith Business School, University of Glasgow, Glasgow, United Kingdom; NUS Business School, National University of Singapore, Singapore, Singapore; Ola Krutrim, Bangalore, India; RespAI Lab, Kalinga Institute of Industrial Technology, Bhubaneswar, India |
|  |  [Causal Estimation of Exposure Shifts with Neural Networks and an Application to Inform Air Quality Standards in the US](https://doi.org/10.1145/3637528.3671761) |  | 0 | A fundamental task in causal inference is estimating the effect of a distribution shift in the treatment variable. We refer to this problem as shift-response function (SRF) estimation. Existing neural network methods for causal inference lack theoretical guarantees and practical implementations for SRF estimation. In this paper, we introduce Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), a method to estimate SRFs with robustness and efficiency guarantees. Our contributions are twofold. First, we propose a targeted regularization loss for neural networks with theoretical properties that ensure double robustness and asymptotic efficiency specific to SRF estimation. Second, we extend targeted regularization to support loss functions from the exponential family to accommodate non-continuous outcome distributions (e.g., discrete counts). We conduct benchmark experiments demonstrating TRESNET's broad applicability and competitiveness. We then apply our method to a key policy question in public health to estimate the causal effect of revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 μg/m3 to 9 μg/m3. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate the reduction in deaths that would result from this anticipated revision using data consisting of 68 million individuals across the U.S. | Mauricio Tec, Kevin Josey, Oladimeji Mudele, Francesca Dominici | Harvard University, Cambridge, MA, USA; Colorado School of Public Health, Aurora, CO, USA |
|  |  [Online Drift Detection with Maximum Concept Discrepancy](https://doi.org/10.1145/3637528.3672016) |  | 0 | Continuous learning from an immense volume of data streams becomes exceptionally critical in the internet era. However, data streams often do not conform to the same distribution over time, leading to a phenomenon called concept drift. Since a fixed static model is unreliable for inferring concept-drifted data streams, establishing an adaptive mechanism for detecting concept drift is crucial. Current methods for concept drift detection primarily assume that the labels or error rates of downstream models are given and/or underlying statistical properties exist in data streams. These approaches, however, struggle to address high-dimensional data streams with intricate irregular distribution shifts, which are more prevalent in real-world scenarios. In this paper, we propose MCD-DD, a novel concept drift detection method based on maximum concept discrepancy, inspired by the maximum mean discrepancy. Our method can adaptively identify varying forms of concept drift by contrastive learning of concept embeddings without relying on labels or statistical properties. With thorough experiments under synthetic and real-world scenarios, we demonstrate that the proposed method outperforms existing baselines in identifying concept drifts and enables qualitative analysis with high explainability. | Ke Wan, Yi Liang, Susik Yoon | Korea University, Seoul, Republic of Korea; Fudan University, Shanghai, China; University of Illinois at Urbana-Champaign, Urbana, IL, USA |
|  |  [CE-RCFR: Robust Counterfactual Regression for Consensus-Enabled Treatment Effect Estimation](https://doi.org/10.1145/3637528.3672054) |  | 0 | Estimating individual treatment effects (ITE) from observational data is challenging due to the absence of counterfactuals and the treatment selection bias. Prevalent ITE estimation methods tackle these challenges by aligning the treated and controlled distributions in the representational space. However, two critical issues have long been overlooked: (1)Mini-batch sampling sensitivity (MSS) issue, where representation distribution alignment at a mini-batch level is vulnerable to poor sampling cases, such as data imbalance and outliers; (2)Inconsistent representation learning (IRL) issue, where representation learning within a unified backbone network suffers from inconsistent gradient update directions due to the distribution skew between different treatment groups. To resolve these issues, we propose CE-RCFR, a Robust CounterFactual Regression framework for Consensus-Enabled causal effect estimation, including a relaxed distribution discrepancy regularizer (RDDR) module and a consensus-enabled aggregator (CEA) module. Specifically, for the robust representation alignment perspective, RDDR addresses the MSS issue by minimizing unbalanced optimal transport divergence between different treatment groups with a relaxed marginal constraint. For the accurate representation optimization perspective, CEA addresses the IRL issue by resolving the consistent gradient update directions on shared parameters within the backbone network. Extensive experiments demonstrate that CE-RCFR significantly outperforms the state-of-the-art methods in treatment effect estimations. | Fan Wang, Chaochao Chen, Weiming Liu, Tianhao Fan, Xinting Liao, Yanchao Tan, Lianyong Qi, Xiaolin Zheng | College of Computer and Data ScienceCollege of Software, Fuzhou University, Fuzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China |
|  |  [Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks](https://doi.org/10.1145/3637528.3671776) |  | 0 | Recently, emergence has received widespread attention from the research community along with the success of large-scale models. Different from the literature, we hypothesize a key factor that promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we first propose a new metric to measure the monosemanticity of neurons with the guarantee of efficiency for online computation, then introduce a theoretically supported method to suppress monosemantic neurons and proactively promote the ratios of polysemantic neurons in training neural networks. We validate our conjecture that monosemanticity brings about performance change at different model scales on a variety of neural networks and benchmark datasets in different areas, including language, image, and physics simulation tasks. Further experiments validate our analysis and theory regarding the inhibition of monosemanticity. | Jiachuan Wang, Shimin Di, Lei Chen, Charles Wang Wai Ng | The Hong Kong University of Science and Technology, (Guangzhou), Guangzhou, China; The Hong Kong University of Science and Technology, Hong Kong SAR, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning](https://doi.org/10.1145/3637528.3671721) |  | 0 | Time series domain adaptation stands as a pivotal and intricate challengewith diverse applications, including but not limited to human activityrecognition, sleep stage classification, and machine fault diagnosis. Despitethe numerous domain adaptation techniques proposed to tackle this complexproblem, they primarily focus on domain adaptation from a single source domain.Yet, it is more crucial to investigate domain adaptation from multiple domainsdue to the potential for greater improvements. To address this, three importantchallenges need to be overcome: 1). The lack of exploration to utilizedomain-specific information for domain adaptation, 2). The difficulty to learndomain-specific information that changes over time, and 3). The difficulty toevaluate learned domain-specific information. In order to tackle thesechallenges simultaneously, in this paper, we introduce PrOmpt-based domaiNDiscrimination (POND), the first framework to utilize prompts for time seriesdomain adaptation. Specifically, to address Challenge 1, we extend the idea ofprompt tuning to time series analysis and learn prompts to capture common anddomain-specific information from all source domains. To handle Challenge 2, weintroduce a conditional module for each source domain to generate prompts fromtime series input data. For Challenge 3, we propose two criteria to select goodprompts, which are used to choose the most suitable source domain for domainadaptation. The efficacy and robustness of our proposed POND model areextensively validated through experiments across 50 scenarios encompassing fourdatasets. Experimental results demonstrate that our proposed POND modeloutperforms all state-of-the-art comparison methods by up to 66% on theF1-score. | Junxiang Wang, Guangji Bai, Wei Cheng, Zhengzhang Chen, Liang Zhao, Haifeng Chen | NEC Labs America, Princeton, NJ, USA; Emory University, Atlanta, GA, USA |
|  |  [DPSW-Sketch: A Differentially Private Sketch Framework for Frequency Estimation over Sliding Windows](https://doi.org/10.1145/3637528.3671694) |  | 0 | The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent w items are used for analysis. In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space. When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy. In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., heavy hitters), in the sliding window model. We propose DPSW-Sketch, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t. w. Extensive experiments on five real-world and synthetic datasets show that DPSW-Sketch provides significantly better utility-privacy trade-offs than state-of-the-art methods. | Yiping Wang, Yanhao Wang, Cen Chen | East China Normal University, Shanghai, China |
|  |  [DFGNN: Dual-frequency Graph Neural Network for Sign-aware Feedback](https://doi.org/10.1145/3637528.3671701) |  | 0 | The graph-based recommendation has achieved great success in recent years.However, most existing graph-based recommendations focus on capturing userpreference based on positive edges/feedback, while ignoring negativeedges/feedback (e.g., dislike, low rating) that widely exist in real-worldrecommender systems. How to utilize negative feedback in graph-basedrecommendations still remains underexplored. In this study, we first conducteda comprehensive experimental analysis and found that (1) existing graph neuralnetworks are not well-suited for modeling negative feedback, which acts as ahigh-frequency signal in a user-item graph. (2) The graph-based recommendationsuffers from the representation degeneration problem. Based on the twoobservations, we propose a novel model that models positive and negativefeedback from a frequency filter perspective called Dual-frequency Graph NeuralNetwork for Sign-aware Recommendation (DFGNN). Specifically, in DFGNN, thedesigned dual-frequency graph filter (DGF) captures both low-frequency andhigh-frequency signals that contain positive and negative feedback.Furthermore, the proposed signed graph regularization is applied to maintainthe user/item embedding uniform in the embedding space to alleviate therepresentation degeneration problem. Additionally, we conduct extensiveexperiments on real-world datasets and demonstrate the effectiveness of theproposed model. Codes of our model will be released upon acceptance. | Yiqing Wu, Ruobing Xie, Zhao Zhang, Xu Zhang, Fuzhen Zhuang, Leyu Lin, Zhanhui Kang, Yongjun Xu | Tencent, Beijing, China; Institute of Artificial Intelligence, Beihang University & Zhongguancun Laboratory, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China |
|  |  [Predicting Cascading Failures with a Hyperparametric Diffusion Model](https://doi.org/10.1145/3637528.3672048) |  | 0 | In this paper, we study cascading failures in power grids through the lens ofinformation diffusion models. Similar to the spread of rumors or influence inan online social network, it has been observed that failures (outages) in apower grid can spread contagiously, driven by viral spread mechanisms. Weemploy a stochastic diffusion model that is Markovian (memoryless) and local(the activation of one node, i.e., transmission line, can only be caused by itsneighbors). Our model integrates viral diffusion principles with physics-basedconcepts, by correlating the diffusion weights (contagion probabilities betweentransmission lines) with the hyperparametric Information Cascades (IC) model.We show that this diffusion model can be learned from traces of cascadingfailures, enabling accurate modeling and prediction of failure propagation.This approach facilitates actionable information through well-understood andefficient graph analysis methods and graph diffusion simulations. Furthermore,by leveraging the hyperparametric model, we can predict diffusion and mitigatethe risks of cascading failures even in unseen grid configurations, whereasexisting methods falter due to a lack of training data. Extensive experimentsbased on a benchmark power grid and simulations therein show that our approacheffectively captures the failure diffusion phenomena and guides decisions tostrengthen the grid, reducing the risk of large-scale cascading failures.Additionally, we characterize our model's sample complexity, improving upon theexisting bound. | Bin Xiang, Bogdan Cautis, Xiaokui Xiao, Olga Mula, Dusit Niyato, Laks V. S. Lakshmanan | University of British Columbia, Vancouver, Canada; CNRSCREATE, Singapore, Singapore; Eindhoven University of Technology, Eindhoven, Netherlands; National University of Singapore, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore; University of Paris-Saclay, CNRS LISN, Saclay, France |
|  |  [FRNet: Frequency-based Rotation Network for Long-term Time Series Forecasting](https://doi.org/10.1145/3637528.3671713) |  | 0 | Long-term time series forecasting (LTSF) aims to predict future values for a long time based on historical data. The period term is an essential component of the time series, which is complex yet important for LTSF. Although existing studies have achieved promising results, they still have limitations in modeling dynamic complicated periods. Most studies only focus on static periods with fixed time steps, while very few studies attempt to capture dynamic periods in the time domain. In this paper, we dissect the original time series in time and frequency domains and empirically find that changes in periods are more easily captured and quantified in the frequency domain. Based on this observation, we propose to explore dynamic period features using rotation in the frequency domain. To this end, we develop the frequency-based rotation network (FRNet), a novel LTSF method to effectively capture the features of the dynamic complicated periods. FRNet decomposes the original time series into period and trend components. Based on the complex-valued linear networks, it leverages a period frequency rotation module to predict the period component and a patch frequency rotation module to predict the trend component, respectively. Extensive experiments on seven real-world datasets consistently demonstrate the superiority of FRNet over various state-of-the-art methods. The source code is available at https://github.com/SiriZhang45/FRNet. | Xinyu Zhang, Shanshan Feng, Jianghong Ma, Huiwei Lin, Xutao Li, Yunming Ye, Fan Li, Yew Soon Ong | Hong Kong Polytechnic University, Hong Kong, China; Centre for Frontier AI Research, ASTAR, Nanyang Technological University, Singapore, Singapore; Harbin Institute of Technology, Shenzhen, China |
|  |  [Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space](https://doi.org/10.1145/3637528.3672039) |  | 0 | Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of \method across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models. | Menglin Yang, Harshit Verma, Delvin Ce Zhang, Jiahong Liu, Irwin King, Rex Ying | Yale University, New Haven, CT, USA; The Chinese University of Hong Kong, Hong Kong, China; Birla Institute of Technology and Science, Hyderabad, India |
|  |  [Practical Single Domain Generalization via Training-time and Test-time Learning](https://doi.org/10.1145/3637528.3671806) |  | 0 | Single domain generalization aims to learn a model that generalizes well to unseen target domains by using a related source domain. However, most existing methods only focus on improving the generalization performance of the model during training, making it difficult to achieve satisfactory performance when deployed in the target domain with large domain shifts. In this paper, we propose a Practical Single Domain Generalization (PSDG) method, which first leverages the knowledge in a source domain to establish a model with good generalization ability in the training phase, and subsequently updates the model to adapt to target domain data using knowledge in the unlabeled target domain during the testing phase. Specifically, during training, PSDG leverages a newly proposed style (e.g., background features) generator named StyIN to generate novel domain data. Moreover, PSDG introduces style-diversity regularization to constantly synthesize distinct styles to expand the coverage of training data, and introduces object-consistency regularization to capture consistency between the currently generated data and the original data, making the model filter style knowledge during training. During testing, PSDG uses a sample-aware and sharpness-aware minimization method to seek for a flat entropy minimum surface for further model optimization by using the knowledge in the unlabeled target domain. Using three real-world datasets the experiments have demonstrated the effectiveness of PSDG, in comparison with several state-of-the-art methods. | Shuai Yang, Zhen Zhang, Lichuan Gu |  |
|  |  [Rethinking Order Dispatching in Online Ride-Hailing Platforms](https://doi.org/10.1145/3637528.3672028) |  | 0 | Achieving optimal order dispatching has been a long-standing challenge for online ride-hailing platforms. Early methods would make shortsighted matchings as they only consider order prices alone as the edge weights in the driver-order bipartite graph, thus harming the platform's revenue. To address this problem, recent works evaluate the value of the order's destination region to be the long-term income a driver could obtain in average in such region and incorporate it into the order's edge weight to influence the matching results. However, they often result in insufficient driver supplies in many regions, as the values evaluated in different regions vary greatly, mainly because the impact of one region's value on the future number of drivers and revenue in other regions is overlooked. This paper models such impact within a cooperative Markov game, which involves each value's impact over the platform's revenue with the goal to find the optimal region values for revenue maximization. To solve this game, our work proposes a novelgoal-reaching collaboration (GRC) algorithm that realizes credit assignment from a novel goal-reaching perspective, addressing the difficulty for accurate credit assignment with large-scale agents of previous methods and resolving the conflict between credit assignment and offline reinforcement learning. Specifically, during training, GRC predicts the city's future state through an environment model and utilizes a scoring model to rate the predicted states to judge their levels of profitability, where high-scoring states are regarded as the goal states. Then, the policies in the game are updated to promote the city to stay in the goal states for as long as possible. To evaluate GRC, we deploy a baseline policy online in several cities for three weeks to collect real-world dataset. Training and testing results on the collected dataset indicate that our GRC consistently outperforms the baselines in different cities and peak periods. | Zhaoxing Yang, Haiming Jin, Guiyun Fan, Min Lu, Yiran Liu, Xinlang Yue, Hao Pan, Zhe Xu, Guobin Wu, Qun Li, Xiaotong Wang, Jiecheng Guo | Shanghai Jiao Tong University, Shanghai, China; Didi Chuxing, Beijing, China |
|  |  [BoKA: Bayesian Optimization based Knowledge Amalgamation for Multi-unknown-domain Text Classification](https://doi.org/10.1145/3637528.3671963) |  | 0 | With breakthroughs in pretrained language models, a large number of finetuned models specialized in distinct domains have surfaced online. Yet, when faced with a fresh dataset covering multiple (sub)domains, their performance might degrade. Reusing these available finetuned models to train a new model is a more feasible solution than the finetuning method that demands extensive manual labeling. Knowledge Amalgamation (KA) is such a model reusing technique, which derives a new model (termed student model) by amalgamating those trained models (termed teacher models) tailored for distinct domains, bypassing the need for manual labeling. However, when the domains of text samples are unknown, selecting a number of appropriate teacher models (simply called a combination) for reuse becomes complicated. To learn an accurate student model, the classical KA method resorts to manual selections, a process both tedious and inefficient. Our study pioneers the automation of this combination selection process for KA in the fundamental text classification task, an area previously unexplored. In this paper, we introduce BoKA : an automatic knowledge amalgamation framework for identifying a combination that can learn a superior student model without human labor. Through the lens of Bayesian optimization, BoKA iteratively samples a subset of possible combinations for amalgamation instead of manual selections. Furthermore, we introduce a novel KA method tailored for text classification, which guides the student model using both soft and pseudo-hard labels from the teacher models when their predictions are closely aligned; in cases of significant disagreement, it uses randomly generated labels. Experiments on two public multi-domain datasets show that BoKA achieves remarkable efficiency by sampling only up to 5.5% of all potential combinations. Moreover, BoKA is capable of matching or even surpassing leading zero-shot large language models, despite having dozens of times fewer parameters. | Linzhu Yu, Huan Li, Ke Chen, Lidan Shou | The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China |
|  |  [Diverse Intra- and Inter-Domain Activity Style Fusion for Cross-Person Generalization in Activity Recognition](https://doi.org/10.1145/3637528.3671828) |  | 0 | Existing domain generalization (DG) methods for cross-person generalization tasks often face challenges in capturing intra- and inter-domain style diversity, resulting in domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity. In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random styles to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible permutations and combinations among existing styles to generate a broad spectrum of new style instances. Empirical evaluations on a broad range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have proven to be significant and valuable, contributing to varying degrees of performance enhancements. Notably, our approach outperforms state-of-the-art DG methods in all human activity recognition tasks. | Junru Zhang, Lang Feng, Zhidan Liu, Yuhan Wu, Yang He, Yabo Dong, Duanqing Xu | Zhejiang University, Hangzhou, China; Shenzhen University, Shenzhen, China; Zhejiang University, Hangzhou, Zhejiang, China |
|  |  [Knowledge Distillation with Perturbed Loss: From a Vanilla Teacher to a Proxy Teacher](https://doi.org/10.1145/3637528.3671851) |  | 0 | Knowledge distillation is a popular technique to transfer knowledge from a large teacher model to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between this "distribution closeness'' and the student model generalizability, which enables us to select the PTLoss's perturbation coefficients in a principled way. Extensive experiments on six public benchmark datasets demonstrate the effectiveness of PTLoss with teachers of different scales. | Rongzhi Zhang, Jiaming Shen, Tianqi Liu, Jialu Liu, Michael Bendersky, Marc Najork, Chao Zhang | Google, New York City, NY, USA; Georgia Institute of Technology, Atlanta, GA, USA |
|  |  [Joint Auction in the Online Advertising Market](https://doi.org/10.1145/3637528.3671746) |  | 0 | Online advertising is a primary source of income for e-commerce platforms. In the current advertising pattern, the oriented targets are the online store owners who are willing to pay extra fees to enhance the position of their stores. On the other hand, brand suppliers are also desirable to advertise their products in stores to boost brand sales. However, the currently used advertising mode cannot satisfy the demand of both stores and brand suppliers simultaneously. To address this, we innovatively propose a joint advertising model termed ''Joint Auction'', allowing brand suppliers and stores to collaboratively bid for advertising slots, catering to both their needs. However, conventional advertising auction mechanisms are not suitable for this novel scenario. In this paper, we propose JRegNet, a neural network architecture for the optimal joint auction design, to generate mechanisms that can achieve the optimal revenue and guarantee (near-)dominant strategy incentive compatibility and individual rationality. Finally, multiple experiments are conducted on synthetic and real data to demonstrate that our proposed joint auction significantly improves platform's revenue compared to the known baselines. | Zhen Zhang, Weian Li, Yahui Lei, Bingzhe Wang, Zhicheng Zhang, Qi Qi, Qiang Liu, Xingxing Wang | School of Software, Shandong University, Jinan, China; Meituan Inc., Beijing, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Long-Term Vessel Trajectory Imputation with Physics-Guided Diffusion Probabilistic Model](https://doi.org/10.1145/3637528.3672086) |  | 0 | Maritime traffic management increasingly relies on vessel position information provided by terrestrial and satellite networks of the Automatic Identification System (AIS). Unfortunately, the problem of missing AIS data can lead to long-term gaps in vessel trajectory, raising corresponding security concerns regarding collision risks and illicit activities. Existing imputation approaches are often constrained by vehicle-based low-sampling trajectories, hindering their ability to address unique characteristics of maritime transportation systems and long-term missing scenarios. To tackle these challenges, we propose a novel generative framework for long-term vessel trajectory imputation. Our framework considers irregular tracks of vessels, which differ from those of cars due to the absence of a structured road network, and ensures the continuity of multi-point imputed trajectories. Specifically, we first utilize a pre-trained trajectory embedding block to capture patterns of vessel movements. Subsequently, we introduce a diffusion-based model for generating missing trajectories, where observed trajectory modeling with transformer encoding architecture and embeddings of both historical vessel trajectory and external factors serve as conditional information. In particular, we design a physics-guided discriminator in the training stage, which imposes kinematic constraints between locations and angles to improve the continuity of the imputed trajectories. Comprehensive experiments and analysis on a real-world AIS dataset confirm the effectiveness of our proposed approach. | Zhiwen Zhang, Zipei Fan, Zewu Lv, Xuan Song, Ryosuke Shibasaki | Research & Development Department, LocationMind Inc., Tokyo, Japan; School of Artificial Intelligence, Jilin University, Changchun, China |
|  |  [All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining](https://doi.org/10.1145/3637528.3671913) |  | 0 | Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term \`All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term \`One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model. | Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, Jia Li |  |
|  |  [Multi-source Unsupervised Domain Adaptation on Graphs with Transferability Modeling](https://doi.org/10.1145/3637528.3671829) |  | 0 | In this paper, we tackle a new problem ofmulti-source unsupervised domain adaptation (MSUDA) for graphs, where models trained on annotated source domains need to be transferred to the unsupervised target graph for node classification. Due to the discrepancy in distribution across domains, the key challenge is how to select good source instances and how to adapt the model. Diverse graph structures further complicate this problem, rendering previous MSUDA approaches less effective. In this work, we present the framework Selective Multi-source Adaptation for Graph (SelMAG ), with a graph-modeling-based domain selector, a sub-graph node selector, and a bi-level alignment objective for the adaptation. Concretely, to facilitate the identification of informative source data, the similarity across graphs is disentangled and measured with the transferability of a graph-modeling task set, and we use it as evidence for source domain selection. A node selector is further incorporated to capture the variation in transferability of nodes within the same source domain. To learn invariant features for adaptation, we align the target domain to selected source data both at the embedding space by minimizing the optimal transport distance and at the classification level by distilling the label function. Modules are explicitly learned to select informative source data and conduct the alignment in virtual training splits with a meta-learning strategy. Experimental results on five graph datasets show the effectiveness of the proposed method. | Tianxiang Zhao, Dongsheng Luo, Xiang Zhang, Suhang Wang | Florida International University, Miami, USA; The Pennsylvania State University, State College, USA |
|  |  [Bridging and Compressing Feature and Semantic Spaces for Robust Graph Neural Networks: An Information Theory Perspective](https://doi.org/10.1145/3637528.3671870) |  | 0 | The emerging Graph Convolutional Networks (GCNs) have attracted widespread attention in graph learning, due to their good ability of aggregating the information between higher-order neighbors. However, real-world graph data contains high noise and redundancy, making it hard for GCNs to accurately depict the complete relationships between nodes, which seriously degrades the quality of graph representations. Moreover, existing studies commonly ignore the distribution difference between feature and semantic spaces in graphs, causing inferior model generalization. To address these challenges, we propose DIB-RGCN, a novel robust GCN framework, to explore the optimal graph representation with the guidance of the well-designed dual information bottleneck principle. First, we analyze the reasons for distribution differences and theoretically prove that minimal sufficient representations in specific spaces cannot promise optimal performance for downstream tasks. Next, we design new dual channels to regularize feature and semantic spaces, eliminating the sharing of task-irrelevant information between spaces. Different from existing denoising algorithms that adopt a random dropping manner, we innovatively replace potential noisy features and edges with local neighboring representations. This design lowers edge-specific coefficient assignment, alleviating the interference of original representations while retaining graph structures. Further, we maximize the sharing of task-relevant information between feature and semantic spaces to alleviate the difference between them. Using real-world datasets, extensive experiments demonstrate the robustness of the proposed DIB-RGCN, which outperforms state-of-the-art methods on classification tasks. | Luying Zhong, Renjie Lin, Jiayin Li, Shiping Wang, Zheyi Chen | Fujian Normal University, Fuzhou, China; Fuzhou University, Fuzhou, China |
|  |  [Dynamic Hotel Pricing at Online Travel Platforms: A Popularity and Competitiveness Aware Demand Learning Approach](https://doi.org/10.1145/3637528.3671921) |  | 0 | Dynamic pricing, which suggests the optimal prices based on the dynamic demands, has received considerable attention in academia and industry. On online hotel booking platforms, room demand fluctuates due to various factors, notably hotel popularity and competition. In this paper, we propose a dynamic pricing approach with popularity and competitiveness-aware demand learning. Specifically, we introduce a novel demand function that incorporates popularity and competitiveness coefficients to comprehensively model the price elasticity of demand. We develop a dynamic demand prediction network that focuses on learning these coefficients in the proposed demand function, enhancing the interpretability and accuracy of price suggestion. The model is trained in a multi-task framework that effectively leverages the correlations of demands among groups of similar hotels to alleviate data sparseness in room-level occupancy prediction. Comprehensive experiments conducted on real-world datasets validate the superiority of our method over state-of-the-art baselines in both demand prediction and dynamic pricing. Our model has been successfully deployed on a popular online travel platform, serving tens of millions of users and hoteliers. | Fanwei Zhu, Wendong Xiao, Yao Yu, Zemin Liu, Zulong Chen, Weibin Cai | Zhejiang University, Hangzhou, China; Hangzhou City University, Hangzhou, China; Syracuse University, Syracuse, USA; Alibaba Group, Hangzhou, China |
|  |  [Repeat-Aware Neighbor Sampling for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672001) |  | 0 | Dynamic graph learning equips the edges with time attributes and allowsmultiple links between two nodes, which is a crucial technology forunderstanding evolving data scenarios like traffic prediction andrecommendation systems. Existing works obtain the evolving patterns mainlydepending on the most recent neighbor sequences. However, we argue that whethertwo nodes will have interaction with each other in the future is highlycorrelated with the same interaction that happened in the past. Onlyconsidering the recent neighbors overlooks the phenomenon of repeat behaviorand fails to accurately capture the temporal evolution of interactions. To fillthis gap, this paper presents RepeatMixer, which considers evolving patterns offirst and high-order repeat behavior in the neighbor sampling strategy andtemporal information learning. Firstly, we define the first-order repeat-awarenodes of the source node as the destination nodes that have interactedhistorically and extend this concept to high orders as nodes in the destinationnode's high-order neighbors. Then, we extract neighbors of the source node thatinteracted before the appearance of repeat-aware nodes with a slide windowstrategy as its neighbor sequence. Next, we leverage both the first andhigh-order neighbor sequences of source and destination nodes to learn temporalpatterns of interactions via an MLP-based encoder. Furthermore, considering thevarying temporal patterns on different orders, we introduce a time-awareaggregation mechanism that adaptively aggregates the temporal representationsfrom different orders based on the significance of their interaction timesequences. Experimental results demonstrate the superiority of RepeatMixer overstate-of-the-art models in link prediction tasks, underscoring theeffectiveness of the proposed repeat-aware neighbor sampling strategy. | Tao Zou, Yuhao Mao, Junchen Ye, Bowen Du | CCSE Lab, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China |
|  |  [Machine Learning for Clinical Management: From the Lab to the Hospital](https://doi.org/10.1145/3637528.3672497) |  | 0 | Population aging, increasing social demands, and rising costs of treatments are stressing healthcare systems to the point of risking the sustainability of universal and accessible healthcare. A hope in this dismal panorama is that there are large inefficiencies, and so opportunities for getting more from the same resources. To name a few, avoidable hospitalizations, unnecessary medication and tests, and lack of coordination among healthcare agents are estimated to cost several hundred billion euros per year in the EU. Technology can be useful for locating and reducing these inefficiencies, and within technology, the full exploitation of the data that the system collects to record its activity. In this talk, I will review the case for activity data analytics in healthcare, with two main considerations: 1) The need to include information about resources and costs in the models, in addition to clinical knowledge and patient outcomes, and 2) the need to use mostly data that healthcare organizations already collect and is not locked and distributed in silos. Fortunately, data collected for administrative and billing purposes, even though imperfect, partial, and low resolution, can be used to improve efficiency and safety, as well as fairness and equity. I will focus on the work carried out at Amalfi Analytics, a spin-off of my research group at UPC in Barcelona. On the one hand, we have addressed predictive management in hospitals, from influx to the emergency room to availability of surgical areas, beds, and staff. Anticipating activity, needs, and resource availability lets managers improve critical KPIs, e.g. waiting times, but also reduce staff stress, which leads to fewer medical errors and accidents. On the other hand, we have developed a patient cohort analyzer, based mostly on a recent clustering algorithm, that gives experts a fresh view of their patient population and lets them refine protocols and identify high-risk patient groups. This tool has also been used to support territorial planning and resource allocation. These problems have been extensively addressed in the past, but actual penetration of solutions in hospitals is smaller than one could expect. For example, one can find hundreds of papers on predicting influx to emergency rooms or bed demands, but many of them conclude after producing an AUC figure, and even fewer describe a working system that can be exported from the hospital where they were developed to others at an affordable cost. I will describe the approach taken at Amalfi so that hospitals can have such a solution up and running in a few days of work for their IT departments, in what I think is an interesting combination of software engineering and automatic Machine Learning. | Ricard Gavaldà | Amalfi Analytics & Universitat Politècnica de Catalunya, BarcelonaTech (on leave), Barcelona, Spain |
|  |  [Metric Decomposition in A/B Tests](https://doi.org/10.1145/3637528.3671556) |  | 0 | More than a decade ago, CUPED (Controlled Experiments Utilizing Pre-Experiment Data) mainstreamed the idea of variance reduction leveraging pre-experiment covariates. Since its introduction, it has been implemented, extended, and modernized by major online experimentation platforms. Despite the wide adoption, it is known by practitioners that the variance reduction rate from CUPED utilizing pre-experimental data varies case by case and has a theoretical limit. In theory, CUPED can be extended to augment a treatment effect estimator utilizing in-experiment data, but practical guidance on how to construct such an augmentation is lacking. In this article, we fill this gap by proposing a new direction for sensitivity improvement via treatment effect augmentation whereby a target metric of interest is decomposed into components with high signal-to-noise disparity. Inference in the context of this decomposition is developed using both frequentist and Bayesian theory. We provide three real world applications demonstrating different flavors of metric decomposition; these applications illustrate the gain in agility metric decomposition yields relative to an un-decomposed analysis. | Alex Deng, Luke Hagar, Nathaniel T. Stevens, Tatiana Xifara, Amit Gandhi | University of Waterloo, Waterloo, ON, Canada; Airbnb, San Francisco, CA, USA; University of Pennsylvania, Philadelphia, PA, USA; Airbnb, Seattle, WA, USA |
|  |  [LASCA: A Large-Scale Stable Customer Segmentation Approach to Credit Risk Assessment](https://doi.org/10.1145/3637528.3671550) |  | 0 | Customer segmentation plays a crucial role in credit risk assessment by dividing users into specific risk levels based on their credit scores. Previous methods fail to comprehensively consider the stability in the segmentation process, resulting in frequent changes and inconsistencies in users' risk levels over time. This increases potential risks to a company. To this end, this paper at first introduces and formalizes the concept of stability regret in the segmentation process. However, evaluating stability is challenging due to its black-box nature and the computational burden posed by vast user data sets. To address these challenges, this paper proposes a large-scale stable customer segmentation approach named LASCA. LASCA consists of two phases: high-quality dataset construction (HDC) and reliable data-driven optimization (RDO). Specifically, HDC utilizes an evolutionary algorithm to collect high-quality binning solutions. RDO subsequently builds a reliable surrogate model to search for the most stable binning solution based on the collected dataset. Extensive experiments conducted on real-world large-scale datasets (up to 0.8 billion) show that LASCA surpasses the state-of-the-art binning methods in finding the most stable binning solution. Notably, HDC greatly enhances data quality by 50%. RDO efficiently discovers more stable binning solutions with a 36% improvement in stability, accelerating the optimization process by 25 times via data-driven evaluation. Currently, LASCA has been successfully deployed in the large-scale credit risk assessment system of Alipay. | Yongfeng Gu, Yupeng Wu, Huakang Lu, Xingyu Lu, Hong Qian, Jun Zhou, Aimin Zhou | School of Computer Science and Technology, East China Normal University, Shanghai, China; Ant Group, Hangzhou, Zhejiang, China |
|  |  [Generative Auto-bidding via Conditional Diffusion Modeling](https://doi.org/10.1145/3637528.3671526) |  | 0 | Auto-bidding plays a crucial role in facilitating online advertising by automatically providing bids for advertisers. Reinforcement learning (RL) has gained popularity for auto-bidding. However, most current RL auto-bidding methods are modeled through the Markovian Decision Process (MDP), which assumes the Markovian state transition. This assumption restricts the ability to perform in long horizon scenarios and makes the model unstable when dealing with highly random online advertising environments. To tackle this issue, this paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding through generative modeling. In this paradigm, we propose DiffBid, a conditional diffusion modeling approach for bid generation. DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons. Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints. Extensive experiments conducted on the real-world dataset and online A/B test on Alibaba advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81% increase in GMV and 3.36% increase in ROI. | Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Bo Zheng, Yan Zhang | Peking University, Beijing, China; Alibaba Group, Beijing, China; Peking University & Alibaba Group, Beijing, China |
|  |  [Learning Metrics that Maximise Power for Accelerated A/B-Tests](https://doi.org/10.1145/3637528.3671512) |  | 0 | Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the p-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation. | Olivier Jeunen, Aleksei Ustimenko | ShareChat, London, United Kingdom; ShareChat, Edinburgh, United Kingdom |
|  |  [Interpretable Cascading Mixture-of-Experts for Urban Traffic Congestion Prediction](https://doi.org/10.1145/3637528.3671507) |  | 0 | Rapid urbanization has significantly escalated traffic congestion,underscoring the need for advanced congestion prediction services to bolsterintelligent transportation systems. As one of the world's largest ride-hailingplatforms, DiDi places great emphasis on the accuracy of congestion predictionto enhance the effectiveness and reliability of their real-time services, suchas travel time estimation and route planning. Despite numerous efforts havebeen made on congestion prediction, most of them fall short in handlingheterogeneous and dynamic spatio-temporal dependencies (e.g., periodic andnon-periodic congestions), particularly in the presence of noisy and incompletetraffic data. In this paper, we introduce a Congestion PredictionMixture-of-Experts, CP-MoE, to address the above challenges. We first propose asparsely-gated Mixture of Adaptive Graph Learners (MAGLs) with congestion-awareinductive biases to improve the model capacity for efficiently capturingcomplex spatio-temporal dependencies in varying traffic scenarios. Then, wedevise two specialized experts to help identify stable trends and periodicpatterns within the traffic data, respectively. By cascading these experts withMAGLs, CP-MoE delivers congestion predictions in a more robust andinterpretable manner. Furthermore, an ordinal regression strategy is adopted tofacilitate effective collaboration among diverse experts. Extensive experimentson real-world datasets demonstrate the superiority of our proposed methodcompared with state-of-the-art spatio-temporal prediction models. Moreimportantly, CP-MoE has been deployed in DiDi to improve the accuracy andreliability of the travel time estimation system. | Wenzhao Jiang, Jindong Han, Hao Liu, Tao Tao, Naiqiang Tan, Hui Xiong | Didichuxing Co. Ltd, Beijing, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, Guangdong, China |
|  |  [False Positives in A/B Tests](https://doi.org/10.1145/3637528.3671631) |  | 0 | A/B tests, or online controlled experiments, are used heavily in the software industry to evaluate implementations of ideas, as the paradigm is the gold standard in science for establishing causality: the changes introduced in the treatment caused the changes to the metrics of interest with high probability. What distinguishes software experiments, or A/B tests, from experiments in many other domains is the scale (e.g., over 100 experiment treatments may launch on a given workday in large companies) and the effect sizes that matter to the business are small (e.g., a 3% improvement to conversion rate from a single experiment is a cause for celebration). The humbling reality is that most experiments fail to improve key metrics, and success rates of only about 10-20% are most common. With low success rates, the industry standard alpha threshold of 0.05 implies a high probability of false positives. We begin with motivation about why false positives are expensive in many software domains. We then offer several approaches to estimate the true success rate of experiments, given the observed "win" rate (statistically significant positive improvements), and show examples from Expedia and Optimizely. We offer a modified procedure for experimentation, based in sequential group testing, that selectively extends experiments to reduce false positives, increase power, at a small increase to runtime. We conclude with a discussion of the difference between ideas and experiments in practice, terms that are often incorrectly used interchangeably. | Ron Kohavi, Nanyu Chen | Expedia Group, San Francisco, CA, USA; Kohavi, Los Altos, CA, USA |
|  |  [Causal Machine Learning for Cost-Effective Allocation of Development Aid](https://doi.org/10.1145/3637528.3671551) |  | 0 | The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by "leaving no one behind", and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. For this, we first show that our framework successfully computes heterogeneous treatment-response curves using semi-synthetic data. Then, we demonstrate our framework using real-world HIV data. Our framework points to large opportunities for a more effective aid allocation, suggesting that the total number of new HIV infections could be reduced by up to 3.3% (~50,000 cases) compared to the current allocation practice. | Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel | ETH Zurich, Zurich, Switzerland; Munich Center for Machine Learning & LMU Munich, Munich, Germany |
|  |  [Chromosomal Structural Abnormality Diagnosis by Homologous Similarity](https://doi.org/10.1145/3637528.3671642) |  | 0 | Pathogenic chromosome abnormalities are very common among the general population. While numerical chromosome abnormalities can be quickly and precisely detected, structural chromosome abnormalities are far more complex and typically require considerable efforts by human experts for identification. This paper focuses on investigating the modeling of chromosome features and the identification of chromosomes with structural abnormalities. Most existing data-driven methods concentrate on a single chromosome and consider each chromosome independently, overlooking the crucial aspect of homologous chromosomes. In normal cases, homologous chromosomes share identical structures, with the exception that one of them is abnormal. Therefore, we propose an adaptive method to align homologous chromosomes and diagnose structural abnormalities through homologous similarity. Inspired by the process of human expert diagnosis, we incorporate information from multiple pairs of homologous chromosomes simultaneously, aiming to reduce noise disturbance and improve prediction performance. Extensive experiments on real-world datasets validate the effectiveness of our model compared to baselines. | Juren Li, Fanzhe Fu, Ran Wei, Yifei Sun, Zeyu Lai, Ning Song, Xin Chen, Yang Yang | Zhejiang University, Hangzhou, China; Hangzhou Diagens Biotechnology Co., Ltd., Hangzhou, China |
|  |  [An Open and Large-Scale Dataset for Multi-Modal Climate Change-aware Crop Yield Predictions](https://doi.org/10.1145/3637528.3671536) |  | 0 | Precise crop yield predictions are of national importance for ensuring food security and sustainable agricultural practices. While AI-for-science approaches have exhibited promising achievements in solving many scientific problems such as drug discovery, precipitation nowcasting, etc., the development of deep learning models for predicting crop yields is constantly hindered by the lack of an open and large-scale deep learning-ready dataset with multiple modalities to accommodate sufficient information. To remedy this, we introduce the CropNet dataset, the first terabyte-sized, publicly available, and multi-modal dataset specifically targeting climate change-aware crop yield predictions for the contiguous United States (U.S.) continent at the county level. Our CropNet dataset is composed of three modalities of data, i.e., Sentinel-2 Imagery, WRF-HRRR Computed Dataset, and USDA Crop Dataset, for over 2200 U.S. counties spanning 6 years (2017-2022), expected to facilitate researchers in developing versatile deep learning models for timely and precisely predicting crop yields at the county-level, by accounting for the effects of both short-term growing season weather variations and long-term climate change on crop yields. Besides, we develop the CropNet package, offering three types of APIs, for facilitating researchers in downloading the CropNet data on the fly over the time and region of interest, and flexibly building their deep learning models for accurate crop yield predictions. Extensive experiments have been conducted on our CropNet dataset via employing various types of deep learning solutions, with the results validating the general applicability and the efficacy of the CropNet dataset in climate change-aware crop yield predictions. We have officially released our CropNet dataset on Hugging Face Datasets https://huggingface.co/datasets/CropNet/CropNet and our CropNet package on the Python Package Index (PyPI) https://pypi.org/project/cropnet. Code and tutorials are available at https://github.com/fudong03/CropNet. | Fudong Lin, Kaleb Guillot, Summer Crawford, Yihe Zhang, Xu Yuan, NianFeng Tzeng | University of Louisiana at Lafeyette, Lafayette, LA, USA; University of Delaware, Newark, DE, USA |
|  |  [Modeling User Retention through Generative Flow Networks](https://doi.org/10.1145/3637528.3671531) |  | 0 | Recommender systems aim to fulfill the user's daily demands. While mostexisting research focuses on maximizing the user's engagement with the system,it has recently been pointed out that how frequently the users come back forthe service also reflects the quality and stability of recommendations.However, optimizing this user retention behavior is non-trivial and posesseveral challenges including the intractable leave-and-return user activities,the sparse and delayed signal, and the uncertain relations between users'retention and their immediate feedback towards each item in the recommendationlist. In this work, we regard the retention signal as an overall estimation ofthe user's end-of-session satisfaction and propose to estimate this signalthrough a probabilistic flow. This flow-based modeling technique canback-propagate the retention reward towards each recommended item in the usersession, and we show that the flow combined with traditional learning-to-rankobjectives eventually optimizes a non-discounted cumulative reward for bothimmediate user feedback and user retention. We verify the effectiveness of ourmethod through both offline empirical studies on two public datasets and onlineA/B tests in an industrial platform. | Ziru Liu, Shuchang Liu, Bin Yang, Zhenghai Xue, Qingpeng Cai, Xiangyu Zhao, Zijian Zhang, Lantao Hu, Han Li, Peng Jiang | Kuaishou Technology, Beijing, China; Nanyang Technological University, Singapore, Singapore; City University of Hong Kong, Hong Kong, China |
|  |  [BacktrackSTL: Ultra-Fast Online Seasonal-Trend Decomposition with Backtrack Technique](https://doi.org/10.1145/3637528.3671510) |  | 0 | Seasonal-trend decomposition (STD) is a crucial task in time series data analysis. Due to the challenges of scalability, there is a pressing need for an ultra-fast online algorithm. However, existing algorithms either fail to handle long-period time series (such as OnlineSTL), or need time-consuming iterative processes (such as OneShotSTL). Therefore, we propose BacktrackSTL, the first non-iterative online STD algorithm with period-independent O(1) update complexity. It is also robust to outlier, seasonality shift and trend jump because of the combination of outlier-resilient smoothing, non-local seasonal filtering and backtrack technique. Experimentally, BacktrackSTL decomposes a value within 1.6 μs, which is 15X faster than the state-of-the-art online algorithm OneShotSTL, while maintaining comparable accuracy to the best offline algorithm RobustSTL. We have also deployed BacktrackSTL on the top of Apache Flink to decompose monitoring metrics in Alibaba Cloud for over a year. Besides, we have open-sourced the artifact of this proposal on GitHub. | Haoyu Wang, Hongke Guo, Zhaoliang Zhu, You Zhang, Yu Zhou, Xudong Zheng | Alibaba Group, Zhejiang, Hangzhou, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Beijing, China |
|  |  [Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising](https://doi.org/10.1145/3637528.3671529) |  | 0 | In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on Click-Through Rate (CTR) and Conversion Rate (CVR) is critical. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration requires achieving calibration in each field. In order to achieve multi-field calibration, it is necessary to have a strong data utilization ability. Because the quantity of pCTR specified range for single field-value (such as user ID and item ID) sample is relatively small, which makes the calibrator more difficult to train. However, existing methods have difficulty effectively addressing these issues. To solve these problems, we propose a new method named Deep Ensemble Shape Calibration (DESC). In terms of business understanding and interpretability, we decompose multi-field calibration into value calibration and shape calibration. We introduce innovative basis calibration functions, which enhance both function expression capabilities and data utilization by combining these basis calibration functions. A significant advancement lies in the development of an allocator capable of allocating the most suitable calibrators to different estimation error distributions within diverse fields and values. We achieve significant improvements in both public and industrial datasets. In online experiments, we observe a +2.5% increase in CVR and +4.0% in GMV (Gross Merchandise Volume). Our code is now available at: https://github.com/HaoYang0123/DESC. | Shuai Yang, Hao Yang, Zhuang Zou, Linhe Xu, Shuo Yuan, Yifan Zeng | Shopee Discovery Ads, Beijing, China |
|  |  [GraphStorm: All-in-one Graph Machine Learning Framework for Industry Applications](https://doi.org/10.1145/3637528.3671603) |  | 0 | Graph machine learning (GML) is effective in many business applications.However, making GML easy to use and applicable to industry applications withmassive datasets remain challenging. We developed GraphStorm, which provides anend-to-end solution for scalable graph construction, graph model training andinference. GraphStorm has the following desirable properties: (a) Easy to use:it can perform graph construction and model training and inference with just asingle command; (b) Expert-friendly: GraphStorm contains many advanced GMLmodeling techniques to handle complex graph data and improve model performance;(c) Scalable: every component in GraphStorm can operate on graphs with billionsof nodes and can scale model training and inference to different hardwarewithout changing any code. GraphStorm has been used and deployed for over adozen billion-scale industry applications after its release in May 2023. It isopen-sourced in Github: https://github.com/awslabs/graphstorm. | Da Zheng, Xiang Song, Qi Zhu, Jian Zhang, Theodore Vasiloudis, Runjie Ma, Houyu Zhang, Zichen Wang, Soji Adeshina, Israt Nisa, Alejandro Mottini, Qingjun Cui, Huzefa Rangwala, Belinda Zeng, Christos Faloutsos, George Karypis | Amazon Search AI, Palo Alto, CA, USA; Amazon AWS AI, Santa Clara, CA, USA; Amazon AWS AI, Washington, D.C., USA; Amazon Search AI, Seattle, WA, USA; Amazon SP, Seattle, WA, USA; Amazon AWS AI, Seattle, WA, USA; Amazon AWS AI, New York, NY, USA |
|  |  [A Tutorial on Multi-Armed Bandit Applications for Large Language Models](https://doi.org/10.1145/3637528.3671440) |  | 0 | This tutorial offers a comprehensive guide on using multi-armed bandit (MAB) algorithms to improve Large Language Models (LLMs). As Natural Language Processing (NLP) tasks grow, efficient and adaptive language generation systems are increasingly needed. MAB algorithms, which balance exploration and exploitation under uncertainty, are promising for enhancing LLMs. The tutorial covers foundational MAB concepts, including the exploration-exploitation trade-off and strategies like epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling. It then explores integrating MAB with LLMs, focusing on designing architectures that treat text generation options as arms in a bandit problem. Practical aspects like reward design, exploration policies, and scalability are discussed. Real-world case studies demonstrate the benefits of MAB-augmented LLMs in content recommendation, dialogue generation, and personalized content creation, showing how these techniques improve relevance, diversity, and user engagement. | Djallel Bouneffouf, Raphaël Féraud | IBM Research, New York, New York, USA; Orange Orange Innovation, Lannion, France |
|  |  [Domain-Driven LLM Development: Insights into RAG and Fine-Tuning Practices](https://doi.org/10.1145/3637528.3671445) |  | 0 | To improve Large Language Model (LLM) performance on domain specific applications, ML developers often leverage Retrieval Augmented Generation (RAG) and LLM Fine-Tuning. RAG extends the capabilities of LLMs to specific domains or an organization's internal knowledge base, without the need to retrain the model. On the other hand, Fine-Tuning approach updates LLM weights with domain-specific data to improve performance on specific tasks. The fine-tuned model is particularly effective to systematically learn new comprehensive knowledge in a specific domain that is not covered by the LLM pre-training. This tutorial walks through the RAG and Fine-Tuning techniques, discusses the insights of their advantages and limitations, and provides best practices of adopting the methodologies for the LLM tasks and use cases. The hands-on labs demonstrate the advanced techniques to optimize the RAG and fine-tuned LLM architecture that handles domain specific LLM tasks. The labs in the tutorial are designed by using a set of open-source python libraries to implement the RAG and fine-tuned LLM architecture. | José Cassio dos Santos Junior, Rachel Hu, Richard Song, Yunfei Bai | Amazon Web Services, Seattle, Washington, USA; Epsilla, Jersey City, New Jersey, USA; CambioML Corp, San Jose, California, USA |
|  |  [Recent and Upcoming Developments in Randomized Numerical Linear Algebra for Machine Learning](https://doi.org/10.1145/3637528.3671461) |  | 0 | Large matrices arise in many machine learning and data analysis applications,including as representations of datasets, graphs, model weights, and first andsecond-order derivatives. Randomized Numerical Linear Algebra (RandNLA) is anarea which uses randomness to develop improved algorithms for ubiquitous matrixproblems. The area has reached a certain level of maturity; but recent hardwaretrends, efforts to incorporate RandNLA algorithms into core numericallibraries, and advances in machine learning, statistics, and random matrixtheory, have lead to new theoretical and practical challenges. This articleprovides a self-contained overview of RandNLA, in light of these developments. | Michal Derezinski, Michael W. Mahoney | University of Michigan, Ann Arbor, USA; ICSI, LBNL, and University of California, Berkeley, USA |
|  |  [Graph Machine Learning Meets Multi-Table Relational Data](https://doi.org/10.1145/3637528.3671471) |  | 0 | While graph machine learning, and notably graph neural networks (GNNs), have gained immense traction in recent years, application is predicated on access to a known input graph upon which predictive models can be trained. And indeed, within the most widely-studied public evaluation benchmarks such graphs are provided, with performance comparisons conditioned on curated data explicitly adhering to this graph. However, in real-world industrial applications, the situation is often quite different. Instead of a known graph, data are originally collected and stored across multiple tables in a repository, at times with ambiguous or incomplete relational structure. As such, to leverage the latest GNN architectures it is then up to a skilled data scientist to first manually construct a graph using intuition and domain knowledge, a laborious process that may discourage adoption in the first place. To narrow this gap and broaden the applicability of graph ML, we survey existing tools and strategies that can be combined to address the more fundamental problem of predictive tabular modeling over data native to multiple tables, with no explicit relational structure assumed a priori. This involves tracing a comprehensive path through related table join discovery and fuzzy table joining, column alignment, automated relational database (RDB) construction, extracting graphs from RDBs, graph sampling, and finally, graph-centric trainable predictive architectures. Although efforts to build deployable systems that integrate all of these components while minimizing manual effort remain in their infancy, this survey will nonetheless reduce barriers to entry and help steer the graph ML community towards promising research directions and wider real-world impact. | Quan Gan, Minjie Wang, David Wipf, Christos Faloutsos | CMU & Amazon, Pittsburgh, PA, USA; Amazon, Shanghai, China |
|  |  [Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods](https://doi.org/10.1145/3637528.3671472) |  | 0 | Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support large-scale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google's Pregel and Spark's GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. In recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems. | Da Yan, Lyuheng Yuan, Akhlaque Ahmad, Chenguang Zheng, Hongzhi Chen, James Cheng | Department of Computer Science, Indiana University Bloomington, Bloomington, Indiana, USA; Kasma Pte. Ltd., Singapore, Singapore |
|  |  [Machine Learning in Finance](https://doi.org/10.1145/3637528.3671488) |  | 0 | We provide a first comprehensive structuring of the literature applying machine learning to finance. We use a probabilistic topic modeling approach to make sense of this diverse body of research spanning across the disciplines of finance, economics, computer sciences, and decision sciences. Through the topic modelling approach, a Latent Dirichlet Allocation technique, we are able to extract the 14 coherent research topics that are the focus of the 5,204 academic articles we analyze from the years 1990 to 2018. We first describe and structure these topics, and then further show how the topic focus has evolved over the last two decades. Our study thus provides a structured topography for finance researchers seeking to integrate machine learning research approaches in their exploration of finance phenomena. We also showcase the benefits to finance researchers of the method of probabilistic modeling of topics for deep comprehension of a body of literature, especially when that literature has diverse multi-disciplinary actors. | Leman Akoglu, Nitesh V. Chawla, Josep DomingoFerrer, Eren Kurshan, Senthil Kumar, Vidyut M. Naware, José A. RodríguezSerrano, Isha Chaturvedi, Saurabh Nagrecha, Mahashweta Das, Tanveer A. Faruquie | Rennes Sch Business, Dept Strategy & Innovat, Rennes, France; Rennes Sch Business, Dept Finance & Accounting, Rennes, France; Rennes Sch Business, 2 Rue Robert Arbrissel, F-35065 Rennes, France; Dublin City Univ, DCU Business Sch, Financial & Operat Performance Grp, Dublin, Ireland |
|  |  [From Word-prediction to Complex Skills: Compositional Thinking and Metacognition in LLMs](https://doi.org/10.1145/3637528.3672193) |  | 0 | The talk will present evidence that today's large language models (LLMs) display somewhat deeper "understanding'' than one would naively expect.1. When asked to solve a task by combining a set of k simpler skills ("test of compositional capability"), they are able to do so despite not having seen the same combination of skills during their training.2. They demonstrate ability to reason about of their own learning processes, which is analogous to "metacognitive knowledge"[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task.We discuss mechanisms by which such complex understanding could arise (including a theory by [Arora,Goyal'23] that tries to explain (a)) and also give examples of how to leverage LLM meta knowledge to improve LLM training pipelines as well as evaluations. 1. When asked to solve a task by combining a set of k simpler skills ("test of compositional capability"), they are able to do so despite not having seen the same combination of skills during their training. 2. They demonstrate ability to reason about of their own learning processes, which is analogous to "metacognitive knowledge"[Flavel'76] in humans. For instance, given examples of an evaluation task, they can produce a catalog of suitably named skills that are relevant for solving each example of that task. Furthermore, this catalog of skills is meaningful, in the sense that incorporating it into training pipelines improves performance (including of other unrelated LLMs) on that task. | Sanjeev Arora | Princeton University, Princeton, NJ, USA |
|  |  [GEO: Generative Engine Optimization](https://doi.org/10.1145/3637528.3671900) |  | 0 | The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of generative engines (GEs), can generate accurate and personalized responses, rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them using LLMs. While this shift significantly improvesuser utility and generative search engine traffic, it poses a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of generative engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, we must ensure the creator economy is not disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), the first novel paradigm to aid content creators in improving their content visibility in generative engine responses through a flexible black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation by introducing GEO-bench, a large-scale benchmark of diverse user queries across multiple domains, along with relevant web sources to answer these queries. Through rigorous evaluation, we demonstrate that GEO can boost visibility by up to 40% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific optimization methods. Our work opens a new frontier in information discovery systems, with profound implications for both developers of generative engines and content creators. | Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande | Independent, Seattle, USA; Indian Institute of Technology Delhi, New Delhi, India; Princeton University, Princeton, USA |
|  |  [AI for Nature: From Science to Impact](https://doi.org/10.1145/3637528.3672192) |  | 0 | Computation has fundamentally changed the way we study nature. New data collection technologies, such as GPS, high-definition cameras, autonomous vehicles under water, on the ground, and in the air, genotyping, acoustic sensors, and crowdsourcing, are generating data about life on the planet that are orders of magnitude richer than any previously collected. Yet, our ability to extract insight from this data lags substantially behind our ability to collect it. The need for understanding is more urgent than ever and the challenges are great. We are in the middle of the 6th extinction, losing the planet's biodiversity at an unprecedented rate and scale. In many cases, we do not even have the basic numbers of what species we are losing, which impacts our ability to understand biodiversity loss drivers, predict the impact on ecosystems, and implement policy. From the basic science perspective, the new data opens the possibility of understanding function of traits of organisms and ecosystems, which is critical for biologists to predict effects of environmental change or genetic manipulation and to understand the significance of patterns in the four-billion-year evolutionary history of life. The key to unlocking the potential of this data are machine learning (ML) and artificial intelligence (AI) methods, which are already beginning to have significant impacts on research across ecology and conservation. AI can turn data into high resolution information source about living organisms, enabling scientific inquiry, conservation, and policy decisions. The talk introduces a new field of science, imageomics, and presents a vision and examples of AI as a trustworthy partner both in science and biodiversity conservation, discussing opportunities and challenges. | Tanya Y. BergerWolf | The Ohio State University, Columbus, OH, USA |
|  |  [Statistical Models of Top-k Partial Orders](https://doi.org/10.1145/3637528.3672014) |  | 0 | In many contexts involving ranked preferences, agents submit partial orders over available alternatives. Statistical models often treat these as marginal in the space of total orders, but this approach overlooks information contained in the list length itself. In this work, we introduce and taxonomize approaches for jointly modeling distributions over top-k partial orders and list lengths k, considering two classes of approaches: composite models that view a partial order as a truncation of a total order, and augmented ranking models that model the construction of the list as a sequence of choice decisions, including the decision to stop. For composite models, we consider three dependency structures for joint modeling of order and truncation length. For augmented ranking models, we consider different assumptions on how the stop-token choice is modeled. Using data consisting of partial rankings from San Francisco school choice and San Francisco ranked choice elections, we evaluate how well the models predict observed data and generate realistic synthetic datasets. We find that composite models, explicitly modeling length as a categorical variable, produce synthetic datasets with accurate length distributions, and an augmented model with position-dependent item utilities jointly models length and preferences in the training data best, as measured by negative log loss. Methods from this work have significant implications on the simulation and evaluation of real-world social systems that solicit ranked preferences. | Amel Awadelkarim, Johan Ugander | Stanford University, Stanford, CA, USA |
|  |  [Resilient k-Clustering](https://doi.org/10.1145/3637528.3671888) |  | 0 | We study the problem of resilient clustering in the metric setting where one is interested in designing algorithms that return high quality solutions that preserve the clustering structure under perturbations of the input points. Our first contribution is to introduce a formal notion of algorithmic resiliency for clustering problems that, roughly speaking, requires an algorithm to have similar outputs on close inputs. Then, we notice that classic algorithms have weak resiliency guarantees and develop new algorithms for fundamental clustering problems such as k-center, k-median, and k-means. Finally, we complement our results with an experimental analysis showing the effectiveness of our techniques on real-world instances. | Sara Ahmadian, MohammadHossein Bateni, Hossein Esfandiari, Silvio Lattanzi, Morteza Monemizadeh, Ashkan NorouziFard | Google, Barcelona, USA; Google, New York, USA; Google, New york, USA; Google, Zurich, Switzerland; Google, Seattle, USA; Department of Mathematics and Computer Science, TU Eindhoven, Eindhoven, Netherlands |
|  |  [A Learned Generalized Geodesic Distance Function-Based Approach for Node Feature Augmentation on Graphs](https://doi.org/10.1145/3637528.3671858) |  | 0 | Geodesic distances on manifolds have numerous applications in image processing, computer graphics and computer vision. In this work, we introduce an approach called \`LGGD' (Learned Generalized Geodesic Distances). This method involves generating node features by learning a generalized geodesic distance function through a training pipeline that incorporates training data, graph topology and the node content features. The strength of this method lies in the proven robustness of the generalized geodesic distances to noise and outliers. Our contributions encompass improved performance in node classification tasks, competitive results with state-of-the-art methods on real-world graph datasets, the demonstration of the learnability of parameters within the generalized geodesic equation on graph, and dynamic inclusion of new labels. | Amitoz Azad, Yuan Fang | Singapore Management University, Singapore, Singapore |
|  |  [Improved Active Covering via Density-Based Space Transformation](https://doi.org/10.1145/3637528.3671794) |  | 0 | In this work, we study active covering, a variant of the active-learning problem that involves labeling (or identifying) all of the examples with a positive label. We propose a couple of algorithms, namely Density-Adjusted Non-Adaptive (DANA) learner and Density-Adjusted Adaptive (DAA) learner, that query the labels according to a distance function that is adjusted by the density function. Under mild assumptions, we prove that our algorithms discover all of the positive labels while querying only a sublinear number of examples from the support of negative labels for constant-dimensional spaces (see Theorems 5 and 6). Our experiments show that our champion algorithm DAA consistently improves over the prior work on some standard benchmark datasets, including those used by the previous work, as well as a couple of data sets on credit card fraud. For instance, when measuring performance using AUC, our algorithm is the best in 25 out of 27 experiments over 7 different datasets. | MohammadHossein Bateni, Hossein Esfandiari, Samira HosseinGhorban, Alipasha Montaseri | Google Research, New York City, New York, USA; Sharif University of Technology, Tehran, Iran; Google Research, London, United Kingdom; School of Computer Science, Institute for Research in Fundamental Sciences, Tehran, Iran |
|  |  [Towards Robust Information Extraction via Binomial Distribution Guided Counterpart Sequence](https://doi.org/10.1145/3637528.3672067) |  | 0 | Information extraction (IE) aims to extract meaningful structured tuples from unstructured text. Existing studies usually utilize a pre-trained generative language model that rephrases the original sentence into a target sequence, which can be easily decoded as tuples. However, traditional evaluation metrics treat a slight error within the tuple as an entire prediction failure, which is unable to perceive the correctness extent of a tuple. For this reason, we first propose a novel IE evaluation metric called Matching Score to evaluate the correctness of the predicted tuples in more detail. Moreover, previous works have ignored the effects of semantic uncertainty when focusing on the generation of the target sequence. We argue that leveraging the built-in semantic uncertainty of language models is beneficial for improving its robustness. In this work, we propose Binomial distribution guided counterpart sequence (BCS) method, which is a model-agnostic approach. Specifically, we propose to quantify the built-in semantic uncertainty of the language model by bridging all local uncertainties with the whole sequence. Subsequently, with the semantic uncertainty and Matching Score, we formulate a unique binomial distribution for each local decoding step. By sampling from this distribution, a counterpart sequence is obtained, which can be regarded as a semantic complement to the target sequence. Finally, we employ the Kullback-Leibler divergence to align the semantics of the target sequence and its counterpart. Extensive experiments on 14 public datasets over 5 information extraction tasks demonstrate the effectiveness of our approach on various methods. Our code and dataset are available at https://github.com/byinhao/BCS. | Yinhao Bai, Yuhua Zhao, Zhixin Han, Hang Gao, Chao Xue, Mengting Hu | JD Explore Academy, Beijing, Chile; College of Software, Nankai University, Tianjin, China; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China |
|  |  [Graph Mamba: Towards Learning on Graphs with State Space Models](https://doi.org/10.1145/3637528.3672044) |  | 0 | Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional Encodings (PE). In this paper, we show that while Transformers, complex message-passing, and PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), we present Graph Mamba Networks (GMNs), a framework for a new class of GNNs based on selective SSMs. We discuss the new challenges when adapting SSMs to graph-structured data, and present four required steps to design GMNs, where we choose (1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of SSM Encoder, and (4) Local Encoding. We provide theoretical justification for the power of GMNs, and experimentally show that GMNs attain an outstanding performance in various benchmark datasets. The code is available in this link. | Ali Behrouz, Farnoosh Hashemi | Cornell University, Ithaca, NY, USA |
|  |  [FaultInsight: Interpreting Hyperscale Data Center Host Faults](https://doi.org/10.1145/3637528.3672051) |  | 0 | Operating and maintaining hyperscale data centers involving millions of service hosts has been an extremely intricate task to tackle for top Internet companies. Incessant system failures cost operators countless hours of browsing through performance metrics to diagnose the underlying root cause to prevent the recurrence. Although many state-of-the-art (SOTA) methods have used time-series causal discovery to construct causal relationships among anomalous metrics, they only focus on homogeneous service-level performance metrics and fail to yield useful insights on heterogeneous host-level metrics. To address the challenge, this study presents FaultInsight, a highly interpretable deep causal host fault diagnosing framework that offers diagnostic insights from various perspectives to reduce human effort in troubleshooting. We evaluate FaultInsight using dozens of incidents collected from our production environment. FaultInsight provides markedly better root cause identification accuracy than SOTA baselines in our incident dataset. It also shows outstanding advantages in terms of deployability in real production systems. Our engineers are deeply impressed by FaultInsight's ability to interpret incidents from multiple perspectives, helping them quickly understand the mechanism behind the faults. | Tingzhu Bi, Yang Zhang, Yicheng Pan, Yu Zhang, Meng Ma, Xinrui Jiang, Linlin Han, Feng Wang, Xian Liu, Ping Wang | Peking University & Shuanghu Laboratory, Beijing, China; ByteDance Inc., Beijing, China; Peking University, Beijing, China |
|  |  [Learning the Covariance of Treatment Effects Across Many Weak Experiments](https://doi.org/10.1145/3637528.3672034) |  | 0 | When primary objectives are insensitive or delayed, experimenters may instead focus on proxy metrics derived from secondary outcomes. For example, technology companies often infer the long-term impacts of product interventions from their effects on short-term user engagement signals. We consider the meta-analysis of many historical experiments to learn the covariance of treatment effects on these outcomes, which can support the construction of such proxies. Even when experiments are plentiful, if treatment effects are weak, the covariance of estimated treatment effects across experiments can be highly biased. We overcome this with techniques inspired by weak instrumental variable analysis. We show that Limited Information Maximum Likelihood (LIML) learns a parameter equivalent to fitting total least squares to a transformation of the scatterplot of treatment effects, and that Jackknife Instrumental Variables Estimation (JIVE) learns another parameter computable from the average of Jackknifed covariance matrices across experiments. We also present a total covariance estimator for the latter estimand under homoskedasticity, which is equivalent to a k-class estimator. We show how these parameters can be used to construct unbiased proxy metrics under various structural models. Lastly, we discuss the real-world application of our methods at Netflix. | Aurélien Bibaut, Winston Chou, Simon Ejdemyr, Nathan Kallus | Netflix, Los Gatos, CA, USA; Netflix & Cornell University, Los Gatos, CA, USA |
|  |  [Making Temporal Betweenness Computation Faster and Restless](https://doi.org/10.1145/3637528.3671825) |  | 0 | Buss et al [KDD 2020] recently proved that the problem of computing the betweenness of all nodes of a temporal graph is computationally hard in the case of foremost and fastest paths, while it is solvable in time O(n3T2) in the case of shortest and shortest foremost paths, where n is the number of nodes and T is the number of distinct time steps. A new algorithm for temporal betweenness computation is introduced in this paper. In the case of shortest and shortest foremost paths, it requires O(n + M) space and runs in time O(nM)=O(n3T), where M is the number of temporal edges, thus significantly improving the algorithm of Buss et al in terms of time complexity (note that T is usually large). Experimental evidence is provided that our algorithm performs between twice and almost 250 times better than the algorithm of Buss et al. Moreover, we were able to compute the exact temporal betweenness values of several large temporal graphs with over a million of temporal edges. For such size, only approximate computation was possible by using the algorithm of Santoro and Sarpe [WWW 2022]. Maybe more importantly, our algorithm extends to the case of restless walks (that is, walks with waiting constraints in each node), thus providing a polynomial-time algorithm (with complexity O(nM)) for computing the temporal betweenness in the case of several different optimality criteria. Such restless computation was known only for the shortest criterion (Rymar et al [JGAA 2023]), with complexity O(n2MT2). We performed an extensive experimental validation by comparing different waiting constraints and different optimisation criteria. Moreover, as a case study, we investigate six public transit networks including Berlin, Rome, and Paris. Overall we find a general consistency between the different variants of betweenness centrality. However, we do measure a sensible influence of waiting constraints, and note some cases of low correlation for certain pairs of criteria in some networks. | Filippo Brunelli, Pierluigi Crescenzi, Laurent Viennot | Inria, DI ENS, Paris, France; European Commission -- JRC, Seville, Spain; Gran Sasso Science Institute, L'Aquila, Italy |
|  |  [Tackling Instance-Dependent Label Noise with Class Rebalance and Geometric Regularization](https://doi.org/10.1145/3637528.3671707) |  | 0 | In label-noise learning, accurately identifying the transition matrix is crucial for developing statistically consistent classifiers. This task is complicated by instance-dependent noise, which introduces identifiability challenges in the absence of stringent assumptions. Existing methods use neural networks to estimate the transition matrix by initially extracting confident clean instances. However, this extraction process is hindered by severe inter-class imbalance and a bias toward selecting unambiguous intra-class instances, leading to a distorted understanding of noise patterns. To tackle these challenges, our paper introduces a Class Rebalance and Geometric Regularization-based Framework (CRGR). CRGR employs a smoothed, noise-tolerant reweighting mechanism to equilibrate inter-class representation, thereby mitigating the risk of model overfitting to dominant classes. Additionally, recognizing that instances with similar characteristics often exhibit parallel noise patterns, we propose that the transition matrix should mirror the similarity of the feature space. This insight promotes the inclusion of ambiguous instances in training, serving as a form of geometric regularization. Such a strategy enhances the model's ability to navigate diverse noise patterns and strengthens its generalization capabilities. By addressing both inter-class and intra-class biases, CRGR offers a more balanced and robust classification model. Extensive experiments on both synthetic and real-world datasets demonstrate CRGR's superiority over existing state-of-the-art methods, significantly boosting classification accuracy and showcasing its effectiveness in handling instance-dependent noise. | Shuzhi Cao, Jianfei Ruan, Bo Dong, Bin Shi |  |
|  |  [DiffusionE: Reasoning on Knowledge Graphs via Diffusion-based Graph Neural Networks](https://doi.org/10.1145/3637528.3671997) |  | 0 | Graph Neural Networks (GNNs) have demonstrated powerful capabilities in reasoning within Knowledge Graphs (KGs), gathering increasing attention. Our idea stems from the observation that the prior work typically employs hand-designed or sample-designed paradigms in the process of message propagation, engaging a set of adjacent entities at each step of propagation. As a result, such methods struggle with the increasing number of entities involved as propagation steps extend. Moreover, they neglect the message interactions between adjacent entities and propagation relations in KG reasoning, leading to semantic inconsistency during the message aggregation phase. To address these issues, we introduce a novel knowledge graph embedding method through a diffusion process, termed DiffusionE. Specifically, we reformulate the message propagation in knowledge reasoning as a diffusion process, regarding the message semantics as the diffusion signal. In this sense, guided by semantic information, messages can be transmitted between nodes effectively and adaptively. Furthermore, the theoretical analysis suggests our method can leverage an optimal diffusivity for message propagation in the semantic interactions of KGs. It shows that DiffusionE effectively leverages message interactions between entities and propagation relations, ensuring semantic consistency in KG reasoning. Comprehensive experiments reveal that our method attains state-of-the-art performance compared to prior work on several well-established benchmarks. | Zongsheng Cao, Jing Li, Zigan Wang, Jinliang Li | School of Economics and Management, Tsinghua University, Beijing, China; School of Economics and Management, Tsinghua University, Haidian, Beijing, China; University of Chinese Academy of Sciences, Beijing, China |
|  |  [Path-based Explanation for Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671683) |  | 0 | Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability. The code is available at https://github.com/OUTHIM/power-link | Heng Chang, Jiangnan Ye, Alejo LopezAvila, Jinhua Du, Jia Li | Huawei Technologies Co., Ltd., Beijing, China; Hong Kong University of Science and Technology, Guangzhou, China; Huawei Technologies Co., Ltd., London, United Kingdom |
|  |  [Cluster-Wide Task Slowdown Detection in Cloud System](https://doi.org/10.1145/3637528.3671936) |  | 0 | Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in descending amplitude order and Reconstructing Non-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets. | Feiyi Chen, Yingying Zhang, Lunting Fan, Yuxuan Liang, Guansong Pang, Qingsong Wen, Shuiguang Deng | Zhejiang University, Hangzhou, China; Alibaba Group, Hangzhou, China; Singapore Management University, Singapore, Singapore; The Hong Kong University of Science and Technology (Guangzhou), Hong Kong, China; Zhejiang University & Alibaba Group, Hangzhou, China; Squirrel AI, Bellevue, USA |
|  |  [Scalable Algorithm for Finding Balanced Subgraphs with Tolerance in Signed Networks](https://doi.org/10.1145/3637528.3671674) |  | 0 | Signed networks, characterized by edges labeled as either positive or negative, offer nuanced insights into interaction dynamics beyond the capabilities of unsigned graphs. Central to this is the task of identifying the maximum balanced subgraph, crucial for applications like polarized community detection in social networks and portfolio analysis in finance. Traditional models, however, are limited by an assumption of perfect partitioning, which fails to mirror the complexities of real-world data. Addressing this gap, we introduce an innovative generalized balanced subgraph model that incorporates tolerance for imbalance. Our proposed region-based heuristic algorithm, tailored for this NP -hard problem, strikes a balance between low time complexity and high-quality outcomes. Comparative experiments validate its superior performance against leading solutions, delivering enhanced effectiveness (notably larger subgraph sizes) and efficiency (achieving up to 100× speedup) in both traditional and generalized contexts. | Jingbang Chen, Qiuyang Mang, Hangrui Zhou, Richard Peng, Yu Gao, Chenhao Ma | David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen, Guangdong, China; Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; Independent, Beijing, China |
|  |  [QGRL: Quaternion Graph Representation Learning for Heterogeneous Feature Data Clustering](https://doi.org/10.1145/3637528.3671839) |  | 0 | Clustering is one of the most commonly used techniques for unsupervised data analysis. As real data sets are usually composed of numerical and categorical features that are heterogeneous in nature, the heterogeneity in the distance metric and feature coupling prevents deep representation learning from achieving satisfactory clustering accuracy. Currently, supervised Quaternion Representation Learning (QRL) has achieved remarkable success in efficiently learning informative representations of coupled features from multiple views derived endogenously from the original data. To inherit the advantages of QRL for unsupervised heterogeneous feature representation learning, we propose a deep QRL model that works in an encoder-decoder manner. To ensure that the implicit couplings of heterogeneous feature data can be well characterized by representation learning, a hierarchical coupling encoding strategy is designed to convert the data set into an attributed graph to be the input of QRL. We also integrate the clustering objective into the model training to facilitate a joint optimization of the representation and clustering. Extensive experimental evaluations illustrate the superiority of the proposed Quaternion Graph Representation Learning (QGRL) method in terms of clustering accuracy and robustness to various data sets composed of arbitrary combinations of numerical and categorical features. The source code is opened at https://github.com/Juny-Chen/QGRL.git. | Junyang Chen, Yuzhu Ji, Rong Zou, Yiqun Zhang, Yiuming Cheung | Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, China |
|  |  [Can a Deep Learning Model be a Sure Bet for Tabular Prediction?](https://doi.org/10.1145/3637528.3671893) |  | 0 | Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer. | Jintai Chen, Jiahuan Yan, Qiyuan Chen, Danny Z. Chen, Jian Wu, Jimeng Sun | University of Notre Dame, South Bend, IN, USA; Zhejiang University, Hangzhou, Zhejiang, China; Univ. of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [Profiling Urban Streets: A Semi-Supervised Prediction Model Based on Street View Imagery and Spatial Topology](https://doi.org/10.1145/3637528.3671918) |  | 0 | With the expansion and growth of cities, profiling urban areas with the advent of multi-modal urban datasets (e.g., points-of-interest and street view imagery) has become increasingly important in urban planing and management. Particularly, street view images have gained popularity for understanding the characteristics of urban areas due to its abundant visual information and inherent correlations with human activities. In this study, we define a street segment represented by multiple street view images as the minimum spatial unit for analysis and predict its functional and socioeconomic indicators, which presents several challenges in modeling spatial distributions of images on a street and the spatial topology (adjacency) of streets. Meanwhile, Large Language Models are capable of understanding imagery data based on its extraordinary knowledge base and unveil a remarkable opportunity for profiling streets with images. In view of the challenges and opportunity, we present a semi-supervised Urban Street Profiling Model (USPM) based on street view imagery and spatial adjacency of urban streets. Specifically, given a street with multiple images, we first employ a newly designed spatial context-based contrastive learning method to generate feature vectors of images and then apply the LSTM-based fusion method to encode multiple images on a street to yield the street visual representation; we then create the descriptions of street scenes for street view images based on the SPHINX (a large language model) and produce the street textual representation; finally, we build an urban street graph based on spatial topology (adjacency) and employ a semi-supervised graph learning algorithm to further encode the street representations for prediction. We conduct thorough experiments with real-world datasets to assess the proposed USPM. The experimental results demonstrate that USPM considerably outperforms baseline methods in two urban prediction tasks. | Meng Chen, Zechen Li, Weiming Huang, Yongshun Gong, Yilong Yin | School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Software, Shandong University, Jinan, Shandong, China |
|  |  [Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network](https://doi.org/10.1145/3637528.3671965) |  | 0 | Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at https://github.com/LinChen-65/ReStruct. | Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui | BNRist, Department of Electronic Engineering, Tsinghua University, Beijing, China; Hong Kong University of Science and Technology, Hong Kong, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Hefei University of Technology, Hefei, China |
|  |  [Hate Speech Detection with Generalizable Target-aware Fairness](https://doi.org/10.1145/3637528.3671821) |  | 0 | To counter the side effect brought by the proliferation of social media platforms, hate speech detection (HSD) plays a vital role in halting the dissemination of toxic online posts at an early stage. However, given the ubiquitous topical communities on social media, a trained HSD classifier can easily become biased towards specific targeted groups (e.g.,female andblack people), where a high rate of either false positive or false negative results can significantly impair public trust in the fairness of content moderation mechanisms, and eventually harm the diversity of online society. Although existing fairness-aware HSD methods can smooth out some discrepancies across targeted groups, they are mostly specific to a narrow selection of targets that are assumed to be known and fixed. This inevitably prevents those methods from generalizing to real-world use cases where new targeted groups constantly emerge (e.g., new forums created on Reddit) over time. To tackle the defects of existing HSD practices, we propose Generalizable target-aware Fairness (GetFair), a new method for fairly classifying each post that contains diverse and even unseen targets during inference. To remove the HSD classifier's spurious dependence on target-related features, GetFair trains a series of filter functions in an adversarial pipeline, so as to deceive the discriminator that recovers the targeted group from filtered post embeddings. To maintain scalability and generalizability, we innovatively parameterize all filter functions via a hypernetwork. Taking a target's pretrained word embedding as input, the hypernetwork generates the weights used by each target-specific filter on-the-fly without storing dedicated filter parameters. In addition, a novel semantic gap alignment scheme is imposed on the generation process, such that the produced filter function for an unseen target is rectified by its semantic affinity with existing targets used for training. Finally, experiments are conducted on two benchmark HSD datasets, showing advantageous performance of GetFair on out-of-sample targets among baselines. | Tong Chen, Danny Wang, Xurong Liang, Marten Risius, Gianluca Demartini, Hongzhi Yin | The University of Queensland, Brisbane, Australia |
|  |  [GraphWiz: An Instruction-Following Language Model for Graph Computational Problems](https://doi.org/10.1145/3637528.3672010) |  | 0 | Large language models (LLMs) have achieved impressive success across various domains, but their capability in understanding and resolving complex graph problems is less explored. To bridge this gap, we introduce GraphInstruct, a novel instruction-tuning dataset aimed at enabling language models to tackle a broad spectrum of graph problems through explicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an open-source language model capable of solving various graph computational problems while generating clear reasoning processes. To further enhance the model's performance and reliability, we integrate the Direct Preference Optimization (DPO) framework within the graph problem-solving context. The improved model, GraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with different complexity levels, surpassing GPT-4 which has an average accuracy of 43.8%. Our study also investigates the relationship between training data volume and model performance, emphasizing the risk of overfitting as data volume increases. Additionally, we explore the transferability of the proposed model across different tasks and datasets, demonstrating its robust zero-shot generalization capability. GraphWiz offers a new blueprint and valuable insights for developing LLMs specialized in graph reasoning and problem-solving. | Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li |  |
|  |  [Calibration of Time-Series Forecasting: Detecting and Adapting Context-Driven Distribution Shift](https://doi.org/10.1145/3637528.3671926) |  | 0 | Recent years have witnessed the success of introducing deep learning modelsto time series forecasting. From a data generation perspective, we illustratethat existing models are susceptible to distribution shifts driven by temporalcontexts, whether observed or unobserved. Such context-driven distributionshift (CDS) introduces biases in predictions within specific contexts and poseschallenges for conventional training paradigms. In this paper, we introduce auniversal calibration methodology for the detection and adaptation of CDS witha trained model. To this end, we propose a novel CDS detector, termed the"residual-based CDS detector" or "Reconditionor", which quantifies the model'svulnerability to CDS by evaluating the mutual information between predictionresiduals and their corresponding contexts. A high Reconditionor scoreindicates a severe susceptibility, thereby necessitating model adaptation. Inthis circumstance, we put forth a straightforward yet potent adapter frameworkfor model calibration, termed the "sample-level contextualized adapter" or"SOLID". This framework involves the curation of a contextually similar datasetto the provided test sample and the subsequent fine-tuning of the model'sprediction layer with a limited number of steps. Our theoretical analysisdemonstrates that this adaptation strategy can achieve an optimal bias-variancetrade-off. Notably, our proposed Reconditionor and SOLID are model-agnostic andreadily adaptable to a wide range of models. Extensive experiments show thatSOLID consistently enhances the performance of current forecasting models onreal-world datasets, especially on cases with substantial CDS detected by theproposed Reconditionor, thus validating the effectiveness of the calibrationapproach. | Mouxiang Chen, Lefei Shen, Han Fu, Zhuo Li, Jianling Sun, Chenghao Liu | Zhejiang University, Hangzhou, China; Salesforce Research Asia, Singapore, Singapore; State Street Technology (Zhejiang) Ltd., Hangzhou, China |
|  |  [Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction](https://doi.org/10.1145/3637528.3671770) |  | 0 | Structure encoding has proven to be the key feature to distinguishing links in a graph. However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction. We develop the Co-Neighbor Encoding Schema (CNES) to address this issue. Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations. Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel. Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information. A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques. Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method. | Ke Cheng, Linzhi Peng, Junchen Ye, Leilei Sun, Bowen Du | CCSE Lab, Beihang University, Beijing, China; School of Transportation Science and Engineering, Beihang University, Beijing, China |
|  |  [Resurrecting Label Propagation for Graphs with Heterophily and Label Noise](https://doi.org/10.1145/3637528.3671774) |  | 0 | Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One significant limitation is that they operate under the assumption that the graph exhibits homophily and that the labels are distributed smoothly. However, real-world graphs can exhibit varying degrees of heterophily, or even be dominated by heterophily, which results in the inadequacy of the current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following observations, we propose a efficient algorithm, denoted as R2LP. Specifically, R2LP is an iterative algorithm with three steps: (1) reconstruct the graph to recover the homophily property, (2) utilize label propagation to rectify the noisy labels, (3) select high-confidence labels to retain for the next iteration. By iterating these steps, we obtain a set of ''correct'' labels, ultimately achieving high accuracy in the node classification task. The theoretical analysis is also provided to demonstrate its remarkable denoising effect. Finally, we perform experiments on ten benchmark datasets with different levels of graph heterophily and various types of noise. In these experiments, we compare the performance of R2LP against ten typical baseline methods. Our results illustrate the superior performance of the proposed øurs. The code and data of this paper can be accessed at: https://github.com/cy623/R2LP.git. | Yao Cheng, Caihua Shan, Yifei Shen, Xiang Li, Siqiang Luo, Dongsheng Li | Microsoft Research Asia, Shanghai, China; Nanyang Technological University, Singapore, Singapore; East China Normal University, Shanghai, China |
|  |  [DyGKT: Dynamic Graph Learning for Knowledge Tracing](https://doi.org/10.1145/3637528.3671773) |  | 0 | Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT. | Ke Cheng, Linzhi Peng, Pengyang Wang, Junchen Ye, Leilei Sun, Bowen Du | SKL-IOTSC, Department of Computer and Information Science, University of Macau, Macau, China; School of Transportation Science and Engineering, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China |
|  |  [Conformal Counterfactual Inference under Hidden Confounding](https://doi.org/10.1145/3637528.3671976) |  | 0 | Personalized decision making requires the knowledge of potential outcomes under different treatments, and confidence intervals about the potential outcomes further enrich this decision-making process and improve its reliability in high-stakes scenarios. Predicting potential outcomes along with its uncertainty in a counterfactual world poses the foundamental challenge in causal inference. Existing methods that construct confidence intervals for counterfactuals either rely on the assumption of strong ignorability that completely ignores hidden confounders, or need access to un-identifiable lower and upper bounds that characterize the difference between observational and interventional distributions. In this paper, to overcome these limitations, we first propose a novel approach wTCP-DR based on transductive weighted conformal prediction, which provides confidence intervals for counterfactual outcomes with marginal converage guarantees, even under hidden confounding. With less restrictive assumptions, our approach requires access to a fraction of interventional data (from randomized controlled trials) to account for the covariate shift from observational distributoin to interventional distribution. Theoretical results explicitly demonstrate the conditions under which our algorithm is strictly advantageous to the naive method that only uses interventional data. Since transductive conformal prediction is notoriously costly, we propose wSCP-DR, a two-stage variant of wTCP-DR, based on split conformal prediction with same marginal coverage guarantees but at a significantly lower computational cost. After ensuring valid intervals on counterfactuals, it is straightforward to construct intervals for individual treatment effects (ITEs). We demonstrate our method across synthetic and real-world data, including recommendation systems, to verify the superiority of our methods compared against state-of-the-art baselines in terms of both coverage and efficiency. Our code can be found at https://github.com/rguo12/KDD24-Conformal. | Zonghao Chen, Ruocheng Guo, JeanFrancois Ton, Yang Liu | Bytedance Research, London, United Kingdom; Bytedance Research, San Jose, USA; University College London, London, United Kingdom |
|  |  [Leveraging Pedagogical Theories to Understand Student Learning Process with Graph-based Reasonable Knowledge Tracing](https://doi.org/10.1145/3637528.3671853) |  | 0 | Knowledge tracing (KT) is a crucial task in intelligent education, focusingon predicting students' performance on given questions to trace their evolvingknowledge. The advancement of deep learning in this field has led todeep-learning knowledge tracing (DLKT) models that prioritize high predictiveaccuracy. However, many existing DLKT methods overlook the fundamental goal oftracking students' dynamical knowledge mastery. These models do not explicitlymodel knowledge mastery tracing processes or yield unreasonable results thateducators find difficulty to comprehend and apply in real teaching scenarios.In response, our research conducts a preliminary analysis of mainstream KTapproaches to highlight and explain such unreasonableness. We introduce GRKT, agraph-based reasonable knowledge tracing method to address these issues. Byleveraging graph neural networks, our approach delves into the mutualinfluences of knowledge concepts, offering a more accurate representation ofhow the knowledge mastery evolves throughout the learning process.Additionally, we propose a fine-grained and psychological three-stage modelingprocess as knowledge retrieval, memory strengthening, and knowledgelearning/forgetting, to conduct a more reasonable knowledge tracing process.Comprehensive experiments demonstrate that GRKT outperforms eleven baselinesacross three datasets, not only enhancing predictive accuracy but alsogenerating more reasonable knowledge tracing results. This makes our model apromising advancement for practical implementation in educational settings. Thesource code is available at https://github.com/JJCui96/GRKT. | Jiajun Cui, Hong Qian, Bo Jiang, Wei Zhang | East China Normal University, Shanghai, China |
|  |  [Iterative Weak Learnability and Multiclass AdaBoost](https://doi.org/10.1145/3637528.3671842) |  | 0 | We propose an efficient boosting algorithm for multiclass classification, called AdaBoost.Iter, that extends SAMME and AdaBoost. The algorithm iteratively applies the weak learnability condition of SAMME to eliminate classes to find the correct classificiation. The iterative weak learnability is a sufficient and necessary condition for boostability, but it is also easier to validate than the EOR criterion of AdaBoost.MM \citeMukherjeeSchapire2013. We show that the training error of AdaBoost.Iter vanishes at the exponential rate, while the generalization error converges to zero at the same rate as AdaBoost. AdaBoost.Iter numerically outperforms SAMME and achieves performance comparable to AdaBoost.MM on benchmark datasets. | InKoo Cho, Jonathan A. Libgober, Cheng Ding | Emory University, ATLANTA, GA, USA; Emory University & Hanyang University, Atlanta, GA, USA; University of Southern California, Los Angeles, CA, USA |
|  |  [Divide and Denoise: Empowering Simple Models for Robust Semi-Supervised Node Classification against Label Noise](https://doi.org/10.1145/3637528.3671798) |  | 0 | Graph neural networks (GNNs) based on message passing have achieved remarkable performance in graph machine learning. By combining it with the power of pseudo labeling, one can further push forward the performance on the task of semi-supervised node classification. However, most existing works assume that the training node labels are purely noise-free, while this strong assumption usually does not hold in practice. GNNs will overfit the noisy training labels and the adverse effects of mislabeled nodes can be exaggerated by being propagated to the remaining nodes through the graph structure, exacerbating the model failure. Worse still, the noisy pseudo labels could also largely undermine the model's reliability without special treatment. In this paper, we revisit the role of (1) message passing and (2) pseudo labels in the studied problem and try to address two denoising subproblems from the model architecture and algorithm perspective, respectively. Specifically, we first develop a label-noise robust GNN that discards the coupled message-passing scheme. Despite its simple architecture, this learning backbone prevents overfitting to noisy labels and also inherently avoids the noise propagation issue. Moreover, we propose a novel reliable graph pseudo labeling algorithm that can effectively leverage the knowledge of unlabeled nodes while mitigating the adverse effects of noisy pseudo labels. Based on those novel designs, we can attain exceptional effectiveness and efficiency in solving the studied problem. We conduct extensive experiments on benchmark datasets for semi-supervised node classification with different levels of label noise and show new state-of-the-art performance. The code is available at https://github.com/DND-NET/DND-NET. | Kaize Ding, Xiaoxiao Ma, Yixin Liu, Shirui Pan | Macquarie University, Sydney, Australia; Monash University, Melbourne, Australia; Griffith University, Gold Coast, Australia; Northwestern University, Evanston, USA |
|  |  [Unraveling Block Maxima Forecasting Models with Counterfactual Explanation](https://doi.org/10.1145/3637528.3671923) |  | 0 | Disease surveillance, traffic management, and weather forecasting are some of the key applications that could benefit from block maxima forecasting of a time series as the extreme block maxima values often signify events of critical importance such as disease outbreaks, traffic gridlock, and severe weather conditions. As the use of deep neural network models for block maxima forecasting increases, so does the need for explainable AI methods that could unravel the inner workings of such black box models. To fill this need, this paper presents a novel counterfactual explanation framework for block maxima forecasting models. Unlike existing methods, our proposed framework, DiffusionCF, combines deep anomaly detection with a conditional diffusion model to identify unusual patterns in the time series that could help explain the forecasted extreme block maxima. Experimental results on several real-world datasets demonstrate the superiority of DiffusionCF over other baseline methods when evaluated according to various metrics, particularly their informativeness and closeness. Our data and codes are available at https://github.com/yue2023cs/DiffusionCF. | Yue Deng, Asadullah Hill Galib, PangNing Tan, Lifeng Luo | Michigan State University, East Lansing, MI, USA |
|  |  [Explanatory Model Monitoring to Understand the Effects of Feature Shifts on Performance](https://doi.org/10.1145/3637528.3671959) |  | 0 | Monitoring and maintaining machine learning models are among the most critical challenges in translating recent advances in the field into real-world applications. However, current monitoring methods lack the capability of provide actionable insights answering the question of why the performance of a particular model really degraded. In this work, we propose a novel approach to explain the behavior of a black-box model under feature shifts by attributing an estimated performance change to interpretable input characteristics. We refer to our method that combines concepts from Optimal Transport and Shapley Values as Explanatory Performance Estimation (XPE). We analyze the underlying assumptions and demonstrate the superiority of our approach over several baselines on different data sets across various data modalities such as images, audio, and tabular data. We also indicate how the generated results can lead to valuable insights, enabling explanatory model monitoring by revealing potential root causes for model deterioration and guiding toward actionable countermeasures. | Thomas Decker, Alexander Koebler, Michael Lebacher, Ingo Thon, Volker Tresp, Florian Buettner | Siemens AG, Munich, Germany; Ludwig-Maximilians-Universität & Siemens AG, Munich, Germany; Goethe University Frankfurt & Siemens AG, Frankfurt, Germany; Ludwig-Maximilians-Universität & Munich Center for Machine Learning, Munich, Germany |
|  |  [Fast Unsupervised Deep Outlier Model Selection with Hypernetworks](https://doi.org/10.1145/3637528.3672003) |  | 0 | Deep neural network based Outlier Detection (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HP settings, the issue is ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled outliers), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding to varying HPs), which offers significant speed-up. In addition, it employs meta-learning on historical OD tasks with labels to train a proxy validation function, likewise trained with our proposed HN efficiently. Extensive experiments on different OD tasks show that HYPER achieves competitive performance against 8 baselines with significant efficiency gains. | Xueying Ding, Yue Zhao, Leman Akoglu | University of Southern California, Los Angeles, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA |
|  |  [Enhancing On-Device LLM Inference with Historical Cloud-Based LLM Interactions](https://doi.org/10.1145/3637528.3671679) |  | 0 | Many billion-scale large language models (LLMs) have been released for resource-constraint mobile devices to provide local LLM inference service when cloud-based powerful LLMs are not available. However, the capabilities of current on-device LLMs still lag behind those of cloud-based LLMs, and how to effectively and efficiently enhance on-device LLM inference becomes a practical requirement. We thus propose to collect the user's historical interactions with the cloud-based LLM and build an external datastore on the mobile device for enhancement using nearest neighbors search. Nevertheless, the full datastore improves the quality of token generation at the unacceptable expense of much slower generation speed. To balance performance and efficiency, we propose to select an optimal subset of the full datastore within the given size limit, the optimization objective of which is proven to be submodular. We further design an offline algorithm, which selects the subset after the construction of the full datastore, as well as an online algorithm, which performs selection over the stream and can be flexibly scheduled. We theoretically analyze the performance guarantee and the time complexity of the offline and the online designs to demonstrate effectiveness and scalability. We finally take three ChatGPT related dialogue datasets and four different on-device LLMs for evaluation. Evaluation results show that the proposed designs significantly enhance LLM performance in terms of perplexity while maintaining fast token generation speed. Practical overhead testing on the smartphone reveal the efficiency of on-device datastore subset selection from memory usage and computation overhead. | Yucheng Ding, Chaoyue Niu, Fan Wu, Shaojie Tang, Chengfei Lyu, Guihai Chen | University of Texas at Dallas, Richardson, Texas, USA; Shanghai Jiao Tong University, Shanghai, China; Alibaba Group, Hangzhou, China |
|  |  [IDEA: A Flexible Framework of Certified Unlearning for Graph Neural Networks](https://doi.org/10.1145/3637528.3671744) |  | 0 | Graph Neural Networks (GNNs) have been increasingly deployed in a plethora of applications. However, the graph data used for training may contain sensitive personal information of the involved individuals. Once trained, GNNs typically encode such information in their learnable parameters. As a consequence, privacy leakage may happen when the trained GNNs are deployed and exposed to potential attackers. Facing such a threat, machine unlearning for GNNs has become an emerging technique that aims to remove certain personal information from a trained GNN. Among these techniques, certified unlearning stands out, as it provides a solid theoretical guarantee of the information removal effectiveness. Nevertheless, most of the existing certified unlearning methods for GNNs are only designed to handle node and edge unlearning requests. Meanwhile, these approaches are usually tailored for either a specific design of GNN or a specially designed training objective. These disadvantages significantly jeopardize their flexibility. In this paper, we propose a principled framework named IDEA to achieve flexible and certified unlearning for GNNs. Specifically, we first instantiate four types of unlearning requests on graphs, and then we propose an approximation approach to flexibly handle these unlearning requests over diverse GNNs. We further provide theoretical guarantee of the effectiveness for the proposed approach as a certification. Different from existing alternatives, IDEA is not designed for any specific GNNs or optimization objectives to perform certified unlearning, and thus can be easily generalized. Extensive experiments on real-world datasets demonstrate the superiority of IDEA in multiple key perspectives. | Yushun Dong, Binchi Zhang, Zhenyu Lei, Na Zou, Jundong Li | The University of Houston, Houston, TX, USA; The University of Virginia, Charlottesville, USA; The University of Virginia, Charlottesville, VA, USA |
|  |  [Unsupervised Alignment of Hypergraphs with Different Scales](https://doi.org/10.1145/3637528.3671955) |  | 0 | People usually interact in groups, and such groups may appear on different platforms. For instance, people often create various group chats on messaging apps (e.g., Facebook Messenger and WhatsApp) to communicate with families, friends, or colleagues. How do we identify the same people across the two platforms based on the information about the groups? This gives rise to the hypergraph alignment problem, whose objective is to find the correspondences between the sets of nodes of two hypergraphs. In a hypergraph, a node represents a person, and each hyperedge represents a group of several people. In addition, the two sets of hyperedges in the two hypergraphs can vary significantly in scales as people may use different apps at different time periods. In this work, we propose and tackle the problem of unsupervised hypergraph alignment. Given two hypergraphs with potentially different scales and without any side information or prior ground-truth correspondences, we develop ØurMethod, a learning framework, to find node correspondences across the two hypergraphs. ØurMethod directly addresses each challenge of the problem. In particular, it (a) extracts node features from the hypergraph topology, (b) employs contrastive learning, as a "supervised pseudo-alignment'' task to pre-train the learning model (c) applies topological augmentation to help a generative adversarial network to align the two embedding spaces from the two hypergraphs. The purpose of augmentation is to add virtual hyperedges from one hypergraph in order to the other to resolve the scale difference and share information across the two hypergraphs. Our extensive experiments on 12 real-world datasets demonstrate the significant and consistent superiority of ØurMethod over the baseline approaches. | Manh Tuan Do, Kijung Shin | KAIST, Seoul, Republic of Korea |
|  |  [Heterogeneity-Informed Meta-Parameter Learning for Spatiotemporal Time Series Forecasting](https://doi.org/10.1145/3637528.3671961) |  | 0 | Spatiotemporal time series forecasting plays a key role in a wide range of real-world applications. While significant progress has been made in this area, fully capturing and leveraging spatiotemporal heterogeneity remains a fundamental challenge. Therefore, we propose a novel Heterogeneity-Informed Meta-Parameter Learning scheme. Specifically, our approach implicitly captures spatiotemporal heterogeneity through learning spatial and temporal embeddings, which can be viewed as a clustering process. Then, a novel spatiotemporal meta-parameter learning paradigm is proposed to learn spatiotemporal-specific parameters from meta-parameter pools, which is informed by the captured heterogeneity. Based on these ideas, we develop a Heterogeneity-Informed Spatiotemporal Meta-Network (HimNet) for spatiotemporal time series forecasting. Extensive experiments on five widely-used benchmarks demonstrate our method achieves state-of-the-art performance while exhibiting superior interpretability. Our code is available at https://github.com/XDZhelheim/HimNet. | Zheng Dong, Renhe Jiang, Haotian Gao, Hangchen Liu, Jinliang Deng, Qingsong Wen, Xuan Song | Hong Kong University of Science and Technology, Hong Kong, China; Squirrel AI, Seattle, USA; The University of Tokyo, Tokyo, Japan; Southern University of Science and Technology, Shenzhen, China; Jilin University & Southern University of Science and Technology, Changchun, China |
|  |  [Representation Learning of Temporal Graphs with Structural Roles](https://doi.org/10.1145/3637528.3671854) |  | 0 | Temporal graph representation learning has drawn considerable attention in recent years. Most existing works mainly focus on modeling local structural dependencies of temporal graphs. However, underestimating the inherent global structural role information in many real-world temporal graphs inevitably leads to sub-optimal graph representations. To overcome this shortcoming, we propose a novel Role-based Temporal Graph Convolution Network (RTGCN) that fully leverages the global structural role information in temporal graphs. Specifically, RTGCN can effectively capture the static global structural roles by using hypergraph convolution neural networks. To capture the evolution of nodes' structural roles, we further design structural role-based gated recurrent units. Finally, we integrate structural role proximity in our objective function to preserve global structural similarity, further promoting temporal graph representation learning. Experimental results on multiple real-world datasets demonstrate that RTGCN consistently outperforms state-of-the-art temporal graph representation learning methods by significant margins in various temporal link prediction and node classification tasks. Specifically, RTGCN achieves AUC improvement of up to 5.1% for link prediction and F1 improvement of up to 6.2% for new link prediction. In addition, RTGCN achieves AUC improvement up to 4.6% for node classification and 2.7% for structural role classification. | Huaming Du, Long Shi, Xingyan Chen, Yu Zhao, Hegui Zhang, Carl Yang, Fuzhen Zhuang, Gang Kou | Department of Computer Science, Emory University, Atlanta, Georgia, USA |
|  |  [Reserving-Masking-Reconstruction Model for Self-Supervised Heterogeneous Graph Representation](https://doi.org/10.1145/3637528.3671719) |  | 0 | Self-supervised Heterogeneous Graph Representation (SSHGRL) learning is widely used in data mining. The latest SSHGRL methods normally use metapaths to describe the heterogeneous information (multiple relations and node types) to learn the heterogeneous graph representation and achieve impressive results. However, establishing metapaths requires lofty computational costs that are too high for the medium and large graphs. To this end, this paper proposes a Reserving-Masking-Reconstruction (RMR) model that can fully consider heterogeneous information without relying on the metapaths. In detail, we propose a reserving method to reserve to-be-masked nodes' (target nodes) information before graph masking. Second, we split the reserved graph into relation subgraphs according to the type of relations that require much less computational overheads than metapath. Then, the target nodes in each relation subgraph are randomly masked with minimal topology information loss. After, a novel reconstruction method is proposed to reconstruct the masked nodes on different relation subgraphs to establish the self-supervised signal. The proposed method requires low computational complexity and can establish a self-supervised signal without deeply changing the graph topology. Experimental results show the proposed method achieves state-of-the-art records on medium and large-scale heterogeneous graphs and competitive records on small-scale heterogeneous graphs. The code is available at https://github.com/DuanhaoranCC/RMR. | Haoran Duan, Cheng Xie, Linyu Li | Yunnan University, Kunming, China |
|  |  [Pre-Training Identification of Graph Winning Tickets in Adaptive Spatial-Temporal Graph Neural Networks](https://doi.org/10.1145/3637528.3671912) |  | 0 | In this paper, we present a novel method to significantly enhance the computational efficiency of Adaptive Spatial-Temporal Graph Neural Networks (ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived from the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star topology as a GWT prior to training, we balance edge reduction with efficient information propagation, reducing computational demands while maintaining high model performance. Both the time and memory computational complexity of generating adaptive spatial-temporal graphs is significantly reduced from O(N2) to O(N). Our approach streamlines the ASTGNN deployment by eliminating the need for exhaustive training, pruning, and retraining cycles, and demonstrates empirically across various datasets that it is possible to achieve comparable performance to full models with substantially lower computational costs. Specifically, our approach enables training ASTGNNs on the largest scale spatial-temporal dataset using a single A6000 equipped with 48 GB of memory, overcoming the out-of-memory issue encountered during original training and even achieving state-of-the-art performance. Furthermore, we delve into the effectiveness of the GWT from the perspective of spectral graph theory, providing substantial theoretical support. This advancement not only proves the existence of efficient sub-networks within ASTGNNs but also broadens the applicability of the LTH in resource-constrained settings, marking a significant step forward in the field of graph neural networks. Code is available at https://anonymous.4open.science/r/paper-1430. | Wenying Duan, Tianxiang Fang, Hong Rao, Xiaoxi He | Nanchang University; University of Macau |
|  |  [Auctions with LLM Summaries](https://doi.org/10.1145/3637528.3672022) |  | 0 | We study an auction setting in which bidders bid for placement of theircontent within a summary generated by a large language model (LLM), e.g., an adauction in which the display is a summary paragraph of multiple ads. Thisgeneralizes the classic ad settings such as position auctions to an LLMgenerated setting, which allows us to handle general display formats. Wepropose a novel factorized framework in which an auction module and an LLMmodule work together via a prediction model to provide welfare maximizingsummary outputs in an incentive compatible manner. We provide a theoreticalanalysis of this framework and synthetic experiments to demonstrate thefeasibility and validity of the system together with welfare comparisons. | Avinava Dubey, Zhe Feng, Rahul Kidambi, Aranyak Mehta, Di Wang | Google Research, Mountain View, CA, USA |
|  |  [GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models](https://doi.org/10.1145/3637528.3672035) |  | 0 | This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM. | Yi Fang, Dongzhe Fan, Daochen Zha, Qiaoyu Tan | Department of Computer Science, Rice University, Huston, USA; SFSC of AI and DL, New York University (Shanghai), Shanghai, China |
|  |  [CAT: Interpretable Concept-based Taylor Additive Models](https://doi.org/10.1145/3637528.3672020) |  | 0 | As an emerging interpretable technique, Generalized Additive Models (GAMs) adopt neural networks to individually learn non-linear functions for each feature, which are then combined through a linear model for final predictions. Although GAMs can explain deep neural networks (DNNs) at the feature level, they require large numbers of model parameters and are prone to overfitting, making them hard to train and scale. Additionally, in real-world datasets with many features, the interpretability of feature-based explanations diminishes for humans. To tackle these issues, recent research has shifted towards concept-based interpretable methods. These approaches try to integrate concept learning as an intermediate step before making predictions, explaining the predictions in terms of human-understandable concepts. However, these methods require domain experts to extensively label concepts with relevant names and their ground-truth values. In response, we propose CAT, a novel interpretable Concept-bAsed Taylor additive model to simplify this process. CAT does not require domain experts to annotate concepts and their ground-truth values. Instead, it only requires users to simply categorize input features into broad groups, which can be easily accomplished through a quick metadata review. Specifically, CAT first embeds each group of input features into one-dimensional high-level concept representation, and then feeds the concept representations into a new white-box Taylor Neural Network (TaylorNet). The TaylorNet aims to learn the non-linear relationship between the inputs and outputs using polynomials. Evaluation results across multiple benchmarks demonstrate that CAT can outperform or compete with the baselines while reducing the need of extensive model parameters. Importantly, it can effectively explain model predictions through high-level concepts. Source code is available at github.com/vduong143/CAT-KDD-2024. | Viet Duong, Qiong Wu, Zhengyi Zhou, Hongjue Zhao, Chenxiang Luo, Eric Zavesky, Huaxiu Yao, Huajie Shao | University of Illinois at Urbana-Champaign, Champaign, IL, USA; William & Mary, Williamsburg, VA, USA; The University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; AT&T Labs, Austin, TX, USA; AT&T Labs, Bedminster, NJ, USA |
|  |  [SensitiveHUE: Multivariate Time Series Anomaly Detection by Enhancing the Sensitivity to Normal Patterns](https://doi.org/10.1145/3637528.3671919) |  | 0 | Unsupervised anomaly detection in multivariate time series (MTS) has always been a challenging problem, and the modeling based on reconstruction has garnered significant attention. The insensitivity of these methods towards normal patterns poses challenges in distinguishing between normal and abnormal points. Firstly, the general reconstruction strategies may exhibit limited sensitivity to spatio-temporal dependencies, and their performance remains largely unaffected by such dependencies. Secondly, most methods fail to model the heteroscedastic uncertainty in MTS, hindering their abilities to derive a distinguishable criterion. For instance, normal data with high noise levels may lead to detection failure due to excessively high reconstruction errors. In this work, we emphasize the necessity of sensitivity to normal patterns, which could improve the discrimination between normal and abnormal points remarkably. To this end, we propose SensitiveHUE, a probabilistic network by implementing both reconstruction and heteroscedastic uncertainty estimation. Its core includes a statistical feature removal strategy to ensure the dependency sensitive property, and a novel MTS-NLL loss for modeling the normal patterns in important regions. Experimental results demonstrate that SensitiveHUE exhibits nontrivial sensitivity to normal patterns and outperforms the existing state-of-the-art alternatives by a large margin. Code is publicly available at this URL\footnotehttp://github.com/yuesuoqingqiu/SensitiveHUE. | Yuye Feng, Wei Zhang, Yao Fu, Weihao Jiang, Jiang Zhu, Wenqi Ren | Hikvision Research Institute, Hangzhou, China |
|  |  [Communication-efficient Multi-service Mobile Traffic Prediction by Leveraging Cross-service Correlations](https://doi.org/10.1145/3637528.3671730) |  | 0 | Mobile traffic prediction plays a crucial role in enabling efficient network management and service provisioning. Traditional prediction approaches treat different mobile application services (such as Uber, Facebook, Twitter, etc) as isolated entities, neglecting potential correlation among them. Moreover, such isolated prediction methods necessitate the uploading of historical traffic data from all regions to forecast city-wide traffic, resulting in consuming substantial bandwidth resources and risking prediction failure in the event of data loss in specific regions. To address these challenges, we propose a novel Cross-service Attention-based Spatial-Temporal Graph Convolutional Network (CsASTGCN) for precise and communication-efficient multi-service mobile traffic prediction. Our methodology allows each mobile service to transmit the traffic data of only a fraction of regions for city-wide traffic prediction of all mobile services, which reduces the resource consumption caused by data transmission. Specifically, the sparse traffic data are initially transmitted to the cloud server and the masked graph autoencoder is utilized to roughly reconstruct the traffic volume for regions with missing data. Subsequently, a cross-service attention-based predictor is designed to calculate the data correlation among different mobile services within the same region. Considering the constantly emerging mobile services, we incorporate a novel model-based adaptive transfer learning scheme to extract valuable knowledge from the existing models and expedite the training of a new model for a new service without training from scratch, thereby enhancing the scalability of our framework. Extensive experiments conducted on a large-scale real-world mobile traffic dataset demonstrate that our model greatly outperforms the existing schemes, enhancing both the communication-efficiency and robustness of large-scale multi-service traffic prediction. | Zhiying Feng, Qiong Wu, Xu Chen | School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China |
|  |  [Federated Graph Learning with Structure Proxy Alignment](https://doi.org/10.1145/3637528.3671717) |  | 0 | Federated Graph Learning (FGL) aims to learn graph learning models over graph data distributed in multiple data owners, which has been applied in various applications such as social recommendation and financial fraud detection. Inherited from generic Federated Learning (FL), FGL similarly has the data heterogeneity issue where the label distribution may vary significantly for distributed graph data across clients. For instance, a client can have the majority of nodes from a class, while another client may have only a few nodes from the same class. This issue results in divergent local objectives and impairs FGL convergence for node-level tasks, especially for node classification. Moreover, FGL also encounters a unique challenge for the node classification task: the nodes from a minority class in a client are more likely to have biased neighboring information, which prevents FGL from learning expressive node embeddings with Graph Neural Networks (GNNs). To grapple with the challenge, we propose FedSpray, a novel FGL framework that learns local class-wise structure proxies in the latent space and aligns them to obtain global structure proxies in the server. Our goal is to obtain the aligned structure proxies that can serve as reliable, unbiased neighboring information for node classification. To achieve this, FedSpray trains a global feature-structure encoder and generates unbiased soft targets with structure proxies to regularize local training of GNN models in a personalized way. We conduct extensive experiments over four datasets, and experiment results validate the superiority of FedSpray compared with other baselines. Our code is available at https://github.com/xbfu/FedSpray. | Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, Jundong Li | University of Virginia, Charlottesville, Virginia, USA |
|  |  [Policy-Based Bayesian Active Causal Discovery with Deep Reinforcement Learning](https://doi.org/10.1145/3637528.3671705) |  | 0 | Causal discovery with observational and interventional data plays an important role in numerous fields. Due to the costly and potentially risky nature of intervention experiments, selecting informative interventions is critical in real-world situations. Several recent works introduce Bayesian active learning to select interventions that maximize the expected information gain about the underlying causal relationship at each optimization step. However, there are still some limitations within these methods: (1) Local optimality. With multiple intervention experiments, selecting optimal intervention myopically at each step may drop into the local optimal point. (2) Expensive time cost. Optimizing the most informative intervention at each step is time-consuming and not suitable for adaptive experiments with strict inference speed requirements. In this study, we propose a novel method called Reinforcement Learning-based Causal Bayesian Experimental Design (RL-CBED) to reduce the risk of local optimality and accelerate intervention selection inference. Specifically, we formulate the active causal discovery problem as a partially observable Markov decision process (POMDP). We design an information gain-based sparse reward function and then improve it to a dense reward function, providing fine-grained feedback to help the RL policy learn more quickly in complex environments. Moreover, we theoretically prove that the Q-function estimator can be learned using only trajectories sampled from the prior, which can significantly reduce the time cost of training process, enabling the real-world application of our method. Extensive experiments on both synthetic and real world-inspired semi-synthetic datasets demonstrate the effectiveness of our proposed method. | Heyang Gao, Zexu Sun, Hao Yang, Xu Chen | Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Graph Condensation for Open-World Graph Learning](https://doi.org/10.1145/3637528.3671917) |  | 0 | The burgeoning volume of graph data presents significant computational challenges in training graph neural networks (GNNs), critically impeding their efficiency in various applications. To tackle this challenge, graph condensation (GC) has emerged as a promising acceleration solution, focusing on the synthesis of a compact yet representative graph for efficiently training GNNs while retaining performance. Despite the potential to promote scalable use of GNNs, existing GC methods are limited to aligning the condensed graph with merely the observed static graph distribution. This limitation significantly restricts the generalization capacity of condensed graphs, particularly in adapting to dynamic distribution changes. In real-world scenarios, however, graphs are dynamic and constantly evolving, with new nodes and edges being continually integrated. Consequently, due to the limited generalization capacity of condensed graphs, applications that employ GC for efficient GNN training end up with sub-optimal GNNs when confronted with evolving graph structures and distributions in dynamic real-world situations. To overcome this issue, we propose open-world graph condensation (OpenGC), a robust GC framework that integrates structure-aware distribution shift to simulate evolving graph patterns and exploit the temporal environments for invariance condensation. This approach is designed to extract temporal invariant patterns from the original graph, thereby enhancing the generalization capabilities of the condensed graph and, subsequently, the GNNs trained on it. Furthermore, to support the periodic re-condensation and expedite condensed graph updating in life-long graph learning, OpenGC reconstructs the sophisticated optimization scheme with kernel ridge regression and non-parametric graph convolution, significantly accelerating the condensation process while ensuring the exact solutions. Extensive experiments on both real-world and synthetic evolving graphs demonstrate that OpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic changes in open-world graph environments. | Xinyi Gao, Tong Chen, Wentao Zhang, Yayong Li, Xiangguo Sun, Hongzhi Yin | Data 61, CSIRO, Brisbane, Australia; The University of Queensland, Brisbane, Australia; The Chinese University of Hong Kong, Hong Kong, China; Peking University, Beijing, China |
|  |  [PATE: Proximity-Aware Time Series Anomaly Evaluation](https://doi.org/10.1145/3637528.3671971) |  | 0 | Evaluating anomaly detection algorithms in time series data is critical asinaccuracies can lead to flawed decision-making in various domains wherereal-time analytics and data-driven strategies are essential. Traditionalperformance metrics assume iid data and fail to capture the complex temporaldynamics and specific characteristics of time series anomalies, such as earlyand delayed detections. We introduce Proximity-Aware Time series anomalyEvaluation (PATE), a novel evaluation metric that incorporates the temporalrelationship between prediction and anomaly intervals. PATE usesproximity-based weighting considering buffer zones around anomaly intervals,enabling a more detailed and informed assessment of a detection. Using theseweights, PATE computes a weighted version of the area under the Precision andRecall curve. Our experiments with synthetic and real-world datasets show thesuperiority of PATE in providing more sensible and accurate evaluations thanother evaluation metrics. We also tested several state-of-the-art anomalydetectors across various benchmark datasets using the PATE evaluation scheme.The results show that a common metric like Point-Adjusted F1 Score fails tocharacterize the detection performances well, and that PATE is able to providea more fair model comparison. By introducing PATE, we redefine theunderstanding of model efficacy that steers future studies toward developingmore effective and accurate detection models. | Ramin Ghorbani, Marcel J. T. Reinders, David M. J. Tax | Delft University of Technology, Delft, Netherlands |
|  |  [Hierarchical Neural Constructive Solver for Real-world TSP Scenarios](https://doi.org/10.1145/3637528.3672053) |  | 0 | Existing neural constructive solvers for routing problems have predominantly employed transformer architectures, conceptualizing the route construction as a set-to-sequence learning task. However, their efficacy has primarily been demonstrated on entirely random problem instances that inadequately capture real-world scenarios. In this paper, we introduce realistic Traveling Salesman Problem (TSP) scenarios relevant to industrial settings and derive the following insights: (1) The optimal next node (or city) to visit often lies within proximity to the current node, suggesting the potential benefits of biasing choices based on current locations. (2) Effectively solving the TSP requires robust tracking of unvisited nodes and warrants succinct grouping strategies. Building upon these insights, we propose integrating a learnable choice layer inspired by Hypernetworks to prioritize choices based on the current location, and a learnable approximate clustering algorithm inspired by the Expectation-Maximization algorithm to facilitate grouping the unvisited cities. Together, these two contributions form a hierarchical approach towards solving the realistic TSP by considering both immediate local neighbourhoods and learning an intermediate set of node representations. Our hierarchical approach yields superior performance compared to both classical and recent transformer models, showcasing the efficacy of the key designs. | Yong Liang Goh, Zhiguang Cao, Yining Ma, Yanfei Dong, Mohammed Haroon Dupty, Wee Sun Lee | Grabtaxi Holdings Pte Ltd & National University of Singapore, Singapore, Singapore; Singapore Management University, Singapore, Singapore; National University of Singapore, Singapore, Singapore |
|  |  [An Energy-centric Framework for Category-free Out-of-distribution Node Detection in Graphs](https://doi.org/10.1145/3637528.3671939) |  | 0 | Graph neural networks have garnered notable attention for effectively processing graph-structured data. Prevalent models prioritize improving in-distribution (IND) data performance, frequently overlooking the risks from potential out-of-distribution (OOD) nodes during training and inference. In real-world graphs, the automated network construction can introduce noisy nodes from unknown distributions. Previous research into OOD node detection, typically referred to as entropy-based methods, calculates OOD measurements from the prediction entropy alongside category classification training. However, the nodes in the graph might not be pre-labeled with specific categories, rendering entropy-based OOD detectors inapplicable in such category-free situations. To tackle this issue, we propose an energy-centric density estimation framework for OOD node detection, referred to as EnergyDef. Within this framework, we introduce an energy-based GNN to compute node energies that act as indicators of node density and reveal the OOD uncertainty of nodes. Importantly, EnergyDef can efficiently identify OOD nodes with low-resource OOD node annotations, achieved by sampling hallucinated nodes via Langevin Dynamics and structure estimation, along with training through Contrastive Divergence. Our comprehensive experiments on real-world datasets substantiate that our framework markedly surpasses state-of-the-art methods in terms of detection quality, even under conditions of scarce or entirely absent OOD node annotations. | Zheng Gong, Ying Sun |  |
|  |  [Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective](https://doi.org/10.1145/3637528.3671792) |  | 0 | Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two model-agnostic perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known GNN model architectures on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism and the decoupled architecture contribute positively to graph OOD generalization. In contrast, we observe that the linear classification layer tends to compromise graph OOD generalization capability. Furthermore, we provide in-depth theoretical insights and discussions to underpin these discoveries. These insights have empowered us to develop a novel GNN backbone model, DGat, designed to harness the robust properties of both graph self-attention mechanism and the decoupled architecture. Extensive experimental results demonstrate the effectiveness of our model under graph OOD, exhibiting substantial and consistent enhancements across various training strategies. Our codes are available at https://github.com/KaiGuo20/DGAT \*\*REMOVE 2nd URL\*\*://github.com/KaiGuo20/DGAT. | Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang | School of Artificial Intelligence, Jilin University, Changchun, Jilan, China; Department of Computer Science, Emory University, Atlanta, GA, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA |
|  |  [HiFGL: A Hierarchical Framework for Cross-silo Cross-device Federated Graph Learning](https://doi.org/10.1145/3637528.3671660) |  | 0 | Federated Graph Learning (FGL) has emerged as a promising way to learnhigh-quality representations from distributed graph data with privacypreservation. Despite considerable efforts have been made for FGL under eithercross-device or cross-silo paradigm, how to effectively capture graph knowledgein a more complicated cross-silo cross-device environment remains anunder-explored problem. However, this task is challenging because of theinherent hierarchy and heterogeneity of decentralized clients, diversifiedprivacy constraints in different clients, and the cross-client graph integrityrequirement. To this end, in this paper, we propose a Hierarchical FederatedGraph Learning (HiFGL) framework for cross-silo cross-device FGL. Specifically,we devise a unified hierarchical architecture to safeguard federated GNNtraining on heterogeneous clients while ensuring graph integrity. Moreover, wepropose a Secret Message Passing (SecMP) scheme to shield unauthorized accessto subgraph-level and node-level sensitive information simultaneously.Theoretical analysis proves that HiFGL achieves multi-level privacypreservation with complexity guarantees. Extensive experiments on real-worlddatasets validate the superiority of the proposed framework against severalbaselines. Furthermore, HiFGL's versatile nature allows for its application ineither solely cross-silo or cross-device settings, further broadening itsutility in real-world FGL applications. | Zhuoning Guo, Duanyi Yao, Qiang Yang, Hao Liu | The Hong Kong University of Science and Technology, Hong Kong, Hong Kong |
|  |  [AnyLoss: Transforming Classification Metrics into Loss Functions](https://doi.org/10.1145/3637528.3672017) |  | 0 | Many evaluation metrics can be used to assess the performance of models inbinary classification tasks. However, most of them are derived from a confusionmatrix in a non-differentiable form, making it very difficult to generate adifferentiable loss function that could directly optimize them. The lack ofsolutions to bridge this challenge not only hinders our ability to solvedifficult tasks, such as imbalanced learning, but also requires the deploymentof computationally expensive hyperparameter search processes in modelselection. In this paper, we propose a general-purpose approach that transformsany confusion matrix-based metric into a loss function, AnyLoss, thatis available in optimization processes. To this end, we use an approximationfunction to make a confusion matrix represented in a differentiable form, andthis approach enables any confusion matrix-based metric to be directly used asa loss function. The mechanism of the approximation function is provided toensure its operability and the differentiability of our loss functions isproved by suggesting their derivatives. We conduct extensive experiments underdiverse neural networks with many datasets, and we demonstrate their generalavailability to target any confusion matrix-based metrics. Our method,especially, shows outstanding achievements in dealing with imbalanced datasets,and its competitive learning speed, compared to multiple baseline models,underscores its efficiency. | Do Heon Han, Nuno Moniz, Nitesh V. Chawla | Lucy Family Institute for Data & Society, University of Notre Dame, Notre Dame, IN, USA |
|  |  [Expander Hierarchies for Normalized Cuts on Graphs](https://doi.org/10.1145/3637528.3671978) |  | 0 | Expander decompositions of graphs have significantly advanced theunderstanding of many classical graph problems and led to numerous fundamentaltheoretical results. However, their adoption in practice has been hindered dueto their inherent intricacies and large hidden factors in their asymptoticrunning times. Here, we introduce the first practically efficient algorithm forcomputing expander decompositions and their hierarchies and demonstrate itseffectiveness and utility by incorporating it as the core component in a novelsolver for the normalized cut graph clustering objective. Our extensive experiments on a variety of large graphs show that ourexpander-based algorithm outperforms state-of-the-art solvers for normalizedcut with respect to solution quality by a large margin on a variety of graphclasses such as citation, e-mail, and social networks or web graphs whileremaining competitive in running time. | Kathrin Hanauer, Monika Henzinger, Robin Münk, Harald Räcke, Maximilian Vötsch | Technical University of Munich, Munich, Germany; Faculty of Computer Science, University of Vienna, Vienna, Austria; Institute of Science and Technology Austria (ISTA), Klosterneuburg, Austria |
|  |  [Model-Agnostic Random Weighting for Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671762) |  | 0 | Despite the encouraging successes in numerous applications, machine learning methods grounded on the i.i.d. assumption often experience performance deterioration when confronted with the distribution shift between training and test data. This challenge has instigated recent research endeavors focusing on out-of-distribution (OOD) generalization. A particularly pervasive and intricate OOD problem is to enhance the model's generalization ability by training it on samples drawn from a single environment. In response to the problem, we propose a simple model-agnostic method tailored for a practical OOD scenario in this paper. Our approach centers on pursuing robust weighted empirical risks, utilizing randomly shifted training distributions derived through a specific sample-based weighting strategy. Furthermore, we theoretically establish that the expected risk of the shifted training distribution can bound the expected risk of the test distribution. This theoretical foundation ensures the improved prediction performance of our method when employed in uncertain test distributions. Extensive experiments conducted on diverse real-world datasets affirm the effectiveness of our method, highlighting its potential to address the distribution shifts in machine learning applications. | Yue He, Pengfei Tian, Renzhe Xu, Xinwei Shen, Xingxuan Zhang, Peng Cui | Tsinghua University, Beijing, China; ETH Zürich, Zürich, Switzerland |
|  |  [RoutePlacer: An End-to-End Routability-Aware Placer with Graph Neural Network](https://doi.org/10.1145/3637528.3671895) |  | 0 | Placement is a critical and challenging step of modern chip design, with routability being an essential indicator of placement quality. Current routability-oriented placers typically apply an iterative two-stage approach, wherein the first stage generates a placement solution, and the second stage provides non-differentiable routing results to heuristically improve the solution quality. This method hinders jointly optimizing the routability aspect during placement. To address this problem, this work introduces RoutePlacer, an end-to-end routability-aware placement method. It trains RouteGNN, a customized graph neural network, to efficiently and accurately predict routability by capturing and fusing geometric and topological representations of placements. Well-trained RouteGNN then serves as a differentiable approximation of routability, enabling end-to-end gradient-based routability optimization. In addition, RouteGNN can improve two-stage placers as a plug-and-play alternative to external routers. Our experiments on DREAMPlace, an open-source AI4EDA platform, show that RoutePlacer can reduce Total Overflow by up to 16% while maintaining routed wirelength, compared to the state-of-the-art; integrating RouteGNN within two-stage placers leads to a 44% reduction in Total Overflow without compromising wirelength. | Yunbo Hou, Haoran Ye, Yingxue Zhang, Siyuan Xu, Guojie Song | Huawei Noah's Ark Lab, Markham, Canada; Huawei Noah's Ark Lab, Shenzhen, China; School of Software and Microelectronics, Peking University, Beijing, China |
|  |  [Is Aggregation the Only Choice? Federated Learning via Layer-wise Model Recombination](https://doi.org/10.1145/3637528.3671722) |  | 0 | Although Federated Learning (FL) enables global model training across clients without compromising their raw data, due to the unevenly distributed data among clients, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance. Specifically, different data distributions among clients lead to various optimization directions of local models. Aggregating local models usually results in a low-generalized global model, which performs worse on most of the clients. To address the above issue, inspired by the observation from a geometric perspective that a well-generalized solution is located in a flat area rather than a sharp area, we propose a novel and heuristic FL paradigm named FedMR (Federated Model Recombination). The goal of FedMR is to guide the recombined models to be trained towards a flat area. Unlike conventional FedAvg-based methods, in FedMR, the cloud server recombines collected local models by shuffling each layer of them to generate multiple recombined models for local training on clients rather than an aggregated global model. Since the area of the flat area is larger than the sharp area, when local models are located in different areas, recombined models have a higher probability of locating in a flat area. When all recombined models are located in the same flat area, they are optimized towards the same direction. We theoretically analyze the convergence of model recombination. Experimental results show that, compared with state-of-the-art FL methods, FedMR can significantly improve the inference accuracy without exposing the privacy of each client. | Ming Hu, Zhihao Yue, Xiaofei Xie, Cheng Chen, Yihao Huang, Xian Wei, Xiang Lian, Yang Liu, Mingsong Chen | Chinese Academy of Sciences, Shanghai, China; Kent State University, Kent, OH, USA; Singapore Management University, Singapore, Singapore; East China Normal University, Shanghai, China; Nanyang Technological University, Singapore, Singapore |
|  |  [Privacy-Preserved Neural Graph Databases](https://doi.org/10.1145/3637528.3671678) |  | 0 | In the era of large language models (LLMs), efficient and accurate data retrieval has become increasingly crucial for the use of domain-specific or private data in the retrieval augmented generation (RAG). Neural graph databases (NGDBs) have emerged as a powerful paradigm that combines the strengths of graph databases (GDBs) and neural networks to enable efficient storage, retrieval, and analysis of graph-structured data which can be adaptively trained with LLMs. The usage of neural embedding storage and Complex neural logical Query Answering (CQA) provides NGDBs with generalization ability. When the graph is incomplete, by extracting latent patterns and representations, neural graph databases can fill gaps in the graph structure, revealing hidden relationships and enabling accurate query answering. Nevertheless, this capability comes with inherent trade-offs, as it introduces additional privacy risks to the domain-specific or private databases. Malicious attackers can infer more sensitive information in the database using well-designed queries such as from the answer sets of where Turing Award winners born before 1950 and after 1940 lived, the living places of Turing Award winner Hinton are probably exposed, although the living places may have been deleted in the training stage due to the privacy concerns. In this work, we propose a privacy-preserved neural graph database (P-NGDB) framework to alleviate the risks of privacy leakage in NGDBs. We introduce adversarial training techniques in the training stage to enforce the NGDBs to generate indistinguishable answers when queried with private information, enhancing the difficulty of inferring sensitive information through combinations of multiple innocuous queries. Extensive experimental results on three datasets show that our framework can effectively protect private information in the graph database while delivering high-quality public answers responses to queries. The code is available at https://github.com/HKUST-KnowComp/PrivateNGDB. | Qi Hu, Haoran Li, Jiaxin Bai, Zihao Wang, Yangqiu Song | Department of CSE, Hong Kong University of Science and Technology, Hong Kong, China |
|  |  [EntropyStop: Unsupervised Deep Outlier Detection with Loss Entropy](https://doi.org/10.1145/3637528.3671943) |  | 0 | Unsupervised Outlier Detection (UOD) is an important data mining task. With the advance of deep learning, deep Outlier Detection (OD) has received broad interest. Most deep UOD models are trained exclusively on clean datasets to learn the distribution of the normal data, which requires huge manual efforts to clean the real-world data if possible. Instead of relying on clean datasets, some approaches directly train and detect on unlabeled contaminated datasets, leading to the need for methods that are robust to such challenging conditions. Ensemble methods emerged as a superior solution to enhance model robustness against contaminated training sets. However, the training time is greatly increased by the ensemble mechanism. In this study, we investigate the impact of outliers on training, aiming to halt training on unlabeled contaminated datasets before performance degradation. Initially, we noted that blending normal and anomalous data causes AUC fluctuations-a label-dependent measure of detection accuracy. To circumvent the need for labels, we propose a zero-label entropy metric named Loss Entropy for loss distribution, enabling us to infer optimal stopping points for training without labels. Meanwhile, a negative correlation between entropy metric and the label-based AUC score is demonstrated by theoretical proofs. Based on this, an automated early-stopping algorithm called EntropyStop is designed to halt training when loss entropy suggests the maximum model detection capability. We conduct extensive experiments on ADBench (including 47 real datasets), and the overall results indicate that AutoEncoder (AE) enhanced by our approach not only achieves better performance than ensemble AEs but also requires under 2% of training time. Lastly, loss entropy and EntropyStop are evaluated on other deep OD models, exhibiting their broad potential applicability. | Yihong Huang, Yuang Zhang, Liping Wang, Fan Zhang, Xuemin Lin | Guangzhou University, Guangzhou, China; Shanghai Jiao Tong University, Shanghai, China; East China Normal University, Shanghai, China |
|  |  [RC-Mixup: A Data Augmentation Strategy against Noisy Data for Regression Tasks](https://doi.org/10.1145/3637528.3671993) |  | 0 | We study the problem of robust data augmentation for regression tasks in the presence of noisy data. Data augmentation is essential for generalizing deep learning models, but most of the techniques like the popular Mixup are primarily designed for classification tasks on image data. Recently, there are also Mixup techniques that are specialized to regression tasks like C-Mixup. In comparison to Mixup, which takes linear interpolations of pairs of samples, C-Mixup is more selective in which samples to mix based on their label distances for better regression performance. However, C-Mixup does not distinguish noisy versus clean samples, which can be problematic when mixing and lead to suboptimal model performance. At the same time, robust training has been heavily studied where the goal is to train accurate models against noisy data through multiple rounds of model training. We thus propose our data augmentation strategy RC-Mixup, which tightly integrates C-Mixup with multi-round robust training methods for a synergistic effect. In particular, C-Mixup improves robust training in identifying clean data, while robust training provides cleaner data to C-Mixup for it to perform better. A key advantage of RC-Mixup is that it is data-centric where the robust model training algorithm itself does not need to be modified, but can simply benefit from data mixing. We show in our experiments that RC-Mixup significantly outperforms C-Mixup and robust training baselines on noisy data benchmarks and can be integrated with various robust training methods. | Seonghyeon Hwang, Minsu Kim, Steven Euijong Whang | KAIST, Daejeon, Republic of Korea |
|  |  [Learn Together Stop Apart: An Inclusive Approach to Ensemble Pruning](https://doi.org/10.1145/3637528.3672018) |  | 0 | Gradient Boosting is a leading learning method that builds ensembles and adapts their sizes to particular tasks, consistently delivering top-tier results across various applications. However, determining the optimal number of models in the ensemble remains a critical yet underexplored aspect. Traditional approaches assume a universal ensemble size effective for all data points, which may not always hold true due to data heterogeneity. This paper introduces an adaptive approach to early stopping in Gradient Boosting, addressing data heterogeneity by assigning different stop moments to different data regions at inference time while still training a common ensemble on the entire dataset. We propose two methods: Direct Supervised Partition (DSP) and Indirect Supervised Partition (ISP). The DSP method uses a decision tree to partition the data based on learning curves, while ISP leverages the dataset's geometric and target distribution characteristics. An effective validation protocol is developed to determine the optimal number of early stopping regions or detect when the heterogeneity assumption does not hold. Experiments using state-of-the-art implementations of Gradient Boosting, LightGBM, and CatBoost, on standard benchmarks demonstrate that our methods enhance model precision by up to 2%, underscoring the significance of this research direction. This approach does not increase computational complexity and can be easily integrated into existing learning pipelines. | Bulat Ibragimov, Gleb Gusev | Sber AI Lab, Moscow, Russian Federation |
|  |  [Efficient Discovery of Time Series Motifs under both Length Differences and Warping](https://doi.org/10.1145/3637528.3671726) |  | 0 | Over the past two decades, time series motif discovery has become a crucial subroutine for many time series data mining tasks; concurrently, it has been established that Dynamic Time Warping (DTW) outperforms other similarity measures like Euclidean Distance in most scenarios. Against this backdrop, a DTW motif discovery algorithm was recently developed; however, it is confined to working with fixed-length subsequences. In this work, we propose a novel approach that allows us to find motifs under both length differences and warping. Our algorithm exploits a promising time series representation called Spikelets and introduces the first lower bound for DTW in the Spikelet space. Extensive empirical studies demonstrate that our method scales effectively across various real-world datasets and efficiently identifies DTW motif pairs of different lengths. | Makoto Imamura, Takaaki Nakamura | Mitsubishi Electric Corporation, Kamakura, Japan; Tokai University, Minato-ku, Tokyo, Japan |
|  |  [Promoting Fairness and Priority in Selecting k-Winners Using IRV](https://doi.org/10.1145/3637528.3671735) |  | 0 | We investigate the problem of finding winner(s) given a large number of users' (voters') preferences casted as ballots, one from each of the m users, where each ballot is a ranked order of preference of up to ℓ out of n items (candidates). Given a group protected attribute with k different values and a priority that imposes a selection order among these groups, the goal is to satisfy the priority order and select a winner per group that is most representative. It is imperative that at times the original users' preferences may require further manipulation to meet these fairness and priority requirement. We consider manipulation by modifications and formalize the margin finding problem under modification problem. We study the suitability of Instant Run-off Voting (IRV) as a preference aggregation method and demonstrate its advantages over positional methods. We present a suite of technical results on the hardness of the problem, design algorithms with theoretical guarantees and further investigate efficiency opportunities. We present exhaustive experimental evaluations using multiple applications and large-scale datasets to demonstrate the effectiveness of IRV, and efficacy of our designed solutions qualitatively and scalability-wise. | Md Mouinul Islam, Soroush Vahidi, Baruch Schieber, Senjuti Basu Roy | CS, NJIT, Newark, NJ, USA |
|  |  [FreQuant: A Reinforcement-Learning based Adaptive Portfolio Optimization with Multi-frequency Decomposition](https://doi.org/10.1145/3637528.3671668) |  | 0 | How can we leverage inherent frequency features of stock signals for effective portfolio optimization? Portfolio optimization in the domain of finance revolves around strategically allocating assets to maximize returns. Recent advancements highlight the efficacy of deep learning and reinforcement learning (RL) in capturing temporal asset patterns for portfolio optimization. However, previous methodologies focusing on time-domain often fail to detect sudden market shifts and abrupt events because their models are overly tailored to prevalent patterns, resulting in significant losses. In this paper, we propose FreQuant (Adaptive Portfolio Optimization via Multi-Frequency Quantitative Analysis), an effective deep RL framework for portfolio optimization that fully operates in the frequency domain, tackling the limitations of time domain-focused models. By bringing the analysis into the frequency domain with the Discrete Fourier Transform, our framework captures both prominent and subtle market frequencies, enhancing its adaptability and stability in response to market shifts. This approach allows FreQuant to adeptly identify primary asset patterns while also effectively responding to less common and abrupt market events, providing a more accurate and comprehensive asset representation. Empirical validation on diverse real-world trading datasets underscores the remarkable performance of FreQuant, showing its superiority in terms of profitability. Notably, FreQuant achieves up to 2.1x higher Annualized Rate of Return and 2.9x higher Portfolio Value than the best-performing competitors. | Jihyeong Jeon, Jiwon Park, Chanhee Park, U Kang | Seoul National University & DeepTrade Technologies Inc., Seoul, Republic of Korea; Seoul National University, Seoul, Republic of Korea |
|  |  [Addressing Prediction Delays in Time Series Forecasting: A Continuous GRU Approach with Derivative Regularization](https://doi.org/10.1145/3637528.3671969) |  | 0 | Time series forecasting has been an essential field in many different application areas, including economic analysis, meteorology, and so forth. The majority of time series forecasting models are trained using the mean squared error (MSE). However, this training based on MSE causes a limitation known as prediction delay. The prediction delay, which implies the ground-truth precedes the prediction, can cause serious problems in a variety of fields, e.g., finance and weather forecasting --- as a matter of fact, predictions succeeding ground-truth observations are not practically meaningful although their MSEs can be low. This paper proposes a new perspective on traditional time series forecasting tasks and introduces a new solution to mitigate the prediction delay. We introduce a continuous-time gated recurrent unit (GRU) based on the neural ordinary differential equation (NODE) which can supervise explicit time-derivatives. We generalize the GRU architecture in a continuous-time manner and minimize the prediction delay through our time-derivative regularization. Our method outperforms in metrics such as MSE, Dynamic Time Warping (DTW) and Time Distortion Index (TDI). In addition, we demonstrate the low prediction delay of our method in a variety of datasets. | Sheo Yon Jhin, Seojin Kim, Noseong Park | Yonsei University, Seoul, Seodaemun-gu, Republic of Korea; KAIST, Daejeon, Republic of Korea |
|  |  [MemMap: An Adaptive and Latent Memory Structure for Dynamic Graph Learning](https://doi.org/10.1145/3637528.3672060) |  | 0 | Dynamic graph learning has attracted much attention in recent years due to the fact that most of the real-world graphs are dynamic and evolutionary. As a result, many dynamic learning methods have been proposed to cope with the changes of node states over time. Among these studies, a critical issue is how to update the representations of nodes when new temporal events are observed. In this paper, we provide a novel memory structure - Memory Map (MemMap) for this problem. MemMap is an adaptive and evolutionary latent memory space, where each cell corresponds to an evolving "topic" of the dynamic graph. Moreover, the representation of a node is generated from its semantically correlated memory cells, rather than linked neighbors of the node. We have conducted experiments on real-world datasets and compared our method with the SOTA ones. It can be concluded that: 1) By constructing an adaptive and evolving memory structure during the dynamic learning process, our method can capture the dynamic graph changes, and the learned MemMap is actually a compact evolving structure organized according to the latent "topics" of the graph nodes. 2) Our research suggests that it is a more effective and efficient way to generate node representations from a latent semantic space (like MemMap in our method) than from directly connected neighbors (like most of the previous graph learning methods). The reason is that the number of memory cells in latent space could be much smaller than the number of nodes in a real-world graph, and the representation learning process could well balance the global and local message passing by leveraging the semantic similarity of graph nodes via the correlated memory cells. | Shuo Ji, Mingzhe Liu, Leilei Sun, Chuanren Liu, Tongyu Zhu | CCSE Lab, Beihang University, Beijing, China; The University of Tennessee, Knoxville, TN, USA |
|  |  [Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning](https://doi.org/10.1145/3637528.3671689) |  | 0 | The Unaligned Multi-view Clustering (UMC) problem is currently receiving widespread attention, focusing on clustering unaligned multi-view data generated in real-world applications. Although some algorithms have emerged to address this issue, there still exist the following drawbacks: 1) The fully unknown correspondence of samples across views can significantly limit the exploration of consistent clustering structure. 2) The fixed representation space makes it difficult to mine the comprehensive information in the original data. 3) Unbiased tensor rank approximation is desired to capture the high-order correlation among different views. To address these issues, we proposed a novel UMC framework termed Tensorized Unaligned Multi-view Clustering with Multi-scale Representation Learning (TUMCR). Specifically, TUMCR designs a multi-scale representation learning and alignment framework, which constructs multi-scale representation spaces to comprehensively explore the unknown correspondence across views. Then, a tensorial multi-scale fusion module is proposed to fuse multi-scale representations and explore the high-order correlation hidden in different views, which utilizes the Enhanced Tensor Rank (ETR) to learn the low-rank structure. Furthermore, TUMCR is solved by an efficient algorithm with good convergence. Extensive experiments on different types of datasets demonstrate the effectiveness and superiority of our TUMCR compared with state-of-the-art methods. Our code is publicly available at: https://github.com/jijintian/TUMCR. | Jintian Ji, Songhe Feng, Yidong Li |  |
|  |  [Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks](https://doi.org/10.1145/3637528.3671742) |  | 0 | In recent years, Graph Neural Networks (GNNs) and Large Language Models (LLMs) have exhibited remarkable capability in addressing different graph learning and natural language tasks, respectively. Motivated by this, integrating LLMs with GNNs has been increasingly studied to acquire transferable knowledge across modalities, which leads to improved empirical performance in language and graph domains. However, existing studies mainly focused on a single-domain scenario by designing complicated integration techniques to manage multimodal data effectively. Therefore, a concise and generic learning framework for multi-domain tasks, i.e., graph and language domains, is highly desired yet remains under-exploited due to two major challenges. First, the language corpus of downstream tasks differs significantly from graph data, making it hard to bridge the knowledge gap between modalities. Second, not all knowledge demonstrates immediate benefits for downstream tasks, potentially introducing disruptive noise to context-sensitive models like LLMs. To tackle these challenges, we propose a novel plug-and-play framework for incorporating a lightweight cross-domain prompting method into both language and graph learning tasks. Specifically, we first convert the textual input into a domain-scalable prompt, which not only preserves the semantic and logical contents of the textual input, but also highlights related graph information as external knowledge for different domains. Then, we develop a reinforcement learning-based method to learn the optimal edge selection strategy for useful knowledge extraction, which profoundly sharpens the multi-domain model capabilities. In addition, we introduce a joint multi-view optimization module to regularize agent-level collaborative learning across two domains. Finally, extensive empirical justifications over 23 public and synthetic datasets demonstrate that our approach can be applied to diverse multi-domain tasks more accurately, robustly, and reasonably, and improve the performances of the state-of-the-art graph and language models in different learning paradigms. | Wenyuan Jiang, Wenwei Wu, Le Zhang, Zixuan Yuan, Jian Xiang, Jingbo Zhou, Hui Xiong | Baidu Research, Baidu Inc., Beijing, China |
|  |  [Sketch-Based Replay Projection for Continual Learning](https://doi.org/10.1145/3637528.3671714) |  | 0 | Continual learning closely emulates human learning, which allows a model to learn from a stream of tasks sequentially without forgetting previously learned knowledge. Replay-based continual learning methods mitigate forgetting and improve performance by reintroducing data belonging to old tasks, however a replay method's performance may deteriorate when the reintroduced data does not effectively represent all experienced data. To address this concern, we propose the Sketch-based Replay Projection (SRP) method to capture and retain the original data stream's distribution within stored memory. SRP augments existing replay frameworks and introduces a two-fold approach. First, we develop a sketch-based sample selection technique to approximate feature distributions within distinct tasks, thereby capturing a wide distribution of examples for subsequent replay. Second, we propose a data compression method which projects examples into a reduced-dimensional space while preserving inter-example relationships and emphasizing inter-class disparities, encouraging diverse representations of each class while maintaining memory requirements similar to existing replay methodologies. Our experimental results demonstrate that SRP enhances replay diversity and improves the performance of existing replay models. | Jack Julian, Yun Sing Koh, Albert Bifet | AI Institute, The University of Waikato & LTCI, Télécom Paris, IP Paris, Hamilton, New Zealand; School of Computer Science, University of Auckland, Auckland, New Zealand |
|  |  [RCTD: Reputation-Constrained Truth Discovery in Sybil Attack Crowdsourcing Environment](https://doi.org/10.1145/3637528.3671803) |  | 0 | Sybil attacks are a prevalent concern within the realm of crowdsourcing, underscoring the significance of quality control in this domain. Truth discovery has been extensively studied to deduce the most trustworthy information from conflicting data based on the principle that reliable workers yield reliable answers. However, existing truth discovery approaches overlook the metric of workers' reputations, e.g., workers' historical approval rates on crowdsourcing platforms, despite being inflated and noisy, they offer a rough indication of workers' ability. In this paper, we first refine the approval rate using Wilson Lower Bound to enhance its confidence, and then mitigate its noise and inflation through a method based on ranking similarity. Specifically, we propose a method called RCTD (Reputation-Constrained Truth Discovery), which introduces a similarity metric between the rankings of workers' weights and the refined approval rates. This metric serves as a penalizing factor in the objective function of the truth discovery, restricting workers' weights to avoid excessively deviating from their historical reputation during the weight estimation process. We solve the objective function by introducing the block coordinate descent coupled with heuristics approach method. Experimental results on real-world datasets demonstrate that our approach achieves more accurate inference of true results in the Sybil attack environment compared to the state-of-the-art methods. | Xing Jin, Zhihai Gong, Jiuchuan Jiang, Chao Wang, Jian Zhang, Zhen Wang | School of Cyberspace, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; Research Center for Data Hub and Security, Zhejiang Lab, Hangzhou, Zhejiang, China |
|  |  [Bivariate Decision Trees: Smaller, Interpretable, More Accurate](https://doi.org/10.1145/3637528.3671903) |  | 0 | Univariate decision trees, commonly used since the 1950s, predict by asking questions about a single feature in each decision node. While they are interpretable, they often lack competitive predictive accuracy due to their inability to model feature correlations. Multivariate (oblique) trees use multiple features in each node, capturing high-dimensional correlations better, but sometimes they can be difficult to interpret. We advocate for a model that strikes a useful middle ground: bivariate decision trees, which use two features in each node. This typically produces trees that not only are more accurate than univariate trees, but much smaller, which offsets the small increase in node complexity and keeps them interpretable. They also help data mining by constructing new features that are useful for discrimination, and by providing a form of supervised, hierarchical 2D visualization that reveals patterns such as clusters or linear structure. We give two new algorithms to learn bivariate trees: a fast one based on CART; and a slower one based on alternating optimization with a feature regularization term, which produces the best trees while still scaling to large datasets. | Rasul Kairgeldin, Miguel Á. CarreiraPerpiñán | University of California, Merced, Merced, CA, USA |
|  |  [CAFO: Feature-Centric Explanation on Time Series Classification](https://doi.org/10.1145/3637528.3671724) |  | 0 | In multivariate time series (MTS) classification, finding the important features (e.g., sensors) for model performance is crucial yet challenging due to the complex, high-dimensional nature of MTS data, intricate temporal dynamics, and the necessity for domain-specific interpretations. Current explanation methods for MTS mostly focus on time-centric explanations, apt for pinpointing important time periods but less effective in identifying key features. This limitation underscores the pressing need for a feature-centric approach, a vital yet often overlooked perspective that complements time-centric analysis. To bridge this gap, our study introduces a novel feature-centric explanation and evaluation framework for MTS, named CAFO (Channel Attention and Feature Orthgonalization). CAFO employs a convolution-based approach with channel attention mechanisms, incorporating a depth-wise separable channel attention module (DepCA) and a QR decomposition-based loss for promoting feature-wise orthogonality. We demonstrate that this orthogonalization enhances the separability of attention distributions, thereby refining and stabilizing the ranking of feature importance. This improvement in feature-wise ranking enhances our understanding of feature explainability in MTS. Furthermore, we develop metrics to evaluate global and class-specific feature importance. Our framework's efficacy is validated through extensive empirical analyses on two major public benchmarks and real-world datasets, both synthetic and self-collected, specifically designed to highlight class-wise discriminative features. The results confirm CAFO's robustness and informative capacity in assessing feature importance in MTS classification tasks. This study not only advances the understanding of feature-centric explanations in MTS but also sets a foundation for future explorations in feature-centric explanations. The codes are available at https://github.com/eai-lab/CAFO. | Jaeho Kim, SeokJu Hahn, Yoontae Hwang, Junghye Lee, Seulki Lee |  |
|  |  [Gandalf: Learning Label-label Correlations in Extreme Multi-label Classification via Label Features](https://doi.org/10.1145/3637528.3672063) |  | 0 | Extreme Multi-label Text Classification (XMC) involves learning a classifier that can assign an input with a subset of most relevant labels from millions of label choices. Recent works in this domain have increasingly focused on a symmetric problem setting where both input instances and label features are short-text in nature. Short-text XMC with label features has found numerous applications in areas such as query-to-ad-phrase matching in search ads, title-based product recommendation, prediction of related searches. In this paper, we propose Gandalf, a novel approach which makes use of a label co-occurrence graph to leverage label features as additional data points to supplement the training distribution. By exploiting the characteristics of the short-text XMC problem, it leverages the label features to construct valid training instances, and uses the label graph for generating the corresponding soft-label targets, hence effectively capturing the label-label correlations. Surprisingly, models trained on these new training instances, although being less than half of the original dataset, can outperform models trained on the original dataset, particularly on the PSP@k metric for tail labels. With this insight, we aim to train existing XMC algorithms on both, the original and new training instances, leading to an average 5% relative improvements for 6 state-of-the-art algorithms across 4 benchmark datasets consisting of up to 1.3M labels. Gandalf can be applied in a plug-and-play manner to various methods and thus forwards the state-of-the-art in the domain, without incurring any additional computational overheads. Code has been open-sourced at www.github.com/xmc-aalto/InceptionXML. | Siddhant Kharbanda, Devaansh Gupta, Erik Schultheis, Atmadeep Banerjee, ChoJui Hsieh, Rohit Babbar | University of California, Los Angeles, Los Angeles, USA; Aalto University & University of Bath, Espoo, Finland; Aalto University, Espoo, Finland |
|  |  [OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing](https://doi.org/10.1145/3637528.3671745) |  | 0 | Fine-grained entity typing (FET), which assigns entities in text withcontext-sensitive, fine-grained semantic types, is a basic but important taskfor knowledge extraction from unstructured text. FET has been studiedextensively in natural language processing and typically relies onhuman-annotated corpora for training, which is costly and difficult to scale.Recent studies explore the utilization of pre-trained language models (PLMs) asa knowledge base to generate rich and context-aware weak supervision for FET.However, a PLM still requires direction and guidance to serve as a knowledgebase as they often generate a mixture of rough and fine-grained types, ortokens unsuitable for typing. In this study, we vision that an ontologyprovides a semantics-rich, hierarchical structure, which will help select thebest results generated by multiple PLM models and head words. Specifically, wepropose a novel annotation-free, ontology-guided FET method, OntoType, whichfollows a type ontological structure, from coarse to fine, ensembles multiplePLM prompting results to generate a set of type candidates, and refines itstype resolution, under the local context with a natural language inferencemodel. Our experiments on the Ontonotes, FIGER, and NYT datasets using theirassociated ontological structures demonstrate that our method outperforms thestate-of-the-art zero-shot fine-grained entity typing methods as well as atypical LLM method, ChatGPT. Our error analysis shows that refinement of theexisting ontology structures will further improve fine-grained entity typing. | Tanay Komarlu, Minhao Jiang, Xuan Wang, Jiawei Han | Virginia Tech, Blacksburg, VA, USA; University of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [LeMon: Automating Portrait Generation for Zero-Shot Story Visualization with Multi-Character Interactions](https://doi.org/10.1145/3637528.3671850) |  | 0 | Zero-Shot Story Visualization (ZSV) seeks to depict textual narratives through a sequence of images without relying on pre-existing text-image pairs for training. In this paper, we address the challenge of automated multi-character ZSV, aiming to create distinctive yet compatible character portraits for high-quality story visualization without the need of manual human interventions. Our study is motivated by the limitation of current ZSV approaches that necessitate inefficient manual collection of external images as initial character portraits and suffer from low-quality story visualization, especially with multi-character interactions, when the portraits are not well initiated. To overcome these issues, we develop LeMon, an LLM enhanced Multi-Character Zero-Shot Visualization framework that automates character portrait initialization and supports iterative portrait refinement by exploring the semantic content of the story. In particular, we design an LLM-based portrait generation strategy that matches the story characters with external movie characters, and leverage the matched resources as in-context learning (ICL) samples for LLMs to accurately initialize the character portraits. We then propose a graph-based Text2Image diffusion model that constructs a character interaction graph from the story to iteratively refine the character portraits by maximizing the distinctness of different characters while minimizing their incompatibility in the multi-character story visualization. Our evaluation results show that LeMon outperforms existing ZSV approaches in generating high-quality visualizations for stories across various types with multiple interacted characters. Our code is available at https://github.com/arxrean/LLM-LeMon. | Ziyi Kou, Shichao Pei, Xiangliang Zhang | Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science, University of Massachusetts Boston, Boston, MA, USA |
|  |  [Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Leman Go Indifferent](https://doi.org/10.1145/3637528.3671890) |  | 0 | Prior attacks on graph neural networks have focused on graph poisoning and evasion, neglecting the network's weights and biases. For convolutional neural networks, however, the risk arising from bit flip attacks is well recognized. We show that the direct application of a traditional bit flip attack to graph neural networks is of limited effectivity. Hence, we discuss the Injectivity Bit Flip Attack, the first bit flip attack designed specifically for graph neural networks. Our attack targets the learnable neighborhood aggregation functions in quantized message passing neural networks, degrading their ability to distinguish graph structures and impairing the expressivity of the Weisfeiler-Leman test. We find that exploiting mathematical properties specific to certain graph neural networks significantly increases their vulnerability to bit flip attacks. The Injectivity Bit Flip Attack can degrade the maximal expressive Graph Isomorphism Networks trained on graph property prediction datasets to random output by flipping only a small fraction of the network's bits, demonstrating its higher destructive power compared to traditional bit flip attacks transferred from convolutional neural networks. Our attack is transparent, motivated by theoretical insights and confirmed by extensive empirical results. | Lorenz Kummer, Samir Moustafa, Sebastian Schrittwieser, Wilfried N. Gansterer, Nils M. Kriege | Faculty of Computer Science, University of Vienna, Vienna, Austria |
|  |  [Max-Min Diversification with Asymmetric Distances](https://doi.org/10.1145/3637528.3671757) |  | 0 | One of the most well-known and simplest models for diversity maximization is the Max-Min Diversification (MMD) model, which has been extensively studied in the data mining and database literature. In this paper, we initiate the study of the Asymmetric Max-Min Diversification (AMMD) problem. The input is a positive integer k and a complete digraph over n vertices, together with a nonnegative distance function over the edges obeying the directed triangle inequality. The objective is to select a set of k vertices, which maximizes the smallest pairwise distance between them. AMMD reduces to the well-studied MMD problem in case the distances are symmetric, and has natural applications to query result diversification, web search, and facility location problems. Although the MMD problem admits a simple 1/2-approximation by greedily selecting the next-furthest point, this strategy fails for AMMD and it remained unclear how to design good approximation algorithms for AMMD. We propose a combinatorial 1/(6k)-approximation algorithm for AMMD by leveraging connections with the Maximum Antichain problem. We discuss several ways of speeding up the algorithm and compare its performance against heuristic baselines on real-life and synthetic datasets. | Iiro Kumpulainen, Florian Adriaens, Nikolaj Tatti | University of Helsinki, Helsinki, Finland; University of Helsinki & HIIT, Helsinki, Finland |
|  |  [Compact Decomposition of Irregular Tensors for Data Compression: From Sparse to Dense to High-Order Tensors](https://doi.org/10.1145/3637528.3671846) |  | 0 | An irregular tensor is a collection of matrices with different numbers of rows. Real-world data from diverse domains, including medical and stock data, are effectively represented as irregular tensors due to the inherent variations in data length. For their analysis, various tensor decomposition methods (e.g., PARAFAC2) have been devised. While they are expected to be effective in compressing large-scale irregular tensors, akin to regular tensor decomposition methods, our analysis reveals that their compression performance is limited due to the larger number of first mode factor matrices. In this work, we propose accurate and compact decomposition methods for lossy compression of irregular tensors. First, we propose Light-IT, which unifies all first mode factor matrices into a single matrix, dramatically reducing the size of compressed outputs. Second, motivated by the success of Tucker decomposition in regular tensor compression, we extend Light-IT to Light-IT++ to enhance its expressive power and thus reduce compression error. Finally, we generalize both methods to handle irregular tensors of any order and leverage the sparsity of tensors for acceleration. Extensive experiments on 6 real-world datasets demonstrate that our methods are (a) Compact: their compressed output is up to 37× smaller than that of the most concise baseline, (b) Accurate: our methods are up to 5× more accurate, with smaller compressed output, than the most accurate baseline, and (c) Versatile: our methods are effective for sparse, dense, and higher-order tensors. | Taehyung Kwon, Jihoon Ko, Jinhong Jung, JunGi Jang, Kijung Shin | Soongsil University, Seoul, Republic of Korea; KAIST, Seoul, Republic of Korea; UIUC, Champaign, IL, USA |
|  |  [Efficient Topology-aware Data Augmentation for High-Degree Graph Neural Networks](https://doi.org/10.1145/3637528.3671765) |  | 0 | In recent years, graph neural networks (GNNs) have emerged as a potent tool for learning on graph-structured data and won fruitful successes in varied fields. The majority of GNNs follow the message-passing paradigm, where representations of each node are learned by recursively aggregating features of its neighbors. However, this mechanism brings severe over-smoothing and efficiency issues over high-degree graphs (HDGs), wherein most nodes have dozens (or even hundreds) of neighbors, such as social networks, transaction graphs, power grids, etc. Additionally, such graphs usually encompass rich and complex structure semantics, which are hard to capture merely by feature aggregations in GNNs. Motivated by the above limitations, we propose TADA, an efficient and effective front-mounted data augmentation framework for GNNs on HDGs. Under the hood, TADA includes two key modules: (i) feature expansion with structure embeddings, and (ii) topology- and attribute-aware graph sparsification. The former obtains augmented node features and enhanced model capacity by encoding the graph structure into high-quality structure embeddings with our highly-efficient sketching method. Further, by exploiting task-relevant features extracted from graph structures and attributes, the second module enables the accurate identification and reduction of numerous redundant/noisy edges from the input graph, thereby alleviating over-smoothing and facilitating faster feature aggregations over HDGs. Empirically, \algo considerably improves the predictive performance of mainstream GNN models on 8 real homophilic/heterophilic HDGs in terms of node classification, while achieving efficient training and inference processes. | Yurui Lai, Xiaoyang Lin, Renchi Yang, Hongtao Wang | Hong Kong Baptist University, Hong Kong, China |
|  |  [ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification](https://doi.org/10.1145/3637528.3671862) |  | 0 | Multivariate time series classification (MTSC) has attracted significantresearch attention due to its diverse real-world applications. Recently,exploiting transformers for MTSC has achieved state-of-the-art performance.However, existing methods focus on generic features, providing a comprehensiveunderstanding of data, but they ignore class-specific features crucial forlearning the representative characteristics of each class. This leads to poorperformance in the case of imbalanced datasets or datasets with similar overallpatterns but differing in minor class-specific details. In this paper, wepropose a novel Shapelet Transformer (ShapeFormer), which comprisesclass-specific and generic transformer modules to capture both of thesefeatures. In the class-specific module, we introduce the discovery method toextract the discriminative subsequences of each class (i.e. shapelets) from thetraining set. We then propose a Shapelet Filter to learn the differencefeatures between these shapelets and the input time series. We found that thedifference feature for each shapelet contains important class-specificfeatures, as it shows a significant distinction between its class and others.In the generic module, convolution filters are used to extract generic featuresthat contain information to distinguish among all classes. For each module, weemploy the transformer encoder to capture the correlation between theirfeatures. As a result, the combination of two transformer modules allows ourmodel to exploit the power of both types of features, thereby enhancing theclassification performance. Our experiments on 30 UEA MTSC datasets demonstratethat ShapeFormer has achieved the highest accuracy ranking compared tostate-of-the-art methods. The code is available athttps://github.com/xuanmay2701/shapeformer. | XuanMay Thi Le, Ling Luo, Uwe Aickelin, MinhTuan Tran | Monash University, Melbourne, VIC, Australia; The University of Melbourne, Melbourne, VIC, Australia |
|  |  [ReCTSi: Resource-efficient Correlated Time Series Imputation via Decoupled Pattern Learning and Completeness-aware Attentions](https://doi.org/10.1145/3637528.3671816) |  | 0 | Imputation of Correlated Time Series (CTS) is essential in data preprocessing for many tasks, particularly when sensor data is often incomplete. Deep learning has enabled sophisticated models that improve CTS imputation by capturing temporal and spatial patterns. However, deep models often incur considerable consumption of computational resources and thus cannot be deployed in resource-limited settings. This paper presents ReCTSi (Resource-efficient CTS imputation), a method that adopts a new architecture for decoupled pattern learning in two phases: (1) the Persistent Pattern Extraction phase utilizes a multi-view learnable codebook mechanism to identify and archive persistent patterns common across different time series, enabling rapid pattern retrieval during inference. (2) the Transient Pattern Adaptation phase introduces completeness-aware attention modules that allocate attention to the complete and hence more reliable data segments. Extensive experimental results show that ReCTSi achieves state-of-the-art imputation accuracy while consuming much fewer computational resources than the leading existing model, consuming only 0.004% of the FLOPs for inference compared to its closest competitor. The blend of high accuracy and very low resource consumption makes ReCTSi the currently best method for resource-limited scenarios. The related code is available at https://github.com/ryanlaics/RECTSI. | Zhichen Lai, Dalin Zhang, Huan Li, Dongxiang Zhang, Hua Lu, Christian S. Jensen | Department of People and Technology, Roskilde University, Roskilde, Denmark; The State Key Laboratory of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Department of Computer Science, Aalborg University, Aalborg, Denmark |
|  |  [Layer-Wise Adaptive Gradient Norm Penalizing Method for Efficient and Accurate Deep Learning](https://doi.org/10.1145/3637528.3671728) |  | 0 | Sharpness-aware minimization (SAM) is known to improve the generalization performance of neural networks. However, it is not widely used in real-world applications yet due to its expensive model perturbation cost. A few variants of SAM have been proposed to tackle such an issue, but they commonly do not alleviate the cost noticeably. In this paper, we propose a lightweight layer-wise gradient norm penalizing method that tackles the expensive computational cost of SAM while maintaining its superior generalization performance. Our study empirically proves that the gradient norm of the whole model can be effectively suppressed by penalizing the gradient norm of only a few critical layers. We also theoretically show that such a partial model perturbation does not harm the convergence rate of SAM, allowing them to be safely adapted in real-world applications. To demonstrate the efficacy of the proposed method, we perform extensive experiments comparing the proposed method to mini-batch SGD and the conventional SAM using representative computer vision and language modeling benchmarks. | Sunwoo Lee | Department of Computer Engineering, Inha University, Incheon, Republic of Korea |
|  |  [Label Learning Method Based on Tensor Projection](https://doi.org/10.1145/3637528.3671671) |  | 0 | Multi-view clustering method based on anchor graph has been widely concerneddue to its high efficiency and effectiveness. In order to avoidpost-processing, most of the existing anchor graph-based methods learnbipartite graphs with connected components. However, such methods have highrequirements on parameters, and in some cases it may not be possible to obtainbipartite graphs with clear connected components. To end this, we propose alabel learning method based on tensor projection (LLMTP). Specifically, weproject anchor graph into the label space through an orthogonal projectionmatrix to obtain cluster labels directly. Considering that the spatialstructure information of multi-view data may be ignored to a certain extentwhen projected in different views separately, we extend the matrix projectiontransformation to tensor projection, so that the spatial structure informationbetween views can be fully utilized. In addition, we introduce the tensorSchatten p-norm regularization to make the clustering label matrices ofdifferent views as consistent as possible. Extensive experiments have provedthe effectiveness of the proposed method. | Jing Li, Quanxue Gao, Qianqian Wang, Cheng Deng, DeYan Xie | School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, China; School of Science and Information Science, Qingdao Agricultural University, Qingdao, Shandong, China; School of Telecommunications Engineering, Xidian University, Xi'an, Shaanxi, China |
|  |  [Physics-informed Neural ODE for Post-disaster Mobility Recovery](https://doi.org/10.1145/3637528.3672027) |  | 0 | Urban mobility undergoes a profound decline in the aftermath of a disaster, subsequently exhibiting a complex recovery trajectory. Effectively capturing and predicting this dynamic recovery process holds paramount importance for devising more efficient post-disaster recovery strategies, such as resource allocation to areas with protracted recovery periods. Existing models for post-disaster mobility recovery predominantly employ basic mathematical methods, which are strongly based on simplifying assumptions, and their limited parameters restrict their capacity to fully capture the mobility recovery patterns. In response to this gap, we introduce the Coupled Dynamic Graph ODE Network (CDGON) to model the intricate dynamics of post-disaster mobility recovery. Our model seamlessly integrates existing physical knowledge pertaining to post-disaster mobility recovery and incorporates the nuanced interactions between intra-regional and inter-regional population flows. Extensive experimental results demonstrate the efficiency of our model in capturing the dynamic recovery patterns of urban population mobility in post-disaster scenarios, surpassing the capabilities of current dynamic graph prediction models. | Jiahao Li, Huandong Wang, Xinlei Chen | Department of Electronic Engineering, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University & Pengcheng Laboratory, Shenzhen, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China |
|  |  [Causal Subgraph Learning for Generalizable Inductive Relation Prediction](https://doi.org/10.1145/3637528.3671972) |  | 0 | Inductive relation reasoning in knowledge graphs aims at predicting missing triplets involving unseen entities and/or unseen relations. While subgraph-based methods that reason about the local structure surrounding a candidate triplet have shown promise, they often fall short in accurately modeling the causal dependence between a triplet's subgraph and its ground-truth label. This limitation typically results in a susceptibility to spurious correlations caused by confounders, adversely affecting generalization capabilities. Herein, we introduce a novel front-door adjustment-based approach designed to learn the causal relationship between subgraphs and their ground-truth labels, specifically for inductive relation prediction. We conceptualize the semantic information of subgraphs as a mediator and employ a graph data augmentation mechanism to create augmented subgraphs. Furthermore, we integrate a fusion module and a decoder within the front-door adjustment framework, enabling the estimation of the mediator's combination with augmented subgraphs. We also introduce the reparameterization trick in the fusion model to enhance model robustness. Extensive experiments on widely recognized benchmark datasets demonstrate the proposed method's superiority in inductive relation prediction, particularly for tasks involving unseen entities and unseen relations. Additionally, the subgraphs reconstructed by our decoder offer valuable insights into the model's decision-making process, enhancing transparency and interpretability. | Mei Li, Xiaoguang Liu, Hua Ji, Shuangjia Zheng | Global Institute of Future Technology, Shanghai Jiao Tong University, Shanghai, China; College of Computer Science, TMCC, SysNet, DISSec, GTIISC, Nankai University, Tianjin, China; Department of Computer Science, Civil Aviation University of China, Tianjin, China |
|  |  [SimDiff: Simple Denoising Probabilistic Latent Diffusion Model for Data Augmentation on Multi-modal Knowledge Graph](https://doi.org/10.1145/3637528.3671769) |  | 0 | In this paper, we address the challenges of data augmentation in Multi-Modal Knowledge Graphs (MMKGs), a relatively under-explored area. We propose a novel diffusion-based generative model, the Simple Denoising Probabilistic Latent Diffusion Model (SimDiff). SimDiff is capable of handling different data modalities including the graph topology in a unified manner by the same diffusion model in the latent space. It enhances the utilization of multi-modal data and encourage the multi-modal fusion and reduces the dependency on limited training data. We validate our method in downstream Entity Alignment (EA) tasks in MMKGs, demonstrating that even when using only half of the seed entities in training, our methods can still achieve superior performance. This work contributes to the field by providing a new data generation or augmentation method for MMKGs, potentially paving the way for more effective use of MMKGs in various applications. Code is made available at https://github.com/ranlislz/SimDiff. | Ran Li, Shimin Di, Lei Chen, Xiaofang Zhou | HKUST, Hong Kong SAR, China; HKUST(GZ) & HKUST, Guangzhou, China |
|  |  [ITPNet: Towards Instantaneous Trajectory Prediction for Autonomous Driving](https://doi.org/10.1145/3637528.3671681) |  | 0 | Trajectory prediction of moving traffic agents is crucial for the safety of autonomous vehicles, whereas previous approaches usually rely on sufficiently long-observed trajectory (e.g., 2 seconds) to predict the future trajectory of the agents. However, in many real-world scenarios, it is not realistic to collect adequate observed locations for moving agents, leading to the collapse of most prediction models. For instance, when a moving car suddenly appears and is very close to an autonomous vehicle because of the obstruction, it is quite necessary for the autonomous vehicle to quickly and accurately predict the future trajectories of the car with limited observed trajectory locations. In light of this, we focus on investigating the task of instantaneous trajectory prediction, i.e., two observed locations are available during inference. To this end, we put forward a general and plug-and-play instantaneous trajectory prediction approach, called ITPNet. Specifically, we propose a backward forecasting mechanism to reversely predict the latent feature representations of unobserved historical trajectories of the agent based on its two observed locations and then leverage them as complementary information for future trajectory prediction. Meanwhile, due to the inevitable existence of noise and redundancy in the predicted latent feature representations, we further devise a Noise Redundancy Reduction Former (NRRFormer) module, which aims to filter out noise and redundancy from unobserved trajectories and integrate the filtered features and observed features into a compact query representation for future trajectory predictions. In essence, ITPNet can be naturally compatible with existing trajectory prediction models, enabling them to gracefully handle the case of instantaneous trajectory prediction. Extensive experiments on the Argoverse and nuScenes datasets demonstrate ITPNet outperforms the baselines by a large margin and shows its efficacy with different trajectory prediction models. | Rongqing Li, Changsheng Li, Yuhang Li, Hanjie Li, Yi Chen, Ye Yuan, Guoren Wang | Beijing Institute of Technology, Beijing, China |
|  |  [InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising](https://doi.org/10.1145/3637528.3672032) |  | 0 | In today's fast-paced world, advertisers are increasingly demanding real-time and accurate personalized ad delivery based on dynamic preference modeling, which emphasizes the temporality existing in both user preference and product characteristics. Meanwhile, with the development of graph neural networks (GNNs), E-commerce knowledge graphs (KG) with rich semantic relatedness are invoked to improve accuracy and provide appropriate explanations to encourage advertisers' willingness to invest in ad expenses. However, it is still challenging for existing methods to comprehensively consider both time-series interactions and graph-structured knowledge triples in a unified model, i.e., the case in knowledge-aware dynamic advertising. The interaction graph between users and products changes rapidly over time, while the knowledge in KG remains relatively stable. This results in an uneven distribution of temporal and semantic information, causing existing GNNs to fail in this scenario. In this work, we quantitatively define the above phenomenon as temporal unevenness and introduce the Incremental Leveling Network (InLN) with three novel techniques: the periodic-focusing window for node-level dynamic modeling, the biased temporal walk for subgraph-level dynamic modeling and the incremental leveling mechanism for KG updating. Verified by comprehensive and intensive experiments, InLN outperforms nine baseline models in three tasks by substantial margins, reaching up to a 9.9% improvement and averaging a 5.7% increase. | Xujia Li, Jingshu Peng, Lei Chen | Hong Kong University of Science and Technology, Hong Kong SAR, China |
|  |  [Bi-Objective Contract Allocation for Guaranteed Delivery Advertising](https://doi.org/10.1145/3637528.3671752) |  | 0 | Contemporary systems of Guaranteed Delivery (GD) advertising work with two different stages, namely, the offline selling stage and the online serving stage. The former deals with contract allocation, and the latter fulfills the impression allocation of signed contracts. Existing work usually handles these two stages separately. For example, contracts are formulated offline without concerning practical situations in the online serving stage. Therefore, we address in this paper a bi-objective contract allocation for GD advertising, which maximizes the impressions, i.e., Ad resource assignments, allocated for the new incoming advertising orders, and at the same time, controls the balance in the inventories. Since the proposed problem is high dimensional and heavily constrained, we design an efficient local search that focuses on the two objectives alternatively. The experimental results indicate that our algorithm outperforms multi-objective evolutionary algorithms and Gurobi, the former of which is commonly applied for multi-objective optimization and the latter of which is a well-known competitive commercial tool. | Yan Li, Yundu Huang, Wuyang Mao, Furong Ye, Xiang He, Zhonglin Zu, Shaowei Cai | Alibaba Group, Hangzhou, China; Alibaba Group, Beijing, China |
|  |  [Improving Robustness of Hyperbolic Neural Networks by Lipschitz Analysis](https://doi.org/10.1145/3637528.3671875) |  | 0 | Hyperbolic neural networks (HNNs) are emerging as a promising tool for representing data embedded in non-Euclidean geometries, yet their adoption has been hindered by challenges related to stability and robustness. In this work, we conduct a rigorous Lipschitz analysis for HNNs and propose using Lipschitz regularization as a novel strategy to enhance their robustness. Our comprehensive investigation spans both the Poincaré ball model and the hyperboloid model, establishing Lipschitz bounds for HNN layers. Importantly, our analysis provides detailed insights into the behavior of the Lipschitz bounds as they relate to feature norms, particularly distinguishing between scenarios where features have unit norms and those with large norms. Further, we study regularization using the derived Lipschitz bounds. Our empirical validations demonstrate consistent improvements in HNN robustness against noisy perturbations. | Yuekang Li, Yidan Mao, Yifei Yang, Dongmian Zou | Applied Mathematics and Computational Sciences, DNAS, Duke Kunshan University, Kunshan, China; Electronic Information School, Wuhan University, Wuhan, China; Zu Chongzhi Center and Data Science Research Center, DNAS, Duke Kunshan University, Kunshan, China |
|  |  [ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs](https://doi.org/10.1145/3637528.3671982) |  | 0 | With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to Zero-shot transferability in Graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we leverage a language model to encode both node attributes and class semantics, ensuring consistent feature dimensions across datasets. We also propose a prompt-based subgraph sampling module that enriches the semantic information and structure information of extracted subgraphs using prompting nodes and neighborhood aggregation, respectively. We further adopt a lightweight fine-tuning strategy that reduces the risk of overfitting and maintains the zero-shot learning efficacy of the language model. The results underscore the effectiveness of our model in achieving significant cross-dataset zero-shot transferability, opening pathways for the development of graph foundation models. | Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, Jia Li | THU, Shenzhen, China; CUHK, Hong Kong SAR, China; HKUST (GZ), Guangzhou, China |
|  |  [Rethinking Fair Graph Neural Networks from Re-balancing](https://doi.org/10.1145/3637528.3671826) |  | 0 | Driven by the powerful representation ability of Graph Neural Networks (GNNs), plentiful GNN models have been widely deployed in many real-world applications. Nevertheless, due to distribution disparities between different demographic groups, fairness in high-stake decision-making systems is receiving increasing attention. Although lots of recent works devoted to improving the fairness of GNNs and achieved considerable success, they all require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Surprisingly, we find that simple re-balancing methods can easily match or surpass existing fair GNN methods. We claim that the imbalance across different demographic groups is a significant source of unfairness, resulting in imbalanced contributions from each group to the parameters updating. However, these simple re-balancing methods have their own shortcomings during training. In this paper, we propose FairGB, Fair Graph Neural Network via re-Balancing, which mitigates the unfairness of GNNs by group balancing. Technically, FairGB consists of two modules: counterfactual node mixup and contribution alignment loss. Firstly, we select counterfactual pairs across inter-domain and inter-class, and interpolate the ego-networks to generate new samples. Guided by analysis, we can reveal the debiasing mechanism of our model by the causal view and prove that our strategy can make sensitive attributes statistically independent from target labels. Secondly, we reweigh the contribution of each group according to gradients. By combining these two modules, they can mutually promote each other. Experimental results on benchmark datasets show that our method can achieve state-of-the-art results concerning both utility and fairness metrics. Code is available at https://github.com/ZhixunLEE/FairGB. | Zhixun Li, Yushun Dong, Qiang Liu, Jeffrey Xu Yu | University of Virginia, Charlottesville, USA; The Chinese University of Hong Kong, Hong Kong, Hong Kong; Institute of Automation, Chinese Academy of Sciences, Beijing, China |
|  |  [MulSTE: A Multi-view Spatio-temporal Learning Framework with Heterogeneous Event Fusion for Demand-supply Prediction](https://doi.org/10.1145/3637528.3672030) |  | 0 | Recently, integrated warehouse and distribution logistics systems are widely used in E-commerce industries to adjust to constantly changing customer demands. It makes the prediction of purchase demand and delivery supply capacity a crucial problem to streamline operations and improve efficiency. The interaction between such demand and supply not only relies on their economic relationships but also on consumer psychology caused by daily events, such as epidemics, promotions, and festivals. Although existing studies have made great efforts in the joint prediction of demand and supply considering modeling the demand-supply interactions, they seldom refer to the impacts of diverse events. In this work, we propose MulSTE, a Multi-view Spatio-Temporal learning framework with heterogeneous Event fusion. Firstly, an Event Fusion Representation (EFR) module is designed to fuse the textual, numerical, and categorical heterogeneous information for emergent and periodic events. Secondly, a Multi-graph Adaptive Convolution Recurrent Network (MGACRN) is developed as the spatio-temporal encoder (ST-Encoder) to capture the evolutional features of demand, supply, and events. Thirdly, the Event Gated Demand-Supply Interaction Attention (EGIA) module is designed to model the demand-supply interactions during events. The evaluations are conducted on two real-world datasets collected from JD Logistics and public websites. The experimental results show that our method outperforms state-of-the-art baselines in various metrics. | Li Lin, Zhiqiang Lu, Shuai Wang, Yunhuai Liu, Zhiqing Hong, Haotian Wang, Shuai Wang | Rutgers University, Piscataway, USA; Southeast University, Nanjing, Jiangsu, China; Peking University, Beijing, China; Southeast University, Nanjing, China; JD Logistics, Beijing, China |
|  |  [PSMC: Provable and Scalable Algorithms for Motif Conductance Based Graph Clustering](https://doi.org/10.1145/3637528.3671666) |  | 0 | Higher-order graph clustering aims to partition the graph using frequently occurring subgraphs (i.e., motifs), instead of the lower-order edges, as the atomic clustering unit, which has been recognized as the state-of-the-art solution in ground truth community detection and knowledge discovery. Motif conductance is one of the most promising higher-order graph clustering models due to its strong interpretability. However, existing motif conductance based graph clustering algorithms are mainly limited by a seminal two-stage reweighting computing framework, needing to enumerate all motif instances to obtain an edge-weighted graph for partitioning. However, such a framework has two-fold vital defects: (1) It can only provide a quadratic bound for the motif with three vertices, and whether there is provable clustering quality for other motifs is still an open question. (2) The enumeration procedure of motif instances incurs prohibitively high costs against large motifs or large dense graphs due to combinatorial explosions. Besides, expensive spectral clustering or local graph diffusion on the edge-weighted graph also makes existing methods unable to handle massive graphs with millions of nodes. To overcome these dilemmas, we propose a Provable and Scalable Motif Conductance algorithm PSMC, which has a fixed and motif-independent approximation ratio for any motif. Specifically, PSMC first defines a new vertex metric Motif Resident based on the given motif, which can be computed locally. Then, it iteratively deletes the vertex with the smallest motif resident value very efficiently using novel dynamic update technologies. Finally, it outputs the locally optimal result during the above iterative process. To further boost efficiency, we propose several effective bounds to estimate the motif resident value of each vertex, which can greatly reduce computational costs. Empirical results on real-life and synthetic demonstrate that our proposed algorithms achieve 3.2-32 times speedup and improve the quality by at least 12 times than the state-of-the art baselines. | Longlong Lin, Tao Jia, Zeli Wang, Jin Zhao, RongHua Li | College of Computer and Information Science, Southwest University, Chongqing, China; Chongqing University of Post and Telecommunications, Chongqing, China; Shenzhen Institute of Technology & Beijing Institute of Technology, Shenzhen, China |
|  |  [CONFIDE: Contextual Finite Difference Modelling of PDEs](https://doi.org/10.1145/3637528.3671676) |  | 0 | We introduce a method for inferring an explicit PDE from a data sample generated by previously unseen dynamics, based on a learned context. The training phase integrates knowledge of the form of the equation with a differential scheme, while the inference phase yields a PDE that fits the data sample and enables both signal prediction and data explanation. We include results of extensive experimentation, comparing our method to SOTA approaches, together with ablation studies that examine different flavors of our solution in terms of prediction error and explainability. | Ori Linial, Orly Avner, Dotan Di Castro |  |
|  |  [CASA: Clustered Federated Learning with Asynchronous Clients](https://doi.org/10.1145/3637528.3671979) |  | 0 | Clustered Federated Learning (CFL) is an emerging paradigm to extract insights from data on IoT devices. Through iterative client clustering and model aggregation, CFL adeptly manages data heterogeneity, ensures privacy, and delivers personalized models to heterogeneous devices. Traditional CFL approaches, which operate synchronously, suffer from prolonged latency for waiting slow devices during clustering and aggregation. This paper advocates a shift to asynchronous CFL, allowing the server to process client updates as they arrive. This shift enhances training efficiency yet introduces complexities to the iterative training cycle. To this end, we present CASA, a novel CFL scheme for Clustering-Aggregation Synergy under Asynchrony. Built upon a holistic theoretical understanding of asynchrony's impact on CFL, CASA adopts a bi-level asynchronous aggregation method and a buffer-aided dynamic clustering strategy to harmonize between clustering and aggregation. Extensive evaluations on standard benchmarks show that CASA outperforms representative baselines in model accuracy and achieves 2.28-6.49× higher convergence speed. | Boyi Liu, Yiming Ma, Zimu Zhou, Yexuan Shi, Shuyuan Li, Yongxin Tong | School of Data Science, City University of Hong Kong, Hong Kong, China; SKLCCSE Lab, Beihang University, Beijing, China |
|  |  [FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML](https://doi.org/10.1145/3637528.3671996) |  | 0 | We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models ~2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines[20]. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models. | Brian Liu, Rahul Mazumder | Massachusetts Institute of Technology, Cambridge, MA, USA |
|  |  [TDNetGen: Empowering Complex Network Resilience Prediction with Generative Augmentation of Topology and Dynamics](https://doi.org/10.1145/3637528.3671934) |  | 0 | Predicting the resilience of complex networks, which represents the ability to retain fundamental functionality amidst external perturbations or internal failures, plays a critical role in understanding and improving real-world complex systems. Traditional theoretical approaches grounded in nonlinear dynamical systems rely on prior knowledge of network dynamics. On the other hand, data-driven approaches frequently encounter the challenge of insufficient labeled data, a predicament commonly observed in real-world scenarios. In this paper, we introduce a novel resilience prediction framework for complex networks, designed to tackle this issue through generative data augmentation of network topology and dynamics. The core idea is the strategic utilization of the inherent joint distribution present in unlabeled network data, facilitating the learning process of the resilience predictor by illuminating the relationship between network topology and dynamics. Experiment results on three network datasets demonstrate that our proposed framework TDNetGen can achieve high prediction accuracy up to 85%-95%. Furthermore, the framework still demonstrates a pronounced augmentation capability in extreme low-data regimes, thereby underscoring its utility and robustness in enhancing the prediction of network resilience. We have open-sourced our code in the following link, https://github.com/tsinghua-fib-lab/TDNetGen. | Chang Liu, Jingtao Ding, Yiwen Song, Yong Li | Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China |
|  |  [Asymmetric Beta Loss for Evidence-Based Safe Semi-Supervised Multi-Label Learning](https://doi.org/10.1145/3637528.3671756) |  | 0 | The goal of semi-supervised multi-label learning (SSMLL) is to improve model performance by leveraging the information of unlabeled data. Recent studies usually adopt the pseudo-labeling strategy to tackle unlabeled data based on the assumption that labeled and unlabeled data share the same distribution. However, in realistic scenarios, unlabeled examples are often collected through cost-effective methods, inevitably introducing out-of-distribution (OOD) data, leading to a significant decline in model performance. In this paper, we propose a safe semi-supervised multi-label learning framework based on the theory of evidential deep learning (EDL), with the goal of achieving robust and effective unlabeled data exploitation. On one hand, we propose the asymmetric beta loss to not only compensate for the lack of robustness in common MLL losses, but also to solve the inherent positive-negative imbalance problem faced by the EDL losses in MLL. On the other hand, to construct a robust SSMLL framework, we adopt a dual-head structure to generate class probabilities and instance uncertainties. The former are used to generate pseudo-labels, while the latter are utilized to filter OOD examples. To avoid the need for threshold estimation, we develop a dual-measurement weighted loss function to safely perform unlabeled training. Extensive experiments on multiple benchmark datasets verify the effectiveness of the proposed method in both OOD detection and SSMLL tasks. | HaoZhe Liu, MingKun Xie, ChenChen Zong, ShengJun Huang | Nanjing University of Aeronautics and Astronautics, Nanjing, China |
|  |  [An Unsupervised Learning Framework Combined with Heuristics for the Maximum Minimal Cut Problem](https://doi.org/10.1145/3637528.3671704) |  | 0 | The Maximum Minimal Cut Problem (MMCP), a NP-hard combinatorial optimization (CO) problem, has not received much attention due to the demanding and challenging bi-connectivity constraint. Moreover, as a CO problem, it is also a daunting task for machine learning, especially without labeled instances. To deal with these problems, this work proposes an unsupervised learning framework combined with heuristics for MMCP that can provide valid and high-quality solutions. As far as we know, this is the first work that explores machine learning and heuristics to solve MMCP. The unsupervised solver is inspired by a relaxation-plus-rounding approach, the relaxed solution is parameterized by graph neural networks, and the cost and penalty of MMCP are explicitly written out, which can train the model end-to-end. A crucial observation is that each solution corresponds to at least one spanning tree. Based on this finding, a heuristic solver that implements tree transformations by adding vertices is utilized to repair and improve the solution quality of the unsupervised solver. Alternatively, the graph is simplified while guaranteeing solution consistency, which reduces the running time. We conduct extensive experiments to evaluate our framework and give a specific application. The results demonstrate the superiority of our method against two techniques designed. | Huaiyuan Liu, Xianzhang Liu, Donghua Yang, Hongzhi Wang, Yingchi Long, Mengtong Ji, Dongjing Miao, Zhiyu Liang | Harbin Institute of Technology, Harbin, China |
|  |  [ACER: Accelerating Complex Event Recognition via Two-Phase Filtering under Range Bitmap-Based Indexes](https://doi.org/10.1145/3637528.3671814) |  | 0 | Complex event recognition (CER) refers to identifying specific patterns composed of several primitive events in event stores. Since full-scanning event stores to identify primitive events holding query constraint conditions will incur costly I/O overhead, a mainstream and practical approach is using index techniques to obtain these events. However, prior index-based approaches suffer from significant I/O and sorting overhead when dealing with high predicate selectivity or long query window (common in real-world applications), which leads to high query latency. To address this issue, we propose ACER, a Range Bitmap-based index, to accelerate CER. Firstly, ACER achieves a low index space overhead by grouping the events with the same type into a cluster and compressing the cluster data, alleviating the I/O overhead of reading indexes. Secondly, ACER builds Range Bitmaps in batch (block) for queried attributes and ensures that the events of each cluster in the index block are chronologically ordered. Then, ACER can always obtain ordered query results for a specific event type through merge operations, avoiding sorting overhead. Most importantly, ACER avoids unnecessary disk access in indexes and events via two-phase filtering based on the window condition, thus alleviating the I/O overhead further. Our experiments on six real-world and synthetic datasets demonstrate that ACER reduces the query latency by up to one order of magnitude compared with SOTA techniques. | Shizhe Liu, Haipeng Dai, Shaoxu Song, Meng Li, Jingsong Dai, Rong Gu, Guihai Chen | State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; BNRist, Tsinghua University, Beijing, China |
|  |  [Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective](https://doi.org/10.1145/3637528.3671967) |  | 0 | Graph clustering, a fundamental and challenging task in graph mining, aims to classify nodes in a graph into several disjoint clusters. In recent years, graph contrastive learning (GCL) has emerged as a dominant line of research in graph clustering and advances the new state-of-the-art. However, GCL-based methods heavily rely on graph augmentations and contrastive schemes, which may potentially introduce challenges such as semantic drift and scalability issues. Another promising line of research involves the adoption of modularity maximization, a popular and effective measure for community detection, as the guiding principle for clustering tasks. Despite the recent progress, the underlying mechanism of modularity maximization is still not well understood. In this work, we dig into the hidden success of modularity maximization for graph clustering. Our analysis reveals the strong connections between modularity maximization and graph contrastive learning, where positive and negative examples are naturally defined by modularity. In light of our results, we propose a community-aware graph clustering framework, coined øurs, which leverages modularity maximization as a contrastive pretext task to effectively uncover the underlying information of communities in graphs, while avoiding the problem of semantic drift. Extensive experiments on multiple graph datasets verify the effectiveness of øurs in terms of scalability and clustering performance compared to state-of-the-art graph clustering methods. Notably, øurs easily scales a sufficiently large graph with 100M nodes while outperforming strong baselines. | Yunfei Liu, Jintang Li, Yuehe Chen, Ruofan Wu, Ericbk Wang, Jing Zhou, Sheng Tian, Shuheng Shen, Xing Fu, Changhua Meng, Weiqiang Wang, Liang Chen | Unaffiliated, Guangzhou, China; Ant Group, Hangzhou, China; Ant Group, Hangzhou, Zhejiang, China |
|  |  [Graph Data Condensation via Self-expressive Graph Structure Reconstruction](https://doi.org/10.1145/3637528.3671710) |  | 0 | With the increasing demands of training graph neural networks (GNNs) on large-scale graphs, graph data condensation has emerged as a critical technique to relieve the storage and time costs during the training phase. It aims to condense the original large-scale graph to a much smaller synthetic graph while preserving the essential information necessary for efficiently training a downstream GNN. However, existing methods concentrate either on optimizing node features exclusively or endeavor to independently learn node features and the graph structure generator. They could not explicitly leverage the information of the original graph structure and failed to construct an interpretable graph structure for the synthetic dataset. To address these issues, we introduce a novel framework named Graph Data Condensation via Self-expressive Graph Structure Reconstruction (GCSR). Our method stands out by (1) explicitly incorporating the original graph structure into the condensing process and (2) capturing the nuanced interdependencies between the condensed nodes by reconstructing an interpretable self-expressive graph structure. Extensive experiments and comprehensive analysis validate the efficacy of the proposed method across diverse GNN models and datasets. Our code is available at https://github.com/zclzcl0223/GCSR. | Zhanyu Liu, Chaolv Zeng, Guanjie Zheng | Shanghai Jiao Tong University, Shanghai, China |
|  |  [Generative Pretrained Hierarchical Transformer for Time Series Forecasting](https://doi.org/10.1145/3637528.3671855) |  | 0 | Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings. To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset under the channel-independent assumption for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enablinga single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretraining time series large models. We make our codes publicly available\footnotehttps://github.com/icantnamemyself/GPHT. | Zhiding Liu, Jiqian Yang, Mingyue Cheng, Yucong Luo, Zhi Li | Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong, China |
|  |  [AIM: Attributing, Interpreting, Mitigating Data Unfairness](https://doi.org/10.1145/3637528.3671797) |  | 0 | Data collected in the real world often encapsulates historical discriminationagainst disadvantaged groups and individuals. Existing fair machine learning(FairML) research has predominantly focused on mitigating discriminative biasin the model prediction, with far less effort dedicated towards exploring howto trace biases present in the data, despite its importance for thetransparency and interpretability of FairML. To fill this gap, we investigate anovel research problem: discovering samples that reflect biases/prejudices fromthe training data. Grounding on the existing fairness notions, we lay out asample bias criterion and propose practical algorithms for measuring andcountering sample bias. The derived bias score provides intuitive sample-levelattribution and explanation of historical bias in data. On this basis, wefurther design two FairML strategies via sample-bias-informed minimal dataediting. They can mitigate both group and individual unfairness at the cost ofminimal or zero predictive utility loss. Extensive experiments and analyses onmultiple real-world datasets demonstrate the effectiveness of our methods inexplaining and mitigating unfairness. Code is available athttps://github.com/ZhiningLiu1998/AIM. | Zhining Liu, Ruizhong Qiu, Zhichen Zeng, Yada Zhu, Hendrik F. Hamann, Hanghang Tong | IBM Research, Yorktown Heights, NY, USA; University of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [High-Dimensional Distributed Sparse Classification with Scalable Communication-Efficient Global Updates](https://doi.org/10.1145/3637528.3672038) |  | 0 | As the size of datasets used in statistical learning continues to grow, distributed training of models has attracted increasing attention. These methods partition the data and exploit parallelism to reduce memory and runtime, but suffer increasingly from communication costs as the data size or the number of iterations grows. Recent work on linear models has shown that a surrogate likelihood can be optimized locally to iteratively improve on an initial solution in a communication-efficient manner. However, existing versions of these methods experience multiple shortcomings as the data size becomes massive, including diverging updates and efficiently handling sparsity. In this work we develop solutions to these problems which enable us to learn a communication-efficient distributed logistic regression model even beyond millions of features. In our experiments we demonstrate a large improvement in accuracy over distributed algorithms with only a few distributed update steps needed, and similar or faster runtimes. Our code is available at https://github.com/FutureComputing4AI/ProxCSL. | Fred Lu, Ryan R. Curtin, Edward Raff, Francis Ferraro, James Holt | Booz Allen Hamilton & University of Maryland, Baltimore County, McLean, USA; University of Maryland, Baltimore County, Baltimore, USA; Booz Allen Hamilton, McLean, USA; Laboratory for Physical Sciences, College Park, USA |
|  |  [FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks](https://doi.org/10.1145/3637528.3671834) |  | 0 | Fairness-aware Graph Neural Networks (GNNs) often face a challenging trade-off, where prioritizing fairness may require compromising utility. In this work, we re-examine fairness through the lens of spectral graph theory, aiming to reconcile fairness and utility within the framework of spectral graph learning. We explore the correlation between sensitive features and spectrum in GNNs, using theoretical analysis to delineate the similarity between original sensitive features and those after convolution under different spectra. Our analysis reveals a reduction in the impact of similarity when the eigenvectors associated with the largest magnitude eigenvalue exhibit directional similarity. Based on these theoretical insights, we propose FUGNN, a novel spectral graph learning approach that harmonizes the conflict between fairness and utility. FUGNN ensures algorithmic fairness and utility by truncating the spectrum and optimizing eigenvector distribution during the encoding process. The fairness-aware eigenvector selection reduces the impact of convolution on sensitive features while concurrently minimizing the sacrifice of utility. FUGNN further optimizes the distribution of eigenvectors through a transformer architecture. By incorporating the optimized spectrum into the graph convolution network, FUGNN effectively learns node representations. Experiments on six real-world datasets demonstrate the superiority of FUGNN over baseline methods. The codes are available at https://github.com/yushuowiki/FUGNN. | Renqiang Luo, Huafei Huang, Shuo Yu, Zhuoyang Han, Estrid He, Xiuzhen Zhang, Feng Xia | RMIT University, Melbourne, Victoria, Australia; Dalian University of Technology, Dalian, Liaoning, China |
|  |  [Learning Multi-view Molecular Representations with Structured and Unstructured Knowledge](https://doi.org/10.1145/3637528.3672043) |  | 0 | Capturing molecular knowledge with representation learning approaches holdssignificant potential in vast scientific fields such as chemistry and lifescience. An effective and generalizable molecular representation is expected tocapture the consensus and complementary molecular expertise from diverse viewsand perspectives. However, existing works fall short in learning multi-viewmolecular representations, due to challenges in explicitly incorporating viewinformation and handling molecular knowledge from heterogeneous sources. Toaddress these issues, we present MV-Mol, a molecular representation learningmodel that harvests multi-view molecular expertise from chemical structures,unstructured knowledge from biomedical texts, and structured knowledge fromknowledge graphs. We utilize text prompts to model view information and designa fusion architecture to extract view-based molecular representations. Wedevelop a two-stage pre-training procedure, exploiting heterogeneous data ofvarying quality and quantity. Through extensive experiments, we show thatMV-Mol provides improved representations that substantially benefit molecularproperty prediction. Additionally, MV-Mol exhibits state-of-the-art performancein multi-modal comprehension of molecular structures and texts. Code and dataare available at https://github.com/PharMolix/OpenBioMed. | Yizhen Luo, Kai Yang, Massimo Hong, Xing Yi Liu, Zikun Nie, Hao Zhou, Zaiqing Nie | Institute for AI Industry Research (AIR), Tsinghua University & Pharmolix Inc., Beijing, China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China |
|  |  [Cross-Context Backdoor Attacks against Graph Prompt Learning](https://doi.org/10.1145/3637528.3671956) |  | 0 | Graph Prompt Learning (GPL) bridges significant disparities betweenpretraining and downstream applications to alleviate the knowledge transferbottleneck in real-world graph learning. While GPL offers superioreffectiveness in graph knowledge transfer and computational efficiency, thesecurity risks posed by backdoor poisoning effects embedded in pretrainedmodels remain largely unexplored. Our study provides a comprehensive analysisof GPL's vulnerability to backdoor attacks. We introduce CrossBA, thefirst cross-context backdoor attack against GPL, which manipulates only thepretraining phase without requiring knowledge of downstream applications. Ourinvestigation reveals both theoretically and empirically that tuning triggergraphs, combined with prompt transformations, can seamlessly transfer thebackdoor threat from pretrained encoders to downstream applications. Throughextensive experiments involving 3 representative GPL methods across 5 distinctcross-context scenarios and 5 benchmark datasets of node and graphclassification tasks, we demonstrate that CrossBA consistentlyachieves high attack success rates while preserving the functionality ofdownstream applications over clean input. We also explore potentialcountermeasures against CrossBA and conclude that current defenses areinsufficient to mitigate CrossBA. Our study highlights the persistentbackdoor threats to GPL systems, raising trustworthiness concerns in thepractices of GPL techniques. | Xiaoting Lyu, Yufei Han, Wei Wang, Hangwei Qian, Ivor Tsang, Xiangliang Zhang | School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University & Xi'an Jiaotong University, Beijing, China; University of Notre Dame, Notre Dame, USA; CFAR, ASTAR, Singapore, Singapore; Inria, Univ. Rennes, IRISA, Rennes, France |
|  |  [PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer](https://doi.org/10.1145/3637528.3671849) |  | 0 | Spectral Graph Neural Networks have demonstrated superior performance in graph representation learning. However, many current methods focus on employing shared polynomial coefficients for all nodes, i.e., learning node-unified filters, which limits the filters' flexibility for node-level tasks. The recent DSF attempts to overcome this limitation by learning node-wise coefficients based on positional encoding. However, the initialization and updating process of the positional encoding are burdensome, hindering scalability on large-scale graphs. In this work, we propose a scalable node-wise filter, PolyAttn. Leveraging the attention mechanism, PolyAttn can directly learn node-wise filters in an efficient manner, offering powerful representation capabilities. Building on PolyAttn, we introduce the whole model, named PolyFormer. In the lens of Graph Transformer models, PolyFormer, which calculates attention scores within nodes, shows great scalability. Moreover, the model captures spectral information, enhancing expressiveness while maintaining efficiency. With these advantages, PolyFormer offers a desirable balance between scalability and expressiveness for node-level tasks. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes. The code is available at https://github.com/air029/PolyFormer. | Jiahong Ma, Mingguo He, Zhewei Wei | Renmin University of China, Beijing, China |
|  |  [Scalable Differentiable Causal Discovery in the Presence of Latent Confounders with Skeleton Posterior](https://doi.org/10.1145/3637528.3672031) |  | 0 | Differentiable causal discovery has made significant advancements in the learning of directed acyclic graphs. However, its application to real-world datasets remains restricted due to the ubiquity of latent confounders and the requirement to learn maximal ancestral graphs (MAGs). To date, existing differentiable MAG learning algorithms have been limited to small datasets and failed to scale to larger ones (e.g., with more than 50 variables). The key insight in this paper is that the causal skeleton, which is the undirected version of the causal graph, has potential for improving accuracy and reducing the search space of the optimization procedure, thereby enhancing the performance of differentiable causal discovery. Therefore, we seek to address a two-fold challenge to harness the potential of the causal skeleton for differentiable causal discovery in the presence of latent confounders: (1) scalable and accurate estimation of skeleton and (2) universal integration of skeleton estimation with differentiable causal discovery. To this end, we propose SPOT (Skeleton Posterior-guided OpTimization), a two-phase framework that harnesses skeleton posterior for differentiable causal discovery in the presence of latent confounders. On the contrary to a "point-estimation", SPOT seeks to estimate the posterior distribution of skeletons given the dataset. It first formulates the posterior inference as an instance of amortized inference problem and concretizes it with a supervised causal learning (SCL)-enabled solution to estimate the skeleton posterior. To incorporate the skeleton posterior with differentiable causal discovery, SPOT then features a skeleton posterior-guided stochastic optimization procedure to guide the optimization of MAGs. Extensive experiments on various datasets show that SPOT substantially outperforms SOTA methods for MAG learning. SPOT also demonstrates its effectiveness in the accuracy of skeleton posterior estimation in comparison with non-parametric bootstrap-based, or more recently, variational inference-based methods. Finally, we observe that the adoption of skeleton posterior exhibits strong promise in various causal discovery tasks. | Pingchuan Ma, Rui Ding, Qiang Fu, Jiaru Zhang, Shuai Wang, Shi Han, Dongmei Zhang | Microsoft Research, Beijing, China; Shanghai Jiao Tong University, Shanghai, China; HKUST, Hong Kong, Hong Kong |
|  |  [Graph Anomaly Detection with Few Labels: A Data-Centric Approach](https://doi.org/10.1145/3637528.3671929) |  | 0 | Anomalous node detection in a static graph faces significant challenges due to the rarity of anomalies and the substantial cost of labeling their deviant structure and attribute patterns. These challenges give rise to data-centric problems, including extremely imbalanced data distributions and intricate graph learning, which significantly impede machine learning and deep learning methods from discerning the patterns of graph anomalies with few labels. While these issues remain crucial, much of the current research focuses on addressing the induced technical challenges, treating the shortage of labeled data as a given. Distinct from previous efforts, this work focuses on tackling the data-centric problems by generating auxiliary training nodes that conform to the original graph topology and attribute distribution. We categorize this approach as data-centric, aiming to enhance existing anomaly detectors by training them on our synthetic data. However, the methods for generating nodes and the effectiveness of utilizing synthetic data for graph anomaly detection remain unexplored in the realm. To answer these questions, we thoroughly investigate the denoising diffusion model. Drawing from our observations on the diffusion process, we illuminate the shifts in graph energy distribution and establish two principles for designing denoising neural networks tailored to graph anomaly generation. From the insights, we propose a diffusion-based graph generation method to synthesize training nodes, which can be promptly integrated to work with existing anomaly detectors. The empirical results on eight widely-used datasets demonstrate our generated data can effectively enhance the nine state-of-the-art graph detectors' performance. | Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, Jia Wu | Department of Statistics and Data Science, Northwestern University, Evanston, Illinois, USA; School of Computing, Macquarie University, Sydney, New South Wales, Australia; Business School, The University of Sydney, Sydney, New South Wales, Australia |
|  |  [A Uniformly Bounded Correlation Function for Spatial Point Patterns](https://doi.org/10.1145/3637528.3671891) |  | 0 | A point pattern is a dataset of coordinates, typically in 2D or 3D space. Point patterns are ubiquitous in diverse applications including Geographic Information Systems, Astronomy, Ecology, Biology and Medicine. Among the statistics used to quantify point patterns, most are based on Ripley's K-function, which measures the deviation of the observed pattern from a completely random arrangement of points. This approach is useful for constructing null hypothesis tests, but Ripley's K and its variants are less suitable as quantitative effect sizes because their ranges and expected values generally depend on the scale or the size of the region in which the pattern is observed. To address this, we propose a new function that behaves like a correlation coefficient for point patterns: it is tightly bounded by -1 and 1, with a value of -1 corresponding to a maximally dispersed arrangement of points, 0 indicating complete spatial randomness, and 1 representing maximal clustering. These properties are independent of scale and observation window size assuming appropriate edge correction. Evaluating our function on simulated data, we show that it has comparable statistical calibration and power to K-based baselines. We hope that the ease of interpretation of our bounded function will facilitate the analysis of spatial data across multiple fields. | Evgenia Martynova, Johannes Textor | Radboud University Medical Center, Nijmegen, Netherlands; Radboud University, Nijmegen, Netherlands |
|  |  [Fair Column Subset Selection](https://doi.org/10.1145/3637528.3672005) |  | 0 | We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data. | Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi | Max Planck Institute for Software Systems, Kaiserslautern, Germany; Unaffiliated, London, United Kingdom; Aalto University, Espoo, Finland |
|  |  [FLAIM: AIM-based Synthetic Data Generation in the Federated Setting](https://doi.org/10.1145/3637528.3671990) |  | 0 | Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead. | Samuel Maddock, Graham Cormode, Carsten Maple | University of Warwick, Coventry, United Kingdom; Meta AI & University of Warwick, Coventry, United Kingdom |
|  |  [Interpretable Transformer Hawkes Processes: Unveiling Complex Interactions in Social Networks](https://doi.org/10.1145/3637528.3671720) |  | 0 | Social networks represent complex ecosystems where the interactions between users or groups play a pivotal role in information dissemination, opinion formation, and social interactions. Effectively harnessing event sequence data within social networks to unearth interactions among users or groups has persistently posed a challenging frontier within the realm of point processes. Current deep point process models face inherent limitations within the context of social networks, constraining both their interpretability and expressive power. These models encounter challenges in capturing interactions among users or groups and often rely on parameterized extrapolation methods when modeling intensity over non-event intervals, limiting their capacity to capture intricate intensity patterns, particularly beyond observed events. To address these challenges, this study proposes modifications to Transformer Hawkes processes (THP), leading to the development of interpretable Transformer Hawkes processes (ITHP). ITHP inherits the strengths of THP while aligning with statistical nonlinear Hawkes processes, thereby enhancing its interpretability and providing valuable insights into interactions between users or groups. Additionally, ITHP enhances the flexibility of the intensity function over non-event intervals, making it better suited to capture complex event propagation patterns in social networks. Experimental results, both on synthetic and real data, demonstrate the effectiveness of ITHP in overcoming the identified limitations. Moreover, they highlight ITHP's applicability in the context of exploring the complex impact of users or groups within social networks. Our code is available at https://github.com/waystogetthere/Interpretable-Transformer- Hawkes-Process.git. | Zizhuo Meng, Ke Wan, Yadong Huang, Zhidong Li, Yang Wang, Feng Zhou | Zhoushan Academy of Marine Data Science, Zhoushan, China; The University of Technology Sydney, Sydney, Australia; University of Illinois at Urbana-Champaign, Urbana, IL, USA |
|  |  [Scaling Training Data with Lossy Image Compression](https://doi.org/10.1145/3637528.3671904) |  | 0 | Empirically-determined scaling laws have been broadly successful in predicting the evolution of large machine learning models with training data and number of parameters. As a consequence, they have been useful for optimizing the allocation of limited resources, most notably compute time. In certain applications, storage space is an important constraint, and data format needs to be chosen carefully as a consequence. Computer vision is a prominent example: images are inherently analog, but are always stored in a digital format using a finite number of bits. Given a dataset of digital images, the number of bits L to store each of them can be further reduced using lossy data compression. This, however, can degrade the quality of the model trained on such images, since each example has lower resolution. In order to capture this trade-off and optimize storage of training data, we propose a \`storage scaling law' that describes the joint evolution of test error with sample size and number of bits per image. We prove that this law holds within a stylized model for image compression, and verify it empirically on two computer vision tasks, extracting the relevant parameters. We then show that this law can be used to optimize the lossy compression level. At given storage, models trained on optimally compressed images present a significantly smaller test error with respect to models trained on the original data. Finally, we investigate the potential benefits of randomizing the compression level. | Katherine L. Mentzer, Andrea Montanari | Granica, Mountain View, CA, USA |
|  |  [Learning Causal Networks from Episodic Data](https://doi.org/10.1145/3637528.3671999) |  | 0 | In numerous real-world domains, spanning from environmental monitoring to long-term medical studies, observations do not arrive in a single batch but rather over time in episodes. This challenges the traditional assumption in causal discovery of a single, observational dataset, not only because each episode may be a biased sample of the population but also because multiple episodes could differ in the causal interactions underlying the observed variables. We address these issues using notions of context switches and episodic selection bias, and introduce a framework for causal modeling of episodic data. We show under which conditions we can apply information-theoretic scoring criteria for causal discovery while preserving consistency. To in practice discover the causal model progressively over time, we propose the CONTINENT algorithm which, taking inspiration from continual learning, discovers the causal model in an online fashion without having to re-learn the model upon arrival of each new episode. Our experiments over a variety of settings including selection bias, unknown interventions, and network changes showcase that CONTINENT works well in practice and outperforms the baselines by a clear margin. | Osman Mian, Sarah Mameche, Jilles Vreeken | CISPA Helmholtz Center for Information Security, Saarbruecken, Germany |
|  |  [Money Never Sleeps: Maximizing Liquidity Mining Yields in Decentralized Finance](https://doi.org/10.1145/3637528.3671942) |  | 0 | The popularity of decentralized finance has drawn attention to liquidity mining (LM). In LM, a user deposits her cryptocurrencies into liquidity pools to provide liquidity for exchanges and earn yields. Different liquidity pools offer varying yields and require different pairs of cryptocurrencies. A user can exchange a cryptocurrency for another with some exchange costs. Thus, an LM solution consists of exchange transactions and deposit transactions, guaranteeing (1) each exchange transaction must exchange one cryptocurrency for another at a specific rate (i.e., the exchange constraint); (2) the amounts of cryptocurrencies deposited in a liquidity pool must exceed the required threshold (i.e., the minimum constraint); (3) each deposit transaction must deposit a specific pair of cryptocurrencies at a certain rate in a liquidity pool (i.e., the deposit constraint); and (4) the cryptocurrencies used in the solution do not exceed the cryptocurrencies that the user has (i.e., the budget constraint). Selecting the most profitable LM solution is challenging due to the vast number of candidate solutions. To address this challenge, we define the yield maximization liquidity mining (YMLM) problem. Given a set of liquidity pools, a set of the user's cryptocurrencies, a set of exchange rates, and an evaluation function, YMLM aims to find an LM solution with maximal yields, satisfying the minimum, exchange, deposit, and budget constraints. We prove that YMLM is NP-hard and cannot be solved by algorithms with constant approximation ratios. To tackle YMLM, we propose two algorithms, namely YMLM\_GD and YMLM\_SK, with parameterized approximation ratios. Extensive experiments on both real and synthetic datasets show that our approaches outperform the baselines in yields. | Wangze Ni, Yiwei Zhao, Weijie Sun, Lei Chen, Peng Cheng, Chen Jason Zhang, Xuemin Lin | Hong Kong Polytechnic University, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Shanghai Jiao Tong University, Shanghai, China; East China Normal University, Shanghai, China |
|  |  [Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series](https://doi.org/10.1145/3637528.3671760) |  | 0 | Multivariate time series data suffer from the problem of missing values, which hinders the application of many analytical methods. To achieve the accurate imputation of these missing values, exploiting inter-correlation by employing the relationships between sequences (i.e., a network) is as important as the use of temporal dependency, since a sequence normally correlates with other sequences. Moreover, exploiting an adequate network depending on time is also necessary since the network varies over time. However, in real-world scenarios, we normally know neither the network structure nor when the network changes beforehand. Here, we propose a missing value imputation method for multivariate time series, namely MissNet, that is designed to exploit temporal dependency with a state-space model and inter-correlation by switching sparse networks. The network encodes conditional independence between features, which helps us understand the important relationships for imputation visually. Our algorithm, which scales linearly with reference to the length of the data, alternatively infers networks and fills in missing values using the networks while discovering the switching of the networks. Extensive experiments demonstrate that MissNet outperforms the state-of-the-art algorithms for multivariate time series imputation and provides interpretable results. | Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai | SANKEN, Osaka University, Suita, Osaka, Japan |
|  |  [Ontology Enrichment for Effective Fine-grained Entity Typing](https://doi.org/10.1145/3637528.3671857) |  | 0 | Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose øurs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that øurs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. øurs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance. | Siru Ouyang, Jiaxin Huang, Pranav Pillai, Yunyi Zhang, Yu Zhang, Jiawei Han | University of Illinois Urbana-Champaign, Urbana, USA; Washington University in Saint Louis, St. Louis, MO, USA; University of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [BTTackler: A Diagnosis-based Framework for Efficient Deep Learning Hyperparameter Optimization](https://doi.org/10.1145/3637528.3671933) |  | 0 | Hyperparameter optimization (HPO) is known to be costly in deep learning, especially when leveraging automated approaches. Most of the existing automated HPO methods are accuracy-based, i.e., accuracy metrics are used to guide the trials of different hyperparameter configurations amongst a specific search space. However, many trials may encounter severe training problems, such as vanishing gradients and insufficient convergence, which can hardly be reflected by accuracy metrics in the early stages of the training and often result in poor performance. This leads to an inefficient optimization trajectory because the bad trials occupy considerable computation resources and reduce the probability of finding excellent hyperparameter configurations within a time limitation. In this paper, we propose Bad Trial Tackler (BTTackler), a novel HPO framework that introduces training diagnosis to identify training problems automatically and hence tackles bad trials. BTTackler diagnoses each trial by calculating a set of carefully designed quantified indicators and triggers early termination if any training problems are detected. Evaluations are performed on representative HPO tasks consisting of three classical deep neural networks (DNN) and four widely used HPO methods. To better quantify the effectiveness of an automated HPO method, we propose two new measurements based on accuracy and time consumption. Results show the advantage of BTTackler on two-fold: (1) it reduces 40.33% of time consumption to achieve the same accuracy comparable to baseline methods on average and (2) it conducts 44.5% more top-10 trials than baseline methods on average within a given time budget. We also released an open-source Python library that allows users to easily apply BTTackler to automated HPO processes with minimal code changes\footnotehttps://github.com/thuml/BTTackler. | Zhongyi Pei, Zhiyao Cen, Yipeng Huang, Chen Wang, Lin Liu, Philip S. Yu, Mingsheng Long, Jianmin Wang | School of Software, EIRI, Tsinghua University, Beijing, China; School of Software, BNRist, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China |
|  |  [Fast Multidimensional Partial Fourier Transform with Automatic Hyperparameter Selection](https://doi.org/10.1145/3637528.3671667) |  | 0 | Given a multidimensional array, how can we optimize the computation process for a part of Fourier coefficients? Discrete Fourier transform plays an overarching role in various data mining tasks. Recent interest has focused on efficiently calculating a small part of Fourier coefficients, exploiting the energy compaction property of real-world data. Current methods for partial Fourier transform frequently encounter efficiency issues, yet the adoption of pre-computation techniques within the PFT algorithm has shown promising performance. However, PFT still faces limitations in handling multidimensional data efficiently and requires manual hyperparameter tuning, leading to additional costs. In this paper, we propose Auto-MPFT (Automatic Multidimensional Partial Fourier Transform), which efficiently computes a subset of Fourier coefficients in multidimensional data without the need for manual hyperparameter search. Auto-MPFT leverages multivariate polynomial approximation for trigonometric functions, generalizing its domain to multidimensional Euclidean space. Moreover, we present a convex optimization-based algorithm for automatically selecting the optimal hyperparameter of Auto-MPFT. We provide a rigorous proof for the explicit reformulation of the original optimization problem of Auto-MPFT, demonstrating the process that converts it into a well-established unconstrained convex optimization problem. Extensive experiments show that Auto-MPFT surpasses existing partial Fourier transform methods and optimized FFT libraries, achieving up to 7.6x increase in speed without sacrificing accuracy. In addition, our optimization algorithm accurately finds the optimal hyperparameter for Auto-MPFT, significantly reducing the cost associated with hyperparameter search. | Yongchan Park, Jongjin Kim, U Kang | Seoul National University, Seoul, Republic of Korea |
|  |  [CoMAL: Contrastive Active Learning for Multi-Label Text Classification](https://doi.org/10.1145/3637528.3671754) |  | 0 | Multi-label text classification (MLTC) allows a given text to be associated with multiple labels, which well suits many real-world data mining scenarios. However, the annotation effort of MLTC is inevitably expensive and time-consuming. Although multi-label active learning provides a cost-effective solution, it still faces two major challenges: (i) constructing decent feature space to distinguish the confusing semantics of different labels; (ii) defining proper sampling criteria to measure a sample's joint effect over the entire label space. To bridge these gaps, we propose a Contrastive Multi-label Active Learning framework (CoMAL) that gives an effective data acquisition strategy. Specifically, a contrastive decoupling mechanism is introduced to fully release the semantic information of multiple labels into the latent space. Then, we devise a hybrid criterion that balances two data value measures: (i) similarity-enhanced label cardinality inconsistency reflects the uncertainty of data predictions. (ii) positive feature diversity evaluates the positive-propensity semantic diversity to handle the label sparsity. Extensive experiments demonstrate that our CoMAL outperforms the current state-of-the-art multi-label active learning approaches. Code for CoMAL is available at https://github.com/chengzju/CoMAL. | Cheng Peng, Haobo Wang, Ke Chen, Lidan Shou, Chang Yao, Runze Wu, Gang Chen | State Key Lab of Blockchain and Data Security, Zhejiang University, Hangzhou, China; Fuxi AI Lab, NetEase Corp., Hangzhou, China; School of Software Technology, Zhejiang University, Ningbo, China |
|  |  [TSC: A Simple Two-Sided Constraint against Over-Smoothing](https://doi.org/10.1145/3637528.3671954) |  | 0 | Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once. Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN. | Furong Peng, Kang Liu, Xuan Lu, Yuhua Qian, HongRen Yan, Chao Ma | HOPERUN Infomation Technology, Nanjing, Jiangsu, China; College of Physics and Electronic Engineering, Shanxi University, Taiyuan, Shanxi, China |
|  |  [CASH via Optimal Diversity for Ensemble Learning](https://doi.org/10.1145/3637528.3671894) |  | 0 | The Combined Algorithm Selection and Hyperparameter Optimization (CASH) problem is pivotal in Automatic Machine Learning (AutoML). Most leading approaches combine Bayesian optimization with post-hoc ensemble building to create advanced AutoML systems. Bayesian optimization (BO) typically focuses on identifying a singular algorithm and its hyperparameters that outperform all other configurations. Recent developments have highlighted an oversight in prior CASH methods: the lack of consideration for diversity among the base learners of the ensemble. This oversight was overcome by explicitly injecting the search for diversity into the traditional CASH problem. However, despite recent developments, BO's limitation lies in its inability to directly optimize ensemble generalization error, offering no theoretical assurance that increased diversity correlates with enhanced ensemble performance. Our research addresses this gap by establishing a theoretical foundation that integrates diversity into the core of BO for direct ensemble learning. We explore a theoretically sound framework that describes the relationship between pair-wise diversity and ensemble performance, which allows our Bayesian optimization framework Optimal Diversity Bayesian Optimization (OptDivBO) to directly and efficiently minimize ensemble generalization error. OptDivBO guarantees an optimal balance between pairwise diversity and individual model performance, setting a new precedent in ensemble learning within CASH. Empirical results on 20 public datasets show that OptDivBO achieves the best average test ranks of 1.57 and 1.4 in classification and regression tasks. | Pranav Poduval, Sanjay Kumar Patnala, Gaurav Oberoi, Nitish Srivasatava, Siddhartha Asthana | Mastercard AI Garage, Gurgaon, India; MasterCard AI Garage, Gurgaon, India |
|  |  [Unifying Evolution, Explanation, and Discernment: A Generative Approach for Dynamic Graph Counterfactuals](https://doi.org/10.1145/3637528.3671831) |  | 0 | We present GRACIE (Graph Recalibration and Adaptive Counterfactual Inspection and Explanation), a novel approach for generative classification and counterfactual explanations of dynamically changing graph data. We study graph classification problems through the lens of generative classifiers. We propose a dynamic, self-supervised latent variable model that updates by identifying plausible counterfactuals for input graphs and recalibrating decision boundaries through contrastive optimization. Unlike prior work, we do not rely on linear separability between the learned graph representations to find plausible counterfactuals. Moreover, GRACIE eliminates the need for stochastic sampling in latent spaces and graph-matching heuristics. Our work distills the implicit link between generative classification and loss functions in the latent space, a key insight to understanding recent successes with this architecture. We further observe the inherent trade-off between validity and pulling explainee instances towards the central region of the latent space, empirically demonstrating our theoretical findings. In extensive experiments on synthetic and real-world graph data, we attain considerable improvements, reaching ~99% validity when sampling sets of counterfactuals even in the challenging setting of dynamic data landscapes. | Bardh Prenkaj, Mario VillaizánVallelado, Tobias Leemann, Gjergji Kasneci | Technical University of Munich, Munich, Germany; University of Tübingen, Tübingen, Germany; University of Valladolid & Telefónica Research and Development, Valladolid, Spain |
|  |  [Reimagining Graph Classification from a Prototype View with Optimal Transport: Algorithm and Theorem](https://doi.org/10.1145/3637528.3671696) |  | 0 | Recently, Graph Neural Networks (GNNs) have achieved inspiring performances in graph classification tasks. However, the message passing mechanism in GNNs implicitly utilizes the topological information of the graph, which may lead to a potential loss of structural information. Furthermore, the graph classification decision process based on GNNs resembles a black box and lacks sufficient transparency. The non-linear classifier following the GNNs also defaults to the assumption that each class is represented by a single vector, thereby limiting the diversity of intra-class representations. To address these issues, we propose a novel prototype-based graph classification framework that introduces the Fused Gromov-Wasserstein (FGW) distance in Optimal Transport (OT) as the similarity measure. In this way, the model explicitly exploits the structural information on the graph through OT while leading to a more transparent and straightforward classification process. The introduction of prototypes also inherently addresses the issue of limited within-class representations. Besides, to alleviate the widely acknowledged computational complexity issue of FGW distance calculation, we devise a simple yet effective NN-based FGW distance approximator, which can enable full GPU training acceleration with a marginal performance loss. In theory, we analyze the generalization performance of the proposed method and derive an O (1 over N) generalization bound, where the proof techniques can be extended to a broader range of prototype-based classification frameworks. Experimental results show that the proposed framework achieves competitive and superior performance on several widely used graph classification benchmark datasets. The code is avaliable at https://github.com/ChnQ/PGOT. | Chen Qian, Huayi Tang, Hong Liang, Yong Liu | School of Electronic and Computer Engineering, Peking University, Shenzhen, China; Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China |
|  |  [Pre-train and Refine: Towards Higher Efficiency in K-Agnostic Community Detection without Quality Degradation](https://doi.org/10.1145/3637528.3671686) |  | 0 | Community detection (CD) is a classic graph inference task that partitionsnodes of a graph into densely connected groups. While many CD methods have beenproposed with either impressive quality or efficiency, balancing the twoaspects remains a challenge. This study explores the potential of deep graphlearning to achieve a better trade-off between the quality and efficiency ofK-agnostic CD, where the number of communities K is unknown. We propose PRoCD(Pre-training Refinement fOr Community Detection), a simple yet effectivemethod that reformulates K-agnostic CD as the binary node pair classification.PRoCD follows a pre-training refinement paradigm inspired by recent advancesin pre-training techniques. We first conduct the offline pre-training of PRoCDon small synthetic graphs covering various topology properties. Based on theinductive inference across graphs, we then generalize the pre-trained model(with frozen parameters) to large real graphs and use the derived CD results asthe initialization of an existing efficient CD method (e.g., InfoMap) tofurther refine the quality of CD results. In addition to benefiting from thetransfer ability regarding quality, the online generalization and refinementcan also help achieve high inference efficiency, since there is notime-consuming model optimization. Experiments on public datasets with variousscales demonstrate that PRoCD can ensure higher efficiency in K-agnostic CDwithout significant quality degradation. | Meng Qin, Chaorui Zhang, Yu Gao, Weixi Zhang, DitYan Yeung | Department of CSE, HKUST, Hong Kong, Hong Kong; Theory Lab, Huawei, Hong Kong, Hong Kong; Theory Lab, Huawei, Beijing, China |
|  |  [RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms](https://doi.org/10.1145/3637528.3672062) |  | 0 | We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS) framework, designed to assess the robustness of hierarchical time series forecasting models and algorithms on real-world datasets. Hierarchical time series, where lower-level forecasts must sum to upper-level ones, are prevalent in various contexts, such as retail sales across countries. Current empirical evaluations of forecasting methods are often limited to a small set of benchmark datasets, offering a narrow view of algorithm behavior. RHiOTS addresses this gap by systematically altering existing datasets and modifying the characteristics of individual series and their interrelations. It uses a set of parameterizable transformations to simulate those changes in the data distribution. Additionally, RHiOTS incorporates an innovative visualization component, turning complex, multidimensional robustness evaluation results into intuitive, easily interpretable visuals. This approach allows an in-depth analysis of algorithm and model behavior under diverse conditions. We illustrate the use of RHiOTS by analyzing the predictive performance of several algorithms. Our findings show that traditional statistical methods are more robust than state-of-the-art deep learning algorithms, except when the transformation effect is highly disruptive. Furthermore, we found no significant differences in the robustness of the algorithms when applying specific reconciliation methods, such as MinT. RHiOTS provides researchers with a comprehensive tool for understanding the nuanced behavior of forecasting algorithms, offering a more reliable basis for selecting the most appropriate method for a given problem. | Luis Roque, Carlos Soares, Luís Torgo | Dalhousie University, Halifax, Canada; LIACCFaculty of Engineering, University of Porto, Porto, Portugal; LIACCFaculty of Engineering, University of Porto & Fraunhofer AICOS Portugal, Porto, Portugal |
|  |  [A Fast Exact Algorithm to Enumerate Maximal Pseudo-cliques in Large Sparse Graphs](https://doi.org/10.1145/3637528.3672066) |  | 0 | Pseudo-cliques (subgraphs with almost all possible edges) have many applications. But they do not satisfy the convertible antimonotone constraint (as we prove here). So, it is hard to reduce the search space of pseudo-cliques and list them efficiently. To our knowledge, only two exact algorithms, namely, ODES and PCE, were proposed for this purpose, but both have high execution times. Here, we present an exact algorithm named Fast Pseudo-Clique Enumerator (FPCE). It employs some pruning techniques we derived to reduce the search space. Our experiment on 15 real and 16 synthetic graphs shows that (i) on real graphs, FPCE is, on average, 38.6 and 6.5 times faster than ODES and PCE, respectively, whereas (ii) on synthetic graphs, FPCE is, on average, 39.7 and 3.1 times faster than ODES and PCE, respectively. We apply FPCE and a popular heuristic method on a PPI network to identify pseudo-cliques. FPCE outputs match with more known protein complexes, are more accurate, and are biologically more significant - suggesting that the exact computation of pseudo-cliques may give better insights. For its speed, FPCE is a suitable choice in such cases. | Ahsanur Rahman, Kalyan Roy, Ramiza Maliha, Townim Faisal Chowdhury | North South University, Dhaka, Bangladesh; Australian Institute for Machine Learning, University of Adelaide, Adelaide, Australia |
|  |  [CoSLight: Co-optimizing Collaborator Selection and Decision-making to Enhance Traffic Signal Control](https://doi.org/10.1145/3637528.3671998) |  | 0 | Effective multi-intersection collaboration is pivotal for reinforcement-learning-based traffic signal control to alleviate congestion. Existing work mainly chooses neighboring intersections as collaborators. However, quite a lot of congestion, even some wide-range congestion, is caused by non-neighbors failing to collaborate. To address these issues, we propose to separate the collaborator selection as a second policy to be learned, concurrently being updated with the original signal-controlling policy. Specifically, the selection policy in real-time adaptively selects the best teammates according to phase- and intersection-level features. Empirical results on both synthetic and real-world datasets provide robust validation for the superiority of our approach, offering significant improvements over existing state-of-the-art methods. Code is available at https://github.com/bonaldli/CoSLight. | Jingqing Ruan, Ziyue Li, Hua Wei, Haoyuan Jiang, Jiaming Lu, Xuantang Xiong, Hangyu Mao, Rui Zhao | Baidu Inc., Shenzhen, China; Institute of Automation, Chinese Academy of Science, Chinese Academy of Sciences, Beijing, China; University of Cologne, EWI gGmbH, Cologne, Germany; Fudan University, Shanghai, China; Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China; Arizona State University, Arizona, USA; Peking University, Beijing, China |
|  |  [A Novel Feature Space Augmentation Method to Improve Classification Performance and Evaluation Reliability](https://doi.org/10.1145/3637528.3671736) |  | 0 | Classification tasks in many real-world domains are exacerbated by class imbalance, relatively small sample sizes compared to high dimensionality, and measurement uncertainty. The problem of class imbalance has been extensively studied, and data augmentation methods based on interpolation of minority class instances have been proposed as a viable solution to mitigate imbalance. It remains to be seen whether augmentation can be applied to improve the overall performance while maintaining stability, especially with a limited number of samples. In this paper, we present a novel feature-space augmentation technique that can be applied to high-dimensional data for classification tasks and address these issues. Our method utilizes uniform random sampling and introduces synthetic instances by taking advantage of the local distributions of individual features in the observed instances. The core augmentation algorithm is class-invariant, which opens up an unexplored avenue of simultaneously improving and stabilizing performance by augmenting unlabeled instances. The proposed method is evaluated using a comprehensive performance analysis involving multiple classifiers and metrics. Comparative analysis with existing feature space augmentation methods strongly suggests that the proposed algorithm can result in improved classification performance while also increasing the overall reliability of the performance evaluation. | Sakhawat Hossain Saimon, Tanzira Najnin, Jianhua Ruan | Department of Computer Science, The University of Texas at San Antonio, San Antonio, TX, USA |
|  |  [Scalable Temporal Motif Densest Subnetwork Discovery](https://doi.org/10.1145/3637528.3671889) |  | 0 | Finding dense subnetworks, with density based on edges or more complex structures, such as subgraphs or k-cliques, is a fundamental algorithmic problem with many applications. While the problem has been studied extensively in static networks, much remains to be explored for temporal networks. In this work we introduce the novel problem of identifying the temporal motif densest subnetwork, i.e., the densest subnetwork with respect to temporal motifs, which are high-order patterns characterizing temporal networks. Identifying temporal motifs is an extremely challenging task, and thus, efficient methods are required. To address this challenge, we design two novel randomized approximation algorithms with rigorous probabilistic guarantees that provide high-quality solutions. We perform extensive experiments showing that our methods outperform baselines. Furthermore, our algorithms scale on networks with up to billions of temporal edges, while baselines cannot handle such large networks. We use our techniques to analyze a financial network and show that our formulation reveals important network structures, such as bursty temporal events and communities of users with similar interests. | Ilie Sarpe, Fabio Vandin, Aristides Gionis | KTH Royal Institute of Technology, Stockholm, Sweden; University of Padova, Padova, Italy |
|  |  [DPHGNN: A Dual Perspective Hypergraph Neural Networks](https://doi.org/10.1145/3637528.3672047) |  | 0 | Message passing on hypergraphs has been a standard framework for learning higher-order correlations between hypernodes. Recently-proposed hypergraph neural networks (HGNNs) can be categorized into spatial and spectral methods based on their design choices. In this work, we analyze the impact of change in hypergraph topology on the suboptimal performance of HGNNs and propose DPHGNN, a novel dual-perspective HGNN that introduces equivariant operator learning to capture lower-order semantics by inducing topology-aware spatial and spectral inductive biases. DPHGNN employs a unified framework to dynamically fuse lower-order explicit feature representations from the underlying graph into the super-imposed hypergraph structure. We benchmark DPHGNN over eight benchmark hypergraph datasets for the semi-supervised hypernode classification task and obtain superior performance compared to seven state-of-the-art baselines. We also provide a theoretical framework and a synthetic hypergraph isomorphism test to express the power of spatial HGNNs and quantify the expressivity of DPHGNN beyond the Generalized Weisfeiler Leman (1-GWL) test. Finally, DPHGNN was deployed by our partner e-commerce company, Meesho for the Return-to-Origin (RTO) prediction task, which shows ~7% higher macro F1-Score than the best baseline. | Siddhant Saxena, Shounak Ghatak, Raghu Kolla, Debashis Mukherjee, Tanmoy Chakraborty | IIT Delhi, New Delhi, India; Meesho, Bangalore, India |
|  |  [Self-Supervised Learning of Time Series Representation via Diffusion Process and Imputation-Interpolation-Forecasting Mask](https://doi.org/10.1145/3637528.3671673) |  | 0 | Time Series Representation Learning (TSRL) focuses on generating informativerepresentations for various Time Series (TS) modeling tasks. TraditionalSelf-Supervised Learning (SSL) methods in TSRL fall into four main categories:reconstructive, adversarial, contrastive, and predictive, each with a commonchallenge of sensitivity to noise and intricate data nuances. Recently,diffusion-based methods have shown advanced generative capabilities. However,they primarily target specific application scenarios like imputation andforecasting, leaving a gap in leveraging diffusion models for generic TSRL. Ourwork, Time Series Diffusion Embedding (TSDE), bridges this gap as the firstdiffusion-based SSL TSRL approach. TSDE segments TS data into observed andmasked parts using an Imputation-Interpolation-Forecasting (IIF) mask. Itapplies a trainable embedding function, featuring dual-orthogonal Transformerencoders with a crossover mechanism, to the observed part. We train a reversediffusion process conditioned on the embeddings, designed to predict noiseadded to the masked part. Extensive experiments demonstrate TSDE's superiorityin imputation, interpolation, forecasting, anomaly detection, classification,and clustering. We also conduct an ablation study, present embeddingvisualizations, and compare inference speed, further substantiating TSDE'sefficiency and validity in learning representations of TS data. | Zineb Senane, Lele Cao, Valentin Leonhard Buchner, Yusuke Tashiro, Lei You, Pawel Andrzej Herman, Mats Nordahl, Ruibo Tu, Vilhelm von Ehrenheim | Motherbrain, EQT Group & QA.tech, Stockholm, Sweden; Technical University of Denmark, Ballerup, Denmark; Motherbrain, EQT Group, Stockholm, Sweden; Motherbrain, EQT Group & KTH Royal Institute of Technology, Stockholm, Sweden; Mitsubishi UFJ Trust Investment Technology Institute, Tokyo, Japan; KTH Royal Institute of Technology, Stockholm, Sweden |
|  |  [Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck](https://doi.org/10.1145/3637528.3671962) |  | 0 | Temporal Graph Neural Networks (TGNN) have the ability to capture both the graph topology and dynamic dependencies of interactions within a graph over time. There has been a growing need to explain the predictions of TGNN models due to the difficulty in identifying how past events influence their predictions. Since the explanation model for a static graph cannot be readily applied to temporal graphs due to its inability to capture temporal dependencies, recent studies proposed explanation models for temporal graphs. However, existing explanation models for temporal graphs rely on post-hoc explanations, requiring separate models for prediction and explanation, which is limited in two aspects: efficiency and accuracy of explanation. In this work, we propose a novel built-in explanation framework for temporal graphs, called Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck (TGIB). TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory. Experimental results demonstrate the superiority of TGIB in terms of both the link prediction performance and explainability compared to state-of-the-art methods. This is the first work that simultaneously performs prediction and explanation for temporal graphs in an end-to-end manner. The source code of TGIB is available at https://github.com/sang-woo-seo/TGIB. | Sangwoo Seo, Sungwon Kim, Jihyeong Jung, Yoonho Lee, Chanyoung Park | KAIST, Daejeon, Republic of Korea |
|  |  [Offline Imitation Learning with Model-based Reverse Augmentation](https://doi.org/10.1145/3637528.3672059) |  | 0 | In offline Imitation Learning (IL), one of the main challenges is the covariate shift between the expert observations and the actual distribution encountered by the agent, because it is difficult to determine what action an agent should take when outside the state distribution of the expert demonstrations. Recently, the model-free solutions introduced supplementary data and identified the latent expert-similar samples to augment the reliable samples during learning. Model-based solutions build forward dynamic models with conservatism quantification and then generate additional trajectories in the neighborhood of expert demonstrations. However, without reward supervision, these methods are often over-conservative in the out-of-expert-support regions, because only in states close to expert-observed states can there be a preferred action enabling policy optimization. To encourage more exploration on expert-unobserved states, we propose a novel model-based framework, called offline Imitation Learning with Self-paced Reverse Augmentation (SRA). Specifically, we build a reverse dynamic model from the offline demonstrations, which can efficiently generate trajectories leading to the expert-observed states in a self-paced style. Then, we use the subsequent reinforcement learning method to learn from the augmented trajectories and transit from expert-unobserved states to expert-observed states. This framework not only explores the expert-unobserved states but also guides maximizing long-term returns on these states, ultimately enabling generalization beyond the expert data. Empirical results show that our proposal could effectively mitigate the covariate shift and achieve the state-of-the-art performance on the offline imitation learning benchmarks. Project website: https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/. | JieJing Shao, HaoSen Shi, LanZhe Guo, YuFeng Li | National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China |
|  |  [NeuroCut: A Neural Approach for Robust Graph Partitioning](https://doi.org/10.1145/3637528.3671815) |  | 0 | Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications. | Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu |  |
|  |  [Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems](https://doi.org/10.1145/3637528.3672002) |  | 0 | Cognitive diagnosis is a vital upstream task in intelligent education systems. It models the student-exercise interaction, aiming to infer the students' proficiency levels on each knowledge concept. This paper observes that most existing methods can hardly effectively capture the homogeneous influence due to its inherent complexity. That is to say, although students exhibit similar performance on given exercises, their proficiency levels inferred by these methods vary significantly, resulting in shortcomings in interpretability and efficacy. Given the complexity of homogeneous influence, a hypergraph could be a choice due to its flexibility and capability of modeling high-order similarity which aligns with the nature of homogeneous influence. However, before incorporating hypergraph, one at first needs to address the challenges of distorted homogeneous influence, sparsity of response logs, and over-smoothing. To this end, this paper proposes a hypergraph cognitive diagnosis model (HyperCDM) to address these challenges and effectively capture the homogeneous influence. Specifically, to avoid distortion, HyperCDM employs a divide-and-conquer strategy to learn student, exercise and knowledge representations in their own hypergraphs respectively, and interconnects them via a feature-based interaction function. To construct hypergraphs based on sparse response logs, the auto-encoder is utilized to preprocess response logs and K-means is applied to cluster students. To mitigate over-smoothing, momentum hypergraph convolution networks are designed to partially keep previous representations during the message propagation. Extensive experiments on both offline and online real-world datasets show that HyperCDM achieves state-of-the-art performance in terms of interpretability and capturing homogeneous influence effectively, and is competitive in generalization. The ablation study verifies the efficacy of each component, and the case study explicitly showcases the homogeneous influence captured by HyperCDM. | Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, Aimin Zhou | School of Computer Science and Technology, East China Normal University, Shanghai, China |
|  |  [Optimizing OOD Detection in Molecular Graphs: A Novel Approach with Diffusion Models](https://doi.org/10.1145/3637528.3671785) |  | 0 | Despite the recent progress of molecular representation learning, its effectiveness is assumed on the close-world assumptions that training and testing graphs are from identical distribution. The open-world test dataset is often mixed with out-of-distribution (OOD) samples, where the deployed models will struggle to make accurate predictions. The misleading estimations of molecules' properties in drug screening or design can result in the tremendous waste of wet-lab resources and delay the discovery of novel therapies. Traditional detection methods need to trade off OOD detection and in-distribution (ID) classification performance since they share the same representation learning model. In this work, we propose to detect OOD molecules by adopting an auxiliary diffusion model-based framework, which compares similarities between input molecules and reconstructed graphs. Due to the generative bias towards reconstructing ID training samples, the similarity scores of OOD molecules will be much lower to facilitate detection. Although it is conceptually simple, extending this vanilla framework to practical detection applications is still limited by two significant challenges. First, the popular similarity metrics based on Euclidian distance fail to consider the complex graph structure. Second, the generative model involving iterative denoising steps is notoriously time-consuming especially when it runs on the enormous pool of drugs. To address these challenges, our research pioneers an approach of Prototypical Graph Reconstruction for Molecular OOd Detection, dubbed as PGR-MOOD. Specifically, PGR-MOOD hinges on three innovations: i) An effective metric to comprehensively quantify the matching degree of input and reconstructed molecules according to their discrete edges and continuous node features; ii) A creative graph generator to construct a list of prototypical graphs that are in line with ID distribution but away from OOD one; iii) An efficient and scalable OOD detector to compare the similarity between test samples and pre-constructed prototypical graphs and omit the generative process on every new molecule. Extensive experiments on ten benchmark datasets and six baselines are conducted to demonstrate our superiority: PGR-MOOD achieves more than 8% of average improvement in terms of detection AUC and AUPR accompanied by the reduced cost of testing time and memory consumption. The anonymous code is in: https://github.com/se7esx/PGR-MOOD. | Xu Shen, Yili Wang, Kaixiong Zhou, Shirui Pan, Xin Wang | Griffith University, Goldcoast, Australia; Massachusetts Institute of Technology, Cambridge, MA, USA; Jilin University, Changchun, China |
|  |  [Efficient and Long-Tailed Generalization for Pre-trained Vision-Language Model](https://doi.org/10.1145/3637528.3671945) |  | 0 | Pre-trained vision-language models like CLIP have shown powerful zero-shotinference ability via image-text matching and prove to be strong few-shotlearners in various downstream tasks. However, in real-world scenarios,adapting CLIP to downstream tasks may encounter the following challenges: 1)data may exhibit long-tailed data distributions and might not have abundantsamples for all the classes; 2) There might be emerging tasks with new classesthat contain no samples at all. To overcome them, we propose a novel frameworkto achieve efficient and long-tailed generalization, which can be termed asCandle. During the training process, we propose compensating logit-adjustedloss to encourage large margins of prototypes and alleviate imbalance bothwithin the base classes and between the base and new classes. For efficientadaptation, we treat the CLIP model as a black box and leverage the extractedfeatures to obtain visual and textual prototypes for prediction. To make fulluse of multi-modal information, we also propose cross-modal attention to enrichthe features from both modalities. For effective generalization, we introducevirtual prototypes for new classes to make up for their lack of trainingimages. Candle achieves state-of-the-art performance over extensive experimentson 11 diverse datasets while substantially reducing the training time,demonstrating the superiority of our approach. The source code is available athttps://github.com/shijxcs/Candle. | JiangXin Shi, Chi Zhang, Tong Wei, YuFeng Li | National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China |
|  |  [MSPipe: Efficient Temporal GNN Training via Staleness-Aware Pipeline](https://doi.org/10.1145/3637528.3671844) |  | 0 | Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, these optimizations do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for memory-based TGNNs that maximizes training throughput while maintaining model accuracy. Our design specifically addresses the unique challenges associated with fetching and updating node memory states in MTGNNs by integrating staleness into the memory module. However, simply introducing a predefined staleness bound in the memory module to break temporal dependencies may lead to suboptimal performance and lack of generalizability across different models and datasets. To overcome this, we introduce an online pipeline scheduling algorithm in MSPipe that strategically breaks temporal dependencies with minimal staleness and delays memory fetching to obtain fresher memory states. This is achieved without stalling the MTGNN training stage or causing resource contention. Additionally, we design a staleness mitigation mechanism to enhance training convergence and model accuracy. Furthermore, we provide convergence analysis and demonstrate that MSPipe maintains the same convergence rate as vanilla sampling-based GNN training. Experimental results show that MSPipe achieves up to 2.45× speed-up without sacrificing accuracy, making it a promising solution for efficient MTGNN training. The implementation of our paper can be found at the following link: https://github.com/PeterSH6/MSPipe. | Guangming Sheng, Junwei Su, Chao Huang, Chuan Wu | The University of Hong Kong, Hong Kong, China |
|  |  [LPFormer: An Adaptive Graph Transformer for Link Prediction](https://doi.org/10.1145/3637528.3672025) |  | 0 | Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency. The code is available at The code is available at https://github.com/HarryShomer/LPFormer. | Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang | Colorado School of Mines, Golden, CO, USA; Rensselaer Polytechnic Institute, Troy, NY, USA; Michigan State University, East Lansing, MI, USA |
|  |  [Orthogonality Matters: Invariant Time Series Representation for Out-of-distribution Classification](https://doi.org/10.1145/3637528.3671768) |  | 0 | Previous works for time series classification tend to assume that both the training and testing sets originate from the same distribution. This oversimplification deviates from the complexity of reality and makes it challenging to generalize methods to out-of-distribution (OOD) time series data. Currently, there are limited works focusing on time series OOD generalization, and they typically disentangle time series into domain-agnostic and domain-specific features and design tasks to intensify the distinction between the two. However, previous models purportedly yielding domain-agnostic features continue to harbor domain-specific information, thereby diminishing their adaptability to OOD data. To address this gap, we introduce a novel model called Invariant Time Series Representation (ITSR). ITSR achieves a learnable orthogonal decomposition of time series using two sets of orthogonal axes. In detail, ITSR projects time series onto these two sets of axes separately and obtains mutually orthogonal invariant features and relevant features. ITSR theoretically ensures low similarity between these two features and further incorporates various tasks to optimize them. Furthermore, we explore the benefits of preserving orthogonality between invariant and relevant features for OOD time series classification in theory. The results on four real-world datasets underscore the superiority of ITSR over state-of-the-art methods and demonstrate the critical role of maintaining orthogonality between invariant and relevant features. Our code is available at https://github.com/CGCL-codes/ITSR. | Ruize Shi, Hong Huang, Kehan Yin, Wei Zhou, Hai Jin | Huazhong University of Science and Technology, Wuhan, China |
|  |  [CoLiDR: Concept Learning using Aggregated Disentangled Representations](https://doi.org/10.1145/3637528.3671938) |  | 0 | Interpretability of Deep Neural Networks using concept-based models offers a promising way to explain model behavior through human-understandable concepts. A parallel line of research focuses on disentangling the data distribution into its underlying generative factors, in turn explaining the data generation process. While both directions have received extensive attention, little work has been done on explaining concepts in terms of generative factors to unify mathematically disentangled representations and human-understandable concepts as an explanation for downstream tasks. In this paper, we propose a novel method CoLiDR - which utilizes a disentangled representation learning setup for learning mutually independent generative factors and subsequently learns to aggregate the said representations into human-understandable concepts using a novel aggregation/decomposition module. Experiments are conducted on datasets with both known and unknown latent generative factors. Our method successfully aggregates disentangled generative factors into concepts while maintaining parity with state-of-the-art concept-based approaches. Quantitative and visual analysis of the learned aggregation procedure demonstrates the advantages of our work compared to commonly used concept-based models over four challenging datasets. Lastly, our work is generalizable to an arbitrary number of concepts and generative factors - making it flexible enough to be suitable for various types of data. | Sanchit Sinha, Guangzhi Xiong, Aidong Zhang | University of Virginia, Charlottesville, VA, USA |
|  |  [On Early Detection of Hallucinations in Factual Question Answering](https://doi.org/10.1145/3637528.3671796) |  | 0 | While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes detection a difficult task. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts tend to differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We also show that tokens preceding a hallucination can already predict the subsequent hallucination even before it occurs. | Ben Snyder, Marius Moisescu, Muhammad Bilal Zafar | Amazon Web Services, Santa Clara, CA, USA; Amazon Web Services, Seattle, WA, USA |
|  |  [MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning](https://doi.org/10.1145/3637528.3671905) |  | 0 | Adapting large language models (LLMs) to unseen tasks with incontext training samples without fine-tuning remains an important research problem. To learn a robust LLM that adapts well to unseen tasks, multiple meta-training approaches have been proposed such as MetaICL and MetaICT, which involve meta-training pre-trained LLMs on a wide variety of diverse tasks. These meta-training approaches essentially perform in-context multi-task fine-tuning and evaluate on a disjointed test set of tasks. Even though they achieve impressive performance, their goal is never to compute a truly general set of parameters. In this paper, we propose MAML-en-LLM, a novel method for meta-training LLMs, which can learn truly generalizable parameters that not only performs well on disjointed tasks but also adapts to unseen tasks. We see an average increase of 2% on unseen domains in the performance while a massive 4% improvement on adaptation performance. Furthermore, we demonstrate that MAML-en-LLM outperforms baselines in settings with limited amount of training data on both seen and unseen domains by an average of 2%. Finally, we discuss the effects of type of tasks, optimizers and task complexity, an avenue barely explored in metatraining literature. Exhaustive experiments across 7 task settings along with two data settings demonstrate that models trained with MAML-en-LLM outperform SOTA meta-training approaches. | Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, Aidong Zhang | Amazon AGI, Cambridge, MA, USA; Amazon AGI, New York, NY, USA; University of Virginia, Charlottesville, VA, USA |
|  |  [Fast Computation for the Forest Matrix of an Evolving Graph](https://doi.org/10.1145/3637528.3671822) |  | 0 | The forest matrix plays a crucial role in network science, opinion dynamics, and machine learning, offering deep insights into the structure of and dynamics on networks. In this paper, we study the problem of querying entries of the forest matrix in evolving graphs, which more accurately represent the dynamic nature of real-world networks compared to static graphs. To address the unique challenges posed by evolving graphs, we first introduce two approximation algorithms, SFQ and SFQPlus, for static graphs. SFQ employs a probabilistic interpretation of the forest matrix, while SFQPlus incorporates a novel variance reduction technique and is theoretically proven to offer enhanced accuracy. Based on these two algorithms, we further devise two dynamic algorithms centered around efficiently maintaining a list of spanning converging forests. This approach ensures O(1) runtime complexity for updates, including edge additions and deletions, as well as for querying matrix elements, and provides an unbiased estimation of forest matrix entries. Finally, through extensive experiments on various real-world networks, we demonstrate the efficiency and effectiveness of our algorithms. Particularly, our algorithms are scalable to massive graphs with more than forty million nodes. | Haoxin Sun, Xiaotian Zhou, Zhongzhi Zhang | Fudan University, Shanghai, China |
|  |  [Dual-Assessment Driven Pruning: Iterative Optimizing Layer-wise Sparsity for Large Language Model](https://doi.org/10.1145/3637528.3671780) |  | 0 | Large Language Models (LLMs) have demonstrated efficacy in various domains, but deploying these models is economically challenging due to extensive parameter counts. Numerous efforts have been dedicated to reducing the parameter count of these models without compromising performance, employing a technique known as model pruning. Conventional pruning methods assess the significance of weights within individual layers and typically apply uniform sparsity levels across all layers, potentially neglecting the varying significance of each layer. To address this oversight, we first propose a dual-assessment driven pruning strategy that employs both intra-layer metric and global performance metric to comprehensively evaluate the impact of pruning. Then our method leverages an iterative optimization algorithm to find the optimal layer-wise sparsity distribution, thereby minimally impacting model performance. Extensive benchmark evaluations on state-of-the-art LLM architectures such as LLaMAv2 and OPT across a variety of NLP tasks demonstrate the effectiveness of our approach. When applied to the LLaMaV2-7B model with an overall pruning sparsity of 80%, our method achieves a 50% reduction in perplexity compared to the benchmark. The results indicate that our method significantly outperforms existing state-of-the-art methods in preserving performance after pruning. | Qinghui Sun, Weilun Wang, Yanni Zhu, Shenghuan He, Hao Yi, Zehua Cai, Hong Liu | Alibaba Group, HangZhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China |
|  |  [DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization](https://doi.org/10.1145/3637528.3671878) |  | 0 | This paper addresses the challenge of out-of-distribution (OOD) generalization in graph machine learning, a field rapidly advancing yet grappling with the discrepancy between source and target data distributions. Traditional graph learning algorithms, based on the assumption of uniform distribution between training and test data, falter in real-world scenarios where this assumption fails, resulting in suboptimal performance. A principal factor contributing to this suboptimal performance is the inherent simplicity bias of neural networks trained through Stochastic Gradient Descent (SGD), which prefer simpler features over more complex yet equally or more predictive ones. This bias leads to a reliance on spurious correlations, adversely affecting OOD performance in various tasks such as image recognition, natural language understanding, and graph classification. Current methodologies, including subgraph-mixup and information bottleneck approaches, have achieved partial success but struggle to overcome simplicity bias, often reinforcing spurious correlations. To tackle this, our study introduces a new learning paradigm for graph OOD issue. We propose DIVE, training a collection of models to focus on all label-predictive subgraphs by encouraging the models to foster divergence on the subgraph mask, which circumvents the limitation of a model solely focusing on the subgraph corresponding to simple structural patterns. Specifically, we employs a regularizer to punish overlap in extracted subgraphs across models, thereby encouraging different models to concentrate on distinct structural patterns. Model selection for robust OOD performance is achieved through validation accuracy. Tested across four datasets from GOOD benchmark and one dataset from DrugOOD benchmark, our approach demonstrates significant improvement over existing methods, effectively addressing the simplicity bias and enhancing generalization in graph machine learning. | Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, Liang Wang | NLPR, MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Science and Technology of China, Hefei, China |
|  |  [Hierarchical Linear Symbolized Tree-Structured Neural Processes](https://doi.org/10.1145/3637528.3671861) |  | 0 | Traditional Neural Processes (NPs) and their variants aim to learn relationships between context sample points but do not consider multi-level information, resulting in a limited ability to learn complex distributions.This paper draws inspiration from features such as the hierarchical nature and interpretability of tree-like structures. This paper proposes a Hierarchical Linear Symbolized Tree-structured Neural Processes (HLNPs) architecture. This framework utilizes variables to build a top-down hierarchical linear symbolized tree-structured network architecture, enhancing positional representation information in a hierarchical manner along the deterministic path. In the latent distribution, the hierarchical linear symbolized tree-structured network approximates functions discretely through a layered approach. By decomposing the latent complex distribution into several simpler sub-problems using sum and product symbols, the upper bound of optimization is thereby increased. The tree structure discretizes variables to capture model uncertainty in the form of entropy. This approach also imparts a causal effect to the HLNPs model. Finally, we demonstrate the effectiveness of the HLNPs models for 1D data, Bayesian optimization, and 2D data. | Jin yang Tai, YiKe Guo | School of Computer Engineering and Science, Shanghai University, Shanghai, China |
|  |  [Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute](https://doi.org/10.1145/3637528.3671970) |  | 0 | The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naïve implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the proximal gradient descent and a graph mining tree search. Our pruning strategy can ensure that the quality of the solution is maintained compared to the result without pruning. We empirically demonstrate that LAGRA has superior or comparable prediction performance to the standard existing algorithms including graph neural networks, while using only a small number of AGs in an interpretable manner. | Tajima Shinji, Ren Sugihara, Ryota Kitahara, Masayuki Karasuyama | Nagoya Institute of Technology, Nagoya, Japan |
|  |  [HiGPT: Heterogeneous Graph Language Model](https://doi.org/10.1145/3637528.3671987) |  | 0 | Heterogeneous graph learning aims to capture complex relationships and diverse relational semantics among entities in a heterogeneous graph to obtain meaningful representations for nodes and edges. Recent advancements in heterogeneous graph neural networks (HGNNs) have achieved state-of-the-art performance by considering relation heterogeneity and using specialized message functions and aggregation rules. However, existing frameworks for heterogeneous graph learning have limitations in generalizing across diverse heterogeneous graph datasets. Most of these frameworks follow the "pre-train" and "fine-tune" paradigm on the same dataset, which restricts their capacity to adapt to new and unseen data. This raises the question: "Can we generalize heterogeneous graph models to be well-adapted to diverse downstream learning tasks with distribution shifts in both node token sets and relation type heterogeneity?" To tackle those challenges, we propose HiGPT, a general large graph model with Heterogeneous graph instruction-tuning paradigm. Our framework enables learning from arbitrary heterogeneous graphs without the need for any fine-tuning process from downstream datasets. To handle distribution shifts in heterogeneity, we introduce an in-context heterogeneous graph tokenizer that captures semantic relationships in different heterogeneous graphs, facilitating model adaptation. We incorporate a large corpus of heterogeneity-aware graph instructions into our HiGPT, enabling the model to effectively comprehend complex relation heterogeneity and distinguish between various types of graph tokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction augmentation paradigm to mitigate data scarcity by generating diverse and informative instructions. Through comprehensive evaluations conducted in various settings, our proposed framework demonstrates exceptional performance in terms of generalization performance, surpassing current leading benchmarks. We make our model implementation openly available, along with comprehensive details at: https://github.com/HKUDS/HiGPT. | Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, Chao Huang | University of Hong Kong, Hong Kong, China; Baidu Inc., Beijing, China |
|  |  [URRL-IMVC: Unified and Robust Representation Learning for Incomplete Multi-View Clustering](https://doi.org/10.1145/3637528.3671887) |  | 0 | Incomplete multi-view clustering (IMVC) aims to cluster multi-view data that are only partially available. This poses two main challenges: effectively leveraging multi-view information and mitigating the impact of missing views. Prevailing solutions employ cross-view contrastive learning and missing view recovery techniques. However, they either neglect valuable complementary information by focusing only on consensus between views or provide unreliable recovered views due to the absence of supervision. To address these limitations, we propose a novel Unified and Robust Representation Learning for Incomplete Multi-View Clustering (URRL-IMVC). URRL-IMVC directly learns a unified embedding that is robust to view missing conditions by integrating information from multiple views and neighboring samples. Firstly, to overcome the limitations of cross-view contrastive learning, URRL-IMVC incorporates an attention-based auto-encoder framework to fuse multi-view information and generate unified embeddings. Secondly, URRL-IMVC directly enhances the robustness of the unified embedding against view-missing conditions through KNN imputation and data augmentation techniques, eliminating the need for explicit missing view recovery. Finally, incremental improvements are introduced to further enhance the overall performance, such as the Clustering Module and the customization of the Encoder. We extensively evaluate the proposed URRL-IMVC framework on various benchmark datasets, demonstrating its state-of-the-art performance. Furthermore, comprehensive ablation studies are performed to validate the effectiveness of our design. | Ge Teng, Ting Mao, Chen Shen, Xiang Tian, Xuesong Liu, Yaowu Chen, Jieping Ye | Zhejiang University, Hangzhou, China; Alibaba Cloud, Hangzhou, China |
|  |  [Rotative Factorization Machines](https://doi.org/10.1145/3637528.3671740) |  | 0 | Feature interaction learning (FIL) focuses on capturing the complex relationships among multiple features for building predictive models, which is widely used in real-world tasks. Despite the research progress, existing FIL methods suffer from two major limitations. Firstly, they mainly model the feature interactions within a bounded order (e.g., small integer order) due to the exponential growth of the interaction terms. Secondly, the interaction order of each feature is often independently learned, which lacks the flexibility to capture the feature dependencies in varying contexts. To address these issues, we present Rotative Factorization Machines (RFM), based on the key idea that represents each feature as a polar angle in the complex plane. As such, the feature interactions are converted into a series of complex rotations, where the orders are cast into the rotation coefficients, thereby allowing for the learning of arbitrarily large order. Further, we propose a novel self-attentive rotation function that models the rotation coefficients through a rotation-based attention mechanism, which can adaptively learn the interaction orders under different interaction contexts. Moreover, it incorporates a modulus amplification network to learn the modulus of the complex features, which further enhances the expressive capacity. Our proposed approach provides a general FIL framework, and many existing models can be instantiated in this framework, e.g., factorization machines. In theory, it possesses more strong capacities to model complex feature relationships, and can learn arbitrary features from varied contexts. Extensive experiments conducted on five widely used datasets have demonstrated the effectiveness of our approach. | Zhen Tian, Yuhong Shi, Xiangkun Wu, Wayne Xin Zhao, JiRong Wen | Zhejiang University, Hangzhou, China |
|  |  [Latent Diffusion-based Data Augmentation for Continuous-Time Dynamic Graph Model](https://doi.org/10.1145/3637528.3671863) |  | 0 | Continuous-Time Dynamic Graph (CTDG) precisely models evolving real-world relationships, drawing heightened interest in dynamic graph learning across academia and industry. However, existing CTDG models encounter challenges stemming from noise and limited historical data. Graph Data Augmentation (GDA) emerges as a critical solution, yet current approaches primarily focus on static graphs and struggle to effectively address the dynamics inherent in CTDGs. Moreover, these methods often demand substantial domain expertise for parameter tuning and lack theoretical guarantees for augmentation efficacy. To address these issues, we propose Conda, a novel latent diffusion-based GDA method tailored for CTDGs. Conda features a sandwich-like architecture, incorporating a Variational Auto-Encoder (VAE) and a conditional diffusion model, aimed at generating enhanced historical neighbor embeddings for target nodes. Unlike conventional diffusion models trained on entire graphs via pre-training, Conda requires historical neighbor sequence embeddings of target nodes for training, thus facilitating more targeted augmentation. We integrate Conda into the CTDG model and adopt an alternating training strategy to optimize performance. Extensive experimentation across six widely used real-world datasets showcases the consistent performance improvement of our approach, particularly in scenarios with limited historical data. | Yuxing Tian, Aiwen Jiang, Qi Huang, Jian Guo, Yiyan Qi | Jiangxi Normal University, Nanchang, China; International Digital Economy Academy, IDEA Research, Shenzhen, China |
|  |  [Flexible Graph Neural Diffusion with Latent Class Representation Learning](https://doi.org/10.1145/3637528.3671860) |  | 0 | In existing graph data, the connection relationships often exhibit uniform weights, leading to the model aggregating neighboring nodes with equal weights across various connection types. However, this uniform aggregation of diverse information diminishes the discriminability of node representations, contributing significantly to the over-smoothing issue in models. In this paper, we propose the Flexible Graph Neural Diffusion (FGND) model, incorporating latent class representation to address the misalignment between graph topology and node features. In particular, we combine latent class representation learning with the inherent graph topology to reconstruct the diffusion matrix during the graph diffusion process. We introduce the sim metric to quantify the degree of mismatch between graph topology and node features. By flexibly adjusting the dependency level on node features through the hyperparameter, we accommodate diverse adjacency relationships. The effective filtering of noise in the topology also allows the model to capture higher order information, significantly alleviating the over-smoothing problem. Meanwhile, we model the graphical diffusion process as a set of differential equations and employ advanced partial differential equation tools to obtain more accurate solutions. Empirical evaluations on five benchmarks reveal that our FGND model outperforms existing popular GNN methods in terms of both overall performance and stability under data perturbations. Meanwhile, our model exhibits superior performance in comparison to models tailored for heterogeneous graphs and those designed to address oversmoothing issues. | Liangtian Wan, Huijin Han, Lu Sun, Zixun Zhang, Zhaolong Ning, Xiaoran Yan, Feng Xia | School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; School of Computing Technologies, RMIT University, Melbourne, Australia; Research Center for Data Hub and Security, Zhejiang Lab, Hangzhou, China |
|  |  [STONE: A Spatio-temporal OOD Learning Framework Kills Both Spatial and Temporal Shifts](https://doi.org/10.1145/3637528.3671680) |  | 0 | Traffic prediction is a crucial task in the Intelligent Transportation System (ITS), receiving significant attention from both industry and academia. Numerous spatio-temporal graph convolutional networks have emerged for traffic prediction and achieved remarkable success. However, these models have limitations in terms of generalization and scalability when dealing with Out-of-Distribution (OOD) graph data with both structural and temporal shifts. To tackle the challenges of spatio-temporal shift, we propose a framework called STONE by learning invariable node dependencies, which achieve stable performance in variable environments. STONE initially employs gated-transformers to extract spatial and temporal semantic graphs. These two kinds of graphs represent spatial and temporal dependencies, respectively. Then we design three techniques to address spatio-temporal shifts. Firstly, we introduce a Fréchet embedding method that is insensitive to structural shifts, and this embedding space can integrate loose position dependencies of nodes within the graph. Secondly, we propose a graph intervention mechanism to generate multiple variant environments by perturbing two kinds of semantic graphs without any data augmentations, and STONE can explore invariant node representation from environments. Finally, we further introduce an explore-to-extrapolate risk objective to enhance the variety of generated environments. We conduct experiments on multiple traffic datasets, and the results demonstrate that our proposed model exhibits competitive performance in terms of generalization and scalability. | Binwu Wang, Jiaming Ma, Pengkun Wang, Xu Wang, Yudong Zhang, Zhengyang Zhou, Yang Wang | University of Science and Technology of China, Hefei, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, China |
|  |  [Provable Adaptivity of Adam under Non-uniform Smoothness](https://doi.org/10.1145/3637528.3671718) |  | 0 | Adam is widely adopted in practical applications due to its fast convergence. However, its theoretical analysis is still far from satisfactory. Existing convergence analyses for Adam rely on the bounded smoothness assumption, referred to as the L-smooth condition. Unfortunately, this assumption does not hold for many deep learning tasks. Moreover, we believe that this assumption obscures the true benefit of Adam, as the algorithm can adapt its update magnitude according to local smoothness. This important feature of Adam becomes irrelevant when assuming globally bounded smoothness. This paper studies the convergence of randomly reshuffled Adam (RR Adam) with diminishing learning rate, which is the major version of Adam adopted in deep learning tasks. We present the first convergence analysis of RR Adam without the bounded smoothness assumption. We demonstrate that RR Adam can maintain its convergence properties when smoothness is linearly bounded by the gradient norm, referred to as the (L0, L1)-smooth condition. We further compare Adam to SGD when both methods use diminishing learning rate. We refine the existing lower bound of SGD and show that SGD can be slower than Adam. To our knowledge, this is the first time that Adam and SGD are rigorously compared in the same setting and the advantage of Adam is revealed. | Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi Meng, Ruoyu Sun, ZhiMing Ma, TieYan Liu, ZhiQuan Luo, Wei Chen | Chinese Academy of Mathematics and Systems Science, Beijing, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, Guangdong, China; Microsoft, Beijing, China; University of Science and Technology of China & Microsoft Research Asia, Beijing, Haidian, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Peking University, Beijing, China |
|  |  [Multi-Scale Detection of Anomalous Spatio-Temporal Trajectories in Evolving Trajectory Datasets](https://doi.org/10.1145/3637528.3671874) |  | 0 | A trajectory is a sequence of timestamped point locations that captures the movement of an object such as a vehicle. Such trajectories encode complex spatial and temporal patterns and provide rich information about object mobility and the underlying infrastructures, typically road networks, within which the movements occur. A trajectory dataset is evolving when new trajectories are included continuously. The ability to detect anomalous trajectories in online fashion in this setting is fundamental and challenging functionality that has many applications, e.g., location-based services. State-of-the-art solutions determine anomalies based on the shapes or routes of trajectories, ignoring potential anomalies caused by different sampling rates or time offsets. We propose a multi-scale model, termed MST-OATD, for anomalous streaming trajectory detection that considers both the spatial and temporal aspects of trajectories. The model's multi-scale capabilities aim to enable extraction of trajectory features at multiple scales. In addition, to improve model evolvability and to contend with changes in trajectory patterns, the model is equipped with a learned ranking model that updates the training set as new trajectories are included. Experiments on real datasets offer evidence that the model can outperform state-of-the-art solutions and is capable of real-time anomaly detection. Further, the learned ranking model achieves promising results when updating the training set with newly arrived trajectories. | Chenhao Wang, Lisi Chen, Shuo Shang, Christian S. Jensen, Panos Kalnis | Aalborg University, Aalborg, Denmark; King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; University of Electronic Science and Technology of China, Chengdu, China |
|  |  [Global Human-guided Counterfactual Explanations for Molecular Properties via Reinforcement Learning](https://doi.org/10.1145/3637528.3672045) |  | 0 | Counterfactual explanations of Graph Neural Networks (GNNs) offer a powerful way to understand data that can naturally be represented by a graph structure. Furthermore, in many domains, it is highly desirable to derive data-driven global explanations or rules that can better explain the high-level properties of the models and data in question. However, evaluating global counterfactual explanations is hard in real-world datasets due to a lack of human-annotated ground truth, which limits their use in areas like molecular sciences. Additionally, the increasing scale of these datasets provides a challenge for random search-based methods. In this paper, we develop a novel global explanation model RLHEX for molecular property prediction. It aligns the counterfactual explanations with human-defined principles, making the explanations more interpretable and easy for experts to evaluate. RLHEX includes a VAE-based graph generator to generate global explanations and an adapter to adjust the latent representation space to human-defined principles. Optimized by Proximal Policy Optimization (PPO), the global explanations produced by RLHEX cover 4.12% more input graphs and reduce the distance between the counterfactual explanation set and the input set by 0.47% on average across three molecular datasets. RLHEX provides a flexible framework to incorporate different human-designed principles into the counterfactual explanation generation process, aligning these explanations with domain expertise. The code and data are released at https://github.com/dqwang122/RLHEX. | Danqing Wang, Antonis Antoniades, KhaDinh Luong, Edwin Zhang, Mert Kosan, Jiachen Li, Ambuj Singh, William Yang Wang, Lei Li | Harvard University & Founding, Cambridge, MA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA; University of California, Santa Barbara, Santa Barbara, USA; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA |
|  |  [Mastering Long-Tail Complexity on Graphs: Characterization, Learning, and Generalization](https://doi.org/10.1145/3637528.3671880) |  | 0 | In the context of long-tail classification on graphs, the vast majority of existing work primarily revolves around the development of model debiasing strategies, intending to mitigate class imbalances and enhance the overall performance. Despite the notable success, there is very limited literature that provides a theoretical tool for characterizing the behaviors of long-tail classes in graphs and gaining insight into generalization performance in real-world scenarios. To bridge this gap, we propose a generalization bound for long-tail classification on graphs by formulating the problem in the fashion of multi-task learning, i.e., each task corresponds to the prediction of one particular class. Our theoretical results show that the generalization performance of long-tail classification is dominated by the overall loss range and the task complexity. Building upon the theoretical findings, we propose a novel generic framework HierTail for long-tail classification on graphs. In particular, we start with a hierarchical task grouping module that allows us to assign related tasks into hypertasks and thus control the complexity of the task space; then, we further design a balanced contrastive learning module to adaptively balance the gradients of both head and tail classes to control the loss range across all tasks in a unified fashion. Extensive experiments demonstrate the effectiveness of HierTail in characterizing long-tail classes on real graphs, which achieves up to 12.9% improvement over the leading baseline method in balanced accuracy. | Haohui Wang, Baoyu Jing, Kaize Ding, Yada Zhu, Wei Cheng, Si Zhang, Yonghui Fan, Liqing Zhang, Dawei Zhou | IBM Research, Yorktown Heights, USA; NEC Labs America, Princeton, USA; Meta, Sunnyvale, USA; Amazon AGI, Sunnyvale, USA; Virginia Tech, Blacksburg, USA; Northwestern University, Evanston, USA; University of Illinois Urbana-Champaign, Urbana, USA |
|  |  [Unsupervised Heterogeneous Graph Rewriting Attack via Node Clustering](https://doi.org/10.1145/3637528.3671716) |  | 0 | Self-supervised learning (SSL) has become one of the most popular learning paradigms and has achieved remarkable success in the graph field. Recently, a series of pre-training studies on heterogeneous graphs (HGs) using SSL have been proposed considering the heterogeneity of real-world graph data. However, verification of the robustness of heterogeneous graph pre-training is still a research gap. Most existing researches focus on supervised attacks on graphs, which are limited to a specific scenario and will not work when labels are not available. In this paper, we propose a novel unsupervised heterogeneous graph rewriting attack via node clustering (HGAC) that can effectively attack HG pre-training models without using labels. Specifically, a heterogeneous edge rewriting strategy is designed to ensure the rationality and concealment of the attacks. Then, a tailored heterogeneous graph contrastive learning (HGCL) is used as a surrogate model. Moreover, we leverage node clustering results of the clean HGs as the pseudo-labels to guide the optimization of structural attacks. Extensive experiments exhibit powerful attack performances of our HGAC on various downstream tasks (i.e., node classification, node clustering, metapath prediction, and visualization) under poisoning attack and evasion attack. | Haosen Wang, Can Xu, Chenglong Shi, Pengfei Zheng, Shiming Zhang, Minhao Cheng, Hongyang Chen | Pennsylvania State University, Philadelphia, PA, USA; University of Science and Technology of China, Hefei, China; Southeast University & Zhejiang Lab, Nanjing, China; East China Normal University, Shanghai, China; Zhejiang Lab, Hangzhou, China; Southeast University, Nanjing, China |
|  |  [Effective Edge-wise Representation Learning in Edge-Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671805) |  | 0 | Graph representation learning (GRL) is to encode graph elements into informative vector representations, which can be used in downstream tasks for analyzing graph-structured data and has seen extensive applications in various domains. However, the majority of extant studies on GRL are geared towards generating node representations, which cannot be readily employed to perform edge-based analytics tasks in edge-attributed bipartite graphs (EABGs) that pervade the real world, e.g., spam review detection in customer-product reviews and identifying fraudulent transactions in user-merchant networks. Compared to node-wise GRL, learning edge representations (ERL) on such graphs is challenging due to the need to incorporate the structure and attribute semantics from the perspective of edges while considering the separate influence of two heterogeneous node sets U and V in bipartite graphs. To our knowledge, despite its importance, limited research has been devoted to this frontier, and existing workarounds all suffer from sub-par results. Motivated by this, this paper designs EAGLE, an effective ERL method for EABGs. Building on an in-depth and rigorous theoretical analysis, we propose the factorized feature propagation (FFP) scheme for edge representations with adequate incorporation of long-range dependencies of edges/features without incurring tremendous computation overheads. We further ameliorate FFP as a dual-view FFP by taking into account the influences from nodes in U and V severally in ERL. Extensive experiments on 5 real datasets showcase the effectiveness of the proposed EAGLE models in semi-supervised edge classification tasks. In particular, EAGLE can attain a considerable gain of at most 38.11% in AP and 1.86% in AUC when compared to the best baselines. | Hewen Wang, Renchi Yang, Xiaokui Xiao | Hong Kong Baptist University, Hong Kong, China; National University of Singapore, Singapore, Singapore |
|  |  [FedNLR: Federated Learning with Neuron-wise Learning Rates](https://doi.org/10.1145/3637528.3672042) |  | 0 | Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Some existing work suggests that the fundamental reason is that data heterogeneity can cause local model drift, and therefore proposes to calibrate the direction of local updates to solve this problem. Though effective, existing methods generally take the model as a whole, which lacks a deep understanding of how the neurons within deep classification models evolve during local training to form model drift. In this paper, we bridge this gap by performing an intuitive and theoretical analysis of the activation changes of each neuron during local training. Our analysis shows that the high activation of some neurons on the samples of a certain class will be reduced during local training when these samples are not included in the client, which we call neuron drift, thus leading to the performance reduction of this class. Motivated by this, we propose a novel and simple algorithm called FedNLR, which utilizes Neuron-wise Learning Rates during the FL local training process. The principle behind this is to enhance the learning of neurons bound to local classes on local data knowledge while reducing the decay of non-local classes knowledge stored in neurons. Experimental results demonstrate that FedNLR achieves state-of-the-art performance on federated learning with popular deep neural networks. | Haozhao Wang, Peirong Zheng, Xingshuo Han, Wenchao Xu, Ruixuan Li, Tianwei Zhang | Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Nanyang Technological University, Singapore, Singapore |
|  |  [Robust Predictions with Ambiguous Time Delays: A Bootstrap Strategy](https://doi.org/10.1145/3637528.3671920) |  | 0 | In contemporary data-driven environments, the generation and processing of multivariate time series data is an omnipresent challenge, often complicated by time delays between different time series. These delays, originating from a multitude of sources like varying data transmission dynamics, sensor interferences, and environmental changes, introduce significant complexities. Traditional Time Delay Estimation methods, which typically assume a fixed constant time delay, may not fully capture these variabilities, compromising the precision of predictive models in diverse settings. To address this issue, we introduce the Time Series Model Bootstrap (TSMB), a versatile framework designed to handle potentially varying or even nondeterministic time delays in time series modeling. Contrary to traditional approaches that hinge on the assumption of a single, consistent time delay, TSMB adopts a non-parametric stance, acknowledging and incorporating time delay uncertainties. TSMB significantly bolsters the performance of models that are trained and make predictions using this framework, making it highly suitable for a wide range of dynamic and interconnected data environments. Our comprehensive evaluations, conducted on real-world datasets with different types of time delays, confirm the adaptability and effectiveness of TSMB in multiple contexts. These include, but are not limited to, power and occupancy forecasting in intelligent infrastructures, air quality monitoring, and intricate processes like mineral processing. Further diagnostic analyses strengthen the case for the TSMB estimator's robustness, underlining its significance in scenarios where ambiguity in time delays can have a significant impact on the predictive task. | Jiajie Wang, Zhiyuan Jerry Lin, Wen Chen | Meta, Menlo Park, USA; Changsha Research Institute of Mining and Metallurgy, Changsha, China |
|  |  [A Novel Prompt Tuning for Graph Transformers: Tailoring Prompts to Graph Topologies](https://doi.org/10.1145/3637528.3671804) |  | 0 | Deep graph prompt tuning (DeepGPT), which only tunes a set of continuous prompts for graph transformers, significantly decreases the storage usage during training. However, DeepGPT is limited by its uniform prompts to input graphs with various structures. This is because different graph structures dictate various feature interactions between nodes, while the uniform prompts are not dynamic to tailor the feature transformation for the graph topology. In this paper, we propose a Topo-specific Graph Prompt Tuning (TGPT ), which provides topo-specific prompts tailored to the topological structures of input graphs. Specifically, TGPT learns trainable embeddings for graphlets and frequencies, where graphlets are fundamental sub-graphs that describe the structure around specific nodes. Based on the statistic data about graphlets of input graph, topo-specific prompts are generated by graphlet embeddings and frequency embeddings. The topo-specific prompts include node-level topo-specific prompts for specified nodes, a graph-level topo-specific prompt for the entire graph, and a task-specific prompt to learn task-related information. They are all inserted into specific graph nodes to perform feature transformation, providing specified feature transformation for input graphs with different topological structures. Extensive experiments show that our method outperforms existing lightweight fine-tuning methods and DeepGPT in molecular graph classification and regression with comparable parameters. | Jingchao Wang, Zhengnan Deng, Tongxu Lin, Wenyuan Li, Shaobin Ling | Guangdong University of Technology, Guangzhou, China; South China Normal University, Guangzhou, --- Select One ---, China; Guangdong University of Technology, Guangzhou, --- Select One ---, China; Beijing University of Posts and Telecommunications, Beijing, China |
|  |  [DyPS: Dynamic Parameter Sharing in Multi-Agent Reinforcement Learning for Spatio-Temporal Resource Allocation](https://doi.org/10.1145/3637528.3672052) |  | 0 | In large-scale metropolis, it is critical to efficiently allocate various resources such as electricity, medical care, and transportation to meet the living demands of citizens, according to the spatio-temporal distributions of resources and demands. Previous researchers have done plentiful work on such problems by leveraging Multi-Agent Reinforcement Learning (MARL) methods, where multiple agents cooperatively regulate and allocate the resources to meet the demands. However, facing the great number of agents in large cities, existing MARL methods lack efficient parameter sharing strategies among agents to reduce computational complexity. There remain two primary challenges in efficient parameter sharing: (1) during the RL training process, the behavior of agents changes significantly, limiting the performance of group parameter sharing based on fixed role division decided before training; (2) the behavior of agents forms complicated action trajectories, where their role characteristics are implicit, adding difficulty to dynamically adjusting agent role divisions during the training process. In this paper, we propose Dynamic Parameter Sharing (DyPS) to solve the above challenges. We design self-supervised learning tasks to extract the implicit behavioral characteristics from the action trajectories of agents. Based on the obtained behavioral characteristics, we propose a hierarchical MARL framework capable of dynamically revising the agent role divisions during the training process and thus shares parameters among agents with the same role, reducing computational complexity. In addition, our framework can be combined with various typical MARL algorithms, including IPPO, MAPPO, etc. We conduct 7 experiments in 4 representative resource allocation scenarios, where extensive results demonstrate our method's superior performance, outperforming the state-of-the-art baseline methods by up to 31%. Our source codes are available at https://github.com/tsinghua-fib-lab/DyPS. | Jingwei Wang, Qianyue Hao, Wenzhen Huang, Xiaochen Fan, Zhentao Tang, Bin Wang, Jianye Hao, Yong Li | Department of EE, BNRist, Tsinghua University, Beijing, China; Huawei Noah's Ark Lab, Beijing, China; Tianjin University & Huawei Noah's Ark Lab, Beijing, China |
|  |  [The Snowflake Hypothesis: Training and Powering GNN with One Node One Receptive Field](https://doi.org/10.1145/3637528.3671766) |  | 0 | Despite Graph Neural Networks (GNNs) demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with overfitting and over-smoothing as they go deeper as models of computer vision (CV) realm. The success of artificial intelligence in computer vision and natural language processing largely stems from its ability to train deep models effectively. We have thus conducted a systematic study on deep GNN models. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. In this paper, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of "one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs. We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training scheme; (2) various shallow and deep GNN backbones; (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks; (4) compare with different aggregation strategies. The observational results demonstrate that our framework can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. Code is available at: https://github.com/CunWang520/Snowhypothe. | Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Junfeng Fang, Xiaojiang Peng, Yuxuan Liang, Yang Wang | Oxford University, London, United Kingdom; University of Science and Technology of China, Hefei, China; Shenzhen Technology University, shenzhen, China; International Digital Economy Academy, Shanghai, China; University of Science and Technology of China (USTC), Hefei, China; National University of Singapore, Singapore, Singapore; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [The Heterophilic Snowflake Hypothesis: Training and Empowering GNNs for Heterophilic Graphs](https://doi.org/10.1145/3637528.3671791) |  | 0 | Graph Neural Networks (GNNs) have become pivotal tools for a range of graph-based learning tasks. Notably, most current GNN architectures operate under the assumption of homophily, whether explicitly or implicitly. While this underlying assumption is frequently adopted, it is not universally applicable, which can result in potential shortcomings in learning effectiveness. In this paper, or the first time, we transfer the prevailing concept of "one node one receptive field" to the heterophilic graph. By constructing a proxy label predictor, we enable each node to possess a latent prediction distribution, which assists connected nodes in determining whether they should aggregate their associated neighbors. Ultimately, every node can have its own unique aggregation hop and pattern, much like each snowflake is unique and possesses its own characteristics. Based on observations, we innovatively introduce the Heterophily Snowflake Hypothesis and provide an effective solution to guide and facilitate research on heterophilic graphs and beyond. We conduct comprehensive experiments including (1) main results on 10 graphs with varying heterophily ratios across 10 backbones; (2) scalability on various deep GNN backbones (SGC, JKNet, etc.) across various large number of layers (2,4,6,8,16,32 layers); (3) comparison with conventional snowflake hypothesis; (4) efficiency comparison with existing graph pruning algorithms. Our observations show that our framework acts as a versatile operator for diverse tasks. It can be integrated into various GNN frameworks, boosting performance in-depth and offering an explainable approach to choosing the optimal network depth. The source code is available at https://github.com/bingreeky/HeteroSnoH. | Kun Wang, Guibin Zhang, Xinnan Zhang, Junfeng Fang, Xun Wu, Guohao Li, Shirui Pan, Wei Huang, Yuxuan Liang | Tsinghua University, Beijing, China; Griffith University, Queensland, Australia; RIKEN AIP, Tokyo, Japan; University of Minnesota, Twin Cities, MN, USA; University of Science and Technology of China (USTC), Hefei, China; Oxford University, Oxford, United Kingdom; Tongji University, Shanghai, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [CutAddPaste: Time Series Anomaly Detection by Exploiting Abnormal Knowledge](https://doi.org/10.1145/3637528.3671739) |  | 0 | Detecting time-series anomalies is extremely intricate due to the rarity of anomalies and imbalanced sample categories, which often result in costly and challenging anomaly labeling. Most of the existing approaches largely depend on assumptions of normality, overlooking labeled abnormal samples. While anomaly assumptions based methods can incorporate prior knowledge of anomalies for data augmentation in training classifiers, the adopted random or coarse-grained augmentation approaches solely focus on pointwise anomalies and lack cutting-edge domain knowledge, making them less likely to achieve better performance. This paper introduces CutAddPaste, a novel anomaly assumption-based approach for detecting time-series anomalies. It primarily employs a data augmentation strategy to generate pseudo anomalies, by exploiting prior knowledge of anomalies as much as possible. At the core of CutAddPaste is cutting patches from random positions in temporal subsequence samples, adding linear trend terms, and pasting them into other samples, so that it can well approximate a variety of anomalies, including point and pattern anomalies. Experiments on standard benchmark datasets demonstrate that our method outperforms the state-of-the-art approaches. | Rui Wang, Xudong Mou, Renyu Yang, Kai Gao, Pin Liu, Chongwei Liu, Tianyu Wo, Xudong Liu | School of Software, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Kuaishou Inc., Beijing, China; Institute of Future Cities, The Chinese University of Hong Kong, Hong Kong, China; School of Software, Beihang University & Zhongguancun Laboratory, Beijing, China; School of Information Engineering, China University of Geosciences Beijing, Beijing, China |
|  |  [Advancing Molecule Invariant Representation via Privileged Substructure Identification](https://doi.org/10.1145/3637528.3671886) |  | 0 | Graph neural networks (GNNs) have revolutionized molecule representation learning by modeling molecules as graphs, with atoms represented as nodes and chemical bonds as edges. Despite their progress, they struggle with out-of-distribution scenarios, such as changes in size or scaffold of molecules with identical properties. Some studies attempt to mitigate this issue through graph invariant learning, which penalizes prediction variance across environments to learn invariant representations. But in the realm of molecules, core functional groups forming privileged substructures dominate molecular properties and remain invariant across distribution shifts. This highlights the need for integrating this prior knowledge and ensuring the environment split compatible with molecule invariant learning. To bridge this gap, we propose a novel framework named MILI. Specifically, we first formalize molecule invariant learning based on privileged substructure identification and introduce substructure invariance constraint. Building on this foundation, we theoretically establish two criteria for environment splits conducive to molecule invariant learning. Inspired by these criteria, we develop a dual-head graph neural network. A shared identifier identifies privileged substructures, while environment and task heads generate predictions based on variant and privileged substructures. Through the interaction of two heads, the environments are split and optimized to meet our criteria. The unified MILI guarantees that molecule invariant learning and environment split achieve mutual enhancement from theoretical analysis and network design. Extensive experiments across eight benchmarks validate the effectiveness of MILI compared to state-of-the-art baselines. | Ruijia Wang, Haoran Dai, Cheng Yang, Le Song, Chuan Shi | BioMap Research & MBZUAI, Beijing, China; Beijing University of Post and Telecommunication, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China |
|  |  [Optimizing Long-tailed Link Prediction in Graph Neural Networks through Structure Representation Enhancement](https://doi.org/10.1145/3637528.3671864) |  | 0 | Link prediction, as a fundamental task for graph neural networks (GNNs), has boasted significant progress in varied domains. Its success is typically influenced by the expressive power of node representation, but recent developments reveal the inferior performance of low-degree nodes owing to their sparse neighbor connections, known as the degree-based long-tailed problem. Will the degree-based long-tailed distribution similarly constrain the efficacy of GNNs on link prediction? Unexpectedly, our study reveals that only a mild correlation exists between node degree and predictive accuracy, and more importantly, the number of common neighbors between node pairs exhibits a strong correlation with accuracy. Considering node pairs with less common neighbors, i.e., tail node pairs, make up a substantial fraction of the dataset but achieve worse performance, we propose that link prediction also faces the long-tailed problem. Therefore, link prediction of GNNs is greatly hindered by the tail node pairs. After knowing the weakness of link prediction, a natural question is how can we eliminate the negative effects of the skewed long-tailed distribution on common neighbors so as to improve the performance of link prediction? Towards this end, we introduce our long-tailed framework (LTLP), which is designed to enhance the performance of tail node pairs on link prediction by increasing common neighbors. Two key modules in LTLP respectively supplement high-quality edges for tail node pairs and enforce representational alignment between head and tail node pairs within the same category, thereby improving the performance of tail node pairs. Empirical results across five datasets confirm that our approach not only achieves SOTA performance but also greatly reduces the performance bias between the head and tail. These findings underscore the efficacy and superiority of our framework in addressing the long-tailed problem in link prediction. | Yakun Wang, Daixin Wang, Hongrui Liu, Binbin Hu, Yingcui Yan, Qiyang Zhang, Zhiqiang Zhang | Ant Group, Hangzhou, China; Ant Group, Shanghai, China; Ant Group, Beijing, China |
|  |  [DiffCrime: A Multimodal Conditional Diffusion Model for Crime Risk Map Inference](https://doi.org/10.1145/3637528.3671843) |  | 0 | Crime risk map plays a crucial role in urban planning and public security management. Traditionally, it is obtained solely from historical crime incidents or inferred from limited environmental factors, which are not sufficient to accurately model the occurrences of crimes over the geographical space well. Motivated by the impressive and realistic conditional generating power of diffusion models, in this paper, we propose a multimodal conditional diffusion method, namely, DiffCrime, to infer the crime risk map based on datasets in various domains, i.e., historical crime incidents, satellite imagery, and map imagery. It is equipped with a history-gated multimodal denoising network, i.e., HamNet, dedicated to the crime risk map inference. HamNet emphasizes the importance of historical crime data via a Gated-based History Fusion (GHF) module and adaptively controls multimodal conditions to be fused across different diffusion time steps via a Time step-Aware Modality Fusion (TAMF) module. Extensive experiments on two real-world datasets demonstrate the effectiveness of DiffCrime, which outperforms baselines by at least 43% and 31% in terms of RMSE, respectively. | Shuliang Wang, Xinyu Pan, Sijie Ruan, Haoyu Han, Ziyu Wang, Hanning Yuan, Jiabao Zhu, Qi Li | Beijing Institute of Technology, Beijing, China |
|  |  [AsyncET: Asynchronous Representation Learning for Knowledge Graph Entity Typing](https://doi.org/10.1145/3637528.3671832) |  | 0 |  | YunCheng Wang, Xiou Ge, Bin Wang, C.C. Jay Kuo |  |
|  |  [Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks](https://doi.org/10.1145/3637528.3671838) |  | 0 | Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available¹. | Yuwen Wang, Shunyu Liu, Tongya Zheng, Kaixuan Chen, Mingli Song |  |
|  |  [Self-Supervised Learning for Graph Dataset Condensation](https://doi.org/10.1145/3637528.3671682) |  | 0 | Graph dataset condensation (GDC) reduces a dataset with many graphs into a smaller dataset with fewer graphs while maintaining model training accuracy. GDC saves the storage cost and hence accelerates training. Although several GDC methods have been proposed, they are all supervised and require massive labels for the graphs, while graph labels can be scarce in many practical scenarios. To fill this gap, we propose a self-supervised graph dataset condensation method called SGDC, which does not require label information. Our initial design starts with the classical bilevel optimization paradigm for dataset condensation and incorporates contrastive learning techniques. But such a solution yields poor accuracy due to the biased gradient estimation caused by data augmentation. To solve this problem, we introduce representation matching, which conducts training by aligning the representations produced by the condensed graphs with the target representations generated by a pre-trained SSL model. This design eliminates the need for data augmentation and avoids biased gradient. We further propose a graph attention kernel, which not only improves accuracy but also reduces running time when combined with self-supervised kernel ridge regression (KRR). To simplify SGDC and make it more robust, we adopt a adjacency matrix reusing approach, which reuses the topology of the original graphs for the condensed graphs instead of repeatedly learning topology during training. Our evaluations on seven graph datasets find that SGDC improves model accuracy by up to 9.7% compared with 5 state-of-the-art baselines, even if they use label information. Moreover, SGDC is significantly more efficient than the baselines. | Yuxiang Wang, Xiao Yan, Shiyu Jin, Hao Huang, Quanqing Xu, Qingchen Zhang, Bo Du, Jiawei Jiang | School of Computer Science and Technology, Hainan University, Haikou, China; School of Computer Science, Wuhan University, Wuhan, China; Centre for Perceptual and Interactive Intelligence (CPII), Hong Kong, China; OceanBase, Ant Group, Hangzhou, China |
|  |  [From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models](https://doi.org/10.1145/3637528.3671975) |  | 0 | Tabular data is foundational to predictive modeling in various crucial industries, including healthcare, finance, retail, sustainability, etc. Despite the progress made in specialized models, there is an increasing demand for universal models that can transfer knowledge, generalize from limited data, and follow human instructions. These are challenges that current tabular deep learning approaches have not fully tackled. Here we introduce Generative Tabular Learning (GTL), a novel framework that integrates the advanced functionalities of large language models (LLMs)-such as prompt-based zero-shot generalization and in-context learning-into tabular deep learning. GTL capitalizes on the pre-training of LLMs on diverse tabular data, enhancing their understanding of domain-specific knowledge, numerical sequences, and statistical dependencies critical for accurate predictions. Our empirical study spans 384 public datasets, rigorously analyzing GTL's convergence and scaling behaviors and assessing the impact of varied data templates. The GTL-enhanced LLaMA-2 model demonstrates superior zero-shot and in-context learning capabilities across numerous classification and regression tasks. Notably, it achieves this without fine-tuning, outperforming traditional methods and rivaling state-of-the-art models like GPT-4 in certain cases. Through GTL, we not only foster a deeper integration of LLMs' sophisticated abilities into tabular data comprehension and application but also offer a new training resource and a test bed for LLMs to enhance their ability to comprehend tabular data. To facilitate reproducible research, we release our code, data, and model checkpoints at https://github.com/microsoft/Industrial-Foundation-Models. | Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian | Microsoft Research Asia, Beijing, China; Tsinghua University, Beijing, China |
|  |  [Dense Subgraph Discovery Meets Strong Triadic Closure](https://doi.org/10.1145/3637528.3671697) |  | 0 | Finding dense subgraphs is a core problem with numerous graph mining applications such as community detection in social networks and anomaly detection. However, in many real-world networks connections are not equal. One way to label edges as either strong or weak is to use strong triadic closure~(STC). Here, if one node connects strongly with two other nodes, then those two nodes should be connected at least with a weak edge. STC-labelings are not unique and finding the maximum number of strong edges is NP-hard. In this paper, we apply STC to dense subgraph discovery. More formally, our score for a given subgraph is the ratio between the sum of the number of strong edges and weak edges, weighted by a user parameter λ, and the number of nodes of the subgraph. Our goal is to find a subgraph and an STC-labeling maximizing the score. We show that for λ = 1, our problem is equivalent to finding the densest subgraph, while for λ = 0, our problem is equivalent to finding the largest clique, making our problem NP-hard. We propose an exact algorithm based on integer linear programming and four practical polynomial-time heuristics. We present an extensive experimental study that shows that our algorithms can find the ground truth in synthetic datasets and run efficiently in real-world datasets. | Chamalee Wickrama Arachchi, Iiro Kumpulainen, Nikolaj Tatti | University of Helsinki, Helsinki, Finland; HIIT, University of Helsinki, Helsinki, Finland |
|  |  [FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model](https://doi.org/10.1145/3637528.3671897) |  | 0 | Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL. Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels. | Feijie Wu, Zitao Li, Yaliang Li, Bolin Ding, Jing Gao | Alibaba Group, Bellevue, USA; Purdue University, West Lafayette, USA |
|  |  [Neural Manifold Operators for Learning the Evolution of Physical Dynamics](https://doi.org/10.1145/3637528.3671779) |  | 0 | Modeling the evolution of physical dynamics is a foundational problem in science and engineering, and it is regarded as the modeling of an operator mapping between infinite-dimensional functional spaces. Operator learning methods, learning the underlying infinite-dimensional operator in a high-dimensional latent space, have shown significant potential in modeling physical dynamics. However, there remains insufficient research on how to approximate an infinite-dimensional operator using a finite-dimensional parameter space. Inappropriate dimensionality representation of the underlying operator leads to convergence difficulties, decreasing generalization capability, and violating the physical consistency. To address the problem, we present Neural Manifold Operator (NMO) to learn the invariant subspace with the intrinsic dimension to parameterize infinite-dimensional underlying operators. NMO achieves state-of-the-art performance in statistical and physical metrics and gains 23.35% average improvement on three real-world scenarios and four equation-governed scenarios across a wide range of multi-disciplinary fields. Our paradigm has demonstrated universal effectiveness across various model structure implementations, including Multi-Layer Perceptron, Convolutional Neural Networks, and Transformers. Experimentally, we prove that the intrinsic dimension calculated by our paradigm is the optimal dimensional representation of the underlying operators. We release our code at https://github.com/AI4EarthLab/Neural-Manifold-Operators. | Hao Wu, Kangyu Weng, Shuyi Zhou, Xiaomeng Huang, Wei Xiong | Xingjian College, Tsinghua University, Beijing, China; Tencent TEG, Beijing, China |
|  |  [Distributional Network of Networks for Modeling Data Heterogeneity](https://doi.org/10.1145/3637528.3671994) |  | 0 | Heterogeneous data widely exists in various high-impact applications. Domain adaptation and out-of-distribution generalization paradigms have been formulated to handle the data heterogeneity across domains. However, most existing domain adaptation and out-of-distribution generalization algorithms do not explicitly explain how the label information can be adaptively propagated from the source domains to the target domain. Furthermore, little effort has been devoted to theoretically understanding the convergence of existing algorithms based on neural networks. To address these problems, in this paper, we propose a generic distributional network of networks (TENON) framework, where each node of the main network represents an individual domain associated with a domain-specific network. In this case, the edges within the main network indicate the domain similarity, and the edges within each network indicate the sample similarity. The crucial idea of TENON is to characterize the within-domain label smoothness and cross-domain parameter smoothness in a unified framework. The convergence and optimality of TENON are theoretically analyzed. Furthermore, we show that based on the TENON framework, domain adaptation and out-of-distribution generalization can be naturally formulated as transductive and inductive distribution learning problems, respectively. This motivates us to develop two instantiated algorithms (TENON-DA and TENON-OOD) of the proposed TENON framework for domain adaptation and out-of-distribution generalization. The effectiveness and efficiency of TENON-DA and TENON-OOD are verified both theoretically and empirically. | Jun Wu, Jingrui He, Hanghang Tong | University of Illinois at Urbana-Champaign, Champaign, IL, USA |
|  |  [Fake News in Sheep's Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks](https://doi.org/10.1145/3637528.3671977) |  | 0 | It is commonly perceived that fake news and real news exhibit distinct writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the advent of powerful Large Language Models (LLMs) has empowered malicious actors to mimic the style of trustworthy news sources, doing so swiftly, cost-effectively, and at scale. Our analysis reveals that LLM-camouflaged fake news content significantly undermines the effectiveness of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), implying a severe vulnerability to stylistic variations. To address this, we introduce SheepDog, a style-robust fake news detector that prioritizes content over style in determining news veracity. SheepDog achieves this resilience through (1) LLM-empowered news reframings that inject style diversity into the training process by customizing articles to match different styles; (2) a style-agnostic training scheme that ensures consistent veracity predictions across style-diverse reframings; and (3) content-focused veracity attributions that distill content-centric guidelines from LLMs for debunking fake news, offering supplementary cues and potential intepretability that assist veracity prediction. Extensive experiments on three real-world benchmarks demonstrate SheepDog's style robustness and adaptability to various backbones. | Jiaying Wu, Jiafeng Guo, Bryan Hooi | University of Chinese Academy of Sciences & Institute of Computing Technology, CAS, Beijing, China; National University of Singapore, Singapore, Singapore |
|  |  [Counterfactual Generative Models for Time-Varying Treatments](https://doi.org/10.1145/3637528.3671950) |  | 0 | Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines. | Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu | Princeton University, Princeton, NJ, USA; Carnegie Mellon University, Pittsburgh, PA, USA |
|  |  [ProCom: A Few-shot Targeted Community Detection Algorithm](https://doi.org/10.1145/3637528.3671749) |  | 0 | Targeted community detection aims to distinguish a particular type of community in the network. This is an important task with a lot of real-world applications, e.g., identifying fraud groups in transaction networks. Traditional community detection methods fail to capture the specific features of the targeted community and detect all types of communities indiscriminately. Semi-supervised community detection algorithms, emerged as a feasible alternative, are inherently constrained by their limited adaptability and substantial reliance on a large amount of labeled data, which demands extensive domain knowledge and manual effort. In this paper, we address the aforementioned weaknesses in targeted community detection by focusing on few-shot scenarios. We propose ProCom, a novel framework that extends the "pre-train, prompt'' paradigm, offering a low-resource, high-efficiency, and transferable solution. Within the framework, we devise a dual-level context-aware pre-training method that fosters a deep understanding of latent communities in the network, establishing a rich knowledge foundation for downstream tasks. In the prompt learning stage, we reformulate the targeted community detection task into pre-training objectives, allowing the extraction of specific knowledge relevant to the targeted community to facilitate effective and efficient inference. By leveraging both the general community knowledge acquired during pre-training and the specific insights gained from the prompt communities, ProCom exhibits remarkable adaptability across different datasets. We conduct extensive experiments on five benchmarks to evaluate the ProCom framework, demonstrating its SOTA performance under few-shot scenarios, strong efficiency, and transferability across diverse datasets. | Xixi Wu, Kaiyu Xiong, Yun Xiong, Xiaoxin He, Yao Zhang, Yizhu Jiao, Jiawei Zhang | University of Illinois at Urbana-Champaign, Champaign, Illinois, USA; IFM Lab, Department of Computer Science, University of California, Davis, Davis, California, USA; National University of Singapore, Singapore, Singapore |
|  |  [Cost-Efficient Fraud Risk Optimization with Submodularity in Insurance Claim](https://doi.org/10.1145/3637528.3672012) |  | 0 | The fraudulent insurance claim is critical for the insurance industry. Insurance companies or agency platforms aim to confidently estimate the fraud risk of claims by gathering data from various sources. Although more data sources can improve the estimation accuracy, they inevitably lead to increased costs. Therefore, a great challenge of fraud risk verification lies in well balancing these two aspects. To this end, this paper proposes a framework named cost-efficient fraud risk optimization with submodularity (CEROS) to optimize the process of fraud risk verification. CEROS efficiently allocates investigation resources across multiple information sources, balancing the trade-off between accuracy and cost. CEROS consists of two parts that we propose: a submodular set-wise classification model called SSCM to estimate the submodular objective function, and a primal-dual algorithm with segmentation point called PDA-SP to solve the objective function. Specifically, SSCM models the fraud probability associated with multiple information sources and ensures the properties of submodularity of fraud risk without making independence assumption. The submodularity in SSCM enables PDA-SP to significantly speed up dual optimization. Theoretically, we disclose that when PDA-SP optimizes this dual optimization problem, the process is monotonicity. Finally, the trade-off coefficients output by PDA-SP that balance accuracy and cost in fraud risk verification are applied to online insurance claim decision-making. We conduct experiments on offline trials and online A/B tests in two business areas at Alipay: healthcare insurance recommendation and claim verification. The extensive results indicate that, compared with other methods, CEROS achieves acceleration of 66.9% in convergence speed and meanwhile 18.8% in cost reduction. Currently, CEROS has been successfully deployed in Alipay. | Yupeng Wu, Zhibo Zhu, Chaoyi Ma, Hong Qian, Xingyu Lu, Yangwenhui Zhang, Xiaobo Qin, Binjie Fei, Jun Zhou, Aimin Zhou | School of Computer Science and Technology, East China Normal University, Shanghai, China; AntGroup, Hangzhou, China; Ant Group, Hangzhou, China |
|  |  [A Deep Prediction Framework for Multi-Source Information via Heterogeneous GNN](https://doi.org/10.1145/3637528.3671966) |  | 0 | Predicting information diffusion is a fundamental task in online social networks (OSNs). Recent studies mainly focus on the popularity prediction of specific content but ignore the correlation between multiple pieces of information. The topic is often used to correlate such information and can correspond to multi-source information. The popularity of a topic relies not only on information diffusion time but also on users' followership. Current solutions concentrate on hard time partition, lacking versatility. Meanwhile, the hop-based sampling adopted in state-of-the-art (SOTA) methods encounters redundant user followership. Moreover, many SOTA methods are not designed with good modularity and lack evaluation for each functional module and enlightening discussion. This paper presents a novel extensible framework, coined as HIF, for effective popularity prediction in OSNs with four original contributions. First, HIF adopts a soft partition of users and time intervals to better learn users' behavioral preferences over time. Second, HIF utilizes weighted sampling to optimize the construction of heterogeneous graphs and reduce redundancy. Furthermore, HIF supports multi-task collaborative optimization to improve its learning capability. Finally, as an extensible framework, HIF provides generic module slots to combine different submodules (e.g., RNNs, Transformer encoders). Experiments show that HIF significantly improves performance and interpretability compared to SOTAs. | Zhen Wu, Jingya Zhou, Jinghui Zhang, Ling Liu, Chizhou Huang | School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science, Georgia Institute of Technology, Atlanta, USA; School of Computer Science and Technology, Soochow University, Suzhou, China |
|  |  [Fast Computation of Kemeny's Constant for Directed Graphs](https://doi.org/10.1145/3637528.3671859) |  | 0 | Kemeny's constant for random walks on a graph is defined as the mean hitting time from one node to another selected randomly according to the stationary distribution. It has found numerous applications and attracted considerable research interest. However, exact computation of Kemeny's constant requires matrix inversion, which scales poorly for large networks with millions of nodes. Existing approximation algorithms either leverage properties exclusive to undirected graphs or involve inefficient simulation, leaving room for further optimization. To address these limitations for directed graphs, we propose two novel approximation algorithms for estimating Kemeny's constant on directed graphs with theoretical error guarantees. Extensive numerical experiments on real-world networks validate the superiority of our algorithms over baseline methods in terms of efficiency and accuracy. | Haisong Xia, Zhongzhi Zhang | Fudan University, Shanghai, China |
|  |  [FLea: Addressing Data Scarcity and Label Skew in Federated Learning via Privacy-preserving Feature Augmentation](https://doi.org/10.1145/3637528.3671899) |  | 0 | Federated Learning (FL) enables model development by leveraging data distributed across numerous edge devices without transferring local data to a central server. However, existing FL methods still face challenges when dealing with scarce and label-skewed data across devices, resulting in local model overfitting and drift, consequently hindering the performance of the global model. In response to these challenges, we propose a pioneering framework called FLea, incorporating the following key components: i) A global feature buffer that stores activation-target pairs shared from multiple clients to support local training. This design mitigates local model drift caused by the absence of certain classes; ii) A feature augmentation approach based on local and global activation mix-ups for local training. This strategy enlarges the training samples, thereby reducing the risk of local overfitting; iii) An obfuscation method to minimize the correlation between intermediate activations and the source data, enhancing the privacy of shared features. To verify the superiority of FLea, we conduct extensive experiments using a wide range of data modalities, simulating different levels of local data scarcity and label skew. The results demonstrate that FLea consistently outperforms state-of-the-art FL counterparts (among 13 of the experimented 18 settings, the improvement is over 5%) while concurrently mitigating the privacy vulnerabilities associated with shared features. Code is available at https://github.com/XTxiatong/FLea.git | Tong Xia, Abhirup Ghosh, Xinchi Qiu, Cecilia Mascolo | University of Birmingham & University of Cambridge, Birmingham, United Kingdom; University of Cambridge, Cambridge, United Kingdom |
|  |  [Motif-Consistent Counterfactuals with Adversarial Refinement for Graph-level Anomaly Detection](https://doi.org/10.1145/3637528.3672050) |  | 0 | Graph-level anomaly detection is significant in diverse domains. To improve detection performance, counterfactual graphs have been exploited to benefit the generalization capacity by learning causal relations. Most existing studies directly introduce perturbations (e.g., flipping edges) to generate counterfactual graphs, which are prone to alter the semantics of generated examples and make them off the data manifold, resulting in sub-optimal performance. To address these issues, we propose a novel approach, Motif-consistent Counterfactuals with Adversarial Refinement (MotifCAR), for graph-level anomaly detection. The model combines the motif of one graph, the core subgraph containing the identification (category) information, and the contextual subgraph (non-motif) of another graph to produce a raw counterfactual graph. However, the produced raw graph might be distorted and cannot satisfy the important counterfactual properties: Realism, Validity, Proximity and Sparsity. Towards that, we present a Generative Adversarial Network (GAN)-based graph optimizer to refine the raw counterfactual graphs. It adopts the discriminator to guide the generator to generate graphs close to realistic data, i.e., meet the property Realism. Further, we design the motif consistency to force the motif of the generated graphs to be consistent with the realistic graphs, meeting the property Validity. Also, we devise the contextual loss and connection loss to control the contextual subgraph and the newly added links to meet the properties Proximity and Sparsity. As a result, the model can generate high-quality counterfactual graphs. Experiments demonstrate the superiority of MotifCAR. | Chunjing Xiao, Shikang Pang, Wenxin Tai, Yanlong Huang, Goce Trajcevski, Fan Zhou | Iowa State University, Ames, IA, USA; University of Electronic Science and Technology of China, Chengdu, China; Henan University, Kaifeng, China |
|  |  [ReFound: Crafting a Foundation Model for Urban Region Understanding upon Language and Visual Foundations](https://doi.org/10.1145/3637528.3671992) |  | 0 | Understanding urban regional characteristics is pivotal in driving critical insights for urban planning and management. We have witnessed the successful application of pre-trained Foundation Models (FMs) in generating universal representations for various downstream tasks. However, applying this principle to the geospatial domain remains challenging, primarily due to the difficulty of gathering extensive data for developing a dedicated urban foundation model. Though there have been some attempts to empower the existing FMs with urban data, most of them focus on single-modality FMs without considering the multi-modality nature of urban region understanding tasks. To address this gap, we introduce ReFound - a novel framework for Re-training a Foundation model for urban region understanding, harnessing the strengths of both language and visual FMs. In this framework, we first invent a Mixture-of-Geospatial-Expert (MoGE) Transformer, to effectively integrate the embedding of multi-source geospatial data. Building on this, ReFound is enhanced by jointly distilling knowledge from language, visual, and visual-language FMs respectively, thus augmenting its generalization capabilities. Meanwhile, we design a masked geospatial data modeling approach alongside a cross-modal spatial alignment mechanism, to enhance the spatial knowledge of ReFound derived from geospatial data. Extensive experiments conducted on six real-world datasets over three urban region understanding tasks demonstrate the superior performance of our framework. | Congxi Xiao, Jingbo Zhou, Yixiong Xiao, Jizhou Huang, Hui Xiong | Business Intelligence Lab, Baidu Research, Beijing, China; Baidu Inc., Beijing, China |
|  |  [How to Avoid Jumping to Conclusions: Measuring the Robustness of Outstanding Facts in Knowledge Graphs](https://doi.org/10.1145/3637528.3671763) |  | 0 | An outstanding fact (OF) is a striking claim by which some entities stand out from their peers on some attribute. OFs serve data journalism, fact checking, and recommendation. However, one could jump to conclusions by selecting truthful OFs while intentionally or inadvertently ignoring lateral contexts and data that render them less striking. This jumping conclusion bias from unstable OFs may disorient the public, including voters and consumers, raising concerns about fairness and transparency in political and business competition. It is thus ethically imperative for several stakeholders to measure the robustness of OFs with respect to lateral contexts and data. Unfortunately, a capacity for such inspection of OFs mined from knowledge graphs (KGs) is missing. In this paper, we propose a methodology that inspects the robustness of OFs in KGs by perturbation analysis. We define (1) entity perturbation, which detects outlying contexts by perturbing context entities in the OF; and (2) data perturbation, which considers plausible data that render an OF less striking. We compute the expected strikingness scores of OFs over perturbation relevance distributions and assess an OF as robust if its measured strikingness does not deviate significantly from the expected. We devise a suite of exact and sampling algorithms for perturbation analysis on large KGs. Extensive experiments reveal that our methodology accurately and efficiently detects frail OFs generated by existing mining approaches on KGs. We also show the effectiveness of our approaches through case and user studies. | Hanhua Xiao, Yuchen Li, Yanhao Wang, Panagiotis Karras, Kyriakos Mouratidis, Natalia Rozalia Avlona | Singapore Management University, Singapore, Singapore; East China Normal University, Shanghai, China; University of Copenhagen, Copenhagen, Denmark |
|  |  [Temporal Prototype-Aware Learning for Active Voltage Control on Power Distribution Networks](https://doi.org/10.1145/3637528.3671790) |  | 0 | Active Voltage Control (AVC) on the Power Distribution Networks (PDNs) aims to stabilize the voltage levels to ensure efficient and reliable operation of power systems. With the increasing integration of distributed energy resources, recent efforts have explored employing multi-agent reinforcement learning (MARL) techniques to realize effective AVC. Existing methods mainly focus on the acquisition of short-term AVC strategies, i.e., only learning AVC within the short-term training trajectories of a singular diurnal cycle. However, due to the dynamic nature of load demands and renewable energy, the operation states of real-world PDNs may exhibit significant distribution shifts across varying timescales (e.g., daily and seasonal changes). This can render those short-term strategies suboptimal or even obsolete when performing continuous AVC over extended periods. In this paper, we propose a novel temporal prototype-aware learning method, abbreviated as TPA, to learn time-adaptive AVC under short-term training trajectories. At the heart of TPA are two complementary components, namely multi-scale dynamic encoder and temporal prototype-aware policy, that can be readily incorporated into various MARL methods. The former component integrates a stacked transformer network to learn underlying temporal dependencies at different timescales of the PDNs, while the latter implements a learnable prototype matching mechanism to construct a dedicated AVC policy that can dynamically adapt to the evolving operation states. Experimental results on the AVC benchmark with different PDN sizes demonstrate that the proposed TPA surpasses the state-of-the-art counterparts not only in terms of control performance but also by offering model transferability. Our code is available at https://github.com/Canyizl/TPA-for-AVC. | Feiyang Xu, Shunyu Liu, Yunpeng Qing, Yihe Zhou, Yuwen Wang, Mingli Song |  |
|  |  [FlexCare: Leveraging Cross-Task Synergy for Flexible Multimodal Healthcare Prediction](https://doi.org/10.1145/3637528.3671974) |  | 0 | Multimodal electronic health record (EHR) data can offer a holistic assessment of a patient's health status, supporting various predictive healthcare tasks. Recently, several studies have embraced the multitask learning approach in the healthcare domain, exploiting the inherent correlations among clinical tasks to predict multiple outcomes simultaneously. However, existing methods necessitate samples to possess complete labels for all tasks, which places heavy demands on the data and restricts the flexibility of the model. Meanwhile, within a multitask framework with multimodal inputs, how to comprehensively consider the information disparity among modalities and among tasks still remains a challenging problem. To tackle these issues, a unified healthcare prediction model, also named by FlexCare, is proposed to flexibly accommodate incomplete multimodal inputs, promoting the adaption to multiple healthcare tasks. The proposed model breaks the conventional paradigm of parallel multitask prediction by decomposing it into a series of asynchronous single-task prediction. Specifically, a task-agnostic multimodal information extraction module is presented to capture decorrelated representations of diverse intra- and inter-modality patterns. Taking full account of the information disparities between different modalities and different tasks, we present a task-guided hierarchical multimodal fusion module that integrates the refined modality-level representations into an individual patient-level representation. Experimental results on multiple tasks from MIMIC-IV/MIMIC-CXR/MIMIC-NOTE datasets demonstrate the effectiveness of the proposed method. Additionally, further analysis underscores the feasibility and potential of employing such a multitask strategy in the healthcare domain. The source code is available at https://github.com/mhxu1998/FlexCare \*\*REMOVE 2nd URL\*\*://github.com/mhxu1998/FlexCare. | Muhao Xu, Zhenfeng Zhu, Youru Li, Shuai Zheng, Yawei Zhao, Kunlun He, Yao Zhao | Medical Big Data Research Center, Chinese PLA General Hospital, Beijing, China |
|  |  [PeFAD: A Parameter-Efficient Federated Framework for Time Series Anomaly Detection](https://doi.org/10.1145/3637528.3671753) |  | 0 | With the proliferation of mobile sensing techniques, huge amounts of timeseries data are generated and accumulated in various domains, fueling plenty ofreal-world applications. In this setting, time series anomaly detection ispractically important. It endeavors to identify deviant samples from the normalsample distribution in time series. Existing approaches generally assume thatall the time series is available at a central location. However, we arewitnessing the decentralized collection of time series due to the deployment ofvarious edge devices. To bridge the gap between the decentralized time seriesdata and the centralized anomaly detection algorithms, we propose aParameter-efficient Federated Anomaly Detection framework named PeFAD with theincreasing privacy concerns. PeFAD for the first time employs the pre-trainedlanguage model (PLM) as the body of the client's local model, which can benefitfrom its cross-modality knowledge transfer capability. To reduce thecommunication overhead and local model adaptation cost, we propose aparameter-efficient federated training module such that clients only need tofine-tune small-scale parameters and transmit them to the server for update.PeFAD utilizes a novel anomaly-driven mask selection strategy to mitigate theimpact of neglected anomalies during training. A knowledge distillationoperation on a synthetic privacy-preserving dataset that is shared by all theclients is also proposed to address the data heterogeneity issue acrossclients. We conduct extensive evaluations on four real datasets, where PeFADoutperforms existing state-of-the-art baselines by up to 28.74%. | Ronghui Xu, Hao Miao, Senzhang Wang, Philip S. Yu, Jianxin Wang | Central South University, Changsha, China; University of Illinois at Chicago, Chicago, USA; Aalborg University, Aalborg, Denmark |
|  |  [ProtoMix: Augmenting Health Status Representation Learning via Prototype-based Mixup](https://doi.org/10.1145/3637528.3671937) |  | 0 | With the widespread adoption of electronic health records (EHR) data, deep learning techniques have been broadly utilized for various health prediction tasks. Nevertheless, the labeled data scarcity issue restricts the prediction power of these deep models. To enhance the generalization capability of deep learning models when faced with such situations, a common trend is to train generative adversarial networks (GANs) or diffusion models for data augmentation. However, due to limitations in sample size and potential label imbalance issues, these methods are prone to mode collapse problems. This results in the generation of new samples that fail to preserve the subtype structure within EHR data, thereby limiting their practicality in health prediction tasks that generally require detailed patient phenotyping. Aiming at the above problems, we propose a Prototype-based Mixup method, dubbed ProtoMix, which combines prior knowledge of intrinsic data features from subtype centroids (i.e., prototypes) to guide the synthesis of new samples. Specifically, ProtoMix employs a prototype-guided mixup training task to shift the decision boundary away from the subtypes. Then, ProtoMix optimizes the sampling weights in different areas of the data manifold via a prototype-guided mixup sampling strategy. Throughout the training process, ProtoMix dynamically expands the training distribution using an adaptive mixing coefficient computation method. Experimental evaluations on three real-world datasets demonstrate the efficacy of ProtoMix. | Yongxin Xu, Xinke Jiang, Xu Chu, Yuzhen Xiao, Chaohe Zhang, Hongxin Ding, Junfeng Zhao, Yasha Wang, Bing Xie |  |
|  |  [FedRoLA: Robust Federated Learning Against Model Poisoning via Layer-based Aggregation](https://doi.org/10.1145/3637528.3671906) |  | 0 | Federated Learning (FL) is increasingly vulnerable to model poisoning attacks, where malicious clients degrade the global model's accuracy with manipulated updates. Unfortunately, most existing defenses struggle to handle the scenarios when multiple adversaries exist, and often rely on historical or validation data, rendering them ill-suited for the dynamic and diverse nature of real-world FL environments. Exacerbating these limitations is the fact that most existing defenses also fail to account for the distinctive contributions of Deep Neural Network (DNN) layers in detecting malicious activity, leading to the unnecessary rejection of benign updates. To bridge these gaps, we introduce FedRoLa, a cutting-edge similarity-based defense method optimized for FL. Specifically, FedRoLa leverages global model parameters and client updates independently, moving away from reliance on historical or validation data. It features a unique layer-based aggregation with dynamic layer selection, enhancing threat detection, and includes a dynamic probability method for balanced security and model performance. Through comprehensive evaluations using different DNN models and real-world datasets, FedRoLa demonstrates substantial improvements over the status quo approaches in global model accuracy, achieving up to 4% enhancement in terms of accuracy, reducing false positives to 6.4%, and securing an 92.8% true positive rate. | Gang Yan, Hao Wang, Xu Yuan, Jian Li | Stony Brook University, Stony Brook, NY, USA; Binghamton University, Binghamton, NY, USA; Stevens Institute of Technology, Hoboken, NJ, USA; University of Delaware, Newark, DE, USA |
|  |  [Team up GBDTs and DNNs: Advancing Efficient and Effective Tabular Prediction with Tree-hybrid MLPs](https://doi.org/10.1145/3637528.3671964) |  | 0 | Tabular datasets play a crucial role in various applications. Thus, developing efficient, effective, and widely compatible prediction algorithms for tabular data is important. Currently, two prominent model types, Gradient Boosted Decision Trees (GBDTs) and Deep Neural Networks (DNNs), have demonstrated performance advantages on distinct tabular prediction tasks. However, selecting an effective model for a specific tabular dataset is challenging, often demanding time-consuming hyperparameter tuning. To address this model selection dilemma, this paper proposes a new framework that amalgamates the advantages of both GBDTs and DNNs, resulting in a DNN algorithm that is as efficient as GBDTs and is competitively effective regardless of dataset preferences for GBDTs or DNNs. Our idea is rooted in an observation that deep learning (DL) offers a larger parameter space that can represent a well-performing GBDT model, yet the current back-propagation optimizer struggles to efficiently discover such optimal functionality. On the other hand, during GBDT development, hard tree pruning, entropy-driven feature gate, and model ensemble have proved to be more adaptable to tabular data. By combining these key components, we present a Tree-hybrid simple MLP (T-MLP). In our framework, a tensorized, rapidly trained GBDT feature gate, a DNN architecture pruning approach, as well as a vanilla back-propagation optimizer collaboratively train a randomly initialized MLP model. Comprehensive experiments show that T-MLP is competitive with extensively tuned DNNs and GBDTs in their dominating tabular benchmarks (88 datasets) respectively, all achieved with compact model storage and significantly reduced training duration. The codes and full experiment results are available at https://github.com/jyansir/tmlp. | Jiahuan Yan, Jintai Chen, Qianxing Wang, Danny Z. Chen, Jian Wu | Zhejiang University, Hangzhou, China; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Notre Dame, Notre Dame, IN, USA |
|  |  [Efficient Mixture of Experts based on Large Language Models for Low-Resource Data Preprocessing](https://doi.org/10.1145/3637528.3671873) |  | 0 | Data preprocessing (DP) that transforms erroneous and raw data to a clean version is a cornerstone of the data mining pipeline. Due to the diverse requirements of downstream tasks, data scientists and domain experts have to handcraft domain-specific rules or train ML models with annotated examples, which is costly/time-consuming. In this paper, we present MELD (Mixture of Experts on Large Language Models for Data Preprocessing), a universal solver for low-resource DP. MELD adopts a Mixture-of-Experts (MoE) architecture that enables the amalgamation and enhancement of domain-specific experts trained on limited annotated examples. To fine-tune MELD, we develop a suite of expert-tuning and MoE-tuning techniques, including a retrieval augmented generation (RAG) system, meta-path search for data augmentation, expert refinement and router network training based on information bottleneck. To further verify the effectiveness of MELD, we theoretically prove that MoE in MELD is superior than a single expert and the router network is able to dispatch data to the right experts. Finally, we conducted extensive experiments on 19 datasets over 10 DP tasks to show that MELD outperforms the state-of-the-art methods in both effectiveness and efficiency. More importantly, MELD is able to be fine-tuned in a low-resource environment, e.g. a local, single and low-priced 3090 GPU. | Mengyi Yan, Yaoshu Wang, Kehan Pang, Min Xie, Jianxin Li | Beihang University, Beijing, China; Shenzhen Institute of Computing Sciences, Shenzhen, China |
|  |  [An Efficient Subgraph GNN with Provable Substructure Counting Power](https://doi.org/10.1145/3637528.3671731) |  | 0 | We investigate the enhancement of graph neural networks' (GNNs)representation power through their ability in substructure counting. Recentadvances have seen the adoption of subgraph GNNs, which partition an inputgraph into numerous subgraphs, subsequently applying GNNs to each to augmentthe graph's overall representation. Despite their ability to identify varioussubstructures, subgraph GNNs are hindered by significant computational andmemory costs. In this paper, we tackle a critical question: Is it possible forGNNs to count substructures both efficiently and provably?Our approach begins with a theoretical demonstration that the distance torooted nodes in subgraphs is key to boosting the counting power of subgraphGNNs. To avoid the need for repetitively applying GNN across all subgraphs, weintroduce precomputed structural embeddings that encapsulate this crucialdistance information. Experiments validate that our proposed model retains thecounting power of subgraph GNNs while achieving significantly fasterperformance. | Zuoyu Yan, Junru Zhou, Liangcai Gao, Zhi Tang, Muhan Zhang | Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Beijing Institute for General Artificial Intelligence, Peking University, Beijing, China; Institute for Artificial Intelligence, Peking University, Beijing, China |
|  |  [Towards Test Time Adaptation via Calibrated Entropy Minimization](https://doi.org/10.1145/3637528.3671672) |  | 0 | Robust models must demonstrate strong generalizability, even amid environmental changes. However, the complex variability and noise in real-world data often lead to a pronounced performance gap between the training and testing phases. Researchers have recently introduced test-time-domain adaptation (TTA) to address this challenge. TTA methods primarily adapt source-pretrained models to a target domain using only unlabeled test data. This study found that existing TTA methods consider only the largest logit as a pseudo-label and aim to minimize the entropy of test time predictions. This maximizes the predictive confidence of the model. However, this corresponds to the model being overconfident in the local test scenarios. In response, we introduce a novel confidence-calibration loss function called Calibrated Entropy Test-Time Adaptation (CETA), which considers the model's largest logit and the next-highest-ranked one, aiming to strike a balance between overconfidence and underconfidence. This was achieved by incorporating a sample-wise regularization term. We also provide a theoretical foundation for the proposed loss function. Experimentally, our method outperformed existing strategies on benchmark corruption datasets across multiple models, underscoring the efficacy of our approach. | Hao Yang, Min Wang, Jinshen Jiang, Yun Zhou | National University of Defense Technology, ChangSha, China |
|  |  [Noisy Label Removal for Partial Multi-Label Learning](https://doi.org/10.1145/3637528.3671677) |  | 0 | This paper addresses the problem of partial multi-label learning (PML), a challenging weakly supervised learning framework, where each sample is associated with a candidate label set comprising both ground-true labels and noisy labels. We theoretically reveal that an increased number of noisy labels in the candidate label set leads to an enlarged generalization error bound, consequently degrading the classification performance. Accordingly, the key to solving PML lies in accurately removing the noisy labels within the candidate label set. To achieve this objective, we leverage prior knowledge about the noisy labels in PML, which suggests that they only exist within the candidate label set and possess binary values. Specifically, we propose a constrained regression model to learn a PML classifier and select the noisy labels. The constraints of the model strictly enforce the location and value of the noisy labels. Simultaneously, the supervision information provided by the candidate label set is unreliable due to the presence of noisy labels. In contrast, the non-candidate labels of a sample precisely indicate the classes to which the sample does not belong. To aid in the selection of noisy labels, we construct a competitive classifier based on the non-candidate labels. The PML classifier and the competitive classifier form a competitive relationship, encouraging mutual learning. We formulate the proposed model as a discrete optimization problem to effectively remove the noisy labels, and we solve it using an alternative algorithm. Extensive experiments conducted on 6 real-world partial multi-label data sets and 7 synthetic data sets, employing various evaluation metrics, demonstrate that our method significantly outperforms state-of-the-art PML methods. The code implementation is publicly available at https://github.com/Yangfc-ML/NLR. | Fuchao Yang, Yuheng Jia, Hui Liu, Yongqiang Dong, Junhui Hou | School of Computing & Information Sciences, Saint Francis University, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; College of Software Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China |
|  |  [Balanced Confidence Calibration for Graph Neural Networks](https://doi.org/10.1145/3637528.3671741) |  | 0 | This paper delves into the confidence calibration in prediction when using Graph Neural Networks (GNNs), which has emerged as a notable challenge in the field. Despite their remarkable capabilities in processing graph-structured data, GNNs are prone to exhibit lower confidence in their predictions than what the actual accuracy warrants. Recent advances attempt to address this by minimizing prediction entropy to enhance confidence levels. However, this method inadvertently risks leading to over-confidence in model predictions. Our investigation in this work reveals that most existing GNN calibration methods predominantly focus on the highest logit, thereby neglecting the entire spectrum of prediction probabilities. To alleviate this limitation, we introduce a novel framework called Balanced Calibrated Graph Neural Network (BCGNN), specifically designed to establish a balanced calibration between over-confidence and under-confidence in GNNs' prediction. To theoretically support our proposed method, we further demonstrate the mechanism of the BCGNN framework in effective confidence calibration and significant trustworthiness improvement in prediction. We conduct extensive experiments to examine the developed framework. The empirical results show our method's superior performance in predictive confidence and trustworthiness, affirming its practical applicability and effectiveness in real-world scenarios. | Hao Yang, Min Wang, Qi Wang, Mingrui Lao, Yun Zhou | National University of Defense Technology, ChangSha, China |
|  |  [Efficient Decision Rule List Learning via Unified Sequence Submodular Optimization](https://doi.org/10.1145/3637528.3671827) |  | 0 | Interpretable models are crucial in many high-stakes decision-making applications. In this paper, we focus on learning a decision rule list for binary and multi-class classification. Different from rule set learning problems, learning an optimal rule list involves not only learning a set of rules, but also their orders. In addition, many existing algorithms rely on rule pre-mining to handle large-scale high-dimensional data, which leads to suboptimal rule list model and degrades its generalization accuracy and interpretablity. In this paper, we learn a rule list from the sequence submodular perspective. We consider the rule list as a sequence and define the cover set for each rule. Then we formulate a sequence function which combines both model complexity and classification accuracy. Based on its appealing sequence submodular property, we propose a general distorted greedy insert algorithm under Minorization-Maximization (MM) framework, which gradually inserts rules with highest inserting gain to the rule list. The rule generation process is treated as a subproblem, allowing our method to learn the rule list through a unified framework which avoids rule pre-mining. We further provide a theoretical lower bound of our greedy insert algorithm in rule list learning. Experimental results show that our algorithm achieves better accuracy and interpretability than the state-of-the-art rule learning methods, and in particular it scales well on large-scale datasets, especially on high-dimensional data. | Linxiao Yang, Jingbang Yang, Liang Sun | DAMO Academy, Alibaba Group, Hangzhou, Zhejiang, China |
|  |  [Effective Clustering on Large Attributed Bipartite Graphs](https://doi.org/10.1145/3637528.3671764) |  | 0 | Attributed bipartite graphs (ABGs) are an expressive data model fordescribing the interactions between two sets of heterogeneous nodes that areassociated with rich attributes, such as customer-product purchase networks andauthor-paper authorship graphs. Partitioning the target node set in such graphsinto k disjoint clusters (referred to as k-ABGC) finds widespread use invarious domains, including social network analysis, recommendation systems,information retrieval, and bioinformatics. However, the majority of existingsolutions towards k-ABGC either overlook attribute information or fail tocapture bipartite graph structures accurately, engendering severely compromisedresult quality. The severity of these issues is accentuated in real ABGs, whichoften encompass millions of nodes and a sheer volume of attribute data,rendering effective k-ABGC over such graphs highly challenging. In this paper, we propose TPO, an effective and efficient approach to k-ABGCthat achieves superb clustering performance on multiple real datasets. TPOobtains high clustering quality through two major contributions: (i) a novelformulation and transformation of the k-ABGC problem based on multi-scaleattribute affinity specialized for capturing attribute affinities between nodeswith the consideration of their multi-hop connections in ABGs, and (ii) ahighly efficient solver that includes a suite of carefully-craftedoptimizations for sidestepping explicit affinity matrix construction andfacilitating faster convergence. Extensive experiments, comparing TPO against19 baselines over 5 real ABGs, showcase the superior clustering quality of TPOmeasured against ground-truth labels. Moreover, compared to the state of thearts, TPO is often more than 40x faster over both small and large ABGs. | Renchi Yang, Yidu Wu, Xiaoyang Lin, Qichen Wang, Tsz Nam Chan, Jieming Shi | Shenzhen University, Shenzhen, China; Hong Kong Baptist University, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China |
|  |  [ReCDA: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection](https://doi.org/10.1145/3637528.3672007) |  | 0 | The deployment of learning-based models to detect malicious activities in network traffic flows is significantly challenged by concept drift. With evolving attack technology and dynamic attack behaviors, the underlying data distribution of recently arrived traffic flows deviates from historical empirical distributions over time. Existing approaches depend on a significant amount of labeled drifting samples to facilitate the deep model to handle concept drift, which faces labor-intensive manual labeling and the risk of label noise. In this paper, we propose ReCDA, a Concept Drift Adaptation method with Representation enhancement, which consists of a self-supervised representation enhancement stage and a weakly-supervised classifier tuning stage. Specifically, in the initial stage, ReCDA introduces drift-aware perturbation and representation alignment to facilitate the model in acquiring robust representations from drift-aware and drift-invariant perspectives. Moreover, in the subsequent stage, a meticulously crafted instructive sampling strategy and a robust representation constraint encourage the model to learn discriminative knowledge about benign and malicious activities during fine-tuning, thereby enhancing performance further. We conduct comprehensive evaluations on several benchmark datasets under varying degrees of concept drift. The experiment results demonstrate the superior adaptability and robustness of the proposed method. | Shuo Yang, Xinran Zheng, Jinze Li, Jinfeng Xu, Xingjun Wang, Edith C. H. Ngai | Tsinghua University, Beijing, China; The University of Hong Kong, Hong Kong SAR, China |
|  |  [Your Neighbor Matters: Towards Fair Decisions Under Networked Interference](https://doi.org/10.1145/3637528.3671960) |  | 0 | In the era of big data, decision-making in social networks may introduce bias due to interconnected individuals. For instance, in peer-to-peer loan platforms on the Web, considering an individual's attributes along with those of their interconnected neighbors, including sensitive attributes, is vital for loan approval or rejection downstream. Unfortunately, conventional fairness approaches often assume independent individuals, overlooking the impact of one person's sensitive attribute on others' decisions. To fill this gap, we introduce "Interference-aware Fairness" (IAF) by defining two forms of discrimination as Self-Fairness (SF) and Peer-Fairness (PF), leveraging advances in interference analysis within causal inference. Specifically, SF and PF causally capture and distinguish discrimination stemming from an individual's sensitive attributes (with fixed neighbors' sensitive attributes) and from neighbors' sensitive attributes (with fixed self's sensitive attributes), separately. Hence, a network-informed decision model is fair only when SF and PF are satisfied simultaneously, as interventions in individuals' sensitive attributes or those of their peers both yield equivalent outcomes. To achieve IAF, we develop a deep doubly robust framework to estimate and regularize SF and PF metrics for decision models. Extensive experiments on synthetic and real-world datasets validate our proposed concepts and methods. | Wenjing Yang, Haotian Wang, Haoxuan Li, Hao Zou, Ruochun Jin, Kun Kuang, Peng Cui | ZGC laboratory, Beijing, China; Tsinghua University, Beijing, China; Institute of Artificial Intelligence, Zhejiang University, Hangzhou, China; Peking University, Beijing, China |
|  |  [SEBot: Structural Entropy Guided Multi-View Contrastive learning for Social Bot Detection](https://doi.org/10.1145/3637528.3671871) |  | 0 | Recent advancements in social bot detection have been driven by the adoption of Graph Neural Networks. The social graph, constructed from social network interactions, contains benign and bot accounts that influence each other. However, previous graph-based detection methods that follow the transductive message-passing paradigm may not fully utilize hidden graph information and are vulnerable to adversarial bot behavior. The indiscriminate message passing between nodes from different categories and communities results in excessively homogeneous node representations, ultimately reducing the effectiveness of social bot detectors. In this paper, we propose \SEBot, a novel multi-view graph-based contrastive learning-enabled social bot detector. In particular, we use structural entropy as an uncertainty metric to optimize the entire graph's structure and subgraph-level granularity, revealing the implicitly existing hierarchical community structure. And we design an encoder to enable message passing beyond the homophily assumption, enhancing robustness to adversarial behaviors of social bots. Finally, we employ multi-view contrastive learning to maximize mutual information between different views and enhance the detection performance through multi-task learning. Experimental results demonstrate that our approach significantly improves the performance of social bot detection compared with SOTA methods. | Yingguang Yang, Qi Wu, Buyun He, Hao Peng, Renyu Yang, Zhifeng Hao, Yong Liao | Beihang University, Beijing, China; Shantou University, Shantou, China; University of Science and Technology of China, Hefei, China |
|  |  [AdaRD: An Adaptive Response Denoising Framework for Robust Learner Modeling](https://doi.org/10.1145/3637528.3671684) |  | 0 | Learner modeling is a crucial task in online learning environments, where Cognitive Diagnosis Models (CDMs) are employed to assess learners' knowledge mastery levels based on recorded response logs. However, the prevalence of noise in recorded response data poses significant challenges, including various behaviors such as guess and slip, casual answers, and system-induced errors. The existence of noise degrades the accuracy of diagnosis results and learner performance predictions. In this work, we propose a general framework, Adaptive Response Denoising (AdaRD), designed to salvage CDMs from the influence of noisy learner-exercise responses. AdaRD extends existing CDMs, incorporating primary training for denoised CDMs and auxiliary training for additional denoising support. The primary training employs binary Generalized Cross Entropy (GCE) loss to slow down the large update of learner knowledge states caused by noisy responses. Simultaneously, we utilize the variance of diagnosed knowledge mastery levels between primary and auxiliary diagnosis modules as a criterion to downweight high-variance responses that are likely to be noisy. In this manner, the proposed framework can prune noisy response learning during training, thereby enhancing the accuracy and robustness of CDMs. Extensive experiments on both real-world and synthetic datasets validate AdaRD's effectiveness in mitigating the impact of noisy learner-exercise responses. | Fangzhou Yao, Qi Liu, Linan Yue, Weibo Gao, Jiatong Li, Xin Li, Yuanjing He | The Open University of China, Beijing, China |
|  |  [RPMixer: Shaking Up Time Series Forecasting with Random Projections for Large Spatial-Temporal Data](https://doi.org/10.1145/3637528.3671881) |  | 0 | Spatial-temporal forecasting systems play a crucial role in addressing numerous real-world challenges. In this paper, we investigate the potential of addressing spatial-temporal forecasting problems using general time series forecasting models, i.e., models that do not leverage the spatial relationships among the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series forecasting architecture called RPMixer. The all-MLP architecture was chosen due to its recent success in time series forecasting benchmarks. Furthermore, our method capitalizes on the ensemble-like behavior of deep neural networks, where each individual block within the network behaves like a base learner in an ensemble model, particularly when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby improving the overall performance of the network. Extensive experiments conducted on the largest spatial-temporal forecasting benchmark datasets demonstrate that the proposed method outperforms 14 alternative methods. | ChinChia Michael Yeh, Yujie Fan, Xin Dai, Uday Singh Saini, Vivian Lai, Prince Osei Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang | Visa Research, Foster City, CA, USA |
|  |  [Using Self-supervised Learning Can Improve Model Fairness](https://doi.org/10.1145/3637528.3671991) |  | 0 | Self-supervised learning (SSL) has become the de facto training paradigm of large models, where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Despite demonstrating comparable performance with supervised methods, comprehensive efforts to assess SSL's impact on machine learning fairness (i.e., performing equally on different demographic breakdowns) are lacking. Hypothesizing that SSL models would learn more generic, hence less biased representations, this study explores the impact of pre-training and fine-tuning strategies on fairness. We introduce a fairness assessment framework for SSL, comprising five stages: defining dataset requirements, pre-training, fine-tuning with gradual unfreezing, assessing representation similarity conditioned on demographics, and establishing domain-specific evaluation processes. We evaluate our method's generalizability on three real-world human-centric datasets (i.e., MIMIC, MESA, and GLOBEM) by systematically comparing hundreds of SSL and fine-tuned models on various dimensions spanning from the intermediate representations to appropriate evaluation metrics. Our findings demonstrate that SSL can significantly improve model fairness, while maintaining performance on par with supervised methods-exhibiting up to a 30% increase in fairness with minimal loss in performance through self-supervision. We posit that such differences can be attributed to representation dissimilarities found between the best- and the worst-performing demographics across models-up to x13 greater for protected attributes with larger performance discrepancies between segments. Code: https://github.com/Nokia-Bell-Labs/SSLfairness | Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Athena Vakali, Daniele Quercia, Fahim Kawsar | Aristotle University of Thessaloniki, Thessaloniki, Greece; Nokia Bell Labs, Cambridge, United Kingdom |
|  |  [Self-consistent Deep Geometric Learning for Heterogeneous Multi-source Spatial Point Data Prediction](https://doi.org/10.1145/3637528.3671737) |  | 0 | Multi-source spatial point data prediction is crucial in fields like environmental monitoring and natural resource management, where integrating data from various sensors is the key to achieving a holistic environmental understanding. Existing models in this area often fall short due to their domain-specific nature and lack a strategy for integrating information from various sources in the absence of ground truth labels. Key challenges include evaluating the quality of different data sources and modeling spatial relationships among them effectively. Addressing these issues, we introduce an innovative multi-source spatial point data prediction framework that adeptly aligns information from varied sources without relying on ground truth labels. A unique aspect of our method is the 'fidelity score,' a quantitative measure for evaluating the reliability of each data source. Furthermore, we develop a geo-location-aware graph neural network tailored to accurately depict spatial relationships between data points. Our framework has been rigorously tested on two real-world datasets and one synthetic dataset. The results consistently demonstrate its superior performance over existing state-of-the-art methods. | Dazhou Yu, Xiaoyun Gong, Yun Li, Meikang Qiu, Liang Zhao | Augusta University, Augusta, GA, USA; Emory University, Atlanta, GA, USA |
|  |  [PolygonGNN: Representation Learning for Polygonal Geometries with Heterogeneous Visibility Graph](https://doi.org/10.1145/3637528.3671738) |  | 0 | Polygon representation learning is essential for diverse applications, encompassing tasks such as shape coding, building pattern classification, and geographic question answering. While recent years have seen considerable advancements in this field, much of the focus has been on single polygons, overlooking the intricate inner- and inter-polygonal relationships inherent in multipolygons. To address this gap, our study introduces a comprehensive framework specifically designed for learning representations of polygonal geometries, particularly multipolygons. Central to our approach is the incorporation of a heterogeneous visibility graph, which seamlessly integrates both inner- and inter-polygonal relationships. To enhance computational efficiency and minimize graph redundancy, we implement a heterogeneous spanning tree sampling method. Additionally, we devise a rotation-translation invariant geometric representation, ensuring broader applicability across diverse scenarios. Finally, we introduce Multipolygon-GNN, a novel model tailored to leverage the spatial and semantic heterogeneity inherent in the visibility graph. Experiments on five real-world and synthetic datasets demonstrate its ability to capture informative representations for polygonal geometries. | Dazhou Yu, Yuntong Hu, Yun Li, Liang Zhao | Emory University, Atlanta, GA, USA |
|  |  [GinAR: An End-To-End Multivariate Time Series Forecasting Model Suitable for Variable Missing](https://doi.org/10.1145/3637528.3672055) |  | 0 | Multivariate time series forecasting (MTSF) is crucial for decision-making to precisely forecast the future values/trends, based on the complex relationships identified from historical observations of multiple sequences. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have gradually become the theme of MTSF model as their powerful capability in mining spatial-temporal dependencies, but almost of them heavily rely on the assumption of historical data integrity. In reality, due to factors such as data collector failures and time-consuming repairment, it is extremely challenging to collect the whole historical observations without missing any variable. In this case, STGNNs can only utilize a subset of normal variables and easily suffer from the incorrect spatial-temporal dependency modeling issue, resulting in the degradation of their forecasting performance. To address the problem, in this paper, we propose a novel Graph Interpolation Attention Recursive Network (named GinAR) to precisely model the spatial-temporal dependencies over the limited collected data for forecasting. In GinAR, it consists of two key components, that is, interpolation attention and adaptive graph convolution to take place of the fully connected layer of simple recursive units, and thus are capable of recovering all missing variables and reconstructing the correct spatial-temporal dependencies for recursively modeling of multivariate time series data, respectively. Extensive experiments conducted on five real-world datasets demonstrate that GinAR outperforms 11 SOTA baselines, and even when 90% of variables are missing, it can still accurately predict the future values of all variables. | Chengqing Yu, Fei Wang, Zezhi Shao, Tangwen Qian, Zhao Zhang, Wei Wei, Yongjun Xu | Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China |
|  |  [RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes](https://doi.org/10.1145/3637528.3671711) |  | 0 | In the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. The former allows learners to self-direct their studies, while the latter is typically characterized by teacher-directed scenarios. Recent studies in the field of intelligent education have leveraged deep temporal models to trace the learning process, capturing the dynamics of students' knowledge states, and have achieved remarkable performance. However, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. Moreover, the reciprocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. To this end, in this paper, we propose RIGL, a unified Reciprocal model to trace knowledge states at both the individual and group levels, drawing from the Independent and Group Learning processes. Specifically, we first introduce a time frame-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. Subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. Furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. It is used to delve into the dynamic influence of individual and group interactions throughout the learning processes, which is crafted to explore the dynamic intricacies of both individual and group interactions during the learning sequences. Conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model's training. Extensive experiments on four real-world educational datasets clearly demonstrate the effectiveness of the proposed RIGL model. Our codes are available at https://github.com/LabyrinthineLeo/RIGL. | Xiaoshan Yu, Chuan Qin, Dazhong Shen, Shangshang Yang, Haiping Ma, Hengshu Zhu, Xingyi Zhang | School of Artificial Intelligence, Anhui University, Hefei, Anhui, China; Career Science Lab, BOSS Zhipin, Beijing, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Career Science Lab, BOSS Zhipin & PBC School of Finance, Tsinghua University, Beijing, China; School of Computer Science and Technology, Anhui University, Hefei, Anhui, China |
|  |  [Unveiling Privacy Vulnerabilities: Investigating the Role of Structure in Graph Data](https://doi.org/10.1145/3637528.3672013) |  | 0 | The public sharing of user information opens the door for adversaries to infer private data, leading to privacy breaches and facilitating malicious activities. While numerous studies have concentrated on privacy leakage via public user attributes, the threats associated with the exposure of user relationships, particularly through network structure, are often neglected. This study aims to fill this critical gap by advancing the understanding and protection against privacy risks emanating from network structure, moving beyond direct connections with neighbors to include the broader implications of indirect network structural patterns. To achieve this, we first investigate the problem of Graph Privacy Leakage via Structure (GPS), and introduce a novel measure, the Generalized Homophily Ratio, to quantify the various mechanisms contributing to privacy breach risks in GPS. Based on this insight, we develop a novel graph private attribute inference attack, which acts as a pivotal tool for evaluating the potential for privacy leakage through network structures under worst-case scenarios. To protect users' private data from such vulnerabilities, we propose a graph data publishing method incorporating a learnable graph sampling technique, effectively transforming the original graph into a privacy-preserving version. Extensive experiments demonstrate that our attack model poses a significant threat to user privacy, and our graph data publishing method successfully achieves the optimal privacy-utility trade-off compared to baselines. | Hanyang Yuan, Jiarong Xu, Cong Wang, Ziqi Yang, Chunping Wang, Keting Yin, Yang Yang | Zhejiang University, Hangzhou, China; Finvolution Group, Shanghai, China; Fudan University, Shanghai, China; Zhejiang University & Fudan University, Hangzhou, China; Peking University, Beijing, China |
|  |  [Graph Cross Supervised Learning via Generalized Knowledge](https://doi.org/10.1145/3637528.3671830) |  | 0 | The success of GNNs highly relies on the accurate labeling of data. Existing methods of ensuring accurate labels, such as weakly-supervised learning, mainly focus on the existing nodes in the graphs. However, in reality, new nodes always continuously emerge on dynamic graphs, with different categories and even label noises. To this end, we formulate a new problem, Graph Cross-Supervised Learning, or Graph Weak-Shot Learning, that describes the challenges of modeling new nodes with novel classes and potential label noises. To solve this problem, we propose Lipshitz-regularized Mixture-of-Experts similarity network (LIME), a novel framework to encode new nodes and handle label noises. Specifically, we first design a node similarity network to capture the knowledge from the original classes, aiming to obtain insights for the emerging novel classes. Then, to enhance the similarity network's generalization to new nodes that could have a distribution shift, we employ the Mixture-of-Experts technique to increase the generalization of knowledge learned by the similarity network. To further avoid losing generalization ability during training, we introduce the Lipschitz bound to stabilize model output and alleviate the distribution shift issue. Empirical experiments validate LIME's effectiveness: we observe a substantial enhancement of up to 11.34% in node classification accuracy compared to the backbone model when subjected to the challenges of label noise on novel classes across five benchmark datasets. The code can be accessed through https://github.com/xiangchi-yuan/Graph-Cross-Supervised-Learning. | Xiangchi Yuan, Yijun Tian, Chunhui Zhang, Yanfang Ye, Nitesh V. Chawla, Chuxu Zhang | Dartmouth College, Hanover, NH, USA; University of Notre Dame, South Bend, IN, USA; Brandeis University & Georgia Institute of Technology, Waltham, MA, USA; Brandeis University, Waltham, MA, USA |
|  |  [UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction](https://doi.org/10.1145/3637528.3671662) |  | 0 | Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergence response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging. Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data, (ii) effective pre-training to capture complex spatio-temporal relationships, (iii) spatio-temporal knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios. Extensive experiments on more than 20 spatio-temporal scenarios demonstrate UniST's efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on https://github.com/tsinghua-fib-lab/UniST. | Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, Yong Li | Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China |
|  |  [Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion](https://doi.org/10.1145/3637528.3671783) |  | 0 | Feasible solutions are crucial for Integer Programming (IP) since they can substantially speed up the solving process. In many applications, similar IP instances often exhibit similar structures and shared solution distributions, which can be potentially modeled by deep learning methods. Unfortunately, existing deep-learning-based algorithms, such as Neural Diving [21] and Predict-and-search framework [8], are limited to generating only partial feasible solutions, and they must rely on solvers like SCIP and Gurobi to complete the solutions for a given IP problem. In this paper, we propose a novel framework that generates complete feasible solutions end-to-end. Our framework leverages contrastive learning to characterize the relationship between IP instances and solutions, and learns latent embeddings for both IP instances and their solutions. Further, the framework employs diffusion models to learn the distribution of solution embeddings conditioned on IP representations, with a dedicated guided sampling strategy that accounts for both constraints and objectives. We empirically evaluate our framework on four typical datasets of IP problems, and show that it effectively generates complete feasible solutions with a high probability (> 89.7 %) without the reliance of Solvers and the quality of solutions is comparable to the best heuristic solutions from Gurobi. Furthermore, by integrating our method's sampled partial solutions with the CompleteSol heuristic from SCIP [19], the resulting feasible solutions outperform those from state-of-the-art methods across all datasets, exhibiting a 3.7 to 33.7% improvement in the gap to optimal values, and maintaining a feasible ratio of over 99.7% for all datasets. | Hao Zeng, Jiaqi Wang, Avirup Das, Junying He, Kunpeng Han, Haoyuan Hu, Mingfei Sun | University of Manchester, Manchester, United Kingdom; Cainiao Network, Hangzhou, China |
|  |  [Path-Specific Causal Reasoning for Fairness-aware Cognitive Diagnosis](https://doi.org/10.1145/3637528.3672049) |  | 0 | Cognitive Diagnosis~(CD), which leverages students and exercise data to predict students' proficiency levels on different knowledge concepts, is one of fundamental components in Intelligent Education. Due to the scarcity of student-exercise interaction data, most existing methods focus on making the best use of available data, such as exercise content and student information~(e.g., educational context). Despite the great progress, the abuse of student sensitive information has not been paid enough attention. Due to the important position of CD in Intelligent Education, employing sensitive information when making diagnosis predictions will cause serious social issues. Moreover, data-driven neural networks are easily misled by the shortcut between input data and output prediction, exacerbating this problem. Therefore, it is crucial to eliminate the negative impact of sensitive information in CD models. In response, we argue that sensitive attributes of students can also provide useful information, and only the shortcuts directly related to the sensitive information should be eliminated from the diagnosis process. Thus, we employ causal reasoning and design a novel Path-Specific Causal Reasoning Framework (PSCRF) to achieve this goal. Specifically, we first leverage an encoder to extract features and generate embeddings for general information and sensitive information of students. Then, we design a novel attribute-oriented predictor to decouple the sensitive attributes, in which fairness-related sensitive features will be eliminated and other useful information will be retained. Finally, we designed a multi-factor constraint to ensure the performance of fairness and diagnosis performance simultaneously. Extensive experiments over real-world datasets (e.g., PISA dataset) demonstrate the effectiveness of our proposed PSCRF. | Dacao Zhang, Kun Zhang, Le Wu, Mi Tian, Richang Hong, Meng Wang |  |
|  |  [Brant-X: A Unified Physiological Signal Alignment Framework](https://doi.org/10.1145/3637528.3671953) |  | 0 | Physiological signals serve as indispensable clues for understanding various physiological states of human bodies. Most existing works have focused on a single type of physiological signals for a range of application scenarios. However, as the body is a holistic biological system, the inherent interconnection among various physiological data should not be neglected. In particular, given the brain's role as the control center for vital activities, electroencephalogram (EEG) exhibits significant correlations with other physiological signals. Therefore, the correlation between EEG and other physiological signals holds potential to improve performance in various scenarios. Nevertheless, achieving this goal is still constrained by several challenges: the scarcity of simultaneously collected physiological data, the differences in correlations between various signals, and the correlation differences between various tasks. To address these issues, we propose a unified physiological signal alignment framework, Brant-X, to model the correlation between EEG and other signals. Our approach (1) employs the EEG foundation model to data-efficiently transfer the rich knowledge in EEG to other physiological signals, and (2) introduces the two-level alignment to fully align the semantics of EEG and other signals from different semantic scales. In the experiments, Brant-X achieves state-of-the-art performance compared with task-agnostic and task-specific baselines on various downstream tasks in diverse scenarios, including sleep stage classification, emotion recognition, freezing of gaits detection, and eye movement communication. Moreover, the analysis on the arrhythmia detection task and the visualization in case study further illustrate the effectiveness of Brant-X in the knowledge transfer from EEG to other physiological signals. The model's homepage is at https://github.com/zjunet/Brant-X/. | Daoze Zhang, Zhizhang Yuan, Junru Chen, Kerui Chen, Yang Yang | Zhejiang University, Hangzhou, China |
|  |  [Subspace Selection based Prompt Tuning with Nonconvex Nonsmooth Black-Box Optimization](https://doi.org/10.1145/3637528.3671986) |  | 0 | In this paper, we introduce a novel framework for black-box prompt tuning with a subspace learning and selection strategy, leveraging derivative-free optimization algorithms. This approach is crucial for scenarios where user interaction with language models is restricted to API usage, without direct access to their internal structures or gradients, a situation typical in Language-Model-as-a-Service (LMaaS). Our framework focuses on exploring the low-dimensional subspace of continuous prompts. Previous work on black-box prompt tuning necessitates a substantial number of API calls due to the random choice of the subspace. To tackle this problem, we propose to use a simple zeroth-order optimization algorithm to tackle nonconvex optimization challenges with nonsmooth nonconvex regularizers: the Zeroth-Order Mini-Batch Stochastic Proximal Gradient method (ZO-MB-SPG). A key innovation is the incorporation of nonsmooth nonconvex regularizers, including the indicator function of the l0 constraint, which enhances our ability to select optimal subspaces for prompt optimization. The experimental results show that our proposed black-box prompt tuning method on a few labeled samples can attain similar performance to the methods applicable to LMaaS with much fewer API calls. | Haozhen Zhang, Hualin Zhang, Bin Gu, Yi Chang | School of Artificial Intelligence, Jilin University, Changchun, Jilin, China; Mohamed bin Zayed University of Artificial Intelligence, Masdar, United Arab Emirates |
|  |  [Heuristic Learning with Graph Neural Networks: A Unified Framework for Link Prediction](https://doi.org/10.1145/3637528.3671946) |  | 0 | Link prediction is a fundamental task in graph learning, inherently shaped bythe topology of the graph. While traditional heuristics are grounded in graphtopology, they encounter challenges in generalizing across diverse graphs.Recent research efforts have aimed to leverage the potential of heuristics, yeta unified formulation accommodating both local and global heuristics remainsundiscovered. Drawing insights from the fact that both local and globalheuristics can be represented by adjacency matrix multiplications, we propose aunified matrix formulation to accommodate and generalize various heuristics. Wefurther propose the Heuristic Learning Graph Neural Network (HL-GNN) toefficiently implement the formulation. HL-GNN adopts intra-layer propagationand inter-layer connections, allowing it to reach a depth of around 20 layerswith lower time complexity than GCN. Extensive experiments on the Planetoid,Amazon, and OGB datasets underscore the effectiveness and efficiency of HL-GNN.It outperforms existing methods by a large margin in prediction performance.Additionally, HL-GNN is several orders of magnitude faster thanheuristic-inspired methods while requiring only a few trainable parameters. Thecase study further demonstrates that the generalized heuristics and learnedweights are highly interpretable. | Juzheng Zhang, Lanning Wei, Zhen Xu, Quanming Yao | Department of Electronic Engineering, Tsinghua University, Beijing, China |
|  |  [Asynchronous Vertical Federated Learning for Kernelized AUC Maximization](https://doi.org/10.1145/3637528.3671930) |  | 0 | Vertical Federated Learning (VFL) has garnered significant attention due to its applicability in multi-party collaborative learning and the increasing demand for privacy-preserving measures. Most existing VFL algorithms primarily focus on accuracy as the training model metric. However, the data we access is often imbalanced in the real world, making it difficult for models based on accuracy to correctly classify minority samples. The Area Under the Curve (AUC) serves as an effective metric to evaluate the performance of a model on imbalanced data. Therefore, optimizing AUC can enhance the model's ability to handle imbalanced data. Besides, computational resources within VFL systems are also imbalanced, which makes synchronous VFL algorithms are difficult to apply in the real world. To address the double imbalance issue, we propose Asynchronous Vertical Federated Kernelized AUC Maximization (AVFKAM). Specifically, AVFKAM asynchronously updates a kernel model based on triply stochastic gradients with respect to (w.r.t.) the pairwise loss and random feature approximation. To facilitate theoretical analysis, we transfer the asynchrony of model coefficients to the functional gradient through a dual relationship between coefficients and objective function. Furthermore, we demonstrate that AVFKAM converges to the optimal solution at a rate of O(1/t), where t represents the global iteration number, and discuss the security of the model. If t is denoted as the global iteration number, we provide that it converges to the optimal solution with the rate of O(1/t). Finally, experimental results on various benchmark datasets demonstrate that AVFKAM maintains high AUC performance and efficiency. | Ke Zhang, Ganyu Wang, Han Li, Yulong Wang, Hong Chen, Bin Gu | College of Informatics, Huazhong Agricultural University, Wuhan, Hubei, China; Department of Computer Science, Western University, London, Ontario, Canada |
|  |  [Multivariate Log-based Anomaly Detection for Distributed Database](https://doi.org/10.1145/3637528.3671725) |  | 0 | Distributed databases are fundamental infrastructures of today's large-scalesoftware systems such as cloud systems. Detecting anomalies in distributeddatabases is essential for maintaining software availability. Existingapproaches, predominantly developed using Loghub-a comprehensive collection oflog datasets from various systems-lack datasets specifically tailored todistributed databases, which exhibit unique anomalies. Additionally, there's anotable absence of datasets encompassing multi-anomaly, multi-node logs.Consequently, models built upon these datasets, primarily designed forstandalone systems, are inadequate for distributed databases, and the prevalentmethod of deeming an entire cluster anomalous based on irregularities in asingle node leads to a high false-positive rate. This paper addresses theunique anomalies and multivariate nature of logs in distributed databases. Weexpose the first open-sourced, comprehensive dataset with multivariate logsfrom distributed databases. Utilizing this dataset, we conduct an extensivestudy to identify multiple database anomalies and to assess the effectivenessof state-of-the-art anomaly detection using multivariate log data. Our findingsreveal that relying solely on logs from a single node is insufficient foraccurate anomaly detection on distributed database. Leveraging these insights,we propose MultiLog, an innovative multivariate log-based anomaly detectionapproach tailored for distributed databases. Our experiments, based on thisnovel dataset, demonstrate MultiLog's superiority, outperforming existingstate-of-the-art methods by approximately 12 | Lingzhe Zhang, Tong Jia, Mengxi Jia, Ying Li, Yong Yang, Zhonghai Wu | Peking University, Beijing, China |
|  |  [Logical Reasoning with Relation Network for Inductive Knowledge Graph Completion](https://doi.org/10.1145/3637528.3671911) |  | 0 | Inductive knowledge graph completion (KGC) aims to infer the missing relationfor a set of newly-coming entities that never appeared in the training set.Such a setting is more in line with reality, as real-world KGs are constantlyevolving and introducing new knowledge. Recent studies have shown promisingresults using message passing over subgraphs to embed newly-coming entities forinductive KGC. However, the inductive capability of these methods is usuallylimited by two key issues. (i) KGC always suffers from data sparsity, and thesituation is even exacerbated in inductive KGC where new entities often havefew or no connections to the original KG. (ii) Cold-start problem. It is overcoarse-grained for accurate KG reasoning to generate representations for newentities by gathering the local information from few neighbors. To this end, wepropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KGcompletion. It aims to mine latent relation patterns for inductive KGcompletion. Specifically, by centering on relations, NORAN provides a hyperview towards KG modeling, where the correlations between relations can benaturally captured as entity-independent logical evidence to conduct inductiveKGC. Extensive experiment results on five benchmarks show that our frameworksubstantially outperforms the state-of-the-art KGC methods. | Qinggang Zhang, Keyu Duan, Junnan Dong, Pai Zheng, Xiao Huang | The Hong Kong Polytechnic University, Kowloon, Hong Kong; The Hong Kong Polytechnic University, Hung Hom, Hong Kong; National University of Singapore, Singapore, Singapore |
|  |  [Towards Adaptive Neighborhood for Advancing Temporal Interaction Graph Modeling](https://doi.org/10.1145/3637528.3671877) |  | 0 | Temporal Graph Networks (TGNs) have demonstrated their remarkable performance in modeling temporal interaction graphs. These works can generate temporal node representations by encoding the surrounding neighborhoods for the target node. However, an inherent limitation of existing TGNs is their reliance onfixed, hand-crafted rules for neighborhood encoding, overlooking the necessity for an adaptive and learnable neighborhood that can accommodate both personalization and temporal evolution across different timestamps. In this paper, we aim to enhance existing TGNs by introducing anadaptive neighborhood encoding mechanism. We present SEAN (Selective Encoding for Adaptive Neighborhood), a flexible plug-and-play model that can be seamlessly integrated with existing TGNs, effectively boosting their performance. To achieve this, we decompose the adaptive neighborhood encoding process into two phases: (i) representative neighbor selection, and (ii) temporal-aware neighborhood information aggregation. Specifically, we propose the Representative Neighbor Selector component, which automatically pinpoints the most important neighbors for the target node. It offers a tailored understanding of each node's unique surrounding context, facilitating personalization. Subsequently, we propose a Temporal-aware Aggregator, which synthesizes neighborhood aggregation by selectively determining the utilization of aggregation routes and decaying the outdated information, allowing our model to adaptively leverage both the contextually significant and current information during aggregation. We conduct extensive experiments by integrating SEAN into three representative TGNs, evaluating their performance on four public datasets and one financial benchmark dataset introduced in this paper. The results demonstrate that SEAN consistently leads to performance improvements across all models, achieving SOTA performance and exceptional robustness. | Siwei Zhang, Xi Chen, Yun Xiong, Xixi Wu, Yao Zhang, Yongrui Fu, Yinglong Zhao, Jiawei Zhang | IFM Lab, Department of Computer Science, University of California, Davis, Davis, CA, USA; Ant Group, Shanghai, China |
|  |  [Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Networks](https://doi.org/10.1145/3637528.3671665) |  | 0 | Accurate traffic forecasting is crucial for the development of Intelligent Transportation Systems (ITS), playing a pivotal role in modern urban traffic management. Traditional forecasting methods, however, struggle with the irregular traffic time series resulting from adaptive traffic signal controls, presenting challenges in asynchronous spatial dependency, irregular temporal dependency, and predicting variable-length sequences. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) tailored for irregular traffic time series forecasting. Specifically, we first propose an Asynchronous Graph Diffusion Network to capture the spatial dependency between asynchronously measured traffic states regulated by adaptive traffic signals. After that, to capture the temporal dependency within irregular traffic state sequences, a personalized time encoding is devised to embed the continuous time signals. Then, we propose a Transformable Time-aware Convolution Network, which adapts meta-filters for time-aware convolution on the sequences with inconsistent temporal flow. Additionally, a Semi-Autoregressive Prediction Network, comprising a state evolution unit and a semiautoregressive predictor, is designed to predict variable-length traffic sequences effectively and efficiently. Extensive experiments on a newly established benchmark demonstrate the superiority of ASeer compared with twelve competitive baselines across six metrics. | Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Yanjie Fu, Jingbo Zhou, Yu Mei, Hui Xiong | Arizona State University, Phoenix, USA; HKUST(GZ) & HKUST, Guangzhou, China; Baidu Inc., Beijing, China; HKUST(GZ), Guangzhou, China; Baidu Research, Beijing, China; HKUST, Hong Kong, China |
|  |  [A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist](https://doi.org/10.1145/3637528.3671801) |  | 0 | Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 12 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks. | Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An | Zhejiang University, Hangzhou, China; Nanyang Technological University & Skywork AI, Singapore, Singapore; National University of Singapore, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore; National Technological University, Singapore, Singapore |
|  |  [Geometric View of Soft Decorrelation in Self-Supervised Learning](https://doi.org/10.1145/3637528.3671914) |  | 0 | Contrastive learning, a form of Self-Supervised Learning (SSL), typically consists of an alignment term and a regularization term. The alignment term minimizes the distance between the embeddings of a positive pair, while the regularization term prevents trivial solutions and expresses prior beliefs about the embeddings. As a widely used regularization technique, soft decorrelation has been employed by several non-contrastive SSL methods to avoid trivial solutions. While the decorrelation term is designed to address the issue of dimensional collapse, we find that it fails to achieve this goal theoretically and experimentally. Based on such a finding, we extend the soft decorrelation regularization to minimize the distance between the covariance matrix and an identity matrix. We provide a new perspective on the geometric distance between positive definite matrices to investigate why the soft decorrelation cannot efficiently solve the dimensional collapse. Furthermore, we construct a family of loss functions utilizing the Bregman Matrix Divergence (BMD), with the soft decorrelation representing a specific instance within this family. We prove that a loss function (LogDet) in this family can solve the issue of dimensional collapse. Our novel loss functions based on BMD exhibit superior performance compared to the soft decorrelation and other baseline techniques, as demonstrated by experimental results on graph and image datasets. | Yifei Zhang, Hao Zhu, Zixing Song, Yankai Chen, Xinyu Fu, Ziqiao Meng, Piotr Koniusz, Irwin King | Data61, CSIRO, Canberra, Australia; The Chinese University of Hong Kong, Hong Kong, China; CSIRO, Sydney, Australia |
|  |  [LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?](https://doi.org/10.1145/3637528.3671709) |  | 0 | In an era marked by the increasing adoption of Large Language Models (LLMs)for various tasks, there is a growing focus on exploring LLMs' capabilities inhandling web data, particularly graph data. Dynamic graphs, which capturetemporal network evolution patterns, are ubiquitous in real-world web data.Evaluating LLMs' competence in understanding spatial-temporal information ondynamic graphs is essential for their adoption in web applications, whichremains unexplored in the literature. In this paper, we bridge the gap viaproposing to evaluate LLMs' spatial-temporal understanding abilities on dynamicgraphs, to the best of our knowledge, for the first time. Specifically, wepropose the LLM4DyG benchmark, which includes nine specially designed tasksconsidering the capability evaluation of LLMs from both temporal and spatialdimensions. Then, we conduct extensive experiments to analyze the impacts ofdifferent data generators, data statistics, prompting techniques, and LLMs onthe model performance. Finally, we propose Disentangled Spatial-TemporalThoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporalunderstanding abilities. Our main observations are: 1) LLMs have preliminaryspatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graphtasks show increasing difficulties for LLMs as the graph size and densityincrease, while not sensitive to the time span and data generation mechanism,3) the proposed DST2 prompting method can help to improve LLMs'spatial-temporal understanding abilities on dynamic graphs for most tasks. Thedata and codes will be open-sourced at publication time. | Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu | DCST, BNRist, Tsinghua University, Beijing, China; DCST, Tsinghua University, Beijing, China |
|  |  [Representation Learning of Geometric Trees](https://doi.org/10.1145/3637528.3671688) |  | 0 | Geometric trees are characterized by their tree-structured layout and spatially constrained nodes and edges, which significantly impacts their topological attributes. This inherent hierarchical structure plays a crucial role in domains such as neuron morphology and river geomorphology, but traditional graph representation methods often overlook these specific characteristics of tree structures. To address this, we introduce a new representation learning framework tailored for geometric trees. It first features a unique message passing neural network, which is both provably geometrical structure-recoverable and rotation-translation invariant. To address the data label scarcity issue, our approach also includes two innovative training targets that reflect the hierarchical ordering and geometric structure of these geometric trees. This enables fully self-supervised learning without explicit labels. We validate our method's effectiveness on eight real-world datasets, demonstrating its capability to represent geometric trees. | Zheng Zhang, Allen Zhang, Ruth Nelson, Giorgio Ascoli, Liang Zhao | George Mason University, Fairfax, VA, USA; Yale University, New Haven, CT, USA; Emory University, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA |
|  |  [Rethinking Graph Backdoor Attacks: A Distribution-Preserving Perspective](https://doi.org/10.1145/3637528.3671910) |  | 0 | Graph Neural Networks (GNNs) have shown remarkable performance in varioustasks. However, recent works reveal that GNNs are vulnerable to backdoorattacks. Generally, backdoor attack poisons the graph by attaching backdoortriggers and the target class label to a set of nodes in the training graph. AGNN trained on the poisoned graph will then be misled to predict test nodesattached with trigger to the target class. Despite their effectiveness, ourempirical analysis shows that triggers generated by existing methods tend to beout-of-distribution (OOD), which significantly differ from the clean data.Hence, these injected triggers can be easily detected and pruned with widelyused outlier detection methods in real-world applications. Therefore, in thispaper, we study a novel problem of unnoticeable graph backdoor attacks within-distribution (ID) triggers. To generate ID triggers, we introduce an OODdetector in conjunction with an adversarial learning strategy to generate theattributes of the triggers within distribution. To ensure a high attack successrate with ID triggers, we introduce novel modules designed to enhance triggermemorization by the victim model trained on poisoned graph. Extensiveexperiments on real-world datasets demonstrate the effectiveness of theproposed method in generating in distribution triggers that can by-pass variousdefense strategies while maintaining a high attack success rate. | Zhiwei Zhang, Minhua Lin, Enyan Dai, Suhang Wang | The Pennsylvania State University, State College, PA, USA |
|  |  [Learning Flexible Time-windowed Granger Causality Integrating Heterogeneous Interventional Time Series Data](https://doi.org/10.1145/3637528.3672023) |  | 0 | Granger causality, commonly used for inferring causal structures from timeseries data, has been adopted in widespread applications across various fieldsdue to its intuitive explainability and high compatibility with emerging deepneural network prediction models. To alleviate challenges in better decipheringcausal structures unambiguously from time series, the use of interventionaldata has become a practical approach. However, existing methods have yet to beexplored in the context of imperfect interventions with unknown targets, whichare more common and often more beneficial in a wide range of real-worldapplications. Additionally, the identifiability issues of Granger causalitywith unknown interventional targets in complex network models remain unsolved.Our work presents a theoretically-grounded method that infers Granger causalstructure and identifies unknown targets by leveraging heterogeneousinterventional time series data. We further illustrate that learning Grangercausal structure and recovering interventional targets can mutually promoteeach other. Comparative experiments demonstrate that our method outperformsseveral robust baseline methods in learning Granger causal structure frominterventional time series data. | Ziyi Zhang, Shaogang Ren, Xiaoning Qian, Nick Duffield | Texas A&M University, College Station, Texas, USA; Texas A&M University & Brookhaven National Laboratory, College Station, Texas, USA |
|  |  [Algorithmic Fairness Generalization under Covariate and Dependence Shifts Simultaneously](https://doi.org/10.1145/3637528.3671909) |  | 0 | The endeavor to preserve the generalization of a fair and invariantclassifier across domains, especially in the presence of distribution shifts,becomes a significant and intricate challenge in machine learning. In responseto this challenge, numerous effective algorithms have been developed with afocus on addressing the problem of fairness-aware domain generalization. Thesealgorithms are designed to navigate various types of distribution shifts, witha particular emphasis on covariate and dependence shifts. In this context,covariate shift pertains to changes in the marginal distribution of inputfeatures, while dependence shift involves alterations in the joint distributionof the label variable and sensitive attributes. In this paper, we introduce asimple but effective approach that aims to learn a fair and invariantclassifier by simultaneously addressing both covariate and dependence shiftsacross domains. We assert the existence of an underlying transformation modelcan transform data from one domain to another, while preserving the semanticsrelated to non-sensitive attributes and classes. By augmenting varioussynthetic data domains through the model, we learn a fair and invariantclassifier in source domains. This classifier can then be generalized tounknown target domains, maintaining both model prediction and fairnessconcerns. Extensive empirical studies on four benchmark datasets demonstratethat our approach surpasses state-of-the-art methods. | Chen Zhao, Kai Jiang, Xintao Wu, Haoliang Wang, Latifur Khan, Christan Grant, Feng Chen | University of Arkansas, Fayetteville, AR, USA; The University of Texas at Dallas, Richardson, TX, USA; University of Florida, Gainesville, FL, USA; Baylor University, Waco, TX, USA; The University of Texas, Dallas, Richardson, TX, USA |
|  |  [VertiMRF: Differentially Private Vertical Federated Data Synthesis](https://doi.org/10.1145/3637528.3671771) |  | 0 | Data synthesis is a promising solution to share data for various downstream analytic tasks without exposing raw data. However, without a theoretical privacy guarantee, a synthetic dataset would still leak some sensitive information in raw data. As a countermeasure, differential privacy is widely adopted to safeguard data synthesis by strictly limiting the released information. This technique is advantageous yet presents significant challenges in the vertical federated setting, where data attributes are distributed among different data parties. The main challenge lies in maintaining privacy while efficiently and precisely reconstructing the correlation between attributes. In this paper, we propose a novel algorithm called VertiMRF, designed explicitly for generating synthetic data in the vertical setting and providing differential privacy protection for all information shared from data parties. We introduce techniques based on the Flajolet-Martin (FM) sketch for encoding local data satisfying differential privacy and estimating cross-party marginals. We provide theoretical privacy and utility proof for encoding in this multi-attribute data. Collecting the locally generated private Markov Random Field (MRF) and the sketches, a central server can reconstruct a global MRF, maintaining the most useful information. Two critical techniques introduced in our VertiMRF are dimension reduction and consistency enforcement, preventing the noise of FM sketch from overwhelming the information of attributes with large domain sizes when building the global MRF. These two techniques allow flexible and inconsistent binning strategies of local private MRF and the data sketching module, which can preserve information to the greatest extent. We conduct extensive experiments on four real-world datasets to evaluate the effectiveness of VertiMRF. End-to-end comparisons demonstrate the superiority of VertiMRF. | Fangyuan Zhao, Zitao Li, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li | Xi'an Jiaotong University, Xi'an, China; Alibaba Group, Bellevue, WA, USA |
|  |  [Pre-Training and Prompting for Few-Shot Node Classification on Text-Attributed Graphs](https://doi.org/10.1145/3637528.3671952) |  | 0 | The text-attributed graph (TAG) is one kind of important real-world graph-structured data with each node associated with raw texts. For TAGs, traditional few-shot node classification methods directly conduct training on the pre-processed node features and do not consider the raw texts. The performance is highly dependent on the choice of the feature pre-processing method. In this paper, we propose P2TAG, a framework designed for few-shot node classification on TAGs with graph pre-training and prompting. P2TAG first pre-trains the language model (LM) and graph neural network (GNN) on TAGs with self-supervised loss. To fully utilize the ability of language models, we adapt the masked language modeling objective for our framework. The pre-trained model is then used for the few-shot node classification with a mixed prompt method, which simultaneously considers both text and graph information. We conduct experiments on six real-world TAGs, including paper citation networks and product co-purchasing networks. Experimental results demonstrate that our proposed framework outperforms existing graph few-shot learning methods on these datasets with +18.98% ~ +32.14% improvements. | Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, Jie Tang | Tsinghua University, Beijing, China; Software Engineering, Tsinghua University, Beijing, China; University of Edinburgh, Edinburgh, United Kingdom; Anhui University, Hefei, Anhui, China; Zhipu AI, Beijing, China; Bosch Center for Artifcial Intelligence, Renningen, Germany |
|  |  [Conformalized Link Prediction on Graph Neural Networks](https://doi.org/10.1145/3637528.3672061) |  | 0 | Graph Neural Networks (GNNs) excel in diverse tasks, yet their applications in high-stakes domains are often hampered by unreliable predictions. Although numerous uncertainty quantification methods have been proposed to address this limitation, they often lackrigorous uncertainty estimates. This work makes the first attempt to introduce a distribution-free and model-agnostic uncertainty quantification approach to construct a predictive interval with a statistical guarantee for GNN-based link prediction. We term it asconformalized link prediction. Our approach builds upon conformal prediction (CP), a framework that promises to construct statistically robust prediction sets or intervals. There are two primary challenges: first, given dependent data like graphs, it is unclear whether the critical assumption in CP --- exchangeability --- still holds when applied to link prediction. Second, even if the exchangeability assumption is valid for conformalized link prediction, we need to ensure high efficiency, i.e., the resulting prediction set or the interval length is small enough to provide useful information. To tackle these challenges, we first theoretically and empirically establish a permutation invariance condition for the application of CP in link prediction tasks, along with an exact test-time coverage. Leveraging the important structural information in graphs, we then identify a novel and crucial connection between a graph's adherence to the power law distribution and the efficiency of CP. This insight leads to the development of a simple yet effective sampling-based method to align the graph structure with a power law distribution prior to the standard CP procedure. Extensive experiments demonstrate that for conformalized link prediction, our approach achieves the desired marginal coverage while significantly improving the efficiency of CP compared to baseline methods. | Tianyi Zhao, Jian Kang, Lu Cheng | University of Illinois Chicago, Chicago, IL, USA; University of Rochester, Rochester, NY, USA; University of Southern California, Los Angeles, CA, USA |
|  |  [GeoMix: Towards Geometry-Aware Data Augmentation](https://doi.org/10.1145/3637528.3671700) |  | 0 | Mixup has shown considerable success in mitigating the challenges posed by limited labeled data in image classification. By synthesizing samples through the interpolation of features and labels, Mixup effectively addresses the issue of data scarcity. However, it has rarely been explored in graph learning tasks due to the irregularity and connectivity of graph data. Specifically, in node classification tasks, Mixup presents a challenge in creating connections for synthetic data. In this paper, we propose Geometric Mixup (GeoMix), a simple and interpretable Mixup approach leveraging in-place graph editing. It effectively utilizes geometry information to interpolate features and labels with those from the nearby neighborhood, generating synthetic nodes and establishing connections for them. We conduct theoretical analysis to elucidate the rationale behind employing geometry information for node Mixup, emphasizing the significance of locality enhancement-a critical aspect of our method's design. Extensive experiments demonstrate that our lightweight Geometric Mixup achieves state-of-the-art results on a wide variety of standard datasets with limited labeled data. Furthermore, it significantly improves the generalization capability of underlying GNNs across various challenging out-of-distribution generalization tasks. Our code is available at https://github.com/WtaoZhao/geomix. | Wentao Zhao, Qitian Wu, Chenxiao Yang, Junchi Yan |  |
|  |  [Spuriousness-Aware Meta-Learning for Learning Robust Classifiers](https://doi.org/10.1145/3637528.3672006) |  | 0 | Spurious correlations are brittle associations between certain attributes of inputs and target variables, such as the correlation between an image background and an object class. Deep image classifiers often leverage them for predictions, leading to poor generalization on the data where the correlations do not hold. Mitigating the impact of spurious correlations is crucial towards robust model generalization, but it often requires annotations of the spurious correlations in data -- a strong assumption in practice. In this paper, we propose a novel learning framework based on meta-learning, termed SPUME -- SPUriousness-aware MEta-learning, to train an image classifier to be robust to spurious correlations. We design the framework to iteratively detect and mitigate the spurious correlations that the classifier excessively relies on for predictions. To achieve this, we first propose to utilize a pre-trained vision-language model to extract text-format attributes from images. These attributes enable us to curate data with various class-attribute correlations, and we formulate a novel metric to measure the degree of these correlations' spuriousness. Then, to mitigate the reliance on spurious correlations, we propose a meta-learning strategy in which the support (training) sets and query (test) sets in tasks are curated with different spurious correlations that have high degrees of spuriousness. By meta-training the classifier on these spuriousness-aware meta-learning tasks, our classifier can learn to be invariant to the spurious correlations. We demonstrate that our method is robust to spurious correlations without knowing them a priori and achieves the best on five benchmark datasets with different robustness measures. Our code is available at https://github.com/gtzheng/SPUME. | Guangtao Zheng, Wenqian Ye, Aidong Zhang | University of Virginia, Charlottesville, VA, USA |
|  |  [SiGeo: Sub-One-Shot NAS via Geometry of Loss Landscape](https://doi.org/10.1145/3637528.3671712) |  | 0 | Neural Architecture Search (NAS) has become a widely used tool for automating neural network design. While one-shot NAS methods have successfully reduced computational requirements, they often require extensive training. On the other hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate architecture's test performance but has two limitations: (1) inability to use the information gained as a network improves with training and (2) unreliable performance, particularly in complex domains like RecSys, due to the multi-modal data inputs and complex architecture configurations. To synthesize the benefits of both methods, we introduce a "sub-one-shot" paradigm that serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is trained using only a small subset of the training data, a phase we refer to as "warm-up." Within this framework, we present SiGeo, a proxy founded on a novel theoretical framework that connects the supernet warm-up with the efficacy of the proxy. Extensive experiments have consistently shown that SiGeo, when properly warmed up, surpasses state-of-the-art NAS proxies in many established NAS benchmarks in the computer vision domain. Furthermore, when tested on recommendation system benchmarks, SiGeo demonstrates its ability to match the performance of state-of-the-art weight-sharing one-shot NAS methods while significantly reducing computational costs by approximately 60%. | Hua Zheng, KuangHung Liu, Igor Fedorov, Xin Zhang, WenYen Chen, Wei Wen | Meta, Menlo Park, CA, USA; Northeastern University, Boston, MA, USA |
|  |  [Relaxing Continuous Constraints of Equivariant Graph Neural Networks for Broad Physical Dynamics Learning](https://doi.org/10.1145/3637528.3671957) |  | 0 | Incorporating Euclidean symmetries (e.g. rotation equivariance) as inductive biases into graph neural networks has improved their generalization ability and data efficiency in unbounded physical dynamics modeling. However, in various scientific and engineering applications, the symmetries of dynamics are frequently discrete due to the boundary conditions. Thus, existing GNNs either overlook necessary symmetry, resulting in suboptimal representation ability, or impose excessive equivariance, which fails to generalize to unobserved symmetric dynamics. In this work, we propose a general Discrete Equivariant Graph Neural Network (DEGNN) that guarantees equivariance to a given discrete point group. Specifically, we show that such discrete equivariant message passing could be constructed by transforming geometric features into permutation-invariant embeddings. Through relaxing continuous equivariant constraints, DEGNN can employ more geometric feature combinations to approximate unobserved physical object interaction functions. Two implementation approaches of DEGNN are proposed based on ranking or pooling permutation-invariant functions. We apply DEGNN to various physical dynamics, ranging from particle, molecular, crowd to vehicle dynamics. In twenty scenarios, DEGNN significantly outperforms existing state-of-the-art approaches. Moreover, we show that DEGNN is data efficient, learning with less data, and can generalize across scenarios such as unobserved orientation. | Zinan Zheng, Yang Liu, Jia Li, Jianhua Yao, Yu Rong | Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; Tencent AI Lab, Shenzhen, China |
|  |  [LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models](https://doi.org/10.1145/3637528.3671810) |  | 0 | Logs are ubiquitous digital footprints, playing an indispensable role in system diagnostics, security analysis, and performance optimization. The extraction of actionable insights from logs is critically dependent on the log parsing process, which converts raw logs into structured formats for downstream analysis. Yet, the complexities of contemporary systems and the dynamic nature of logs pose significant challenges to existing automatic parsing techniques. The emergence of Large Language Models (LLM) offers new horizons. With their expansive knowledge and contextual prowess, LLMs have been transformative across diverse applications. Building on this, we introduce LogParser-LLM, a novel log parser integrated with LLM capabilities. This union seamlessly blends semantic insights with statistical nuances, obviating the need for hyper-parameter tuning and labeled training data, while ensuring rapid adaptability through online parsing. Further deepening our exploration, we address the intricate challenge of parsing granularity, proposing a new metric and integrating human interactions to allow users to calibrate granularity to their specific needs. Our method's efficacy is empirically demonstrated through evaluations on the Loghub-2k and the large-scale LogPub benchmark. In evaluations on the LogPub benchmark, involving an average of 3.6 million logs per dataset across 14 datasets, our LogParser-LLM requires only 272.5 LLM invocations on average, achieving a 90.6% F1 score for grouping accuracy and an 81.1% for parsing accuracy. These results demonstrate the method's high efficiency and accuracy, outperforming current state-of-the-art log parsers, including pattern-based, neural network-based, and existing LLM-enhanced approaches. | Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li, Qingsong Wen | Alibaba Group, Hangzhou, China; Harvard University & Alibaba Group, Cambridge, MA, USA; CAMCA, Harvard Medical School, Massachusetts General Hospital, Boston, MA, USA; Alibaba Group, Bellevue, WA, USA |
|  |  [BitLINK: Temporal Linkage of Address Clusters in Bitcoin Blockchain](https://doi.org/10.1145/3637528.3672037) |  | 0 | In the Bitcoin blockchain, an entity (e.g., a gambling service) may control multiple distinct address clusters. Links (i.e., trust relationships) between these disjoint address clusters can be established when one cluster is abandoned, and a new one is formed shortly thereafter. To link the clusters across time, we have developed a deep neural network model that exploits these synchronous actions derived from unlabeled data in a self-supervised manner. This model assesses whether two clusters exhibit synchronous temporal signatures indicative of a shared entity ownership. We validated our model on 26 real-world entities identified by WalletExplorer [36]. In addition to the existing knowledge, our analysis revealed more transaction history by linking address clusters for three major services: HelixMixer, Primedice, and Bitcoin Fog, as well as 60 other services. This enables us to address questions related to the revenue and expenditures of these services and create informative aggregate statistics. Readers can find code and data on our support website: http://www.bitlinkwallet.com. | Sheng Zhong, Abdullah Mueen | The University of New Mexico, Albuquerque, NM, USA |
|  |  [Efficient and Effective Implicit Dynamic Graph Neural Network](https://doi.org/10.1145/3637528.3672026) |  | 0 | Implicit graph neural networks have gained popularity in recent years as they capture long-range dependencies while improving predictive performance in static graphs. Despite the tussle between performance degradation due to the oversmoothing of learned embeddings and long-range dependency being more pronounced in dynamic graphs, as features are aggregated both across neighborhood and time, no prior work has proposed an implicit graph neural model in a dynamic setting. In this paper, we present Implicit Dynamic Graph Neural Network (IDGNN) a novel implicit neural network for dynamic graphs which is the first of its kind. A key characteristic of IDGNN is that it demonstrably is well-posed, i.e., it is theoretically guaranteed to have a fixed-point representation. We then demonstrate that the standard iterative algorithm often used to train implicit models is computationally expensive in our dynamic setting as it involves computing gradients, which themselves have to be estimated in an iterative manner. To overcome this, we pose an equivalent bilevel optimization problem and propose an efficient single-loop training algorithm that avoids iterative computation by maintaining moving averages of key components of the gradients. We conduct extensive experiments on real-world datasets on both classification and regression tasks to demonstrate the superiority of our approach over state-of-the-art baselines. We also demonstrate that our bi-level optimization framework maintains the performance of the expensive iterative algorithm while obtaining up to 1600x speed-up. | Yongjian Zhong, Hieu Vu, Tianbao Yang, Bijaya Adhikari | Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA; Department of Computer Science, University of Iowa, Iowa City, IA, USA |
|  |  [CURLS: Causal Rule Learning for Subgroups with Significant Treatment Effect](https://doi.org/10.1145/3637528.3671951) |  | 0 | In causal inference, estimating heterogeneous treatment effects (HTE) is critical for identifying how different subgroups respond to interventions, with broad applications in fields such as precision medicine and personalized advertising. Although HTE estimation methods aim to improve accuracy, how to provide explicit subgroup descriptions remains unclear, hindering data interpretation and strategic intervention management. In this paper, we propose CURLS, a novel rule learning method leveraging HTE, which can effectively describe subgroups with significant treatment effects. Specifically, we frame causal rule learning as a discrete optimization problem, finely balancing treatment effect with variance and considering the rule interpretability. We design an iterative procedure based on the minorize-maximization algorithm and solve a submodular lower bound as an approximation for the original. Quantitative experiments and qualitative case studies verify that compared with state-of-the-art methods, CURLS can find subgroups where the estimated and true effects are 16.1% and 13.8% higher and the variance is 12.0% smaller, while maintaining similar or better estimation accuracy and rule interpretability. Code is available at https://osf.io/zwp2k/. | Jiehui Zhou, Linxiao Yang, Xingyu Liu, Xinyue Gu, Liang Sun, Wei Chen | State Key Lab of CAD&CG, Zhejiang University, Hangzhou, Zhejiang, China; DAMO Academy, Alibaba Group, Hangzhou, Zhejiang, China |
|  |  [Neural Collapse Anchored Prompt Tuning for Generalizable Vision-Language Models](https://doi.org/10.1145/3637528.3671690) |  | 0 | Large-scale vision-language (V-L) models have demonstrated remarkable generalization capabilities for downstream tasks through prompt tuning. However, the mechanisms behind the learned text representations are unknown, limiting further generalization gains, and the limitations are more severe when faced with the prevalent class imbalances seen in web-sourced datasets. Recent advances in the neural collapse (NC) phenomenon of vision-only models suggest that the optimal representation structure is the simplex ETF, which paves the way to study representations in V-L models. In this paper, we make the first attempt to use NC for examining the representations in V-L models via prompt tuning. It is found that NC optimality of text-to-image representations shows a positive correlation with downstream generalizability, which is more severe under class imbalance settings. To improve the representations, we propose Neural-collapse-anchored Prompt Tuning (NPT), a novel method that learns prompts with text and image representations that satisfy the same simplex Equiangular Tight Frame (ETF). NPT incorporates two regularization terms: language-modality collapse and multi-modality isomorphism; and it is compatible with other prompt tuning methods. Extensive experiments show that NPT can consistently help to improve existing prompt tuning techniques across 11 datasets for both balanced and imbalanced settings. | Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Kun Kuang, Chao Wu | Zhejiang University, Hangzhou, China; Tsinghua University, Beijing, China |
|  |  [Distributed Thresholded Counting with Limited Interaction](https://doi.org/10.1145/3637528.3671868) |  | 0 | Problems in the area of distributed computing have been extensively studied. In this paper, we focus on the Distributed Thresholded Counting problem in the coordinator model. In this problem, we have k sites holding their input and communicating with a central coordinator. The coordinator's task is to determine whether the sum of inputs is larger than a threshold. While the communication complexity of this basic problem has been studied for decades, it is still not well understood. Our work considers the worst-case communication cost for an algorithm that uses limited interaction - i.e. a bounded number of rounds of communication. Algorithms in previous research usually need O(łogłog N) or O(k) rounds. In comparison, in the deterministic case, our algorithm achieves optimal communication complexity in only α(k) rounds, where α(k) denotes the inverse Ackermann function and is nearly constant. We also give a randomized algorithm that balances communication, rounds, and error probability. | Xiaoyi Zhu, Yuxiang Tian, Zengfeng Huang | School of Data Science, Fudan University, Shanghai, China |
|  |  [Propagation Structure-Aware Graph Transformer for Robust and Interpretable Fake News Detection](https://doi.org/10.1145/3637528.3672024) |  | 0 | The rise of social media has intensified fake news risks, prompting a growing focus on leveraging graph learning methods such as graph neural networks (GNNs) to understand post-spread patterns of news. However, existing methods often produce less robust and interpretable results as they assume that all information within the propagation graph is relevant to the news item, without adequately eliminating noise from engaged users. Furthermore, they inadequately capture intricate patterns inherent in long-sequence dependencies of news propagation due to their use of shallow GNNs aimed at avoiding the over-smoothing issue, consequently diminishing their overall accuracy. In this paper, we address these issues by proposing the Propagation Structure-aware Graph Transformer (PSGT). Specifically, to filter out noise from users within propagation graphs, PSGT first designs a noise-reduction self-attention mechanism based on the information bottleneck principle, aiming to minimize or completely remove the noise attention links among task-irrelevant users. Moreover, to capture multi-scale propagation structures while considering long-sequence features, we present a novel relational propagation graph as a position encoding for the graph Transformer, enabling the model to capture both propagation depth and distance relationships of users. Extensive experiments demonstrate the effectiveness, interpretability, and robustness of our PSGT. | Junyou Zhu, Chao Gao, Ze Yin, Xianghua Li, Jürgen Kurths | College of Computer Science and Electronic Engineering, Hunan University, Changsha, China |
|  |  [ControlTraj: Controllable Trajectory Generation with Topology-Constrained Diffusion Model](https://doi.org/10.1145/3637528.3671866) |  | 0 | Generating trajectory data is among promising solutions to addressing privacyconcerns, collection costs, and proprietary restrictions usually associatedwith human mobility analyses. However, existing trajectory generation methodsare still in their infancy due to the inherent diversity and unpredictabilityof human activities, grappling with issues such as fidelity, flexibility, andgeneralizability. To overcome these obstacles, we propose ControlTraj, aControllable Trajectory generation framework with the topology-constraineddiffusion model. Distinct from prior approaches, ControlTraj utilizes adiffusion model to generate high-fidelity trajectories while integrating thestructural constraints of road network topology to guide the geographicaloutcomes. Specifically, we develop a novel road segment autoencoder to extractfine-grained road segment embedding. The encoded features, along with tripattributes, are subsequently merged into the proposed geographic denoising UNetarchitecture, named GeoUNet, to synthesize geographic trajectories from whitenoise. Through experimentation across three real-world data settings,ControlTraj demonstrates its ability to produce human-directed, high-fidelitytrajectory generation with adaptability to unexplored geographical contexts. | Yuanshao Zhu, James Jian Qiao Yu, Xiangyu Zhao, Qidong Liu, Yongchao Ye, Wei Chen, Zijian Zhang, Xuetao Wei, Yuxuan Liang | City University of Hong Kong, Hong Kong, China; Jilin University & City University of Hong Kong, Jilin, China; University of York, York, United Kingdom; Southern University of Science and Technology, Shenzhen, China; The Hong Kong University of Science and Technology (Guangzhou), Guanzhou, China; Southern University of Science and Technology & City University of Hong Kong, Shenzhen, China; Xi'an Jiao Tong University & City University of Hong Kong, Xi'an, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes](https://doi.org/10.1145/3637528.3672029) |  | 0 | Recent studies have highlighted fairness issues in Graph Neural Networks(GNNs), where they produce discriminatory predictions against specificprotected groups categorized by sensitive attributes such as race and age.While various efforts to enhance GNN fairness have made significant progress,these approaches are often tailored to specific sensitive attributes.Consequently, they necessitate retraining the model from scratch to accommodatechanges in the sensitive attribute requirement, resulting in high computationalcosts. To gain deeper insights into this issue, we approach the graph fairnessproblem from a causal modeling perspective, where we identify the confoundingeffect induced by the sensitive attribute as the underlying reason. Motivatedby this observation, we formulate the fairness problem in graphs from aninvariant learning perspective, which aims to learn invariant representationsacross environments. Accordingly, we propose a graph fairness framework basedon invariant learning, namely FairINV, which enables the training of fair GNNsto accommodate various sensitive attributes within a single training session.Specifically, FairINV incorporates sensitive attribute partition and trainsfair GNNs by eliminating spurious correlations between the label and varioussensitive attributes. Experimental results on several real-world datasetsdemonstrate that FairINV significantly outperforms state-of-the-art fairnessapproaches, underscoring its effectiveness. Our code is available via:https://github.com/ZzoomD/FairINV/. | Yuchang Zhu, Jintang Li, Yatao Bian, Zibin Zheng, Liang Chen | Sun Yat-sen University, Guangzhou, China; Tencent AI Lab, Shenzhen, China |
|  |  [Topology-monitorable Contrastive Learning on Dynamic Graphs](https://doi.org/10.1145/3637528.3671777) |  | 0 | Graph contrastive learning is a representative self-supervised graph learning that has demonstrated excellent performance in learning node representations. Despite the extensive studies on graph contrastive learning models, most existing models are tailored to static graphs, hindering their application to real-world graphs which are often dynamically evolving. Directly applying these models to dynamic graphs brings in severe efficiency issues in repetitively updating the learned embeddings. To address this challenge, we propose IDOL, a novel contrastive learning framework for dynamic graph representation learning. IDOL conducts the graph propagation process based on a specially designed Personalized PageRank algorithm which can capture the topological changes incrementally. This effectively eliminates heavy recomputation while maintaining high learning quality. Our another main design is a topology-monitorable sampling strategy which lays the foundation of graph contrastive learning. We further show that the design in IDOL achieves a desired performance guarantee. Our experimental results on multiple dynamic graphs show that IDOL outperforms the strongest baselines on node classification tasks in various performance metrics. | Zulun Zhu, Kai Wang, Haoyu Liu, Jintang Li, Siqiang Luo | Nanyang Technological University, Singapore, Singapore; Sun Yat-Sen University, Guangzhou, China |
|  |  [MacroHFT: Memory Augmented Context-aware Reinforcement Learning On High Frequency Trading](https://doi.org/10.1145/3637528.3672064) |  | 0 | High-frequency trading (HFT) that executes algorithmic trading in short time scales, has recently occupied the majority of cryptocurrency market. Besides traditional quantitative trading methods, reinforcement learning (RL) has become another appealing approach for HFT due to its terrific ability of handling high-dimensional financial data and solving sophisticated sequential decision-making problems, e.g., hierarchical reinforcement learning (HRL) has shown its promising performance on second-level HFT by training a router to select only one sub-agent from the agent pool to execute the current transaction. However, existing RL methods for HFT still have some defects: 1) standard RL-based trading agents suffer from the overfitting issue, preventing them from making effective policy adjustments based on financial context; 2) due to the rapid changes in market conditions, investment decisions made by an individual agent are usually one-sided and highly biased, which might lead to significant loss in extreme markets. To tackle these problems, we propose a novel Memory Augmented Context-aware Reinforcement learning method On HFT, a.k.a. MacroHFT, which consists of two training phases: 1) we first train multiple types of sub-agents with the market data decomposed according to various financial indicators, specifically market trend and volatility, where each agent owns a conditional adapter to adjust its trading policy according to market conditions; 2) then we train a hyper-agent to mix the decisions from these sub-agents and output a consistently profitable meta-policy to handle rapid market fluctuations, equipped with a memory mechanism to enhance the capability of decision-making. Extensive experiments on various cryptocurrency markets demonstrate that MacroHFT can achieve state-of-the-art performance on minute-level trading tasks. Code has been released in https://github.com/ZONG0004/MacroHFT. | Chuqiao Zong, Chaojie Wang, Molei Qin, Lei Feng, Xinrun Wang, Bo An | Singapore University of Technology and Design, Singapore, Singapore; Skywork AI, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore |
|  |  [Lessons Learned while Running ML Models in Harsh Environments](https://doi.org/10.1145/3637528.3672499) |  | 0 | Once a very large payment processor client told us: 'if we are down for 5 minutes, we open the evening news - so don't screw up'. Processing billions of dollars per day, many financial institutions, need to continuously fight organized crime in the form of transaction fraud, stolen cards, anti-money laundering, account opening fraud, impersonations scams, phishing, and many other exotic and ever changing attacks from organized crime groups worldwide. In fact, it is estimated that in 2023 the global losses in fraud scams and bank fraud reached 485.6 billion. However, in addition to having very good detection rates and very low false positive rates, financial institutions also need to maintain very high availability rates, very low latencies, very high throughputs, automatic fault tolerance, auto scale up and down, and more. In this talk we cover some lessons related to running ML models in harsh, mission critical environments. We describe data issues, scale issues, ethical issues, system issues, security issues, compliance issues, business and regulation issues, and some architectural tradeoffs and architectural evolutions. | Pedro Bizarro | Research, Feedzai, Lisboa, Portugal |
|  |  [Next-generation Intelligent Assistants for Wearable Devices](https://doi.org/10.1145/3637528.3672500) |  | 0 | Multifunctional wearable electronic devices based on natural materials are highly desirable for versatile applications of energy conversion, electronic skin and artificial intelligence. Herein, multifunctional wearable silver nanowire decorated leather (AgNW/leather) nanocomposites with hierarchical structures for integrated visual Joule heating, electromagnetic interference (EMI) shielding and piezoresistive sensing are fabricated via the facile vacuum-assisted filtration process. The AgNWs penetrate the micro-nanoporous structures in the corium side of leather constructing highly-efficient conductive networks. The resultant flexible and mechanically strong AgNW/leather nanocomposites exhibit extremely low sheet resistance of 0.8 omega/sq, superior visual Joule heating temperatures up to 108 degrees C at low supplied voltage of 2.0 V due to efficient energy conversion, excellent EMI shielding effectiveness (EMI SE) of approximate to 55 dB and outstanding piezoresistive sensing ability in human motion detection. This work demonstrates the fabrication of multifunctional AgNW/leather nanocomposites for next-generation wearable electronic devices in energy conversion, electronic skin and artificial intelligence, etc. | Xin Luna Dong | Shaanxi Univ Sci & Technol, Coll Chem & Chem Engn, Key Lab Auxiliary Chem & Technol Chem Ind, Minist Educ,Shaanxi Key Lab Chem Addit Ind, Xian 710021, Shaanxi, Peoples R China; Northwestern Polytech Univ, Sch Chem & Chem Engn, Shaanxi Key Lab Macromol Sci & Technol, Xian 710072, Shaanxi, Peoples R China |
|  |  [Scalable Graph Learning for your Enterprise](https://doi.org/10.1145/3637528.3672501) |  | 0 | Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. At Kumo.ai we have worked with researchers worldwide to develop an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables [1]. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Our relational deep learning method to encode graph structure into low-dimensional embeddings brings several benefits: (1) automatic learning from the entire data spread across multiple relational tables (2) no manual feature engineering as the system learns optimal embeddings for a target problem; (3) state-of-the-art predictive performance. | Hema Raghavan | Kumo.AI., Inc., Mountain View, CA, USA |
|  |  [Dynamic Pricing for Multi-Retailer Delivery Platforms with Additive Deep Learning and Evolutionary Optimization](https://doi.org/10.1145/3637528.3671634) |  | 0 | Dynamic Pricing for online retail has been discussed extensively in literature. However, past solutions fell short of addressing the unique challenges of independent multi-retailer platforms for grocery delivery. From limited visibility of retailers' inventories to diverse demand-side dynamics across retail brands and locations, the highly decentralized nature of multi-retailer platforms deviates from the classical framework of modeling price elasticity and cross-elasticity of demand. In this paper, we present a novel scheme to scalable and practical price adjustment in the highly dynamic multi-retailer context. First, we present a deep learning framework to distinctly model complex cross-elasticity relationships via additive neural networks augmented with adversarial data. Second, we present evolutionary optimization agents for adjusting itemized prices in a location-decentralized manner, while adhering to custom business constraints and objectives. The optimization utilizes the genetic algorithm structure, where we introduce a potential mechanism, inspired by bandit algorithms, in order to improve convergence speed by managing exploitation and exploration trade-offs. Our solution is deployed at Shipt and is extendable to other types of multi-retailer platforms, such as restaurant delivery. Finally, we empirically demonstrate performance using public and industry datasets of hundreds and thousands of diverse products across tens of stores, offering an optimization targets coverage scale in the tens of thousands, far larger than experimental setups in past research. | Ahmed Abdulaal, Ali Polat, Hari Narayan, Wenrong Zeng, Yimin Yi | Walmart Global Tech, Sunnyvale, California, USA; Shipt, San Francisco, California, USA |
|  |  [Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale](https://doi.org/10.1145/3637528.3671532) |  | 0 | In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimodal essence of these debates [27]. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of a prime-time television debate show in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization [18]. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. To catalyze further research in this area, we also release the code, dataset collected and supplemental pdf.1 | Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Ponnurangam Kumaraguru, Kiran Garimella | Rutgers University, New Brunswick, USA; International Institute of Information Technology, Hyderabad, India; Indraprastha Institute of Information Technology, Delhi, India |
|  |  [Large Scale Generative AI Text Applied to Sports and Music](https://doi.org/10.1145/3637528.3671542) |  | 0 | We address the problem of scaling up the production of media content,including commentary and personalized news stories, for large-scale sports andmusic events worldwide. Our approach relies on generative AI models totransform a large volume of multimodal data (e.g., videos, articles, real-timescoring feeds, statistics, and fact sheets) into coherent and fluent text.Based on this approach, we introduce, for the first time, an AI commentarysystem, which was deployed to produce automated narrations for highlightpackages at the 2023 US Open, Wimbledon, and Masters tournaments. In the samevein, our solution was extended to create personalized content for ESPN FantasyFootball and stories about music artists for the Grammy awards. Theseapplications were built using a common software architecture achieved a 15xspeed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Ourwork was successfully deployed at the aforementioned events, supporting 90million fans around the world with 8 billion page views, continuously pushingthe bounds on what is possible at the intersection of sports, entertainment,and AI. | Aaron K. Baughman, Eduardo Morales, Rahul Agarwal, Gozde Akay, Rogério Feris, Tony Johnson, Stephen Hammer, Leonid Karlinsky | IBM, Fredericton, NB, Canada; IBM, Atlanta, GA, USA; IBM, Boston, MA, USA; IBM, New York, NY, USA; IBM, RTP, NC, USA |
|  |  [LiGNN: Graph Neural Networks at LinkedIn](https://doi.org/10.1145/3637528.3671566) |  | 0 | In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active user lift from people recommendation. We believe that this work can provide practical solutions and insights for engineers who are interested in applying Graph neural networks at large scale. | Fedor Borisyuk, Shihai He, Yunbo Ouyang, Morteza Ramezani, Peng Du, Xiaochen Hou, Chengming Jiang, Nitin Pasumarthy, Priya Bannur, Birjodh Tiwana, Ping Liu, Siddharth Dangi, Daqi Sun, Zhoutao Pei, Xiao Shi, Sirou Zhu, Qianqi Shen, KuangHsuan Lee, David Stein, Baolei Li, Haichao Wei, Amol Ghoting, Souvik Ghosh | LinkedIn, Mountain View, CA, USA |
|  |  [Diffusion Model-based Mobile Traffic Generation with Open Data for Network Planning and Optimization](https://doi.org/10.1145/3637528.3671544) |  | 0 | With the rapid development of the Fifth Generation Mobile Communication Technology (5G) networks, network planning and optimization have become increasingly crucial. Generating high-fidelity network traffic data can preemptively estimate the network demands of mobile users, which holds potential for network operators to improve network performance. However, the data required by existing generation methods is predominantly inaccessible to the public, resulting in a lack of reproducibility for the models and high deployment costs in practice. In this article, we propose an Open data-based Diffusion model for mobile traffic generation (OpenDiff), where a multi-positive contrastive learning algorithm is designed to construct conditional information for the diffusion model using entirely publicly available satellite remote sensing images, Point of Interest (POI), and population data. The conditional information contains relevant human activities in geographical areas, which can effectively guide the generation of network traffic data. We further design an attention-based fusion mechanism to capture the implicit correlations between network traffic and human activity features, enhancing the model's controllable generation capability. We conduct evaluations on three different cities with varying scales, where experimental results verify that our proposed model outperforms existing methods by 14.36% and 13.05% in terms of generation fidelity and controllability. To further validate the effectiveness of the model, we leverage the generated traffic data to assist the operators with network planning on a real-world network optimization platform of China Mobile Communications Corporation. The source code is available online:https://github.com/impchai/OpenDiff-diffusion-model-with-open-data. | Haoye Chai, Tao Jiang, Li Yu | Chinamobile Research Institute, Beijing, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China |
|  |  [RareBench: Can LLMs Serve as Rare Diseases Specialists?](https://doi.org/10.1145/3637528.3671576) |  | 0 | Generalist Large Language Models (LLMs), such as GPT-4, have shownconsiderable promise in various domains, including medical diagnosis. Rarediseases, affecting approximately 300 million people worldwide, often haveunsatisfactory clinical diagnosis rates primarily due to a lack of experiencedphysicians and the complexity of differentiating among many rare diseases. Inthis context, recent news such as "ChatGPT correctly diagnosed a 4-year-old'srare disease after 17 doctors failed" underscore LLMs' potential, yetunderexplored, role in clinically diagnosing rare diseases. To bridge thisresearch gap, we introduce RareBench, a pioneering benchmark designed tosystematically evaluate the capabilities of LLMs on 4 critical dimensionswithin the realm of rare diseases. Meanwhile, we have compiled the largestopen-source dataset on rare disease patients, establishing a benchmark forfuture studies in this domain. To facilitate differential diagnosis of rarediseases, we develop a dynamic few-shot prompt methodology, leveraging acomprehensive rare disease knowledge graph synthesized from multiple knowledgebases, significantly enhancing LLMs' diagnostic performance. Moreover, wepresent an exhaustive comparative study of GPT-4's diagnostic capabilitiesagainst those of specialist physicians. Our experimental findings underscorethe promising potential of integrating LLMs into the clinical diagnosticprocess for rare diseases. This paves the way for exciting possibilities infuture advancements in this field. | Xuanzhong Chen, Xiaohao Mao, Qihan Guo, Lun Wang, Shuyang Zhang, Ting Chen | Tsinghua University, Beijing, China; Chinese Academy of Medical Sciences & Peking Union Medical College, Beijing, China |
|  |  [MARLP: Time-series Forecasting Control for Agricultural Managed Aquifer Recharge](https://doi.org/10.1145/3637528.3671533) |  | 0 | The rapid decline in groundwater around the world poses a significant challenge to sustainable agriculture. To address this issue, agricultural managed aquifer recharge (Ag-MAR) is proposed to recharge the aquifer by artificially flooding agricultural lands using surface water. Ag-MAR requires a carefully selected flooding schedule to avoid affecting the oxygen absorption of crop roots. However, current Ag-MAR scheduling does not take into account complex environmental factors such as weather and soil oxygen, resulting in crop damage and insufficient recharging amounts. This paper proposes MARLP, the first end-to-end data-driven control system for Ag-MAR. We first formulate Ag-MAR as an optimization problem. To that end, we analyze four-year in-field datasets, which reveal the multi-periodicity feature of the soil oxygen level trends and the opportunity to use external weather forecasts and flooding proposals as exogenous clues for soil oxygen prediction. Then, we design a two-stage forecasting framework. In the first stage, it extracts both the cross-variate dependency and the periodic patterns from historical data to conduct preliminary forecasting. In the second stage, it uses weather-soil and flooding-soil causality to facilitate an accurate prediction of soil oxygen levels. Finally, we conduct model predictive control (MPC) for Ag-MAR flooding. To address the challenge of large action spaces, we devise a heuristic planning module to reduce the number of flooding proposals to enable the search for optimal solutions. Real-world experiments show that MARLP reduces the oxygen deficit ratio by 86.8% while improving the recharging amount in unit time by 35.8%, compared with the previous four years. | Yuning Chen, Kang Yang, Zhiyu An, Brady Holder, Luke Paloutzian, Khaled M. Bali, Wan Du | University of California, Agriculture and Natural Resources, Parlier, CA, USA; University of California, Merced, Merced, CA, USA |
|  |  [Time-Aware Attention-Based Transformer (TAAT) for Cloud Computing System Failure Prediction](https://doi.org/10.1145/3637528.3671547) |  | 0 | Log-based failure prediction helps identify and mitigate system failures ahead of time, increasing the reliability of cloud elastic computing systems. However, most existing log-based failure prediction approaches only focus on semantic information, and do not make full use of the information contained in the timestamps of log messages. This paper proposes time-aware attention-based transformer (TAAT), a failure prediction approach that extracts semantic and temporal information simultaneously from log messages and their timestamps. TAAT first tokenizes raw log messages into specific exceptions, and then performs: 1) exception sequence embedding that reorganizes the exceptions of each node as an ordered sequence and converts them to vectors; 2) time relation estimation that computes time relation matrices from the timestamps; and, 3) time-aware attention that computes semantic correlation matrices from the exception sequences and then combines them with time relation matrices. Experiments on Alibaba Cloud demonstrated that TAAT achieves an approximately 10% performance improvement compared with the state-of-the-art approaches. TAAT is now used in the daily operation of Alibaba Cloud. Moreover, this paper also releases the real-world cloud computing failure prediction dataset used in our study, which consists of about 2.7 billion syslogs from about 300,000 node controllers during a 4-month period. To our knowledge, this is the largest dataset of its kind, and is expected to be very useful to the community. | Lingfei Deng, Yunong Wang, Haoran Wang, Xuhua Ma, Xiaoming Du, Xudong Zheng, Dongrui Wu | Alibaba Cloud, Alibaba Group, Hangzhou, China; Huazhong University of Science and Technology, Wuhan, China |
|  |  [FNSPID: A Comprehensive Financial News Dataset in Time Series](https://doi.org/10.1145/3637528.3671629) |  | 0 | Financial market predictions utilize historical data to anticipate future stock prices and market trends. Traditionally, these predictions have focused on the statistical analysis of quantitative factors, such as stock prices, trading volumes, inflation rates, and changes in industrial production. Recent advancements in large language models motivate the integrated financial analysis of both sentiment data, particularly market news, and numerical factors. Nonetheless, this methodology frequently encounters constraints due to the paucity of extensive datasets that amalgamate both quantitative and qualitative sentiment analyses. To address this challenge, we introduce a large-scale financial dataset, namely, Financial News and Stock Price Integration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7 million time-aligned financial news records for 4,775 S&P500 companies, covering the period from 1999 to 2023, sourced from 4 stock market news websites. We demonstrate that FNSPID excels existing stock market datasets in scale and diversity while uniquely incorporating sentiment information. Through financial analysis experiments on FNSPID, we propose: (1) the dataset's size and quality significantly boost market prediction accuracy; (2) adding sentiment scores modestly enhances performance on the transformer-based model; (3) a reproducible procedure that can update the dataset. Completed work, code, documentation, and examples are available at this http URL. FNSPID offers unprecedented opportunities for the financial research community to advance predictive modeling and analysis. | Zihan Dong, Xinyu Fan, Zhiyuan Peng | North Carolina State University, Raleigh, NC, USA; SiChuan University, Chengdu, Sichuan Province, China |
|  |  [Transportation Marketplace Rate Forecast Using Signature Transform](https://doi.org/10.1145/3637528.3671637) |  | 0 | Freight transportation marketplace rates are typically challenging to forecast accurately. In this work, we have developed a novel statistical technique based on signature transforms and have built a predictive and adaptive model to forecast these marketplace rates. Our technique is based on two key elements of the signature transform: one being its universal nonlinearity property, which linearizes the feature space and hence translates the forecasting problem into linear regression, and the other being the signature kernel, which allows for comparing computationally efficiently similarities between time series data. Combined, it allows for efficient feature generation and precise identification of seasonality and regime switching in the forecasting process. An algorithm based on our technique has been deployed by Amazon trucking operations, with far superior forecast accuracy and better interpretability versus commercially available industry models, even during the COVID-19 pandemic and the Ukraine conflict. Furthermore, our technique is able to capture the influence of business cycles and the heterogeneity of the marketplace, improving prediction accuracy by more than fivefold, with an estimated annualized saving of \50 million. | Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip M. Kaminsky, Xinyu Li | University of California, Berkeley, Berkeley, CA, USA; University of California Department of Industrial Engineering & Operations Research; Worldwide Operations Research Science, Amazon.com Inc., Bellevue, WA, USA; Middle Mile Marketplace Science |
|  |  [Intelligent Agents with LLM-based Process Automation](https://doi.org/10.1145/3637528.3671646) |  | 0 | While intelligent virtual assistants like Siri, Alexa, and Google Assistant have become ubiquitous in modern life, they still face limitations in their ability to follow multi-step instructions and accomplish complex goals articulated in natural language. However, recent breakthroughs in large language models (LLMs) show promise for overcoming existing barriers by enhancing natural language processing and reasoning capabilities. Though promising, applying LLMs to create more advanced virtual assistants still faces challenges like ensuring robust performance and handling variability in real-world user commands. This paper proposes a novel LLM-based virtual assistant that can automatically perform multi-step operations within mobile apps based on high-level user requests. The system represents an advance in assistants by providing an end-to-end solution for parsing instructions, reasoning about goals, and executing actions. LLM-based Process Automation (LLMPA) has modules for decomposing instructions, generating descriptions, detecting interface elements, predicting next actions, and error checking. Experiments demonstrate the system completing complex mobile operation tasks in Alipay based on natural language instructions. This showcases how large language models can enable automated assistants to accomplish real-world tasks. The main contributions are the novel LLMPA architecture optimized for app process automation, the methodology for applying LLMs to mobile apps, and demonstrations of multi-step task completion in a real-world environment. Notably, this work represents the first real-world deployment and extensive evaluation of a large language model-based virtual assistant in a widely used mobile application with an enormous user base numbering in the hundreds of millions. | Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, Chenyi Zhuang | Zhejiang University, Hangzhou, China; Ant Group, Hangzhou, China; Renmin University of China, Beijing, China |
|  |  [SentHYMNent: An Interpretable and Sentiment-Driven Model for Algorithmic Melody Harmonization](https://doi.org/10.1145/3637528.3671626) |  | 0 | Music composition and analysis is an inherently creative task, involving a combination of heart and mind. However, the vast majority of algorithmic music models completely ignore the "heart" component of music, resulting in output that often lacks the rich emotional direction found in human-composed music. Models that try to incorporate musical sentiment rely on a "valence-arousal" model, which insufficiently characterizes emotion in two dimensions. Furthermore, existing methods typically adopt a black-box, music agnostic approach, treating music-theoretical and sentimental understanding as a by-product that can be inferred given sufficient data. In this study, we introduce two major novel elements: a nuanced mixture-based representation for musical sentiment, including a web tool to gather data, as well as a sentiment- and theory-driven harmonization model, SentHYMNent. SentHYMNent employs a novel Hidden Markov Model based on both key and chord transitions, as well as sentiment mixtures, to provide a probabilistic framework for learning key modulations and chordal progressions from a given melodic line and sentiment. Furthermore, our approach leverages compositional principles, resulting in a simpler model that significantly reduces computational burden and enhances interpretability compared to current state-of-the-art algorithmic harmonization methods. Importantly, as shown in our experiments, these improvements do not come at the expense of harmonization quality. We also provide a web app where users can upload their own melodies for SentHYMNent to harmonize. | Stephen Hahn, Jerry Yin, Rico Zhu, Weihan Xu, Yue Jiang, Simon Mak, Cynthia Rudin | Duke University, Durham, NC, USA |
|  |  [FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs](https://doi.org/10.1145/3637528.3671545) |  | 0 | This paper introduces FedSecurity, an end-to-end benchmark that serves as asupplementary component of the FedML library for simulating adversarial attacksand corresponding defense mechanisms in Federated Learning (FL). FedSecurityeliminates the need for implementing the fundamental FL procedures, e.g., FLtraining and data loading, from scratch, thus enables users to focus ondeveloping their own attack and defense strategies. It contains two keycomponents, including FedAttacker that conducts a variety of attacks during FLtraining, and FedDefender that implements defensive mechanisms to counteractthese attacks. FedSecurity has the following features: i) It offers extensivecustomization options to accommodate a broad range of machine learning models(e.g., Logistic Regression, ResNet, and GAN) and FL optimizers (e.g., FedAVG,FedOPT, and FedNOVA); ii) it enables exploring the effectiveness of attacks anddefenses across different datasets and models; and iii) it supports flexibleconfiguration and customization through a configuration file and some APIs. Wefurther demonstrate FedSecurity's utility and adaptability through federatedtraining of Large Language Models (LLMs) to showcase its potential on a widerange of complex applications. | Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin, Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang, Qifan Zhang, Yuhui Zhang, Carlee JoeWong, Salman Avestimehr, Chaoyang He | Zhejiang University, Hangzhou, China; Texas A&M University, College Station, TX, USA; Lehigh University, Bethlehem, PA, USA; TensorOpera Inc., Palo Alto, CA, USA; University of California, Irvine, Irvine, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; UIUC, Urbana, IL, USA; University of Southern California, Los Angeles, CA, USA |
|  |  [Paths2Pair: Meta-path Based Link Prediction in Billion-Scale Commercial Heterogeneous Graphs](https://doi.org/10.1145/3637528.3671563) |  | 0 | Link prediction, determining if a relation exists between two entities, is an essential task in the analysis of heterogeneous graphs with diverse entities and relations. Despite extensive research in link prediction, most existing works focus on predicting the relation type between given pairs of entities. However, it is almost impractical to check every entity pair when trying to find most hidden relations in a billion-scale heterogeneous graph due to the billion squared number of possible pairs. Meanwhile, most methods aggregate information at the node level, potentially leading to the loss of direct connection information between the two nodes. In this paper, we introduce Paths2Pair, a novel framework to address these limitations for link prediction in billion-scale commercial heterogeneous graphs. (i) First, it selects a subset of reliable entity pairs for prediction based on relevant meta-paths. (ii) Then, it utilizes various types of content information from the meta-paths between each selected entity pair to predict whether a target relation exists. We first evaluate our Paths2Pair based on a large-scale dataset, and results show Paths2Pair outperforms state-of-the-art baselines significantly. We then deploy our Paths2Pair on JD Logistics, one of the largest logistics companies in the world, for business expansion. The uncovered relations by Paths2Pair have helped JD Logistics identify 108,709 contacts to attract new company customers, resulting in an 84% increase in the success rate compared to the state-of-the-practice solution, demonstrating the practical value of our framework. We have released the code of our framework at https://github.com/JQHang/Paths2Pair. | Jinquan Hang, Zhiqing Hong, Xinyue Feng, Guang Wang, Guang Yang, Feng Li, Xining Song, Desheng Zhang | Florida State University, Tallahassee, FL, USA; Rutgers University, Piscataway, NJ, USA; JD Logistics, Beijing, China; JD Logistics & Rutgers University, Beijing, China |
|  |  [Distributed Harmonization: Federated Clustered Batch Effect Adjustment and Generalization](https://doi.org/10.1145/3637528.3671590) |  | 0 | Independent and identically distributed (i.i.d.) data is essential to many data analysis and modeling techniques. In the medical domain, collecting data from multiple sites or institutions is a common strategy that guarantees sufficient clinical diversity, determined by the decentralized nature of medical data. However, data from various sites are easily biased by the local environment or facilities, thereby violating the i.i.d. rule. A common strategy is to harmonize the site bias while retaining important biological information. The ComBat is among the most popular harmonization approaches and has recently been extended to handle distributed sites. However, when faced with situations involving newly joined sites in training or evaluating data from unknown/unseen sites, ComBat lacks compatibility and requires retraining with data from all the sites. The retraining leads to significant computational and logistic overhead that is usually prohibitive. In this work, we develop a novel Cluster ComBat harmonization algorithm, which leverages cluster patterns of the data in different sites and greatly advances the usability of ComBat harmonization. We use extensive simulation and real medical imaging data from ADNI to demonstrate the superiority of the proposed approach. Our codes are provided in https://github.com/illidanlab/distributed-cluster-harmonization. | Bao Hoang, Yijiang Pang, Siqi Liang, Liang Zhan, Paul M. Thompson, Jiayu Zhou | Michigan State University, East Lansing, Michigan, USA; University of Pittsburgh, Pittsburgh, Pennsylvania, USA; University of Southern California, Los Angeles, California, USA |
|  |  [Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay](https://doi.org/10.1145/3637528.3671657) |  | 0 | Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a new benchmark via: (i) achieving 25% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance. | Hussain Jagirdar, Rukma Talwadker, Aditya Pareek, Pulkit Agrawal, Tridib Mukherjee | Games24x7, Bengaluru, India |
|  |  [Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction](https://doi.org/10.1145/3637528.3671546) |  | 0 | As the focus of space exploration shifts from national agencies to private companies, the interest in space industry has been steadily increasing. With the increasing number of satellites, the risk of collisions between satellites and space debris has escalated, potentially leading to significant property and human losses. Therefore, accurately modeling the orbit is critical for satellite operations. In this work, we propose the Decomposed Attention Segment Recurrent Neural Network (DASR) model, adding two key components, Multi-Head Attention and Tensor Train Decomposition, to SegRNN for orbit prediction. The DASR model applies Multi-Head Attention before segmenting at input data and before the input of the GRU layers. In addition, Tensor Train (TT) Decomposition is applied to the weight matrices of the Multi-Head Attention in both the encoder and decoder. For evaluation, we use three real-world satellite datasets from the Korea Aerospace Research Institute (KARI), which are currently operating: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5 satellites. Our proposed model demonstrates superior performance compared to other SOTA baseline models. We demonstrate that our approach has 94.13% higher predictive performance than the second-best model in the KOMPSAT-3 dataset, 89.79% higher in the KOMPSAT-3A dataset, and 76.71% higher in the KOMPSAT-5 dataset. | Seungwon Jeong, Soyeon Woo, Daewon Chung, Simon S. Woo, Youjin Shin | National Satellite Operation Center Korea Aerospace Research Institute, Daejeon, Republic of Korea; Computer Science & Engineering Department, Sungkyunkwan University, Suwon, Republic of Korea; Sejong University, Seoul, Republic of Korea; The Catholic University of Korea, Bucheon, Republic of Korea |
|  |  [RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning](https://doi.org/10.1145/3637528.3671644) |  | 0 | Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we establish a comprehensive benchmark in the field of medical specialization and introduced RJUA-MedDQA, which contains 2000 real-world Chinese medical report images poses several challenges: comprehensively interpreting imgage content across a wide variety of challenging layouts, possessing the numerical reasoning ability to identify abnormal indicators and demonstrating robust clinical reasoning ability to provide the statement of disease diagnosis, status and advice based on a collection of medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy. We conduct extensive evaluations, including few-shot assessments of 5 LMMs which are capable of solving Chinese medical QA tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong LLMs by using image-text generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to LLMs. (3) Reasoning across context and image content present significant challenges. We hope this benchmark helps the community make progress on these challenging tasks in multi-modal medical document understanding and facilitate its application in healthcare. Our dataset will be publicly available for noncommercial use at https://github.com/Alipay-Med/medDQA_benchmark.git | Congyun Jin, Ming Zhang, Weixiao Ma, Yujiao Li, Yingbo Wang, Yabo Jia, Yuliang Du, Tao Sun, Haowen Wang, Cong Fan, Jinjie Gu, Chenfei Chi, Xiangguo Lv, Fangzhou Li, Wei Xue, Yiran Huang | Shanghai Jiao Tong University School of Medicine Affiliated Renji Hospital, Shanghai, China; Ant Group, Hangzhou, China; Ant Group, Shanghai, China; Shanghai Jiao Tong University School of Medicine Affiliated Renji Hospital, Qingdao, Shandong, China |
|  |  [Large Scale Hierarchical Industrial Demand Time-Series Forecasting incorporating Sparsity](https://doi.org/10.1145/3637528.3671632) |  | 0 | Hierarchical time-series forecasting (HTSF) is an important problem for many real-world business applications where the goal is to simultaneously forecast multiple time-series that are related to each other via a hierarchical relation. Recent works, however, do not address two important challenges that are typically observed in many demand forecasting applications at large companies. First, many time-series at lower levels of the hierarchy have high sparsity i.e., they have a significant number of zeros. Most HTSF methods do not address this varying sparsity across the hierarchy. Further, they do not scale well to the large size of the real-world hierarchy typically unseen in benchmarks used in literature. We resolve both these challenges by proposing HAILS, a novel probabilistic hierarchical model that enables accurate and calibrated probabilistic forecasts across the hierarchy by adaptively modeling sparse and dense time-series with different distributional assumptions and reconciling them to adhere to hierarchical constraints. We show the scalability and effectiveness of our methods by evaluating them against real-world demand forecasting datasets. We deploy HAILS at a large chemical manufacturing company for a product demand forecasting application with over ten thousand products and observe a significant 8.5% improvement in forecast accuracy and 23% better improvement for sparse time-series. The enhanced accuracy and scalability make HAILS a valuable tool for improved business planning and customer experience. | Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou, James Peters, Joe Czyzyk, B. Aditya Prakash | The Dow Chemical Company, Midland, MI, USA; The Dow Chemical Company, Houston, TX, USA; Georgia Institute of Technology, Atlanta, GA, USA |
|  |  [Know, Grow, and Protect Net Worth: Using ML for Asset Protection by Preventing Overdraft Fees](https://doi.org/10.1145/3637528.3671628) |  | 0 | When a customer overdraws their bank account and their balance is negative they are assessed an overdraft fee. Americans pay approximately $15 billion in unnecessary overdraft fees a year, often in $35 increments; users of the Mint personal finance app pay approximately $250 million in fees a year in particular. These overdraft fees are an excessive financial burden and lead to cascading overdraft fees trapping customers in financial hardship. To address this problem, we have created an ML-driven overdraft early warning system (ODEWS) that assesses a customer's risk of overdrafting within the next week using their banking and transaction data in the Mint app. At-risk customers are sent an alert so they can take steps to avoid the fee, ultimately changing their behavior and financial habits. The system deployed resulted in a $3 million savings in overdraft fees for Mint customers compared to a control group. Moreover, the methodology outlined here is part of a greater effort to provide ML-driven personalized financial advice to help our members know, grow, and protect their net worth, ultimately, achieving their financial goals. | Avishek Kumar, Tyson Silver | Intuit CreditKarma, Oakland, CA, USA; Lightcast, Moscow, ID, USA |
|  |  [FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning](https://doi.org/10.1145/3637528.3671573) |  | 0 | Large language models (LLMs) have demonstrated great capabilities in various natural language understanding and generation tasks. These pre-trained LLMs can be further improved for specific downstream tasks by fine-tuning. However, the adoption of LLM in real-world applications can be hindered by privacy concerns and the resource-intensive nature of model training and fine-tuning. When multiple entities have similar interested tasks but cannot directly share their local data due to privacy regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. Besides avoiding direct data sharing, FL can also achieve rigorous data privacy protection, model intelligent property protection, and model customization via composition with different techniques. Despite the aforementioned advantages of FL, fine-tuning LLMs in FL settings still lacks adequate support from the existing frameworks and, therefore, faces challenges in optimizing the consumption of significant communication and computational resources, preparing various data for different tasks, and satisfying diverse information protection demands. In this paper, we discuss these challenges and introduce our package FederatedScope-LLM (FS-LLM) as a main contribution, which consists: (1) We build a complete end-to-end benchmarking pipeline under real-world scenarios, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation; (2) We provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension, enhancing the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model; (3) We adopt several accelerating and resource-efficient operators, and provide flexible pluggable sub-routines for interdisciplinary study. We conduct extensive and reproducible experiments to show the effectiveness of FS-LLM and benchmark advanced LLMs with PEFT algorithms in FL. We release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm. | Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou | Alibaba Group, Beijing, China; Alibaba Group, Bellevue, USA; Alibaba Group, Hangzhou, China |
|  |  [AutoWebGLM: A Large Language Model-based Web Navigating Agent](https://doi.org/10.1145/3637528.3671620) |  | 0 | Large language models (LLMs) have fueled many intelligent web agents, but most existing ones perform far from satisfying in real-world web navigation tasks due to three factors: (1) the complexity of HTML text data (2) versatility of actions on webpages, and (3) task difficulty due to the open-domain nature of the web. In light of these challenges, we develop the open AutoWebGLM based on ChatGLM3-6B. AutoWebGLM can serve as a powerful automated web navigation agent that outperform GPT-4. Inspired by human browsing patterns, we first design an HTML simplification algorithm to represent webpages with vital information preserved succinctly. We then employ a hybrid human-AI method to build web browsing data for curriculum training. Finally, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For comprehensive evaluation, we establish a bilingual benchmark---AutoWebBench---for real-world web navigation tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, demonstrating its potential to tackle challenging tasks in real environments. Related code, model, and data are released at https://github.com/THUDM/AutoWebGLM. | Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang | Tsinghua University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Tsinghua University & Zhipu AI, Beijing, China; Zhipu AI, Beijing, China |
|  |  [SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning](https://doi.org/10.1145/3637528.3671534) |  | 0 | Graph-based fraud detection has widespread application in modern industry scenarios, such as spam review and malicious account detection. While considerable efforts have been devoted to designing adequate fraud detectors, the interpretability of their results has often been overlooked. Previous works have attempted to generate explanations for specific instances using post-hoc explaining methods such as a GNNExplainer. However, post-hoc explanations can not facilitate the model predictions and the computational cost of these methods cannot meet practical requirements, thus limiting their application in real-world scenarios. To address these issues, we propose SEFraud, a novel graph-based self-explainable fraud detection framework that simultaneously tackles fraud detection and result in interpretability. Concretely, SEFraud first leverages customized heterogeneous graph transformer networks with learnable feature masks and edge masks to learn expressive representations from the informative heterogeneously typed transactions. A new triplet loss is further designed to enhance the performance of mask learning. Empirical results on various datasets demonstrate the effectiveness of SEFraud as it shows considerable advantages in both the fraud detection performance and interpretability of prediction results. Specifically, SEFraud achieves the most significant improvement with 8.6% on AUC and 8.5% on Recall over the second best on fraud detection, as well as an average of 10x speed-up regarding the inference time. Last but not least, SEFraud has been deployed and offers explainable fraud detection service for the largest bank in China, Industrial and Commercial Bank of China Limited (ICBC). Results collected from the production environment of ICBC show that SEFraud can provide accurate detection results and comprehensive explanations that align with the expert business understanding, confirming its efficiency and applicability in large-scale online services. | Kaidi Li, Tianmeng Yang, Min Zhou, Jiahao Meng, Shendi Wang, Yihui Wu, Boshuai Tan, Hu Song, Lujia Pan, Fan Yu, Zhenli Sheng, Yunhai Tong | Huawei Inc, Shenzhen, China; ICBC Limited, Shanghai, China; Peking University, Beijing, China |
|  |  [UrbanGPT: Spatio-Temporal Large Language Models](https://doi.org/10.1145/3637528.3671578) |  | 0 | Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. In certain cases, it becomes challenging to collect any labeled data from downstream scenarios, intensifying the problem further. Consequently, it becomes necessary to build a spatio-temporal model that can exhibit strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is to create a spatio-temporal LLM that can exhibit exceptional generalization capabilities across a wide range of downstream urban tasks. To achieve this objective, we present the UrbanGPT, which seamlessly integrates a spatio-temporal dependency encoder with the instruction-tuning paradigm. This integration enables LLMs to comprehend the complex inter-dependencies across time and space, facilitating more comprehensive and accurate predictions under data scarcity. To validate the effectiveness of our approach, we conduct extensive experiments on various public datasets, covering different spatio-temporal prediction tasks. The results consistently demonstrate that our UrbanGPT, with its carefully designed architecture, consistently outperforms state-of-the-art baselines. These findings highlight the potential of building large language models for spatio-temporal learning, particularly in zero-shot scenarios where labeled data is scarce. The code and data are available at: https://github.com/HKUDS/UrbanGPT. | Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, Chao Huang | South China University of Technology & The University of Hong Kong, Guangzhou, China; The University of Hong Kong, Hong Kong SAR, China; Baidu Inc., Beijing, China; South China University of Technology, Guangzhou, China |
|  |  [Harvesting Efficient On-Demand Order Pooling from Skilled Couriers: Enhancing Graph Representation Learning for Refining Real-time Many-to-One Assignments](https://doi.org/10.1145/3637528.3671643) |  | 0 | The recent past has witnessed a notable surge in on-demand food delivery (OFD) services, offering delivery fulfillment within dozens of minutes after an order is placed. In OFD, pooling multiple orders for simultaneous delivery in real-time order assignment is a pivotal efficiency source, which may in turn extend delivery time. Constructing high-quality order pooling to harmonize platform efficiency with the experiences of consumers and couriers, is crucial to OFD platforms. However, the complexity and real-time nature of order assignment, making extensive calculations impractical, significantly limit the potential for order consolidation. Moreover, offline environment is frequently riddled with unknown factors, posing challenges for the platform's perceptibility and pooling decisions. Nevertheless, delivery behaviors of skilled couriers (SCs) who know the environment well, can improve system awareness and effectively inform decisions. Hence a SC delivery network (SCDN) is constructed, based on an enhanced attributed heterogeneous network embedding approach tailored for OFD. It aims to extract features from rich temporal and spatial information, and uncover the latent potential for order combinations embedded within SC trajectories. Accordingly, the vast search space of order assignment can be effectively pruned through scalable similarity calculations of low-dimensional vectors, making comprehensive and high-quality pooling outcomes more easily identified in real time. In addition, the acquired embedding outcomes highlight promising subspaces embedded within this space, i.e., scale-effect hotspot areas, which can offer significant potential for elevating courier efficiency. SCDN has now been deployed in Meituan dispatch system. Online tests reveal that with SCDN, the pooling quality and extent have been greatly improved. And our system can boost couriers' efficiency by 45-55% during noon peak hours, while upholding the timely delivery commitment. | Yile Liang, Jiuxia Zhao, Donghui Li, Jie Feng, Chen Zhang, Xuetao Ding, Jinghua Hao, Renqing He | Meituan, Beijing, China; Tsinghua University, Beijing, China |
|  |  [Hyper-Local Deformable Transformers for Text Spotting on Historical Maps](https://doi.org/10.1145/3637528.3671589) |  | 0 | Text on historical maps contains valuable information providing georeferenced historical, political, and cultural contexts. However, text extraction from historical maps has been challenging due to the lack of (1) effective methods and (2) training data. Previous approaches use ad-hoc steps tailored to only specific map styles. Recent machine learning-based text spotters (e.g., for scene images) have the potential to solve these challenges because of their flexibility in supporting various types of text instances. However, these methods remain challenges in extracting precise image features for predicting every sub-component (boundary points and characters) in a text instance. This is critical because map text can be lengthy and highly rotated with complex backgrounds, posing difficulties in detecting relevant image features from a rough text region. This paper proposes PALETTE, an end-to-end text spotter for scanned historical maps of a wide variety. PALETTE introduces a novel hyper-local sampling module to explicitly learn localized image features around the target boundary points and characters of a text instance for detection and recognition. PALETTE also enables hyper-local positional embeddings to learn spatial interactions between boundary points and characters within and across text instances. In addition, this paper presents a novel approach to automatically generate synthetic map images, SYNTHMAP+, for training text spotters for historical maps. The experiment shows that PALETTE with SYNTHMAP+ outperforms SOTA text spotters on two new benchmark datasets of historical maps, particularly for long and angled text. We have deployed PALETTE with SYNTHMAP+ to process over 60,000 maps in the David Rumsey Historical Map collection and generated over 100 million text labels to support map searching. | Yijun Lin, YaoYi Chiang | University of Minnesota, Twin Cities, Minneapolis, MN, USA |
|  |  [Source Localization for Cross Network Information Diffusion](https://doi.org/10.1145/3637528.3671624) |  | 0 | Source localization aims to locate information diffusion sources only given the diffusion observation, which has attracted extensive attention in the past few years. Existing methods are mostly tailored for single networks and may not be generalized to handle more complex networks like cross-networks. Cross-network is defined as two interconnected networks, where one network's functionality depends on the other. Source localization on cross-networks entails locating diffusion sources on the source network by only giving the diffused observation in the target network. The task is challenging due to challenges including: 1) diffusion sources distribution modeling; 2) jointly considering both static and dynamic node features; and 3) heterogeneous diffusion patterns learning. In this work, we propose a novel method, namely CNSL, to handle the three primary challenges. Specifically, we propose to learn the distribution of diffusion sources through Bayesian inference and leverage disentangled encoders to learn static and dynamic node features separately. The learning objective is coupled with the cross-network information propagation estimation model to make the inference of diffusion sources considering the overall diffusion process. Additionally, we also provide two novel cross-network datasets collected by ourselves. Extensive experiments are conducted on both datasets to demonstrate the effectiveness of CNSL in handling the source localization on cross-networks. | Chen Ling, Tanmoy Chowdhury, Jie Ji, Sirui Li, Andreas Züfle, Liang Zhao | Emory University, Atlanta, VA, USA; Emory University, Atlanta, GA, USA |
|  |  [MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning](https://doi.org/10.1145/3637528.3671609) |  | 0 | Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTcoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTcoder offers efficient training capabilities, including efficient data tokenization modes and PEFT fine-tuning, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTcoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Leveraging the CodeLLama foundation, our MFTcoder fine-tuned model, \textsc{CodeFuse-CodeLLama-34B}, achieves an impressive pass@1 score of 74.4\% on the HumaneEval benchmark, surpassing GPT-4 performance (67\%, zero-shot). MFTCoder is open-sourced at \url{https://github.com/codefuse-ai/MFTCOder} | Bingchang Liu, Chaoyu Chen, Zi Gong, Cong Liao, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Wei Jiang, Hang Yu, Jianguo Li |  |
|  |  [Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm](https://doi.org/10.1145/3637528.3671575) |  | 0 | Large language models (LLMs) are gaining increasing interests to improve clinical efficiency, owing to their unprecedented performance in modelling natural language. Ensuring the reliable clinical applications, the evaluation of LLMs indeed becomes critical for better mitigating the potential risks, e.g., hallucinations. However, current evaluation methods heavily rely on labor-intensive human participation to achieve human-preferred judgements. To overcome this challenge, we propose an automatic evaluation paradigm tailored to assess the LLMs' capabilities in delivering clinical services, e.g., disease diagnosis and treatment. The evaluation paradigm contains three basic elements: metric, data, and algorithm. Specifically, inspired by professional clinical practice pathways, we formulate a LLM-specific clinical pathway (LCP) to define the clinical capabilities that a doctor agent should possess. Then, Standardized Patients (SPs) from the medical education are introduced as the guideline for collecting medical data for evaluation, which can well ensure the completeness of the evaluation procedure. Leveraging these steps, we develop a multi-agent framework to simulate the interactive environment between SPs and a doctor agent, which is equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the behaviors of a doctor agent are in accordance with LCP. The above paradigm can be extended to any similar clinical scenarios to automatically evaluate the LLMs' medical capabilities. Applying such paradigm, we construct an evaluation benchmark in the field of urology, including a LCP, a SPs dataset, and an automated RAE. Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice. | Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu, Ming Zhang, Xiaowei Ma, Xiangguo Lv, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu | Renji Hospital, Shanghai, China; The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Ant Group, Hangzhou, China; Ant Group, Shanghai, China |
|  |  [DAG: Deep Adaptive and Generative K-Free Community Detection on Attributed Graphs](https://doi.org/10.1145/3637528.3671615) |  | 0 | Community detection on attributed graphs with rich semantic and topological information offers great potential for real-world network analysis, especially user matching in online games. Graph Neural Networks (GNNs) have recently enabled Deep Graph Clustering (DGC) methods to learn cluster assignments from semantic and topological information. However, their success depends on the prior knowledge related to the number of communities K, which is unrealistic due to the high costs and privacy issues of acquisition. In this paper, we investigate the community detection problem without prior K, referred to as K-Free Community Detection problem. To address this problem, we propose a novel Deep Adaptive and Generative model~(DAG) for community detection without specifying the prior K. DAG consists of three key components, i.e., a node representation learning module with masked attribute reconstruction, a community affiliation readout module, and a community number search module with group sparsity. These components enable DAG to convert the process of non-differentiable grid search for the community number, i.e., a discrete hyperparameter in existing DGC methods, into a differentiable learning process. In such a way, DAG can simultaneously perform community detection and community number search end-to-end. To alleviate the cost of acquiring community labels in real-world applications, we design a new metric, EDGE, to evaluate community detection methods even when the labels are not feasible. Extensive offline experiments on five public datasets and a real-world online mobile game dataset demonstrate the superiority of our DAG over the existing state-of-the-art (SOTA) methods. DAG has a relative increase of 7.35% in teams in a Tencent online game compared with the best competitor. | Chang Liu, Yuwen Yang, Yue Ding, Hongtao Lu, Wenqing Lin, Ziming Wu, Wendong Bi | Shanghai Jiao Tong University, Shanghai, China; Tencent, Shenzhen, China |
|  |  [EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis](https://doi.org/10.1145/3637528.3671552) |  | 0 | Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of large language models (LLMs), researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on 3 classification tasks and 2 regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 8 regression tasks and 6 classification tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our models with a variety of LLMs and sentiment analysis tools on AEB, where our models outperform all other open-sourced LLMs and sentiment analysis tools, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. This project is available at https://github.com/lzw108/EmoLLMs/. | Zhiwei Liu, Kailai Yang, Qianqian Xie, Tianlin Zhang, Sophia Ananiadou | The University of Manchester & Artificial Intelligence Research Center, Manchester, United Kingdom; The University of Manchester, Manchester, United Kingdom |
|  |  [MISP: A Multimodal-based Intelligent Server Failure Prediction Model for Cloud Computing Systems](https://doi.org/10.1145/3637528.3671568) |  | 0 | Traditional server failure prediction methods predominantly rely on single-modality data such as system logs or system status curves. This reliance may lead to an incomplete understanding of system health and impending issues, proving inadequate for the complex and dynamic landscape of contemporary cloud computing environments. The potential of multimodal data to provide comprehensive insights is widely acknowledged, yet the lack of a holistic dataset and the challenges inherent in integrating features from both structured and unstructured data have impeded the exploration of multimodal-based server failure prediction. Addressing these challenges, this paper presents an industrial-scale, comprehensive dataset for server failure prediction, comprising nearly 80 types of structured and unstructured data sourced from real-world industrial cloud systems 1. Building on this resource, we introduce MISP, a model that leverages multimodal fusion techniques for server failure prediction. MISP transforms multimodal data into multi-dimensional sequences, extracts and encodes features both within and across the modalities, and ultimately computes the failure probability from the synthesized features. Experiments demonstrate that MISP significantly outperforms existing methods, enhancing prediction accuracy by approximately 25% over previous state-of-the-art approaches. | Xianting Lu, Yunong Wang, Yu Fu, Qi Sun, Xuhua Ma, Xudong Zheng, Cheng Zhuo | Zhejiang University, Hangzhou, China; Lanzhou University & Zhejiang University, Lanzhou, China; Alibaba Cloud, Alibaba Group, Hangzhou, China |
|  |  [Integrating System State into Spatio Temporal Graph Neural Network for Microservice Workload Prediction](https://doi.org/10.1145/3637528.3671508) |  | 0 | Microservice architecture has become a driving force in enhancing the modularity and scalability of web applications, as evidenced by the Alipay platform's operational success. However, a prevalent issue within such infrastructures is the suboptimal utilization of CPU resources due to inflexible resource allocation policies. This inefficiency necessitates the development of dynamic, accurate workload prediction methods to improve resource allocation. In response to this challenge, we present STAMP, a Spatio Temporal Graph Network for Microservice Workload Prediction. STAMP is designed to comprehensively address the multifaceted interdependencies between microservices, the temporal variability of workloads, and the critical role of system state in resource utilization. Through a graph-based representation, STAMP effectively maps the intricate network of microservice interactions. It employs time series analysis to capture the dynamic nature of workload changes and integrates system state insights to enhance prediction accuracy. Our empirical analysis, using three distinct real-world datasets, establishes that STAMP exceeds baselines by achieving an average boost of 5.72% in prediction precision, as measured by RMSE. Upon deployment in Alipay's microservice environment, STAMP achieves a 33.10% reduction in resource consumption, significantly outperforming existing online methods. This research solidifies STAMP as a validated framework, offering meaningful contributions to the field of resource management in microservice architecture-based applications. | Yang Luo, Mohan Gao, Zhemeng Yu, Haoyuan Ge, Xiaofeng Gao, Tengwei Cai, Guihai Chen | Shanghai Jiao Tong University, Shanghai, China; Ant Group, Hangzhou, China |
|  |  [FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting](https://doi.org/10.1145/3637528.3671509) |  | 0 | Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW. | Ziqing Ma, Wenwei Wang, Tian Zhou, Chao Chen, Bingqing Peng, Liang Sun, Rong Jin |  |
|  |  [Valuing an Engagement Surface using a Large Scale Dynamic Causal Model](https://doi.org/10.1145/3637528.3671604) |  | 0 | With recent rapid growth in online shopping, AI-powered Engagement Surfaces (ES) have become ubiquitous across retail services. These engagement surfaces perform an increasing range of functions, including recommending new products for purchase, reminding customers of their orders and providing delivery notifications. Understanding the causal effect of engagement surfaces on value driven for customers and businesses remains an open scientific question. In this paper, we develop a dynamic causal model at scale to disentangle value attributable to an ES, and to assess its effectiveness. We demonstrate the application of this model to inform business decision-making by understanding returns on investment in the ES, and identifying product lines and features where the ES adds the most value. | Abhimanyu Mukerji, Sushant More, Ashwin Viswanathan Kannan, Lakshmi Ravi, Hua Chen, Naman Kohli, Chris Khawand, Dinesh Mandalapu | Amazon, Sunnyvale, CA, USA; Amazon, Vancouver, BC, Canada; Amazon, Sunnyvale, WA, USA; Amazon, Seattle, WA, USA |
|  |  [EEG2Rep: Enhancing Self-supervised EEG Representation Through Informative Masked Inputs](https://doi.org/10.1145/3637528.3671600) |  | 0 | Self-supervised approaches for electroencephalography (EEG) representationlearning face three specific challenges inherent to EEG data: (1) The lowsignal-to-noise ratio which challenges the quality of the representationlearned, (2) The wide range of amplitudes from very small to relatively largedue to factors such as the inter-subject variability, risks the models to bedominated by higher amplitude ranges, and (3) The absence of explicitsegmentation in the continuous-valued sequences which can result in lessinformative representations. To address these challenges, we introduce EEG2Rep,a self-prediction approach for self-supervised representation learning fromEEG. Two core novel components of EEG2Rep are as follows: 1) Instead oflearning to predict the masked input from raw EEG, EEG2Rep learns to predictmasked input in latent representation space, and 2) Instead of conventionalmasking methods, EEG2Rep uses a new semantic subsequence preserving (SSP)method which provides informative masked inputs to guide EEG2Rep to generaterich semantic representations. In experiments on 6 diverse EEG tasks withsubject variability, EEG2Rep significantly outperforms state-of-the-artmethods. We show that our semantic subsequence preserving improves the existingmasking methods in self-prediction literature and find that preserving 50% ofEEG recordings will result in the most accurate results on all 6 tasks onaverage. Finally, we show that EEG2Rep is robust to noise addressing asignificant challenge that exists in EEG data. Models and code are availableat: https://github.com/Navidfoumani/EEG2Rep | Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad Irtza, Nam Nguyen, Mahsa Salehi | Emotiv Research, Melbourne, Australia; Monash University, Melbourne, Australia; Emotiv Research, Sydney, Australia |
|  |  [Detecting Abnormal Operations in Concentrated Solar Power Plants from Irregular Sequences of Thermal Images](https://doi.org/10.1145/3637528.3671623) |  | 0 | Concentrated Solar Power (CSP) plants store energy by heating a storage medium with an array of mirrors that focus sunlight onto solar receivers atop a central tower. Operating at high temperatures these receivers face risks such as freezing, deformation, and corrosion, leading to operational failures, downtime, or costly equipment damage. We study the problem of anomaly detection (AD) in sequences of thermal images collected over a year from an operational CSP plant. These images are captured at irregular intervals ranging from one to five minutes throughout the day by infrared cameras mounted on solar receivers. Our goal is to develop a method to extract useful representations from high-dimensional thermal images for AD. It should be able to handle temporal features of the data, which include irregularity, temporal dependency between images and non-stationarity due to a strong daily seasonal pattern. The co-occurrence of low-temperature anomalies that resemble normal images from the start and the end of the operational cycle with high-temperature anomalies poses an additional challenge. We first evaluate state-of-the-art deep image-based AD methods, which have been shown to be effective in deriving meaningful image representations for the detection of anomalies. Then, we introduce a forecasting-based AD method that predicts future thermal images from past sequences and timestamps via a deep sequence model. This method effectively captures specific temporal data features and distinguishes between difficult-to-detect temperature-based anomalies. Our experiments demonstrate the effectiveness of our approach compared to multiple SOTA baselines across multiple evaluation metrics. We have also successfully deployed our solution on five months of unseen data, providing critical insights to our industry partner for the maintenance of the CSP plant. Our code is publicly accessible from https://github.com/sukanyapatra1997/ForecastAD. Additionally, as our dataset is confidential, we release a simulated dataset at https://tinyurl.com/kdd2024Dataset. | Sukanya Patra, Nicolas Sournac, Souhaib Ben Taieb | University of Mons, Mons, Belgium |
|  |  [Spatio-Temporal Consistency Enhanced Differential Network for Interpretable Indoor Temperature Prediction](https://doi.org/10.1145/3637528.3671608) |  | 0 | Indoor temperature prediction is crucial for decision-making in central heating systems. Beyond accuracy, predictions shall be interpretable, i.e. conform to the laws of physics; otherwise, it may lead to system failures or unsafe conditions. However, deep learning models often face criticism regarding interpretability, which limits their application in such settings. To this end, we propose a Spatio-Temporal Consistency enhanced Differential Network (CONST) for interpretable indoor temperature prediction. Our approach mainly consists of a differential predictive module and a spatio-temporal consistency module. Modeling the influential factors, the first module solves the issue of multicollinearity through the differential operation. Considering the heterogeneity of global and local data distributions, the second module characterizes the temporal and spatial consistency to mine the universal pattern by multi-task learning, thereby improving the prediction interpretability. Besides, we propose a set of interpretability metrics to overcome the drawbacks of partial dependence plot metric, which are more practical, zero-centered, flexible, and numerical. We conclude experiments on a real-world dataset with four heating stations. The results demonstrate the advantages of our approach over various baselines, where the interpretability can be improved by more than 8 times on cRPD while maintaining high accuracy. We developed CONST on the SmartHeat system, providing hourly indoor temperature forecasts for 13 heating stations in northern China. | Dekang Qi, Xiuwen Yi, Chengjie Guo, Yanyong Huang, Junbo Zhang, Tianrui Li, Yu Zheng | Southwestern University of Finance and Economics, Chengdu, China; Xidian University, Xi'an, China; Southwest Jiaotong University & JD iCity, JD Technology, Chengdu, China; Southwest Jiaotong University, Chengdu, China; JD iCity, JD Technology & JD Intelligent Cities Research, Beijing, China |
|  |  [Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark](https://doi.org/10.1145/3637528.3671616) |  | 0 | Fair graph learning plays a pivotal role in numerous practical applications.Recently, many fair graph learning methods have been proposed; however, theirevaluation often relies on poorly constructed semi-synthetic datasets orsubstandard real-world datasets. In such cases, even a basic MultilayerPerceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utilityand fairness. In this work, we illustrate that many datasets fail to providemeaningful information in the edges, which may challenge the necessity of usinggraph structures in these problems. To address these issues, we develop andintroduce a collection of synthetic, semi-synthetic, and real-world datasetsthat fulfill a broad spectrum of requirements. These datasets are thoughtfullydesigned to include relevant graph structures and bias information crucial forthe fair evaluation of models. The proposed synthetic and semi-syntheticdatasets offer the flexibility to create data with controllable biasparameters, thereby enabling the generation of desired datasets withuser-defined bias values with ease. Moreover, we conduct systematic evaluationsof these proposed datasets and establish a unified evaluation approach for fairgraph learning models. Our extensive experimental results with fair graphlearning methods across our datasets demonstrate their effectiveness inbenchmarking the performance of these methods. Our datasets and the code forreproducing our experiments are available athttps://github.com/XweiQ/Benchmark-GraphFairness. | Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma | New Jersey Institute of Technology, Newark, NJ, USA; The Pennsylvania State University, University Park, PA, USA; Rensselaer Polytechnic Institute, Troy, NY, USA; Michigan State University, East Lansing, MI, USA |
|  |  [Class-incremental Learning for Time Series: Benchmark and Evaluation](https://doi.org/10.1145/3637528.3671581) |  | 0 | Real-world environments are inherently non-stationary, frequently introducingnew classes over time. This is especially common in time series classification,such as the emergence of new disease classification in healthcare or theaddition of new activities in human activity recognition. In such cases, alearning system is required to assimilate novel classes effectively whileavoiding catastrophic forgetting of the old ones, which gives rise to theClass-incremental Learning (CIL) problem. However, despite the encouragingprogress in the image and language domains, CIL for time series data remainsrelatively understudied. Existing studies suffer from inconsistent experimentaldesigns, necessitating a comprehensive evaluation and benchmarking of methodsacross a wide range of datasets. To this end, we first present an overview ofthe Time Series Class-incremental Learning (TSCIL) problem, highlight itsunique challenges, and cover the advanced methodologies. Further, based onstandardized settings, we develop a unified experimental framework thatsupports the rapid development of new algorithms, easy integration of newdatasets, and standardization of the evaluation process. Using this framework,we conduct a comprehensive evaluation of various generic andtime-series-specific CIL methods in both standard and privacy-sensitivescenarios. Our extensive experiments not only provide a standard baseline tosupport future research but also shed light on the impact of various designfactors such as normalization layers or memory budget thresholds. Codes areavailable at https://github.com/zqiao11/TSCIL. | Zhongzheng Qiao, Quang Pham, Zhen Cao, Hoang H. Le, Ponnuthurai N. Suganthan, Xudong Jiang, Savitha Ramasamy | Qatar University, Dohar, Qatar; I2R, ASTAR, Singapore, Singapore; School of Electrical and Electronic Engineering, NTU, Singapore, Singapore; I2R, ASTAR & CNRSCREATE, Singapore, Singapore; Ho Chi Minh University of Science, Vietnam National University, Ho Chi Minh City, Vietnam; IGP-ERIN, NTU & I2R, ASTAR, Singapore, Singapore |
|  |  [Leveraging Exposure Networks for Detecting Fake News Sources](https://doi.org/10.1145/3637528.3671539) |  | 0 | The scale and dynamic nature of the Web makes real-time detection of misinformation an extremely difficult task. Prior research mostly focused on offline (retrospective) detection of stories or claims using linguistic features of the content, flagging by users, and crowdsourced labels. Here, we develop a novel machine-learning methodology for detecting fake news sources using active learning, and examine the contribution of network, audience, and text features to the model accuracy. Importantly, we evaluate performance in both offline and online settings, mimicking the strategic choices fact-checkers have to make in practice as news sources emerge over time. We find that exposure networks provide information on considerably more sources than sharing networks (+49.6%), and that the inclusion of exposure features greatly improves classification PR-AUC in both offline (+33%) and online (+69.2%) settings. Textual features perform best in offline settings, but their performance deteriorates by 12.0-18.7% in online settings. Finally, the results show that a few iterations of active learning are sufficient for our model to attain predictive performance to comparable exhaustive labeling while incurring only 24.7% of the labeling costs. These results stress the importance of exposure networks as a source of valuable information for the investigation of information dissemination in social networks and question the robustness of textual features. | Maor Reuben, Lisa Friedland, Rami Puzis, Nir Grinberg | Ben-Gurion University of the Negev Software and Information Systems Engineering, Beer Sheva, Israel; Independent researcher, Boston, MA, USA |
|  |  [Tackling Concept Shift in Text Classification using Entailment-style Modeling](https://doi.org/10.1145/3637528.3671541) |  | 0 | Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as concept shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world & synthetic datasets achieving absolute F1 gains upto ~6% and ~30% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75% direct labeling costs and 40% downstream labeling costs overall in a span of 3 months. | Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy | Amazon, Bangalore, India; Amazon, Bengaluru, India |
|  |  [Hierarchical Knowledge Guided Fault Intensity Diagnosis of Complex Industrial Systems](https://doi.org/10.1145/3637528.3671610) |  | 0 | Fault intensity diagnosis (FID) plays a pivotal role in monitoring and maintaining mechanical devices within complex industrial systems. As current FID methods are based on chain of thought without considering dependencies among target classes. To capture and explore dependencies, we propose a hierarchical knowledge guided fault intensity diagnosis framework (HKG) inspired by the tree of thought, which is amenable to any representation learning methods. The HKG uses graph convolutional networks to map the hierarchical topological graph of class representations into a set of interdependent global hierarchical classifiers, where each node is denoted by word embeddings of a class. These global hierarchical classifiers are applied to learned deep features extracted by representation learning, allowing the entire model to be end-to-end learnable. In addition, we develop a re-weighted hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding inter-class hierarchical knowledge into a data-driven statistical correlation matrix (SCM) which effectively guides the information sharing of nodes in graphical convolutional neural networks and avoids over-smoothing issues. The Re-HKCM is derived from the SCM through a series of mathematical transformations. Extensive experiments are performed on four real-world datasets from different industrial domains (three cavitation datasets from SAMSON AG and one existing publicly) for FID, all showing superior results and outperform recent state-of-the-art FID methods. | Yu Sha, Shuiping Gou, Bo Liu, Johannes Faber, Ningtao Liu, Stefan Schramm, Horst Stoecker, Thomas Steckenreiter, Domagoj Vnucec, Nadine Wetzstein, Andreas Widl, Kai Zhou | SAMSON AG, Frankfurt, Hessian, Germany; Xidian University, FIAS and XF-IJRC, Xian, Shaanxi, China; SAMSON AG, Feankfurt, Hessian, Germany; FIAS, Goethe Universität and GSI, Frankfurt, Hessian, Germany; CUHK-SZ and FIAS, Shenzhen, Guangdong, China; Xidian University, Xian, Shaanxi, China; FIAS, Frankfurt, Hessian, Germany |
|  |  [Lumos: Empowering Multimodal LLMs with Scene Text Recognition](https://doi.org/10.1145/3637528.3671633) |  | 0 | We introduce Lumos, the first end-to-end multimodal question-answering systemwith text understanding capabilities. At the core of Lumos is a Scene TextRecognition (STR) component that extracts text from first person point-of-viewimages, the output of which is used to augment input to a Multimodal LargeLanguage Model (MM-LLM). While building Lumos, we encountered numerouschallenges related to STR quality, overall latency, and model inference. Inthis paper, we delve into those challenges, and discuss the systemarchitecture, design choices, and modeling techniques employed to overcomethese obstacles. We also provide a comprehensive evaluation for each component,showcasing high quality and efficiency. | Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu, Shicong Zhao, Longfang Zhao, Ankit Ramchandani, Xin Luna Dong, Anuj Kumar | Meta Reality Labs, Redmond, WA, USA; Meta Reality Labs, Menlo Park, CA, USA; Meta, Menlo Park, CA, USA; Reality Labs, Meta, Redmond, WA, USA |
|  |  [From Variability to Stability: Advancing RecSys Benchmarking Practices](https://doi.org/10.1145/3637528.3671655) |  | 0 | In the rapidly evolving domain of Recommender Systems (RecSys), newalgorithms frequently claim state-of-the-art performance based on evaluationsover a limited set of arbitrarily selected datasets. However, this approach mayfail to holistically reflect their effectiveness due to the significant impactof dataset characteristics on algorithm performance. Addressing thisdeficiency, this paper introduces a novel benchmarking methodology tofacilitate a fair and robust comparison of RecSys algorithms, thereby advancingevaluation practices. By utilizing a diverse set of 30 open datasets,including two introduced in this work, and evaluating 11 collaborativefiltering algorithms across 9 metrics, we critically examine the influence ofdataset characteristics on algorithm performance. We further investigate thefeasibility of aggregating outcomes from multiple datasets into a unifiedranking. Through rigorous experimental analysis, we validate the reliability ofour methodology under the variability of datasets, offering a benchmarkingstrategy that balances quality and computational demands. This methodologyenables a fair yet effective means of evaluating RecSys algorithms, providingvaluable guidance for future research endeavors. | Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey Savchenko, Alexey Zaytsev | Skoltech & MIPT, Moscow, Russian Federation; AIRI & Sber AI Lab, Moscow, Russian Federation; Skoltech & BIMSA, Moscow, Russian Federation; Sber AI Lab, Moscow, Russian Federation; Skoltech, Moscow, Russian Federation |
|  |  [Improving Ego-Cluster for Network Effect Measurement](https://doi.org/10.1145/3637528.3671557) |  | 0 | Network effect is common in social network platforms. Many new features in social networks are designed to specifically create network effect to improve user engagement. For example, content creators tend to produce more when their articles and posts receive more positive feedback from followers. This paper discusses a new cluster-level experimentation methodology to measure the creator-side metrics in the context of A/B experiment. The methodology is designed to address the cases when the experiment randomization unit and the metric measurement unit are not the same, and it is a part of the overall strategy at LinkedIn to promote a robust creator community and ecosystem. The method is developed based on the widely-cited research at LinkedIn, but significantly improves the clustering algorithm efficiency and flexibility, leading to a stronger capability of the creator-side metrics measurement and increasing velocity for creator-related experiments. | Wentao Su, Weitao Duan | LinkedIn Corporation, Sunnyvale, CA, USA |
|  |  [Beimingwu: A Learnware Dock System](https://doi.org/10.1145/3637528.3671617) |  | 0 | The learnware paradigm proposed by Zhou (2016) aims to enable users to leverage numerous existing high-performing models instead of building machine learning models from scratch. This paradigm envisions that: Any developer worldwide can submit their well-trained models spontaneously into a learnware dock system (formerly known as learnware market). The system uniformly generates a specification for each model to form a learnware and accommodates it. As the key component, a specification should represent the capabilities of the model while preserving developer's original data. Based on the specifications, the learnware dock system can identify and assemble existing learnwares for users to solve new machine learning tasks. Recently, based on reduced kernel mean embedding (RKME) specification, a series of studies have shown the effectiveness of the learnware paradigm theoretically and empirically. However, the realization of a learnware dock system is still missing and remains a big challenge. This paper proposes Beimingwu, the first open-source learnware dock system, providing foundational support for future research. The system provides implementations and extensibility for the entire process of learnware paradigm, including the submitting, usability testing, organization, identification, deployment, and reuse of learnwares. Utilizing Beimingwu, the model development for new user tasks can be significantly streamlined, thanks to integrated architecture and engine design, specifying unified learnware structure and scalable APIs, and the integration of various algorithms for learnware identification and reuse. Notably, this is possible even for users with limited data and minimal expertise in machine learning, without compromising the raw data's security. The system facilitates the future research implementations in learnware-related algorithms and systems, and lays the ground for hosting a vast array of learnwares and establishing a learnware ecosystem. The system is fully open-source and we expect the research community to benefit from the system. The system and research toolkit have been released on GitLink and GitHub. | ZhiHao Tan, JianDong Liu, XiaoDong Bi, Peng Tan, QinCheng Zheng, HaiTian Liu, Yi Xie, XiaoChuan Zou, Yang Yu, ZhiHua Zhou | Nanjing University; Nanjing University School of Artificial Intelligence |
|  |  [Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash](https://doi.org/10.1145/3637528.3671574) |  | 0 | This paper investigates an approach to both speed up business decision-making and lower the cost of learning through experimentation by factorizing business policies and employing fractional factorial experimental designs for their evaluation. We illustrate how this method integrates with advances in the estimation of heterogeneous treatment effects, elaborating on its advantages and foundational assumptions. We empirically demonstrate the implementation and benefits of our approach and assess its validity in evaluating consumer promotion policies at DoorDash, which is one of the largest delivery platforms in the US. Our approach discovers a policy with 5% incremental profit at 67% lower implementation cost. | Yixin Tang, Yicong Lin, Navdeep S. Sahni | Stanford GSB, Stanford, CA, USA; DoorDash, Inc., San Francisco, CA, USA |
|  |  [Choosing a Proxy Metric from Past Experiments](https://doi.org/10.1145/3637528.3671543) |  | 0 | In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines. | Nilesh Tripuraneni, Lee Richardson, Alexander D'Amour, Jacopo Soriano, Steve Yadlowsky |  |
|  |  [TnT-LLM: Text Mining at Scale with Large Language Models](https://doi.org/10.1145/3637528.3671647) |  | 0 | Transforming unstructured text into structured and meaningful forms, organized by useful category labels, is a fundamental step in text mining for downstream analysis and application. However, most existing methods for producing label taxonomies and building text-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. | Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W. White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan | University of Washington, Seattle, WA, USA; Microsoft Corporation, Redmond, WA, USA |
|  |  [Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs](https://doi.org/10.1145/3637528.3671583) |  | 0 | In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. In practical scenarios, the demands of non-expert marketers are often abstract and diverse. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. To stimulate the LLMs' reasoning ability, the chain-of-thought (CoT) prompting method is widely used, but existing methods still have some limitations in our scenario: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and concrete questions, making LLMs ineffective when the marketers' demands are abstract and diverse. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation. Then, we adopt a retrieval-based method to conduct analogical reasoning with the help of the reasoning library. The experimental results show that this prompting strategy achieves better performance than the ordinary prompting method. Beyond that, we distill knowledge from super LLMs (GPT-3.5) to fine-tune smaller student LLMs in a multi-task training paradigm, enabling the models to be easily deployed in practical environments. Part of our data and code can be found at https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model. | Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, Jinjie Gu | Ant Group, Hangzhou, China; Zhejiang University & Ant Group, Hangzhou, China; Zhejiang University, Hangzhou, China |
|  |  [COMET: NFT Price Prediction with Wallet Profiling](https://doi.org/10.1145/3637528.3671621) |  | 0 | As the non-fungible token (NFT) market flourishes, price prediction emergesas a pivotal direction for investors gaining valuable insight to maximizereturns. However, existing works suffer from a lack of practical definitionsand standardized evaluations, limiting their practical application. Moreover,the influence of users' multi-behaviour transactions that are publiclyaccessible on NFT price is still not explored and exhibits challenges. In thispaper, we address these gaps by presenting a practical and hierarchical problemdefinition. This approach unifies both collection-level and token-level taskand evaluation methods, which cater to varied practical requirements ofinvestors. To further understand the impact of user behaviours on the variationof NFT price, we propose a general wallet profiling framework and develop aCOmmunity enhanced Multi-bEhavior Transaction graph model, named COMET. COMETprofiles wallets with a comprehensive view and considers the impact of diverserelations and interactions within the NFT ecosystem on NFT price variations,thereby improving prediction performance. Extensive experiments conducted inour deployed system demonstrate the superiority of COMET, underscoring itspotential in the insight toolkit for NFT investors. | Tianfu Wang, Liwei Deng, Chao Wang, Jianxun Lian, Yue Yan, Nicholas Jing Yuan, Qi Zhang, Hui Xiong | Microsoft Research Asia, Beijing, China; Microsoft Inc., Suzhou, China; Microsoft Inc., Beijing, China |
|  |  [Neural Optimization with Adaptive Heuristics for Intelligent Marketing System](https://doi.org/10.1145/3637528.3671591) |  | 0 | Computational marketing has become increasingly important in today's digitalworld, facing challenges such as massive heterogeneous data, multi-channelcustomer journeys, and limited marketing budgets. In this paper, we propose ageneral framework for marketing AI systems, the Neural Optimization withAdaptive Heuristics (NOAH) framework. NOAH is the first general framework formarketing optimization that considers both to-business (2B) and to-consumer(2C) products, as well as both owned and paid channels. We describe key modulesof the NOAH framework, including prediction, optimization, and adaptiveheuristics, providing examples for bidding and content optimization. We thendetail the successful application of NOAH to LinkedIn's email marketing system,showcasing significant wins over the legacy ranking system. Additionally, weshare details and insights that are broadly useful, particularly on: (i)addressing delayed feedback with lifetime value, (ii) performing large-scalelinear programming with randomization, (iii) improving retrieval with audienceexpansion, (iv) reducing signal dilution in targeting tests, and (v) handlingzero-inflated heavy-tail metrics in statistical testing. | Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T. Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Sathiya Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida | LinkedIn Corporation, Seattle, USA; LinkedIn Corporation, New York, NY, USA; LinkedIn Corporation, New York, USA; LinkedIn Corporation, Sunnyvale, USA |
|  |  [On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications](https://doi.org/10.1145/3637528.3671521) |  | 0 | Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diversity of the initial rule set and we empirically find that SpectralRules further improves the quality of the found Pareto front. On two real application scenarios within Alipay, we demonstrate the advantages of our proposed methodology compared to existing work. | Chengyao Wen, Yin Lou |  |
|  |  [Nested Fusion: A Method for Learning High Resolution Latent Structure of Multi-Scale Measurement Data on Mars](https://doi.org/10.1145/3637528.3671596) |  | 0 | The Mars Perseverance Rover represents a generational change in the scale of measurements that can be taken on Mars, however this increased resolution introduces new challenges for techniques in exploratory data analysis. The multiple different instruments on the rover each measures specific properties of interest to scientists, so analyzing how underlying phenomena affect multiple different instruments together is important to understand the full picture. However each instrument has a unique resolution, making the mapping between overlapping layers of data non-trivial. In this work, we introduce Nested Fusion, a method to combine arbitrarily layered datasets of different resolutions and produce a latent distribution at the highest possible resolution, encoding complex interrelationships between different measurements and scales. Our method is efficient for large datasets, can perform inference even on unseen data, and outperforms existing methods of dimensionality reduction and latent analysis on real-world Mars rover data. We have deployed our method Nested Fusion within a Mars science team at NASA Jet Propulsion Laboratory (JPL) and through multiple rounds of participatory design enabled greatly enhanced exploratory analysis workflows for real scientists. To ensure the reproducibility of our work we have open sourced our code on GitHub at https://github.com/pixlise/NestedFusion. | Austin P. Wright, Scott Davidoff, Duen Horng Chau | Georgia Tech, Atlanta, GA, USA; California Institute of Technology, Jet Propulsion Laboratory, Pasadena, CA, USA |
|  |  [TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records](https://doi.org/10.1145/3637528.3671558) |  | 0 | Accurate vehicle trajectory recovery enables providing indispensable data foundations in intelligent urban transportation. However, existing methods face two challenges: i) the inability to process city-wide vehicle trajectories, and ii) the dependence on a substantial amount of accurate GPS trajectories for model training, leading to poor generalization ability. To address these issues, we propose a novel trajectory recovery system based on vehicle snapshots captured by traffic cameras, named TrajRecovery. TrajRecovery consists of three main components: i) Preprocessor processes traffic cameras and vehicle snapshots to provide necessary data for trajectory recovery; ii) Spatial Transfer Probabilistic Model (STPM) integrates road conditions and driver behavior to compute turning probability at intersections; iii) Trajectory Generator utilizes the output probabilities from STPM to recover a continuous and most likely complete trajectory. We evaluate TrajRecovery on two real datasets from a city in China, demonstrating substantial performance gains compared to state-of-the-art methods. Furthermore, our system is deployed in practical applications at Huawei Company, achieving extraordinary profits in business scenarios. | Dongen Wu, Ziquan Fang, Qichen Sun, Lu Chen, Haiyang Hu, Fei Wang, Yunjun Gao | Zhejiang University, Hangzhou, China; Huawei Cloud Computing Technologies Co., Ltd, Hangzhou, China |
|  |  [LaDe: The First Comprehensive Last-mile Express Dataset from Industry](https://doi.org/10.1145/3637528.3671548) |  | 0 | Real-world last-mile express datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile express dataset exists to support research in this field. In this paper, we introduce LaDe, the first publicly available last-mile express dataset with millions of packages from the industry. LaDe has three unique characteristics: (1)Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2)Comprehensive information. It offers original package information, task-event information, as well as couriers' detailed trajecotries and road networks. (3)Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-temporal patterns due to their distinct characteristics such as populations. We verify LaDe on three tasks by running several classical baseline models per task. We believe that the large-scale, comprehensive, diverse feature of LaDe can offer unparalleled opportunities to researchers in the supply chain community, data mining community, and beyond. The dataset and code is publicly available at https://huggingface.co/datasets/Cainiao-AI/LaDe. | Lixia Wu, Haomin Wen, Haoyuan Hu, Xiaowei Mao, Yutong Xia, Ergang Shan, Jianbin Zheng, Junhong Lou, Yuxuan Liang, Liuqing Yang, Roger Zimmermann, Youfang Lin, Huaiyu Wan | Cainiao Network, Hangzhou, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Jiaotong University; National University of Singapore, Singapore, Singapore; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [Xinyu: An Efficient LLM-based System for Commentary Generation](https://doi.org/10.1145/3637528.3671537) |  | 0 | Commentary provides readers with a deep understanding of events by presenting diverse arguments and evidence. However, creating commentary is a time-consuming task, even for skilled commentators. Large language models (LLMs) have simplified the process of natural language generation, but their direct application in commentary creation still faces challenges due to unique task requirements. These requirements can be categorized into two levels: 1) fundamental requirements, which include creating well-structured and logically consistent narratives, and 2) advanced requirements, which involve generating quality arguments and providing convincing evidence. In this paper, we introduce Xinyu, an efficient LLM-based system designed to assist commentators in generating Chinese commentaries. To meet the fundamental requirements, we deconstruct the generation process into sequential steps, proposing targeted strategies and supervised fine-tuning (SFT) for each step. To address the advanced requirements, we present an argument ranking model for arguments and establish a comprehensive evidence database that includes up-to-date events and classic books, thereby strengthening the substantiation of the evidence with retrieval augmented generation (RAG) technology. To evaluate the generated commentaries more fairly, corresponding to the two-level requirements, we introduce a comprehensive evaluation metric that considers five distinct perspectives in commentary generation. Our experiments confirm the effectiveness of our proposed system. We also observe a significant increase in the efficiency of commentators in real-world scenarios, with the average time spent on creating a commentary dropping from 4 hours to 20 minutes. Importantly, such an increase in efficiency does not compromise the quality of the commentaries. | Yiquan Wu, Bo Tang, Chenyang Xi, Yu Yu, Pengyu Wang, Yifei Liu, Kun Kuang, Haiying Deng, Zhiyu Li, Feiyu Xiong, Jie Hu, Peng Cheng, Zhonghao Wang, Yi Wang, Yi Luo, Mingchuan Yang | Zhejiang University, Hangzhou, China; Research Institute of China Telecom, Beijing, China; Institute for Advanced Algorithms Research, Shanghai, China; Northeastern University, Shenyang, China; State Key Laboratory of Media Convergence Production Technology and Systems, Beijing, China |
|  |  [DuMapNet: An End-to-End Vectorization System for City-Scale Lane-Level Map Generation](https://doi.org/10.1145/3637528.3671579) |  | 0 | Generating city-scale lane-level maps faces significant challenges due to the intricate urban environments, such as blurred or absent lane markings. Additionally, a standard lane-level map requires a comprehensive organization of lane groupings, encompassing lane direction, style, boundary, and topology, yet has not been thoroughly examined in prior research. These obstacles result in labor-intensive human annotation and high maintenance costs. This paper overcomes these limitations and presents an industrial-grade solution named DuMapNet that outputs standardized, vectorized map elements and their topology in an end-to-end paradigm. To this end, we propose a group-wise lane prediction (GLP) system that outputs vectorized results of lane groups by meticulously tailoring a transformer-based network. Meanwhile, to enhance generalization in challenging scenarios, such as road wear and occlusions, as well as to improve global consistency, a contextual prompts encoder (CPE) module is proposed, which leverages the predicted results of spatial neighborhoods as contextual information. Extensive experiments conducted on large-scale real-world datasets demonstrate the superiority and effectiveness of DuMapNet. Additionally, DuMapNet has already been deployed in production at Baidu Maps since June 2023, supporting lane-level map generation tasks for over 360 cities while bringing a 95% reduction in costs. This demonstrates that DuMapNet serves as a practical and cost-effective industrial solution for city-scale lane-level map generation. | Deguo Xia, Weiming Zhang, Xiyan Liu, Wei Zhang, Chenting Gong, Jizhou Huang, Mengmeng Yang, Diange Yang | Tsinghua University & Baidu Inc., Beijing, China; Tsinghua University, Beijing, China; Baidu Inc., Beijing, China |
|  |  [VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for Enhanced Detection](https://doi.org/10.1145/3637528.3671527) |  | 0 | Fraud detection presents a challenging task characterized by ever-evolving fraud patterns and scarce labeled data. Existing methods predominantly rely on graph-based or sequence-based approaches. While graph-based approaches connect users through shared entities to capture structural information, they remain vulnerable to fraudsters who can disrupt or manipulate these connections. In contrast, sequence-based approaches analyze users' behavioral patterns, offering robustness against tampering but overlooking the interactions between similar users. Inspired by cohort analysis in retention and healthcare, this paper introduces VecAug, a novel cohort-augmented learning framework that addresses these challenges by enhancing the representation learning of target users with personalized cohort information. To this end, we first propose a vector burn-in technique for automatic cohort identification, which retrieves a task-specific cohort for each target user. Then, to fully exploit the cohort information, we introduce an attentive cohort aggregation technique for augmenting target user representations. To improve the robustness of such cohort augmentation, we also propose a novel label-aware cohort neighbor separation mechanism to distance negative cohort neighbors and calibrate the aggregated cohort information. By integrating this cohort information with target user representations, VecAug enhances the modeling capacity and generalization capabilities of the model to be augmented. Our framework is flexible and can be seamlessly integrated with existing fraud detection models. We deploy our framework on e-commerce platforms and evaluate it on three fraud detection datasets, and results show that VecAug improves the detection performance of base models by up to 2.48% in AUC and 22.5% in R@P_0.9, outperforming state-of-the-art methods significantly. | Fei Xiao, Shaofeng Cai, Gang Chen, H. V. Jagadish, Beng Chin Ooi, Meihui Zhang | Zhejiang University, Hangzhou, China; University of Michigan, Ann Arbor, USA; Beijing Institute of Technology, Beijing, China; National University of Singapore & Shopee Singapore, Singapore, Singapore; National University of Singapore, Singapore, Singapore |
|  |  [Weather Knows What Will Occur: Urban Public Nuisance Events Prediction and Control with Meteorological Assistance](https://doi.org/10.1145/3637528.3671639) |  | 0 | Urban public nuisance events, like garbage exposure, illegal parking, facilities damage, and etc., impair the quality of life for city residents. Predicting and controlling these nuisances is crucial but complicated due to their ties to subjective and psychological factors. In this study, we reveal a significant correlation between such nuisances and meteorological indicators, influenced by the impact of climate on people's psychological states. We employ meteorology predictions that are integrated in Hawkes processes to enhance the accuracy of predicting the category and timing of these nuisances. To this end, we propose Spatial-Temporal Two-Tower Transformer (ST-T3), which simultaneously considers spatial data and further improves the prediction accuracy. Evaluated by about three-year data from both downtown and suburban Shanghai, our method outperforms both traditional and advanced prediction systems. We share a portion of the de-identified dataset for open research. | Yi Xie, Tianyu Qiu, Yun Xiong, Xiuqi Huang, Xiaofeng Gao, Chao Chen, Qiang Wang, Haihong Li | Shanghai Key Lab of Data Science, School of Computer Science, Fudan University, Shanghai, China; College of Computer Science, Chongqing University, Chongqing, China; MoE Key Lab of Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, China; Meteorological Disaster Prevention Centre, Shanghai Meteorological Bureau, Shanghai, China |
|  |  [Microservice Root Cause Analysis With Limited Observability Through Intervention Recognition in the Latent Space](https://doi.org/10.1145/3637528.3671530) |  | 0 | Many failure root cause analysis (RCA) algorithms for microservices have been proposed with the widespread adoption of microservices systems. Existing algorithms generally focus on RCA with ranking single-level (e.g. metric-level or service-level) root cause candidates (RCCs) with comprehensive monitoring metrics. However, many heterogeneous RCCs exist with limited observability in real-world microservices systems. Further, we find that the limited observability may result in inaccurate RCA through real-world failures in eBay. In this paper, for the first time, we propose to "model RCCs as latent variables". The core idea is to infer the status of RCCs as latent variables with related monitoring metrics instead of directly extracting features from only the observable metrics. Based on this, we propose LatentScope, an unsupervised RCA framework with heterogeneous RCCs under limited observability. A dual-space graph is proposed to model both observable and unobservable variables, with many-to-many relationships between spaces. To achieve fast inference of latent variables and RCA, we propose the LatentRegressor algorithm, which includes Regression-based Latent-space Intervention Recognition (RLIR) to achieve intervention recognition-based RCA in latent space. LatentScope has been deployed in eBay's production environment and evaluated on both eBay's real-world failures and a testbed dataset. The evaluation results show that, compared with baseline algorithms, our model significantly improves the Top-1 recall by 9.7%-57.9%. The source code of LatentScope and the dataset are available at https://github.com/NetManAIOps/LatentScope. | Zhe Xie, Shenglin Zhang, Yitong Geng, Yao Zhang, Minghua Ma, Xiaohui Nie, Zhenhe Yao, Longlong Xu, Yongqian Sun, Wentao Li, Dan Pei | BNRist, Tsinghua University, Beijing, China; Microsoft, Redmond, USA; eBay Inc., Shanghai, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; Nankai University, Tianjin, China |
|  |  [Understanding the Weakness of Large Language Model Agents within a Complex Android Environment](https://doi.org/10.1145/3637528.3671650) |  | 0 | Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to address the issue of non-unique solutions. Our findings reveal that even state-of-the-art LLM agents struggle in cross-APP scenarios and adhering to specific constraints. Additionally, we identify a lack of four key capabilities, i.e. understanding, reasoning, exploration, and reflection, as primary reasons for the failure of LLM agents. Furthermore, we provide empirical analysis on the failure of reflection, and improve the success rate by 27% with our proposed exploration strategy. This work is the first to present valuable insights in understanding fine-grained weakness of LLM agents, and offers a path forward for future research in this area. Environment, benchmark, prompt, and evaluation code for AndroidArena are released at https://github.com/AndroidArenaAgent/AndroidArena. | Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao | Microsoft Research, Beijing, China; Peking University, Beijing, China |
|  |  [XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques](https://doi.org/10.1145/3637528.3671595) |  | 0 | Reinforcement Learning (RL) has demonstrated substantial potential acrossdiverse fields, yet understanding its decision-making process, especially inreal-world scenarios where rationality and safety are paramount, is an ongoingchallenge. This paper delves in to Explainable RL (XRL), a subfield ofExplainable AI (XAI) aimed at unravelling the complexities of RL models. Ourfocus rests on state-explaining techniques, a crucial subset within XRLmethods, as they reveal the underlying factors influencing an agent's actionsat any given time. Despite their significant role, the lack of a unifiedevaluation framework hinders assessment of their accuracy and effectiveness. Toaddress this, we introduce XRL-Bench, a unified standardized benchmark tailoredfor the evaluation and comparison of XRL methods, encompassing three mainmodules: standard RL environments, explainers based on state importance, andstandard evaluators. XRL-Bench supports both tabular and image data for stateexplanation. We also propose TabularSHAP, an innovative and competitive XRLmethod. We demonstrate the practical utility of TabularSHAP in real-worldonline gaming services and offer an open-source benchmark platform for thestraightforward implementation and evaluation of XRL methods. Our contributionsfacilitate the continued progression of XRL technology. | Yu Xiong, Zhipeng Hu, Ye Huang, Runze Wu, Kai Guan, Xingchen Fang, Ji Jiang, Tianze Zhou, Yujing Hu, Haoyu Liu, Tangjie Lyu, Changjie Fan | Fuxi AI Lab, NetEase Inc., Hangzhou, China; Fuxi AI Lab, NetEase Inc., Hangzhou, Zhejiang, China |
|  |  [FedGTP: Exploiting Inter-Client Spatial Dependency in Federated Graph-based Traffic Prediction](https://doi.org/10.1145/3637528.3671613) |  | 0 | Graph-based methods have witnessed tremendous success in traffic prediction, largely attributed to their superior ability in capturing and modeling spatial dependencies. However, urban-scale traffic data are usually distributed among various owners, limited in sharing due to privacy restrictions. This fragmentation of data severely hinders interaction across clients, impeding the utilization of inter-client spatial dependencies. Existing studies have yet to address this non-trivial issue, thereby leading to sub-optimal performance. To fill this gap, we propose FedGTP, a new federated graph-based traffic prediction framework that promotes adaptive exploitation of inter-client spatial dependencies to recover close-to-optimal performance complying with privacy regulations like GDPR. We validate FedGTP via large-scale application-driven experiments on real-world datasets. Extensive baseline comparison, ablation study and case study demonstrate that FedGTP indeed surpasses existing methods through fully recovering inter-client spatial dependencies, achieving 21.08%, 13.48%, 19.90% decrease on RMSE, MAE and MAPE, respectively. Our code is available at https://github.com/LarryHawkingYoung/KDD2024_FedGTP | Linghua Yang, Wantong Chen, Xiaoxi He, Shuyue Wei, Yi Xu, Zimu Zhou, Yongxin Tong | School of Data Science, City University of Hong Kong, Hong Kong, China; SKLCCSE Lab, Institute of Artificial Intelligence, Beihang University, Beijing, China; Faculty of Science and Technology, University of Macau, Macau, China; SKLCCSE Lab, Beihang University, Beijing, China |
|  |  [OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning](https://doi.org/10.1145/3637528.3671582) |  | 0 | Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cover 30+ evaluation metrics. Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings. Notably, in a financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can outperform GPT-4 by a significant margin, while the model obtained through individual training cannot, demonstrating strong motivation for clients to participate in FL. The code is available at https://github.com/rui-ye/OpenFedLLM. The full version of our paper is available at https://arxiv.org/pdf/2402.06954. | Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen | Shanghai Jiao Tong University & Shanghai AI Laboratory, Shanghai, China; University of Southern California, Los Angeles, USA; Shanghai Jiao Tong University, Shanghai, China; Zhejiang University, Zhejiang, China |
|  |  [PAIL: Performance based Adversarial Imitation Learning Engine for Carbon Neutral Optimization](https://doi.org/10.1145/3637528.3671611) |  | 0 | Achieving carbon neutrality within industrial operations has become increasingly imperative for sustainable development. It is both a significant challenge and a key opportunity for operational optimization in industry 4.0. In recent years, Deep Reinforcement Learning (DRL) based methods offer promising enhancements for sequential optimization processes and can be used for reducing carbon emissions. However, existing DRL methods need a pre-defined reward function to assess the impact of each action on the final sustainable development goals (SDG). In many real applications, such a reward function cannot be given in advance. To address the problem, this study proposes a Performance based Adversarial Imitation Learning (PAIL) engine. It is a novel method to acquire optimal operational policies for carbon neutrality without any pre-defined action rewards. Specifically, PAIL employs a Transformer-based policy generator to encode historical information and predict following actions within a multi-dimensional space. The entire action sequence will be iteratively updated by an environmental simulator. Then PAIL uses a discriminator to minimize the discrepancy between generated sequences and real-world samples of high SDG. In parallel, a Q-learning framework based performance estimator is designed to estimate the impact of each action on SDG. Based on these estimations, PAIL refines generated policies with the rewards from both discriminator and performance estimator. PAIL is evaluated on multiple real-world application cases and datasets. The experiment results demonstrate the effectiveness of PAIL comparing to other state-of-the-art baselines. In addition, PAIL offers meaningful interpretability for the optimization in carbon neutrality. | Yuyang Ye, LuAn Tang, Haoyu Wang, Runlong Yu, Wenchao Yu, Erhu He, Haifeng Chen, Hui Xiong | Department of Data Science and System Security, NEC Laboratories, Princeton, NJ, USA; Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA |
|  |  [SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing](https://doi.org/10.1145/3637528.3671586) |  | 0 | Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis. | Changchang Yin, PinYu Chen, Bingsheng Yao, Dakuo Wang, Jeffrey M. Caterino, Ping Zhang | The Ohio State University, Columbus, OH, USA; IBM Research, Yorktown Heights, NY, USA; Northeastern University, Boston, MA, USA; The Ohio State University Wexner Medical Center, Columbus, OH, USA |
|  |  [Pre-trained KPI Anomaly Detection Model Through Disentangled Transformer](https://doi.org/10.1145/3637528.3671522) |  | 0 | In large-scale online service systems, numerous Key Performance Indicators (KPIs), such as service response time and error rate, are gathered in a time-series format. KPI Anomaly Detection (KAD) is a critical data mining problem due to its widespread applications in real-world scenarios. However, KAD faces the challenges of dealing with KPI heterogeneity and noisy data. We propose KAD-Disformer, a KPI Anomaly Detection approach through Disentangled Transformer. KAD-Disformer pre-trains a model on existing accessible KPIs, and the pre-trained model can be effectively "fine-tuned" to unseen KPI using only a handful of samples from the unseen KPI. We propose a series of innovative designs, including disentangled projection for transformer, unsupervised few-shot fine-tuning (uTune), and denoising modules, each of which significantly contributes to the overall performance. Our extensive experiments demonstrate that KAD-Disformer surpasses the state-of-the-art universal anomaly detection model by 13% in F1-score and achieves comparable performance using only 1/8 of the finetuning samples saving about 25 hours. KAD-Disformer has been successfully deployed in the real-world cloud system serving millions of users, attesting to its feasibility and robustness. Our code is available at https://github.com/NetManAIOps/KAD-Disformer. | Zhaoyang Yu, Changhua Pei, Xin Wang, Minghua Ma, Chetan Bansal, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, Xidao Wen, Jianhui Li, Gaogang Xie, Dan Pei | Tsinghua University & BNRist, Beijing, China; Microsoft, Beijing, China; Microsoft, Redmond, USA; Stony Brook University, New York, USA; BizSeer Technology, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China |
|  |  [An Offline Meta Black-box Optimization Framework for Adaptive Design of Urban Traffic Light Management Systems](https://doi.org/10.1145/3637528.3671606) |  | 0 | Complex urban road networks with high vehicle occupancy frequently face severe traffic congestion. Designing an effective strategy for managing multiple traffic lights plays a crucial role in managing congestion. However, most current traffic light management systems rely on human-crafted decisions, which may not adapt well to diverse traffic patterns. In this paper, we delve into two pivotal design components of the traffic light management system that can be dynamically adjusted to various traffic conditions: phase combination and phase time allocation. While numerous studies have sought an efficient strategy for managing traffic lights, most of these approaches consider a fixed traffic pattern and are limited to relatively small road networks. To overcome these limitations, we introduce a novel and practical framework to formulate the optimization of such design components using an offline meta black-box optimization. We then present a simple yet effective method to efficiently find a solution for the aforementioned problem. In our framework, we first collect an offline meta dataset consisting of pairs of design choices and corresponding congestion measures from various traffic patterns. After collecting the dataset, we employ the Attentive Neural Process (ANP) to predict the impact of the proposed design on congestion across various traffic patterns with well-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a surrogate model, is utilized to find an optimal design for unseen traffic patterns through limited online simulations. Our experiment results show that our method outperforms state-of-the-art baselines on complex road networks in terms of the number of waiting vehicles. Surprisingly, the deployment of our method into a real-world traffic system was able to improve traffic throughput by 4.80% compared to the original strategy. | Taeyoung Yun, Kanghoon Lee, Sujin Yun, Ilmyung Kim, WonWoo Jung, MinCheol Kwon, Kyujin Choi, Yoohyeon Lee, Jinkyoo Park | KAIST, Daejeon, Republic of Korea; Korea Telecom, Seoul, Republic of Korea |
|  |  [OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining](https://doi.org/10.1145/3637528.3672354) |  | 0 | With the rapid proliferation of scientific literature, versatile academicknowledge services increasingly rely on comprehensive academic graph mining.Despite the availability of public academic graphs, benchmarks, and datasets,these resources often fall short in multi-aspect and fine-grained annotations,are constrained to specific task types and domains, or lack underlying realacademic graphs. In this paper, we present OAG-Bench, a comprehensive,multi-aspect, and fine-grained human-curated benchmark based on the OpenAcademic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,and 120+ experimental results to date. We propose new data annotationstrategies for certain tasks and offer a suite of data pre-processing codes,algorithm implementations, and standardized evaluation protocols to facilitateacademic graph mining. Extensive experiments reveal that even advancedalgorithms like large language models (LLMs) encounter difficulties inaddressing key challenges in certain tasks, such as paper source tracing andscholar profiling. We also introduce the Open Academic Graph Challenge(OAG-Challenge) to encourage community input and sharing. We envisage thatOAG-Bench can serve as a common ground for the community to evaluate andcompare algorithms in academic graph mining, thereby accelerating algorithmdevelopment and advancement in this field. OAG-Bench is accessible athttps://www.aminer.cn/data/. | Fanjin Zhang, Shijie Shi, Yifan Zhu, Bo Chen, Yukuo Cen, Jifan Yu, Yelin Chen, Lulu Wang, Qingfei Zhao, Yuqing Cheng, Tianyi Han, Yuwei An, Dan Zhang, Weng Lam Tam, Kun Cao, Yunhe Pang, Xinyu Guan, Huihui Yuan, Jian Song, Xiaoyan Li, Yuxiao Dong, Jie Tang | Zhipu AI, Beijing, China; Tsinghua University, Beijing, China; Biendata, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China |
|  |  [Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English](https://doi.org/10.1145/3637528.3671554) |  | 0 | Despite Spanish's pivotal role in the global finance industry, a pronouncedgap exists in Spanish financial natural language processing (NLP) andapplication studies compared to English, especially in the era of largelanguage models (LLMs). To bridge this gap, we unveil Toisón de Oro, thefirst bilingual framework that establishes instruction datasets, finetunedLLMs, and evaluation benchmark for financial LLMs in Spanish joint withEnglish. We construct a rigorously curated bilingual instruction datasetincluding over 144K Spanish and English samples from 15 datasets covering 7tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingualfinancial applications. We evaluate our model and existing LLMs using FLARE-ES,the first comprehensive bilingual evaluation benchmark with 21 datasetscovering 9 tasks. The FLARE-ES benchmark results reveal a significantmultilingual performance gap and bias in existing LLMs. FinMA-ES models surpassSOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategicinstruction tuning and leveraging data from diverse linguistic resources,highlighting the positive impact of cross-linguistic transfer. All ourdatasets, models, and benchmarks have been released. | Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro LopezLira, XiaoYang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, Qianqian Xie | Sichuan University, Chengdu, Sichuan, China; University of Florida, Gainesville, USA; The Fin AI, Singapore, Singapore; Wuhan University, Wuhan, Hubei, China; Columbia University, New York, NY, USA; Augusta University, Augusta, USA; The University of Manchester, Manchester, United Kingdom |
|  |  [Large Language Model with Curriculum Reasoning for Visual Concept Recognition](https://doi.org/10.1145/3637528.3671653) |  | 0 | Visual concept recognition aims to capture the basic attributes of an image and reason about the relationships among them to determine whether the image satisfies a certain concept, and has been widely used in various tasks such as human action recognition and image risk warning. Most existing works adopt deep neural networks for visual concept recognition, which are black-box and incomprehensible to humans, thus making them unacceptable for sensitive domains such as prohibited event detection and risk early warning etc. To address this issue, we propose to combine large language model (LLM) with explainable symbolic reasoning via curriculum reweighting to increase the interpretability and accuracy of visual concept recognition in this paper. However, realizing this goal is challenging given that i) the performance of symbolic representations are limited by the lack of annotated reasoning symbols and rules for most tasks, and ii) the LLMs may suffer from knowlege hallucination and dynamic open environment. To address these issues, in this paper, we propose CurLLM-Reasoner, a curriculum reasoning method based on symbolic reasoning and large language model for visual concept recognition. Specifically, we propose a novel rule enhancement module with a tool library, which fully leverage the reasoning capability of large language models and can generate human-understandable rules without any annotation. We further propose a curriculum data resampling methodology to help the large language model accurately extract from easy to complex rules at different reasoning stages. Extensive experiments on various datasets demonstrate that CurLLM-Reasoner can achieve the state-of-the-art visual concept recognition results with explainable rules while free of human annotations. | Yipeng Zhang, Xin Wang, Hong Chen, Jiapei Fan, Weigao Wen, Hui Xue, Hong Mei, Wenwu Zhu | Alibaba Group, Hangzhou, China; DCST, BNRist, Tsinghua University, Beijing, China; MoE Lab, Peking University, Beijing, China; DCST, Tsinghua University, Beijing, China |
|  |  [GraSS: Combining Graph Neural Networks with Expert Knowledge for SAT Solver Selection](https://doi.org/10.1145/3637528.3671627) |  | 0 | Boolean satisfiability (SAT) problems are routinely solved by SAT solvers inreal-life applications, yet solving time can vary drastically between solversfor the same instance. This has motivated research into machine learning modelsthat can predict, for a given SAT instance, which solver to select amongseveral options. Existing SAT solver selection methods all rely on somehand-picked instance features, which are costly to compute and ignore thestructural information in SAT graphs. In this paper we present GraSS, a novelapproach for automatic SAT solver selection based on tripartite graphrepresentations of instances and a heterogeneous graph neural network (GNN)model. While GNNs have been previously adopted in other SAT-related tasks, theydo not incorporate any domain-specific knowledge and ignore the runtimevariation introduced by different clause orders. We enrich the graphrepresentation with domain-specific decisions, such as novel node featuredesign, positional encodings for clauses in the graph, a GNN architecturetailored to our tripartite graphs and a runtime-sensitive loss function.Through extensive experiments, we demonstrate that this combination of rawrepresentations and domain-specific choices leads to improvements in runtimefor a pool of seven state-of-the-art solvers on both an industrial circuitdesign benchmark, and on instances from the 20-year Anniversary Track of the2022 SAT Competition. | Zhanguang Zhang, Didier Chételat, Joseph Cotnareanu, Amur Ghose, Wenyi Xiao, HuiLing Zhen, Yingxue Zhang, Jianye Hao, Mark Coates, Mingxuan Yuan | McGill University, Montreal, Canada; Huawei Noah's Ark Lab, Beijing, China; Huawei Noah's Ark Lab, Montreal, Canada; Huawei Noah's Ark Lab, Hong Kong, China |
|  |  [Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns](https://doi.org/10.1145/3637528.3671587) |  | 0 | The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with INterpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their associated dietary patterns. Specifically, in Diet-ODIN, we first construct an HG to comprehensively incorporate both dietary and health-related information, and then we devise a holistic graph learning framework with noise reduction to fully capitalize both users' individual dietary habits and shared dietary patterns for the detection of users with opioid misuse. To further delve into the intricate correlations between dietary patterns and opioid misuse, we exploit an LLM by utilizing the knowledge obtained from the graph learning model for interpretation. The extensive experimental results based on our established benchmark with quantitative and qualitative measures demonstrate the outstanding performance of Diet-ODIN on exploring the complex interplay between opioid misuse and dietary patterns, by comparison with state-of-the-art baseline methods. Our code, built benchmark and system demo are available at https://github.com/JasonZhangzy1757/Diet-ODIN. | Zheyuan Zhang, Zehong Wang, Shifu Hou, Evan Hall, Landon Bachman, Jasmine White, Vincent Galassi, Nitesh V. Chawla, Chuxu Zha, Yanfang Ye | Purdue University, West Lafayette, IN, USA; University of Notre Dame, Notre Dame, IN, USA; Brandeis University, Waltham, MA, USA |
|  |  [TACCO: Task-guided Co-clustering of Clinical Concepts and Patient Visits for Disease Subtyping based on EHR Data](https://doi.org/10.1145/3637528.3671594) |  | 0 | The growing availability of well-organized Electronic Health Records (EHR) data has enabled the development of various machine learning models towards disease risk prediction. However, existing risk prediction methods overlook the heterogeneity of complex diseases, failing to model the potential disease subtypes regarding their corresponding patient visits and clinical concept subgroups. In this work, we introduce TACCO, a novel framework that jointly discovers clusters of clinical concepts and patient visits based on a hypergraph modeling of EHR data. Specifically, we develop a novel self-supervised co-clustering framework that can be guided by the risk prediction task of specific diseases. Furthermore, we enhance the hypergraph model of EHR data with textual embeddings and enforce the alignment between the clusters of clinical concepts and patient visits through a contrastive objective. Comprehensive experiments conducted on the public MIMIC-III dataset and Emory internal CRADLE dataset over the downstream clinical tasks of phenotype classification and cardiovascular risk prediction demonstrate an average 31.25% performance improvement compared to traditional ML baselines and a 5.26% improvement on top of the vanilla hypergraph model without our co-clustering mechanism. In-depth model analysis, clustering results analysis, and clinical case studies further validate the improved utilities and insightful interpretations delivered by TACCO. Code is available at https://github.com/PericlesHat/TACCO. | Ziyang Zhang, Hejie Cui, Ran Xu, Yuzhang Xie, Joyce C. Ho, Carl Yang | Emory University, Atlanta, GA, USA |
|  |  [DUE: Dynamic Uncertainty-Aware Explanation Supervision via 3D Imputation](https://doi.org/10.1145/3637528.3671641) |  | 0 | Explanation supervision aims to enhance deep learning models by integrating additional signals to guide the generation of model explanations, showcasing notable improvements in both the predictability and explainability of the model. However, the application of explanation supervision to higher-dimensional data, such as 3D medical images, remains an under-explored domain. Challenges associated with supervising visual explanations in the presence of an additional dimension include: 1) spatial correlation changed, 2) lack of direct 3D annotations, and 3) uncertainty varies across different parts of the explanation. To address these challenges, we propose a Dynamic Uncertainty-aware Explanation supervision (DUE\footnoteCode available at: https://github.com/AlexQilong/DUE.) framework for 3D explanation supervision that ensures uncertainty-aware explanation guidance when dealing with sparsely annotated 3D data with diffusion-based 3D interpolation. Our proposed framework is validated through comprehensive experiments on diverse real-world medical imaging datasets. The results demonstrate the effectiveness of our framework in enhancing the predictability and explainability of deep learning models in the context of medical imaging diagnosis applications. | Qilong Zhao, Yifei Zhang, Mengdan Zhu, Siyi Gu, Yuyang Gao, Xiaofeng Yang, Liang Zhao | The Home Depot, Atlanta, GA, USA; Stanford University, Palo Alto, CA, USA; Emory University, Atlanta, GA, USA |
|  |  [Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy](https://doi.org/10.1145/3637528.3671614) |  | 0 | As Large Language Models (LLMs) have made significant advancements across various tasks, such as question answering, translation, text summarization, and dialogue systems, the need for accuracy in information becomes crucial, especially for serious financial products serving billions of users like Alipay. However, for a real-world product serving millions of users, the inference speed of LLMs becomes a critical factor compared to a mere experimental model. Hence, this paper presents a generic framework for accelerating the inference process, resulting in a substantial increase in speed and cost reduction for our LLM-based scenarios, with lossless generation accuracy. In the traditional inference process, each token is generated sequentially by the LLM, leading to a time consumption proportional to the number of generated tokens. To enhance this process, our framework, named lookahead, introduces a multi-branch strategy. Instead of generating a single token at a time, we propose a Trie-based retrieval and verification mechanism to be able to accept several tokens at a forward step. Our strategy offers two distinct advantages: (1) it guarantees absolute correctness of the output, avoiding any approximation algorithms, and (2) the worst-case performance of our approach could be comparable with the performance of the conventional process. We conduct extensive experiments to demonstrate the significant improvements achieved by applying our inference acceleration framework. Our framework has been widely deployed in Alipay since April 2023, and obtained remarkable 2.66x to 6.26x speedup. Our code is available at https://github.com/alipay/PainlessInferenceAcceleration. | Yao Zhao, Zhitian Xie, Chen Liang, Chenyi Zhuang, Jinjie Gu | Ant Group, Hangzhou, China |
|  |  [Decision Focused Causal Learning for Direct Counterfactual Marketing Optimization](https://doi.org/10.1145/3637528.3672353) |  | 0 | Marketing optimization plays an important role to enhance user engagement in online Internet platforms. Existing studies usually formulate this problem as a budget allocation problem and solve it by utilizing two fully decoupled stages, i.e., machine learning (ML) and operation research (OR). However, the learning objective in ML does not take account of the downstream optimization task in OR, which causes that the prediction accuracy in ML may be not positively related to the decision quality. Decision Focused Learning (DFL) integrates ML and OR into an end-to-end framework, which takes the objective of the downstream task as the decision loss function and guarantees the consistency of the optimization direction between ML and OR. However, deploying DFL in marketing is non-trivial due to multiple technological challenges. Firstly, the budget allocation problem in marketing is a 0-1 integer stochastic programming problem and the budget is uncertain and fluctuates a lot in real-world settings, which is beyond the general problem background in DFL. Secondly, the counterfactual in marketing causes that the decision loss cannot be directly computed and the optimal solution can never be obtained, both of which disable the common gradient-estimation approaches in DFL. Thirdly, the OR solver is called frequently to compute the decision loss during model training in DFL, which produces huge computational cost and cannot support large-scale training data. In this paper, we propose a decision focused causal learning framework (DFCL) for direct counterfactual marketing optimization, which overcomes the above technological challenges. Both offline experiments and online A/B testing demonstrate the effectiveness of DFCL over the state-of-the-art methods. Currently, DFCL has been deployed in several marketing scenarios in Meituan, one of the largest online food delivery platform in the world. | Hao Zhou, Rongxiao Huang, Shaoming Li, Guibin Jiang, Jiaqi Zheng, Bing Cheng, Wei Lin | State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Meituan, Beijing, China; State Key Laboratory for Novel Software Technology, Nanjing University & Meituan, Nanjing, China |
|  |  [A Hands-on Introduction to Time Series Classification and Regression](https://doi.org/10.1145/3637528.3671443) |  | 0 | Time series classification and regression are rapidly evolving fields that find areas of application in all domains of machine learning and data science. This hands on tutorial will provide an accessible overview of the recent research in these fields, using code examples to introduce the process of implementing and evaluating an estimator. We will show how to easily reproduce published results and how to compare a new algorithm to state-of-the-art. Finally, we will work through real world examples from the field of Electroencephalogram (EEG) classification and regression. EEG machine learning tasks arise in medicine, brain-computer interface research and psychology. We use these problems to how to compare algorithms on problems from a single domain and how to deal with data with different characteristics, such as missing values, unequal length and high dimensionality. The latest advances in the fields of time series classification and regression are all available through the aeon toolkit, an open source, scikit-learn compatible framework for time series machine learning which we use to provide our code examples. | Anthony J. Bagnall, Matthew Middlehurst, Germain Forestier, Ali IsmailFawaz, Antoine Guillaume, David GuijoRubio, Chang Wei Tan, Angus Dempster, Geoffrey I. Webb | IRIMAS, Université de Haute-Alsace, Mulhouse, France; University of Southampton, Southampton, United Kingdom; Novahe & Constellation, Saint-Cloud, France; Universidad de Córdoba, Córdoba, Spain; Monash University, Melbourne, Australia |
|  |  [Multi-modal Data Processing for Foundation Models: Practical Guidances and Use Cases](https://doi.org/10.1145/3637528.3671441) |  | 0 | In the foundation models era, efficiently processing multi-modal data is crucial. This tutorial covers key techniques for multi-modal data processing and introduces the open-source Data-Juicer system, designed to tackle the complexities of data variety, quality, and scale. Participants will learn how to use Data-Juicer's operators and tools for formatting, mapping, filtering, deduplicating, and selecting multi-modal data efficiently and effectively. They will also be familiar with the Data-Juicer Sandbox Lab, where users can easily experiment with diverse data recipes that represent methodical sequences of operators and streamline the creation of scalable data processing pipelines. This experience solidifies the concepts discussed, as well as provides a space for innovation and exploration, highlighting how data recipes can be optimized and deployed in high-performance distributed environments. By the end of this tutorial, attendees will be equipped with the practical knowledge and skills to navigate the multi-modal data processing for foundation models. They will leave with actionable knowledge with an industrial open-source system and an enriched perspective on the importance of high-quality data in AI, poised to implement sustainable and scalable solutions in their projects. The system and related materials are available at https://github.com/modelscope/data-juicer. | Daoyuan Chen, Yaliang Li, Bolin Ding | Alibaba Group, Bellevue, USA; Alibaba Group, Hangzhou, China |
|  |  [DARE to Diversify: DAta Driven and Diverse LLM REd Teaming](https://doi.org/10.1145/3637528.3671444) |  | 0 | Large language models (LLMs) have been rapidly adopted, as showcased by ChatGPT's overnight popularity, and are integrated in products used by millions of people every day, such as search engines and productivity suites. Yet the societal impact of LLMs, encompassing both benefits and harms, is not well understood. Inspired by cybersecurity practices, red-teaming is emerging as a technique to uncover model vulnerabilities. Despite increasing attention from industry, academia, and government centered around red-teaming LLMs, such efforts are still limited in the diversity of the red-teaming focus, approaches and participants. Importantly, given that LLMs are becoming ubiquitous, it is imperative that red-teaming efforts are scaled out to include large segments of the research, practitioners and the people whom are directly affected by the deployment of these systems. The goal of this tutorial is two fold. First, we introduce the topic of LLM red-teaming by reviewing the state of the art for red-teaming practices, from participatory events to automatic AI-focused approaches, exposing the gaps in both the techniques and coverage of the targeted harms. Second, we plan to engage the audience in a hands-on and interactive exercise in LLM red-teaming to showcase the ease (or difficulty) of exposing model vulnerabilities, contingent on both the targeted harm and model capabilities. We believe that the KDD community of researchers and practitioners are in a unique position to address the existing gaps in red-teaming approaches, given their longstanding research and practice of extracting knowledge from data. | Manish Nagireddy, Bernat Guillen Pegueroles, Ioana Baldini | Google, Zurich, CH; IBM Research, Cambridge, Massachusetts, USA; IBM Research, Yorktown Heights, New York, USA |
|  |  [Privacy-Preserving Federated Learning using Flower Framework](https://doi.org/10.1145/3637528.3671447) |  | 0 | AI projects often face the challenge of limited access to meaningful amounts of training data. In traditional approaches, collecting data in a central location can be problematic, especially in industry settings with sensitive and distributed data. However, there is a solution -"moving the computation to the data" through Federated Learning. Federated Learning, a distributed machine learning approach, offers a promising solution by enabling model training across devices. It is a data minimization approach where direct access to data is not required. Furthermore, federated learning can be combined with techniques like differential privacy, secure aggregation, homomorphic encryption, and others, to further enhance privacy protection. In this hands-on tutorial, we delve into the realm of privacy-preserving machine learning using federated learning, leveraging the Flower framework which is specifically designed to simplify the process of building federated learning systems, as our primary tool. Moreover, we present the foundations of federated learning, explore how different techniques can enhance its privacy aspects, how it is being used in real-world settings today and a series of practical, hands-on code examples that showcase how you can federate any AI project with Flower, an open-source framework for all-this federated. | Mohammad Naseri, Javier FernándezMarqués, Yan Gao, Heng Pan | Flower Labs, Cambridge, UK |
|  |  [Graph Reasoning with LLMs (GReaL)](https://doi.org/10.1145/3637528.3671448) |  | 0 | Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications. Large Language Models (LLMs) have demonstrated impressive capabilities by advancing state-of-the-art on many language-based benchmarks. Their ability to process and understand natural language open exciting possibilities in various domains. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with LLMs remains an understudied problem that has recently gained more attention. This tutorial builds upon recent advances in expressing reasoning problems through the lens of tasks on graph data. The first part of the tutorial will provide an in-depth discussion of techniques for representing graphs as inputs to LLMs. The second, hands-on, portion will demonstrate these techniques in a practical setting. As a learning outcome of participating in the tutorial, participants will be able to analyze graphs either on free-tier Colab or their local machines with the help of LLMs. | Anton Tsitsulin, Bryan Perozzi, Bahare Fatemi, Jonathan J. Halcrow | Google Research, New York, NY, USA; Google Research, Montreal, QC, Canada; Google Research, Atlanta, GA, USA |
|  |  [Breaking Barriers: A Hands-On Tutorial on AI-Enabled Accessibility to Social Media Content](https://doi.org/10.1145/3637528.3671446) |  | 0 | Reddit's mission is to bring community, belonging, and empowerment to everyone in the world. This hands-on tutorial explores the immense potential of Artificial Intelligence (AI) to improve accessibility to social media content for individuals with different disabilities, including hearing, visual, and cognitive impairments. We will design and implement a variety of AI-based approaches based on multimodal open-source Large Language Models (LLMs) to bridge the gap between research and real-world applications. | Julio Villena, Rosa Català, Janine García, Concepción Polo, Yessika Labrador, Francisco delValle, Bhargav Ayyagari | Reddit, Inc., San Francisco, USA; Reddit, Inc., Toronto, Canada; Reddit, Inc., Madrid, Spain |
|  |  [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://doi.org/10.1145/3637528.3671463) |  | 0 | Large Language Models (LLMs) have revolutionized the field of NaturalLanguage Generation (NLG) by demonstrating an impressive ability to generatehuman-like text. However, their widespread usage introduces challenges thatnecessitate thoughtful examination, ethical scrutiny, and responsiblepractices. In this study, we delve into these challenges, explore existingstrategies for mitigating them, with a particular emphasis on identifyingAI-generated text as the ultimate solution. Additionally, we assess thefeasibility of detection from a theoretical perspective and propose novelresearch directions to address the current limitations in this domain. | Sara Abdali, Richard Anarfi, C. J. Barberan, Jia He | Microsoft, Redmond, WA, USA; Microsoft, Cambridge, MA, USA |
|  |  [Advances in Human Event Modeling: From Graph Neural Networks to Language Models](https://doi.org/10.1145/3637528.3671466) |  | 0 | Human events such as hospital visits, protests, and epidemic outbreaks directly affect individuals, communities, and societies. These events are often influenced by factors such as economics, politics, and public policies of our society. The abundance of online data sources such as social networks, official news articles, and personal blogs chronicle societal events, facilitating the development of AI models for social science, public health care, and decision making. Human event modeling generally comprises both the forecasting stage, which estimates future events based on historical data, and interpretation, which seeks to identify influential factors of such events to understand their causative attributes. Recent achievements, fueled by deep learning and the availability of public data, have significantly advanced the field of human event modeling. This survey offers a systematic overview of deep learning technologies for forecasting and interpreting human events, with a primary focus on political events. We first introduce the existing challenges and background in this domain. We then present the problem formulation of event forecasting and interpretation. We investigate recent achievements in graph neural networks, owing to the prevalence of relational data and the efficacy of graph learning models. We also discuss the latest studies that utilize large language models for event reasoning. Lastly, we provide summaries of data resources, open challenges, and future research directions in the study of human event modeling. | Songgaojun Deng, Maarten de Rijke, Yue Ning | University of Amsterdam, Amsterdam, The Netherlands; Stevens Institute of Technology, Hoboken, New Jersey, USA |
|  |  [Reasoning and Planning with Large Language Models in Code Development](https://doi.org/10.1145/3637528.3671452) |  | 0 | Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development. | Hao Ding, Ziwei Fan, Ingo Gühring, Gaurav Gupta, Wooseok Ha, Jun Huan, Linbo Liu, Behrooz OmidvarTehrani, Shiqi Wang, Hao Zhou | AWS AI labs, Santa Clara, CA, USA; AWS AI Labs, Santa Clara, CA, USA; AWS AI Labs, Berlin, Germany; AWS AI Labs, New York, NY, USA |
|  |  [Sharing is Caring: A Practical Guide to FAIR(ER) Open Data Release](https://doi.org/10.1145/3637528.3671468) |  | 0 | Findable. Accessible. Interoperable. Reusable. Since their introduction in 2016, the FAIR data principles have defined the standards by which scientific researchers share data. However, modern research in data editing and management consistently shows that while the FAIR data principles are widely accepted in theory, they can be much more difficult to understand and implement in practice. In this tutorial, we explore some of the simple, realistic steps scientists can take to FAIRly release open data. We also explore areas where the current FAIR guidelines fall short and offer practical suggestions for making open data FAIR(ER): more Equitable and Realistic. This first involves ways to make datasets themselves more equitably accessible for researchers with disabilities. While equitably accessible data design has some research overlap with paper, presentation, and website design, we suggest several unique distinctions specific to datasets. The "Realistic'' aspect of FAIR(ER) data facilitates a path to translate open data (and research on that data) back to true applications. Driven by national security applications pipelines, we call out important considerations for balancing data editing against data realism. | Amelia Henriksen, Miranda Mundt | Sandia National Laboratories, Albuquerque, New Mexico, USA |
|  |  [Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)](https://doi.org/10.1145/3637528.3671467) |  | 0 | With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our tutorial, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms. | Krishnaram Kenthapadi, Mehrnoosh Sameki, Ankur Taly | Microsoft Azure AI, Boston, MA, USA; Oracle Health AI, Redwood City, CA, USA; Google Cloud AI, Sunnyvale, CA, USA |
|  |  [A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide](https://doi.org/10.1145/3637528.3671457) |  | 0 | Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications. Investigation of deep learning for HOIs, thus, has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, bioinformatics and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions. | Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato, Kijung Shin | Tsinghua University, Beijing, China; KAIST, Seoul, Republic of Korea; University of Turin, Turin, Italy |
|  |  [Graph Intelligence with Large Language Models and Prompt Learning](https://doi.org/10.1145/3637528.3671456) |  | 0 | Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Graph intelligence is rapidly becoming a crucial aspect of understanding and exploiting the intricate interconnections within graph data. Recently, large language models (LLMs) and prompt learning techniques have pushed graph intelligence forward, outperforming traditional Graph Neural Network (GNN) pre-training methods and setting new benchmarks for performance. In this tutorial, we begin by offering a comprehensive review and analysis of existing methods that integrate LLMs with graphs. We introduce existing works based on a novel taxonomy that classifies them into three distinct categories according to the roles of LLMs in graph tasks: as enhancers, predictors, or alignment components. Secondly, we introduce a new learning method that utilizes prompting on graphs, offering substantial potential to enhance graph transfer capabilities across diverse tasks and domains. We discuss existing works on graph prompting within a unified framework and introduce our developed tool for executing a variety of graph prompting tasks. Additionally, we discuss the applications of combining Graphs, LLMs, and prompt learning across various tasks, such as urban computing, recommendation systems, and anomaly detection. This lecture-style tutorial is an extension of our original work published in IJCAI 2024[44] and arXiv[77] with the invitation of KDD24. | Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, Jeffrey Xu Yu | The Chinese University of Hong Kong, HongKong, China; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [A Review of Graph Neural Networks in Epidemic Modeling](https://doi.org/10.1145/3637528.3671455) |  | 0 | Since the onset of the COVID-19 pandemic, there has been a growing interestin studying epidemiological models. Traditional mechanistic modelsmathematically describe the transmission mechanisms of infectious diseases.However, they often fall short when confronted with the growing challenges oftoday. Consequently, Graph Neural Networks (GNNs) have emerged as aprogressively popular tool in epidemic research. In this paper, we endeavor tofurnish a comprehensive review of GNNs in epidemic tasks and highlightpotential future directions. To accomplish this objective, we introducehierarchical taxonomies for both epidemic tasks and methodologies, offering atrajectory of development within this domain. For epidemic tasks, we establisha taxonomy akin to those typically employed within the epidemic domain. Formethodology, we categorize existing work into Neural Models andHybrid Models. Following this, we perform an exhaustive and systematicexamination of the methodologies, encompassing both the tasks and theirtechnical details. Furthermore, we discuss the limitations of existing methodsfrom diverse perspectives and systematically propose future researchdirections. This survey aims to bridge literature gaps and promote theprogression of this promising field. We hope that it will facilitate synergiesbetween the communities of GNNs and epidemiology, and contribute to theircollective progress. | Zewen Liu, Guancheng Wan, B. Aditya Prakash, Max S. Y. Lau, Wei Jin | Emory University, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA |
|  |  [Foundation Models for Time Series Analysis: A Tutorial and Survey](https://doi.org/10.1145/3637528.3671451) |  | 0 | Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration. | Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, Qingsong Wen | Squirrel AI, Seattle, WA, USA; University of Connecticut, Storrs, CT, USA; Princeton University, Princeton, NJ, USA; The Hong Kong University of Science and Technology (Guangzhou; Griffith University, Brisbane, Australia; Monash University, Melbourne, Australia; The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China |
|  |  [Symbolic Regression: A Pathway to Interpretability Towards Automated Scientific Discovery](https://doi.org/10.1145/3637528.3671464) |  | 0 | Symbolic regression is a machine learning technique employed for learning mathematical equations directly from data. Mathematical equations capture both functional and causal relationships in the data. In addition, they are simple, compact, generalizable, and interpretable models, making them the best candidates for i) learning inherently transparent models and ii) boosting scientific discovery. Symbolic regression has received a growing interest since the last decade and is tackled using different approaches in supervised and unsupervised deep learning, thanks to the enormous progress achieved in deep learning in the last twenty years. Symbolic regression remains underestimated in conference coverage as a primary form of interpretable AI and a potential candidate for automating scientific discovery. This tutorial overviews symbolic regression: problem definition, approaches, and key limitations, discusses why physical sciences are beneficial to symbolic regression, and explores possible future directions in this research area. | Nour Makke, Sanjay Chawla | Qatar Airways (Qatar) |
|  |  [A Survey of Large Language Models for Graphs](https://doi.org/10.1145/3637528.3671460) |  | 0 | Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at https://github.com/HKUDS/Awesome-LLM4Graph-Papers. | Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang | University of Hong Kong, Hong Kong, China; Baidu, Beijing, China; University of Notre Dame, Indiana, USA |
|  |  [Explainable Artificial Intelligence on Biosignals for Clinical Decision Support](https://doi.org/10.1145/3637528.3671459) |  | 0 | Deep learning has proven effective in several areas, including computer vision, natural language processing, and disease prediction, which can support clinicians in making decisions along the clinical pathway. However, in order to successfully integrate these algorithms into clinical practice, it is important that their decision-making processes are transparent, explainable, and interpretable. Firstly, this tutorial will introduce targeted eXplainable Artificial Intelligence (XAI) methods to address the urgent need for explainability of deep learning in healthcare applications. In particular, it focuses on algorithms for raw biosignals without prior feature extraction that enable medical diagnoses, specifically electrocardiograms (ECG) -- stemming from the heart -- and electroencephalograms (EEG) representing the electrical activity of the brain. Secondly, participants are provided with a comprehensive workflow that includes both data processing and an introduction to relevant network architectures. Subsequently, various XAI methods are described and it is shown, how the resulting relevance attributions can be visualized on biosignals. Finally, two compelling real-world use cases are presented that demonstrate the effectiveness of XAI in analyzing ECG and EEG signals for disease prediction and sleep classification, respectively. In summary, the tutorial will provide the skills required for gaining insight into the decision process of deep neural networks processing authentic clinical biosignal data. | Miriam Cindy Maurer, Jacqueline Michelle Metsch, Philip Hempel, Theresa Bender, Nicolai Spicher, AnneChristin Hauschild | Department of Medical Informatics, University Medical Center, Göttingen, Germany |
|  |  [Urban Foundation Models: A Survey](https://doi.org/10.1145/3637528.3671453) |  | 0 | Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models (UFMs), this burgeoning field faces challenges such as a lack of clear definitions and systematic reviews. To this end, this paper first introduces the concept of UFMs and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes and clarifies current UFM-related works, based on urban data modalities and types. Furthermore, we explore the application landscape of UFMs, detailing their potential impact in various urban contexts. Relevant papers and open-source resources have been collated and are continuously updated at: https://github.com/usail-hkust/Awesome-Urban-Foundation-Models. | Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, Hui Xiong | HKUST, Hong Kong, China; HKUST(GZ) & HKUST, Guangzhou, China; HKUST(GZ), Guangzhou, China |
|  |  [Inference Optimization of Foundation Models on AI Accelerators](https://doi.org/10.1145/3637528.3671465) |  | 0 | Powerful foundation models, including large language models (LLMs), with Transformer architectures have ushered in a new era of Generative AI across various industries. Industry and research community have witnessed a large number of new applications, based on those foundation models. Such applications include question and answer, customer services, image and video generation, and code completions, among others. However, as the number of model parameters reaches to hundreds of billions, their deployment incurs prohibitive inference costs and high latency in real-world scenarios. As a result, the demand for cost-effective and fast inference using AI accelerators is ever more higher. To this end, our tutorial offers a comprehensive discussion on complementary inference optimization techniques using AI accelerators. Beginning with an overview of basic Transformer architectures and deep learning system frameworks, we deep dive into system optimization techniques for fast and memory-efficient attention computations and discuss how they can be implemented efficiently on AI accelerators. Next, we describe architectural elements that are key for fast transformer inference. Finally, we examine various model compression and fast decoding strategies in the same context. | Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas M. Kübler, Jiaji Huang, Matthäus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, George Karypis | AWS AI & EPFL, Tübingen, Germany; AWS AI, Santa Clara, CA, USA; AWS AI, Tübingen, Germany |
|  |  [Automated Mining of Structured Knowledge from Text in the Era of Large Language Models](https://doi.org/10.1145/3637528.3671469) |  | 0 | Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures. | Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, Jiawei Han | University of Illinois Urbana-Champaign, Urbana, IL, USA |
|  |  [Causal Inference with Latent Variables: Recent Advances and Future Prospectives](https://doi.org/10.1145/3637528.3671450) |  | 0 | Causality lays the foundation for the trajectory of our world. Causalinference (CI), which aims to infer intrinsic causal relations among variablesof interest, has emerged as a crucial research topic. Nevertheless, the lack ofobservation of important variables (e.g., confounders, mediators, exogenousvariables, etc.) severely compromises the reliability of CI methods. The issuemay arise from the inherent difficulty in measuring the variables.Additionally, in observational studies where variables are passively recorded,certain covariates might be inadvertently omitted by the experimenter.Depending on the type of unobserved variables and the specific CI task, variousconsequences can be incurred if these latent variables are carelessly handled,such as biased estimation of causal effects, incomplete understanding of causalmechanisms, lack of individual-level causal consideration, etc. In this survey,we provide a comprehensive review of recent developments in CI with latentvariables. We start by discussing traditional CI techniques when variables ofinterest are assumed to be fully observed. Afterward, under the taxonomy ofcircumvention and inference-based methods, we provide an in-depth discussion ofvarious CI strategies to handle latent variables, covering the tasks of causaleffect estimation, mediation analysis, counterfactual reasoning, and causaldiscovery. Furthermore, we generalize the discussion to graph data whereinterference among units may exist. Finally, we offer fresh aspects for furtheradvancement of CI with latent variables, especially new opportunities in theera of large language models (LLMs). | Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, Jundong Li | Case Western Reserve University, Charlottesville, VA, USA; University of Virginia, Charlottesville, VA, USA |
|  |  [A Survey on Safe Multi-Modal Learning Systems](https://doi.org/10.1145/3637528.3671462) |  | 0 | In the rapidly evolving landscape of artificial intelligence, multimodal learning systems (MMLS) have gained traction for their ability to process and integrate information from diverse modality inputs. Their expanding use in vital sectors such as healthcare has made safety assurance a critical concern. However, the absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy that systematically categorizes and assesses MMLS safety. This taxonomy is structured around four fundamental pillars that are critical to ensuring the safety of MMLS: robustness, alignment, monitoring, and controllability. Leveraging this taxonomy, we review existing methodologies, benchmarks, and the current state of research, while also pinpointing the principal limitations and gaps in knowledge. Finally, we discuss unique challenges in MMLS safety. In illuminating these challenges, we aim to pave the way for future research, proposing potential directions that could lead to significant advancements in the safety protocols of MMLS. | Tianyi Zhao, Liangliang Zhang, Yao Ma, Lu Cheng | University of Illinois Chicago, Chicago, USA; University of Southern California, Los Angeles, USA; Rensselaer Polytechnic Institute, Troy, USA |
|  |  [Responsible AI Day](https://doi.org/10.1145/3637528.3673867) |  | 0 | We summarize the goals of the Responsible AI day, giving a glimpse on the program as well as a short biography of the organizers. | Ricardo BaezaYates, Nataly Buslón |  |
|  |  [Heterogeneous Contrastive Learning for Foundation Models and Beyond](https://doi.org/10.1145/3637528.3671454) |  | 0 | In the era of big data and Artificial Intelligence, an emerging paradigm isto utilize contrastive self-supervised learning to model large-scaleheterogeneous data. Many existing foundation models benefit from thegeneralization capability of contrastive self-supervised learning by learningcompact and high-quality representations without relying on any labelinformation. Amidst the explosive advancements in foundation models acrossmultiple domains, including natural language processing and computer vision, athorough survey on heterogeneous contrastive learning for the foundation modelis urgently needed. In response, this survey critically evaluates the currentlandscape of heterogeneous contrastive learning for foundation models,highlighting the open challenges and future trends of contrastive learning. Inparticular, we first present how the recent advanced contrastive learning-basedmethods deal with view heterogeneity and how contrastive learning is applied totrain and fine-tune the multi-view foundation models. Then, we move tocontrastive learning methods for task heterogeneity, including pretrainingtasks and downstream tasks, and show how different tasks are combined withcontrastive learning loss for different purposes. Finally, we conclude thissurvey by discussing the open challenges and shedding light on the futuredirections of contrastive learning. | Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He | University of Illinois at Urbana-Champaign, Champaign, USA |
|  |  [Equity, Diversity & Inclusion (EDI): Special Day at ACM KDD 2024](https://doi.org/10.1145/3637528.3673870) |  | 0 | The Equity, Diversity & Inclusion event is a special day organized in conjunction with KDD '24, the 30 ^th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, which will take place from Sunday, August 25 to Thursday, August 29, 2024 at the Center de Convencions Internacional de Barcelona in Barcelona, Spain. This special day, scheduled for August 28, 2024, promotes equity, diversity, and inclusion (EDI) in data science, artificial intelligence, and beyond. It will bring together academics, researchers, practitioners, and human resources professionals (i) to present algorithms, techniques, methodologies, and projects in data science that enable responsible data processing and modeling; (ii) to discuss policies, best practices, and guidelines to promote an inclusive work environment and effective collaboration; (iii) to share personal stories to encourage young researchers, including those from groups unrepresented in the research community, to develop strong careers in data science; and (iv) to collaboratively develop and discuss an EDI Manifesto to promote an inclusive workplace environment and guiding principles in the development of research activities. | Tania Cerquitelli, Amin Mantrach | Politecnico di Torino, Turin, Piedmont, Italy; Amazon, Luxembourg, Luxembourg |
|  |  [Health Day: Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery](https://doi.org/10.1145/3637528.3673866) |  | 0 | The ACM KDD 2024 Health Day theme, "Building Health AI Ecosystem: From Data Harmonization to Knowledge Discovery," highlights the transformative potential of AI-driven ecosystems in healthcare, translational biomedical research, and basic biological research. This extended abstract discusses recent advancements, challenges, and future directions, focusing on integrating AI-ready data sets, interdisciplinary collaborations, and ethical AI practices. It aims to catalyze discussions on the potential of AI ecosystems in revolutionizing healthcare and related fields. | Jake Chen, Peipei Ping | School of Medicine, University of California, Los Angeles, Los Angeles, CA, USA |
|  |  [Overview of ACM SIGKDD 2024 AI4Science4AI Special Day](https://doi.org/10.1145/3637528.3673871) |  | 0 | This paper provides an overview of the ACM SIGKDD 2024 AI4Science4AI special day. It includes information about the organizers, invited speakers, keynote speakers, the event agenda, and insights from related workshops. The AI4Science4AI special day aims to bring together experts in artificial intelligence (AI) and science to discuss the latest developments, challenges, and future directions. | Wei Ding, Gustau CampsValls | Image Processing Laboratory (IPL), Universitat de València, Paterna, Spain |
|  |  [KDD 2024 Special Day - AI for Environment](https://doi.org/10.1145/3637528.3673869) |  | 0 | Environmental problems such as air pollution monitoring and prevention, flood detection and prevention, land use, forest management, river water quality, wastewater treatment supervision, etc. are more complex than typical real-world problems usually AI faces to. This added complexity rises from several aspects, such as the randomness shown by most of environmental processes involved, the 2D/3D nature of involved problems, the temporal aspects, the spatial aspects, the inexactness of the information, etc. In fact, environmental problems belong to the most difficult problems with a lot of inexactness and uncertainty, and possibly conflicting objectives to be solved according to several classifications such as the one by Funtowicz & Ravetz (Funtowicz & Ravetz, 1999), which states that there are 3 kinds of problems. Also, they are non-structured problems in the classification proposed by H. Simon (Simon, 1966). All this complexity means that to effectively solve those problems a lot of knowledge is needed. This knowledge can be theoretical knowledge expressed in mechanistic models, such as the Gravidity Newton's Theory, or it can be empirical knowledge that can be expressed by means of empirical models, originated by some data and observations (data-driven knowledge) or by the expertise gathered by people when coping with such problems (model-driven knowledge, particularly expert-based knowledge). The KDD 2024 Special Day for AI for environment brings together researchers and practitioners to present their perspective on this very timely topic on how AI can be used for good, and improving the environment where we all live in. | Karina Gibert, Wee Hyong Tok, Miquel SànchezMarrè | Microsoft, Redmond, Washington, USA |
|  |  [European Data Science Day: KDD-2024 Special Day](https://doi.org/10.1145/3637528.3673868) |  | 0 | The European Data Science Day offers a full day focused exclusively on innovative KDD-relevant research and development projects from national and regional funding programs, as well as corporate, start-up, and nonprofit channels. The idea is to bring together a diverse community of researchers in Data Science, Machine Learning, Language Technologies, and Knowledge Discovery, as well as partnerships in the social and physical sciences/arts, to showcase the state-of-the-art in research and applications. | Dunja Mladenic, Dumitru Roman | Oslo Metropolitan University, SINTEF AS, Oslo, Norway; Jozef Stefan Institute, Ljubljana, Slovenia |
|  |  [Generative AI Day](https://doi.org/10.1145/3637528.3673872) |  | 0 | The Generative AI (AIGC) Day at KDD'24 is a dedicated full-day event for generative AI at KDD. This is an opportunity to bring together researchers, practitioners, and startups to share the insights about the cutting-edge advancements and to discuss the potential societal impacts of LLMs and AIGC. It is exciting that this year, we have invited speakers from both industry (e.g., Amazon, Zhipu AI) and academia (e.g., USC, UCLA). The topics cover various perspectives of generative AI including foundation models, streaming LLMs, LLM training and inference. As demonstrated, data plays a crucial role in developing cutting-edge generative AI models. For example, the Gemini Team has found that "data quality is an important factor for highly-performing models...''. To date, there is still significant room to define design principles and develop methods for improved data collection, selection, and synthetic data generation for the pre-training and alignment of language, vision, and multi-modal models. Therefore, the Day will invite the speakers and KDD audience to discuss the challenges and opportunities for data mining researchers in the era of generative AI. | Jie Tang, Yuxiao Dong, Michalis Vazirgiannis | Tsinghua University, Beijing, China; Ecole Polytechnique & Mohamed bin Zayed University of Artificial Intelligence, Paris, France |
|  |  [KDD 2024 Finance Day](https://doi.org/10.1145/3637528.3673865) |  | 0 | The Finance Day at KDD 2024 will take place on August 26th in Barcelona, Spain. Following the success of the inaugural event last year, the second edition highlights the significant role of AI in transforming the financial industry. This special day serves as a forum for discussion of innovations at the intersection of AI and finance. An exciting lineup of 12 influential speakers from nine different countries will be featured, representing a mix of government organizations, leading banks, innovative hedge funds, and top academic institutions. These experts will delve into a range of topics, from cutting-edge FinTech innovations to ethical considerations in machine learning, providing a comprehensive overview of the finance and AI. The distinguished speakers include Avanidhar Subrahmanyam from UCLA, Henrike Mueller from the Financial Conduct Authority, Claudia Perlich from Two Sigma, Eyke Hüllermeier from Ludwig-Maximilians-Universität München, Senthil Kumar from Capital One, Stefan Zohren from the University of Oxford, Dumitru Roman from SINTEF ICT, Kubilay Atasu from TU Delft, Xiao-Ming Wu from Hong Kong Polytechnic University, Yongjae Lee from UNIST, Jundong Li from the University of Virginia, and Milos Blagojevic from BlackRock. | Guiling Wang, Daniel Borrajo | New Jersey Institute of Technology, Newark, NJ, USA; Universidad Carlos III de Madrid, Madrid, Spain |
|  |  [AdKDD 2024](https://doi.org/10.1145/3637528.3671476) |  | 0 | The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2024 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising. | Abraham Bagherjeiran, Nemanja Djuric, KuangChih Lee, Linsey Pang, Vladan Radosavljevic, Suju Rajan | Amazon, Palo Alto, CA, USA; Spotify, New York, NY, USA; eBay, Inc., San Jose, CA, USA; Aurora Innovation, Inc., Pittsburgh, PA, USA; Salesforce, San Francisco, CA, USA; Walmart, Sunnyvale, CA, United States |
|  |  [Fragile Earth: Generative and Foundational Models for Sustainable Development](https://doi.org/10.1145/3637528.3671493) |  | 0 | The Fragile Earth Workshop is a recurring event in ACM's KDD Conference on research in knowledge discovery and data mining that gathers the research community to find and explore how data science can measure and progress climate and social issues, following the United Nations Sustainable Development Goals (SDGs) framework. | Emre Eftelioglu, Bistra Dilkina, Naoki Abe, Ramakrishnan Kannan, Yuzhou Chen, Yulia R. Gel, Kathleen Buckingham, Auroop R. Ganguly, James Hodson, Jiafu Mao | veritree, Vancouver, BC, Canada; The University of Texas at Dallas, Richardson, TX, USA; AIForGood, San Francisco, CA, USA; Northeastern University, Boston, MA, USA; Temple University, Philadelphia, PA, USA; IBM Research, Yorktown Heights, New York, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Amazon, Bellevue, WA, USA; University of Southern California, Los Angeles, CA, USA |
|  |  [Artificial Intelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare](https://doi.org/10.1145/3637528.3671497) |  | 0 | KDD AIDSH 2024 aims to foster discussions and developments that push the boundaries of Artificial Intelligence (AI) and Data Science (DS) in healthcare, enhance diagnostic accuracy and promote human-centric approaches to healthcare, thus stimulating future interdisciplinary collaborations. This year's symposium will focus on expanding the application of AI/DS in healthcare/medicine and bridging existing gaps. The workshop invites submissions of full papers as well as work-in-progress on the application of AI/DS in healthcare. The workshop will feature three invited talks from eminent speakers, spanning academia, industry, and clinical researchers. In addition, selected papers will be invited to publish in Health Data Science, a Science Partner Journal. This summary provides a brief description of the half-day workshop to be held on August 26th, 2024. The webpage for the workshop can be found at https://aimel.ai/kdd2024aidsh. | Shenda Hong, Daoxin Yin, Gongzheng Tang, Tianfan Fu, Liantao Ma, Junyi Gao, Mengling Feng, Mai Wang, Yu Yang, Fei Wang, Hongfang Liu, Luxia Zhang | National Engineering Research Center for Software, Peking University, Beijing, China; Department of Population Health Sciences, Cornell University, New York, NY, USA; National Institute of Health Data Science, Peking University, Beijing, China; Saw Swee Hock School of Public Health, National University of Singapore, Singapore, Singapore; Computer Science Department, Rensselaer Polytechnic Institute, New York, USA; Department of Health Data Science and Artificial Intelligence, UTHealth Houston, Houston, TX, USA; Health Data Research UK, University of Edinburgh, Edinburgh, United Kingdom |
|  |  [TSMO 2024: Two-sided Marketplace Optimization](https://doi.org/10.1145/3637528.3671484) |  | 0 | In recent years, two-sided marketplaces have emerged as viable business models in many real-world applications. In particular, we have moved from the social network paradigm to a network with two distinct types of participants representing the supply and demand of a specific good. Examples of industries include but are not limited to accommodation (Airbnb, Booking.com), video content (YouTube, Instagram, TikTok), ridesharing (Uber, Lyft), online shops (Etsy, Ebay, Facebook Marketplace), music (Spotify, Amazon), app stores (Apple App Store, Google App Store) or job sites (LinkedIn). The traditional research in most of these industries focused on satisfying the demand. OTAs would sell hotel accommodation, TV networks would broadcast their own content, or taxi companies would own their own vehicle fleet. In modern examples like Airbnb, YouTube, Instagram, or Uber, the platforms operate by outsourcing the service they provide to their users, whether they are hosts, content creators or drivers, and have to develop their models considering their needs and goals. | Mihajlo Grbovic, Vladan Radosavljevic, Minmin Chen, Katerina IliakopoulouZanos, Thanasis Noulas, Amit Goyal, Fabrizio Silvestri | Amazon, San Francisco, CA, USA; Meta, New York City, NY, USA; Google Deepmind, Mountain View, CA, USA; Airbnb, Inc., San Francisco, CA, USA; Spotify, New York City, NY, USA; Sapienza University of Rome, Rome, Italy; Bitvavo, Thessaloniki, Greece |
|  |  [KDD workshop on Evaluation and Trustworthiness of Generative AI Models](https://doi.org/10.1145/3637528.3671481) |  | 0 | The KDD workshop on Evaluation and Trustworthiness of Generative AI Models aims to address the critical need for reliable generative AI technologies by exploring comprehensive evaluation strategies. This workshop will delve into various aspects of assessing generative AI models, including Large Language Models (LLMs) and diffusion models, focusing on trustworthiness, safety, bias, fairness, and ethical considerations. With an emphasis on interdisciplinary collaboration, the workshop will feature invited talks, peer-reviewed paper presentations, and panel discussions to advance the state of the art in generative AI evaluation. | Yuan Ling, Shujing Dong, Yarong Feng, Zongyi Joe Liu, George Karypis, Chandan K. Reddy | Virginia Tech Amazon, Arlington, VA, USA; Univ. of Minnesota Amazon, Santa Clara, CA, USA; Amazon, Irvine, WA, USA; Amazon, Seattle, WA, USA |
|  |  [NL2Code-Reasoning and Planning with LLMs for Code Development](https://doi.org/10.1145/3637528.3671505) |  | 0 | There is huge value in making software development more productive with AI. An important component of this vision is the capability to translate natural language to a programming language ("NL2Code") and thus to significantly accelerate the speed at which code is written. This workshop gathers researchers, practitioners, and users from industry and academia that are working on NL2Code, specifically on the problem of using large language models to convert statements posed in a human language to a formal programming language. | Ye Xing, Jun Huan, Wee Hyong Tok, Cong Shen, Johannes Gehrke, Katherine Lin, Arjun Guha, Omer Tripp, Murali Krishna Ramanathan | Microsoft, Boston, USA; University of Virginia, Charlottesville, USA; Microsoft, Redmond, USA; Amazon, Seattle, USA; Microsoft, Richmond, USA; Northeastern University, Boston, USA |
