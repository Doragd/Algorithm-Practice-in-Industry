# ICLR2021

## 会议论文列表

本会议共有 860 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study](https://openreview.net/forum?id=nIAxjsniDzg) |  | 0 | In recent years, reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such \`\`"choices" in a unified on-policy deep actor-critic framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for the training of on-policy deep actor-critic RL agents. | Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, Olivier Bachem |  |
| 2 |  |  [Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data](https://openreview.net/forum?id=rC8sJ4i6kaH) |  | 0 | Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization. | Colin Wei, Kendrick Shen, Yining Chen, Tengyu Ma |  |
| 3 |  |  [Learning to Reach Goals via Iterated Supervised Learning](https://openreview.net/forum?id=rALA0Xo6yNJ) |  | 0 | Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks. | Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Manon Devin, Benjamin Eysenbach, Sergey Levine |  |
| 4 |  |  [Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients](https://openreview.net/forum?id=m5Qsh0kBQG) |  | 0 | Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of $\textit{symbolic regression}$. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance. | Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Cláudio Prata Santiago, Sookyung Kim, Joanne Taery Kim |  |
| 5 |  |  [Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime](https://openreview.net/forum?id=PULSD5qI2N1) |  | 0 | We analyze the convergence of the averaged stochastic gradient descent for overparameterized two-layer neural networks for regression problems. It was recently found that a neural tangent kernel (NTK) plays an important role in showing the global convergence of gradient-based methods under the NTK regime, where the learning dynamics for overparameterized neural networks can be almost characterized by that for the associated reproducing kernel Hilbert space (RKHS). However, there is still room for a convergence rate analysis in the NTK regime. In this study, we show that the averaged stochastic gradient descent can achieve the minimax optimal convergence rate, with the global convergence guarantee, by exploiting the complexities of the target function and the RKHS associated with the NTK. Moreover, we show that the target function specified by the NTK of a ReLU network can be learned at the optimal convergence rate through a smooth approximation of a ReLU network under certain conditions. | Atsushi Nitanda, Taiji Suzuki |  |
| 6 |  |  [Free Lunch for Few-shot Learning: Distribution Calibration](https://openreview.net/forum?id=JWOiYxMG92s) |  | 0 | Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation. | Shuo Yang, Lu Liu, Min Xu |  |
| 7 |  |  [Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes](https://openreview.net/forum?id=HajQFbx_yB) |  | 0 | Determinantal point processes (DPPs) have attracted significant attention in machine learning for their ability to model subsets drawn from a large item collection. Recent work shows that nonsymmetric DPP (NDPP) kernels have significant advantages over symmetric kernels in terms of modeling power and predictive performance. However, for an item collection of size $M$, existing NDPP learning and inference algorithms require memory quadratic in $M$ and runtime cubic (for learning) or quadratic (for inference) in $M$, making them impractical for many typical subset selection tasks. In this work, we develop a learning algorithm with space and time requirements linear in $M$ by introducing a new NDPP kernel decomposition. We also derive a linear-complexity NDPP maximum a posteriori (MAP) inference algorithm that applies not only to our new kernel but also to that of prior work. Through evaluation on real-world datasets, we show that our algorithms scale significantly better, and can match the predictive performance of prior work. | Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, VictorEmmanuel Brunel |  |
| 8 |  |  [Randomized Automatic Differentiation](https://openreview.net/forum?id=xpx9zj7CUlY) |  | 0 | The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor. | Deniz Oktay, Nick McGreivy, Joshua Aduol, Alex Beatson, Ryan P. Adams |  |
| 9 |  |  [Learning Generalizable Visual Representations via Interactive Gameplay](https://openreview.net/forum?id=UuchYL8wSZo) |  | 0 | A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning. | Luca Weihs, Aniruddha Kembhavi, Kiana Ehsani, Sarah M. Pratt, Winson Han, Alvaro Herrasti, Eric Kolve, Dustin Schwenk, Roozbeh Mottaghi, Ali Farhadi |  |
| 10 |  |  [Global Convergence of Three-layer Neural Networks in the Mean Field Regime](https://openreview.net/forum?id=KvyxFqZS_D) |  | 0 | In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers. In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which – unlike previous works on two-layer networks – does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument. | Huy Tuan Pham, PhanMinh Nguyen |  |
| 11 |  |  [Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator](https://openreview.net/forum?id=Mk6PZtgAgfq) |  | 0 | Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models. | Max B. Paulus, Chris J. Maddison, Andreas Krause |  |
| 12 |  |  [Rethinking Attention with Performers](https://openreview.net/forum?id=Ua6zuk0WRH) |  | 0 | We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. | Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, Adrian Weller |  |
| 13 |  |  [Getting a CLUE: A Method for Explaining Uncertainty Estimates](https://openreview.net/forum?id=XSLF1XFq5h) |  | 0 | Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty. | Javier Antorán, Umang Bhatt, Tameem Adel, Adrian Weller, José Miguel HernándezLobato |  |
| 14 |  |  [When Do Curricula Work?](https://openreview.net/forum?id=tW4QEInpni) |  | 0 | Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum or random ordering can indeed improve the performance either with limited training time budget or in the existence of noisy data. | Xiaoxia Wu, Ethan Dyer, Behnam Neyshabur |  |
| 15 |  |  [Federated Learning Based on Dynamic Regularization](https://openreview.net/forum?id=B7v4QMR6Z9w) |  | 0 | We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data. | Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. Whatmough, Venkatesh Saligrama |  |
| 16 |  |  [Geometry-aware Instance-reweighted Adversarial Training](https://openreview.net/forum?id=iAX0l6Cz8ub) |  | 0 | In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training. | Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, Mohan S. Kankanhalli |  |
| 17 |  |  [Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity](https://openreview.net/forum?id=gvxJzw8kW4b) |  | 0 | While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup. | JangHyun Kim, Wonho Choo, Hosan Jeong, Hyun Oh Song |  |
| 18 |  |  [SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness](https://openreview.net/forum?id=DktZb97_Fx) |  | 0 | In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias. | Mikhail Yurochkin, Yuekai Sun |  |
| 19 |  |  [End-to-end Adversarial Text-to-Speech](https://openreview.net/forum?id=rsf1z-JSj87) |  | 0 | Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision. | Jeff Donahue, Sander Dieleman, Mikolaj Binkowski, Erich Elsen, Karen Simonyan |  |
| 20 |  |  [Dataset Condensation with Gradient Matching](https://openreview.net/forum?id=mSAKhLYLSsl) |  | 0 | As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available. | Bo Zhao, Konda Reddy Mopuri, Hakan Bilen |  |
| 21 |  |  [Rethinking Architecture Selection in Differentiable NAS](https://openreview.net/forum?id=PKubaeJkw3) |  | 0 | Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet. | Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, ChoJui Hsieh |  |
| 22 |  |  [A Distributional Approach to Controlled Text Generation](https://openreview.net/forum?id=jWkw45-9AbL) |  | 0 | We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both “pointwise’” and “distributional” constraints over the target LM — to our knowledge, the first model with such generality —while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. Code available at https://github.com/naver/gdc | Muhammad Khalifa, Hady Elsahar, Marc Dymetman |  |
| 23 |  |  [Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency](https://openreview.net/forum?id=QIRlze3I6hX) |  | 0 | At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots; sim-to-real requires correspondence between physics simulators and real hardware; transfer learning requires correspondences between different robot environments. In this paper, we propose to learn correspondence across such domains emphasizing on differing modalities (vision and internal state), physics parameters (mass and friction), and morphologies (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. We propose dynamics cycles that align dynamic robotic behavior across two domains using a cycle consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robots. Our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. Video demonstrations of our results are available at: https://sites.google.com/view/cycledynamics . | Qiang Zhang, Tete Xiao, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang |  |
| 24 |  |  [Human-Level Performance in No-Press Diplomacy via Equilibrium Search](https://openreview.net/forum?id=0-uUGPbIjD) |  | 0 | Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via regret minimization. Regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and ranks in the top 2% of human players when playing anonymous games on a popular Diplomacy website. | Jonathan Gray, Adam Lerer, Anton Bakhtin, Noam Brown |  |
| 25 |  |  [Parrot: Data-Driven Behavioral Priors for Reinforcement Learning](https://openreview.net/forum?id=Ysuv-WOFeKR) |  | 0 | Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl | Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, Sergey Levine |  |
| 26 |  |  [Learning Invariant Representations for Reinforcement Learning without Reconstruction](https://openreview.net/forum?id=-2FCwDKRREu) |  | 0 | We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference. | Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, Sergey Levine |  |
| 27 |  |  [Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs](https://openreview.net/forum?id=FGqiDsBUKL0) |  | 0 | Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape. | Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, Ping Luo |  |
| 28 |  |  [VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments](https://openreview.net/forum?id=RmB-88r9dL) |  | 0 | Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve. | Lizhen Nie, Mao Ye, Qiang Liu, Dan Nicolae |  |
| 29 |  |  [Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability](https://openreview.net/forum?id=dYeAHXnpWJ4) |  | 0 | Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding $p_{\theta} ( y\mid \mathbf{x} )$, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models? In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model $p_{\theta}(\mathbf{x} \mid y)$ implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model $p_{\theta}(\mathbf{x} \mid y)$ with that of the ground truth data distribution $p_{\text{data}} (\mathbf{x} \mid y)$. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models. Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models. | Suraj Srinivas, François Fleuret |  |
| 30 |  |  [Neural Synthesis of Binaural Speech From Mono Audio](https://openreview.net/forum?id=uAX8q61EVRu) |  | 0 | We present a neural rendering approach for binaural sound synthesis that can produce realistic and spatially accurate binaural sound in realtime. The network takes, as input, a single-channel audio source and synthesizes, as output, two-channel binaural sound, conditioned on the relative position and orientation of the listener with respect to the source. We investigate deficiencies of the l2-loss on raw waveforms in a theoretical analysis and introduce an improved loss that overcomes these limitations. In an empirical evaluation, we establish that our approach is the first to generate spatially accurate waveform outputs (as measured by real recordings) and outperforms existing approaches by a considerable margin, both quantitatively and in a perceptual study. Dataset and code are available online. | Alexander Richard, Dejan Markovic, Israel D. Gebru, Steven Krenn, Gladstone Alexander Butler, Fernando De la Torre, Yaser Sheikh |  |
| 31 |  |  [DiffWave: A Versatile Diffusion Model for Audio Synthesis](https://openreview.net/forum?id=a-xFK8Ymz5J) |  | 0 | In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations. | Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro |  |
| 32 |  |  [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/forum?id=YicbFdNTTy) |  | 0 |  | Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby |  |
| 33 |  |  [On the mapping between Hopfield networks and Restricted Boltzmann Machines](https://openreview.net/forum?id=RGJbergVIoO) |  | 0 |  | Matthew Smart, Anton Zilman |  |
| 34 |  |  [SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments](https://openreview.net/forum?id=cPZOyoDloxl) |  | 0 | Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning. | Glen Berseth, Daniel Geng, Coline Manon Devin, Nicholas Rhinehart, Chelsea Finn, Dinesh Jayaraman, Sergey Levine |  |
| 35 |  |  [Evolving Reinforcement Learning Algorithms](https://openreview.net/forum?id=0XXpJ4OtjW) |  | 0 |  | John D. CoReyes, Yingjie Miao, Daiyi Peng, Esteban Real, Quoc V. Le, Sergey Levine, Honglak Lee, Aleksandra Faust |  |
| 36 |  |  [Growing Efficient Deep Networks by Structured Continuous Sparsification](https://openreview.net/forum?id=wb3wxCObbRT) |  | 0 | We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 validation accuracy --- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods. | Xin Yuan, Pedro Henrique Pamplona Savarese, Michael Maire |  |
| 37 |  |  [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://openreview.net/forum?id=gZ9hCDWe6ke) |  | 0 | DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR. | Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai |  |
| 38 |  |  [EigenGame: PCA as a Nash Equilibrium](https://openreview.net/forum?id=NzTU59SYbNq) |  | 0 | We present a novel view on principal components analysis as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm---which combines elements from Oja's rule with a generalized Gram-Schmidt orthogonalization---is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights. | Ian Gemp, Brian McWilliams, Claire Vernade, Thore Graepel |  |
| 39 |  |  [Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting](https://openreview.net/forum?id=kmG8vRXTFv) |  | 0 | Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters. | Yuan Yin, Vincent Le Guen, Jérémie Donà, Emmanuel de Bézenac, Ibrahim Ayed, Nicolas Thome, Patrick Gallinari |  |
| 40 |  |  [Complex Query Answering with Neural Link Predictors](https://openreview.net/forum?id=Mos9F9kDwkz) |  | 0 | Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions ($\land$), disjunctions ($\lor$) and existential quantifiers ($\exists$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd. | Erik Arakelyan, Daniel Daza, Pasquale Minervini, Michael Cochez |  |
| 41 |  |  [Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding](https://openreview.net/forum?id=EbIDjBynYJ8) |  | 0 | Disentangling the underlying generative factors from complex data has so far been limited to carefully constructed scenarios. We propose a path towards natural data by first showing that the statistics of natural data provide enough structure to enable disentanglement, both theoretically and empirically. Specifically, we provide evidence that objects in natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. To address this finding we provide a novel proof that relies on a sparse prior on temporally adjacent observations to recover the true latent variables up to permutations and sign flips, directly providing a stronger result than previous work. We show that equipping practical estimation methods with our prior often surpasses the current state-of-the-art on several established benchmark datasets without any impractical assumptions, such as knowledge of the number of changing generative factors. Furthermore, we contribute two new benchmarks, Natural Sprites and KITTI Masks, which integrate the measured natural dynamics to enable disentanglement evaluation with more realistic datasets. We leverage these benchmarks to test our theory, demonstrating improved performance. We also identify non-obvious challenges for current methods in scaling to more natural domains. Taken together our work addresses key issues in disentanglement research for moving towards more natural settings. | David A. Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, Dylan M. Paiton |  |
| 42 |  |  [Self-training For Few-shot Transfer Across Extreme Task Differences](https://openreview.net/forum?id=O3Y56aqpChA) |  | 0 | Most few-shot learning techniques are pre-trained on a large, labeled “base dataset”. In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different “source” problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains. | Cheng Perng Phoo, Bharath Hariharan |  |
| 43 |  |  [Score-Based Generative Modeling through Stochastic Differential Equations](https://openreview.net/forum?id=PxTIG12RRHS) |  | 0 | Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of $1024\times 1024$ images for the first time from a score-based generative model. | Yang Song, Jascha SohlDickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole |  |
| 44 |  |  [Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation](https://openreview.net/forum?id=Wj4ODo0uyCF) |  | 0 | Using a mix of shared and language-specific (LS) parameters has shown promise in multilingual neural machine translation (MNMT), but the question of when and where LS capacity matters most is still under-studied. We offer such a study by proposing conditional language-specific routing (CLSR). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Moreover, CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many-to-one translation, particularly with unbalanced training data. Our study further verifies the trade-off between the shared capacity and LS capacity for multilingual translation. We corroborate our analysis by confirming the soundness of our findings as foundation of our improved multilingual Transformers. Source code and models are available at https://github.com/bzhangGo/zero/tree/iclr2021_clsr. | Biao Zhang, Ankur Bapna, Rico Sennrich, Orhan Firat |  |
| 45 |  |  [Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering](https://openreview.net/forum?id=yWkP7JuHX1) |  | 0 | Differentiable rendering has paved the way to training neural networks to perform “inverse graphics” tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D “neural renderer", complementing traditional graphics renderers. | Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, Sanja Fidler |  |
| 46 |  |  [How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks](https://openreview.net/forum?id=UH-cmocLJC) |  | 0 | We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently diverse. Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings. | Keyulu Xu, Mozhi Zhang, Jingling Li, Simon Shaolei Du, Kenichi Kawarabayashi, Stefanie Jegelka |  |
| 47 |  |  [Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions](https://openreview.net/forum?id=Ud3DSz72nYR) |  | 0 | We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations. | Zhengxian Lin, KinHo Lam, Alan Fern |  |
| 48 |  |  [Improved Autoregressive Modeling with Distribution Smoothing](https://openreview.net/forum?id=rJA5Pz7lHKb) |  | 0 | While autoregressive models excel at image compression, their sample quality is often lacking. Although not realistic, generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets. | Chenlin Meng, Jiaming Song, Yang Song, Shengjia Zhao, Stefano Ermon |  |
| 49 |  |  [MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training](https://openreview.net/forum?id=wWK7yXkULyh) |  | 0 | Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with $6.5 \times$ speed-up and $6\times$ reduction in memory usage. | Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song, Anshumali Shrivastava, Christopher Ré |  |
| 50 |  |  [Gradient Projection Memory for Continual Learning](https://openreview.net/forum?id=3AOj0RCNC2) |  | 0 | The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches. | Gobinda Saha, Isha Garg, Kaushik Roy |  |
| 51 |  |  [Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?](https://openreview.net/forum?id=uCY5MuAxcxU) |  | 0 | Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of \textquotedblleft better inductive bias.\textquotedblright\ However, this has not been made mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected net can always simulate the convolutional net. Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on $\mathbb{R}^d\times\{\pm 1\}$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires $\Omega(d^2)$ samples to generalize while $O(1)$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an $O(1)$ vs $\Omega(d^2/\varepsilon)$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for $\ell_2$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant. | Zhiyuan Li, Yi Zhang, Sanjeev Arora |  |
| 52 |  |  [Iterated learning for emergent systematicity in VQA](https://openreview.net/forum?id=Pd_oMxH8IlF) |  | 0 | Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR. | Ankit Vani, Max Schwarzer, Yuchen Lu, Eeshan Dhekane, Aaron C. Courville |  |
| 53 |  |  [Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies](https://openreview.net/forum?id=F3s69XzWOia) |  | 0 | Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data. | T. Konstantin Rusch, Siddhartha Mishra |  |
| 54 |  |  [Sparse Quantized Spectral Clustering](https://openreview.net/forum?id=pBqLS-7KYAF) |  | 0 |  | Zhenyu Liao, Romain Couillet, Michael W. Mahoney |  |
| 55 |  |  [Graph-Based Continual Learning](https://openreview.net/forum?id=HHSEKOnPvaO) |  | 0 | Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning. | Binh Tang, David S. Matteson |  |
| 56 |  |  [Dynamic Tensor Rematerialization](https://openreview.net/forum?id=Vfs_2RnOD0H) |  | 0 |  | Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, Zachary Tatlock |  |
| 57 |  |  [Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models](https://openreview.net/forum?id=F1vEjWK-lH_) |  | 0 | Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling. | Zirui Wang, Yulia Tsvetkov, Orhan Firat, Yuan Cao |  |
| 58 |  |  [CPT: Efficient Deep Neural Network Training via Cyclic Precision](https://openreview.net/forum?id=87ZwsaQNHPZ) |  | 0 |  | Yonggan Fu, Han Guo, Meng Li, Xin Yang, Yining Ding, Vikas Chandra, Yingyan Lin |  |
| 59 |  |  [Learning a Latent Simplex in Input Sparsity Time](https://openreview.net/forum?id=04LZCAxMSco) |  | 0 | We consider the problem of learning a latent $k$-vertex simplex $K\in\mathbb{R}^d$, given $\mathbf{A}\in\mathbb{R}^{d\times n}$, which can be viewed as $n$ data points that are formed by randomly perturbing some latent points in $K$, possibly beyond $K$. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a $k$-vertex latent simplex in time roughly $O(k\cdot\text{nnz}(\mathbf{A}))$, where $\text{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. We show that the dependence on $k$ in the running time is unnecessary given a natural assumption about the mass of the top $k$ singular values of $\mathbf{A}$, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. We obtain a spectral low-rank approximation to $\mathbf{A}$ in input-sparsity time and show that the column space thus obtained has small $\sin\Theta$ (angular) distance to the right top-$k$ singular space of $\mathbf{A}$. Our algorithm then selects $k$ points in the low-rank subspace with the largest inner product (in absolute value) with $k$ carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the $\Theta(k\cdot\text{nnz}(\mathbf{A}))$ running time. | Ainesh Bakshi, Chiranjib Bhattacharyya, Ravi Kannan, David P. Woodruff, Samson Zhou |  |
| 60 |  |  [Expressive Power of Invariant and Equivariant Graph Neural Networks](https://openreview.net/forum?id=lxHgXYN4bwl) |  | 0 | Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes. | Waïss Azizian, Marc Lelarge |  |
| 61 |  |  [Discovering a set of policies for the worst case reward](https://openreview.net/forum?id=PUkhWz65dy5) |  | 0 | We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite. | Tom Zahavy, André Barreto, Daniel J. Mankowitz, Shaobo Hou, Brendan O'Donoghue, Iurii Kemaev, Satinder Singh |  |
| 62 |  |  [Model-Based Visual Planning with Self-Supervised Functional Distances](https://openreview.net/forum?id=UcoXdfrORC) |  | 0 | A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods. | Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, Sergey Levine |  |
| 63 |  |  [Noise against noise: stochastic label noise helps combat inherent label noise](https://openreview.net/forum?id=80FMcTSZ6J0) |  | 0 | The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper, we are interested in learning with noisy labels, where we have a collection of samples with potential mislabeling. We show that a previously rarely discussed SGD noise, induced by stochastic label noise (SLN), mitigates the effects of inherent label noise. In contrast, the common SGD noise directly applied to model parameters does not. We formalize the differences and connections of SGD noise variants, showing that SLN induces SGD noise dependent on the sharpness of output landscape and the confidence of output probability, which may help escape from sharp minima and prevent overconfidence. SLN not only improves generalization in its simplest form but also boosts popular robust training methods, including sample selection and label correction. Specifically, we present an enhanced algorithm by applying SLN to label correction. Our code is released. | Pengfei Chen, Guangyong Chen, Junjie Ye, Jingwei Zhao, PhengAnn Heng |  |
| 64 |  |  [Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning](https://openreview.net/forum?id=qda7-sVg84) |  | 0 | Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite. | Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare |  |
| 65 |  |  [VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models](https://openreview.net/forum?id=5m3SEczOV8L) |  | 0 | Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. | Zhisheng Xiao, Karsten Kreis, Jan Kautz, Arash Vahdat |  |
| 66 |  |  [Geometry-Aware Gradient Algorithms for Neural Architecture Search](https://openreview.net/forum?id=MuSYkd1hxRP) |  | 0 | Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces. | Liam Li, Mikhail Khodak, Nina Balcan, Ameet Talwalkar |  |
| 67 |  |  [Learning-based Support Estimation in Sublinear Time](https://openreview.net/forum?id=tilovEHA3YS) |  | 0 | We consider the problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to $ \pm \varepsilon n$ from a sample of size $O(\log^2(1/\varepsilon) \cdot n/\log n)$, where $n$ is the data set size. Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of its frequency. We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly, to $$ \ \log (1/\varepsilon) \cdot n^{1-\Theta(1/\log(1/\varepsilon))}. $$ We evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm. | Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, Tal Wagner |  |
| 68 |  |  [Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach](https://openreview.net/forum?id=Cri3xz59ga) |  | 0 | This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large ($p$) and numerous ($n$) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as $n,p\to\infty$, to a deterministic limit involving simple (small-dimensional) statistics of the data. We prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. Experiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques. | Malik Tiomoko, Hafiz Tiomoko Ali, Romain Couillet |  |
| 69 |  |  [Autoregressive Entity Retrieval](https://openreview.net/forum?id=5k8F6UU39V) |  | 0 | Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE. | Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni |  |
| 70 |  |  [Systematic generalisation with group invariant predictions](https://openreview.net/forum?id=b9PoimzZFJ) |  | 0 |  | Faruk Ahmed, Yoshua Bengio, Harm van Seijen, Aaron C. Courville |  |
| 71 |  |  [Iterative Empirical Game Solving via Single Policy Best Response](https://openreview.net/forum?id=R4aWTjmrEKM) |  | 0 | Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (DRL). At each iteration, DRL is invoked to train a best response to a mixture of opponent policies. The repeated application of DRL poses an expensive computational burden as we look to apply this algorithm to more complex domains. We introduce two variations of PSRO designed to reduce the amount of simulation required during DRL training. Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy. The first, Mixed-Oracles, transfers knowledge from previous iterations of DRL, requiring training only against the opponent's newest policy. The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies. Learning against a single policy mitigates conflicting experiences on behalf of a learner facing an unobserved distribution of opponents. We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game. | Max Olan Smith, Thomas Anthony, Michael P. Wellman |  |
| 72 |  |  [Understanding the role of importance weighting for deep learning](https://openreview.net/forum?id=_WnwtieRHxM) |  | 0 |  | Da Xu, Yuting Ye, Chuanwei Ruan |  |
| 73 |  |  [Long-tail learning via logit adjustment](https://openreview.net/forum?id=37nvvqkCo5) |  | 0 | Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also makes naive learning biased towards dominant labels. In this paper, we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions. | Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, Sanjiv Kumar |  |
| 74 |  |  [DDPNOpt: Differential Dynamic Programming Neural Optimizer](https://openreview.net/forum?id=6s7ME_X5_Un) |  | 0 | Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory. | GuanHorng Liu, Tianrong Chen, Evangelos A. Theodorou |  |
| 75 |  |  [Learning with Feature-Dependent Label Noise: A Progressive Approach](https://openreview.net/forum?id=ZPa2SyGcbwh) |  | 0 | Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels. | Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, Chao Chen |  |
| 76 |  |  [Information Laundering for Model Privacy](https://openreview.net/forum?id=dyaIRud1zXg) |  | 0 | In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design. | Xinran Wang, Yu Xiang, Jun Gao, Jie Ding |  |
| 77 |  |  [Mutual Information State Intrinsic Control](https://openreview.net/forum?id=OthEq8I5v1) |  | 0 |  | Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, Wei Xu |  |
| 78 |  |  [Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods](https://openreview.net/forum?id=2m0g1wEafh) |  | 0 | Establishing a theoretical analysis that explains why deep learning can outperform shallow learning such as kernel methods is one of the biggest issues in the deep learning literature. Towards answering this question, we evaluate excess risk of a deep learning estimator trained by a noisy gradient descent with ridge regularization on a mildly overparameterized neural network, and discuss its superiority to a class of linear estimators that includes neural tangent kernel approach, random feature model, other kernel methods, $k$-NN estimator and so on. We consider a teacher-student regression model, and eventually show that {\it any} linear estimator can be outperformed by deep learning in a sense of the minimax optimal rate especially for a high dimension setting. The obtained excess bounds are so-called fast learning rate which is faster than $O(1/\sqrt{n})$ that is obtained by usual Rademacher complexity analysis. This discrepancy is induced by the non-convex geometry of the model and the noisy gradient descent used for neural network training provably reaches a near global optimal solution even though the loss landscape is highly non-convex. Although the noisy gradient descent does not employ any explicit or implicit sparsity inducing regularization, it shows a preferable generalization performance that dominates linear estimators. | Taiji Suzuki, Shunta Akiyama |  |
| 79 |  |  [How Does Mixup Help With Robustness and Generalization?](https://openreview.net/forum?id=8yKEo06dKNo) |  | 0 |  | Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, James Zou |  |
| 80 |  |  [Dataset Inference: Ownership Resolution in Machine Learning](https://openreview.net/forum?id=hvdKKV2yt7T) |  | 0 |  | Pratyush Maini, Mohammad Yaghini, Nicolas Papernot |  |
| 81 |  |  [Individually Fair Gradient Boosting](https://openreview.net/forum?id=JBAa9we1AL) |  | 0 |  | Alexander Vargo, Fan Zhang, Mikhail Yurochkin, Yuekai Sun |  |
| 82 |  |  [Large Scale Image Completion via Co-Modulated Generative Adversarial Networks](https://openreview.net/forum?id=sSjqmfsk95O) |  | 0 |  | Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric IChao Chang, Yan Xu |  |
| 83 |  |  [Self-Supervised Policy Adaptation during Deployment](https://openreview.net/forum?id=o_V-MjyyGV_) |  | 0 | In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse simulation environments from DeepMind Control suite and ViZDoom, as well as real robotic manipulation tasks in continuously changing environments, taking observations from an uncalibrated camera. Our method improves generalization in 31 out of 36 environments across various tasks and outperforms domain randomization on a majority of environments. Webpage and implementation: https://nicklashansen.github.io/PAD/. | Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, Xiaolong Wang |  |
| 84 |  |  [Sharpness-aware Minimization for Efficiently Improving Generalization](https://openreview.net/forum?id=6Tm1mposlrM) |  | 0 | In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. | Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur |  |
| 85 |  |  [PMI-Masking: Principled masking of correlated spans](https://openreview.net/forum?id=3Aoft6NWFej) |  | 0 | Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. To address this flaw, we propose PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking, entity/phrase masking, and random-span masking. Specifically, we show experimentally that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of pretraining. | Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin LeytonBrown, Moshe Tennenholtz, Yoav Shoham |  |
| 86 |  |  [Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images](https://openreview.net/forum?id=RLRXCV6DbEJ) |  | 0 |  | Rewon Child |  |
| 87 |  |  [Data-Efficient Reinforcement Learning with Self-Predictive Representations](https://openreview.net/forum?id=uCQfPZwRaUu) |  | 0 |  | Max Schwarzer, Ankesh Anand, Rishab Goel, R. Devon Hjelm, Aaron C. Courville, Philip Bachman |  |
| 88 |  |  [Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration](https://openreview.net/forum?id=w_7JMpGZRh0) |  | 0 | In this paper, we introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as and with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that our challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale. | Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, YuanHong Liao, Joshua B. Tenenbaum, Sanja Fidler, Antonio Torralba |  |
| 89 |  |  [A Good Image Generator Is What You Need for High-Resolution Video Synthesis](https://openreview.net/forum?id=6puCSjH3hwA) |  | 0 |  | Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, Sergey Tulyakov |  |
| 90 |  |  [UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers](https://openreview.net/forum?id=v9c7hr9ADKx) |  | 0 |  | Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang |  |
| 91 |  |  [BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration](https://openreview.net/forum?id=yHeg4PbFHh) |  | 0 | Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation. | Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, Hanjun Dai |  |
| 92 |  |  [Improving Adversarial Robustness via Channel-wise Activation Suppressing](https://openreview.net/forum?id=zQTezqCCtNx) |  | 0 |  | Yang Bai, Yuyuan Zeng, Yong Jiang, ShuTao Xia, Xingjun Ma, Yisen Wang |  |
| 93 |  |  [What are the Statistical Limits of Offline RL with Linear Function Approximation?](https://openreview.net/forum?id=30EvkP2aQLD) |  | 0 |  | Ruosong Wang, Dean P. Foster, Sham M. Kakade |  |
| 94 |  |  [Unlearnable Examples: Making Personal Data Unexploitable](https://openreview.net/forum?id=iAmZUo0DxC0) |  | 0 | The volume of "free" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is "nothing" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important ﬁrst step towards making personal data unexploitable to deep learning models. | Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Yisen Wang |  |
| 95 |  |  [Learning Mesh-Based Simulation with Graph Networks](https://openreview.net/forum?id=roNqYL0_XP) |  | 0 | Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks. | Tobias Pfaff, Meire Fortunato, Alvaro SanchezGonzalez, Peter W. Battaglia |  |
| 96 |  |  [Locally Free Weight Sharing for Network Width Search](https://openreview.net/forum?id=S0UdquAnr9k) |  | 0 |  | Xiu Su, Shan You, Tao Huang, Fei Wang, Chen Qian, Changshui Zhang, Chang Xu |  |
| 97 |  |  [Graph Convolution with Low-rank Learnable Local Filters](https://openreview.net/forum?id=9OHFhefeB86) |  | 0 |  | Xiuyuan Cheng, Zichen Miao, Qiang Qiu |  |
| 98 |  |  [Regularized Inverse Reinforcement Learning](https://openreview.net/forum?id=HgLO8yalfwc) |  | 0 |  | Wonseok Jeon, ChenYang Su, Paul Barde, Thang Doan, Derek Nowrouzezahrai, Joelle Pineau |  |
| 99 |  |  [Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking](https://openreview.net/forum?id=WznmQa42ZAx) |  | 0 | Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected $L_0$ norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions. | Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov |  |
| 100 |  |  [Deep Neural Network Fingerprinting by Conferrable Adversarial Examples](https://openreview.net/forum?id=VqzVhqxkjH1) |  | 0 |  | Nils Lukas, Yuxuan Zhang, Florian Kerschbaum |  |
| 101 |  |  [Tent: Fully Test-Time Adaptation by Entropy Minimization](https://openreview.net/forum?id=uXl3bZLkr3c) |  | 0 |  | Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno A. Olshausen, Trevor Darrell |  |
| 102 |  |  [GAN "Steerability" without optimization](https://openreview.net/forum?id=zDy_nQCXiIj) |  | 0 |  | Nurit Spingarn, Ron Banner, Tomer Michaeli |  |
| 103 |  |  [Contrastive Divergence Learning is a Time Reversal Adversarial Game](https://openreview.net/forum?id=MLSvqIHRidA) |  | 0 |  | Omer Yair, Tomer Michaeli |  |
| 104 |  |  [Topology-Aware Segmentation Using Discrete Morse Theory](https://openreview.net/forum?id=LGgdb4TS4Z) |  | 0 |  | Xiaoling Hu, Yusu Wang, Fuxin Li, Dimitris Samaras, Chao Chen |  |
| 105 |  |  [Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?](https://openreview.net/forum?id=Ut1vF_q_vC) |  | 0 |  | Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork |  |
| 106 |  |  [Predicting Infectiousness for Proactive Contact Tracing](https://openreview.net/forum?id=lVgB2FUbzuQ) |  | 0 |  | Yoshua Bengio, Prateek Gupta, Tegan Maharaj, Nasim Rahaman, Martin Weiss, Tristan Deleu, Eilif Benjamin Müller, Meng Qu, Victor Schmidt, PierreLuc StCharles, Hannah Alsdurf, Olexa Bilaniuk, David L. Buckeridge, Gaétan MarceauCaron, Pierre Luc Carrier, Joumana Ghosn, Satya OrtizGagne, Christopher J. Pal, Irina Rish, Bernhard Schölkopf, Abhinav Sharma, Jian Tang, Andrew Williams |  |
| 107 |  |  [Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control](https://openreview.net/forum?id=yr1mzrH3IC) |  | 0 |  | Zhuang Liu, Xuanlin Li, Bingyi Kang, Trevor Darrell |  |
| 108 |  |  [Minimum Width for Universal Approximation](https://openreview.net/forum?id=O-XJwyoIF-k) |  | 0 |  | Sejun Park, Chulhee Yun, Jaeho Lee, Jinwoo Shin |  |
| 109 |  |  [Towards Robustness Against Natural Language Word Substitutions](https://openreview.net/forum?id=ks5nebunVn_) |  | 0 |  | Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, Hong Liu |  |
| 110 |  |  [On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers](https://openreview.net/forum?id=p-NZIuwqhI4) |  | 0 |  | Kenji Kawaguchi |  |
| 111 |  |  [Structured Prediction as Translation between Augmented Natural Languages](https://openreview.net/forum?id=US-TP-xnXI) |  | 0 |  | Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, Rishita Anubhai, Cícero Nogueira dos Santos, Bing Xiang, Stefano Soatto |  |
| 112 |  |  [How Benign is Benign Overfitting ?](https://openreview.net/forum?id=g-wu9TMPODo) |  | 0 |  | Amartya Sanyal, Puneet K. Dokania, Varun Kanade, Philip H. S. Torr |  |
| 113 |  |  [Correcting experience replay for multi-agent communication](https://openreview.net/forum?id=xvxPuCkCNPO) |  | 0 |  | Sanjeevan Ahilan, Peter Dayan |  |
| 114 |  |  [Emergent Symbols through Binding in External Memory](https://openreview.net/forum?id=LSFCEb3GYU7) |  | 0 |  | Taylor Whittington Webb, Ishan Sinha, Jonathan D. Cohen |  |
| 115 |  |  [Influence Estimation for Generative Adversarial Networks](https://openreview.net/forum?id=opHLcXxYTC_) |  | 0 |  | Naoyuki Terashita, Hiroki Ohashi, Yuichi Nonaka, Takashi Kanemaru |  |
| 116 |  |  [PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics](https://openreview.net/forum?id=xCcdBRQEDW) |  | 0 |  | Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, Chuang Gan |  |
| 117 |  |  [Implicit Normalizing Flows](https://openreview.net/forum?id=8PS8m9oYtNy) |  | 0 |  | Cheng Lu, Jianfei Chen, Chongxuan Li, Qiuhao Wang, Jun Zhu |  |
| 118 |  |  [Support-set bottlenecks for video-text representation learning](https://openreview.net/forum?id=EqoXe2zmhrh) |  | 0 |  | Mandela Patrick, PoYao Huang, Yuki Markus Asano, Florian Metze, Alexander G. Hauptmann, João F. Henriques, Andrea Vedaldi |  |
| 119 |  |  [Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic](https://openreview.net/forum?id=LmUJqB1Cz8) |  | 0 |  | Deunsol Yoon, Sunghoon Hong, ByungJun Lee, KeeEung Kim |  |
| 120 |  |  [Learning Incompressible Fluid Dynamics from Scratch - Towards Fast, Differentiable Fluid Models that Generalize](https://openreview.net/forum?id=KUDUoRsEphu) |  | 0 |  | Nils Wandel, Michael Weinmann, Reinhard Klein |  |
| 121 |  |  [The Traveling Observer Model: Multi-task Learning Through Spatial Variable Embeddings](https://openreview.net/forum?id=qYda4oLEc1) |  | 0 |  | Elliot Meyerson, Risto Miikkulainen |  |
| 122 |  |  [Grounded Language Learning Fast and Slow](https://openreview.net/forum?id=wpSWuz_hyqA) |  | 0 |  | Felix Hill, Olivier Tieleman, Tamara von Glehn, Nathaniel Wong, Hamza Merzic, Stephen Clark |  |
| 123 |  |  [Long-tailed Recognition by Routing Diverse Distribution-Aware Experts](https://openreview.net/forum?id=D9I3drBz4UC) |  | 0 |  | Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, Stella X. Yu |  |
| 124 |  |  [Differentially Private Learning Needs Better Features (or Much More Data)](https://openreview.net/forum?id=YTWGvpFOQD-) |  | 0 |  | Florian Tramèr, Dan Boneh |  |
| 125 |  |  [Unsupervised Object Keypoint Learning using Local Spatial Predictability](https://openreview.net/forum?id=GJwMHetHc73) |  | 0 |  | Anand Gopalakrishnan, Sjoerd van Steenkiste, Jürgen Schmidhuber |  |
| 126 |  |  [On Statistical Bias In Active Learning: How and When to Fix It](https://openreview.net/forum?id=JiYq3eqTKY) |  | 0 |  | Sebastian Farquhar, Yarin Gal, Tom Rainforth |  |
| 127 |  |  [Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time](https://openreview.net/forum?id=0N8jUH4JMv6) |  | 0 |  | Tolga Ergen, Mert Pilanci |  |
| 128 |  |  [Generalization in data-driven models of primary visual cortex](https://openreview.net/forum?id=Tp7kI90Htd) |  | 0 |  | KonstantinKlemens Lurz, Mohammad Bashiri, Konstantin Willeke, Akshay Kumar Jagadish, Eric Wang, Edgar Y. Walker, Santiago A. Cadena, Taliah Muhammad, Erick Cobos, Andreas S. Tolias, Alexander S. Ecker, Fabian H. Sinz |  |
| 129 |  |  [Mathematical Reasoning via Self-supervised Skip-tree Training](https://openreview.net/forum?id=YmqAnY0CMEy) |  | 0 |  | Markus Norman Rabe, Dennis Lee, Kshitij Bansal, Christian Szegedy |  |
| 130 |  |  [Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters](https://openreview.net/forum?id=rcQdycl0zyk) |  | 0 |  | Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, Jie Fu |  |
| 131 |  |  [Distributional Sliced-Wasserstein and Applications to Generative Modeling](https://openreview.net/forum?id=QYjO70ACDK) |  | 0 |  | Khai Nguyen, Nhat Ho, Tung Pham, Hung Bui |  |
| 132 |  |  [Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors](https://openreview.net/forum?id=9EsrXMzlFQY) |  | 0 |  | Yu Sun, Jiaming Liu, Yiran Sun, Brendt Wohlberg, Ulugbek Kamilov |  |
| 133 |  |  [DeepAveragers: Offline Reinforcement Learning By Solving Derived Non-Parametric MDPs](https://openreview.net/forum?id=eMP1j9efXtX) |  | 0 |  | Aayam Kumar Shrestha, Stefan Lee, Prasad Tadepalli, Alan Fern |  |
| 134 |  |  [Learning from Protein Structure with Geometric Vector Perceptrons](https://openreview.net/forum?id=1YLJDvSx6J4) |  | 0 |  | Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, Ron O. Dror |  |
| 135 |  |  [Behavioral Cloning from Noisy Demonstrations](https://openreview.net/forum?id=zrT3HcsWSAt) |  | 0 |  | Fumihiro Sasaki, Ryota Yamashina |  |
| 136 |  |  [Undistillable: Making A Nasty Teacher That CANNOT teach students](https://openreview.net/forum?id=0zvfm-nZqQs) |  | 0 |  | Haoyu Ma, Tianlong Chen, TingKuei Hu, Chenyu You, Xiaohui Xie, Zhangyang Wang |  |
| 137 |  |  [Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows](https://openreview.net/forum?id=WiGQBFuVRv) |  | 0 |  | Kashif Rasul, AbdulSaboor Sheikh, Ingmar Schuster, Urs M. Bergmann, Roland Vollgraf |  |
| 138 |  |  [Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels](https://openreview.net/forum?id=GY6-6sTvGaf) |  | 0 |  | Denis Yarats, Ilya Kostrikov, Rob Fergus |  |
| 139 |  |  [HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark](https://openreview.net/forum?id=_0kaDkv3dVf) |  | 0 |  | Chaojian Li, Zhongzhi Yu, Yonggan Fu, Yongan Zhang, Yang Zhao, Haoran You, Qixuan Yu, Yue Wang, Cong Hao, Yingyan Lin |  |
| 140 |  |  [Practical Real Time Recurrent Learning with a Sparse Approximation](https://openreview.net/forum?id=q3KSThy2GwB) |  | 0 |  | Jacob Menick, Erich Elsen, Utku Evci, Simon Osindero, Karen Simonyan, Alex Graves |  |
| 141 |  |  [Random Feature Attention](https://openreview.net/forum?id=QtTKTdVrFBB) |  | 0 |  | Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong |  |
| 142 |  |  [A Gradient Flow Framework For Analyzing Network Pruning](https://openreview.net/forum?id=rumv7QmLUue) |  | 0 |  | Ekdeep Singh Lubana, Robert P. Dick |  |
| 143 |  |  [Recurrent Independent Mechanisms](https://openreview.net/forum?id=mLcmdlEUxy-) |  | 0 |  | Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, Bernhard Schölkopf |  |
| 144 |  |  [The Intrinsic Dimension of Images and Its Impact on Learning](https://openreview.net/forum?id=XJk19XzGq2J) |  | 0 |  | Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom Goldstein |  |
| 145 |  |  [Uncertainty Sets for Image Classifiers using Conformal Prediction](https://openreview.net/forum?id=eNdiU_DbM9) |  | 0 |  | Anastasios Nikolas Angelopoulos, Stephen Bates, Michael I. Jordan, Jitendra Malik |  |
| 146 |  |  [Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy](https://openreview.net/forum?id=Rhsu5qD36cL) |  | 0 |  | Akinori F. Ebihara, Taiki Miyagawa, Kazuyuki Sakurai, Hitoshi Imaoka |  |
| 147 |  |  [Disentangled Recurrent Wasserstein Autoencoder](https://openreview.net/forum?id=O7ms4LFdsX) |  | 0 |  | Jun Han, Martin Renqiang Min, Ligong Han, Li Erran Li, Xuan Zhang |  |
| 148 |  |  [Generalization bounds via distillation](https://openreview.net/forum?id=EGdFhBzmAwB) |  | 0 |  | Daniel Hsu, Ziwei Ji, Matus Telgarsky, Lan Wang |  |
| 149 |  |  [Neural Approximate Sufficient Statistics for Implicit Models](https://openreview.net/forum?id=SRDuJssQud) |  | 0 |  | Yanzhi Chen, Dinghuai Zhang, Michael U. Gutmann, Aaron C. Courville, Zhanxing Zhu |  |
| 150 |  |  [A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference](https://openreview.net/forum?id=9xC2tWEwBD) |  | 0 |  | Sanghyun Hong, Yigitcan Kaya, IonutVlad Modoranu, Tudor Dumitras |  |
| 151 |  |  [Orthogonalizing Convolutional Layers with the Cayley Transform](https://openreview.net/forum?id=Pbj8H_jEHYv) |  | 0 |  | Asher Trockman, J. Zico Kolter |  |
| 152 |  |  [LambdaNetworks: Modeling long-range Interactions without Attention](https://openreview.net/forum?id=xTJEN-ggl1b) |  | 0 |  | Irwan Bello |  |
| 153 |  |  [Mind the Pad - CNNs Can Develop Blind Spots](https://openreview.net/forum?id=m1CD7tPubNy) |  | 0 |  | Bilal Alsallakh, Narine Kokhlikyan, Vivek Miglani, Jun Yuan, Orion ReblitzRichardson |  |
| 154 |  |  [Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning](https://openreview.net/forum?id=wS0UFjsNYjn) |  | 0 |  | Dong Bok Lee, Dongchan Min, Seanie Lee, Sung Ju Hwang |  |
| 155 |  |  [Fast Geometric Projections for Local Robustness Certification](https://openreview.net/forum?id=zWy1uxjDdZJ) |  | 0 |  | Aymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, Corina S. Pasareanu |  |
| 156 |  |  [Fidelity-based Deep Adiabatic Scheduling](https://openreview.net/forum?id=NECTfffOvn1) |  | 0 |  | Eli Ovits, Lior Wolf |  |
| 157 |  |  [On Self-Supervised Image Representations for GAN Evaluation](https://openreview.net/forum?id=NeRdBeTionN) |  | 0 |  | Stanislav Morozov, Andrey Voynov, Artem Babenko |  |
| 158 |  |  [Retrieval-Augmented Generation for Code Summarization via Hybrid GNN](https://openreview.net/forum?id=zv-typ1gPxA) |  | 0 |  | Shangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow, Yang Liu |  |
| 159 |  |  [Self-supervised Visual Reinforcement Learning with Object-centric Representations](https://openreview.net/forum?id=xppLmXCbOw1) |  | 0 |  | Andrii Zadaianchuk, Maximilian Seitzer, Georg Martius |  |
| 160 |  |  [Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies](https://openreview.net/forum?id=_XYzwxPIQu6) |  | 0 |  | Dominik Schmidt, Georgia Koppe, Zahra Monfared, Max Beutelspacher, Daniel Durstewitz |  |
| 161 |  |  [Neural Topic Model via Optimal Transport](https://openreview.net/forum?id=Oos98K9Lv-k) |  | 0 |  | He Zhao, Dinh Phung, Viet Huynh, Trung Le, Wray L. Buntine |  |
| 162 |  |  [Memory Optimization for Deep Networks](https://openreview.net/forum?id=bnY0jm4l59) |  | 0 |  | Aashaka Shah, ChaoYuan Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krähenbühl |  |
| 163 |  |  [Stabilized Medical Image Attacks](https://openreview.net/forum?id=QfTXQiGYudJ) |  | 0 |  | Gege Qi, Lijun Gong, Yibing Song, Kai Ma, Yefeng Zheng |  |
| 164 |  |  [Quantifying Differences in Reward Functions](https://openreview.net/forum?id=LwEQnp6CYev) |  | 0 |  | Adam Gleave, Michael Dennis, Shane Legg, Stuart Russell, Jan Leike |  |
| 165 |  |  [MARS: Markov Molecular Sampling for Multi-objective Drug Discovery](https://openreview.net/forum?id=kHSu4ebxFXY) |  | 0 |  | Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, Lei Li |  |
| 166 |  |  [Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs](https://openreview.net/forum?id=Jnspzp-oIZE) |  | 0 |  | Pim de Haan, Maurice Weiler, Taco Cohen, Max Welling |  |
| 167 |  |  [RMSprop converges with proper hyper-parameter](https://openreview.net/forum?id=3UDSdyIcBDA) |  | 0 |  | Naichen Shi, Dawei Li, Mingyi Hong, Ruoyu Sun |  |
| 168 |  |  [Revisiting Dynamic Convolution via Matrix Decomposition](https://openreview.net/forum?id=YwpZmcAehZ) |  | 0 |  | Yunsheng Li, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei Chen, Nuno Vasconcelos |  |
| 169 |  |  [Explainable Deep One-Class Classification](https://openreview.net/forum?id=A5VV3UyIQz) |  | 0 |  | Philipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, KlausRobert Müller |  |
| 170 |  |  [Taking Notes on the Fly Helps Language Pre-Training](https://openreview.net/forum?id=lU5Rs_wCweN) |  | 0 |  | Qiyu Wu, Chen Xing, Yatao Li, Guolin Ke, Di He, TieYan Liu |  |
| 171 |  |  [Mixed-Features Vectors and Subspace Splitting](https://openreview.net/forum?id=l-LGlk4Yl6G) |  | 0 |  | Alejandro PimentelAlarcón, Daniel L. PimentelAlarcón |  |
| 172 |  |  [Neural Pruning via Growing Regularization](https://openreview.net/forum?id=o966_Is_nPA) |  | 0 |  | Huan Wang, Can Qin, Yulun Zhang, Yun Fu |  |
| 173 |  |  [Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design](https://openreview.net/forum?id=6k7VdojAIK) |  | 0 |  | Xiufeng Yang, Tanuj Kr Aasawat, Kazuki Yoshizoe |  |
| 174 |  |  [Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition](https://openreview.net/forum?id=5jRVa89sZk) |  | 0 |  | Yangming Li, Lemao Liu, Shuming Shi |  |
| 175 |  |  [Deep Networks and the Multiple Manifold Problem](https://openreview.net/forum?id=O-6Pm_d_Q-) |  | 0 |  | Sam Buchanan, Dar Gilboa, John Wright |  |
| 176 |  |  [Knowledge distillation via softmax regression representation learning](https://openreview.net/forum?id=ZzwDy_wiWv) |  | 0 |  | Jing Yang, Brais Martínez, Adrian Bulat, Georgios Tzimiropoulos |  |
| 177 |  |  [Nearest Neighbor Machine Translation](https://openreview.net/forum?id=7wCBOfJ8hJM) |  | 0 |  | Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis |  |
| 178 |  |  [WrapNet: Neural Net Inference with Ultra-Low-Precision Arithmetic](https://openreview.net/forum?id=3SqrRe8FWQ-) |  | 0 |  | Renkun Ni, HongMin Chu, Oscar Castañeda, Pingyeh Chiang, Christoph Studer, Tom Goldstein |  |
| 179 |  |  [Wandering within a world: Online contextualized few-shot learning](https://openreview.net/forum?id=oZIvHV04XgC) |  | 0 |  | Mengye Ren, Michael Louis Iuzzolino, Michael Curtis Mozer, Richard S. Zemel |  |
| 180 |  |  [Few-Shot Learning via Learning the Representation, Provably](https://openreview.net/forum?id=pW2Q2xLwIMD) |  | 0 |  | Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, Qi Lei |  |
| 181 |  |  [AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models](https://openreview.net/forum?id=QkRbdiiEjM) |  | 0 |  | Ke Sun, Zhanxing Zhu, Zhouchen Lin |  |
| 182 |  |  [MultiModalQA: complex question answering over text, tables and images](https://openreview.net/forum?id=ee6W5UgQLa) |  | 0 |  | Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant |  |
| 183 |  |  [Net-DNF: Effective Deep Modeling of Tabular Data](https://openreview.net/forum?id=73WTGs96kho) |  | 0 |  | Liran Katzir, Gal Elidan, Ran ElYaniv |  |
| 184 |  |  [Optimal Regularization can Mitigate Double Descent](https://openreview.net/forum?id=7R7fAoUygoa) |  | 0 |  | Preetum Nakkiran, Prayaag Venkat, Sham M. Kakade, Tengyu Ma |  |
| 185 |  |  [Meta Back-Translation](https://openreview.net/forum?id=3jjmdp7Hha) |  | 0 |  | Hieu Pham, Xinyi Wang, Yiming Yang, Graham Neubig |  |
| 186 |  |  [Learning A Minimax Optimizer: A Pilot Study](https://openreview.net/forum?id=nkIDwI6oO4_) |  | 0 |  | Jiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin, Zhangyang Wang |  |
| 187 |  |  [A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels](https://openreview.net/forum?id=ajOrOhQOsYx) |  | 0 |  | Leon Lang, Maurice Weiler |  |
| 188 |  |  [Viewmaker Networks: Learning Views for Unsupervised Representation Learning](https://openreview.net/forum?id=enoVQWLsfyL) |  | 0 |  | Alex Tamkin, Mike Wu, Noah D. Goodman |  |
| 189 |  |  [Scalable Transfer Learning with Expert Models](https://openreview.net/forum?id=23ZjUGpjcc) |  | 0 |  | Joan Puigcerver, Carlos Riquelme Ruiz, Basil Mustafa, Cédric Renggli, André Susano Pinto, Sylvain Gelly, Daniel Keysers, Neil Houlsby |  |
| 190 |  |  [Negative Data Augmentation](https://openreview.net/forum?id=Ovp8dvB8IBH) |  | 0 |  | Abhishek Sinha, Kumar Ayush, Jiaming Song, Burak Uzkent, Hongxia Jin, Stefano Ermon |  |
| 191 |  |  [Fantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers](https://openreview.net/forum?id=JCRblSgs34Z) |  | 0 |  | Sahil Singla, Soheil Feizi |  |
| 192 |  |  [CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding](https://openreview.net/forum?id=Ozk9MrX1hvA) |  | 0 |  | Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Weizhu Chen, Jiawei Han |  |
| 193 |  |  [Teaching with Commentaries](https://openreview.net/forum?id=4RbdgBh9gE) |  | 0 |  | Aniruddh Raghu, Maithra Raghu, Simon Kornblith, David Duvenaud, Geoffrey E. Hinton |  |
| 194 |  |  [MixKD: Towards Efficient Distillation of Large-scale Language Models](https://openreview.net/forum?id=UFGEelJkLu5) |  | 0 |  | Kevin J. Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, Lawrence Carin |  |
| 195 |  |  [FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders](https://openreview.net/forum?id=N6JECD-PI5w) |  | 0 |  | Pengyu Cheng, Weituo Hao, Siyang Yuan, Shijing Si, Lawrence Carin |  |
| 196 |  |  [Probabilistic Numeric Convolutional Neural Networks](https://openreview.net/forum?id=T1XmO8ScKim) |  | 0 |  | Marc Anton Finzi, Roberto Bondesan, Max Welling |  |
| 197 |  |  [Computational Separation Between Convolutional and Fully-Connected Networks](https://openreview.net/forum?id=hkMoYYEkBoI) |  | 0 |  | Eran Malach, Shai ShalevShwartz |  |
| 198 |  |  [On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines](https://openreview.net/forum?id=nzpLWnVAyah) |  | 0 |  | Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow |  |
| 199 |  |  [Variational Information Bottleneck for Effective Low-Resource Fine-Tuning](https://openreview.net/forum?id=kvhzKz-_DMF) |  | 0 |  | Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson |  |
| 200 |  |  [Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching](https://openreview.net/forum?id=01olnfLIbD) |  | 0 |  | Jonas Geiping, Liam H. Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein |  |
| 201 |  |  [Deberta: decoding-Enhanced Bert with Disentangled Attention](https://openreview.net/forum?id=XPZIaotutsD) |  | 0 |  | Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen |  |
| 202 |  |  [Optimism in Reinforcement Learning with Generalized Linear Function Approximation](https://openreview.net/forum?id=CBmJwzneppz) |  | 0 |  | Yining Wang, Ruosong Wang, Simon Shaolei Du, Akshay Krishnamurthy |  |
| 203 |  |  [Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning](https://openreview.net/forum?id=6DOZ8XNNfGN) |  | 0 |  | Elan Sopher Markowitz, Keshav Balasubramanian, Mehrnoosh Mirtaheri, Sami AbuElHaija, Bryan Perozzi, Greg Ver Steeg, Aram Galstyan |  |
| 204 |  |  [Diverse Video Generation using a Gaussian Process Trigger](https://openreview.net/forum?id=Qm7R_SdqTpT) |  | 0 |  | Gaurav Shrivastava, Abhinav Shrivastava |  |
| 205 |  |  [Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU](https://openreview.net/forum?id=lqU2cs3Zca) |  | 0 |  | Patrick Kidger, Terry J. Lyons |  |
| 206 |  |  [MoPro: Webly Supervised Learning with Momentum Prototypes](https://openreview.net/forum?id=0-EYBhgw80y) |  | 0 |  | Junnan Li, Caiming Xiong, Steven C. H. Hoi |  |
| 207 |  |  [A Universal Representation Transformer Layer for Few-Shot Image Classification](https://openreview.net/forum?id=04cII6MumYV) |  | 0 |  | Lu Liu, William L. Hamilton, Guodong Long, Jing Jiang, Hugo Larochelle |  |
| 208 |  |  [Primal Wasserstein Imitation Learning](https://openreview.net/forum?id=TtYSU29zgR) |  | 0 |  | Robert Dadashi, Léonard Hussenot, Matthieu Geist, Olivier Pietquin |  |
| 209 |  |  [Learning perturbation sets for robust machine learning](https://openreview.net/forum?id=MIDckA56aD) |  | 0 |  | Eric Wong, J. Zico Kolter |  |
| 210 |  |  [CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks](https://openreview.net/forum?id=XI-OJ5yyse) |  | 0 |  | Jiaqi Ma, Bo Chang, Xuefei Zhang, Qiaozhu Mei |  |
| 211 |  |  [On the Critical Role of Conventions in Adaptive Human-AI Collaboration](https://openreview.net/forum?id=8Ln-Bq0mZcy) |  | 0 |  | Andy Shih, Arjun Sawhney, Jovana Kondic, Stefano Ermon, Dorsa Sadigh |  |
| 212 |  |  [On the Bottleneck of Graph Neural Networks and its Practical Implications](https://openreview.net/forum?id=i80OPhOCVH2) |  | 0 |  | Uri Alon, Eran Yahav |  |
| 213 |  |  [The geometry of integration in text classification RNNs](https://openreview.net/forum?id=42kiJ7n_8xO) |  | 0 |  | Kyle Aitken, Vinay Venkatesh Ramasesh, Ankush Garg, Yuan Cao, David Sussillo, Niru Maheswaranathan |  |
| 214 |  |  [Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://openreview.net/forum?id=jh-rTtvkGeM) |  | 0 |  | Jeremy Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, Ameet Talwalkar |  |
| 215 |  |  [CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning](https://openreview.net/forum?id=SK7A5pdrgov) |  | 0 |  | Ossama Ahmed, Frederik Träuble, Anirudh Goyal, Alexander Neitz, Manuel Wuthrich, Yoshua Bengio, Bernhard Schölkopf, Stefan Bauer |  |
| 216 |  |  [Empirical or Invariant Risk Minimization? A Sample Complexity Perspective](https://openreview.net/forum?id=jrA5GAccy_) |  | 0 |  | Kartik Ahuja, Jun Wang, Amit Dhurandhar, Karthikeyan Shanmugam, Kush R. Varshney |  |
| 217 |  |  [Scaling Symbolic Methods using Gradients for Neural Model Explanation](https://openreview.net/forum?id=V5j-jdoDDP) |  | 0 |  | Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh, Patrick Riley |  |
| 218 |  |  [Control-Aware Representations for Model-based Reinforcement Learning](https://openreview.net/forum?id=dgd4EJqsbW5) |  | 0 |  | Brandon Cui, Yinlam Chow, Mohammad Ghavamzadeh |  |
| 219 |  |  [C-Learning: Learning to Achieve Goals via Recursive Classification](https://openreview.net/forum?id=tc5qisoB-C) |  | 0 |  | Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine |  |
| 220 |  |  [The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers](https://openreview.net/forum?id=guetrIHLFGI) |  | 0 |  | Preetum Nakkiran, Behnam Neyshabur, Hanie Sedghi |  |
| 221 |  |  [Improving VAEs' Robustness to Adversarial Attack](https://openreview.net/forum?id=-Hs_otp2RB) |  | 0 |  | Matthew Willetts, Alexander Camuto, Tom Rainforth, Stephen J. Roberts, Christopher C. Holmes |  |
| 222 |  |  [What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions](https://openreview.net/forum?id=Qm8UNVCFdh) |  | 0 |  | Kiana Ehsani, Daniel Gordon, Thomas Hai Dang Nguyen, Roozbeh Mottaghi, Ali Farhadi |  |
| 223 |  |  [EEC: Learning to Encode and Regenerate Images for Continual Learning](https://openreview.net/forum?id=lWaz5a9lcFU) |  | 0 |  | Ali Ayub, Alan R. Wagner |  |
| 224 |  |  [Impact of Representation Learning in Linear Bandits](https://openreview.net/forum?id=edJ_HipawCa) |  | 0 |  | Jiaqi Yang, Wei Hu, Jason D. Lee, Simon Shaolei Du |  |
| 225 |  |  [MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space](https://openreview.net/forum?id=XjYgR6gbCEc) |  | 0 |  | TszHim Cheung, DitYan Yeung |  |
| 226 |  |  [The Recurrent Neural Tangent Kernel](https://openreview.net/forum?id=3T9iFICe0Y9) |  | 0 |  | Sina Alemohammad, Zichao Wang, Randall Balestriero, Richard G. Baraniuk |  |
| 227 |  |  [Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows](https://openreview.net/forum?id=MBpHUFrcG2x) |  | 0 |  | Chris Cannella, Mohammadreza Soltani, Vahid Tarokh |  |
| 228 |  |  [Learning the Pareto Front with Hypernetworks](https://openreview.net/forum?id=NjF772F4ZZR) |  | 0 |  | Aviv Navon, Aviv Shamsian, Ethan Fetaya, Gal Chechik |  |
| 229 |  |  [Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors](https://openreview.net/forum?id=YLewtnvKgR7) |  | 0 |  | Ali Harakeh, Steven L. Waslander |  |
| 230 |  |  [Predicting Classification Accuracy When Adding New Unobserved Classes](https://openreview.net/forum?id=Y9McSeEaqUh) |  | 0 |  | Yuli Slavutsky, Yuval Benjamini |  |
| 231 |  |  [BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction](https://openreview.net/forum?id=POWv6hDd9XH) |  | 0 |  | Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu |  |
| 232 |  |  [No MCMC for me: Amortized sampling for fast and stable training of energy-based models](https://openreview.net/forum?id=ixpSxO9flk3) |  | 0 |  | Will Sussman Grathwohl, Jacob Jin Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, David Duvenaud |  |
| 233 |  |  [GraphCodeBERT: Pre-training Code Representations with Data Flow](https://openreview.net/forum?id=jLoC4ez43PZ) |  | 0 |  | Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, Ming Zhou |  |
| 234 |  |  [Conservative Safety Critics for Exploration](https://openreview.net/forum?id=iaO86DUuKi) |  | 0 |  | Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, Animesh Garg |  |
| 235 |  |  [Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors](https://openreview.net/forum?id=uKhGRvM8QNH) |  | 0 |  | Linfeng Zhang, Kaisheng Ma |  |
| 236 |  |  [A Temporal Kernel Approach for Deep Learning with Continuous-time Information](https://openreview.net/forum?id=whE31dn74cL) |  | 0 |  | Da Xu, Chuanwei Ruan, Evren Körpeoglu, Sushant Kumar, Kannan Achan |  |
| 237 |  |  [For self-supervised learning, Rationality implies generalization, provably](https://openreview.net/forum?id=Srmggo3b3X6) |  | 0 |  | Yamini Bansal, Gal Kaplun, Boaz Barak |  |
| 238 |  |  [How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision](https://openreview.net/forum?id=Wi5KUNlqWty) |  | 0 |  | Dongkwan Kim, Alice Oh |  |
| 239 |  |  [Interpretable Models for Granger Causality Using Self-explaining Neural Networks](https://openreview.net/forum?id=DEa4JdMWRHp) |  | 0 |  | Ricards Marcinkevics, Julia E. Vogt |  |
| 240 |  |  [Meta-learning Symmetries by Reparameterization](https://openreview.net/forum?id=-QxT4mJdijq) |  | 0 |  | Allan Zhou, Tom Knowles, Chelsea Finn |  |
| 241 |  |  [Removing Undesirable Feature Contributions Using Out-of-Distribution Data](https://openreview.net/forum?id=eIHYL6fpbkA) |  | 0 |  | Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee, Sungroh Yoon |  |
| 242 |  |  [Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models](https://openreview.net/forum?id=a2gqxKDvYys) |  | 0 |  | Justin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt |  |
| 243 |  |  [On the Universality of the Double Descent Peak in Ridgeless Regression](https://openreview.net/forum?id=0IO5VdnSAaH) |  | 0 |  | David Holzmüller |  |
| 244 |  |  [Fair Mixup: Fairness via Interpolation](https://openreview.net/forum?id=DNl5s5BXeBn) |  | 0 |  | ChingYao Chuang, Youssef Mroueh |  |
| 245 |  |  [Self-supervised Learning from a Multi-view Perspective](https://openreview.net/forum?id=-bdp_8Itjwp) |  | 0 |  | YaoHung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, LouisPhilippe Morency |  |
| 246 |  |  [Integrating Categorical Semantics into Unsupervised Domain Translation](https://openreview.net/forum?id=IMPA6MndSXU) |  | 0 |  | Samuel LavoieMarchildon, Faruk Ahmed, Aaron C. Courville |  |
| 247 |  |  [The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods](https://openreview.net/forum?id=aYuZO9DIdnn) |  | 0 |  | Louis Thiry, Michael Arbel, Eugene Belilovsky, Edouard Oyallon |  |
| 248 |  |  [Open Question Answering over Tables and Text](https://openreview.net/forum?id=MmCRswl1UYl) |  | 0 |  | Wenhu Chen, MingWei Chang, Eva Schlinger, William Yang Wang, William W. Cohen |  |
| 249 |  |  [Evaluation of Similarity-based Explanations](https://openreview.net/forum?id=9uvhpyQwzM_) |  | 0 |  | Kazuaki Hanawa, Sho Yokoi, Satoshi Hara, Kentaro Inui |  |
| 250 |  |  [A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima](https://openreview.net/forum?id=wXgk_iCiYGo) |  | 0 |  | Zeke Xie, Issei Sato, Masashi Sugiyama |  |
| 251 |  |  [How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?](https://openreview.net/forum?id=fgd7we_uZa6) |  | 0 |  | Zixiang Chen, Yuan Cao, Difan Zou, Quanquan Gu |  |
| 252 |  |  [Auction Learning as a Two-Player Game](https://openreview.net/forum?id=YHdeAO61l6T) |  | 0 |  | Jad Rahme, Samy Jelassi, S. Matthew Weinberg |  |
| 253 |  |  [Robust Reinforcement Learning on State Observations with Learned Optimal Adversary](https://openreview.net/forum?id=sCZbhBvqQaU) |  | 0 |  | Huan Zhang, Hongge Chen, Duane S. Boning, ChoJui Hsieh |  |
| 254 |  |  [Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning](https://openreview.net/forum?id=-6vS_4Kfz0) |  | 0 |  | Shauharda Khadka, Estelle Aflalo, Mattias Marder, Avrech BenDavid, Santiago Miret, Shie Mannor, Tamir Hazan, Hanlin Tang, Somdeb Majumdar |  |
| 255 |  |  [Hierarchical Autoregressive Modeling for Neural Video Compression](https://openreview.net/forum?id=TK_6nNb_C7q) |  | 0 |  | Ruihan Yang, Yibo Yang, Joseph Marino, Stephan Mandt |  |
| 256 |  |  [Individually Fair Rankings](https://openreview.net/forum?id=71zCSP_HuBN) |  | 0 |  | Amanda Bower, Hamid Eftekhari, Mikhail Yurochkin, Yuekai Sun |  |
| 257 |  |  [Learning Neural Generative Dynamics for Molecular Conformation Generation](https://openreview.net/forum?id=pAbm1qfheGk) |  | 0 |  | Minkai Xu, Shitong Luo, Yoshua Bengio, Jian Peng, Jian Tang |  |
| 258 |  |  [Efficient Certified Defenses Against Patch Attacks on Image Classifiers](https://openreview.net/forum?id=hr-3PMvDpil) |  | 0 |  | Jan Hendrik Metzen, Maksym Yatsura |  |
| 259 |  |  [Convex Regularization behind Neural Reconstruction](https://openreview.net/forum?id=VErQxgyrbfn) |  | 0 |  | Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, John M. Pauly |  |
| 260 |  |  [Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits](https://openreview.net/forum?id=iKQAk8a2kM0) |  | 0 |  | Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, ShuTao Xia |  |
| 261 |  |  [Generalized Multimodal ELBO](https://openreview.net/forum?id=5Y21V0RDBV) |  | 0 |  | Thomas M. Sutter, Imant Daunhawer, Julia E. Vogt |  |
| 262 |  |  [Large-width functional asymptotics for deep Gaussian neural networks](https://openreview.net/forum?id=0aW6lYOYB7d) |  | 0 |  | Daniele Bracale, Stefano Favaro, Sandra Fortini, Stefano Peluchetti |  |
| 263 |  |  [Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent](https://openreview.net/forum?id=H8UHdhWG6A3) |  | 0 |  | El Mahdi El Mhamdi, Rachid Guerraoui, Sébastien Rouault |  |
| 264 |  |  [Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders](https://openreview.net/forum?id=agHLCOBM5jP) |  | 0 |  | Mangal Prakash, Alexander Krull, Florian Jug |  |
| 265 |  |  [Auxiliary Learning by Implicit Differentiation](https://openreview.net/forum?id=n7wIfYPdVet) |  | 0 |  | Aviv Navon, Idan Achituve, Haggai Maron, Gal Chechik, Ethan Fetaya |  |
| 266 |  |  [Balancing Constraints and Rewards with Meta-Gradient D4PG](https://openreview.net/forum?id=TQt98Ya7UMP) |  | 0 |  | Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, Timothy A. Mann |  |
| 267 |  |  [Adversarially Guided Actor-Critic](https://openreview.net/forum?id=_mQp5cr_iNy) |  | 0 |  | Yannis FletBerliac, Johan Ferret, Olivier Pietquin, Philippe Preux, Matthieu Geist |  |
| 268 |  |  [DARTS-: Robustly Stepping out of Performance Collapse Without Indicators](https://openreview.net/forum?id=KLH36ELmwIB) |  | 0 |  | Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, Junchi Yan |  |
| 269 |  |  [Are wider nets better given the same number of parameters?](https://openreview.net/forum?id=_zx8Oka09eF) |  | 0 |  | Anna Golubeva, Guy GurAri, Behnam Neyshabur |  |
| 270 |  |  [Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks](https://openreview.net/forum?id=FZ1oTwcXchK) |  | 0 |  | Shikuang Deng, Shi Gu |  |
| 271 |  |  [Deep Equals Shallow for ReLU Networks in Kernel Regimes](https://openreview.net/forum?id=aDjoksTpXOP) |  | 0 |  | Alberto Bietti, Francis R. Bach |  |
| 272 |  |  [Graph Coarsening with Neural Networks](https://openreview.net/forum?id=uxpzitPEooJ) |  | 0 |  | Chen Cai, Dingkang Wang, Yusu Wang |  |
| 273 |  |  [Early Stopping in Deep Networks: Double Descent and How to Eliminate it](https://openreview.net/forum?id=tlV90jvZbw) |  | 0 |  | Reinhard Heckel, Fatih Furkan Yilmaz |  |
| 274 |  |  [Efficient Inference of Flexible Interaction in Spiking-neuron Networks](https://openreview.net/forum?id=aGfU_xziEX8) |  | 0 |  | Feng Zhou, Yixuan Zhang, Jun Zhu |  |
| 275 |  |  [DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation](https://openreview.net/forum?id=R2ZlTVPx0Gk) |  | 0 |  | Alexandre Ramé, Matthieu Cord |  |
| 276 |  |  [Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks](https://openreview.net/forum?id=KYPz4YsCPj) |  | 0 |  | Yanbang Wang, YenYu Chang, Yunyu Liu, Jure Leskovec, Pan Li |  |
| 277 |  |  [FairBatch: Batch Selection for Model Fairness](https://openreview.net/forum?id=YNnpaAKeCfx) |  | 0 |  | Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh |  |
| 278 |  |  [Representation Balancing Offline Model-based Reinforcement Learning](https://openreview.net/forum?id=QpNz8r_Ri2Y) |  | 0 |  | ByungJun Lee, Jongmin Lee, KeeEung Kim |  |
| 279 |  |  [Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction](https://openreview.net/forum?id=iOnhIy-a-0n) |  | 0 |  | Wei Deng, Qi Feng, Georgios Karagiannis, Guang Lin, Faming Liang |  |
| 280 |  |  [The Importance of Pessimism in Fixed-Dataset Policy Optimization](https://openreview.net/forum?id=E3Ys6a1NTGT) |  | 0 |  | Jacob Buckman, Carles Gelada, Marc G. Bellemare |  |
| 281 |  |  [Interpreting Knowledge Graph Relation Representation from Word Embeddings](https://openreview.net/forum?id=gLWj29369lW) |  | 0 |  | Carl Allen, Ivana Balazevic, Timothy M. Hospedales |  |
| 282 |  |  [Hopfield Networks is All You Need](https://openreview.net/forum?id=tL89RnzIiCd) |  | 0 |  | Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David P. Kreil, Michael K. Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter |  |
| 283 |  |  [Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs](https://openreview.net/forum?id=9EKHN1jOlA) |  | 0 |  | Cheng Wang, Carolin Lawrence, Mathias Niepert |  |
| 284 |  |  [Understanding the failure modes of out-of-distribution generalization](https://openreview.net/forum?id=fSTD6NFIW_b) |  | 0 |  | Vaishnavh Nagarajan, Anders Andreassen, Behnam Neyshabur |  |
| 285 |  |  [Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule](https://openreview.net/forum?id=45uOPa46Kh) |  | 0 |  | Shuhei Kurita, Kyunghyun Cho |  |
| 286 |  |  [Emergent Road Rules In Multi-Agent Driving Environments](https://openreview.net/forum?id=d8Q1mt2Ghw) |  | 0 |  | Avik Pal, Jonah Philion, YuanHong Liao, Sanja Fidler |  |
| 287 |  |  [Wasserstein-2 Generative Networks](https://openreview.net/forum?id=bEoxzW_EXsa) |  | 0 |  | Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, Evgeny Burnaev |  |
| 288 |  |  [Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics](https://openreview.net/forum?id=9r30XCjf5Dt) |  | 0 |  | Yanchao Sun, Da Huo, Furong Huang |  |
| 289 |  |  [Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data](https://openreview.net/forum?id=YtMG5ex0ou) |  | 0 |  | Francesco Tonolini, Pablo Garcia Moreno, Andreas C. Damianou, Roderick MurraySmith |  |
| 290 |  |  [Monotonic Kronecker-Factored Lattice](https://openreview.net/forum?id=0pxiMpCyBtr) |  | 0 |  | William Taylor Bakst, Nobuyuki Morioka, Erez Louidor |  |
| 291 |  |  [LEAF: A Learnable Frontend for Audio Classification](https://openreview.net/forum?id=jM76BCb6F9m) |  | 0 |  | Neil Zeghidour, Olivier Teboul, Félix de Chaumont Quitry, Marco Tagliasacchi |  |
| 292 |  |  [Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms](https://openreview.net/forum?id=GFsU8a0sGB) |  | 0 |  | Maruan AlShedivat, Jennifer Gillenwater, Eric P. Xing, Afshin Rostamizadeh |  |
| 293 |  |  [Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments](https://openreview.net/forum?id=MtEE0CktZht) |  | 0 |  | Daochen Zha, Wenye Ma, Lei Yuan, Xia Hu, Ji Liu |  |
| 294 |  |  [Partitioned Learned Bloom Filters](https://openreview.net/forum?id=6BRLOfrMhW) |  | 0 |  | Kapil Vaidya, Eric Knorr, Michael Mitzenmacher, Tim Kraska |  |
| 295 |  |  [Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval](https://openreview.net/forum?id=zeFrfgyZln) |  | 0 |  | Lee Xiong, Chenyan Xiong, Ye Li, KwokFung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, Arnold Overwijk |  |
| 296 |  |  [Auxiliary Task Update Decomposition: the Good, the Bad and the neutral](https://openreview.net/forum?id=1GTma8HwlYp) |  | 0 |  | Lucio M. Dery, Yann N. Dauphin, David Grangier |  |
| 297 |  |  [SSD: A Unified Framework for Self-Supervised Outlier Detection](https://openreview.net/forum?id=v5gjXpmR8J) |  | 0 |  | Vikash Sehwag, Mung Chiang, Prateek Mittal |  |
| 298 |  |  [Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning](https://openreview.net/forum?id=Y87Ri-GNHYu) |  | 0 |  | Valerie Chen, Abhinav Gupta, Kenneth Marino |  |
| 299 |  |  [Revisiting Few-sample BERT Fine-tuning](https://openreview.net/forum?id=cO1IH43yUF) |  | 0 |  | Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, Yoav Artzi |  |
| 300 |  |  [Tilted Empirical Risk Minimization](https://openreview.net/forum?id=K5YasWXZT3O) |  | 0 |  | Tian Li, Ahmad Beirami, Maziar Sanjabi, Virginia Smith |  |
| 301 |  |  [Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS](https://openreview.net/forum?id=vK9WrZ0QYQ) |  | 0 |  | Lin Chen, Sheng Xu |  |
| 302 |  |  [On the Transfer of Disentangled Representations in Realistic Settings](https://openreview.net/forum?id=8VXvj1QNRl1) |  | 0 |  | Andrea Dittadi, Frederik Träuble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole Winther, Stefan Bauer, Bernhard Schölkopf |  |
| 303 |  |  [Calibration tests beyond classification](https://openreview.net/forum?id=-bxf89v3Nx) |  | 0 |  | David Widmann, Fredrik Lindsten, Dave Zachariah |  |
| 304 |  |  [Overparameterisation and worst-case generalisation: friend or foe?](https://openreview.net/forum?id=jphnJNOwe36) |  | 0 |  | Aditya Krishna Menon, Ankit Singh Rawat, Sanjiv Kumar |  |
| 305 |  |  [You Only Need Adversarial Supervision for Semantic Image Synthesis](https://openreview.net/forum?id=yvQKLaqNE6M) |  | 0 |  | Edgar Schönfeld, Vadim Sushko, Dan Zhang, Juergen Gall, Bernt Schiele, Anna Khoreva |  |
| 306 |  |  [Learning to Recombine and Resample Data For Compositional Generalization](https://openreview.net/forum?id=PS3IMnScugk) |  | 0 |  | Ekin Akyürek, Afra Feyza Akyürek, Jacob Andreas |  |
| 307 |  |  [A Critique of Self-Expressive Deep Subspace Clustering](https://openreview.net/forum?id=FOyuZ26emy) |  | 0 |  | Benjamin David Haeffele, Chong You, René Vidal |  |
| 308 |  |  [INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving](https://openreview.net/forum?id=O6LPudowNQm) |  | 0 |  | Yuhuai Wu, Albert Q. Jiang, Jimmy Ba, Roger Baker Grosse |  |
| 309 |  |  [Improved Estimation of Concentration Under ℓp-Norm Distance Metrics Using Half Spaces](https://openreview.net/forum?id=BUlyHkzjgmA) |  | 0 |  | Jack Prescott, Xiao Zhang, David E. Evans |  |
| 310 |  |  [Adaptive Federated Optimization](https://openreview.net/forum?id=LkFG3lB13U5) |  | 0 |  | Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecný, Sanjiv Kumar, Hugh Brendan McMahan |  |
| 311 |  |  [On the Dynamics of Training Attention Models](https://openreview.net/forum?id=1OCTOShAmqB) |  | 0 |  | Haoye Lu, Yongyi Mao, Amiya Nayak |  |
| 312 |  |  [Linear Convergent Decentralized Optimization with Compression](https://openreview.net/forum?id=84gjULz1t5) |  | 0 |  | Xiaorui Liu, Yao Li, Rongrong Wang, Jiliang Tang, Ming Yan |  |
| 313 |  |  [Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation](https://openreview.net/forum?id=uR9LaO_QxF) |  | 0 |  | Emilio Parisotto, Ruslan Salakhutdinov |  |
| 314 |  |  [Large Associative Memory Problem in Neurobiology and Machine Learning](https://openreview.net/forum?id=X4y_10OX-hX) |  | 0 |  | Dmitry Krotov, John J. Hopfield |  |
| 315 |  |  [Protecting DNNs from Theft using an Ensemble of Diverse Models](https://openreview.net/forum?id=LucJxySuJcE) |  | 0 |  | Sanjay Kariyappa, Atul Prakash, Moinuddin K. Qureshi |  |
| 316 |  |  [Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry](https://openreview.net/forum?id=LVotkZmYyDi) |  | 0 |  | Ziyi Chen, Yi Zhou, Tengyu Xu, Yingbin Liang |  |
| 317 |  |  [Contextual Dropout: An Efficient Sample-Dependent Dropout Module](https://openreview.net/forum?id=ct8_a9h1M) |  | 0 |  | Xinjie Fan, Shujian Zhang, Korawat Tanwisuth, Xiaoning Qian, Mingyuan Zhou |  |
| 318 |  |  [Mirostat: a Neural Text decoding Algorithm that directly controls perplexity](https://openreview.net/forum?id=W1G1JZEIy5_) |  | 0 |  | Sourya Basu, Govardana Sachitanandam Ramachandran, Nitish Shirish Keskar, Lav R. Varshney |  |
| 319 |  |  [DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues](https://openreview.net/forum?id=kDnal_bbb-E) |  | 0 |  | Rishabh Joshi, Vidhisha Balachandran, Shikhar Vashishth, Alan W. Black, Yulia Tsvetkov |  |
| 320 |  |  [Multi-Time Attention Networks for Irregularly Sampled Time Series](https://openreview.net/forum?id=4c0J6lwQ4_) |  | 0 |  | Satya Narayan Shukla, Benjamin M. Marlin |  |
| 321 |  |  [Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling](https://openreview.net/forum?id=aD1_5zowqV) |  | 0 |  | Yang Zhao, Jianwen Xie, Ping Li |  |
| 322 |  |  [Unsupervised Audiovisual Synthesis via Exemplar Autoencoders](https://openreview.net/forum?id=43VKWxg_Sqr) |  | 0 |  | Kangle Deng, Aayush Bansal, Deva Ramanan |  |
| 323 |  |  [A Learning Theoretic Perspective on Local Explainability](https://openreview.net/forum?id=7aL-OtQrBWD) |  | 0 |  | Jeffrey Li, Vaishnavh Nagarajan, Gregory Plumb, Ameet Talwalkar |  |
| 324 |  |  [SEED: Self-supervised Distillation For Visual Representation](https://openreview.net/forum?id=AHm3dbp7D1D) |  | 0 |  | Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, Zicheng Liu |  |
| 325 |  |  [Isometric Propagation Network for Generalized Zero-shot Learning](https://openreview.net/forum?id=-mWcQVLPSPy) |  | 0 |  | Lu Liu, Tianyi Zhou, Guodong Long, Jing Jiang, Xuanyi Dong, Chengqi Zhang |  |
| 326 |  |  [Effective and Efficient Vote Attack on Capsule Networks](https://openreview.net/forum?id=33rtZ4Sjwjn) |  | 0 |  | Jindong Gu, Baoyuan Wu, Volker Tresp |  |
| 327 |  |  [Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization](https://openreview.net/forum?id=mEdwVCRJuX4) |  | 0 |  | Kaidi Cao, Yining Chen, Junwei Lu, Nikos Aréchiga, Adrien Gaidon, Tengyu Ma |  |
| 328 |  |  [Continuous Wasserstein-2 Barycenter Estimation without Minimax Optimization](https://openreview.net/forum?id=3tFAs5E-Pe) |  | 0 |  | Alexander Korotin, Lingxiao Li, Justin Solomon, Evgeny Burnaev |  |
| 329 |  |  [Neural Thompson Sampling](https://openreview.net/forum?id=tkAtoZkcUnm) |  | 0 |  | Weitong Zhang, Dongruo Zhou, Lihong Li, Quanquan Gu |  |
| 330 |  |  [Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics](https://openreview.net/forum?id=q8qLAbQBupm) |  | 0 |  | Daniel Kunin, Javier SagastuyBreña, Surya Ganguli, Daniel L. K. Yamins, Hidenori Tanaka |  |
| 331 |  |  [Neural gradients are near-lognormal: improved quantized and sparse training](https://openreview.net/forum?id=EoFNy62JGd) |  | 0 |  | Brian Chmiel, Liad BenUri, Moran Shkolnik, Elad Hoffer, Ron Banner, Daniel Soudry |  |
| 332 |  |  [RODE: Learning Roles to Decompose Multi-Agent Tasks](https://openreview.net/forum?id=TTUVg6vkNjK) |  | 0 |  | Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, Chongjie Zhang |  |
| 333 |  |  [Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction](https://openreview.net/forum?id=3RLN4EPMdYd) |  | 0 |  | Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, Jing Yu Koh, Thomas E. Huang, Hyungsuk Yoon, Honglak Lee, Seunghoon Hong |  |
| 334 |  |  [Physics-aware, probabilistic model order reduction with guaranteed stability](https://openreview.net/forum?id=vyY0jnWG-tK) |  | 0 |  | Sebastian Kaltenbach, PhaedonStelios Koutsourelakis |  |
| 335 |  |  [Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System](https://openreview.net/forum?id=kLbhLJ8OT12) |  | 0 |  | Jianhong Wang, Yuan Zhang, TaeKyun Kim, Yunjie Gu |  |
| 336 |  |  [Learning explanations that are hard to vary](https://openreview.net/forum?id=hb1sDDSLbV) |  | 0 |  | Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, Bernhard Schölkopf |  |
| 337 |  |  [Efficient Generalized Spherical CNNs](https://openreview.net/forum?id=rWZz3sJfCkm) |  | 0 |  | Oliver J. Cobb, Christopher G. R. Wallis, Augustine N. MavorParker, Augustin Marignier, Matthew A. Price, Mayeul d'Avezac, Jason D. McEwen |  |
| 338 |  |  [Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks](https://openreview.net/forum?id=ULQdiUTHe3y) |  | 0 |  | Jan Schuchardt, Aleksandar Bojchevski, Johannes Klicpera, Stephan Günnemann |  |
| 339 |  |  [Entropic gradient descent algorithms and wide flat minima](https://openreview.net/forum?id=xjXg0bnoDmS) |  | 0 |  | Fabrizio Pittorino, Carlo Lucibello, Christoph Feinauer, Gabriele Perugini, Carlo Baldassi, Elizaveta Demyanenko, Riccardo Zecchina |  |
| 340 |  |  [Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning](https://openreview.net/forum?id=jDdzh5ul-d) |  | 0 |  | Haibo Yang, Minghong Fang, Jia Liu |  |
| 341 |  |  [Categorical Normalizing Flows via Continuous Transformations](https://openreview.net/forum?id=-GLNZeVDuik) |  | 0 |  | Phillip Lippe, Efstratios Gavves |  |
| 342 |  |  [Learning to Represent Action Values as a Hypergraph on the Action Vertices](https://openreview.net/forum?id=Xv_s64FiXTv) |  | 0 |  | Arash Tavakoli, Mehdi Fatemi, Petar Kormushev |  |
| 343 |  |  [Debiasing Concept-based Explanations with Causal Analysis](https://openreview.net/forum?id=6puUoArESGp) |  | 0 |  | Mohammad Taha Bahadori, David Heckerman |  |
| 344 |  |  [Lifelong Learning of Compositional Structures](https://openreview.net/forum?id=ADWd4TJO13G) |  | 0 |  | Jorge A. Mendez, Eric Eaton |  |
| 345 |  |  [Rethinking Embedding Coupling in Pre-trained Language Models](https://openreview.net/forum?id=xpFFI_NtgpW) |  | 0 |  | Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder |  |
| 346 |  |  [Creative Sketch Generation](https://openreview.net/forum?id=gwnoVHIES05) |  | 0 |  | Songwei Ge, Vedanuj Goswami, Larry Zitnick, Devi Parikh |  |
| 347 |  |  [Concept Learners for Few-Shot Learning](https://openreview.net/forum?id=eJIJF3-LoZO) |  | 0 |  | Kaidi Cao, Maria Brbic, Jure Leskovec |  |
| 348 |  |  [Domain Generalization with MixStyle](https://openreview.net/forum?id=6xHJ37MVxxp) |  | 0 |  | Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang |  |
| 349 |  |  [DeLighT: Deep and Light-weight Transformer](https://openreview.net/forum?id=ujmgfuxSLrO) |  | 0 |  | Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi |  |
| 350 |  |  [Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy](https://openreview.net/forum?id=pqZV_srUVmK) |  | 0 |  | Zuyue Fu, Zhuoran Yang, Zhaoran Wang |  |
| 351 |  |  [Mastering Atari with Discrete World Models](https://openreview.net/forum?id=0oabwyZbOu) |  | 0 |  | Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, Jimmy Ba |  |
| 352 |  |  [Learning Neural Event Functions for Ordinary Differential Equations](https://openreview.net/forum?id=kW_zpEmMLdP) |  | 0 |  | Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel |  |
| 353 |  |  [Contemplating Real-World Object Classification](https://openreview.net/forum?id=Q4EUywJIkqr) |  | 0 |  | Ali Borji |  |
| 354 |  |  [Neural Spatio-Temporal Point Processes](https://openreview.net/forum?id=XQQA6-So14) |  | 0 |  | Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel |  |
| 355 |  |  [Generative Time-series Modeling with Fourier Flows](https://openreview.net/forum?id=PpshD0AXfA) |  | 0 |  | Ahmed M. Alaa, Alex James Chan, Mihaela van der Schaar |  |
| 356 |  |  [DOP: Off-Policy Multi-Agent Decomposed Policy Gradients](https://openreview.net/forum?id=6FqKiVAdI3Y) |  | 0 |  | Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, Chongjie Zhang |  |
| 357 |  |  [The Risks of Invariant Risk Minimization](https://openreview.net/forum?id=BbNIbVPJ-42) |  | 0 |  | Elan Rosenfeld, Pradeep Kumar Ravikumar, Andrej Risteski |  |
| 358 |  |  [DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation](https://openreview.net/forum?id=GTGb3M_KcUl) |  | 0 |  | Minjia Zhang, Menghao Li, Chi Wang, Mingqin Li |  |
| 359 |  |  [Bag of Tricks for Adversarial Training](https://openreview.net/forum?id=Xb8xvrtB8Ce) |  | 0 |  | Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu |  |
| 360 |  |  [Learning with Instance-Dependent Label Noise: A Sample Sieve Approach](https://openreview.net/forum?id=2VXyy9mIyU3) |  | 0 |  | Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, Yang Liu |  |
| 361 |  |  [Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL](https://openreview.net/forum?id=fmtSg8591Q) |  | 0 |  | Xiaoyu Chen, Jiachen Hu, Lihong Li, Liwei Wang |  |
| 362 |  |  [Unbiased Teacher for Semi-Supervised Object Detection](https://openreview.net/forum?id=MJIve1zgR_) |  | 0 |  | YenCheng Liu, ChihYao Ma, Zijian He, ChiaWen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, Peter Vajda |  |
| 363 |  |  [Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks](https://openreview.net/forum?id=9l0K4OM-oXE) |  | 0 |  | Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma |  |
| 364 |  |  [Contrastive Learning with Adversarial Perturbations for Conditional Text Generation](https://openreview.net/forum?id=Wga_hrCa3P3) |  | 0 |  | Seanie Lee, Dong Bok Lee, Sung Ju Hwang |  |
| 365 |  |  [When Optimizing f-Divergence is Robust with Label Noise](https://openreview.net/forum?id=WesiCoRVQ15) |  | 0 |  | Jiaheng Wei, Yang Liu |  |
| 366 |  |  [Conditional Generative Modeling via Learning the Latent Space](https://openreview.net/forum?id=VJnrYcnRc6) |  | 0 |  | Sameera Ramasinghe, Kanchana Nisal Ranasinghe, Salman H. Khan, Nick Barnes, Stephen Gould |  |
| 367 |  |  [Text Generation by Learning from Demonstrations](https://openreview.net/forum?id=RovX-uQ1Hua) |  | 0 |  | Richard Yuanzhe Pang, He He |  |
| 368 |  |  [Learning Long-term Visual Dynamics with Region Proposal Interaction Networks](https://openreview.net/forum?id=_X_4Akcd8Re) |  | 0 |  | Haozhi Qi, Xiaolong Wang, Deepak Pathak, Yi Ma, Jitendra Malik |  |
| 369 |  |  [ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations](https://openreview.net/forum?id=xCxXwTzx4L1) |  | 0 |  | Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, Deepak K. Gupta |  |
| 370 |  |  [Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation](https://openreview.net/forum?id=b7g3_ZMHnT0) |  | 0 |  | Mrigank Raman, Aaron Chan, Siddhant Agarwal, Peifeng Wang, Hansen Wang, Sungchul Kim, Ryan A. Rossi, Handong Zhao, Nedim Lipka, Xiang Ren |  |
| 371 |  |  [IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning](https://openreview.net/forum?id=xzqLpqRzxLq) |  | 0 |  | Manli Zhang, Jianhong Zhang, Zhiwu Lu, Tao Xiang, Mingyu Ding, Songfang Huang |  |
| 372 |  |  [The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods](https://openreview.net/forum?id=L7WD8ZdscQ5) |  | 0 |  | Wei Tao, Sheng Long, Gaowei Wu, Qing Tao |  |
| 373 |  |  [Training with Quantization Noise for Extreme Model Compression](https://openreview.net/forum?id=dV19Yyi1fS3) |  | 0 |  | Pierre Stock, Angela Fan, Benjamin Graham, Edouard Grave, Rémi Gribonval, Hervé Jégou, Armand Joulin |  |
| 374 |  |  [Adaptive Extra-Gradient Methods for Min-Max Optimization and Games](https://openreview.net/forum?id=R0a0kFI3dJx) |  | 0 |  | Kimon Antonakopoulos, Elena Veronica Belmega, Panayotis Mertikopoulos |  |
| 375 |  |  [Distilling Knowledge from Reader to Retriever for Question Answering](https://openreview.net/forum?id=NTEz-6wysdb) |  | 0 |  | Gautier Izacard, Edouard Grave |  |
| 376 |  |  [Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization](https://openreview.net/forum?id=lvRTC669EY_) |  | 0 |  | Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Shaolei Du, Yu Wang, Yi Wu |  |
| 377 |  |  [not-MIWAE: Deep Generative Modelling with Missing not at Random Data](https://openreview.net/forum?id=tu29GQT0JFy) |  | 0 |  | Niels Bruun Ipsen, PierreAlexandre Mattei, Jes Frellsen |  |
| 378 |  |  [IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression](https://openreview.net/forum?id=MBOyiNnYthd) |  | 0 |  | Rianne van den Berg, Alexey A. Gritsenko, Mostafa Dehghani, Casper Kaae Sønderby, Tim Salimans |  |
| 379 |  |  [Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning](https://openreview.net/forum?id=unI5ucw_Jk) |  | 0 |  | Alihan Hüyük, Daniel Jarrett, Cem Tekin, Mihaela van der Schaar |  |
| 380 |  |  [Learning with AMIGo: Adversarially Motivated Intrinsic Goals](https://openreview.net/forum?id=ETBc_MIMgoX) |  | 0 |  | Andres Campero, Roberta Raileanu, Heinrich Küttler, Joshua B. Tenenbaum, Tim Rocktäschel, Edward Grefenstette |  |
| 381 |  |  [Incorporating Symmetry into Deep Dynamics Models for Improved Generalization](https://openreview.net/forum?id=wta_8Hx2KD) |  | 0 |  | Rui Wang, Robin Walters, Rose Yu |  |
| 382 |  |  [CaPC Learning: Confidential and Private Collaborative Learning](https://openreview.net/forum?id=h2EbJ4_wMVq) |  | 0 |  | Christopher A. ChoquetteChoo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, Xiao Wang |  |
| 383 |  |  [Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds](https://openreview.net/forum?id=UwGY2qjqoLD) |  | 0 |  | Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee |  |
| 384 |  |  [A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks](https://openreview.net/forum?id=TR-Nj6nFx42) |  | 0 |  | Renjie Liao, Raquel Urtasun, Richard S. Zemel |  |
| 385 |  |  [Clairvoyance: A Pipeline Toolkit for Medical Time Series](https://openreview.net/forum?id=xnC8YwKUE3k) |  | 0 |  | Daniel Jarrett, Jinsung Yoon, Ioana Bica, Zhaozhi Qian, Ari Ercole, Mihaela van der Schaar |  |
| 386 |  |  [Self-supervised Representation Learning with Relative Predictive Coding](https://openreview.net/forum?id=068E_JSq9O) |  | 0 |  | YaoHung Hubert Tsai, Martin Q. Ma, Muqiao Yang, Han Zhao, LouisPhilippe Morency, Ruslan Salakhutdinov |  |
| 387 |  |  [Offline Model-Based Optimization via Normalized Maximum Likelihood Estimation](https://openreview.net/forum?id=FmMKSO4e8JK) |  | 0 |  | Justin Fu, Sergey Levine |  |
| 388 |  |  [On the Impossibility of Global Convergence in Multi-Loss Optimization](https://openreview.net/forum?id=NQbnPjPYaG6) |  | 0 |  | Alistair Letcher |  |
| 389 |  |  [A Block Minifloat Representation for Training Deep Neural Networks](https://openreview.net/forum?id=6zaTwpNSsQ2) |  | 0 |  | Sean Fox, Seyedramin Rasoulinezhad, Julian Faraone, David Boland, Philip H. W. Leong |  |
| 390 |  |  [Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs](https://openreview.net/forum?id=8nl0k08uMi) |  | 0 |  | Matthew L. Leavitt, Ari S. Morcos |  |
| 391 |  |  [Discrete Graph Structure Learning for Forecasting Multiple Time Series](https://openreview.net/forum?id=WEHSlH5mOk) |  | 0 |  | Chao Shang, Jie Chen, Jinbo Bi |  |
| 392 |  |  [Contrastive Learning with Hard Negative Samples](https://openreview.net/forum?id=CR1XOQ0UTh-) |  | 0 |  | Joshua David Robinson, ChingYao Chuang, Suvrit Sra, Stefanie Jegelka |  |
| 393 |  |  [Intraclass clustering: an implicit learning ability that regularizes DNNs](https://openreview.net/forum?id=tqOvYpjPax2) |  | 0 |  | Simon Carbonnelle, Christophe De Vleeschouwer |  |
| 394 |  |  [Sliced Kernelized Stein Discrepancy](https://openreview.net/forum?id=t0TaKv0Gx6Z) |  | 0 |  | Wenbo Gong, Yingzhen Li, José Miguel HernándezLobato |  |
| 395 |  |  [Denoising Diffusion Implicit Models](https://openreview.net/forum?id=St1giarCHLP) |  | 0 |  | Jiaming Song, Chenlin Meng, Stefano Ermon |  |
| 396 |  |  [Hierarchical Reinforcement Learning by Discovering Intrinsic Options](https://openreview.net/forum?id=r-gPPHEjpmw) |  | 0 |  | Jesse Zhang, Haonan Yu, Wei Xu |  |
| 397 |  |  [Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval](https://openreview.net/forum?id=EMHoBG0avc1) |  | 0 |  | Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du, Patrick Lewis, William Yang Wang, Yashar Mehdad, Scott Yih, Sebastian Riedel, Douwe Kiela, Barlas Oguz |  |
| 398 |  |  [Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective](https://openreview.net/forum?id=gIHd-5X324) |  | 0 |  | Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, Qian Zhang |  |
| 399 |  |  [A Design Space Study for LISTA and Beyond](https://openreview.net/forum?id=GMgHyUPrXa) |  | 0 |  | Tianjian Meng, Xiaohan Chen, Yifan Jiang, Zhangyang Wang |  |
| 400 |  |  [What Should Not Be Contrastive in Contrastive Learning](https://openreview.net/forum?id=CZ8Y3NzuVzO) |  | 0 |  | Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell |  |
| 401 |  |  [Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth](https://openreview.net/forum?id=KJNcAkY8tY4) |  | 0 |  | Thao Nguyen, Maithra Raghu, Simon Kornblith |  |
| 402 |  |  [Learning to Set Waypoints for Audio-Visual Navigation](https://openreview.net/forum?id=cR91FAodFMe) |  | 0 |  | Changan Chen, Sagnik Majumder, Ziad AlHalah, Ruohan Gao, Santhosh Kumar Ramakrishnan, Kristen Grauman |  |
| 403 |  |  [Semi-supervised Keypoint Localization](https://openreview.net/forum?id=yFJ67zTeI2) |  | 0 |  | Olga Moskvyak, Frédéric Maire, Feras Dayoub, Mahsa Baktashmotlagh |  |
| 404 |  |  [Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective](https://openreview.net/forum?id=Cnon5ezMHtu) |  | 0 |  | Wuyang Chen, Xinyu Gong, Zhangyang Wang |  |
| 405 |  |  [Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers](https://openreview.net/forum?id=eqBwg3AcIAK) |  | 0 |  | Benjamin Eysenbach, Shreyas Chaudhari, Swapnil Asawa, Sergey Levine, Ruslan Salakhutdinov |  |
| 406 |  |  [Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning](https://openreview.net/forum?id=ce6CFXBh30h) |  | 0 |  | Wonyong Jeong, Jaehong Yoon, Eunho Yang, Sung Ju Hwang |  |
| 407 |  |  [Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates](https://openreview.net/forum?id=P6_q1BRxY8Q) |  | 0 |  | Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, Chuchu Fan |  |
| 408 |  |  [Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate](https://openreview.net/forum?id=3X64RLgzY6O) |  | 0 |  | Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu |  |
| 409 |  |  [Fast And Slow Learning Of Recurrent Independent Mechanisms](https://openreview.net/forum?id=Lc28QAB4ypz) |  | 0 |  | Kanika Madan, Nan Rosemary Ke, Anirudh Goyal, Bernhard Schölkopf, Yoshua Bengio |  |
| 410 |  |  [Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples](https://openreview.net/forum?id=pzpytjk3Xb2) |  | 0 |  | Ziang Yan, Yiwen Guo, Jian Liang, Changshui Zhang |  |
| 411 |  |  [A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks](https://openreview.net/forum?id=vVjIW3sEc1s) |  | 0 |  | Nikunj Saunshi, Sadhika Malladi, Sanjeev Arora |  |
| 412 |  |  [Representation Learning for Sequence Data with Deep Autoencoding Predictive Components](https://openreview.net/forum?id=Naqw7EHIfrv) |  | 0 |  | Junwen Bai, Weiran Wang, Yingbo Zhou, Caiming Xiong |  |
| 413 |  |  [A unifying view on implicit bias in training linear neural networks](https://openreview.net/forum?id=ZsZM-4iMQkH) |  | 0 |  | Chulhee Yun, Shankar Krishnan, Hossein Mobahi |  |
| 414 |  |  [What Makes Instance Discrimination Good for Transfer Learning?](https://openreview.net/forum?id=tC6iW2UUbJf) |  | 0 |  | Nanxuan Zhao, Zhirong Wu, Rynson W. H. Lau, Stephen Lin |  |
| 415 |  |  [Learning Accurate Entropy Model with Global Reference for Image Compression](https://openreview.net/forum?id=cTbIjyrUVwJ) |  | 0 |  | Yichen Qian, Zhiyu Tan, Xiuyu Sun, Ming Lin, Dongyang Li, Zhenhong Sun, Hao Li, Rong Jin |  |
| 416 |  |  [Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search](https://openreview.net/forum?id=5jzlpHvvRk) |  | 0 |  | Peidong Liu, Gengwei Zhang, Bochao Wang, Hang Xu, Xiaodan Liang, Yong Jiang, Zhenguo Li |  |
| 417 |  |  [Effective Abstract Reasoning with Dual-Contrast Network](https://openreview.net/forum?id=ldxlzGYWDmW) |  | 0 |  | Tao Zhuo, Mohan S. Kankanhalli |  |
| 418 |  |  [Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning](https://openreview.net/forum?id=7aogOj_VYO0) |  | 0 |  | Da Yu, Huishuai Zhang, Wei Chen, TieYan Liu |  |
| 419 |  |  [Set Prediction without Imposing Structure as Conditional Density Estimation](https://openreview.net/forum?id=04ArenGOz3) |  | 0 |  | David W. Zhang, Gertjan J. Burghouts, Cees G. M. Snoek |  |
| 420 |  |  [Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation](https://openreview.net/forum?id=e12NDM7wkEY) |  | 0 |  | Yaling Tao, Kentaro Takagi, Kouta Nakata |  |
| 421 |  |  [Language-Agnostic Representation Learning of Source Code from Structure and Context](https://openreview.net/forum?id=Xh5eMZVONGF) |  | 0 |  | Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, Stephan Günnemann |  |
| 422 |  |  [Training GANs with Stronger Augmentations via Contrastive Discriminator](https://openreview.net/forum?id=eo6U4CAwVmg) |  | 0 |  | Jongheon Jeong, Jinwoo Shin |  |
| 423 |  |  [Influence Functions in Deep Learning Are Fragile](https://openreview.net/forum?id=xHKVVHGDOEk) |  | 0 |  | Samyadeep Basu, Phillip Pope, Soheil Feizi |  |
| 424 |  |  [Separation and Concentration in Deep Networks](https://openreview.net/forum?id=8HhkbjrWLdE) |  | 0 |  | John Zarka, Florentin Guth, Stéphane Mallat |  |
| 425 |  |  [Colorization Transformer](https://openreview.net/forum?id=5NA1PinlGFu) |  | 0 |  | Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner |  |
| 426 |  |  [Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization](https://openreview.net/forum?id=kmqjgSNXby) |  | 0 |  | Michael R. Zhang, Thomas Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, Mohammad Norouzi |  |
| 427 |  |  [FedBN: Federated Learning on Non-IID Features via Local Batch Normalization](https://openreview.net/forum?id=6YEQUn0QICG) |  | 0 |  | Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, Qi Dou |  |
| 428 |  |  [Learning Robust State Abstractions for Hidden-Parameter Block MDPs](https://openreview.net/forum?id=fmOOI2a3tQP) |  | 0 |  | Amy Zhang, Shagun Sodhani, Khimya Khetarpal, Joelle Pineau |  |
| 429 |  |  [Meta-Learning with Neural Tangent Kernels](https://openreview.net/forum?id=Ti87Pv5Oc8) |  | 0 |  | Yufan Zhou, Zhenyi Wang, Jiayi Xian, Changyou Chen, Jinhui Xu |  |
| 430 |  |  [Continual learning in recurrent neural networks](https://openreview.net/forum?id=8xeBUgD8u9) |  | 0 |  | Benjamin Ehret, Christian Henning, Maria R. Cervera, Alexander Meulemans, Johannes von Oswald, Benjamin F. Grewe |  |
| 431 |  |  [A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention](https://openreview.net/forum?id=ZK6vTvb84s) |  | 0 |  | Grégoire Mialon, Dexiong Chen, Alexandre d'Aspremont, Julien Mairal |  |
| 432 |  |  [Learning "What-if" Explanations for Sequential Decision-Making](https://openreview.net/forum?id=h0de3QWtGG) |  | 0 |  | Ioana Bica, Daniel Jarrett, Alihan Hüyük, Mihaela van der Schaar |  |
| 433 |  |  [Improving Transformation Invariance in Contrastive Representation Learning](https://openreview.net/forum?id=NomEDgIEBwE) |  | 0 |  | Adam Foster, Rattana Pukdee, Tom Rainforth |  |
| 434 |  |  [Shapley explainability on the data manifold](https://openreview.net/forum?id=OPyWRrcjVQw) |  | 0 |  | Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, Ilya Feige |  |
| 435 |  |  [Noise or Signal: The Role of Image Backgrounds in Object Recognition](https://openreview.net/forum?id=gl3D-xY7wLq) |  | 0 |  | Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, Aleksander Madry |  |
| 436 |  |  [Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation](https://openreview.net/forum?id=HOFxeCutxZR) |  | 0 |  | Peiye Zhuang, Oluwasanmi Koyejo, Alexander G. Schwing |  |
| 437 |  |  [Perceptual Adversarial Robustness: Defense Against Unseen Threat Models](https://openreview.net/forum?id=dFwBosAcJkN) |  | 0 |  | Cassidy Laidlaw, Sahil Singla, Soheil Feizi |  |
| 438 |  |  [Zero-Cost Proxies for Lightweight NAS](https://openreview.net/forum?id=0cmMMy8J5q) |  | 0 |  | Mohamed S. Abdelfattah, Abhinav Mehrotra, Lukasz Dudziak, Nicholas Donald Lane |  |
| 439 |  |  [Usable Information and Evolution of Optimal Representations During Training](https://openreview.net/forum?id=p8agn6bmTbr) |  | 0 |  | Michael Kleinman, Alessandro Achille, Daksh Idnani, Jonathan C. Kao |  |
| 440 |  |  [Exploring the Uncertainty Properties of Neural Networks' Implicit Priors in the Infinite-Width Limit](https://openreview.net/forum?id=MjvduJCsE4) |  | 0 |  | Ben Adlam, Jaehoon Lee, Lechao Xiao, Jeffrey Pennington, Jasper Snoek |  |
| 441 |  |  [On the geometry of generalization and memorization in deep neural networks](https://openreview.net/forum?id=V8jrrnwGbuc) |  | 0 |  | Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin Tang, SueYeon Chung |  |
| 442 |  |  [Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks](https://openreview.net/forum?id=YUGG2tFuPM) |  | 0 |  | Alexander Levine, Soheil Feizi |  |
| 443 |  |  [DC3: A learning method for optimization with hard constraints](https://openreview.net/forum?id=V1ZHVxJ6dSS) |  | 0 |  | Priya L. Donti, David Rolnick, J. Zico Kolter |  |
| 444 |  |  [Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study](https://openreview.net/forum?id=PObuuGVrGaZ) |  | 0 |  | Zhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen, KwangTing Cheng, Marios Savvides |  |
| 445 |  |  [Shape-Texture Debiased Neural Network Training](https://openreview.net/forum?id=Db4yerZTYkz) |  | 0 |  | Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan L. Yuille, Cihang Xie |  |
| 446 |  |  [Using latent space regression to analyze and leverage compositionality in GANs](https://openreview.net/forum?id=sjuuTm4vj0) |  | 0 |  | Lucy Chai, Jonas Wulff, Phillip Isola |  |
| 447 |  |  [Blending MPC & Value Function Approximation for Efficient Reinforcement Learning](https://openreview.net/forum?id=RqCC_00Bg7V) |  | 0 |  | Mohak Bhardwaj, Sanjiban Choudhury, Byron Boots |  |
| 448 |  |  [Model Patching: Closing the Subgroup Performance Gap with Data Augmentation](https://openreview.net/forum?id=9YlaeLfuhJF) |  | 0 |  | Karan Goel, Albert Gu, Sharon Li, Christopher Ré |  |
| 449 |  |  [Non-asymptotic Confidence Intervals of Off-policy Evaluation: Primal and Dual Bounds](https://openreview.net/forum?id=dKg5D1Z1Lm) |  | 0 |  | Yihao Feng, Ziyang Tang, Na Zhang, Qiang Liu |  |
| 450 |  |  [Linear Mode Connectivity in Multitask and Continual Learning](https://openreview.net/forum?id=Fmg_fQYUejf) |  | 0 |  | SeyedIman Mirzadeh, Mehrdad Farajtabar, Dilan Görür, Razvan Pascanu, Hassan Ghasemzadeh |  |
| 451 |  |  [Robust and Generalizable Visual Representation Learning via Random Convolutions](https://openreview.net/forum?id=BVSM0x3EDK6) |  | 0 |  | Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, Marc Niethammer |  |
| 452 |  |  [Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures](https://openreview.net/forum?id=l0mSUROpwY) |  | 0 |  | Pedro Hermosilla, Marco Schäfer, Matej Lang, Gloria Fackelmann, PerePau Vázquez, Barbora Kozlíková, Michael Krone, Tobias Ritschel, Timo Ropinski |  |
| 453 |  |  [Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF](https://openreview.net/forum?id=XAS3uKeFWj) |  | 0 |  | Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt, Justin Bayer |  |
| 454 |  |  [AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights](https://openreview.net/forum?id=Iz3zU3M316D) |  | 0 |  | Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, JungWoo Ha |  |
| 455 |  |  [MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering](https://openreview.net/forum?id=gV3wdEOGy_V) |  | 0 |  | Tsung Wei Tsai, Chongxuan Li, Jun Zhu |  |
| 456 |  |  [HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents](https://openreview.net/forum?id=9GBZBPn0Jx) |  | 0 |  | Deyao Zhu, Mohamed Zahran, Li Erran Li, Mohamed Elhoseiny |  |
| 457 |  |  [Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?](https://openreview.net/forum?id=p5uylG94S68) |  | 0 |  | Balázs Kégl, Gabriel Hurtado, Albert Thomas |  |
| 458 |  |  [Private Image Reconstruction from System Side Channels Using Generative Models](https://openreview.net/forum?id=y06VOYLcQXa) |  | 0 |  | Yuanyuan Yuan, Shuai Wang, Junping Zhang |  |
| 459 |  |  [Contextual Transformation Networks for Online Continual Learning](https://openreview.net/forum?id=zx_uX-BO7CH) |  | 0 |  | Quang Pham, Chenghao Liu, Doyen Sahoo, Steven C. H. Hoi |  |
| 460 |  |  [A Unified Approach to Interpreting and Boosting Adversarial Transferability](https://openreview.net/forum?id=X76iqnUbBjz) |  | 0 |  | Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, Quanshi Zhang |  |
| 461 |  |  [The inductive bias of ReLU networks on orthogonally separable data](https://openreview.net/forum?id=krz7T0xU9Z_) |  | 0 |  | Mary Phuong, Christoph H. Lampert |  |
| 462 |  |  [A statistical theory of cold posteriors in deep neural networks](https://openreview.net/forum?id=Rd138pWXMvG) |  | 0 |  | Laurence Aitchison |  |
| 463 |  |  [IOT: Instance-wise Layer Reordering for Transformer Structures](https://openreview.net/forum?id=ipUPfYxWZvM) |  | 0 |  | Jinhua Zhu, Lijun Wu, Yingce Xia, Shufang Xie, Tao Qin, Wengang Zhou, Houqiang Li, TieYan Liu |  |
| 464 |  |  [Counterfactual Generative Networks](https://openreview.net/forum?id=BXewfAYMmJw) |  | 0 |  | Axel Sauer, Andreas Geiger |  |
| 465 |  |  [Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data](https://openreview.net/forum?id=de11dbHzAMF) |  | 0 |  | Jonathan Pilault, Amine Elhattami, Christopher J. Pal |  |
| 466 |  |  [Towards Impartial Multi-task Learning](https://openreview.net/forum?id=IMPnRXEWpvr) |  | 0 |  | Liyang Liu, Yi Li, Zhanghui Kuang, JingHao Xue, Yimin Chen, Wenming Yang, Qingmin Liao, Wayne Zhang |  |
| 467 |  |  [Theoretical bounds on estimation error for meta-learning](https://openreview.net/forum?id=SZ3wtsXfzQR) |  | 0 |  | James Lucas, Mengye Ren, Irene Raissa Kameni, Toniann Pitassi, Richard S. Zemel |  |
| 468 |  |  [Domain-Robust Visual Imitation Learning with Mutual Information Constraints](https://openreview.net/forum?id=QubpWYfdNry) |  | 0 |  | Edoardo Cetin, Oya Çeliktutan |  |
| 469 |  |  [Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding](https://openreview.net/forum?id=8qDwejCuCN) |  | 0 |  | Sana Tonekaboni, Danny Eytan, Anna Goldenberg |  |
| 470 |  |  [Enforcing robust control guarantees within neural network policies](https://openreview.net/forum?id=5lhWG3Hj2By) |  | 0 |  | Priya L. Donti, Melrose Roderick, Mahyar Fazlyab, J. Zico Kolter |  |
| 471 |  |  [Active Contrastive Learning of Audio-Visual Video Representations](https://openreview.net/forum?id=OMizHuea_HB) |  | 0 |  | Shuang Ma, Zhaoyang Zeng, Daniel McDuff, Yale Song |  |
| 472 |  |  [Parameter Efficient Multimodal Transformers for Video Representation Learning](https://openreview.net/forum?id=6UdQLhqJyFD) |  | 0 |  | Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas M. Breuel, Jan Kautz, Yale Song |  |
| 473 |  |  [Robust Pruning at Initialization](https://openreview.net/forum?id=vXj_ucZQ4hA) |  | 0 |  | Soufiane Hayou, JeanFrancois Ton, Arnaud Doucet, Yee Whye Teh |  |
| 474 |  |  [Efficient Wasserstein Natural Gradients for Reinforcement Learning](https://openreview.net/forum?id=OHgnfSrn2jv) |  | 0 |  | Ted Moskovitz, Michael Arbel, Ferenc Huszar, Arthur Gretton |  |
| 475 |  |  [Probing BERT in Hyperbolic Spaces](https://openreview.net/forum?id=17VnwXYZyhH) |  | 0 |  | Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, Liping Jing |  |
| 476 |  |  [On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning](https://openreview.net/forum?id=o81ZyBCojoA) |  | 0 |  | Ren Wang, Kaidi Xu, Sijia Liu, PinYu Chen, TsuiWei Weng, Chuang Gan, Meng Wang |  |
| 477 |  |  [Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics](https://openreview.net/forum?id=LhY8QdUGSuw) |  | 0 |  | Vinay Venkatesh Ramasesh, Ethan Dyer, Maithra Raghu |  |
| 478 |  |  [Trusted Multi-View Classification](https://openreview.net/forum?id=OOsR8BzCnl5) |  | 0 |  | Zongbo Han, Changqing Zhang, Huazhu Fu, Joey Tianyi Zhou |  |
| 479 |  |  [i-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning](https://openreview.net/forum?id=T6AxtOaWydQ) |  | 0 |  | Kibok Lee, Yian Zhu, Kihyuk Sohn, ChunLiang Li, Jinwoo Shin, Honglak Lee |  |
| 480 |  |  [Initialization and Regularization of Factorized Neural Layers](https://openreview.net/forum?id=KTlJT1nof6d) |  | 0 |  | Mikhail Khodak, Neil A. Tenenholtz, Lester Mackey, Nicolò Fusi |  |
| 481 |  |  [Learning to Generate 3D Shapes with Generative Cellular Automata](https://openreview.net/forum?id=rABUmU3ulQh) |  | 0 |  | Dongsu Zhang, Changwoon Choi, Jeonghwan Kim, Young Min Kim |  |
| 482 |  |  [Self-Supervised Learning of Compressed Video Representations](https://openreview.net/forum?id=jMPcEkJpdD) |  | 0 |  | Youngjae Yu, Sangho Lee, Gunhee Kim, Yale Song |  |
| 483 |  |  [Factorizing Declarative and Procedural Knowledge in Structured, Dynamical Environments](https://openreview.net/forum?id=VVdmjgu7pKM) |  | 0 |  | Anirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Charles Blundell, Sergey Levine, Yoshua Bengio, Michael Curtis Mozer |  |
| 484 |  |  [Cut out the annotator, keep the cutout: better segmentation with weak supervision](https://openreview.net/forum?id=bjkX6Kzb5H) |  | 0 |  | Sarah M. Hooper, Michael Wornow, Ying Hang Seah, Peter Kellman, Hui Xue, Frederic Sala, Curtis P. Langlotz, Christopher Ré |  |
| 485 |  |  [FastSpeech 2: Fast and High-Quality End-to-End Text to Speech](https://openreview.net/forum?id=piLPYqxtWuA) |  | 0 |  | Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, TieYan Liu |  |
| 486 |  |  [On Learning Universal Representations Across Languages](https://openreview.net/forum?id=Uu1Nw-eeTxJ) |  | 0 |  | Xiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, Weihua Luo |  |
| 487 |  |  [Effective Distributed Learning with Random Features: Improved Bounds and Algorithms](https://openreview.net/forum?id=jxdXSW9Doc) |  | 0 |  | Yong Liu, Jiankun Liu, Shuqiang Wang |  |
| 488 |  |  [Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning](https://openreview.net/forum?id=AICNpd8ke-m) |  | 0 |  | Kanil Patel, William H. Beluch, Bin Yang, Michael Pfeiffer, Dan Zhang |  |
| 489 |  |  [Neural ODE Processes](https://openreview.net/forum?id=27acGyyI1BY) |  | 0 |  | Alexander Norcliffe, Cristian Bodnar, Ben Day, Jacob Moss, Pietro Liò |  |
| 490 |  |  [Conformation-Guided Molecular Representation with Hamiltonian Neural Networks](https://openreview.net/forum?id=q-cnWaaoUTH) |  | 0 |  | Ziyao Li, Shuwen Yang, Guojie Song, Lingsheng Cai |  |
| 491 |  |  [An Unsupervised Deep Learning Approach for Real-World Image Denoising](https://openreview.net/forum?id=tIjRAiFmU3y) |  | 0 |  | Dihan Zheng, Sia Huat Tan, Xiaowen Zhang, Zuoqiang Shi, Kaisheng Ma, Chenglong Bao |  |
| 492 |  |  [Uncertainty in Gradient Boosting via Ensembles](https://openreview.net/forum?id=1Jv6b0Zq3qi) |  | 0 |  | Andrey Malinin, Liudmila Prokhorenkova, Aleksei Ustimenko |  |
| 493 |  |  [Lossless Compression of Structured Convolutional Models via Lifting](https://openreview.net/forum?id=oxnp2q-PGL4) |  | 0 |  | Gustav Sourek, Filip Zelezný, Ondrej Kuzelka |  |
| 494 |  |  [Neural networks with late-phase weights](https://openreview.net/forum?id=C0qJUx5dxFb) |  | 0 |  | Johannes von Oswald, Seijin Kobayashi, João Sacramento, Alexander Meulemans, Christian Henning, Benjamin F. Grewe |  |
| 495 |  |  [Disambiguating Symbolic Expressions in Informal Documents](https://openreview.net/forum?id=K5j7D81ABvt) |  | 0 |  | Dennis Müller, Cezary Kaliszyk |  |
| 496 |  |  [Learning Parametrised Graph Shift Operators](https://openreview.net/forum?id=0OlrLvrsHwQ) |  | 0 |  | George Dasoulas, Johannes F. Lutzeyer, Michalis Vazirgiannis |  |
| 497 |  |  [Efficient Conformal Prediction via Cascaded Inference with Expanded Admission](https://openreview.net/forum?id=tnSo6VRLmT) |  | 0 |  | Adam Fisch, Tal Schuster, Tommi S. Jaakkola, Regina Barzilay |  |
| 498 |  |  [GANs Can Play Lottery Tickets Too](https://openreview.net/forum?id=1AoMhc_9jER) |  | 0 |  | Xuxi Chen, Zhenyu Zhang, Yongduo Sui, Tianlong Chen |  |
| 499 |  |  [ResNet After All: Neural ODEs and Their Numerical Solution](https://openreview.net/forum?id=HxzSxSxLOJZ) |  | 0 |  | Katharina Ott, Prateek Katiyar, Philipp Hennig, Michael Tiemann |  |
| 500 |  |  [Semantic Re-tuning with Contrastive Tension](https://openreview.net/forum?id=Ov_sMNau-PF) |  | 0 |  | Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipää Hellqvist, Magnus Sahlgren |  |
| 501 |  |  [Property Controllable Variational Autoencoder via Invertible Mutual Dependence](https://openreview.net/forum?id=tYxG_OMs9WE) |  | 0 |  | Xiaojie Guo, Yuanqi Du, Liang Zhao |  |
| 502 |  |  [Latent Convergent Cross Mapping](https://openreview.net/forum?id=4TSiOTkKe5P) |  | 0 |  | Edward De Brouwer, Adam Arany, Jaak Simm, Yves Moreau |  |
| 503 |  |  [Adaptive Universal Generalized PageRank Graph Neural Network](https://openreview.net/forum?id=n6jl7fLxrP) |  | 0 |  | Eli Chien, Jianhao Peng, Pan Li, Olgica Milenkovic |  |
| 504 |  |  [Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces](https://openreview.net/forum?id=ATp1nW2FuZL) |  | 0 |  | Yatin Nandwani, Deepanshu Jindal, Mausam, Parag Singla |  |
| 505 |  |  [My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control](https://openreview.net/forum?id=N3zUDGN5lO) |  | 0 |  | Vitaly Kurin, Maximilian Igl, Tim Rocktäschel, Wendelin Boehmer, Shimon Whiteson |  |
| 506 |  |  [FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning](https://openreview.net/forum?id=dgtpE6gKjHn) |  | 0 |  | HongYou Chen, WeiLun Chao |  |
| 507 |  |  [MALI: A memory efficient and reverse accurate integrator for Neural ODEs](https://openreview.net/forum?id=blfSjHeFM_e) |  | 0 |  | Juntang Zhuang, Nicha C. Dvornek, Sekhar Tatikonda, James S. Duncan |  |
| 508 |  |  [Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks](https://openreview.net/forum?id=sTeoJiB4uR) |  | 0 |  | Thomas Bird, Friso H. Kingma, David Barber |  |
| 509 |  |  [In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness](https://openreview.net/forum?id=jznizqvr15J) |  | 0 |  | Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, Percy Liang |  |
| 510 |  |  [Incremental few-shot learning via vector quantization in deep embedded space](https://openreview.net/forum?id=3SV-ZePhnZM) |  | 0 |  | Kuilin Chen, ChiGuhn Lee |  |
| 511 |  |  [Contrastive Syn-to-Real Generalization](https://openreview.net/forum?id=F8whUO8HNbP) |  | 0 |  | Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, José M. Álvarez, Zhangyang Wang, Anima Anandkumar |  |
| 512 |  |  [Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting](https://openreview.net/forum?id=tHgJoMfy6nI) |  | 0 |  | Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William Gan, Joseph E. Gonzalez, Marcus Rohrbach, Trevor Darrell |  |
| 513 |  |  [Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online](https://openreview.net/forum?id=zElset1Klrp) |  | 0 |  | Yangchen Pan, Kirby Banman, Martha White |  |
| 514 |  |  [High-Capacity Expert Binary Networks](https://openreview.net/forum?id=MxaY4FzOTa) |  | 0 |  | Adrian Bulat, Brais Martínez, Georgios Tzimiropoulos |  |
| 515 |  |  [Learning What To Do by Simulating the Past](https://openreview.net/forum?id=kBVJ2NtiY-) |  | 0 |  | David Lindner, Rohin Shah, Pieter Abbeel, Anca D. Dragan |  |
| 516 |  |  [Progressive Skeletonization: Trimming more fat from a network at initialization](https://openreview.net/forum?id=9GsFOUyUPi) |  | 0 |  | Pau de Jorge, Amartya Sanyal, Harkirat S. Behl, Philip H. S. Torr, Grégory Rogez, Puneet K. Dokania |  |
| 517 |  |  [Filtered Inner Product Projection for Crosslingual Embedding Alignment](https://openreview.net/forum?id=A2gNouoXE7) |  | 0 |  | Vin Sachidananda, Ziyi Yang, Chenguang Zhu |  |
| 518 |  |  [Learning Manifold Patch-Based Representations of Man-Made Shapes](https://openreview.net/forum?id=Gu5WqN9J3Fn) |  | 0 |  | Dmitriy Smirnov, Mikhail Bessmeltsev, Justin Solomon |  |
| 519 |  |  [Aligning AI With Shared Human Values](https://openreview.net/forum?id=dNy_RKzJacY) |  | 0 |  | Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, Jacob Steinhardt |  |
| 520 |  |  [Kanerva++: Extending the Kanerva Machine With Differentiable, Locally Block Allocated Latent Memory](https://openreview.net/forum?id=QoWatN-b8T) |  | 0 |  | Jason Ramapuram, Yan Wu, Alexandros Kalousis |  |
| 521 |  |  [Measuring Massive Multitask Language Understanding](https://openreview.net/forum?id=d7KBjmI3GmQ) |  | 0 |  | Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt |  |
| 522 |  |  [Towards Robust Neural Networks via Close-loop Control](https://openreview.net/forum?id=2AL06y9cDE-) |  | 0 |  | Zhuotong Chen, Qianxiao Li, Zheng Zhang |  |
| 523 |  |  [Statistical inference for individual fairness](https://openreview.net/forum?id=z9k8BWL-_2u) |  | 0 |  | Subha Maity, Songkai Xue, Mikhail Yurochkin, Yuekai Sun |  |
| 524 |  |  [HyperGrid Transformers: Towards A Single Model for Multiple Tasks](https://openreview.net/forum?id=hiq1rHO8pNT) |  | 0 |  | Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, DaCheng Juan |  |
| 525 |  |  [Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity](https://openreview.net/forum?id=6t_dLShIUyZ) |  | 0 |  | Shaocong Ma, Ziyi Chen, Yi Zhou, Shaofeng Zou |  |
| 526 |  |  [On InstaHide, Phase Retrieval, and Sparse Matrix Factorization](https://openreview.net/forum?id=AhElGnhU2BV) |  | 0 |  | Sitan Chen, Xiaoxiao Li, Zhao Song, Danyang Zhuo |  |
| 527 |  |  [VA-RED2: Video Adaptive Redundancy Reduction](https://openreview.net/forum?id=g21u6nlbPzn) |  | 0 |  | Bowen Pan, Rameswar Panda, Camilo Luciano Fosco, ChungChing Lin, Alex J. Andonian, Yue Meng, Kate Saenko, Aude Oliva, Rogério Feris |  |
| 528 |  |  [SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning](https://openreview.net/forum?id=XLfdzwNKzch) |  | 0 |  | Myeongjang Pyeon, Jihwan Moon, Taeyoung Hahn, Gunhee Kim |  |
| 529 |  |  [ALFWorld: Aligning Text and Embodied Environments for Interactive Learning](https://openreview.net/forum?id=0IOX0YcCdTn) |  | 0 |  | Mohit Shridhar, Xingdi Yuan, MarcAlexandre Côté, Yonatan Bisk, Adam Trischler, Matthew J. Hausknecht |  |
| 530 |  |  [Learning Task Decomposition with Ordered Memory Policy Network](https://openreview.net/forum?id=vcopnwZ7bC) |  | 0 |  | Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron C. Courville, Joshua B. Tenenbaum, Chuang Gan |  |
| 531 |  |  [Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification](https://openreview.net/forum?id=ijJZbomCJIm) |  | 0 |  | Francisco Utrera, Evan Kravitz, N. Benjamin Erichson, Rajiv Khanna, Michael W. Mahoney |  |
| 532 |  |  [UMEC: Unified model and embedding compression for efficient recommendation systems](https://openreview.net/forum?id=BM---bH_RSh) |  | 0 |  | Jiayi Shen, Haotao Wang, Shupeng Gui, Jianchao Tan, Zhangyang Wang, Ji Liu |  |
| 533 |  |  [Exploring Balanced Feature Spaces for Representation Learning](https://openreview.net/forum?id=OqtLIabPTit) |  | 0 |  | Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, Jiashi Feng |  |
| 534 |  |  [Calibration of Neural Networks using Splines](https://openreview.net/forum?id=eQe8DEWNN2W) |  | 0 |  | Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, Richard Hartley |  |
| 535 |  |  [Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein](https://openreview.net/forum?id=DiQD7FWL233) |  | 0 |  | Khai Nguyen, Son Nguyen, Nhat Ho, Tung Pham, Hung Bui |  |
| 536 |  |  [Rethinking Positional Encoding in Language Pre-training](https://openreview.net/forum?id=09-528y2Fgf) |  | 0 |  | Guolin Ke, Di He, TieYan Liu |  |
| 537 |  |  [Discovering Non-monotonic Autoregressive Orderings with Variational Inference](https://openreview.net/forum?id=jP1vTH3inC) |  | 0 |  | Xuanlin Li, Brandon Trabucco, Dong Huk Park, Michael Luo, Sheng Shen, Trevor Darrell, Yang Gao |  |
| 538 |  |  [Differentiable Trust Region Layers for Deep Reinforcement Learning](https://openreview.net/forum?id=qYZD-AO1Vn) |  | 0 |  | Fabian Otto, Philipp Becker, Ngo Anh Vien, Hanna Carolin Maria Ziesche, Gerhard Neumann |  |
| 539 |  |  [SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization](https://openreview.net/forum?id=-M0QkvBGTTq) |  | 0 |  | A. F. M. Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, SungHo Bae |  |
| 540 |  |  [Task-Agnostic Morphology Evolution](https://openreview.net/forum?id=CGQ6ENUMX6) |  | 0 |  | Donald Joseph Hejna III, Pieter Abbeel, Lerrel Pinto |  |
| 541 |  |  [Learning Associative Inference Using Fast Weight Memory](https://openreview.net/forum?id=TuK6agbdt27) |  | 0 |  | Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber |  |
| 542 |  |  [Boost then Convolve: Gradient Boosting Meets Graph Neural Networks](https://openreview.net/forum?id=ebS5NUfoMKL) |  | 0 |  | Sergei Ivanov, Liudmila Prokhorenkova |  |
| 543 |  |  [Degree-Quant: Quantization-Aware Training for Graph Neural Networks](https://openreview.net/forum?id=NSBrFgJAHg) |  | 0 |  | Shyam Anil Tailor, Javier FernándezMarqués, Nicholas Donald Lane |  |
| 544 |  |  [Network Pruning That Matters: A Case Study on Retraining Variants](https://openreview.net/forum?id=Cb54AMqHQFP) |  | 0 |  | Duong H. Le, BinhSon Hua |  |
| 545 |  |  [Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation](https://openreview.net/forum?id=MJAqnaC2vO1) |  | 0 |  | Hao Li, Chenxin Tao, Xizhou Zhu, Xiaogang Wang, Gao Huang, Jifeng Dai |  |
| 546 |  |  [Differentiable Segmentation of Sequences](https://openreview.net/forum?id=4T489T4yav) |  | 0 |  | Erik Scharwächter, Jonathan Lennartz, Emmanuel Müller |  |
| 547 |  |  [Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning](https://openreview.net/forum?id=bhCDO_cEGCz) |  | 0 |  | Zhenfang Chen, Jiayuan Mao, Jiajun Wu, KwanYee Kenneth Wong, Joshua B. Tenenbaum, Chuang Gan |  |
| 548 |  |  [Learning Deep Features in Instrumental Variable Regression](https://openreview.net/forum?id=sy4Kg_ZQmS7) |  | 0 |  | Liyuan Xu, Yutian Chen, Siddarth Srinivasan, Nando de Freitas, Arnaud Doucet, Arthur Gretton |  |
| 549 |  |  [Online Adversarial Purification based on Self-supervised Learning](https://openreview.net/forum?id=_i3ASPp12WS) |  | 0 |  | Changhao Shi, Chester Holtz, Gal Mishne |  |
| 550 |  |  [Graph Information Bottleneck for Subgraph Recognition](https://openreview.net/forum?id=bM4Iqfg8M2k) |  | 0 |  | Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, Ran He |  |
| 551 |  |  [In Search of Lost Domain Generalization](https://openreview.net/forum?id=lQdXeXDoWtI) |  | 0 |  | Ishaan Gulrajani, David LopezPaz |  |
| 552 |  |  [Robust Curriculum Learning: from clean label detection to noisy label self-correction](https://openreview.net/forum?id=lmTWnm3coJJ) |  | 0 |  | Tianyi Zhou, Shengjie Wang, Jeff A. Bilmes |  |
| 553 |  |  [Cross-Attentional Audio-Visual Fusion for Weakly-Supervised Action Localization](https://openreview.net/forum?id=hWr3e3r-oH5) |  | 0 |  | JunTae Lee, Mihir Jain, Hyoungwoo Park, Sungrack Yun |  |
| 554 |  |  [CoCon: A Self-Supervised Approach for Controlled Text Generation](https://openreview.net/forum?id=VD_ozqvBy4W) |  | 0 |  | Alvin Chan, YewSoon Ong, Bill Pung, Aston Zhang, Jie Fu |  |
| 555 |  |  [Group Equivariant Generative Adversarial Networks](https://openreview.net/forum?id=rgFNuJHHXv) |  | 0 |  | Neel Dey, Antong Chen, Soheil Ghafurian |  |
| 556 |  |  [What they do when in doubt: a study of inductive biases in seq2seq learners](https://openreview.net/forum?id=YmA86Zo-P_t) |  | 0 |  | Eugene Kharitonov, Rahma Chaabouni |  |
| 557 |  |  [A teacher-student framework to distill future trajectories](https://openreview.net/forum?id=ECuvULjFQia) |  | 0 |  | Alexander Neitz, Giambattista Parascandolo, Bernhard Schölkopf |  |
| 558 |  |  [Learning a Latent Search Space for Routing Problems using Variational Autoencoders](https://openreview.net/forum?id=90JprVrJBO) |  | 0 |  | André Hottung, Bhanu Bhandari, Kevin Tierney |  |
| 559 |  |  [Universal approximation power of deep residual neural networks via nonlinear control theory](https://openreview.net/forum?id=-IXhmY16R3M) |  | 0 |  | Paulo Tabuada, Bahman Gharesifard |  |
| 560 |  |  [On the Universality of Rotation Equivariant Point Cloud Networks](https://openreview.net/forum?id=6NFBvWlRXaG) |  | 0 |  | Nadav Dym, Haggai Maron |  |
| 561 |  |  [CT-Net: Channel Tensorization Network for Video Classification](https://openreview.net/forum?id=UoaQUQREMOs) |  | 0 |  | Kunchang Li, Xianhang Li, Yali Wang, Jun Wang, Yu Qiao |  |
| 562 |  |  [Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units](https://openreview.net/forum?id=eU776ZYxEpz) |  | 0 |  | Jonathan Cornford, Damjan Kalajdzievski, Marco Leite, Amélie Lamarquette, Dimitri Michael Kullmann, Blake Aaron Richards |  |
| 563 |  |  [Uncertainty Estimation in Autoregressive Structured Prediction](https://openreview.net/forum?id=jN5y-zb5Q7m) |  | 0 |  | Andrey Malinin, Mark J. F. Gales |  |
| 564 |  |  [Transformer protein language models are unsupervised structure learners](https://openreview.net/forum?id=fylclEqgvgd) |  | 0 |  | Roshan Rao, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, Alexander Rives |  |
| 565 |  |  [ANOCE: Analysis of Causal Effects with Multiple Mediators via Constrained Structural Learning](https://openreview.net/forum?id=7I12hXRi8F) |  | 0 |  | Hengrui Cai, Rui Song, Wenbin Lu |  |
| 566 |  |  [Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks](https://openreview.net/forum?id=w2Z2OwVNeK) |  | 0 |  | Ingmar Schubert, Ozgur S. Oguz, Marc Toussaint |  |
| 567 |  |  [CcGAN: Continuous Conditional Generative Adversarial Networks for Image Generation](https://openreview.net/forum?id=PrzjugOsDeE) |  | 0 |  | Xin Ding, Yongwei Wang, Zuheng Xu, William J. Welch, Z. Jane Wang |  |
| 568 |  |  [Single-Photon Image Classification](https://openreview.net/forum?id=CHLhSw9pSw8) |  | 0 |  | Thomas Fischbacher, Luciano Sbaiz |  |
| 569 |  |  [Self-supervised Adversarial Robustness for the Low-label, High-data Regime](https://openreview.net/forum?id=bgQek2O63w) |  | 0 |  | Sven Gowal, PoSen Huang, Aäron van den Oord, Timothy A. Mann, Pushmeet Kohli |  |
| 570 |  |  [Uncertainty-aware Active Learning for Optimal Bayesian Classifier](https://openreview.net/forum?id=Mu2ZxFctAI) |  | 0 |  | Guang Zhao, Edward R. Dougherty, ByungJun Yoon, Francis J. Alexander, Xiaoning Qian |  |
| 571 |  |  [Latent Skill Planning for Exploration and Transfer](https://openreview.net/forum?id=jXe91kq3jAq) |  | 0 |  | Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, Florian Shkurti |  |
| 572 |  |  [Learning continuous-time PDEs from sparse data with graph neural networks](https://openreview.net/forum?id=aUX5Plaq7Oy) |  | 0 |  | Valerii Iakovlev, Markus Heinonen, Harri Lähdesmäki |  |
| 573 |  |  [Characterizing signal propagation to close the performance gap in unnormalized ResNets](https://openreview.net/forum?id=IX3Nnir2omJ) |  | 0 |  | Andrew Brock, Soham De, Samuel L. Smith |  |
| 574 |  |  [Robust Overfitting may be mitigated by properly learned smoothening](https://openreview.net/forum?id=qZzy5urZw9) |  | 0 |  | Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, Zhangyang Wang |  |
| 575 |  |  [Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning](https://openreview.net/forum?id=LXMSvPmsm0g) |  | 0 |  | Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, Zhangyang Wang |  |
| 576 |  |  [Symmetry-Aware Actor-Critic for 3D Molecular Design](https://openreview.net/forum?id=jEYKjPE1xYN) |  | 0 |  | Gregor N. C. Simm, Robert Pinsler, Gábor Csányi, José Miguel HernándezLobato |  |
| 577 |  |  [PseudoSeg: Designing Pseudo Labels for Semantic Segmentation](https://openreview.net/forum?id=-TwO99rbVRu) |  | 0 |  | Yuliang Zou, Zizhao Zhang, Han Zhang, ChunLiang Li, Xiao Bian, JiaBin Huang, Tomas Pfister |  |
| 578 |  |  [NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition](https://openreview.net/forum?id=CU0APx9LMaL) |  | 0 |  | Abhinav Mehrotra, Alberto Gil C. P. Ramos, Sourav Bhattacharya, Lukasz Dudziak, Ravichander Vipperla, Thomas Chau, Mohamed S. Abdelfattah, Samin Ishtiaq, Nicholas Donald Lane |  |
| 579 |  |  [Scaling the Convex Barrier with Active Sets](https://openreview.net/forum?id=uQfOy7LrlTR) |  | 0 |  | Alessandro De Palma, Harkirat S. Behl, Rudy Bunel, Philip H. S. Torr, M. Pawan Kumar |  |
| 580 |  |  [Local Convergence Analysis of Gradient Descent Ascent with Finite Timescale Separation](https://openreview.net/forum?id=AWOSz_mMAPx) |  | 0 |  | Tanner Fiez, Lillian J. Ratliff |  |
| 581 |  |  [Activation-level uncertainty in deep neural networks](https://openreview.net/forum?id=UvBPbpvHRj-) |  | 0 |  | Pablo MoralesAlvarez, Daniel HernándezLobato, Rafael Molina, José Miguel HernándezLobato |  |
| 582 |  |  [Efficient Continual Learning with Modular Networks and Task-Driven Priors](https://openreview.net/forum?id=EKV158tSfwv) |  | 0 |  | Tom Veniat, Ludovic Denoyer, Marc'Aurelio Ranzato |  |
| 583 |  |  [No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks](https://openreview.net/forum?id=193sEnKY1ij) |  | 0 |  | Shyamgopal Karthik, Ameya Prabhu, Puneet K. Dokania, Vineet Gandhi |  |
| 584 |  |  [Ringing ReLUs: Harmonic Distortion Analysis of Nonlinear Feedforward Networks](https://openreview.net/forum?id=TaYhv-q1Xit) |  | 0 |  | Christian H. X. Ali MehmetiGöpel, David Hartmann, Michael Wand |  |
| 585 |  |  [Distance-Based Regularisation of Deep Networks for Fine-Tuning](https://openreview.net/forum?id=IFqrg1p5Bc) |  | 0 |  | Henry Gouk, Timothy M. Hospedales, Massimiliano Pontil |  |
| 586 |  |  [Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning](https://openreview.net/forum?id=TGFO0DbD_pk) |  | 0 |  | Enrico Marchesini, Davide Corsi, Alessandro Farinelli |  |
| 587 |  |  [Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis](https://openreview.net/forum?id=1Fqg133qRaI) |  | 0 |  | Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal |  |
| 588 |  |  [IsarStep: a Benchmark for High-level Mathematical Reasoning](https://openreview.net/forum?id=Pzj6fzU6wkj) |  | 0 |  | Wenda Li, Lei Yu, Yuhuai Wu, Lawrence C. Paulson |  |
| 589 |  |  [Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network](https://openreview.net/forum?id=U_mat0b9iv) |  | 0 |  | James Diffenderfer, Bhavya Kailkhura |  |
| 590 |  |  [Average-case Acceleration for Bilinear Games and Normal Matrices](https://openreview.net/forum?id=H0syOoy3Ash) |  | 0 |  | Carles DomingoEnrich, Fabian Pedregosa, Damien Scieur |  |
| 591 |  |  [Economic Hyperparameter Optimization with Blended Search Strategy](https://openreview.net/forum?id=VbLH04pRA3) |  | 0 |  | Chi Wang, Qingyun Wu, Silu Huang, Amin Saied |  |
| 592 |  |  [BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization](https://openreview.net/forum?id=TiXl51SCNw8) |  | 0 |  | Huanrui Yang, Lin Duan, Yiran Chen, Hai Li |  |
| 593 |  |  [AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly](https://openreview.net/forum?id=SlrqM9_lyju) |  | 0 |  | Yuchen Jin, Tianyi Zhou, Liangyu Zhao, Yibo Zhu, Chuanxiong Guo, Marco Canini, Arvind Krishnamurthy |  |
| 594 |  |  [BERTology Meets Biology: Interpreting Attention in Protein Language Models](https://openreview.net/forum?id=YWtLZvLmud7) |  | 0 |  | Jesse Vig, Ali Madani, Lav R. Varshney, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani |  |
| 595 |  |  [Learning Task-General Representations with Generative Neuro-Symbolic Modeling](https://openreview.net/forum?id=qzBUIzq5XR2) |  | 0 |  | Reuben Feinman, Brenden M. Lake |  |
| 596 |  |  [Zero-shot Synthesis with Group-Supervised Learning](https://openreview.net/forum?id=8wqCDnBmnrT) |  | 0 |  | Yunhao Ge, Sami AbuElHaija, Gan Xin, Laurent Itti |  |
| 597 |  |  [Selective Classification Can Magnify Disparities Across Groups](https://openreview.net/forum?id=N0M_4BkQ05i) |  | 0 |  | Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, Percy Liang |  |
| 598 |  |  [Better Fine-Tuning by Reducing Representational Collapse](https://openreview.net/forum?id=OQ08SN70M1V) |  | 0 |  | Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, Sonal Gupta |  |
| 599 |  |  [Training independent subnetworks for robust prediction](https://openreview.net/forum?id=OGg9XnKxFAH) |  | 0 |  | Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshminarayanan, Andrew Mingbo Dai, Dustin Tran |  |
| 600 |  |  [Meta-Learning of Structured Task Distributions in Humans and Machines](https://openreview.net/forum?id=--gvHfE3Xf5) |  | 0 |  | Sreejan Kumar, Ishita Dasgupta, Jonathan D. Cohen, Nathaniel D. Daw, Thomas L. Griffiths |  |
| 601 |  |  [BiPointNet: Binary Neural Network for Point Clouds](https://openreview.net/forum?id=9QLRCVysdlO) |  | 0 |  | Haotong Qin, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su |  |
| 602 |  |  [Benchmarks for Deep Off-Policy Evaluation](https://openreview.net/forum?id=kWSeGEeHvF8) |  | 0 |  | Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, Tom Le Paine |  |
| 603 |  |  [Planning from Pixels using Inverse Dynamics Models](https://openreview.net/forum?id=V6BjBgku7Ro) |  | 0 |  | Keiran Paster, Sheila A. McIlraith, Jimmy Ba |  |
| 604 |  |  [Understanding the effects of data parallelism and sparsity on neural network training](https://openreview.net/forum?id=rsogjAnYs4z) |  | 0 |  | Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr, Martin Jaggi |  |
| 605 |  |  [NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control](https://openreview.net/forum?id=Iw4ZGwenbXf) |  | 0 |  | Ioannis Exarchos, Marcus Aloysius Pereira, Ziyi Wang, Evangelos A. Theodorou |  |
| 606 |  |  [MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond](https://openreview.net/forum?id=8e6BrwU6AjQ) |  | 0 |  | DuyKien Nguyen, Vedanuj Goswami, Xinlei Chen |  |
| 607 |  |  [NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation](https://openreview.net/forum?id=pmj131uIL9H) |  | 0 |  | Angtian Wang, Adam Kortylewski, Alan L. Yuille |  |
| 608 |  |  [On Graph Neural Networks versus Graph-Augmented MLPs](https://openreview.net/forum?id=tiqI7w64JG2) |  | 0 |  | Lei Chen, Zhengdao Chen, Joan Bruna |  |
| 609 |  |  [Dual-mode ASR: Unify and Improve Streaming ASR with Full-context Modeling](https://openreview.net/forum?id=Pz_dcqfcKW8) |  | 0 |  | Jiahui Yu, Wei Han, Anmol Gulati, ChungCheng Chiu, Bo Li, Tara N. Sainath, Yonghui Wu, Ruoming Pang |  |
| 610 |  |  [Deep Learning meets Projective Clustering](https://openreview.net/forum?id=EQfpYwF3-b) |  | 0 |  | Alaa Maalouf, Harry Lang, Daniela Rus, Dan Feldman |  |
| 611 |  |  [Reinforcement Learning with Random Delays](https://openreview.net/forum?id=QFYnKlBJYR) |  | 0 |  | Yann Bouteiller, Simon Ramstedt, Giovanni Beltrame, Christopher J. Pal, Jonathan Binas |  |
| 612 |  |  [Isotropy in the Contextual Embedding Space: Clusters and Manifolds](https://openreview.net/forum?id=xYGNO86OWDH) |  | 0 |  | Xingyu Cai, Jiaji Huang, Yuchen Bian, Kenneth Church |  |
| 613 |  |  [Spatio-Temporal Graph Scattering Transform](https://openreview.net/forum?id=CF-ZIuSMXRz) |  | 0 |  | Chao Pan, Siheng Chen, Antonio Ortega |  |
| 614 |  |  [Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization](https://openreview.net/forum?id=3hGNqpI4WS) |  | 0 |  | Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, Shixiang Gu |  |
| 615 |  |  [gradSim: Differentiable simulation for system identification and visuomotor control](https://openreview.net/forum?id=c_E8kFWfhp0) |  | 0 |  | J. Krishna Murthy, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jérôme ParentLévesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, Sanja Fidler |  |
| 616 |  |  [Evaluations and Methods for Explanation through Robustness Analysis](https://openreview.net/forum?id=4dXmpCDGNp7) |  | 0 |  | ChengYu Hsieh, ChihKuan Yeh, Xuanqing Liu, Pradeep Kumar Ravikumar, Seungyeon Kim, Sanjiv Kumar, ChoJui Hsieh |  |
| 617 |  |  [RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs](https://openreview.net/forum?id=tGZu6DlbreV) |  | 0 |  | Meng Qu, JunKun Chen, LouisPascal A. C. Xhonneux, Yoshua Bengio, Jian Tang |  |
| 618 |  |  [Can a Fruit Fly Learn Word Embeddings?](https://openreview.net/forum?id=xfmSoxdxFCG) |  | 0 |  | Yuchen Liang, Chaitanya K. Ryali, Benjamin Hoover, Leopold Grinberg, Saket Navlakha, Mohammed J. Zaki, Dmitry Krotov |  |
| 619 |  |  [Neural representation and generation for RNA secondary structures](https://openreview.net/forum?id=snOgiCYZgJ7) |  | 0 |  | Zichao Yan, William L. Hamilton, Mathieu Blanchette |  |
| 620 |  |  [WaNet - Imperceptible Warping-based Backdoor Attack](https://openreview.net/forum?id=eEn8KTtJOx) |  | 0 |  | Tuan Anh Nguyen, Anh Tuan Tran |  |
| 621 |  |  [LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition](https://openreview.net/forum?id=hJmtwocEqzc) |  | 0 |  | Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan, John P. Dickerson, Gavin Taylor, Tom Goldstein |  |
| 622 |  |  [Learning from others' mistakes: Avoiding dataset biases without modeling them](https://openreview.net/forum?id=Hf3qXoiNkR) |  | 0 |  | Victor Sanh, Thomas Wolf, Yonatan Belinkov, Alexander M. Rush |  |
| 623 |  |  [Prototypical Contrastive Learning of Unsupervised Representations](https://openreview.net/forum?id=KmykpuSrjcq) |  | 0 |  | Junnan Li, Pan Zhou, Caiming Xiong, Steven C. H. Hoi |  |
| 624 |  |  [Extreme Memorization via Scale of Initialization](https://openreview.net/forum?id=Z4R1vxLbRLO) |  | 0 |  | Harsh Mehta, Ashok Cutkosky, Behnam Neyshabur |  |
| 625 |  |  [Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling](https://openreview.net/forum?id=IDFQI9OY6K) |  | 0 |  | Benedikt Boecking, Willie Neiswanger, Eric P. Xing, Artur Dubrawski |  |
| 626 |  |  [Adaptive Procedural Task Generation for Hard-Exploration Problems](https://openreview.net/forum?id=8xLkv08d70T) |  | 0 |  | Kuan Fang, Yuke Zhu, Silvio Savarese, Li FeiFei |  |
| 627 |  |  [Multi-timescale Representation Learning in LSTM Language Models](https://openreview.net/forum?id=9ITXiTrAoT) |  | 0 |  | Shivangi Mahto, Vy Ai Vo, Javier S. Turek, Alexander Huth |  |
| 628 |  |  [Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation](https://openreview.net/forum?id=KpfasTaLUpq) |  | 0 |  | Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, Noah A. Smith |  |
| 629 |  |  [Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing](https://openreview.net/forum?id=5NsEIflpbSv) |  | 0 |  | Asish Ghoshal, Xilun Chen, Sonal Gupta, Luke Zettlemoyer, Yashar Mehdad |  |
| 630 |  |  [Predicting Inductive Biases of Pre-Trained Models](https://openreview.net/forum?id=mNtmhaDkAr) |  | 0 |  | Charles Lovering, Rohan Jha, Tal Linzen, Ellie Pavlick |  |
| 631 |  |  [Molecule Optimization by Explainable Evolution](https://openreview.net/forum?id=jHefDGsorp5) |  | 0 |  | Binghong Chen, Tianzhe Wang, Chengtao Li, Hanjun Dai, Le Song |  |
| 632 |  |  [Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies](https://openreview.net/forum?id=Vd7lCMvtLqg) |  | 0 |  | Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed |  |
| 633 |  |  [PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences](https://openreview.net/forum?id=O3bqkf_Puys) |  | 0 |  | Hehe Fan, Xin Yu, Yuhang Ding, Yi Yang, Mohan S. Kankanhalli |  |
| 634 |  |  [Group Equivariant Conditional Neural Processes](https://openreview.net/forum?id=e8W-hsu_q5) |  | 0 |  | Makoto Kawano, Wataru Kumagai, Akiyoshi Sannai, Yusuke Iwasawa, Yutaka Matsuo |  |
| 635 |  |  [When does preconditioning help or hurt generalization?](https://openreview.net/forum?id=S724o4_WB3) |  | 0 |  | Shunichi Amari, Jimmy Ba, Roger Baker Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny Wu, Ji Xu |  |
| 636 |  |  [Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues](https://openreview.net/forum?id=hPWj1qduVw8) |  | 0 |  | Hung Le, Nancy F. Chen, Steven C. H. Hoi |  |
| 637 |  |  [Prototypical Representation Learning for Relation Extraction](https://openreview.net/forum?id=aCgLmfhIy_f) |  | 0 |  | Ning Ding, Xiaobin Wang, Yao Fu, Guangwei Xu, Rui Wang, Pengjun Xie, Ying Shen, Fei Huang, Haitao Zheng, Rui Zhang |  |
| 638 |  |  [Layer-adaptive Sparsity for the Magnitude-based Pruning](https://openreview.net/forum?id=H6ATjJ0TKdf) |  | 0 |  | Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, Jinwoo Shin |  |
| 639 |  |  [Refining Deep Generative Models via Discriminator Gradient Flow](https://openreview.net/forum?id=Zbc-ue9p_rE) |  | 0 |  | Abdul Fatir Ansari, Ming Liang Ang, Harold Soh |  |
| 640 |  |  [Explaining the Efficacy of Counterfactually Augmented Data](https://openreview.net/forum?id=HHiiQKWsOcV) |  | 0 |  | Divyansh Kaushik, Amrith Setlur, Eduard H. Hovy, Zachary Chase Lipton |  |
| 641 |  |  [Lipschitz Recurrent Neural Networks](https://openreview.net/forum?id=-N7PBXqOUJZ) |  | 0 |  | N. Benjamin Erichson, Omri Azencot, Alejandro F. Queiruga, Liam Hodgkinson, Michael W. Mahoney |  |
| 642 |  |  [Learning Hyperbolic Representations of Topological Features](https://openreview.net/forum?id=yqPnIRhHtZv) |  | 0 |  | Panagiotis Kyriakis, Iordanis Fostiropoulos, Paul Bogdan |  |
| 643 |  |  [Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech](https://openreview.net/forum?id=o3iritJHLfO) |  | 0 |  | Yoonhyung Lee, Joongbo Shin, Kyomin Jung |  |
| 644 |  |  [Risk-Averse Offline Reinforcement Learning](https://openreview.net/forum?id=TBIzh9b5eaz) |  | 0 |  | Núria Armengol Urpí, Sebastian Curi, Andreas Krause |  |
| 645 |  |  [Group Equivariant Stand-Alone Self-Attention For Vision](https://openreview.net/forum?id=JkfYjnOEo6M) |  | 0 |  | David W. Romero, JeanBaptiste Cordonnier |  |
| 646 |  |  [A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning](https://openreview.net/forum?id=vYVI1CHPaQg) |  | 0 |  | Samuel Horváth, Peter Richtárik |  |
| 647 |  |  [Neural Delay Differential Equations](https://openreview.net/forum?id=Q1jmmQz72M2) |  | 0 |  | Qunxi Zhu, Yao Guo, Wei Lin |  |
| 648 |  |  [Capturing Label Characteristics in VAEs](https://openreview.net/forum?id=wQRlSUZ5V7B) |  | 0 |  | Tom Joy, Sebastian M. Schmon, Philip H. S. Torr, Siddharth Narayanaswamy, Tom Rainforth |  |
| 649 |  |  [Graph Edit Networks](https://openreview.net/forum?id=dlEJsyHGeaL) |  | 0 |  | Benjamin Paassen, Daniele Grattarola, Daniele Zambon, Cesare Alippi, Barbara Hammer |  |
| 650 |  |  [InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective](https://openreview.net/forum?id=hpH98mK5Puk) |  | 0 |  | Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, Jingjing Liu |  |
| 651 |  |  [DrNAS: Dirichlet Neural Architecture Search](https://openreview.net/forum?id=9FWas6YbmB3) |  | 0 |  | Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, ChoJui Hsieh |  |
| 652 |  |  [Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration](https://openreview.net/forum?id=1rxHOBjeDUW) |  | 0 |  | Jaekyeom Kim, Minjung Kim, Dongyeon Woo, Gunhee Kim |  |
| 653 |  |  [Monte-Carlo Planning and Learning with Language Action Value Estimates](https://openreview.net/forum?id=7_G8JySGecm) |  | 0 |  | Youngsoo Jang, Seokin Seo, Jongmin Lee, KeeEung Kim |  |
| 654 |  |  [Robust early-learning: Hindering the memorization of noisy labels](https://openreview.net/forum?id=Eql5b1_hTE4) |  | 0 |  | Xiaobo Xia, Tongliang Liu, Bo Han, Chen Gong, Nannan Wang, Zongyuan Ge, Yi Chang |  |
| 655 |  |  [Identifying Physical Law of Hamiltonian Systems via Meta-Learning](https://openreview.net/forum?id=45NZvF1UHam) |  | 0 |  | Seungjun Lee, Haesang Yang, Woojae Seong |  |
| 656 |  |  [Reweighting Augmented Samples by Minimizing the Maximal Expected Loss](https://openreview.net/forum?id=9G5MIc-goqB) |  | 0 |  | Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, ZhiMing Ma |  |
| 657 |  |  [Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning](https://openreview.net/forum?id=n1HD8M6WGn) |  | 0 |  | Xuebo Liu, Longyue Wang, Derek F. Wong, Liang Ding, Lidia S. Chao, Zhaopeng Tu |  |
| 658 |  |  [Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling](https://openreview.net/forum?id=I4c4K9vBNny) |  | 0 |  | Ðorðe Miladinovic, Aleksandar Stanic, Stefan Bauer, Jürgen Schmidhuber, Joachim M. Buhmann |  |
| 659 |  |  [Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition](https://openreview.net/forum?id=Yz-XtK5RBxB) |  | 0 |  | SeonHo Lee, ChangSu Kim |  |
| 660 |  |  [Revisiting Locally Supervised Learning: an Alternative to End-to-end Training](https://openreview.net/forum?id=fAbkE6ant2) |  | 0 |  | Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang |  |
| 661 |  |  [Nonseparable Symplectic Neural Networks](https://openreview.net/forum?id=B5VvQrI49Pa) |  | 0 |  | Shiying Xiong, Yunjin Tong, Xingzhe He, Shuqi Yang, Cheng Yang, Bo Zhu |  |
| 662 |  |  [Gradient Origin Networks](https://openreview.net/forum?id=0O_cQfw6uEh) |  | 0 |  | Sam BondTaylor, Chris G. Willcocks |  |
| 663 |  |  [Learning to Sample with Local and Global Contexts in Experience Replay Buffer](https://openreview.net/forum?id=gJYlaqL8i8) |  | 0 |  | Youngmin Oh, Kimin Lee, Jinwoo Shin, Eunho Yang, Sung Ju Hwang |  |
| 664 |  |  [Provable Rich Observation Reinforcement Learning with Combinatorial Latent States](https://openreview.net/forum?id=hx1IXFHAw7R) |  | 0 |  | Dipendra Misra, Qinghua Liu, Chi Jin, John Langford |  |
| 665 |  |  [Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions](https://openreview.net/forum?id=r28GdiQF7vM) |  | 0 |  | Yunwen Lei, Yiming Ying |  |
| 666 |  |  [Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets](https://openreview.net/forum?id=rkQuFUmUOg3) |  | 0 |  | Hayeon Lee, Eunyoung Hyung, Sung Ju Hwang |  |
| 667 |  |  [Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models](https://openreview.net/forum?id=vhKe9UFbrJo) |  | 0 |  | Yuge Shi, Brooks Paige, Philip H. S. Torr, N. Siddharth |  |
| 668 |  |  [FedMix: Approximation of Mixup under Mean Augmented Federated Learning](https://openreview.net/forum?id=Ogga20D2HO-) |  | 0 |  | Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang |  |
| 669 |  |  [Generalized Variational Continual Learning](https://openreview.net/forum?id=_IM-AfFhna9) |  | 0 |  | Noel Loo, Siddharth Swaroop, Richard E. Turner |  |
| 670 |  |  [Understanding and Improving Lexical Choice in Non-Autoregressive Translation](https://openreview.net/forum?id=ZTFeSBIX9C) |  | 0 |  | Liang Ding, Longyue Wang, Xuebo Liu, Derek F. Wong, Dacheng Tao, Zhaopeng Tu |  |
| 671 |  |  [Bayesian Context Aggregation for Neural Processes](https://openreview.net/forum?id=ufZN2-aehFa) |  | 0 |  | Michael Volpp, Fabian Flürenbrock, Lukas Großberger, Christian Daniel, Gerhard Neumann |  |
| 672 |  |  [Variational Intrinsic Control Revisited](https://openreview.net/forum?id=P0p33rgyoE) |  | 0 |  | Taehwan Kwon |  |
| 673 |  |  [Implicit Gradient Regularization](https://openreview.net/forum?id=3q5IqUrkcF) |  | 0 |  | David G. T. Barrett, Benoit Dherin |  |
| 674 |  |  [Return-Based Contrastive Representation Learning for Reinforcement Learning](https://openreview.net/forum?id=_TM6rT7tXke) |  | 0 |  | Guoqing Liu, Chuheng Zhang, Li Zhao, Tao Qin, Jinhua Zhu, Jian Li, Nenghai Yu, TieYan Liu |  |
| 675 |  |  [Scalable Bayesian Inverse Reinforcement Learning](https://openreview.net/forum?id=4qR3coiNaIv) |  | 0 |  | Alex James Chan, Mihaela van der Schaar |  |
| 676 |  |  [Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization](https://openreview.net/forum?id=QO9-y8also-) |  | 0 |  | Judy Borowski, Roland Simon Zimmermann, Judith Schepers, Robert Geirhos, Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel |  |
| 677 |  |  [LiftPool: Bidirectional ConvNet Pooling](https://openreview.net/forum?id=kE3vd639uRW) |  | 0 |  | Jiaojiao Zhao, Cees G. M. Snoek |  |
| 678 |  |  [Adversarial score matching and improved sampling for image generation](https://openreview.net/forum?id=eLfqMl3z3lq) |  | 0 |  | Alexia JolicoeurMartineau, Rémi PichéTaillefer, Ioannis Mitliagkas, Remi Tachet des Combes |  |
| 679 |  |  [Transient Non-stationarity and Generalisation in Deep Reinforcement Learning](https://openreview.net/forum?id=Qun8fv4qSby) |  | 0 |  | Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, Shimon Whiteson |  |
| 680 |  |  [On the Origin of Implicit Regularization in Stochastic Gradient Descent](https://openreview.net/forum?id=rq_Qr0c1Hyo) |  | 0 |  | Samuel L. Smith, Benoit Dherin, David G. T. Barrett, Soham De |  |
| 681 |  |  [Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective](https://openreview.net/forum?id=-qh0M9XWxnv) |  | 0 |  | Muhammet Balcilar, Guillaume Renton, Pierre Héroux, Benoit Gaüzère, Sébastien Adam, Paul Honeine |  |
| 682 |  |  [Pruning Neural Networks at Initialization: Why Are We Missing the Mark?](https://openreview.net/forum?id=Ig-VyQc-MLK) |  | 0 |  | Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, Michael Carbin |  |
| 683 |  |  [SkipW: Resource Adaptable RNN with Strict Upper Computational Limit](https://openreview.net/forum?id=2CjEVW-RGOJ) |  | 0 |  | Tsiry Mayet, Anne Lambert, Pascal Leguyadec, Françoise Le Bolzer, François Schnitzler |  |
| 684 |  |  [Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds](https://openreview.net/forum?id=MDsQkFP1Aw) |  | 0 |  | Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Dan Ellis, John R. Hershey |  |
| 685 |  |  [Simple Augmentation Goes a Long Way: ADRL for DNN Quantization](https://openreview.net/forum?id=Qr0aRliE_Hb) |  | 0 |  | Lin Ning, Guoyang Chen, Weifeng Zhang, Xipeng Shen |  |
| 686 |  |  [Few-Shot Bayesian Optimization with Deep Kernel Surrogates](https://openreview.net/forum?id=bJxgv5C3sYc) |  | 0 |  | Martin Wistuba, Josif Grabocka |  |
| 687 |  |  [AdaSpeech: Adaptive Text to Speech for Custom Voice](https://openreview.net/forum?id=Drynvt7gg4L) |  | 0 |  | Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao, TieYan Liu |  |
| 688 |  |  [HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients](https://openreview.net/forum?id=TNkPBBYFkXg) |  | 0 |  | Enmao Diao, Jie Ding, Vahid Tarokh |  |
| 689 |  |  [DINO: A Conditional Energy-Based GAN for Domain Translation](https://openreview.net/forum?id=WAISmwsqDsb) |  | 0 |  | Konstantinos Vougioukas, Stavros Petridis, Maja Pantic |  |
| 690 |  |  [Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning](https://openreview.net/forum?id=N33d7wjgzde) |  | 0 |  | TsungWei Ke, JyhJing Hwang, Stella X. Yu |  |
| 691 |  |  [PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds](https://openreview.net/forum?id=8X2eaSZxTP) |  | 0 |  | Yujia Liu, Stefano D'Aronco, Konrad Schindler, Jan Dirk Wegner |  |
| 692 |  |  [Multi-resolution modeling of a discrete stochastic process identifies causes of cancer](https://openreview.net/forum?id=KtH8W3S_RE) |  | 0 |  | Adam Uri Yaari, Maxwell Sherman, Oliver Clarke Priebe, PoRu Loh, Boris Katz, Andrei Barbu, Bonnie Berger |  |
| 693 |  |  [C-Learning: Horizon-Aware Cumulative Accessibility Estimation](https://openreview.net/forum?id=W3Wf_wKmqm9) |  | 0 |  | Panteha Naderian, Gabriel LoaizaGanem, Harry J. Braviner, Anthony L. Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg |  |
| 694 |  |  [Shapley Explanation Networks](https://openreview.net/forum?id=vsU0efpivw) |  | 0 |  | Rui Wang, Xiaoqian Wang, David I. Inouye |  |
| 695 |  |  [The role of Disentanglement in Generalisation](https://openreview.net/forum?id=qbH974jKUVy) |  | 0 |  | Milton Llera Montero, Casimir J. H. Ludwig, Rui Ponte Costa, Gaurav Malhotra, Jeffrey S. Bowers |  |
| 696 |  |  [Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch](https://openreview.net/forum?id=K9bw7vqp_s) |  | 0 |  | Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, Hongsheng Li |  |
| 697 |  |  [Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models](https://openreview.net/forum?id=XOjv2HxIF6i) |  | 0 |  | Siavash Khodadadeh, Sharare Zehtabian, Saeed Vahidian, Weijia Wang, Bill Lin, Ladislau Bölöni |  |
| 698 |  |  [On Data-Augmentation and Consistency-Based Semi-Supervised Learning](https://openreview.net/forum?id=7FNqrcPtieT) |  | 0 |  | Atin Ghosh, Alexandre H. Thiéry |  |
| 699 |  |  [Learning from Demonstration with Weakly Supervised Disentanglement](https://openreview.net/forum?id=Ldau9eHU-qO) |  | 0 |  | Yordan Hristov, Subramanian Ramamoorthy |  |
| 700 |  |  [Neurally Augmented ALISTA](https://openreview.net/forum?id=q_S44KLQ_Aa) |  | 0 |  | Freya Behrens, Jonathan Sauder, Peter Jung |  |
| 701 |  |  [Shape or Texture: Understanding Discriminative Features in CNNs](https://openreview.net/forum?id=NcFEZOi-rLa) |  | 0 |  | Md. Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Björn Ommer, Konstantinos G. Derpanis, Neil D. B. Bruce |  |
| 702 |  |  [Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization](https://openreview.net/forum?id=te7PVH1sPxJ) |  | 0 |  | ChinWei Huang, Ricky T. Q. Chen, Christos Tsirigotis, Aaron C. Courville |  |
| 703 |  |  [Wasserstein Embedding for Graph Learning](https://openreview.net/forum?id=AAes_3W-2z) |  | 0 |  | Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, Heiko Hoffmann |  |
| 704 |  |  [Meta-learning with negative learning rates](https://openreview.net/forum?id=60j5LygnmD) |  | 0 |  | Alberto Bernacchia |  |
| 705 |  |  [Representing Partial Programs with Blended Abstract Semantics](https://openreview.net/forum?id=mCtadqIxOJ) |  | 0 |  | Maxwell I. Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B. Tenenbaum, Armando SolarLezama |  |
| 706 |  |  [Fast convergence of stochastic subgradient method under interpolation](https://openreview.net/forum?id=w2mYg3d0eot) |  | 0 |  | Huang Fang, Zhenan Fan, Michael P. Friedlander |  |
| 707 |  |  [A Hypergradient Approach to Robust Regression without Correspondence](https://openreview.net/forum?id=l35SB-_raSQ) |  | 0 |  | Yujia Xie, Yixiu Mao, Simiao Zuo, Hongteng Xu, Xiaojing Ye, Tuo Zhao, Hongyuan Zha |  |
| 708 |  |  [On the role of planning in model-based deep reinforcement learning](https://openreview.net/forum?id=IrM64DGB21) |  | 0 |  | Jessica B. Hamrick, Abram L. Friesen, Feryal M. P. Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Holger Buesing, Petar Velickovic, Theophane Weber |  |
| 709 |  |  [Trajectory Prediction using Equivariant Continuous Convolution](https://openreview.net/forum?id=J8_GttYLFgr) |  | 0 |  | Robin Walters, Jinxi Li, Rose Yu |  |
| 710 |  |  [Grounding Language to Autonomously-Acquired Skills via Goal Generation](https://openreview.net/forum?id=chPj_I5KMHG) |  | 0 |  | Ahmed Akakzia, Cédric Colas, PierreYves Oudeyer, Mohamed Chetouani, Olivier Sigaud |  |
| 711 |  |  [Chaos of Learning Beyond Zero-sum and Coordination via Game Decompositions](https://openreview.net/forum?id=a3wKPZpGtCF) |  | 0 |  | Yun Kuen Cheung, Yixin Tao |  |
| 712 |  |  [Isometric Transformation Invariant and Equivariant Graph Convolutional Networks](https://openreview.net/forum?id=FX0vR39SJ5q) |  | 0 |  | Masanobu Horie, Naoki Morita, Toshiaki Hishinuma, Yu Ihara, Naoto Mitsume |  |
| 713 |  |  [R-GAP: Recursive Gradient Attack on Privacy](https://openreview.net/forum?id=RSU17UoKfJF) |  | 0 |  | Junyi Zhu, Matthew B. Blaschko |  |
| 714 |  |  [Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks](https://openreview.net/forum?id=C70cp4Cn32) |  | 0 |  | Timothy Castiglia, Anirban Das, Stacy Patterson |  |
| 715 |  |  [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://openreview.net/forum?id=qrwe7XHTmYb) |  | 0 |  | Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen |  |
| 716 |  |  [Representation learning for improved interpretability and classification accuracy of clinical factors from EEG](https://openreview.net/forum?id=TVjLza1t4hI) |  | 0 |  | Garrett Honke, Irina Higgins, Nina Thigpen, Vladimir Miskovic, Katie Link, Sunny Duan, Pramod Gupta, Julia Klawohn, Greg Hajcak |  |
| 717 |  |  [Multiplicative Filter Networks](https://openreview.net/forum?id=OmtmcPkkhT) |  | 0 |  | Rizal Fathony, Anit Kumar Sahu, Devin Willmott, J. Zico Kolter |  |
| 718 |  |  [Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks](https://openreview.net/forum?id=7uVcpu-gMD) |  | 0 |  | Róbert Csordás, Sjoerd van Steenkiste, Jürgen Schmidhuber |  |
| 719 |  |  [Modeling the Second Player in Distributionally Robust Optimization](https://openreview.net/forum?id=ZDnzZrTqU9N) |  | 0 |  | Paul Michel, Tatsunori Hashimoto, Graham Neubig |  |
| 720 |  |  [Private Post-GAN Boosting](https://openreview.net/forum?id=6isfR3JCbi) |  | 0 |  | Marcel Neunhoeffer, Steven Wu, Cynthia Dwork |  |
| 721 |  |  [Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis](https://openreview.net/forum?id=Ig53hpHxS4) |  | 0 |  | Rafael Valle, Kevin J. Shih, Ryan Prenger, Bryan Catanzaro |  |
| 722 |  |  [Learning Structural Edits via Incremental Tree Transformations](https://openreview.net/forum?id=v9hAX77--cZ) |  | 0 |  | Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, Graham Neubig |  |
| 723 |  |  [Sample-Efficient Automated Deep Reinforcement Learning](https://openreview.net/forum?id=hSjxQ3B7GWq) |  | 0 |  | Jörg K. H. Franke, Gregor Köhler, André Biedenkapp, Frank Hutter |  |
| 724 |  |  [Unsupervised Discovery of 3D Physical Objects from Video](https://openreview.net/forum?id=lf7st0bJIA5) |  | 0 |  | Yilun Du, Kevin A. Smith, Tomer D. Ullman, Joshua B. Tenenbaum, Jiajun Wu |  |
| 725 |  |  [Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime](https://openreview.net/forum?id=bB2drc7DPuB) |  | 0 |  | Andrea Agazzi, Jianfeng Lu |  |
| 726 |  |  [Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers](https://openreview.net/forum?id=Nc3TJqbcl3) |  | 0 |  | Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Georg Martius |  |
| 727 |  |  [Temporally-Extended ε-Greedy Exploration](https://openreview.net/forum?id=ONBPHFZ7zG4) |  | 0 |  | Will Dabney, Georg Ostrovski, André Barreto |  |
| 728 |  |  [Rapid Task-Solving in Novel Environments](https://openreview.net/forum?id=F-mvpFpn_0q) |  | 0 |  | Samuel Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matthew M. Botvinick, David Raposo |  |
| 729 |  |  [Tradeoffs in Data Augmentation: An Empirical Study](https://openreview.net/forum?id=ZcKPWuhG6wy) |  | 0 |  | Raphael Gontijo Lopes, Sylvia J. Smullin, Ekin Dogus Cubuk, Ethan Dyer |  |
| 730 |  |  [Multiscale Score Matching for Out-of-Distribution Detection](https://openreview.net/forum?id=xoHdgbQJohv) |  | 0 |  | Ahsan Mahmood, Junier Oliva, Martin Andreas Styner |  |
| 731 |  |  [Understanding Over-parameterization in Generative Adversarial Networks](https://openreview.net/forum?id=C3qvk5IQIJY) |  | 0 |  | Yogesh Balaji, Mohammadmahdi Sajedi, Neha Mukund Kalibhat, Mucong Ding, Dominik Stöger, Mahdi Soltanolkotabi, Soheil Feizi |  |
| 732 |  |  [Go with the flow: Adaptive control for Neural ODEs](https://openreview.net/forum?id=giit4HdDNa) |  | 0 |  | Mathieu Chalvidal, Matthew Ricci, Rufin VanRullen, Thomas Serre |  |
| 733 |  |  [Linear Last-iterate Convergence in Constrained Saddle-point Optimization](https://openreview.net/forum?id=dx11_7vm5_r) |  | 0 |  | ChenYu Wei, ChungWei Lee, Mengxiao Zhang, Haipeng Luo |  |
| 734 |  |  [Learning advanced mathematical computations from examples](https://openreview.net/forum?id=-gfhS00XfKj) |  | 0 |  | François Charton, Amaury Hayat, Guillaume Lample |  |
| 735 |  |  [WaveGrad: Estimating Gradients for Waveform Generation](https://openreview.net/forum?id=NsMLjcFaO8O) |  | 0 |  | Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, William Chan |  |
| 736 |  |  [SALD: Sign Agnostic Learning with Derivatives](https://openreview.net/forum?id=7EDgLu9reQD) |  | 0 |  | Matan Atzmon, Yaron Lipman |  |
| 737 |  |  [Generalized Energy Based Models](https://openreview.net/forum?id=0PtUPB9z6qK) |  | 0 |  | Michael Arbel, Liang Zhou, Arthur Gretton |  |
| 738 |  |  [Long Range Arena : A Benchmark for Efficient Transformers](https://openreview.net/forum?id=qVyeW-grC2k) |  | 0 |  | Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler |  |
| 739 |  |  [Beyond Categorical Label Representations for Image Classification](https://openreview.net/forum?id=MyHwDabUHZm) |  | 0 |  | Boyuan Chen, Yu Li, Sunand Raghupathi, Hod Lipson |  |
| 740 |  |  [CoCo: Controllable Counterfactuals for Evaluating Dialogue State Trackers](https://openreview.net/forum?id=eom0IUrF__F) |  | 0 |  | Shiyang Li, Semih Yavuz, Kazuma Hashimoto, Jia Li, Tong Niu, Nazneen Fatema Rajani, Xifeng Yan, Yingbo Zhou, Caiming Xiong |  |
| 741 |  |  [Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models](https://openreview.net/forum?id=gwFTuzxJW0) |  | 0 |  | Mitch Hill, Jonathan Craig Mitchell, SongChun Zhu |  |
| 742 |  |  [X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback](https://openreview.net/forum?id=LiX3ECzDPHZ) |  | 0 |  | Jensen Gao, Siddharth Reddy, Glen Berseth, Nicholas Hardy, Nikhilesh Natraj, Karunesh Ganguly, Anca D. Dragan, Sergey Levine |  |
| 743 |  |  [Mapping the Timescale Organization of Neural Language Models](https://openreview.net/forum?id=J3OUycKwz-) |  | 0 |  | HsiangYun Sherry Chien, Jinhan Zhang, Christopher J. Honey |  |
| 744 |  |  [PDE-Driven Spatiotemporal Disentanglement](https://openreview.net/forum?id=vLaHRtHvfFp) |  | 0 |  | Jérémie Donà, JeanYves Franceschi, Sylvain Lamprier, Patrick Gallinari |  |
| 745 |  |  [OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning](https://openreview.net/forum?id=V69LGwJ0lIN) |  | 0 |  | Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, Ofir Nachum |  |
| 746 |  |  [Does enhanced shape bias improve neural network robustness to common corruptions?](https://openreview.net/forum?id=yUxUNaj2Sl) |  | 0 |  | Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, Volker Fischer, Jan Hendrik Metzen |  |
| 747 |  |  [Directed Acyclic Graph Neural Networks](https://openreview.net/forum?id=JbuYF437WB6) |  | 0 |  | Veronika Thost, Jie Chen |  |
| 748 |  |  [QPLEX: Duplex Dueling Multi-Agent Q-Learning](https://openreview.net/forum?id=Rcmk0xxIQV) |  | 0 |  | Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, Chongjie Zhang |  |
| 749 |  |  [Learning Energy-Based Models by Diffusion Recovery Likelihood](https://openreview.net/forum?id=v_1Soh8QUNc) |  | 0 |  | Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma |  |
| 750 |  |  [Neural Networks for Learning Counterfactual G-Invariances from Single Environments](https://openreview.net/forum?id=7t1FcJUWhi3) |  | 0 |  | S. Chandra Mouli, Bruno Ribeiro |  |
| 751 |  |  [Model-Based Offline Planning](https://openreview.net/forum?id=OMNB1G5xzd4) |  | 0 |  | Arthur Argenson, Gabriel DulacArnold |  |
| 752 |  |  [On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections](https://openreview.net/forum?id=xgGS6PmzNq6) |  | 0 |  | Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, Hongfu Liu |  |
| 753 |  |  [Coping with Label Shift via Distributionally Robust Optimisation](https://openreview.net/forum?id=BtZhsSGNRNi) |  | 0 |  | Jingzhao Zhang, Aditya Krishna Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, Suvrit Sra |  |
| 754 |  |  [Faster Binary Embeddings for Preserving Euclidean Distances](https://openreview.net/forum?id=YCXrx6rRCXO) |  | 0 |  | Jinjie Zhang, Rayan Saab |  |
| 755 |  |  [Learning and Evaluating Representations for Deep One-Class Classification](https://openreview.net/forum?id=HCSgyPUfeDj) |  | 0 |  | Kihyuk Sohn, ChunLiang Li, Jinsung Yoon, Minho Jin, Tomas Pfister |  |
| 756 |  |  [Conditional Negative Sampling for Contrastive Learning of Visual Representations](https://openreview.net/forum?id=v8b3e5jN66j) |  | 0 |  | Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, Noah D. Goodman |  |
| 757 |  |  [On Position Embeddings in BERT](https://openreview.net/forum?id=onxoVA9FxMw) |  | 0 |  | Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, Jakob Grue Simonsen |  |
| 758 |  |  [Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning](https://openreview.net/forum?id=qkLMTphG5-h) |  | 0 |  | Namyeong Kwon, Hwidong Na, Gabriel Huang, Simon LacosteJulien |  |
| 759 |  |  [Dataset Meta-Learning from Kernel Ridge-Regression](https://openreview.net/forum?id=l-PrrQrK0QR) |  | 0 |  | Timothy Nguyen, Zhourong Chen, Jaehoon Lee |  |
| 760 |  |  [AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition](https://openreview.net/forum?id=bM3L3I_853) |  | 0 |  | Yue Meng, Rameswar Panda, ChungChing Lin, Prasanna Sattigeri, Leonid Karlinsky, Kate Saenko, Aude Oliva, Rogério Feris |  |
| 761 |  |  [One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks](https://openreview.net/forum?id=uz5uw6gM0m) |  | 0 |  | Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal Sharan, Xin Wang, Qiuyi Zhang |  |
| 762 |  |  [Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms](https://openreview.net/forum?id=fGF8qAqpXXG) |  | 0 |  | Arda Sahiner, Tolga Ergen, John M. Pauly, Mert Pilanci |  |
| 763 |  |  [In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning](https://openreview.net/forum?id=-ODN6SbiUU) |  | 0 |  | Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S. Rawat, Mubarak Shah |  |
| 764 |  |  [MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning](https://openreview.net/forum?id=D3PcGLdMx0) |  | 0 |  | Nanyi Fei, Zhiwu Lu, Tao Xiang, Songfang Huang |  |
| 765 |  |  [Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients](https://openreview.net/forum?id=iQQK02mxVIT) |  | 0 |  | Jing An, Lexing Ying, Yuhua Zhu |  |
| 766 |  |  [Prediction and generalisation over directed actions by grid cells](https://openreview.net/forum?id=Ptaz_zIFbX) |  | 0 |  | Changmin Yu, Timothy Behrens, Neil Burgess |  |
| 767 |  |  [Hopper: Multi-hop Transformer for Spatiotemporal Reasoning](https://openreview.net/forum?id=MaZFq7bJif7) |  | 0 |  | Honglu Zhou, Asim Kadav, Farley Lai, Alexandru NiculescuMizil, Martin Renqiang Min, Mubbasir Kapadia, Hans Peter Graf |  |
| 768 |  |  [Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization](https://openreview.net/forum?id=D_KeYoqCYC) |  | 0 |  | Joshua C. Chang, Patrick Fletcher, Jungmin Han, Ted L. Chang, Shashaank Vattikuti, Bart Desmet, Ayah Zirikly, Carson C. Chow |  |
| 769 |  |  [Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning](https://openreview.net/forum?id=xGZG2kS5bFk) |  | 0 |  | Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang |  |
| 770 |  |  [PAC Confidence Predictions for Deep Neural Network Classifiers](https://openreview.net/forum?id=Qk-Wq5AIjpq) |  | 0 |  | Sangdon Park, Shuo Li, Insup Lee, Osbert Bastani |  |
| 771 |  |  [BREEDS: Benchmarks for Subpopulation Shift](https://openreview.net/forum?id=mQPBmvyAuk) |  | 0 |  | Shibani Santurkar, Dimitris Tsipras, Aleksander Madry |  |
| 772 |  |  [Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification](https://openreview.net/forum?id=7dpmlkBuJFC) |  | 0 |  | Yingxue Zhou, Steven Wu, Arindam Banerjee |  |
| 773 |  |  [End-to-End Egospheric Spatial Memory](https://openreview.net/forum?id=rRFIni1CYmy) |  | 0 |  | Daniel James Lenton, Stephen James, Ronald Clark, Andrew J. Davison |  |
| 774 |  |  [Evaluating the Disentanglement of Deep Generative Models through Manifold Topology](https://openreview.net/forum?id=djwS0m4Ft_A) |  | 0 |  | Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar E. Carlsson, Stefano Ermon |  |
| 775 |  |  [SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing](https://openreview.net/forum?id=oyZxhRI2RiE) |  | 0 |  | Tao Yu, Rui Zhang, Alex Polozov, Christopher Meek, Ahmed Hassan Awadallah |  |
| 776 |  |  [Decoupling Global and Local Representations via Invertible Generative Flows](https://openreview.net/forum?id=iWLByfvUhN) |  | 0 |  | Xuezhe Ma, Xiang Kong, Shanghang Zhang, Eduard H. Hovy |  |
| 777 |  |  [Pre-training Text-to-Text Transformers for Concept-centric Common Sense](https://openreview.net/forum?id=3k20LAiHYL2) |  | 0 |  | Wangchunshu Zhou, DongHo Lee, Ravi Kiran Selvam, Seyeon Lee, Xiang Ren |  |
| 778 |  |  [Local Search Algorithms for Rank-Constrained Convex Optimization](https://openreview.net/forum?id=tH6_VWZjoq) |  | 0 |  | Kyriakos Axiotis, Maxim Sviridenko |  |
| 779 |  |  [Combining Label Propagation and Simple Models out-performs Graph Neural Networks](https://openreview.net/forum?id=8E1-f3VhX1o) |  | 0 |  | Qian Huang, Horace He, Abhay Singh, SerNam Lim, Austin R. Benson |  |
| 780 |  |  [Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning](https://openreview.net/forum?id=cu7IUiOhujH) |  | 0 |  | Beliz Gunel, Jingfei Du, Alexis Conneau, Veselin Stoyanov |  |
| 781 |  |  [SAFENet: A Secure, Accurate and Fast Neural Network Inference](https://openreview.net/forum?id=Cz3dbFm5u-) |  | 0 |  | Qian Lou, Yilin Shen, Hongxia Jin, Lei Jiang |  |
| 782 |  |  [Provably robust classification of adversarial examples with detection](https://openreview.net/forum?id=sRA5rLNpmQc) |  | 0 |  | Fatemeh Sheikholeslami, Ali Lotfi, J. Zico Kolter |  |
| 783 |  |  [Saliency is a Possible Red Herring When Diagnosing Poor Generalization](https://openreview.net/forum?id=c9-WeM-ceB) |  | 0 |  | Joseph D. Viviano, Becks Simpson, Francis Dutil, Yoshua Bengio, Joseph Paul Cohen |  |
| 784 |  |  [Fourier Neural Operator for Parametric Partial Differential Equations](https://openreview.net/forum?id=c8P9NQVtmnO) |  | 0 |  | Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, Anima Anandkumar |  |
| 785 |  |  [Combining Ensembles and Data Augmentation Can Harm Your Calibration](https://openreview.net/forum?id=g11CZSghXyY) |  | 0 |  | Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W. Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, Dustin Tran |  |
| 786 |  |  [SOLAR: Sparse Orthogonal Learned and Random Embeddings](https://openreview.net/forum?id=fw-BHZ1KjxJ) |  | 0 |  | Tharun Medini, Beidi Chen, Anshumali Shrivastava |  |
| 787 |  |  [Efficient Empowerment Estimation for Unsupervised Stabilization](https://openreview.net/forum?id=u2YNJPcQlwq) |  | 0 |  | Ruihan Zhao, Kevin Lu, Pieter Abbeel, Stas Tiomkin |  |
| 788 |  |  [More or Less: When and How to Build Convolutional Neural Network Ensembles](https://openreview.net/forum?id=z5Z023VBmDZ) |  | 0 |  | Abdul Wasay, Stratos Idreos |  |
| 789 |  |  [VTNet: Visual Transformer Network for Object Goal Navigation](https://openreview.net/forum?id=DILxQP08O3B) |  | 0 |  | Heming Du, Xin Yu, Liang Zheng |  |
| 790 |  |  [Class Normalization for (Continual)? Generalized Zero-Shot Learning](https://openreview.net/forum?id=7pgFL2Dkyyy) |  | 0 |  | Ivan Skorokhodov, Mohamed Elhoseiny |  |
| 791 |  |  [Batch Reinforcement Learning Through Continuation Method](https://openreview.net/forum?id=po-DLlBuAuz) |  | 0 |  | Yijie Guo, Shengyu Feng, Nicolas Le Roux, Ed H. Chi, Honglak Lee, Minmin Chen |  |
| 792 |  |  [Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning](https://openreview.net/forum?id=AHOs7Sm5H7R) |  | 0 |  | Zhiyuan Li, Yuping Luo, Kaifeng Lyu |  |
| 793 |  |  [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](https://openreview.net/forum?id=vYeQQ29Tbvx) |  | 0 |  | Jonathan Frankle, David J. Schwab, Ari S. Morcos |  |
| 794 |  |  [CompOFA - Compound Once-For-All Networks for Faster Multi-Platform Deployment](https://openreview.net/forum?id=IgIk8RRT-Z) |  | 0 |  | Manas Sahni, Shreya Varshini, Alind Khare, Alexey Tumanov |  |
| 795 |  |  [Adaptive and Generative Zero-Shot Learning](https://openreview.net/forum?id=ahAUv8TI2Mz) |  | 0 |  | YuYing Chou, HsuanTien Lin, TyngLuh Liu |  |
| 796 |  |  [Learning to Make Decisions via Submodular Regularization](https://openreview.net/forum?id=ac288vnG_7U) |  | 0 |  | Ayya Alieva, Aiden Aceves, Jialin Song, Stephen Mayo, Yisong Yue, Yuxin Chen |  |
| 797 |  |  [Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains](https://openreview.net/forum?id=M88oFvqp_9) |  | 0 |  | Utkarsh Ojha, Krishna Kumar Singh, Yong Jae Lee |  |
| 798 |  |  [Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning](https://openreview.net/forum?id=O9bnihsFfXU) |  | 0 |  | Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, Sergey Levine |  |
| 799 |  |  [Disentangling 3D Prototypical Networks for Few-Shot Concept Learning](https://openreview.net/forum?id=-Lr-u0b42he) |  | 0 |  | Mihir Prabhudesai, Shamit Lal, Darshan Patil, HsiaoYu Tung, Adam W. Harley, Katerina Fragkiadaki |  |
| 800 |  |  [Anytime Sampling for Autoregressive Models via Ordered Autoencoding](https://openreview.net/forum?id=TSRTzJnuEBS) |  | 0 |  | Yilun Xu, Yang Song, Sahaj Garg, Linyuan Gong, Rui Shu, Aditya Grover, Stefano Ermon |  |
| 801 |  |  [HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks](https://openreview.net/forum?id=pHXfe1cOmA) |  | 0 |  | Zhou Xian, Shamit Lal, HsiaoYu Tung, Emmanouil Antonios Platanios, Katerina Fragkiadaki |  |
| 802 |  |  [Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning](https://openreview.net/forum?id=TgSVWXw22FQ) |  | 0 |  | Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe Gan, Lawrence Carin |  |
| 803 |  |  [GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing](https://openreview.net/forum?id=kyaIeYj4zZ) |  | 0 |  | Tao Yu, ChienSheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir R. Radev, Richard Socher, Caiming Xiong |  |
| 804 |  |  [Estimating Lipschitz constants of monotone deep equilibrium models](https://openreview.net/forum?id=VcB4QkSfyO) |  | 0 |  | Chirag Pabbaraju, Ezra Winston, J. Zico Kolter |  |
| 805 |  |  [Estimating informativeness of samples with Smooth Unique Information](https://openreview.net/forum?id=kEnBH98BGs5) |  | 0 |  | Hrayr Harutyunyan, Alessandro Achille, Giovanni Paolini, Orchid Majumder, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto |  |
| 806 |  |  [NBDT: Neural-Backed Decision Tree](https://openreview.net/forum?id=mCLVeEpplNE) |  | 0 |  | Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal, Joseph E. Gonzalez |  |
| 807 |  |  [Accurate Learning of Graph Representations with Graph Multiset Pooling](https://openreview.net/forum?id=JHcqXGaqiGn) |  | 0 |  | Jinheon Baek, Minki Kang, Sung Ju Hwang |  |
| 808 |  |  [Byzantine-Resilient Non-Convex Stochastic Gradient Descent](https://openreview.net/forum?id=PbEHqvFtcS) |  | 0 |  | Zeyuan AllenZhu, Faeze Ebrahimianghazani, Jerry Li, Dan Alistarh |  |
| 809 |  |  [MetaNorm: Learning to Normalize Few-Shot Batches Across Domains](https://openreview.net/forum?id=9z_dNsC4B5t) |  | 0 |  | YingJun Du, Xiantong Zhen, Ling Shao, Cees G. M. Snoek |  |
| 810 |  |  [Large Batch Simulation for Deep Reinforcement Learning](https://openreview.net/forum?id=cP5IcoAkfKa) |  | 0 |  | Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva, Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian |  |
| 811 |  |  [Personalized Federated Learning with First Order Model Optimization](https://openreview.net/forum?id=ehJqJQk9cw) |  | 0 |  | Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, José M. Álvarez |  |
| 812 |  |  [Combining Physics and Machine Learning for Network Flow Estimation](https://openreview.net/forum?id=l0V53bErniB) |  | 0 |  | Arlei Lopes da Silva, Furkan Kocayusufoglu, Saber Jafarpour, Francesco Bullo, Ananthram Swami, Ambuj K. Singh |  |
| 813 |  |  [Knowledge Distillation as Semiparametric Inference](https://openreview.net/forum?id=m4UCf24r0Y) |  | 0 |  | Tri Dao, Govinda M. Kamath, Vasilis Syrgkanis, Lester Mackey |  |
| 814 |  |  [Learning Value Functions in Deep Policy Gradients using Residual Variance](https://openreview.net/forum?id=NX1He-aFO_F) |  | 0 |  | Yannis FletBerliac, Reda Ouhamma, OdalricAmbrym Maillard, Philippe Preux |  |
| 815 |  |  [Randomized Ensembled Double Q-Learning: Learning Fast Without a Model](https://openreview.net/forum?id=AY8zfZm0tDd) |  | 0 |  | Xinyue Chen, Che Wang, Zijian Zhou, Keith W. Ross |  |
| 816 |  |  [Decentralized Attribution of Generative Models](https://openreview.net/forum?id=_kxlwvhOodK) |  | 0 |  | Changhoon Kim, Yi Ren, Yezhou Yang |  |
| 817 |  |  [Attentional Constellation Nets for Few-Shot Learning](https://openreview.net/forum?id=vujTf_I8Kmc) |  | 0 |  | Weijian Xu, Yifan Xu, Huaijin Wang, Zhuowen Tu |  |
| 818 |  |  [Adapting to Reward Progressivity via Spectral Reinforcement Learning](https://openreview.net/forum?id=dyjPVUc2KB) |  | 0 |  | Michael Dann, John Thangarajah |  |
| 819 |  |  [TropEx: An Algorithm for Extracting Linear Terms in Deep Neural Networks](https://openreview.net/forum?id=IqtonxWI0V3) |  | 0 |  | Martin Trimmel, Henning Petzka, Cristian Sminchisescu |  |
| 820 |  |  [Reset-Free Lifelong Learning with Skill-Space Planning](https://openreview.net/forum?id=HIGSa_3kOx3) |  | 0 |  | Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch |  |
| 821 |  |  [Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time](https://openreview.net/forum?id=euDnVs0Ynts) |  | 0 |  | Yu Cheng, Honghao Lin |  |
| 822 |  |  [Teaching Temporal Logics to Neural Networks](https://openreview.net/forum?id=dOcQK-f4byz) |  | 0 |  | Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, Bernd Finkbeiner |  |
| 823 |  |  [Spatially Structured Recurrent Modules](https://openreview.net/forum?id=5l9zj5G7vDY) |  | 0 |  | Nasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich, Stefan Bauer, Yash Sharma, Yoshua Bengio, Bernhard Schölkopf |  |
| 824 |  |  [Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes](https://openreview.net/forum?id=lgNx56yZh8a) |  | 0 |  | Jake Snell, Richard S. Zemel |  |
| 825 |  |  [Parameter-Based Value Functions](https://openreview.net/forum?id=tV6oBfuyLTQ) |  | 0 |  | Francesco Faccio, Louis Kirsch, Jürgen Schmidhuber |  |
| 826 |  |  [Hyperbolic Neural Networks++](https://openreview.net/forum?id=Ec85b0tUwbA) |  | 0 |  | Ryohei Shimizu, Yusuke Mukuta, Tatsuya Harada |  |
| 827 |  |  [Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering](https://openreview.net/forum?id=JFKR3WqwyXR) |  | 0 |  | Calypso Herrera, Florian Krach, Josef Teichmann |  |
| 828 |  |  [Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections](https://openreview.net/forum?id=dx4b7lm8jMM) |  | 0 |  | Csaba Tóth, Patric Bonnier, Harald Oberhauser |  |
| 829 |  |  [FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization](https://openreview.net/forum?id=8cpHIfgY4Dj) |  | 0 |  | Lanqing Li, Rui Yang, Dijun Luo |  |
| 830 |  |  [On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis](https://openreview.net/forum?id=8Sqhl-nF50) |  | 0 |  | Zhong Li, Jiequn Han, Weinan E, Qianxiao Li |  |
| 831 |  |  [Generating Adversarial Computer Programs using Optimized Obfuscations](https://openreview.net/forum?id=PH5PH9ZO_4) |  | 0 |  | Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu Fan, Gaoyuan Zhang, UnaMay O'Reilly |  |
| 832 |  |  [BOIL: Towards Representation Change for Few-shot Learning](https://openreview.net/forum?id=umIdUL8rMH) |  | 0 |  | Jaehoon Oh, Hyungjun Yoo, ChangHwan Kim, SeYoung Yun |  |
| 833 |  |  [Interpreting and Boosting Dropout from a Game-Theoretic View](https://openreview.net/forum?id=Jacdvfjicf7) |  | 0 |  | Hao Zhang, Sen Li, Yinchao Ma, Mingjie Li, Yichen Xie, Quanshi Zhang |  |
| 834 |  |  [Representation Learning via Invariant Causal Mechanisms](https://openreview.net/forum?id=9p2ekP904Rs) |  | 0 |  | Jovana Mitrovic, Brian McWilliams, Jacob C. Walker, Lars Holger Buesing, Charles Blundell |  |
| 835 |  |  [Fooling a Complete Neural Network Verifier](https://openreview.net/forum?id=4IwieFS44l) |  | 0 |  | Dániel Zombori, Balázs Bánhelyi, Tibor Csendes, István Megyeri, Márk Jelasity |  |
| 836 |  |  [CPR: Classifier-Projection Regularization for Continual Learning](https://openreview.net/forum?id=F2v4aqEL6ze) |  | 0 |  | Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flávio P. Calmon, Taesup Moon |  |
| 837 |  |  [CO2: Consistent Contrast for Unsupervised Visual Representation Learning](https://openreview.net/forum?id=U4XLJhqwNF1) |  | 0 |  | Chen Wei, Huiyu Wang, Wei Shen, Alan L. Yuille |  |
| 838 |  |  [GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images](https://openreview.net/forum?id=SHvF5xaueVn) |  | 0 |  | Sungmin Cha, Taeeon Park, Byeongjoon Kim, Jongduk Baek, Taesup Moon |  |
| 839 |  |  [Learning Subgoal Representations with Slow Dynamics](https://openreview.net/forum?id=wxRwhSdORKG) |  | 0 |  | Siyuan Li, Lulu Zheng, Jianhao Wang, Chongjie Zhang |  |
| 840 |  |  [Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and Novel-View Synthesis](https://openreview.net/forum?id=ESG-DMKQKsD) |  | 0 |  | Zhipeng Bao, YuXiong Wang, Martial Hebert |  |
| 841 |  |  [Taming GANs with Lookahead-Minmax](https://openreview.net/forum?id=ZW0yXJyNmoG) |  | 0 |  | Tatjana Chavdarova, Matteo Pagliardini, Sebastian U. Stich, François Fleuret, Martin Jaggi |  |
| 842 |  |  [Certify or Predict: Boosting Certified Robustness with Compositional Architectures](https://openreview.net/forum?id=USCNapootw) |  | 0 |  | Mark Niklas Müller, Mislav Balunovic, Martin T. Vechev |  |
| 843 |  |  [New Bounds For Distributed Mean Estimation and Variance Reduction](https://openreview.net/forum?id=t86MwoUCCNe) |  | 0 |  | Peter Davies, Vijaykrishna Gurunanthan, Niusha Moshrefi, Saleh Ashkboos, Dan Alistarh |  |
| 844 |  |  [Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels](https://openreview.net/forum?id=j9Rv7qdXjd) |  | 0 |  | Bin Xin Ru, Xingchen Wan, Xiaowen Dong, Michael A. Osborne |  |
| 845 |  |  [A Discriminative Gaussian Mixture Model with Sparsity](https://openreview.net/forum?id=-_Zp7r2-cGK) |  | 0 |  | Hideaki Hayashi, Seiichi Uchida |  |
| 846 |  |  [Communication in Multi-Agent Reinforcement Learning: Intention Sharing](https://openreview.net/forum?id=qpsl2dR9twy) |  | 0 |  | Woojun Kim, Jongeui Park, Youngchul Sung |  |
| 847 |  |  [Is Attention Better Than Matrix Decomposition?](https://openreview.net/forum?id=1FvkSpWosOl) |  | 0 |  | Zhengyang Geng, MengHao Guo, Hongxu Chen, Xia Li, Ke Wei, Zhouchen Lin |  |
| 848 |  |  [Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers](https://openreview.net/forum?id=nVZtXBI6LNn) |  | 0 |  | Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, ChoJui Hsieh |  |
| 849 |  |  [A Geometric Analysis of Deep Generative Image Models and Its Applications](https://openreview.net/forum?id=GH7QRzUDdXG) |  | 0 |  | Binxu Wang, Carlos R. Ponce |  |
| 850 |  |  [Solving Compositional Reinforcement Learning Problems via Task Reduction](https://openreview.net/forum?id=9SS69KwomAM) |  | 0 |  | Yunfei Li, Yilin Wu, Huazhe Xu, Xiaolong Wang, Yi Wu |  |
| 851 |  |  [ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity](https://openreview.net/forum?id=JoCR4h9O3Ew) |  | 0 |  | Kangkang Lu, Cuong Manh Nguyen, Xun Xu, Kiran Chari, Yu Jing Goh, ChuanSheng Foo |  |
| 852 |  |  [Acting in Delayed Environments with Non-Stationary Markov Policies](https://openreview.net/forum?id=j1RMMKeP2gR) |  | 0 |  | Esther Derman, Gal Dalal, Shie Mannor |  |
| 853 |  |  [Overfitting for Fun and Profit: Instance-Adaptive Data Compression](https://openreview.net/forum?id=oFp8Mx_V5FL) |  | 0 |  | Ties van Rozendaal, Iris A. M. Huijben, Taco Cohen |  |
| 854 |  |  [Learnable Embedding sizes for Recommender Systems](https://openreview.net/forum?id=vQzcqQWIS0q) |  | 0 |  | Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, Yong Li |  |
| 855 |  |  [Generative Scene Graph Networks](https://openreview.net/forum?id=RmcPm9m3tnk) |  | 0 |  | Fei Deng, Zhuo Zhi, Donghun Lee, Sungjin Ahn |  |
| 856 |  |  [Deconstructing the Regularization of BatchNorm](https://openreview.net/forum?id=d-XzF81Wg1) |  | 0 |  | Yann N. Dauphin, Ekin Dogus Cubuk |  |
| 857 |  |  [PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection](https://openreview.net/forum?id=TYXs_y84xRj) |  | 0 |  | Xiongwei Wu, Doyen Sahoo, Steven C. H. Hoi |  |
| 858 |  |  [Simple Spectral Graph Convolution](https://openreview.net/forum?id=CYO5T-YjWZV) |  | 0 |  | Hao Zhu, Piotr Koniusz |  |
| 859 |  |  [Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs](https://openreview.net/forum?id=pGIHq1m7PU) |  | 0 |  | Zhen Han, Peng Chen, Yunpu Ma, Volker Tresp |  |
| 860 |  |  [Evaluation of Neural Architectures trained with square Loss vs Cross-Entropy in Classification Tasks](https://openreview.net/forum?id=hsFN92eQEla) |  | 0 |  | Like Hui, Mikhail Belkin |  |
