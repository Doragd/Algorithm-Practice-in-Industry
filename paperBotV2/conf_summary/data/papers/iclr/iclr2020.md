# ICLR2020

## 会议论文列表

本会议共有 687 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://openreview.net/forum?id=Syx4wnEtvH) |  | 0 | Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique... | ChoJui Hsieh, James Demmel, Jing Li, Jonathan Hseu, Kurt Keutzer, Sanjiv Kumar, Sashank J. Reddi, Srinadh Bhojanapalli, Xiaodan Song, Yang You |  |
| 2 |  |  [SELF: Learning to Filter Noisy Labels with Self-Ensembling](https://openreview.net/forum?id=HkgsPhNYPS) |  | 0 | Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than... | Chaithanya Kumar Mummadi, Duc Tam Nguyen, Laura Beggel, Thi Hoai Phuong Nguyen, ThiPhuongNhung Ngo, Thomas Brox |  |
| 3 |  |  [Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation](https://openreview.net/forum?id=HygnDhEtvr) |  | 0 | Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the... | Lingfei Wu, Mohammed J. Zaki, Yu Chen |  |
| 4 |  |  [Sharing Knowledge in Multi-Task Deep Reinforcement Learning](https://openreview.net/forum?id=rkgpv2VFvr) |  | 0 | We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In... | Andrea Bonarini, Carlo D'Eramo, Davide Tateo, Jan Peters, Marcello Restelli |  |
| 5 |  |  [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://openreview.net/forum?id=H1eCw3EKvH) |  | 0 | Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects... | Leshem Choshen, Lior Fox, Omri Abend, Zohar Aizenbud |  |
| 6 |  |  [StructPool: Structured Graph Pooling via Conditional Random Fields](https://openreview.net/forum?id=BJxg_hVtwH) |  | 0 | Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes. We also... | Hao Yuan, Shuiwang Ji |  |
| 7 |  |  [Learning deep graph matching with channel-independent embedding and Hungarian attention](https://openreview.net/forum?id=rJgBd2NYPH) |  | 0 | Graph matching aims to establishing node-wise correspondence between two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the information in each channel to be merged independently. In contrast, only node embedding is accounted in previous works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of... | Baoxin Li, Junchi Yan, Runzhong Wang, Tianshu Yu |  |
| 8 |  |  [Graph inference learning for semi-supervised classification](https://openreview.net/forum?id=r1evOhEKvH) |  | 0 | In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference... | Chunyan Xu, Jian Yang, Tong Zhang, Wei Liu, Xiaobin Hong, Zhen Cui |  |
| 9 |  |  [SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards](https://openreview.net/forum?id=S1xKd24twB) |  | 0 | Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often... | Anca D. Dragan, Sergey Levine, Siddharth Reddy |  |
| 10 |  |  [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data](https://openreview.net/forum?id=r1eiu2VtwH) |  | 0 | Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious... | Artem Babenko, Sergei Popov, Stanislav Morozov |  |
| 11 |  |  [Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification](https://openreview.net/forum?id=rJlnOhVYPS) |  | 0 | Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In... | Dapeng Chen, Hongsheng Li, Yixiao Ge |  |
| 12 |  |  [Automatically Discovering and Learning New Visual Categories with Ranking Statistics](https://openreview.net/forum?id=BJl2_nVFPB) |  | 0 | We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the... | Andrea Vedaldi, Andrew Zisserman, Kai Han, SylvestreAlvise Rebuffi, Sébastien Ehrhardt |  |
| 13 |  |  [Maxmin Q-learning: Controlling the Estimation Bias of Q-learning](https://openreview.net/forum?id=Bkg0u3Etwr) |  | 0 | Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning;... | Alona Fyshe, Martha White, Qingfeng Lan, Yangchen Pan |  |
| 14 |  |  [Federated Adversarial Domain Adaptation](https://openreview.net/forum?id=HJezF3VYPB) |  | 0 | Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated... | Kate Saenko, Xingchao Peng, Yizhe Zhu, Zijun Huang |  |
| 15 |  |  [Depth-Adaptive Transformer](https://openreview.net/forum?id=SJg7KhVKPH) |  | 0 | State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less... | Edouard Grave, Jiatao Gu, Maha Elbayad, Michael Auli |  |
| 16 |  |  [DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures](https://openreview.net/forum?id=rylBK34FDS) |  | 0 | In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present... | Hai Li, Huanrui Yang, Wei Wen |  |
| 17 |  |  [Evaluating The Search Phase of Neural Architecture Search](https://openreview.net/forum?id=H1loF2NFwr) |  | 0 | Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy... | Christian Sciuto, Claudiu Musat, Kaicheng Yu, Martin Jaggi, Mathieu Salzmann |  |
| 18 |  |  [Diverse Trajectory Forecasting with Determinantal Point Processes](https://openreview.net/forum?id=ryxnY3NYPS) |  | 0 | The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from... | Kris M. Kitani, Ye Yuan |  |
| 19 |  |  [ProxSGD: Training Structured Neural Networks under Regularization and Constraints](https://openreview.net/forum?id=HygpthEtvr) |  | 0 | In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to... | Avraam Chatzimichailidis, Lei Lei, Ruud J. G. van Sloun, Symeon Chatzinotas, Yang Yang, Yaxiong Yuan |  |
| 20 |  |  [LAMOL: LAnguage MOdeling for Lifelong Language Learning](https://openreview.net/forum?id=Skgxcn4YDS) |  | 0 | Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL... | ChengHao Ho, FanKeng Sun, HungYi Lee |  |
| 21 |  |  [Learning Expensive Coordination: An Event-Based Deep RL Approach](https://openreview.net/forum?id=ryeG924twB) |  | 0 | Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's... | Bo An, Hanjiang Lai, Rundong Wang, Runsheng Yu, Xinrun Wang, Youzhi Zhang, Zhenyu Shi |  |
| 22 |  |  [Curvature Graph Network](https://openreview.net/forum?id=BylEqnVFDB) |  | 0 | Graph-structured data is prevalent in many domains. Despite the widely celebrated success of deep neural networks, their power in graph-structured data is yet to be fully explored. We propose a novel network architecture that incorporates advanced graph structural features. In particular, we leverage discrete graph curvature, which measures how the neighborhoods of a pair of nodes are structurally related. The curvature of an edge (x, y) defines the distance taken to travel from neighbors of x to neighbors of y, compared with the length of edge (x, y). It is a much more descriptive feature compared to previously used features that only focus on node specific attributes or limited topological information such as degree. Our... | Chao Chen, Jie Gao, Kin Sum Liu, Tengfei Ma, Ze Ye |  |
| 23 |  |  [Distance-Based Learning from Errors for Confidence Calibration](https://openreview.net/forum?id=BJeB5hVtvB) |  | 0 | Deep neural networks (DNNs) are poorly calibrated when trained in conventional ways. To improve confidence calibration of DNNs, we propose a novel training method, distance-based learning from errors (DBLE). DBLE bases its confidence estimation on distances in the representation space. In DBLE, we first adapt prototypical learning to train classification models. It yields a representation space where the distance between a test sample and its ground truth class center can calibrate the model's classification performance. At inference, however, these distances are not available due to the lack of ground truth labels. To circumvent this by inferring the distance for every test sample, we propose to train a confidence model jointly... | Chen Xing, Sercan Ömer Arik, Tomas Pfister, Zizhao Zhang |  |
| 24 |  |  [Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient](https://openreview.net/forum?id=rkeIq2VYPr) |  | 0 | Determinantal point processes (DPPs) is an effective tool to deliver diversity on multiple machine learning and computer vision tasks. Under deep learning framework, DPP is typically optimized via approximation, which is not straightforward and has some conflict with diversity requirement. We note, however, there has been no deep learning paradigms to optimize DPP directly since it involves matrix inversion which may result in highly computational instability. This fact greatly hinders the wide use of DPP on some specific objectives where DPP serves as a term to measure the feature diversity. In this paper, we devise a simple but effective algorithm to address this issue to optimize DPP term directly expressed with L-ensemble in... | Baoxin Li, Tianshu Yu, Yikang Li |  |
| 25 |  |  [N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://openreview.net/forum?id=r1ecqn4YwB) |  | 0 | We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's... | Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio |  |
| 26 |  |  [Automated Relational Meta-learning](https://openreview.net/forum?id=rklp93EtwH) |  | 0 | In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task... | Bolin Ding, Huaxiu Yao, Ruirui Li, Xian Wu, Yaliang Li, Zhenhui Li, Zhiqiang Tao |  |
| 27 |  |  [To Relieve Your Headache of Training an MRF, Take AdVIL](https://openreview.net/forum?id=Sylgsn4Fvr) |  | 0 | We propose a black-box algorithm called {\it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared... | Bo Zhang, Chao Du, Chongxuan Li, Jun Zhu, Kun Xu, Max Welling |  |
| 28 |  |  [Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware](https://openreview.net/forum?id=H1lBj2VFPS) |  | 0 | With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware coordination at design-phase. In this paper, we propose a learned linear symmetric quantizer for integer neural network processors, which not only quantizes neural parameters and activations to low-bit integer... | Cheng Liu, Lei Zhang, Xiandong Zhao, Xuyi Cai, Ying Wang |  |
| 29 |  |  [Weakly Supervised Clustering by Exploiting Unique Class Count](https://openreview.net/forum?id=B1xIj3VYvr) |  | 0 | A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our... | Hwee Kuan Lee, Mustafa Umit Oner, WingKin Sung |  |
| 30 |  |  [Scalable and Order-robust Continual Learning with Additive Parameter Decomposition](https://openreview.net/forum?id=r1gdj2EKPB) |  | 0 | While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, there are issues that remain to be tackled in order to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which... | Eunho Yang, Jaehong Yoon, Saehoon Kim, Sung Ju Hwang |  |
| 31 |  |  [Continual Learning with Adaptive Weights (CLAW)](https://openreview.net/forum?id=Hklso24Kwr) |  | 0 | Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should... | Han Zhao, Richard E. Turner, Tameem Adel |  |
| 32 |  |  [Transferable Perturbations of Deep Feature Distributions](https://openreview.net/forum?id=rJxAo2VYwr) |  | 0 | Almost all current adversarial attacks of CNN classifiers rely on information derived from the output layer of the network. This work presents a new adversarial attack based on the modeling and exploitation of class-wise and layer-wise deep feature distributions. We achieve state-of-the-art targeted blackbox transfer-based attack results for undefended ImageNet models. Further, we place a priority on explainability and interpretability of the attacking process. Our methodology affords an analysis of how adversarial attacks change the intermediate feature distributions of CNNs, as well as a measure of layer-wise and class-wise feature distributional separability/entanglement. We also conceptualize a transition from... | Kevin J. Liang, Lawrence Carin, Nathan Inkawhich, Yiran Chen |  |
| 33 |  |  [A Learning-based Iterative Method for Solving Vehicle Routing Problems](https://openreview.net/forum?id=BJe1334YDH) |  | 0 | This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3 \citep{helsgaun2017extension} are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). However, there is still a considerable gap between the quality of a machine learned solution and what OR methods can offer (e.g., on CVRP-100, the best result of learned solutions is between 16.10-16.80, significantly worse than LKH3's 15.65). In this paper, we present... | Hao Lu, Shuang Yang, Xingwen Zhang |  |
| 34 |  |  [Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring](https://openreview.net/forum?id=SkxgnnNFvH) |  | 0 | The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve... | Jason Weston, Kurt Shuster, MarieAnne Lachaux, Samuel Humeau |  |
| 35 |  |  [AutoQ: Automated Kernel-Wise Neural Network Quantization](https://openreview.net/forum?id=rygfnn4twS) |  | 0 | Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional... | Feng Guo, Lantao Liu, Lei Jiang, Minje Kim, Qian Lou |  |
| 36 |  |  [Understanding Architectures Learnt by Cell-based Neural Architecture Search](https://openreview.net/forum?id=BJxH22EKPS) |  | 0 | Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness has attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient... | Shaofeng Cai, Wei Wang, Yao Shu |  |
| 37 |  |  [SVQN: Sequential Variational Soft Q-Learning Networks](https://openreview.net/forum?id=r1xPh2VtPB) |  | 0 | Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational... | Hang Su, Jun Zhu, Shiyu Huang, Ting Chen |  |
| 38 |  |  [Ranking Policy Gradient](https://openreview.net/forum?id=rJld3hEYvS) |  | 0 | Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency.... | Jiayu Zhou, Kaixiang Lin |  |
| 39 |  |  [On Mutual Information Maximization for Representation Learning](https://openreview.net/forum?id=rkxoh24FPH) |  | 0 | Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both... | Josip Djolonga, Mario Lucic, Michael Tschannen, Paul K. Rubenstein, Sylvain Gelly |  |
| 40 |  |  [Observational Overfitting in Reinforcement Learning](https://openreview.net/forum?id=HJli2hNKDH) |  | 0 | A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning... | Behnam Neyshabur, Stephen Tu, Xingyou Song, Yiding Jiang, Yilun Du |  |
| 41 |  |  [Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier](https://openreview.net/forum?id=BkgWahEFvr) |  | 0 | Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive that the accuracy on clean images would deteriorate, the exact mechanism in which how this occurs is unclear. In this paper, we study the distribution of softmax induced by stochastic transformations. We observe that with... | Connie Kou, EeChien Chang, Hwee Kuan Lee, Teck Khim Ng |  |
| 42 |  |  [Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks](https://openreview.net/forum?id=BkgXT24tDS) |  | 0 | We propose Additive Powers-of-Two~(APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and... | Wei Wang, Xin Dong, Yuhang Li |  |
| 43 |  |  [Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information](https://openreview.net/forum?id=rJx4p3NYDB) |  | 0 | Counterfactual regret minimization (CFR) methods are effective for solving two-player zero-sum extensive games with imperfect information with state-of-the-art results. However, the vanilla CFR has to traverse the whole game tree in each round, which is time-consuming in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round. We prove that the regret of Lazy-CFR is almost the same to the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Thus, Lazy-CFR is provably faster than CFR. Empirical results consistently show that Lazy-CFR is significantly faster than the vanilla CFR. | Dong Yan, Jialian Li, Jun Zhu, Tongzheng Ren, Yichi Zhou |  |
| 44 |  |  [Knowledge Consistency between Neural Networks and Beyond](https://openreview.net/forum?id=BJeS62EtwH) |  | 0 | This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic definition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More... | Jing Wang, Longfei Li, Quanshi Zhang, Ruofan Liang, Tianlin Li |  |
| 45 |  |  [Image-guided Neural Object Rendering](https://openreview.net/forum?id=Hyg9anEFPS) |  | 0 | We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours and sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this... | Christian Theobalt, Justus Thies, Marc Stamminger, Matthias Nießner, Michael Zollhöfer |  |
| 46 |  |  [Implicit Bias of Gradient Descent based Adversarial Training on Separable Data](https://openreview.net/forum?id=HkgTTh4FDH) |  | 0 | Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this paper, we provide new theoretical insights of gradient descent based adversarial training by studying its computational properties, specifically on its implicit bias. We take the binary classification task on linearly separable data as an illustrative example, where the loss asymptotically attains its infimum as the parameter diverges to infinity along certain directions. Specifically, we show that for any fixed iteration $T$, when the adversarial perturbation during training has proper bounded L2 norm, the classifier learned by gradient... | Ethan X. Fang, Huan Xu, Tuo Zhao, Yan Li |  |
| 47 |  |  [TabFact: A Large-scale Dataset for Table-based Fact Verification](https://openreview.net/forum?id=rkeJRhNYDH) |  | 0 | The problem of verifying whether a textual hypothesis holds based on the given evidence, also known as fact verification, plays an important role in the study of natural language understanding and semantic representation. However, existing studies are mainly restricted to dealing with unstructured evidence (e.g., natural language sentences and documents, news, etc), while verification under structured evidence, such as tables, graphs, and databases, remains unexplored. This paper specifically aims to study the fact verification given semi-structured data as evidence. To this end, we construct a large-scale dataset called TabFact with 16k Wikipedia tables as the evidence for 118k human-annotated natural language statements, which... | Hong Wang, Hongmin Wang, Jianshu Chen, Shiyang Li, Wenhu Chen, William Yang Wang, Xiyou Zhou, Yunkai Zhang |  |
| 48 |  |  [ES-MAML: Simple Hessian-Free Meta Learning](https://openreview.net/forum?id=S1exA2NtDB) |  | 0 | We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and... | Aldo Pacchiano, Krzysztof Choromanski, Wenbo Gao, Xingyou Song, Yunhao Tang, Yuxiang Yang |  |
| 49 |  |  [Neural Stored-program Memory](https://openreview.net/forum?id=rkxxA24FDr) |  | 0 | Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have... | Hung Le, Svetha Venkatesh, Truyen Tran |  |
| 50 |  |  [Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation](https://openreview.net/forum?id=H1gzR2VKDH) |  | 0 | Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the... | Chelsea Finn, Suraj Nair |  |
| 51 |  |  [Multi-agent Reinforcement Learning for Networked System Control](https://openreview.net/forum?id=Syx7A3NFvH) |  | 0 | This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the... | Sachin Katti, Sandeep Chinchali, Tianshu Chu |  |
| 52 |  |  [FSPool: Learning Set Representations with Featurewise Sort Pooling](https://openreview.net/forum?id=HJgBA2VYwH) |  | 0 | Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on a variety of datasets. | Adam PrügelBennett, Jonathon S. Hare, Yan Zhang |  |
| 53 |  |  [Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction](https://openreview.net/forum?id=H1xPR3NtPB) |  | 0 | With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences. | Daniel Edmiston, Jihun Choi, Sanggoo Lee, Taeuk Kim |  |
| 54 |  |  [Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning](https://openreview.net/forum?id=rkeuAhVKvB) |  | 0 | We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, we learn an input-dependent subgraph to explicitly model a sequential reasoning process. Each subgraph is dynamically constructed, expanding itself selectively under a flow-style attention mechanism. In this way, we can not only construct graphical explanations to interpret prediction, but also prune message passing in Graph Neural Networks (GNNs) to scale with the size of graphs. We take the inspiration from the consciousness prior proposed by Bengio to design a two-GNN framework to encode global input-invariant graph-structured representation and learn local... | Wei Feng, Xiaohui Xie, Xiaoran Xu, Yunsheng Jiang, ZhiHong Deng, Zhiqing Sun |  |
| 55 |  |  [Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks](https://openreview.net/forum?id=ByxtC2VtPB) |  | 0 | It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the... | Jun Zhu, Kun Xu, Tianyu Pang |  |
| 56 |  |  [Theory and Evaluation Metrics for Learning Disentangled Representations](https://openreview.net/forum?id=HJgK0h4Ywr) |  | 0 | We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept “disentangled representations” used in supervised and unsupervised methods along three dimensions–informativeness, separability and interpretability–which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the... | Kien Do, Truyen Tran |  |
| 57 |  |  [Measuring Compositional Generalization: A Comprehensive Method on Realistic Data](https://openreview.net/forum?id=SygcCnNKwr) |  | 0 | State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of... | Daniel Furrer, Daniel Keysers, Danila Sinopalnikov, Dmitry Tsarkov, Hylke Buisman, Lukasz Stafiniak, Marc van Zee, Nathan Scales, Nathanael Schärli, Nikola Momchev, Olivier Bousquet, Sergii Kashubin, Tibor Tihon, Xiao Wang |  |
| 58 |  |  [Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness](https://openreview.net/forum?id=Byg9A24tvB) |  | 0 | Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly... | Chao Du, Jun Zhu, Kun Xu, Ning Chen, Tianyu Pang, Yinpeng Dong |  |
| 59 |  |  [The Implicit Bias of Depth: How Incremental Learning Drives Generalization](https://openreview.net/forum?id=H1lj0nNFwB) |  | 0 | A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. We formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Our main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit incremental learning dynamics, they require the initialization to be exponentially small for these dynamics to present themselves. However, once the model becomes deeper, the dependence becomes polynomial and... | Amit Daniely, Daniel Gissin, Shai ShalevShwartz |  |
| 60 |  |  [The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget](https://openreview.net/forum?id=Hye1kTVFDS) |  | 0 | In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant. The information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a \`\`privileged'' input, which might correspond to the goal of a task, the output... | Anirudh Goyal, Matthew M. Botvinick, Sergey Levine, Yoshua Bengio |  |
| 61 |  |  [Learning the Arrow of Time for Problems in Reinforcement Learning](https://openreview.net/forum?id=rylJkpEtwS) |  | 0 | We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments,... | Anirudh Goyal, Nasim Rahaman, Roman Remme, Steffen Wolf, Yoshua Bengio |  |
| 62 |  |  [Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives](https://openreview.net/forum?id=ryxgJTEYDr) |  | 0 | Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We... | Anirudh Goyal, Jonathan Binas, Sergey Levine, Shagun Sodhani, Xue Bin Peng, Yoshua Bengio |  |
| 63 |  |  [Robust Local Features for Improving the Generalization of Adversarial Training](https://openreview.net/forum?id=H1lZJpVFvr) |  | 0 | Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adversarial... | Chuanbiao Song, Jiadong Lin, John E. Hopcroft, Kun He, Liwei Wang |  |
| 64 |  |  [Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification](https://openreview.net/forum?id=rJgQkT4twH) |  | 0 | Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted... | Arno Onken, Bennet Breier |  |
| 65 |  |  [Learning Disentangled Representations for CounterFactual Regression](https://openreview.net/forum?id=HkxBJT4YvB) |  | 0 | We consider the challenge of estimating treatment effects from observational data; and point out that, in general, only some factors based on the observed covariates X contribute to selection of the treatment T, and only some to determining the outcomes Y. We model this by considering three underlying sources of {X, T, Y} and show that explicitly modeling these sources offers great insight to guide designing models that better handle selection bias. This paper is an attempt to conceptualize this line of thought and provide a path to explore it further. In this work, we propose an algorithm to (1) identify disentangled representations of the above-mentioned underlying factors from any given observational dataset D and (2) leverage... | Negar Hassanpour, Russell Greiner |  |
| 66 |  |  [Exploration in Reinforcement Learning with Deep Covering Options](https://openreview.net/forum?id=SkeIyaVtwB) |  | 0 | While many option discovery methods have been proposed to accelerate exploration in reinforcement learning, they are often heuristic. Recently, covering options was proposed to discover a set of options that provably reduce the upper bound of the environment's cover time, a measure of the difficulty of exploration. Covering options are computed using the eigenvectors of the graph Laplacian, but they are constrained to tabular tasks and are not applicable to tasks with large or continuous state-spaces. We introduce deep covering options, an online method that extends covering options to large state spaces, automatically discovering task-agnostic options that encourage exploration. We evaluate our method in several challenging... | George Dimitri Konidaris, Jee Won Park, Marlos C. Machado, Yuu Jinnai |  |
| 67 |  |  [Ae-OT: a New Generative Model based on Extended Semi-discrete Optimal transport](https://openreview.net/forum?id=HkldyTNYwH) |  | 0 | Generative adversarial networks (GANs) have attracted huge attention due to its capability to generate visual realistic images. However, most of the existing models suffer from the mode collapse or mode mixture problems. In this work, we give a theoretic explanation of the both problems by Figalli’s regularity theory of optimal transportation maps. Basically, the generator compute the transportation maps between the white noise distributions and the data distributions, which are in general discontinuous. However, DNNs can only represent continuous maps. This intrinsic conflict induces mode collapse and mode mixture. In order to tackle the both problems, we explicitly separate the manifold embedding and the optimal transportation;... | Dongsheng An, Na Lei, ShingTung Yau, Xianfeng Gu, Yang Guo, Zhongxuan Luo |  |
| 68 |  |  [Logic and the 2-Simplicial Transformer](https://openreview.net/forum?id=rkecJ6VFvr) |  | 0 | We introduce the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. | Daniel Murfet, Dmitry Doryn, James Clift, James Wallbridge |  |
| 69 |  |  [Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards](https://openreview.net/forum?id=SJg5J6NtDr) |  | 0 | Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the... | Alexander Herzog, Allan Zhou, Chelsea Finn, Daniel Kappler, Eric Jang, Mohi Khansari, Mrinal Kalakrishnan, Paul Wohlhart, Sergey Levine, Yunfei Bai |  |
| 70 |  |  [Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking](https://openreview.net/forum?id=rJl31TNYPr) |  | 0 | Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target objection detection: we find that a success rate of over 98% is needed for them to actually affect the tracking results, a requirement that no existing attack technique can satisfy. In this paper, we are the first to... | Hao Chan, Junjie Shen, Qi Alfred Chen, Tao Wei, Yantao Lu, Yunhan Jia, Zhenyu Zhong |  |
| 71 |  |  [DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://openreview.net/forum?id=HJgExaVtwr) |  | 0 | Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we... | Junnan Li, Richard Socher, Steven C. H. Hoi |  |
| 72 |  |  [Improving Adversarial Robustness Requires Revisiting Misclassified Examples](https://openreview.net/forum?id=rklOg6EFwS) |  | 0 | Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, we investigate the distinctive influence of misclassified and correctly... | Difan Zou, James Bailey, Jinfeng Yi, Quanquan Gu, Xingjun Ma, Yisen Wang |  |
| 73 |  |  [V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control](https://openreview.net/forum?id=SylOlp4FvH) |  | 0 | Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably... | Abbas Abdolmaleki, Aidan Clark, Arun Ahuja, Dan Belov, Dhruva Tirumala, H. Francis Song, Hubert Soyer, Jack W. Rae, Jost Tobias Springenberg, Martin A. Riedmiller, Matthew M. Botvinick, Nicolas Heess, Seb Noury, Siqi Liu |  |
| 74 |  |  [Interpretable Complex-Valued Neural Networks for Privacy Protection](https://openreview.net/forum?id=S1xFl64tDr) |  | 0 | Previous studies have found that an adversary attacker can often infer unintended input information from intermediate-layer features. We study the possibility of preventing such adversarial inference, yet without too much accuracy degradation. We propose a generic method to revise the neural network to boost the challenge of inferring input attributes from features, while maintaining highly accurate outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the output from the processing result, but without which the party can neither... | Hao Zhang, Haotian Ma, Jie Ren, Liyao Xiang, Quanshi Zhang, Yifan Zhang |  |
| 75 |  |  [Accelerating SGD with momentum for over-parameterized learning](https://openreview.net/forum?id=r1gixp4FPH) |  | 0 | Nesterov SGD is widely used for training modern neural networks and other machine learning models. Yet, its advantages over SGD have not been theoretically clarified. Indeed, as we show in this paper, both theoretically and empirically, Nesterov SGD with any parameter selection does not in general provide acceleration over ordinary SGD. Furthermore, Nesterov SGD may diverge for step sizes that ensure convergence of ordinary SGD. This is in contrast to the classical results in the deterministic setting, where the same step size ensures accelerated convergence of the Nesterov's method over optimal gradient descent. To address the non-acceleration issue, we introduce a compensation term to Nesterov SGD. The resulting algorithm, which... | Chaoyue Liu, Mikhail Belkin |  |
| 76 |  |  [A critical analysis of self-supervision, or what we can learn from a single image](https://openreview.net/forum?id=B1esx6EYvr) |  | 0 | We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through... | Andrea Vedaldi, Christian Rupprecht, Yuki Markus Asano |  |
| 77 |  |  [Disentangling Factors of Variations Using Few Labels](https://openreview.net/forum?id=SygagpEKwB) |  | 0 | Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under... | Bernhard Schölkopf, Francesco Locatello, Gunnar Rätsch, Michael Tschannen, Olivier Bachem, Stefan Bauer |  |
| 78 |  |  [Functional vs. parametric equivalence of ReLU networks](https://openreview.net/forum?id=Bylx-TNKvH) |  | 0 | We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can... | Christoph H. Lampert, Mary Phuong |  |
| 79 |  |  [Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models](https://openreview.net/forum?id=SyxIWpVYvr) |  | 0 | Likelihood-based generative models are a promising resource to detect out-of-distribution (OOD) inputs which could compromise the robustness or reliability of a machine learning system. However, likelihoods derived from such models have been shown to be problematic for detecting certain types of inputs that significantly differ from training data. In this paper, we pose that this problem is due to the excessive influence that input complexity has in generative models' likelihoods. We report a set of experiments supporting this hypothesis, and use an estimate of input complexity to derive an efficient and parameter-free OOD score, which can be seen as a likelihood-ratio, akin to Bayesian model comparison. We find such score to... | David Álvarez, Joan Serrà, Jordi Luque, José F. Núñez, Olga Slizovskaia, Vicenç Gómez |  |
| 80 |  |  [RTFM: Generalising to New Environment Dynamics via Reading](https://openreview.net/forum?id=SJgob6NKvH) |  | 0 | Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and corresponding language descriptions of the dynamics, such that agents must read to understand new environment dynamics instead of memorising any particular information. In addition, we propose txt2π, a model that captures... | Edward Grefenstette, Tim Rocktäschel, Victor Zhong |  |
| 81 |  |  [What graph neural networks cannot learn: depth vs width](https://openreview.net/forum?id=B1l2bp4YwS) |  | 0 | This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a... | Andreas Loukas |  |
| 82 |  |  [Progressive Memory Banks for Incremental Domain Adaptation](https://openreview.net/forum?id=BkepbpNFwr) |  | 0 | This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the... | Kevin D. Pantasdo, Kira A. Selby, Lili Mou, Nabiha Asghar, Pascal Poupart, Xin Jiang |  |
| 83 |  |  [Automated curriculum generation through setter-solver interactions](https://openreview.net/forum?id=H1e0Wp4KvH) |  | 0 | Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula –the breakdown of tasks into simpler, static challenges with dense rewards– to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich,dynamic environments. Using a setter-solver paradigm we show the importance of considering goal... | Adam Santoro, Andrew K. Lampinen, David P. Reichert, Sébastien Racanière, Timothy P. Lillicrap, Vlad Firoiu |  |
| 84 |  |  [On Identifiability in Transformers](https://openreview.net/forum?id=BJg1f6EFDB) |  | 0 | In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases... | Damian Pascual, Gino Brunner, Massimiliano Ciaramita, Oliver Richter, Roger Wattenhofer, Yang Liu |  |
| 85 |  |  [Exploring Model-based Planning with Policy Networks](https://openreview.net/forum?id=H1exf64KwH) |  | 0 | Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in both sample efficiency and asymptotic performance. Despite the successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy... | Jimmy Ba, Tingwu Wang |  |
| 86 |  |  [Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling](https://openreview.net/forum?id=rke-f6NKvS) |  | 0 | Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results in cascading errors of the learned policy. We introduce a notion of conservatively extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement... | Huazhe Xu, Tengyu Ma, Yuping Luo |  |
| 87 |  |  [Geometric Insights into the Convergence of Nonlinear TD Learning](https://openreview.net/forum?id=SJezGp4YPr) |  | 0 | While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here we take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, we consider the expected learning dynamics of the TD(0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear ODE which depends on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. We find a set of function approximators that includes ReLU networks and... | David Brandfonbrener, Joan Bruna |  |
| 88 |  |  [Few-shot Text Classification with Distributional Signatures](https://openreview.net/forum?id=H1emfT4twB) |  | 0 | In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning framework to map these signatures into attention scores, which are then used to weight the lexical representations of words. We demonstrate that our model consistently outperforms prototypical... | Menghua Wu, Regina Barzilay, Shiyu Chang, Yujia Bao |  |
| 89 |  |  [Escaping Saddle Points Faster with Stochastic Momentum](https://openreview.net/forum?id=rkeNfp4tPr) |  | 0 | Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a \`\`momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears... | ChiHeng Lin, Jacob D. Abernethy, JunKun Wang |  |
| 90 |  |  [Adversarial Policies: Attacking Deep Reinforcement Learning](https://openreview.net/forum?id=HJgEMpVFwB) |  | 0 | Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the... | Adam Gleave, Cody Wild, Michael Dennis, Neel Kant, Sergey Levine, Stuart Russell |  |
| 91 |  |  [VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation](https://openreview.net/forum?id=rJgUfTEYvH) |  | 0 | Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct... | Chelsea Finn, Dumitru Erhan, Durk Kingma, Laurent Dinh, Manoj Kumar, Mohammad Babaeizadeh, Sergey Levine |  |
| 92 |  |  [GLAD: Learning Sparse Graph Recovery](https://openreview.net/forum?id=BkxpMTEtPB) |  | 0 | Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an $\ell_1$ regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many... | Binghong Chen, Guanghui Lan, Han Liu, Harsh Shrivastava, Le Song, Srinivas Aluru, Xinshi Chen |  |
| 93 |  |  [Pruned Graph Scattering Transforms](https://openreview.net/forum?id=rJeg7TEYwB) |  | 0 | Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded... | Georgios B. Giannakis, Siheng Chen, Vassilis N. Ioannidis |  |
| 94 |  |  [Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model](https://openreview.net/forum?id=BJlzm64tDH) |  | 0 | Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models... | Jingfei Du, Veselin Stoyanov, Wenhan Xiong, William Yang Wang |  |
| 95 |  |  [Can gradient clipping mitigate label noise?](https://openreview.net/forum?id=rklB76EKPr) |  | 0 | Gradient clipping is a widely-used technique in the training of deep networks, and is generally motivated from an optimisation lens: informally, it controls the dynamics of iterates, thus enhancing the rate of convergence to a local minimum. This intuition has been made precise in a line of recent works, which show that suitable clipping can yield significantly faster convergence than vanilla gradient descent. In this paper, we propose a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to provide robustness to noise, since one does not overly trust any single sample. Surprisingly, we prove that for the common problem of label noise in classification, standard gradient clipping does not... | Aditya Krishna Menon, Ankit Singh Rawat, Sanjiv Kumar, Sashank J. Reddi |  |
| 96 |  |  [Editable Neural Networks](https://openreview.net/forum?id=HJedXaEtvS) |  | 0 | These days deep neural networks are ubiquitously used in a wide range of tasks, from image classification and machine translation to face identification and self-driving cars. In many applications, a single model error can lead to devastating financial, reputational and even life-threatening consequences. Therefore, it is crucially important to correct model mistakes quickly as they appear. In this work, we investigate the problem of neural network editing - how one can efficiently patch a mistake of the model on a particular sample, without influencing the model behavior on other samples. Namely, we propose Editable Training, a model-agnostic training technique that encourages fast editing of the trained model. We empirically... | Anton Sinitsin, Artem Babenko, Dmitry V. Pyrkin, Sergei Popov, Vsevolod Plokhotnyuk |  |
| 97 |  |  [Learning Execution through Neural Code fusion](https://openreview.net/forum?id=SJetQpEYvB) |  | 0 | As the performance of computer systems stagnates due to the end of Moore’s Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks (GNNs) to learn static representations of source code, these representations do not understand how code executes at runtime. In this work, we propose a new approach using GNNs to learn fused representations of general source code and its execution. Our approach defines a multi-task GNN over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and data structures into a simpler, more... | Daniel Tarlow, Kevin Swersky, Milad Hashemi, Parthasarathy Ranganathan, Zhan Shi |  |
| 98 |  |  [FasterSeg: Searching for Faster Real-time Semantic Segmentation](https://openreview.net/forum?id=BJgqQ6NYvB) |  | 0 | We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to "collapsing" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new... | Qian Zhang, Wuyang Chen, Xianming Liu, Xinyu Gong, Yuan Li, Zhangyang Wang |  |
| 99 |  |  [Difference-Seeking Generative Adversarial Network-Unseen Sample Generation](https://openreview.net/forum?id=rygjmpVFvB) |  | 0 | Unseen data, which are not samples from the distribution of training data and are difficult to collect, have exhibited importance in numerous applications, ({\em e.g.,} novelty detection, semi-supervised learning, and adversarial training). In this paper, we introduce a general framework called \textbf{d}ifference-\textbf{s}eeking \textbf{g}enerative \textbf{a}dversarial \textbf{n}etwork (DSGAN), to generate various types of unseen data. Its novelty is the consideration of the probability density of the unseen data distribution as the difference between two distributions $p_{\bar{d}}$ and $p_{d}$ whose samples are relatively easy to collect. The DSGAN can learn the target distribution, $p_{t}$, (or the unseen data distribution)... | ChunShien Lu, SooChang Pei, SungHsien Hsieh, Yi Lin Sung |  |
| 100 |  |  [Stochastic AUC Maximization with Deep Neural Networks](https://openreview.net/forum?id=HJepXaVYDr) |  | 0 | Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we... | Mingrui Liu, Tianbao Yang, Yiming Ying, Zhuoning Yuan |  |
| 101 |  |  [Semantically-Guided Representation Learning for Self-Supervised Monocular Depth](https://openreview.net/forum?id=ByxT7TNFvH) |  | 0 | Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a... | Adrien Gaidon, Jie Li, Rares Ambrus, Rui Hou, Vitor Guizilini |  |
| 102 |  |  [MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius](https://openreview.net/forum?id=rJx1Na4Fwr) |  | 0 | Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10,... | Boqing Gong, Chen Dan, ChoJui Hsieh, Di He, Huan Zhang, Liwei Wang, Pradeep Ravikumar, Runtian Zhai |  |
| 103 |  |  [Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions](https://openreview.net/forum?id=Skgy464Kvr) |  | 0 | Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack... | Colin Raffel, Garrison W. Cottrell, Geoffrey E. Hinton, Nicholas Frosst, Sara Sabour, Yao Qin |  |
| 104 |  |  [GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification](https://openreview.net/forum?id=SJeQEp4YDH) |  | 0 | The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we propose a principled adversarial example detection method that can withstand norm-constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, we train K binary classifiers where the i-th binary classifier is used to distinguish between clean data of class i and adversarially perturbed samples of... | Gustavo K. Rohde, Soheil Kolouri, Xuwang Yin |  |
| 105 |  |  [Variational Recurrent Models for Solving Partially Observable Control Tasks](https://openreview.net/forum?id=r1lL4a4tDB) |  | 0 | In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations to solve the task, and how to improve the policy. In this study, we propose an RL algorithm for solving PO tasks. Our method comprises two parts: a variational recurrent model (VRM) for modeling the environment, and an RL controller that has access to both the environment and the VRM. The proposed algorithm was tested in two types of PO robotic control tasks, those in which either coordinates or velocities were not observable and those that require long-term memorization. Our experiments show that the... | Dongqi Han, Jun Tani, Kenji Doya |  |
| 106 |  |  [Population-Guided Parallel Policy Search for Reinforcement Learning](https://openreview.net/forum?id=rJeINp4KwH) |  | 0 | In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged range enable faster and better policy search, and monotone improvement of the expected cumulative... | Giseung Park, Whiyoung Jung, Youngchul Sung |  |
| 107 |  |  [Compositional languages emerge in a neural iterated learning model](https://openreview.net/forum?id=HkePNpVKPB) |  | 0 | The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents via grounded language learning. Inspired by the iterated learning framework, which simulates the process of language evolution, we propose an effective neural iterated learning algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide specific advantages to neural agents... | Matthieu Labeau, Shangmin Guo, Shay B. Cohen, Simon Kirby, Yi Ren |  |
| 108 |  |  [Black-Box Adversarial Attack with Transferable Model-based Embedding](https://openreview.net/forum?id=SJxhNTNYwB) |  | 0 | We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a... | Tong Zhang, Zhichao Huang |  |
| 109 |  |  [I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively](https://openreview.net/forum?id=rJehNT4YPr) |  | 0 | The learning of hierarchical representations for image classification has experienced an impressive series of successes due in part to the availability of large-scale labeled data for training. On the other hand, the trained classifiers have traditionally been evaluated on small and fixed sets of test images, which are deemed to be extremely sparsely distributed in the space of all natural images. It is thus questionable whether recent performance improvements on the excessively re-used test sets generalize to real-world natural images with much richer content variations. Inspired by efficient stimulus selection for testing perceptual models in psychophysical and physiological studies, we present an alternative framework for... | Haotao Wang, Kede Ma, Tianlong Chen, Zhangyang Wang |  |
| 110 |  |  [Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models](https://openreview.net/forum?id=HkgaETNtDB) |  | 0 | In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as “mixout”, motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the... | Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang |  |
| 111 |  |  [Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP](https://openreview.net/forum?id=BkglSTNFDB) |  | 0 | A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \emph{without} accessing a generative model. We show that the \textit{sample complexity of exploration} of our algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$. This improves the previously best known result of $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and... | Kefan Dong, Liwei Wang, Xiaoyu Chen, Yuanhao Wang |  |
| 112 |  |  [Deep Network Classification by Scattering and Homotopy Dictionary Learning](https://openreview.net/forum?id=SJxWS64FwH) |  | 0 | We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A... | John Zarka, Louis Thiry, Stéphane Mallat, Tomás Angles |  |
| 113 |  |  [Data-Independent Neural Pruning via Coresets](https://openreview.net/forum?id=H1gmHaEKwB) |  | 0 | Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a... | Ben Mussay, Dan Feldman, Margarita Osadchy, Samson Zhou, Vladimir Braverman |  |
| 114 |  |  [Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks](https://openreview.net/forum?id=BkgXHTNtvS) |  | 0 | We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let $m$ and $d$ be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size $n\geq m+2d-2$. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for $n\le m$, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp... | Arsalan SharifNassab, S. Jamaloddin Golestani, Saber Salehkaleybar |  |
| 115 |  |  [Novelty Detection Via Blurring](https://openreview.net/forum?id=ByeNra4FDB) |  | 0 | Conventional out-of-distribution (OOD) detection schemes based on variational autoencoder or Random Network Distillation (RND) are known to assign lower uncertainty to the OOD data than the target distribution. In this work, we discover that such conventional novelty detection schemes are also vulnerable to the blurred images. Based on the observation, we construct a novel RND-based OOD detector, SVD-RND, that utilizes blurred images during training. Our detector is simple, efficient in test time, and outperforms baseline OOD detectors in various domains. Further results show that SVD-RND learns a better target distribution representation than the baselines. Finally, SVD-RND combined with geometric transform achieves near-perfect... | SaeYoung Chung, SungIk Choi |  |
| 116 |  |  [Piecewise linear activations substantially shape the loss surfaces of neural networks](https://openreview.net/forum?id=B1x6BTEKwr) |  | 0 | Understanding the loss surface of a neural network is fundamentally important to the understanding of deep learning. This paper presents how piecewise linear activation functions substantially shape the loss surfaces of neural networks. We first prove that {\it the loss surfaces of many neural networks have infinite spurious local minima} which are defined as the local minima with higher empirical risks than the global minima. Our result demonstrates that the networks with piecewise linear activations possess substantial differences to the well-studied linear neural networks. This result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss... | Bohan Wang, Dacheng Tao, Fengxiang He |  |
| 117 |  |  [Relational State-Space Model for Stochastic Multi-Object Systems](https://openreview.net/forum?id=B1lGU64tDr) |  | 0 | Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By letting GNNs cooperate with SSM, R-SSM provides a flexible way to incorporate relational information into the modeling of multi-object dynamics. We further suggest augmenting the model with normalizing flows... | Fan Yang, Fan Zhou, Ling Chen, Wei Cao, Yusong Gao |  |
| 118 |  |  [Learning Efficient Parameter Server Synchronization Policies for Distributed SGD](https://openreview.net/forum?id=rJxX8T4Kvr) |  | 0 | We apply a reinforcement learning (RL) based approach to learning optimal synchronization policies used for Parameter Server-based distributed training of machine learning models with Stochastic Gradient Descent (SGD). Utilizing a formal synchronization policy description in the PS-setting, we are able to derive a suitable and compact description of states and actions, allowing us to efficiently use the standard off-the-shelf deep Q-learning algorithm. As a result, we are able to learn synchronization policies which generalize to different cluster environments, different training datasets and small model variations and (most importantly) lead to considerable decreases in training time when compared to standard policies such as bulk... | Andreas Pfadler, Jingren Zhou, Rong Zhu, Sheng Yang, Zhengping Qian |  |
| 119 |  |  [Action Semantics Network: Considering the Effects of Actions in Multiagent Systems](https://openreview.net/forum?id=ryg48p4tPH) |  | 0 | In multiagent systems (MASs), each agent makes individual decisions but all of them contribute globally to the system evolution. Learning in MASs is difficult since each agent's selection of actions must take place in the presence of other co-learning agents. Moreover, the environmental stochasticity and uncertainties increase exponentially with the increase in the number of agents. Previous works borrow various multiagent coordination mechanisms into deep learning architecture to facilitate multiagent coordination. However, none of them explicitly consider action semantics between agents that different actions have different influences on other agents. In this paper, we propose a novel network architecture, named Action Semantics... | Changjie Fan, Jianye Hao, Tianpei Yang, Weixun Wang, Xiaotian Hao, Yang Gao, Yingfeng Chen, Yong Liu, Yujing Hu |  |
| 120 |  |  [Vid2Game: Controllable Characters Extracted from Real-World Videos](https://openreview.net/forum?id=SkxBUpEKwH) |  | 0 | We extract a controllable model from a video of a person performing a certain activity. The model generates novel image sequences of that person, according to user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first maps a current pose, and a single-instance control signal to the next pose. The second maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of... | Lior Wolf, Oran Gafni, Yaniv Taigman |  |
| 121 |  |  [Self-Adversarial Learning with Comparative Discrimination for Text Generation](https://openreview.net/forum?id=B1l8L6EtDS) |  | 0 | Conventional Generative Adversarial Networks (GANs) for text generation tend to have issues of reward sparsity and mode collapse that affect the quality and diversity of generated samples. To address the issues, we propose a novel self-adversarial learning (SAL) paradigm for improving GANs' performance in text generation. In contrast to standard GANs that use a binary classifier as its discriminator to predict whether a sample is real or generated, SAL employs a comparative discriminator which is a pairwise classifier for comparing the text quality between a pair of samples. During training, SAL rewards the generator when its currently generated sentence is found to be better than its previously generated samples. This... | Furu Wei, Ke Xu, Ming Zhou, Tao Ge, Wangchunshu Zhou |  |
| 122 |  |  [Robust training with ensemble consensus](https://openreview.net/forum?id=ryxOUTVYDH) |  | 0 | Since deep neural networks are over-parameterized, they can memorize noisy examples. We address such a memorization issue in the presence of label noise. From the fact that deep neural networks cannot generalize to neighborhoods of memorized features, we hypothesize that noisy examples do not consistently incur small losses on the network under a certain perturbation. Based on this, we propose a novel training method called Learning with Ensemble Consensus (LEC) that prevents overfitting to noisy examples by removing them based on the consensus of an ensemble of perturbed networks. One of the proposed LECs, LTEC outperforms the current state-of-the-art methods on noisy MNIST, CIFAR-10, and CIFAR-100 in an efficient manner. | Jisoo Lee, SaeYoung Chung |  |
| 123 |  |  [Identifying through Flows for Recovering Latent Representations](https://openreview.net/forum?id=SklOUpEYvB) |  | 0 | Identifiability, or recovery of the true latent representations from which the observed data originates, is de facto a fundamental goal of representation learning. Yet, most deep generative models do not address the question of identifiability, and thus fail to deliver on the promise of the recovery of the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. Due to the intractablity of KL divergence between variational approximate posterior and the true posterior, however, iVAE has to maximize the evidence lower bound (ELBO) of the marginal likelihood, leading to suboptimal solutions in both theory and... | Bryan Hooi, Gim Hee Lee, Shen Li |  |
| 124 |  |  [Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing](https://openreview.net/forum?id=BkeWw6VFwr) |  | 0 | It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\ell_2$ norm for top-$k$... | Binghui Wang, Jinyuan Jia, Neil Zhenqiang Gong, Xiaoyu Cao |  |
| 125 |  |  [Optimistic Exploration even with a Pessimistic Initialisation](https://openreview.net/forum?id=r1xGP6VYwH) |  | 0 | Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning (RL). In the tabular case, all provably efficient model-free algorithms rely on it. However, model-free deep RL algorithms do not use optimistic initialisation despite taking inspiration from these provably efficient tabular algorithms. In particular, in scenarios with only positive rewards, Q-values are initialised at their lowest possible values due to commonly used network initialisation schemes, a pessimistic initialisation. Merely initialising the network to output optimistic Q-values is not enough, since we cannot ensure that they remain optimistic for novel state-action pairs, which is crucial for exploration. We propose a... | Bei Peng, Shimon Whiteson, Tabish Rashid, Wendelin Boehmer |  |
| 126 |  |  [VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://openreview.net/forum?id=SygXPaEYvH) |  | 0 | We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the... | Bin Li, Furu Wei, Jifeng Dai, Lewei Lu, Weijie Su, Xizhou Zhu, Yue Cao |  |
| 127 |  |  [Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation](https://openreview.net/forum?id=SkxSv6VFvS) |  | 0 | Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -- what really matters to the network is the \*effective\* receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to... | Hang Gao, Jifeng Dai, Stephen Lin, Xizhou Zhu |  |
| 128 |  |  [Ensemble Distribution Distillation](https://openreview.net/forum?id=BygSP6Vtvr) |  | 0 | Ensembles of models often yield improvements in system performance. These ensemble approaches have also been empirically shown to yield robust measures of uncertainty, and are capable of distinguishing between different forms of uncertainty. However, ensembles come at a computational and memory cost which may be prohibitive for many applications. There has been significant work done on the distillation of an ensemble into a single model. Such approaches decrease computational cost and allow a single model to achieve an accuracy comparable to that of an ensemble. However, information about the diversity of the ensemble, which can yield estimates of different forms of uncertainty, is lost. This work considers the novel task of... | Andrey Malinin, Bruno Mlodozeniec, Mark J. F. Gales |  |
| 129 |  |  [Gap-Aware Mitigation of Gradient Staleness](https://openreview.net/forum?id=B1lLw6EYwB) |  | 0 | Cloud computing is becoming increasingly popular as a platform for distributed training of deep neural networks. Synchronous stochastic gradient descent (SSGD) suffers from substantial slowdowns due to stragglers if the environment is non-dedicated, as is common in cloud computing. Asynchronous SGD (ASGD) methods are immune to these slowdowns but are scarcely used due to gradient staleness, which encumbers the convergence process. Recent techniques have had limited success mitigating the gradient staleness when scaling up to many workers (computing nodes). In this paper we define the Gap as a measure of gradient staleness and propose Gap-Aware (GA), a novel asynchronous-distributed method that penalizes stale gradients linearly to... | Assaf Schuster, Ido Hakimi, Saar Barkai |  |
| 130 |  |  [Counterfactuals uncover the modular structure of deep generative models](https://openreview.net/forum?id=SJxDDpEKvH) |  | 0 | Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to \textit{disentangle} latent factors, we argue that such requirement can be advantageously relaxed and propose instead a non-statistical framework that relies on identifying a modular organization of the network, based on counterfactual manipulations. Our experiments support that modularity between groups of channels is achieved to a certain degree on a variety... | Arash Mehrjou, Bernhard Schölkopf, Michel Besserve, Rémy Sun |  |
| 131 |  |  [Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video](https://openreview.net/forum?id=BJeKwTNFvB) |  | 0 | We propose a model that is able to perform physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a \textit{physics-as-inverse-graphics} approach that brings together vision-as-inverse-graphics and differentiable physics engines, where objects and explicit state and velocity representations are discovered by the model. This framework allows us to perform long term extrapolative video prediction, as... | Michael Burke, Miguel Jaques, Timothy M. Hospedales |  |
| 132 |  |  [An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality](https://openreview.net/forum?id=HJeiDpVFPr) |  | 0 | Distances are pervasive in machine learning. They serve as similarity measures, loss functions, and learning targets; it is said that a good distance measure solves a task. When defining distances, the triangle inequality has proven to be a useful constraint, both theoretically---to prove convergence and optimality guarantees---and empirically---as an inductive bias. Deep metric learning architectures that respect the triangle inequality rely, almost exclusively, on Euclidean distance in the latent space. Though effective, this fails to model two broad classes of subadditive distances, common in graphs and reinforcement learning: asymmetric metrics, and metrics that cannot be embedded into Euclidean space. To address these... | Harris Chan, Jimmy Ba, Kiarash Jamali, Silviu Pitis |  |
| 133 |  |  [A Constructive Prediction of the Generalization Error Across Scales](https://openreview.net/forum?id=ryenvpEKDr) |  | 0 | The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We... | Amir Rosenfeld, Jonathan S. Rosenfeld, Nir Shavit, Yonatan Belinkov |  |
| 134 |  |  [Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base](https://openreview.net/forum?id=BJlguT4YPr) |  | 0 | We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB. This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations. The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from... | Haitian Sun, Matthew Siegler, R. Alex Hofer, William W. Cohen |  |
| 135 |  |  [CLN2INV: Learning Loop Invariants with Continuous Logic Networks](https://openreview.net/forum?id=HJlfuTEtvB) |  | 0 | Program verification offers a framework for ensuring program correctness and therefore systematically eliminating different classes of bugs. Inferring loop invariants is one of the main challenges behind automated verification of real-world programs which often contain many loops. In this paper, we present the Continuous Logic Network (CLN), a novel neural architecture for automatically learning loop invariants directly from program execution traces. Unlike existing neural networks, CLNs can learn precise and explicit representations of formulas in Satisfiability Modulo Theories (SMT) for loop invariants from program execution traces. We develop a new sound and complete semantic mapping for assigning SMT formulas to continuous... | Gabriel Ryan, Jianan Yao, Justin Wong, Ronghui Gu, Suman Jana |  |
| 136 |  |  [NAS evaluation is frustratingly hard](https://openreview.net/forum?id=HygrdpVKvr) |  | 0 | Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s... | Antoine Yang, Fabio Maria Carlucci, Pedro M. Esperança |  |
| 137 |  |  [Efficient and Information-Preserving Future Frame Prediction and Beyond](https://openreview.net/forum?id=B1eY_pVYvB) |  | 0 | Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational efficiency. The lightweight nature of our model enables us to incorporate 3D convolutions without concern of memory bottleneck, enhancing the model's ability to capture both short-term and long-term temporal... | Sanja Fidler, Steve Easterbrook, Wei Yu, Yichao Lu |  |
| 138 |  |  [Order Learning and Its Application to Age Estimation](https://openreview.net/forum?id=HygsuaNFwr) |  | 0 | We propose order learning to determine the order graph of classes, representing ranks or priorities, and classify an object instance into one of the classes. To this end, we design a pairwise comparator to categorize the relationship between two instances into one of three cases: one instance is \`greater than,' \`similar to,' or \`smaller than' the other. Then, by comparing an input instance with reference instances and maximizing the consistency among the comparison results, the class of the input can be estimated reliably. We apply order learning to develop a facial age estimator, which provides the state-of-the-art performance. Moreover, the performance is further improved when the order graph is divided into disjoint chains... | ChangSu Kim, Kyungsun Lim, NyeongHo Shin, YoungYoon Lee |  |
| 139 |  |  [ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning](https://openreview.net/forum?id=HJgJtT4tvB) |  | 0 | Recent powerful pre-trained language models have achieved remarkable performance on most of the popular datasets for reading comprehension. It is time to introduce more challenging datasets to push the development of this field towards more comprehensive reasoning of text. In this paper, we introduce a new Reading Comprehension dataset requiring logical reasoning (ReClor) extracted from standardized graduate admission examinations. As earlier studies suggest, human-annotated datasets usually contain biases, which are often exploited by models to achieve high accuracy without truly understanding the text. In order to comprehensively evaluate the logical reasoning ability of models on ReClor, we propose to identify biased data points... | Jiashi Feng, Weihao Yu, Yanfei Dong, Zihang Jiang |  |
| 140 |  |  [AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures](https://openreview.net/forum?id=SJgMK64Ywr) |  | 0 | Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning.... | A. J. Piergiovanni, Anelia Angelova, Michael S. Ryoo, Mingxing Tan |  |
| 141 |  |  [Adversarially Robust Representations with Smooth Encoders](https://openreview.net/forum?id=H1gfFaEYDS) |  | 0 | This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to... | A. Taylan Cemgil, Krishnamurthy (Dj) Dvijotham, Pushmeet Kohli, Sumedh Ghaisas |  |
| 142 |  |  [From Variational to Deterministic Autoencoders](https://openreview.net/forum?id=S1g7tpEYDS) |  | 0 | Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without... | Antonio Vergari, Bernhard Schölkopf, Mehdi S. M. Sajjadi, Michael J. Black, Partha Ghosh |  |
| 143 |  |  [Computation Reallocation for Object Detection](https://openreview.net/forum?id=SkxLFaNKwB) |  | 0 | The allocation of computation resources in the backbone is a crucial issue in object detection. However, classification allocation pattern is usually adopted directly to object detector, which is proved to be sub-optimal. In order to reallocate the engaged computation resources in a more efficient way, we present CR-NAS (Computation Reallocation Neural Architecture Search) that can learn computation reallocation strategies across different feature resolution and spatial position diectly on the target detection dataset. A two-level reallocation space is proposed for both stage and spatial reallocation. A novel hierarchical search procedure is adopted to cope with the complex search space. We apply CR-NAS to multiple backbones and... | Chen Lin, Feng Liang, Junjie Yan, Ming Sun, Ronghao Guo, Wanli Ouyang, Wei Wu |  |
| 144 |  |  [Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://openreview.net/forum?id=rylvYaNYDH) |  | 0 | As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can... | Christian Rupprecht, Christopher J. Pal, Cyril Ibrahim |  |
| 145 |  |  [A Fair Comparison of Graph Neural Networks for Graph Classification](https://openreview.net/forum?id=HygDF6NFPB) |  | 0 | Experimental reproducibility and replicability are critical topics in machine learning. Authors have often raised concerns about their lack in scientific publications to improve the quality of the field. Recently, the graph representation learning field has attracted the attention of a wide research community, which resulted in a large stream of works. As such, several Graph Neural Network models have been developed to effectively tackle graph classification. However, experimental procedures often lack rigorousness and are hardly reproducible. Motivated by this, we provide an overview of common practices that should be avoided to fairly compare with the state of the art. To counter this troubling trend, we ran more than 47000... | Alessio Micheli, Davide Bacciu, Federico Errica, Marco Podda |  |
| 146 |  |  [Generalization bounds for deep convolutional neural networks](https://openreview.net/forum?id=r1e_FpNFDr) |  | 0 | We prove bounds on the generalization error of convolutional networks. The bounds are in terms of the training loss, the number of parameters, the Lipschitz constant of the loss and the distance from the weights to the initial weights. They are independent of the number of pixels in the input, and the height and width of hidden feature maps. We present experiments using CIFAR-10 with varying hyperparameters of a deep convolutional network, comparing our bounds with practical generalization gaps. | Hanie Sedghi, Philip M. Long |  |
| 147 |  |  [SAdam: A Variant of Adam for Strongly Convex Functions](https://openreview.net/forum?id=rye5YaEtPr) |  | 0 | The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed... | Guanghui Wang, Lijun Zhang, Quan Cheng, Shiyin Lu, Weiwei Tu |  |
| 148 |  |  [Continual Learning with Bayesian Neural Networks for Non-Stationary Data](https://openreview.net/forum?id=SJlsFpVtDB) |  | 0 | This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes. We represent the posterior approximation of the network weights by a diagonal Gaussian distribution and a complementary memory of raw data. This raw data corresponds to likelihood terms that cannot be well approximated by the Gaussian. We introduce a novel method for sequentially updating both components of the posterior approximation. Furthermore, we propose Bayesian forgetting and a Gaussian diffusion process for adapting to non-stationary data. The experimental results show that our update method improves on existing approaches for streaming data. Additionally, the adaptation methods lead to... | Alexej Klushyn, Botond Cseke, Patrick van der Smagt, Richard Kurle, Stephan Günnemann |  |
| 149 |  |  [Multiplicative Interactions and Where to Find Them](https://openreview.net/forum?id=rylnK6VtDH) |  | 0 | We explore the role of multiplicative interaction as a unifying framework to describe a range of classical and modern neural network architectural motifs, such as gating, attention layers, hypernetworks, and dynamic convolutions amongst others. Multiplicative interaction layers as primitive operations have a long-established presence in the literature, though this often not emphasized and thus under-appreciated. We begin by showing that such layers strictly enrich the representable function classes of neural networks. We conjecture that multiplicative interactions offer a particularly powerful inductive bias when fusing multiple streams of information or when conditional computation is required. We therefore argue that they should... | Jack W. Rae, Jacob Menick, Jonathan Schwarz, Razvan Pascanu, Siddhant M. Jayakumar, Simon Osindero, Tim Harley, Wojciech M. Czarnecki, Yee Whye Teh |  |
| 150 |  |  [Few-Shot Learning on graphs via super-Classes based on Graph spectral Measures](https://openreview.net/forum?id=Bkeeca4Kvr) |  | 0 | We propose to study the problem of few-shot graph classification in graph neural networks (GNNs) to recognize unseen classes, given limited labeled graph examples. Despite several interesting GNN variants being proposed recently for node and graph classification tasks, when faced with scarce labeled examples in the few-shot setting, these GNNs exhibit significant loss in classification performance. Here, we present an approach where a probability measure is assigned to each graph based on the spectrum of the graph’s normalized Laplacian. This enables us to accordingly cluster the graph base-labels associated with each graph into super-classes, where the L^p Wasserstein distance serves as our underlying distance metric.... | Deepak Nathani, Jatin Chauhan, Manohar Kaul |  |
| 151 |  |  [On Computation and Generalization of Generative Adversarial Imitation Learning](https://openreview.net/forum?id=BJl-5pNKDB) |  | 0 | Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL... | Minshuo Chen, Tianyi Liu, Tuo Zhao, Xingguo Li, Yizhou Wang, Zhaoran Wang, Zhuoran Yang |  |
| 152 |  |  [A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning](https://openreview.net/forum?id=BylVcTNtDS) |  | 0 | Due to insufficient training data and the high computational cost to train a deep neural network from scratch, transfer learning has been extensively used in many deep-neural-network-based applications. A commonly used transfer learning approach involves taking a part of a pre-trained model, adding a few layers at the end, and re-training the new layers with a small dataset. This approach, while efficient and widely used, imposes a security vulnerability because the pre-trained model used in transfer learning is usually publicly available, including to potential attackers. In this paper, we show that without any additional knowledge other than the pre-trained model, an attacker can launch an effective and efficient brute force... | Shahbaz Rezaei, Xin Liu |  |
| 153 |  |  [Low-Resource Knowledge-Grounded Dialogue Generation](https://openreview.net/forum?id=rJeIcTNtvS) |  | 0 | Responding with knowledge has been recognized as an important capability for an intelligent conversational agent. Yet knowledge-grounded dialogues, as training data for learning such a response generation model, are difficult to obtain. Motivated by the challenge in practice, we consider knowledge-grounded dialogue generation under a natural assumption that only limited training examples are available. In such a low-resource setting, we devise a disentangled response decoder in order to isolate parameters that depend on knowledge-grounded dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of ungrounded dialogues and unstructured documents, while the remaining... | Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan, Wei Wu, Xueliang Zhao |  |
| 154 |  |  [Deep 3D Pan via local adaptive "t-shaped" convolutions with global and local adaptive dilations](https://openreview.net/forum?id=B1gF56VYPH) |  | 0 | Recent advances in deep learning have shown promising results in many low-level vision tasks. However, solving the single-image-based view synthesis is still an open problem. In particular, the generation of new images at parallel camera views given a single input image is of great interest, as it enables 3D visualization of the 2D input scenery. We propose a novel network architecture to perform stereoscopic view synthesis at arbitrary camera positions along the X-axis, or “Deep 3D Pan”, with “t-shaped” adaptive kernels equipped with globally and locally adaptive dilations. Our proposed network architecture, the monster-net, is devised with a novel t-shaped adaptive kernel with globally and locally adaptive dilation, which can... | Juan Luis Gonzalez Bello, Munchurl Kim |  |
| 155 |  |  [Tree-Structured Attention with Hierarchical Accumulation](https://openreview.net/forum?id=HJxK5pEYvr) |  | 0 | Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over... | Richard Socher, Shafiq R. Joty, Steven C. H. Hoi, XuanPhi Nguyen |  |
| 156 |  |  [The asymptotic spectrum of the Hessian of DNN throughout training](https://openreview.net/forum?id=SkgscaNYPS) |  | 0 | The dynamics of DNNs during gradient descent is described by the so-called Neural Tangent Kernel (NTK). In this article, we show that the NTK allows one to gain precise insight into the Hessian of the cost of DNNs: we obtain a full characterization of the asymptotics of the spectrum of the Hessian, at initialization and during training. | Arthur Jacot, Clément Hongler, Franck Gabriel |  |
| 157 |  |  [Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games](https://openreview.net/forum?id=H1lhqpEYPr) |  | 0 | We study discrete-time mean-field Markov games with infinite numbers of agents where each agent aims to minimize its ergodic cost. We consider the setting where the agents have identical linear state transitions and quadratic cost func- tions, while the aggregated effect of the agents is captured by the population mean of their states, namely, the mean-field state. For such a game, based on the Nash certainty equivalence principle, we provide sufficient conditions for the existence and uniqueness of its Nash equilibrium. Moreover, to find the Nash equilibrium, we propose a mean-field actor-critic algorithm with linear function approxima- tion, which does not require knowing the model of dynamics. Specifically, at each iteration of... | Yongxin Chen, Zhaoran Wang, Zhuoran Yang, Zuyue Fu |  |
| 158 |  |  [In Search for a SAT-friendly Binarized Neural Network Architecture](https://openreview.net/forum?id=SJx-j64FDr) |  | 0 | Analyzing the behavior of neural networks is one of the most pressing challenges in deep learning. Binarized Neural Networks are an important class of networks that allow equivalent representation in Boolean logic and can be analyzed formally with logic-based reasoning tools like SAT solvers. Such tools can be used to answer existential and probabilistic queries about the network, perform explanation generation, etc. However, the main bottleneck for all methods is their ability to reason about large BNNs efficiently. In this work, we analyze architectural design choices of BNNs and discuss how they affect the performance of logic-based reasoners. We propose changes to the BNN architecture and the training procedure to get a simpler... | Aarti Gupta, Hongce Zhang, Nina Narodytska, Toby Walsh |  |
| 159 |  |  [Generative Ratio Matching Networks](https://openreview.net/forum?id=SJg7spEYDS) |  | 0 | Deep generative models can learn to generate realistic-looking images, but many of the most effective methods are adversarial and involve a saddlepoint optimization, which requires a careful balancing of training between a generator network and a critic network. Maximum mean discrepancy networks (MMD-nets) avoid this issue by using kernel as a fixed adversary, but unfortunately, they have not on their own been able to match the generative quality of adversarial training. In this work, we take their insight of using kernels as fixed adversaries further and present a novel method for training deep generative models that does not involve saddlepoint optimization. We call our method generative ratio matching or GRAM for short. In GRAM,... | Akash Srivastava, Charles Sutton, Kai Xu, Michael U. Gutmann |  |
| 160 |  |  [Learning to Represent Programs with Property Signatures](https://openreview.net/forum?id=rylHspEKPr) |  | 0 | We introduce the notion of property signatures, a representation for programs and program specifications meant for consumption by machine learning algorithms. Given a function with input type τ_in and output type τ_out, a property is a function of type: (τ_in, τ_out) → Bool that (informally) describes some simple property of the function under consideration. For instance, if τ_in and τ_out are both lists of the same type, one property might ask ‘is the input list the same length as the output list?’. If we have a list of such properties, we can evaluate them all for our function to get a list of outputs that we will call the property signature. Crucially, we can ‘guess’ the property signature for a function given only a set of... | Augustus Odena, Charles Sutton |  |
| 161 |  |  [V4D: 4D Convolutional Neural Networks for Video-level Representation Learning](https://openreview.net/forum?id=SJeLopEYDH) |  | 0 | Most existing 3D CNN structures for video representation learning are clip-based methods, and do not consider video-level temporal evolution of spatio-temporal features. In this paper, we propose Video-level 4D Convolutional Neural Networks, namely V4D, to model the evolution of long-range spatio-temporal representation with 4D convolutions, as well as preserving 3D spatio-temporal representations with residual connections. We further introduce the training and inference methods for the proposed V4D. Extensive experiments are conducted on three video recognition benchmarks, where V4D achieves excellent results, surpassing recent 3D CNNs by a large margin. | Limin Wang, Matthew R. Scott, Sheng Guo, Shiwen Zhang, Weilin Huang |  |
| 162 |  |  [Option Discovery using Deep Skill Chaining](https://openreview.net/forum?id=B1gqipNYwH) |  | 0 | Autonomously discovering temporally extended actions, or skills, is a longstanding goal of hierarchical reinforcement learning. We propose a new algorithm that combines skill chaining with deep neural networks to autonomously discover skills in high-dimensional, continuous domains. The resulting algorithm, deep skill chaining, constructs skills with the property that executing one enables the agent to execute another. We demonstrate that deep skill chaining significantly outperforms both non-hierarchical agents and other state-of-the-art skill discovery techniques in challenging continuous control tasks. | Akhil Bagaria, George Konidaris |  |
| 163 |  |  [Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations](https://openreview.net/forum?id=HyxG3p4twS) |  | 0 | Detection of photo manipulation relies on subtle statistical traces, notoriously removed by aggressive lossy compression employed online. We demonstrate that end-to-end modeling of complex photo dissemination channels allows for codec optimization with explicit provenance objectives. We design a lightweight trainable lossy image codec, that delivers competitive rate-distortion performance, on par with best hand-engineered alternatives, but has lower computational footprint on modern GPU-enabled platforms. Our results show that significant improvements in manipulation detection accuracy are possible at fractional costs in bandwidth/storage. Our codec improved the accuracy from 37% to 86% even at very low bit-rates, well below the... | Nasir D. Memon, Pawel Korus |  |
| 164 |  |  [On the Variance of the Adaptive Learning Rate and Beyond](https://openreview.net/forum?id=rkgz2aEKDr) |  | 0 | The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation... | Haoming Jiang, Jianfeng Gao, Jiawei Han, Liyuan Liu, Pengcheng He, Weizhu Chen, Xiaodong Liu |  |
| 165 |  |  [Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery](https://openreview.net/forum?id=H1lmhaVtvr) |  | 0 | Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new... | Kristian Hartikainen, Sergey Levine, Tuomas Haarnoja, Xinyang Geng |  |
| 166 |  |  [A Theoretical Analysis of the Number of Shots in Few-Shot Learning](https://openreview.net/forum?id=HkgB2TNYPS) |  | 0 | Few-shot classification is the task of predicting the category of an example from a set of few labeled examples. The number of labeled examples per category is called the number of shots (or shot number). Recent works tackle this task through meta-learning, where a meta-learner extracts information from observed tasks during meta-training to quickly adapt to new tasks during meta-testing. In this formulation, the number of shots exploited during meta-training has an impact on the recognition performance at meta-test time. Generally, the shot number used in meta-training should match the one used in meta-testing to obtain the best performance. We introduce a theoretical analysis of the impact of the shot number on Prototypical... | Marc T. Law, Sanja Fidler, Tianshi Cao |  |
| 167 |  |  [Unsupervised Model Selection for Variational Disentangled Representation Learning](https://openreview.net/forum?id=SyxL2TNtvr) |  | 0 | Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the... | Alexander Lerchner, Andre Saraiva, Chris Burgess, Irina Higgins, Loic Matthey, Nick Watters, Sunny Duan |  |
| 168 |  |  [Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection](https://openreview.net/forum?id=BkgnhTEtDS) |  | 0 | Recommendation is a prevalent application of machine learning that affects many users; therefore, it is important for recommender models to be accurate and interpretable. In this work, we propose a method to both interpret and augment the predictions of black-box recommender systems. In particular, we propose to interpret feature interactions from a source recommender model and explicitly encode these interactions in a target recommender model, where both source and target models are black-boxes. By not assuming the structure of the recommender system, our approach can be used in general settings. In our experiments, we focus on a prominent use of machine learning recommendation: ad-click prediction. We found that our interaction... | Dehua Cheng, Eric Zhou, Hanpeng Liu, Michael Tsang, Xue Feng, Yan Liu |  |
| 169 |  |  [Understanding the Limitations of Variational Mutual Information Estimators](https://openreview.net/forum?id=B1x62TNtDS) |  | 0 | Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark... | Jiaming Song, Stefano Ermon |  |
| 170 |  |  [GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations](https://openreview.net/forum?id=BkxfaTVFwH) |  | 0 | Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components.... | Adam R. Kosiorek, Ingmar Posner, Martin Engelcke, Oiwi Parker Jones |  |
| 171 |  |  [Language GANs Falling Short](https://openreview.net/forum?id=BJgza6VtPB) |  | 0 | Traditional natural language generation (NLG) models are trained using maximum likelihood estimation (MLE) which differs from the sample generation inference procedure. During training the ground truth tokens are passed to the model, however, during inference, the model instead reads its previously generated samples - a phenomenon coined exposure bias. Exposure bias was hypothesized to be a root cause of poor sample quality and thus many generative adversarial networks (GANs) were proposed as a remedy since they have identical training and inference. However, many of the ensuing GAN variants validated sample quality improvements but ignored loss of sample diversity. This work reiterates the fallacy of quality-only metrics and... | Hugo Larochelle, Joelle Pineau, Laurent Charlin, Lucas Caccia, Massimo Caccia, William Fedus |  |
| 172 |  |  [Stochastic Conditional Generative Networks with Basis Decomposition](https://openreview.net/forum?id=S1lSapVtwS) |  | 0 | While generative adversarial networks (GANs) have revolutionized machine learning, a number of open questions remain to fully understand them and exploit their power. One of these questions is how to efficiently achieve proper diversity and sampling of the multi-mode data space. To address this, we introduce BasisGAN, a stochastic conditional multi-mode image generator. By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of basis elements, we learn a plug-and-played basis generator to stochastically generate basis elements, with just a few hundred of parameters, to fully embed stochasticity into convolutional filters. By sampling basis elements instead of... | Guillermo Sapiro, Qiang Qiu, Xiuyuan Cheng, Ze Wang |  |
| 173 |  |  [Learned Step Size quantization](https://openreview.net/forum?id=rkgO66VKDS) |  | 0 | Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce... | Deepika Bablani, Dharmendra S. Modha, Jeffrey L. McKinstry, Rathinakumar Appuswamy, Steven K. Esser |  |
| 174 |  |  [On the "steerability" of generative adversarial networks](https://openreview.net/forum?id=HylsTT4FvB) |  | 0 | An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise -- these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and... | Ali Jahanian, Lucy Chai, Phillip Isola |  |
| 175 |  |  [Reinforced active learning for image segmentation](https://openreview.net/forum?id=SkgC6TNFvr) |  | 0 | Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire... | Arantxa Casanova, Christopher J. Pal, Negar Rostamzadeh, Pedro O. Pinheiro |  |
| 176 |  |  [Sign Bits Are All You Need for Black-Box Attacks](https://openreview.net/forum?id=SygW0TEFwH) |  | 0 | We present a novel black-box adversarial attack algorithm with state-of-the-art model evasion rates for query efficiency under $\ell_\infty$ and $\ell_2$ metrics. It exploits a \textit{sign-based}, rather than magnitude-based, gradient estimation approach that shifts the gradient estimation from continuous to binary black-box optimization. It adaptively constructs queries to estimate the gradient, one query relying upon the previous, rather than re-estimating the gradient each step with random query construction. Its reliance on sign bits yields a smaller memory footprint and it requires neither hyperparameter tuning or dimensionality reduction. Further, its theoretical performance is guaranteed and it can characterize adversarial... | Abdullah AlDujaili, UnaMay O'Reilly |  |
| 177 |  |  [Deep Semi-Supervised Anomaly Detection](https://openreview.net/forum?id=HkgH0TEYwH) |  | 0 | Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning problem. In practice however, one may have---in addition to a large set of unlabeled samples---access to a small pool of labeled samples, e.g. a subset verified by some domain expert as being normal or anomalous. Semi-supervised approaches to anomaly detection aim to utilize such labeled samples, but most proposed methods are limited to merely including labeled normal samples. Only a few methods take advantage of labeled anomalies, with existing deep approaches being domain-specific. In this work we present Deep SAD, an end-to-end deep... | Alexander Binder, Emmanuel Müller, KlausRobert Müller, Lukas Ruff, Marius Kloft, Nico Görnitz, Robert A. Vandermeulen |  |
| 178 |  |  [Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints](https://openreview.net/forum?id=HyxLRTVKPH) |  | 0 | In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: "given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?" We focus on the number of optimization iterations as the representative resource. Under such a setting, we show... | Deva Ramanan, Ersin Yumer, Mengtian Li |  |
| 179 |  |  [Minimizing FLOPs to Learn Efficient Sparse Representations](https://openreview.net/forum?id=SygpC6Ntvr) |  | 0 | Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key... | Barnabás Póczos, Biswajit Paria, ChihKuan Yeh, Ian EnHsu Yen, Ning Xu, Pradeep Ravikumar |  |
| 180 |  |  [Reanalysis of Variance Reduced Temporal Difference Learning](https://openreview.net/forum?id=S1ly10EKDS) |  | 0 | Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by \cite{korda2015td}, which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in \cite{korda2015td}, and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate.... | Tengyu Xu, Yi Zhou, Yingbin Liang, Zhe Wang |  |
| 181 |  |  [Imitation Learning via Off-Policy Distribution Matching](https://openreview.net/forum?id=Hyg-JC4FDr) |  | 0 | When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data- inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show... | Ilya Kostrikov, Jonathan Tompson, Ofir Nachum |  |
| 182 |  |  [Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML](https://openreview.net/forum?id=rkgMkCEtPB) |  | 0 | An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and... | Aniruddh Raghu, Maithra Raghu, Oriol Vinyals, Samy Bengio |  |
| 183 |  |  [Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space](https://openreview.net/forum?id=H1lmyRNFvr) |  | 0 | Challenges in natural sciences can often be phrased as optimization problems. Machine learning techniques have recently been applied to solve such problems. One example in chemistry is the design of tailor-made organic materials and molecules, which requires efficient methods to explore the chemical space. We present a genetic algorithm (GA) that is enhanced with a neural network (DNN) based discriminator model to improve the diversity of generated molecules and at the same time steer the GA. We show that our algorithm outperforms other generative models in optimization tasks. We furthermore present a way to increase interpretability of genetic algorithms, which helped us to derive design principles | AkshatKumar Nigam, Alán AspuruGuzik, Mario Krenn, Pascal Friederich |  |
| 184 |  |  [Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin](https://openreview.net/forum?id=HJe_yR4Fwr) |  | 0 | For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound – a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the “all-layer margin.” Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds... | Colin Wei, Tengyu Ma |  |
| 185 |  |  [Identity Crisis: Memorization and Generalization Under Extreme Overparameterization](https://openreview.net/forum?id=B1l6y0VFPr) |  | 0 | We study the interplay between memorization and generalization of overparameterized networks in the extreme case of a single training example and an identity-mapping task. We examine fully-connected and convolutional networks (FCN and CNN), both linear and nonlinear, initialized randomly and then trained to minimize the reconstruction error. The trained networks stereotypically take one of two forms: the constant function (memorization) and the identity function (generalization). We formally characterize generalization in single-layer FCNs and CNNs. We show empirically that different architectures exhibit strikingly different inductive biases. For example, CNNs of up to 10 layers are able to generalize from a single example,... | Chiyuan Zhang, Michael C. Mozer, Moritz Hardt, Samy Bengio, Yoram Singer |  |
| 186 |  |  [ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring](https://openreview.net/forum?id=HklkeR4KPB) |  | 0 | We improve the recently-proposed \`\`MixMatch semi-supervised learning algorithm by introducing two new techniques: distribution alignment and augmentation anchoring. - Distribution alignment encourages the marginal distribution of predictions on unlabeled data to be close to the marginal distribution of ground-truth labels. - Augmentation anchoring} feeds multiple strongly augmented versions of an input into the model and encourages each output to be close to the prediction for a weakly-augmented version of the same input. To produce strong augmentations, we propose a variant of AutoAugment which learns the augmentation policy while the model is being trained. Our new algorithm, dubbed ReMixMatch, is significantly more... | Alex Kurakin, Colin Raffel, David Berthelot, Ekin D. Cubuk, Han Zhang, Kihyuk Sohn, Nicholas Carlini |  |
| 187 |  |  [Adaptive Structural Fingerprints for Graph Attention Networks](https://openreview.net/forum?id=BJxWx0NYPr) |  | 0 | Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an \`\`"adaptive structural fingerprint" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to... | Jie Zhang, Jun Wang, Kai Zhang, Yaokang Zhu |  |
| 188 |  |  [CAQL: Continuous Action Q-Learning](https://openreview.net/forum?id=BkxXe0Etwr) |  | 0 | Reinforcement learning (RL) with value-based methods (e.g., Q-learning) has shown success in a variety of domains such as games and recommender systems (RSs). When the action space is finite, these algorithms implicitly finds a policy by learning the optimal value function, which are often very efficient. However, one major challenge of extending Q-learning to tackle continuous-action RL problems is that obtaining optimal Bellman backup requires solving a continuous action-maximization (max-Q) problem. While it is common to restrict the parameterization of the Q-function to be concave in actions to simplify the max-Q problem, such a restriction might lead to performance degradation. Alternatively, when the Q-function is... | Christian Tjandraatmadja, Craig Boutilier, Moonkyung Ryu, Ross Anderson, Yinlam Chow |  |
| 189 |  |  [Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning](https://openreview.net/forum?id=BJluxREKDB) |  | 0 | We demonstrate how to learn efficient heuristics for automated reasoning algorithms for quantified Boolean formulas through deep reinforcement learning. We focus on a backtracking search algorithm, which can already solve formulas of impressive size - up to hundreds of thousands of variables. The main challenge is to find a representation of these formulas that lends itself to making predictions in a scalable way. For a family of challenging problems, we learned a heuristic that solves significantly more formulas compared to the existing handwritten heuristics. | Edward A. Lee, Gil Lederman, Markus N. Rabe, Sanjit Seshia |  |
| 190 |  |  [Pure and Spurious Critical Points: a Geometric Study of Linear Networks](https://openreview.net/forum?id=rkgOlCVYvB) |  | 0 | The critical locus of the loss function of a neural network is determined by the geometry of the functional space and by the parameterization of this space by the network's weights. We introduce a natural distinction between pure critical points, which only depend on the functional space, and spurious critical points, which arise from the parameterization. We apply this perspective to revisit and extend the literature on the loss function of linear neural networks. For this type of network, the functional space is either the set of all linear maps from input to output space, or a determinantal variety, i.e., a set of linear maps with bounded rank. We use geometric properties of determinantal varieties to derive new results on the... | Joan Bruna, Kathlén Kohn, Matthew Trager |  |
| 191 |  |  [Neural Text Generation With Unlikelihood Training](https://openreview.net/forum?id=SJeYe0NtvH) |  | 0 | Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned... | Emily Dinan, Ilia Kulikov, Jason Weston, Kyunghyun Cho, Sean Welleck, Stephen Roller |  |
| 192 |  |  [Semi-Supervised Generative Modeling for Controllable Speech Synthesis](https://openreview.net/forum?id=rJeqeCEtvH) |  | 0 | We present a novel generative model that combines state-of-the-art neural text- to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn’t been possible with purely unsupervised methods. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. We will release audio samples at... | Daisy Stanton, David Kao, Eric Battenberg, Matt Shannon, R. J. SkerryRyan, Raza Habib, Soroosh Mariooryad, Tom Bagby |  |
| 193 |  |  [Dynamic Time Lag Regression: Predicting What & When](https://openreview.net/forum?id=SkxybANtDB) |  | 0 | This paper tackles a new regression problem, called Dynamic Time-Lag Regression (DTLR), where a cause signal drives an effect signal with an unknown time delay. The motivating application, pertaining to space weather modelling, aims to predict the near-Earth solar wind speed based on estimates of the Sun's coronal magnetic field. DTLR differs from mainstream regression and from sequence-to-sequence learning in two respects: firstly, no ground truth (e.g., pairs of associated sub-sequences) is available; secondly, the cause signal contains much information irrelevant to the effect signal (the solar magnetic field governs the solar wind propagation in the heliosphere, of which the Earth's magnetosphere is but a minuscule region). A... | Bala Poduval, Cyril Furtlehner, Enrico Camporeale, Mandar Chandorkar, Michèle Sebag |  |
| 194 |  |  [Scalable Model Compression by Entropy Penalized Reparameterization](https://openreview.net/forum?id=HkgxW0EYDS) |  | 0 | We describe a simple and general neural network weight compression approach, in which the network parameters (weights and biases) are represented in a “latent” space, amounting to a reparameterization. This space is equipped with a learned probability model, which is used to impose an entropy penalty on the parameter representation during training, and to compress the representation using a simple arithmetic coder after training. Classification accuracy and model compressibility is maximized jointly, with the bitrate–accuracy trade-off specified by a hyperparameter. We evaluate the method on the MNIST, CIFAR-10 and ImageNet classification benchmarks using six distinct model architectures. Our results show that state-of-the-art... | Abhinav Shrivastava, Deniz Oktay, Johannes Ballé, Saurabh Singh |  |
| 195 |  |  [AMRL: Aggregated Memory For Reinforcement Learning](https://openreview.net/forum?id=Bkl7bREtDr) |  | 0 | In many partially observable scenarios, Reinforcement Learning (RL) agents must rely on long-term memory in order to learn an optimal policy. We demonstrate that using techniques from NLP and supervised learning fails at RL tasks due to stochasticity from the environment and from exploration. Utilizing our insights on the limitations of traditional memory methods in RL, we propose AMRL, a class of models that can learn better policies with greater sample efficiency and are resilient to noisy inputs. Specifically, our models use a standard memory module to summarize short-term context, and then aggregate all prior states from the standard model without respect to order. We show that this provides advantages both in terms of gradient... | Cheng Zhang, Jacob Beck, Kamil Ciosek, Katja Hofmann, Sam Devlin, Sebastian Tschiatschek |  |
| 196 |  |  [Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform](https://openreview.net/forum?id=HJxV-ANKDH) |  | 0 | Strictly enforcing orthonormality constraints on parameter matrices has been shown advantageous in deep learning. This amounts to Riemannian optimization on the Stiefel manifold, which, however, is computationally expensive. To address this challenge, we present two main contributions: (1) A new efficient retraction map based on an iterative Cayley transform for optimization updates, and (2) An implicit vector transport mechanism based on the combination of a projection of the momentum and the Cayley transform on the Stiefel manifold. We specify two new optimization algorithms: Cayley SGD with momentum, and Cayley ADAM on the Stiefel manifold. Convergence of Cayley SGD is theoretically analyzed. Our experiments for CNN training... | Fuxin Li, Jun Li, Sinisa Todorovic |  |
| 197 |  |  [Unpaired Point Cloud Completion on Real Scans using Adversarial Training](https://openreview.net/forum?id=HkgrZ0EYwB) |  | 0 | As 3D scanning solutions become increasingly popular, several deep learning setups have been developed for the task of scan completion, i.e., plausibly filling in regions that were missed in the raw scans. These methods, however, largely rely on supervision in the form of paired training data, i.e., partial scans with corresponding desired completed scans. While these methods have been successfully demonstrated on synthetic data, the approaches cannot be directly used on real scans in absence of suitable paired training data. We develop a first approach that works directly on input point clouds, does not require paired training data, and hence can directly be applied to real scans for scan completion. We evaluate the approach... | Baoquan Chen, Niloy J. Mitra, Xuelin Chen |  |
| 198 |  |  [Adjustable Real-time Style Transfer](https://openreview.net/forum?id=HJe_Z04Yvr) |  | 0 | Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying \`\`optimum" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which... | Golnaz Ghiasi, Mohammad Babaeizadeh |  |
| 199 |  |  [Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well](https://openreview.net/forum?id=rygFWAEFwS) |  | 0 | We propose Stochastic Weight Averaging in Parallel (SWAP), an algorithm to accelerate DNN training. Our algorithm uses large mini-batches to compute an approximate solution quickly and then refines it by averaging the weights of multiple models computed independently and in parallel. The resulting models generalize equally well as those trained with small mini-batches but are produced in a substantially shorter time. We demonstrate the reduction in training time and the good generalization performance of the resulting models on the computer vision datasets CIFAR10, CIFAR100, and ImageNet. | Dennis DeCoste, Santiago Akle Serrano, Vipul Gupta |  |
| 200 |  |  [Short and Sparse Deconvolution - A Geometric Approach](https://openreview.net/forum?id=Byg5ZANtvH) |  | 0 | Short-and-sparse deconvolution (SaSD) is the problem of extracting localized, recurring motifs in signals with spatial or temporal structure. Variants of this problem arise in applications such as image deblurring, microscopy, neural spike sorting, and more. The problem is challenging in both theory and practice, as natural optimization formulations are nonconvex. Moreover, practical deconvolution problems involve smooth motifs (kernels) whose spectra decay rapidly, resulting in poor conditioning and numerical challenges. This paper is motivated by recent theoretical advances \citep{zhang2017global,kuo2019geometry}, which characterize the optimization landscape of a particular nonconvex formulation of SaSD. This is used to derive a... | HanWen Kuo, John Wright, Pengcheng Zhou, Qing Qu, Yenson Lau, Yuqian Zhang |  |
| 201 |  |  [Selection via Proxy: Efficient Data Selection for Deep Learning](https://openreview.net/forum?id=HJg2b0VYDr) |  | 0 | Data selection methods, such as active learning and core-set selection, are useful tools for machine learning on large datasets. However, they can be prohibitively expensive to apply in deep learning because they depend on feature representations that need to be learned. In this work, we show that we can greatly improve the computational efficiency by using a small proxy model to perform data selection (e.g., selecting data points to label for active learning). By removing hidden layers from the target model, using smaller architectures, and training for fewer epochs, we create proxies that are an order of magnitude faster to train. Although these small proxy models have higher error rates, we find that they empirically provide... | Baharan Mirzasoleiman, Christopher Yeh, Cody Coleman, Jure Leskovec, Matei Zaharia, Percy Liang, Peter Bailis, Stephen Mussmann |  |
| 202 |  |  [Global Relational Models of Source Code](https://openreview.net/forum?id=B1lnbRNtwr) |  | 0 | Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly structured representations of programs, such as trees, graphs and paths therein (e.g. data-flow relations), which are precise and abundantly available for code. This provides a strong inductive bias towards semantically meaningful relations, yielding more generalizable representations than classical sequence-based models. Unfortunately, these models primarily rely on graph-based message passing to represent relations in code, which makes them de facto local due to the high cost of message-passing steps, quite in contrast to modern, global... | Charles Sutton, David Bieber, Petros Maniatis, Rishabh Singh, Vincent J. Hellendoorn |  |
| 203 |  |  [Detecting Extrapolation with Local Ensembles](https://openreview.net/forum?id=BJl6bANtwH) |  | 0 | We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model. We focus on underdetermination as a key component of extrapolation: we aim to detect when many possible predictions are consistent with the training data and model class. Our method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. We compute this approximation by estimating the norm of the component of a test point's gradient that aligns with the low-curvature directions of the Hessian, and provide a tractable method for estimating this quantity. Experimentally, we show that our method is capable of detecting when a pre-trained model is extrapolating on... | Alexander D'Amour, David Madras, James Atwood |  |
| 204 |  |  [Learning to Link](https://openreview.net/forum?id=S1eRbANtDB) |  | 0 | Clustering is an important part of many modern data analysis pipelines, including network analysis and data retrieval. There are many different clustering algorithms developed by various communities, and it is often not clear which algorithm will give the best performance on a specific clustering task. Similarly, we often have multiple ways to measure distances between data points, and the best clustering performance might require a non-trivial combination of those metrics. In this work, we study data-driven algorithm selection and metric learning for clustering problems, where the goal is to simultaneously learn the best algorithm and metric for a specific application. The family of clustering algorithms we consider is... | Manuel Lang, MariaFlorina Balcan, Travis Dick |  |
| 205 |  |  [Adversarially robust transfer learning](https://openreview.net/forum?id=ryebG04YvB) |  | 0 | Transfer learning, in which a network is trained on one task and re-purposed on another, is often used to produce neural network classifiers when data is scarce or full-scale training is too costly. When the goal is to produce a model that is not only accurate but also adversarially robust, data scarcity and computational limitations become even more cumbersome. We consider robust transfer learning, in which we transfer not only performance but also robustness from a source model to a target domain. We start by observing that robust networks contain robust feature extractors. By training classifiers on top of these feature extractors, we produce new models that inherit the robustness of their parent networks. We then consider the... | Ali Shafahi, Amin Ghiasi, Chen Zhu, Christoph Studer, David W. Jacobs, Parsa Saadatpanah, Tom Goldstein |  |
| 206 |  |  [Overlearning Reveals Sensitive Attributes](https://openreview.net/forum?id=SJeNz04tDS) |  | 0 | \`\`"Overlearning'' means that a model trained for a seemingly simple objective implicitly learns to recognize attributes and concepts that are (1) not part of the learning objective, and (2) sensitive from a privacy or bias perspective. For example, a binary gender classifier of facial images also learns to recognize races, even races that are not represented in the training data, and identities. We demonstrate overlearning in several vision and NLP models and analyze its harmful consequences. First, inference-time representations of an overlearned model reveal sensitive attributes of the input, breaking privacy protections such as model partitioning. Second, an overlearned model can be "\`re-purposed'' for a different,... | Congzheng Song, Vitaly Shmatikov |  |
| 207 |  |  [Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness](https://openreview.net/forum?id=SJgwzCEKwH) |  | 0 | Mode connectivity provides novel geometric insights on analyzing loss landscapes and enables building high-accuracy pathways between well-trained neural networks. In this work, we propose to employ mode connectivity in loss landscapes to study the adversarial robustness of deep neural networks, and provide novel methods for improving this robustness. Our experiments cover various types of adversarial attacks applied to different network architectures and datasets. When network models are tampered with backdoor or error-injection attacks, our results demonstrate that the path connection learned using limited amount of bonafide data can effectively mitigate adversarial effects while maintaining the original accuracy on clean data.... | Karthikeyan Natesan Ramamurthy, Payel Das, PinYu Chen, Pu Zhao, Xue Lin |  |
| 208 |  |  [Differentially Private Meta-Learning](https://openreview.net/forum?id=rJgqMRVYvr) |  | 0 | Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, with personalization, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable... | Ameet Talwalkar, Jeffrey Li, Mikhail Khodak, Sebastian Caldas |  |
| 209 |  |  [One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation](https://openreview.net/forum?id=r1e9GCNKvH) |  | 0 | Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network... | Bradly C. Stadie, Matthew Shunshi Zhang |  |
| 210 |  |  [Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples](https://openreview.net/forum?id=rkgAGAVKPr) |  | 0 | Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models’ ability to leverage diverse training sources for improving their generalization. We also... | Carles Gelada, Eleni Triantafillou, Hugo Larochelle, Kelvin Xu, Kevin Swersky, Pascal Lamblin, PierreAntoine Manzagol, Ross Goroshin, Tyler Zhu, Utku Evci, Vincent Dumoulin |  |
| 211 |  |  [Are Transformers universal approximators of sequence-to-sequence functions?](https://openreview.net/forum?id=ByxRM0Ntvr) |  | 0 | Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers... | Ankit Singh Rawat, Chulhee Yun, Sanjiv Kumar, Sashank J. Reddi, Srinadh Bhojanapalli |  |
| 212 |  |  [Pre-training Tasks for Embedding-based Large-scale Retrieval](https://openreview.net/forum?id=rkg-mA4FDr) |  | 0 | We consider the large-scale query-document retrieval problem: given a query (e.g., a question), return the set of relevant documents (e.g., paragraphs containing the answer) from a large document corpus. This problem is often solved in two steps. The retrieval phase first reduces the solution space, returning a subset of candidate documents. The scoring phase then re-ranks the documents. Critically, the retrieval algorithm not only desires high recall but also requires to be highly efficient, returning candidates in time sublinear to the number of documents. Unlike the scoring phase witnessing significant advances recently due to the BERT-style pre-training tasks on cross-attention models, the retrieval phase remains less well... | Felix X. Yu, Sanjiv Kumar, WeiCheng Chang, Yiming Yang, YinWen Chang |  |
| 213 |  |  [Deep Imitative Models for Flexible Inference, Planning, and Control](https://openreview.net/forum?id=Skl4mRNYDr) |  | 0 | Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose "Imitative Models" to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our... | Nicholas Rhinehart, Rowan McAllister, Sergey Levine |  |
| 214 |  |  [CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning](https://openreview.net/forum?id=S1lEX04tPr) |  | 0 | A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with... | Alireza Nakhaei, David Isele, Hongyuan Zha, Jiachen Yang, Kikuo Fujimura |  |
| 215 |  |  [Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks](https://openreview.net/forum?id=HJlSmC4FPS) |  | 0 | We study the generalization properties of deep convolutional neural networks for image denoising in the presence of varying noise levels. We provide extensive empirical evidence that current state-of-the-art architectures systematically overfit to the noise levels in the training set, performing very poorly at new noise levels. We show that strong generalization can be achieved through a simple architectural modification: removing all additive constants. The resulting "bias-free" networks attain state-of-the-art performance over a broad range of noise levels, even when trained over a limited range. They are also locally linear, which enables direct analysis with linear-algebraic tools. We show that the denoising map can be... | Carlos FernandezGranda, Eero P. Simoncelli, Sreyas Mohan, Zahra Kadkhodaie |  |
| 216 |  |  [Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets](https://openreview.net/forum?id=SJxIm0VtwH) |  | 0 | Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish $O(\epsilon^{-4})$ complexity for finding $\epsilon$-first-order stationary point, in which the algorithm only requires invoking... | Jerret Ross, Mingrui Liu, Payel Das, Tianbao Yang, Wei Zhang, Xiaodong Cui, Youssef Mroueh |  |
| 217 |  |  [DeepV2D: Video to Depth with Differentiable Structure from Motion](https://openreview.net/forum?id=HJeO7RNKPr) |  | 0 | We propose DeepV2D, an end-to-end deep learning architecture for predicting depth from video. DeepV2D combines the representation ability of neural networks with the geometric principles governing image formation. We compose a collection of classical geometric algorithms, which are converted into trainable modules and combined into an end-to-end differentiable architecture. DeepV2D interleaves two stages: motion estimation and depth estimation. During inference, motion and depth estimation are alternated and converge to accurate depth. | Jia Deng, Zachary Teed |  |
| 218 |  |  [Learning Space Partitions for Nearest Neighbor Search](https://openreview.net/forum?id=rkenmREFDr) |  | 0 | Space partitions of $\mathbb{R}^d$ underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces (Andoni et al. 2018b,c), we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner (Sanders and Schulz 2013) and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS (Aumuller et al. 2017), our experiments show that the partitions obtained by Neural LSH consistently outperform... | Ilya P. Razenshteyn, Piotr Indyk, Tal Wagner, Yihe Dong |  |
| 219 |  |  [Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP](https://openreview.net/forum?id=S1xnXRVFwH) |  | 0 | The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a “lucky” sub-network initialization being present rather than by helping the optimization process (Frankle& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether “winning ticket” initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer... | Ari S. Morcos, Haonan Yu, Sergey Edunov, Yuandong Tian |  |
| 220 |  |  [Sign-OPT: A Query-Efficient Hard-label Adversarial Attack](https://openreview.net/forum?id=SklTQCNtvS) |  | 0 | We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access: the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be... | ChoJui Hsieh, Minhao Cheng, Patrick H. Chen, PinYu Chen, Sijia Liu, Simranjit Singh |  |
| 221 |  |  [RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering](https://openreview.net/forum?id=HJxR7R4FvS) |  | 0 | We investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to more directly maximize ranking-based objective functions. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require re-running the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. We demonstrate the actor-critic's ability to significantly improve the performance of a variety of prediction models, and achieve... | Chunyuan Li, Jianfeng Gao, Lawrence Carin, Sam Lobel |  |
| 222 |  |  [Intrinsic Motivation for Encouraging Synergistic Behavior](https://openreview.net/forum?id=SJleNCNtDH) |  | 0 | We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. We study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained... | Abhinav Gupta, Rohan Chitnis, Saurabh Gupta, Shubham Tulsiani |  |
| 223 |  |  [Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation](https://openreview.net/forum?id=rygG4AVFvH) |  | 0 | Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to... | Amir Yazdanbakhsh, Byung Hoon Ahn, Hadi Esmaeilzadeh, Prannoy Pilligundla |  |
| 224 |  |  [Recurrent neural circuits for contour detection](https://openreview.net/forum?id=H1gB4RVKvB) |  | 0 | We introduce a deep recurrent neural network architecture that approximates visual cortical circuits (Mély et al., 2018). We show that this architecture, which we refer to as the 𝜸-net, learns to solve contour detection tasks with better sample efficiency than state-of-the-art feedforward networks, while also exhibiting a classic perceptual illusion, known as the orientation-tilt illusion. Correcting this illusion significantly reduces \gnetw contour detection accuracy by driving it to prefer low-level edges over high-level object boundary contours. Overall, our study suggests that the orientation-tilt illusion is a byproduct of neural circuits that help biological visual systems achieve robust and efficient contour detection, and... | Alekh Ashok, Drew Linsley, Junkyung Kim, Thomas Serre |  |
| 225 |  |  [Locality and Compositionality in Zero-Shot Learning](https://openreview.net/forum?id=Hye_V0NKwr) |  | 0 | In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiment show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning. | Linda Petrini, R. Devon Hjelm, Tristan Sylvain |  |
| 226 |  |  [Understanding Knowledge Distillation in Non-autoregressive Machine Translation](https://openreview.net/forum?id=BygFVAEKDH) |  | 0 | Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to... | Chunting Zhou, Graham Neubig, Jiatao Gu |  |
| 227 |  |  [Thieves on Sesame Street! Model Extraction of BERT-based APIs](https://openreview.net/forum?id=Byl5NREFDr) |  | 0 | We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al., 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights... | Ankur P. Parikh, Gaurav Singh Tomar, Kalpesh Krishna, Mohit Iyyer, Nicolas Papernot |  |
| 228 |  |  [Fast is better than free: Revisiting adversarial training](https://openreview.net/forum?id=BJx040EFvH) |  | 0 | Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost.... | Eric Wong, J. Zico Kolter, Leslie Rice |  |
| 229 |  |  [DBA: Distributed Backdoor Attacks against Federated Learning](https://openreview.net/forum?id=rkgyS0VFvr) |  | 0 | Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed... | Bo Li, Chulin Xie, Keli Huang, PinYu Chen |  |
| 230 |  |  [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH) |  | 0 | For sequence models with large vocabularies, a majority of network parameters lie in the input and output layers. In this work, we describe a new method, DeFINE, for learning deep token representations efficiently. Our architecture uses a hierarchical structure with novel skip-connections which allows for the use of low dimensional input and output layers, reducing total parameters and training time while delivering similar or better performance versus existing methods. DeFINE can be incorporated easily in new or existing sequence models. Compared to state-of-the-art methods including adaptive input representations, this technique results in a 6% to 20% drop in perplexity. On WikiText-103, DeFINE reduces the total parameters of... | Hannaneh Hajishirzi, Mohammad Rastegari, Rik KoncelKedziorski, Sachin Mehta |  |
| 231 |  |  [Sampling-Free Learning of Bayesian Quantized Neural Networks](https://openreview.net/forum?id=rylVHR4FPB) |  | 0 | Bayesian learning of model parameters in neural networks is important in scenarios where estimates with well-calibrated uncertainty are important. In this paper, we propose Bayesian quantized networks (BQNs), quantized neural networks (QNNs) for which we learn a posterior distribution over their discrete parameters. We provide a set of efficient algorithms for learning and prediction in BQNs without the need to sample from their parameters or activations, which not only allows for differentiable learning in quantized models but also reduces the variance in gradients estimation. We evaluate BQNs on MNIST, Fashion-MNIST and KMNIST classification datasets compared against bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve... | Furong Huang, Jiahao Su, Milan Cvitkovic |  |
| 232 |  |  [Learning to solve the credit assignment problem](https://openreview.net/forum?id=ByeUBANtvB) |  | 0 | Backpropagation is driving today's artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient... | Benjamin James Lansdell, Konrad Paul Körding, Prashanth Ravi Prakash |  |
| 233 |  |  [Four Things Everyone Should Know to Improve Batch Normalization](https://openreview.net/forum?id=HJx8HANFDH) |  | 0 | A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs.... | Cecilia Summers, Michael J. Dinneen |  |
| 234 |  |  [Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving](https://openreview.net/forum?id=BJedHRVtPB) |  | 0 | Detecting objects such as cars and pedestrians in 3D plays an indispensable role in autonomous driving. Existing approaches largely rely on expensive LiDAR sensors for accurate depth information. While recently pseudo-LiDAR has been introduced as a promising alternative, at a much lower cost based solely on stereo images, there is still a notable performance gap. In this paper we provide substantial advances to the pseudo-LiDAR framework through improvements in stereo depth estimation. Concretely, we adapt the stereo network architecture and loss function to be more aligned with accurate depth estimation of faraway objects --- currently the primary weakness of pseudo-LiDAR. Further, we explore the idea to leverage cheaper but... | Bharath Hariharan, Divyansh Garg, Geoff Pleiss, Kilian Q. Weinberger, Mark E. Campbell, WeiLun Chao, Yan Wang, Yurong You |  |
| 235 |  |  [SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum](https://openreview.net/forum?id=SkxJ8REYPH) |  | 0 | Distributed optimization is essential for training large models on large datasets. Multiple approaches have been proposed to reduce the communication overhead in distributed training, such as synchronizing only after performing multiple local SGD steps, and decentralized methods (e.g., using gossip algorithms) to decouple communications among workers. Although these methods run faster than AllReduce-based methods, which use blocking communication before every update, the resulting models may be less accurate after the same number of updates. Inspired by the BMUF method of Chen & Huo (2016), we propose a slow momentum (SlowMo) framework, where workers periodically synchronize and perform a momentum update, after multiple iterations... | Jianyu Wang, Michael G. Rabbat, Nicolas Ballas, Vinayak Tantia |  |
| 236 |  |  [MetaPix: Few-Shot Video Retargeting](https://openreview.net/forum?id=SJx1URNKwH) |  | 0 | We address the task of unsupervised retargeting of human actions from one video to another. We consider the challenging setting where only a few frames of the target is available. The core of our approach is a conditional generative model that can transcode input skeletal poses (automatically extracted with an off-the-shelf pose estimator) to output target frames. However, it is challenging to build a universal transcoder because humans can appear wildly different due to clothing and background scene geometry. Instead, we learn to adapt – or personalize – a universal generator to the particular human and background in the target. To do so, we make use of meta-learning to discover effective strategies for on-the-fly personalization.... | Deva Ramanan, Jessica Lee, Rohit Girdhar |  |
| 237 |  |  [Learning to Learn by Zeroth-Order Oracle](https://openreview.net/forum?id=ryxz8CVYDH) |  | 0 | In the learning to learn (L2L) framework, we cast the design of optimization algorithms as a machine learning problem and use deep neural networks to learn the update rules. In this paper, we extend the L2L framework to zeroth-order (ZO) optimization setting, where no explicit gradient information is available. Our learned optimizer, modeled as recurrent neural network (RNN), first approximates gradient by ZO gradient estimator and then produces parameter update utilizing the knowledge of previous iterations. To reduce high variance effect due to ZO gradient estimator, we further introduce another RNN to learn the Gaussian sampling rule and dynamically guide the query direction sampling. Our learned optimizer outperforms... | ChoJui Hsieh, Sanjiv Kumar, Sashank J. Reddi, Yangjun Ruan, Yuanhao Xiong |  |
| 238 |  |  [DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames](https://openreview.net/forum?id=H1gX8C4YPr) |  | 0 | We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever "stale"), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of... | Abhishek Kadian, Ari Morcos, Devi Parikh, Dhruv Batra, Erik Wijmans, Irfan Essa, Manolis Savva, Stefan Lee |  |
| 239 |  |  [PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction](https://openreview.net/forum?id=BJxVI04YvB) |  | 0 | We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees---i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem. | Insup Lee, Nikolai Matni, Osbert Bastani, Sangdon Park |  |
| 240 |  |  [Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations](https://openreview.net/forum?id=SJgVU0EKwS) |  | 0 | We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Our experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4× less compute on ImageNet. PG... | G. Edward Suh, Nayun Xu, Ritchie Zhao, Weizhe Hua, Yichi Zhang, Zhiru Zhang |  |
| 241 |  |  [Oblique Decision Trees from Derivatives of ReLU Networks](https://openreview.net/forum?id=Bke8UR4FPB) |  | 0 | We show how neural models can be used to realize piece-wise constant functions such as decision trees. The proposed architecture, which we call locally constant networks, builds on ReLU networks that are piece-wise linear and hence their associated gradients with respect to the inputs are locally constant. We formally establish the equivalence between the classes of locally constant networks and decision trees. Moreover, we highlight several advantageous properties of locally constant networks, including how they realize decision trees with parameter sharing across branching / leaves. Indeed, only $M$ neurons suffice to implicitly model an oblique decision tree with $2^M$ leaf nodes. The neural representation also enables us to... | GuangHe Lee, Tommi S. Jaakkola |  |
| 242 |  |  [Span Recovery for Deep Neural Networks with Applications to Input Obfuscation](https://openreview.net/forum?id=B1guLAVFDB) |  | 0 | The tremendous success of deep neural networks has motivated the need to better understand the fundamental properties of these networks, but many of the theoretical results proposed have only been for shallow networks. In this paper, we study an important primitive for understanding the meaningful input space of a deep network: span recovery. For $k<n$, let $\mathbf{A} \in \mathbb{R}^{k \times n}$ be the innermost weight matrix of an arbitrary feed forward neural network $M: \mathbb{R}^n \to \mathbb{R}$, so $M(x)$ can be written as $M(x) = \sigma(\mathbf{A} x)$, for some network $\sigma: \mathbb{R}^k \to \mathbb{R}$. The goal is then to recover the row span of $\mathbf{A}$ given only oracle access to the value of $M(x)$. We show... | David P. Woodruff, Qiuyi Zhang, Rajesh Jayaram |  |
| 243 |  |  [Improving Neural Language Generation with Spectrum Control](https://openreview.net/forum?id=ByxY8CNtvr) |  | 0 | Recent Transformer-based models such as Transformer-XL and BERT have achieved huge success on various natural language processing tasks. However, contextualized embeddings at the output layer of these powerful models tend to degenerate and occupy an anisotropic cone in the vector space, which is called the representation degeneration problem. In this paper, we propose a novel spectrum control approach to address this degeneration problem. The core idea of our method is to directly guide the spectra training of the output embedding matrix with a slow-decaying singular value prior distribution through a reparameterization framework. We show that our proposed method encourages isotropy of the learned word representations while... | Guangtao Wang, Jing Huang, Kevin Huang, Lingxiao Wang, Quanquan Gu, Ziniu Hu |  |
| 244 |  |  [Learn to Explain Efficiently via Neural Logic Inductive Learning](https://openreview.net/forum?id=SJlh8CEYDB) |  | 0 | The capability of making interpretable and self-explanatory decisions is essential for developing responsible machine learning systems. In this work, we study the learning to explain the problem in the scope of inductive logic programming (ILP). We propose Neural Logic Inductive Learning (NLIL), an efficient differentiable ILP framework that learns first-order logic rules that can explain the patterns in the data. In experiments, compared with the state-of-the-art models, we find NLIL is able to search for rules that are x10 times longer while remaining x3 times faster. We also show that NLIL can scale to large image datasets, i.e. Visual Genome, with 1M entities. | Le Song, Yuan Yang |  |
| 245 |  |  [Improved memory in recurrent neural networks with sequential non-normal dynamics](https://openreview.net/forum?id=ryx1wRNFvB) |  | 0 | Training recurrent neural networks (RNNs) is a hard problem due to degeneracies in the optimization landscape, a problem also known as vanishing/exploding gradients. Short of designing new RNN architectures, previous methods for dealing with this problem usually boil down to orthogonalization of the recurrent dynamics, either at initialization or during the entire training period. The basic motivation behind these methods is that orthogonal transformations are isometries of the Euclidean space, hence they preserve (Euclidean) norms and effectively deal with vanishing/exploding gradients. However, this ignores the crucial effects of non-linearity and noise. In the presence of a non-linearity, orthogonal transformations no longer... | A. Emin Orhan, Xaq Pitkow |  |
| 246 |  |  [Neural Module Networks for Reasoning over Text](https://openreview.net/forum?id=SygWvAVFPr) |  | 0 | Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic... | Dan Roth, Kevin Lin, Matt Gardner, Nitish Gupta, Sameer Singh |  |
| 247 |  |  [Higher-Order Function Networks for Learning Composable 3D Object Representations](https://openreview.net/forum?id=HJgfDREKDB) |  | 0 | We present a new approach to 3D object representation where a neural network encodes the geometry of an object directly into the weights and biases of a second 'mapping' network. This mapping network can be used to reconstruct an object by applying its encoded transformation to points randomly sampled from a simple geometric space, such as the unit sphere. We study the effectiveness of our method through various experiments on subsets of the ShapeNet dataset. We find that the proposed approach can reconstruct encoded objects with accuracy equal to or exceeding state-of-the-art methods with orders of magnitude fewer parameters. Our smallest mapping network has only about 7000 parameters and shows reconstruction quality on par with... | Daniel D. Lee, Eric Mitchell, Selim Engin, Volkan Isler |  |
| 248 |  |  [Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling](https://openreview.net/forum?id=H1x5wRVtvS) |  | 0 | For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that... | Bo Chen, Hao Zhang, Long Tian, Mingyuan Zhou, Zhengjue Wang |  |
| 249 |  |  [Towards Fast Adaptation of Neural Architectures with Meta Learning](https://openreview.net/forum?id=r1eowANFvr) |  | 0 | Recently, Neural Architecture Search (NAS) has been successfully applied to multiple artificial intelligence areas and shows better performance compared with hand-designed networks. However, the existing NAS methods only target a specific task. Most of them usually do well in searching an architecture for single task but are troublesome for multiple datasets or multiple tasks. Generally, the architecture for a new task is either searched from scratch, which is neither efficient nor flexible enough for practical application scenarios, or borrowed from the ones searched on other tasks, which might be not optimal. In order to tackle the transferability of NAS and conduct fast adaptation of neural architectures, we propose a novel... | Dongze Lian, Junzhou Huang, Leyu Lin, Peilin Zhao, Shenghua Gao, Yanxiong Lu, Yin Zheng, Yintao Xu |  |
| 250 |  |  [Graph Constrained Reinforcement Learning for Natural Language Action Spaces](https://openreview.net/forum?id=B1x6w0EtwH) |  | 0 | Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a... | Matthew J. Hausknecht, Prithviraj Ammanabrolu |  |
| 251 |  |  [Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control](https://openreview.net/forum?id=BJxG_0EtDS) |  | 0 | Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed the high-dimensional observations into a lower-dimensional latent representation space, estimate the latent dynamics model, then utilize this model for control in the latent space. An important open question is how to learn a representation that is amenable to existing control algorithms? In this paper, we focus on learning representations for locally-linear control algorithms, such as iterative LQR (iLQR). By formulating and analyzing the representation learning problem from an optimal control perspective, we establish three underlying principles that the... | Ang Li, Hung Bui, Mohammad Ghavamzadeh, Nir Levine, Rui Shu, Yinlam Chow |  |
| 252 |  |  [Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History](https://openreview.net/forum?id=ryxQuANKPB) |  | 0 | We study non-collaborative dialogs, where two agents have a conflict of interest but must strategically communicate to reach an agreement (e.g., negotiation). This setting poses new challenges for modeling dialog history because the dialog's outcome relies not only on the semantic intent, but also on tactics that convey the intent. We propose to model both semantic and tactic history using finite state transducers (FSTs). Unlike RNN, FSTs can explicitly represent dialog history through all the states traversed, facilitating interpretability of dialog structure. We train FSTs on a set of strategies and tactics used in negotiation dialogs. The trained FSTs show plausible tactic structure and can be generalized to other... | Alan W. Black, Yiheng Zhou, Yulia Tsvetkov, Zhou Yu |  |
| 253 |  |  [BERTScore: Evaluating Text Generation with BERT](https://openreview.net/forum?id=SkeHuCVFDr) |  | 0 | We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task and show that BERTScore is more robust to challenging examples compared to existing metrics. | Felix Wu, Kilian Q. Weinberger, Tianyi Zhang, Varsha Kishore, Yoav Artzi |  |
| 254 |  |  [Neural Execution of Graph Algorithms](https://openreview.net/forum?id=SkgKO0EtvS) |  | 0 | Graph Neural Networks (GNNs) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art GNN architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for... | Charles Blundell, Matilde Padovano, Petar Velickovic, Raia Hadsell, Rex Ying |  |
| 255 |  |  [On the Need for Topology-Aware Generative Models for Manifold-Based Defenses](https://openreview.net/forum?id=r1lF_CEYwS) |  | 0 | ML algorithms or models, especially deep neural networks (DNNs), have shown significant promise in several areas. However, recently researchers have demonstrated that ML algorithms, especially DNNs, are vulnerable to adversarial examples (slightly perturbed samples that cause mis-classification). Existence of adversarial examples has hindered deployment of ML algorithms in safety-critical sectors, such as security. Several defenses for adversarial examples exist in the literature. One of the important classes of defenses are manifold-based defenses, where a sample is "pulled back" into the data manifold before classifying. These defenses rely on the manifold assumption (data lie in a manifold of lower dimension than the input... | Somesh Jha, Susmit Jha, Uyeong Jang |  |
| 256 |  |  [FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary](https://openreview.net/forum?id=S1xtORNFwH) |  | 0 | We present a novel method of compression of deep Convolutional Neural Networks (CNNs) by weight sharing through a new representation of convolutional filters. The proposed method reduces the number of parameters of each convolutional layer by learning a $1$D vector termed Filter Summary (FS). The convolutional filters are located in FS as overlapping $1$D segments, and nearby filters in FS share weights in their overlapping regions in a natural way. The resultant neural network based on such weight sharing scheme, termed Filter Summary CNNs or FSNet, has a FS in each convolution layer instead of a set of independent filters in the conventional convolution layer. FSNet has the same architecture as that of the baseline CNN to be... | Jiahui Yu, Jun Huan, Nebojsa Jojic, Thomas S. Huang, Yingzhen Yang |  |
| 257 |  |  [Capsules with Inverted Dot-Product Attention Routing](https://openreview.net/forum?id=HJe6uANtwH) |  | 0 | We introduce a new routing algorithm for capsule networks, in which a child capsule is routed to a parent based only on agreement between the parent's state and the child's vote. The new mechanism 1) designs routing via inverted dot-product attention; 2) imposes Layer Normalization as normalization; and 3) replaces sequential iterative routing with concurrent iterative routing. When compared to previously proposed routing algorithms, our method improves performance on benchmark datasets such as CIFAR-10 and CIFAR-100, and it performs at-par with a powerful CNN (ResNet-18) with 4x fewer parameters. On a different task of recognizing digits from overlayed digit images, the proposed capsule model performs favorably against CNNs given... | Hanlin Goh, Nitish Srivastava, Ruslan Salakhutdinov, YaoHung Hubert Tsai |  |
| 258 |  |  [Composition-based Multi-Relational Graph Convolutional Networks](https://openreview.net/forum?id=BylA_C4tPr) |  | 0 | Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales... | Partha P. Talukdar, Shikhar Vashishth, Soumya Sanyal, Vikram Nitin |  |
| 259 |  |  [Gradient-Based Neural DAG Learning](https://openreview.net/forum?id=rklbKA4YDS) |  | 0 | We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks while being competitive with existing greedy search methods on important metrics for causal... | Philippe Brouillard, Simon LacosteJulien, Sébastien Lachapelle, Tristan Deleu |  |
| 260 |  |  [The Local Elasticity of Neural Networks](https://openreview.net/forum?id=HJxMYANtPH) |  | 0 | This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we... | Hangfeng He, Weijie J. Su |  |
| 261 |  |  [Composing Task-Agnostic Policies with Deep Reinforcement Learning](https://openreview.net/forum?id=H1ezFREtwH) |  | 0 | The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers... | Ahmed Hussain Qureshi, Byron Boots, Jacob J. Johnson, Michael C. Yip, Taylor Henderson, Yuzhe Qin |  |
| 262 |  |  [Convergence of Gradient Methods on Bilinear Zero-Sum Games](https://openreview.net/forum?id=SJlVY04FwH) |  | 0 | Min-max formulations have attracted great attention in the ML community due to the rise of deep generative models and adversarial methods, while understanding the dynamics of gradient algorithms for solving such formulations has remained a grand challenge. As a first step, we restrict to bilinear zero-sum games and give a systematic analysis of popular gradient updates, for both simultaneous and alternating versions. We provide exact conditions for their convergence and find the optimal parameter setup and convergence rates. In particular, our results offer formal evidence that alternating updates converge "better" than simultaneous ones. | Guojun Zhang, Yaoliang Yu |  |
| 263 |  |  [Discovering Motor Programs by Recomposing Demonstrations](https://openreview.net/forum?id=rkgHY0NYwr) |  | 0 | In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing demonstrations into primitives often assume manually defined primitives and bypass the difficulty of discovering these primitives. On the other hand, approaches in primitive discovery put restrictive assumptions on the complexity of a primitive, which limit applicability to narrow tasks. Our approach attempts to circumvent these challenges by jointly learning both the underlying motor primitives and recomposing these primitives to form the original demonstration. Through constraints on both the parsimony of primitive decomposition and the simplicity of a given... | Abhinav Gupta, Lerrel Pinto, Shubham Tulsiani, Tanmay Shankar |  |
| 264 |  |  [Learning from Explanations with Neural Execution Tree](https://openreview.net/forum?id=rJlUt0EYwS) |  | 0 | While deep neural networks have achieved impressive performance on a range of NLP tasks, these data-hungry models heavily rely on labeled data, which restricts their applications in scenarios where data annotation is expensive. Natural language (NL) explanations have been demonstrated very useful additional supervision, which can provide sufficient domain knowledge for generating more labeled data over new instances, while the annotation time only doubles. However, directly applying them for augmenting model learning encounters two challenges: (1) NL explanations are unstructured and inherently compositional, which asks for a modularized model to represent their semantics, (2) NL explanations often have large numbers of linguistic... | Jun Yan, Leonardo Neves, Qinyuan Ye, Wenxuan Zhou, Xiang Ren, Yujia Qin, Zhiyuan Liu, Ziqi Wang |  |
| 265 |  |  [Jelly Bean World: A Testbed for Never-Ending Learning](https://openreview.net/forum?id=Byx_YAVYPH) |  | 0 | Machine learning has shown growing success in recent years. However, current machine learning systems are highly specialized, trained for particular problems or domains, and typically on a single narrow dataset. Human learning, on the other hand, is highly general and adaptable. Never-ending learning is a machine learning paradigm that aims to bridge this gap, with the goal of encouraging researchers to design machine learning systems that can learn to perform a wider variety of inter-related tasks in more complex environments. To date, there is no environment or testbed to facilitate the development and evaluation of never-ending learning systems. To this end, we propose the Jelly Bean World testbed. The Jelly Bean World allows... | Abulhair Saparov, Emmanouil Antonios Platanios, Tom M. Mitchell |  |
| 266 |  |  [Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization](https://openreview.net/forum?id=ryeFY0EFwS) |  | 0 | An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can... | Satrajit Chatterjee |  |
| 267 |  |  [Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks](https://openreview.net/forum?id=HJgCF0VFwr) |  | 0 | Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN’s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques | Jun S. Liu, Long Sha, Pengyu Hong, Xin Xing, Zuofeng Shang |  |
| 268 |  |  [MEMO: A Deep Network for Flexible Combination of Episodic Memories](https://openreview.net/forum?id=rJxlc0EtDr) |  | 0 | Recent research developing neural network architectures with external memory have often used the benchmark bAbI question and answering dataset which provides a challenging number of tasks requiring reasoning. Here we employed a classic associative inference task from the human neuroscience literature in order to more carefully probe the reasoning capacity of existing memory-augmented architectures. This task is thought to capture the essence of reasoning -- the appreciation of distant relationships among elements distributed across multiple facts or memories. Surprisingly, we found that current architectures struggle to reason over long distance associations. Similar results were obtained on a more complex task involving finding... | Adrià Puigdomènech Badia, Andrea Banino, Caswell Barry, Charles Blundell, Demis Hassabis, Dharshan Kumaran, Martin J. Chadwick, Matthew M. Botvinick, Raphael Köster, Vinícius Flores Zambaldi |  |
| 269 |  |  [Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality](https://openreview.net/forum?id=SyxV9ANFDH) |  | 0 | Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes’ time series... | Saurabh Khanna, Vincent Y. F. Tan |  |
| 270 |  |  [Bayesian Meta Sampling for Fast Uncertainty Adaptation](https://openreview.net/forum?id=Bkxv90EKPB) |  | 0 | Meta learning has been making impressive progress for fast model adaptation. However, limited work has been done on learning fast uncertainty adaption for Bayesian modeling. In this paper, we propose to achieve the goal by placing meta learning on the space of probability measures, inducing the concept of meta sampling for fast uncertainty adaption. Specifically, we propose a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. The meta sampler is constructed by adopting a neural-inverse-autoregressive-flow (NIAF) structure, a variant of the recently proposed neural autoregressive flows, to efficiently generate meta samples to be adapted. The sample adapter moves meta samples to... | Changyou Chen, Ping Yu, Ruiyi Zhang, Yang Zhao, Zhenyi Wang |  |
| 271 |  |  [Non-Autoregressive Dialog State Tracking](https://openreview.net/forum?id=H1e_cC4twS) |  | 0 | Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have progressed toward open-vocabulary or generation-based approaches where the models can generate slot value candidates from the dialogue history itself. These approaches have shown good performance gain, especially in complicated dialogue domains with dynamic slot values. However, they fall short in two aspects: (1) they do not allow models to explicitly learn signals across domains and slots to detect potential dependencies among \textit{(domain, slot)} pairs; and (2) existing models follow auto-regressive approaches which incur high time cost when the dialogue evolves over multiple domains and multiple turns. In this paper, we propose a novel framework... | Hung Le, Richard Socher, Steven C. H. Hoi |  |
| 272 |  |  [Extreme Tensoring for Low-Memory Preconditioning](https://openreview.net/forum?id=SklKcRNYDH) |  | 0 | State-of-the-art models are now trained with billions of parameters, reaching hardware limits in terms of memory consumption. This has created a recent demand for memory-efficient optimizers. To this end, we investigate the limits and performance tradeoffs of memory-efficient adaptively preconditioned gradient methods. We propose \emph{extreme tensoring} for high-dimensional stochastic optimization, showing that an optimizer needs very little memory to benefit from adaptive preconditioning. Our technique applies to arbitrary models (not necessarily with tensor-shaped parameters), and is accompanied by regret and convergence guarantees, which shed light on the tradeoffs between preconditioner quality and expressivity. On a... | Cyril Zhang, Elad Hazan, Naman Agarwal, Xinyi Chen, Yi Zhang |  |
| 273 |  |  [RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?](https://openreview.net/forum?id=HylpqA4FwS) |  | 0 | Recurrent neural networks (RNNs) are particularly well-suited for modeling long-term dependencies in sequential data, but are notoriously hard to train because the error backpropagated in time either vanishes or explodes at an exponential rate. While a number of works attempt to mitigate this effect through gated recurrent units, skip-connections, parametric constraints and design choices, we propose a novel incremental RNN (iRNN), where hidden state vectors keep track of incremental changes, and as such approximate state-vector increments of Rosenblatt's (1962) continuous-time RNNs. iRNN exhibits identity gradients and is able to account for long-term dependencies (LTD). We show that our method is computationally efficient... | Anil Kag, Venkatesh Saligrama, Ziming Zhang |  |
| 274 |  |  [The Early Phase of Neural Network Training](https://openreview.net/forum?id=Hkl1iRNFwS) |  | 0 | Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state and its updates during these early iterations of training, and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that,... | Ari S. Morcos, David J. Schwab, Jonathan Frankle |  |
| 275 |  |  [NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension](https://openreview.net/forum?id=ryxgsCVYPr) |  | 0 | Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question "What was the projection of sea level increases in the fourth assessment report?", the answer should properly satisfy several conditions, such as "increases" (but not decreases) and "fourth" (but not third). To address this, we... | Akhil Kedia, Haejun Lee, Jaegul Choo, Sai Chetan Chinthakindi, Seohyun Back |  |
| 276 |  |  [Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization](https://openreview.net/forum?id=SkgGjRVKDS) |  | 0 | Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra... | Jian Sun, Junjie Yan, Ruosi Wan, Wei Zhang, Xiangyu Zhang, Yichen Wei |  |
| 277 |  |  [Single Episode Policy Transfer in Reinforcement Learning](https://openreview.net/forum?id=rJeQoCNYDS) |  | 0 | Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning (RL). An even greater challenge is performing near-optimally in a single attempt at test time, possibly without access to dense rewards, which is not addressed by current methods that require multiple experience rollouts for adaptation. To achieve single episode transfer in a family of environments with related dynamics, we propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then immediately used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational... | Brenden K. Petersen, Daniel M. Faissol, Hongyuan Zha, Jiachen Yang |  |
| 278 |  |  [Generalization through Memorization: Nearest Neighbor Language Models](https://openreview.net/forum?id=HklBjCEKvH) |  | 0 | We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this transformation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 -- a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest... | Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, Omer Levy, Urvashi Khandelwal |  |
| 279 |  |  [Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention](https://openreview.net/forum?id=r1eIiCNYwS) |  | 0 | Transformers have achieved new heights modeling natural language as a sequence of text tokens. However, in many real world scenarios, textual data inherently exhibits structures beyond a linear sequence such as trees and graphs; many tasks require reasoning with evidence scattered across multiple pieces of texts. This paper presents Transformer-XH, which uses eXtra Hop attention to enable intrinsic modeling of structured texts in a fully data-driven way. Its new attention mechanism naturally “hops” across the connected text sequences in addition to attending over tokens within each sequence. Thus, Transformer-XH better conducts joint multi-evidence reasoning by propagating information between documents and constructing global... | Chen Zhao, Chenyan Xiong, Corby Rosset, Paul N. Bennett, Saurabh Tiwary, Xia Song |  |
| 280 |  |  [Synthesizing Programmatic Policies that Inductively Generalize](https://openreview.net/forum?id=S1l8oANFDH) |  | 0 | Deep reinforcement learning has successfully solved a number of challenging control tasks. However, learned policies typically have difficulty generalizing to novel environments. We propose an algorithm for learning programmatic state machine policies that can capture repeating behaviors. By doing so, they have the ability to generalize to instances requiring an arbitrary number of repetitions, a property we call inductive generalization. However, state machine policies are hard to learn since they consist of a combination of continuous and discrete structures. We propose a learning framework called adaptive teaching, which learns a state machine policy by imitating a teacher; in contrast to traditional imitation learning, our... | Armando SolarLezama, Jeevana Priya Inala, Osbert Bastani, Zenna Tavares |  |
| 281 |  |  [Decoding As Dynamic Programming For Recurrent Autoregressive Models](https://openreview.net/forum?id=HklOo0VFDH) |  | 0 | Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model. Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings. Our method introduces discrete variables for output tokens, and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a... | Gholamreza Haffari, Najam Zaidi, Trevor Cohn |  |
| 282 |  |  [Deep Double Descent: Where Bigger Models and More Data Hurt](https://openreview.net/forum?id=B1g5sA4twr) |  | 0 | We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity, and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance. | Boaz Barak, Gal Kaplun, Ilya Sutskever, Preetum Nakkiran, Tristan Yang, Yamini Bansal |  |
| 283 |  |  [Intriguing Properties of Adversarial Training at Scale](https://openreview.net/forum?id=HyxJhCEFDS) |  | 0 | Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties. First, we study the role of normalization. Batch normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images... | Alan L. Yuille, Cihang Xie |  |
| 284 |  |  [Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks](https://openreview.net/forum?id=Bkxe2AVtPS) |  | 0 | Training with larger number of parameters while keeping fast iterations is an increasingly adopted strategy and trend for developing better performing Deep Neural Network (DNN) models. This necessitates increased memory footprint and computational requirements for training. Here we introduce a novel methodology for training deep neural networks using 8-bit floating point (FP8) numbers. Reduced bit precision allows for a larger effective memory and increased computational speed. We name this method Shifted and Squeezed FP8 (S2FP8). We show that, unlike previous 8-bit precision training methods, the proposed method works out of the box for representative models: ResNet50, Transformer and NCF. The method can maintain model accuracy... | Anahita Bhiwandiwalla, Hanlin Tang, Léopold Cambier, Mehran Nekuii, Oguz H. Elibol, Ting Gong |  |
| 285 |  |  [Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication](https://openreview.net/forum?id=SJxZnR4YvB) |  | 0 | We study the problem of regret minimization for distributed bandits learning, in which $M$ agents work collaboratively to minimize their total regret under the coordination of a central server. Our goal is to design communication protocols with near-optimal regret and little communication cost, which is measured by the total amount of transmitted data. For distributed multi-armed bandits, we propose a protocol with near-optimal regret and only $O(M\log(MK))$ communication cost, where $K$ is the number of arms. The communication cost is independent of the time horizon $T$, has only logarithmic dependence on the number of arms, and matches the lower bound except for a logarithmic factor. For distributed $d$-dimensional linear... | Jiachen Hu, Liwei Wang, Xiaoyu Chen, Yuanhao Wang |  |
| 286 |  |  [Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks](https://openreview.net/forum?id=r1xGnA4Kvr) |  | 0 | Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network may perform well on similar testing data, inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to design inputs with very small perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are noisy,... | Giri P. Krishnan, Maxim Bazhenov, Ramyaa Ramyaa, Timothy Tadros |  |
| 287 |  |  [A Closer Look at the Optimization Landscapes of Generative Adversarial Networks](https://openreview.net/forum?id=HJeVnCEKwH) |  | 0 | Generative adversarial networks have been very successful in generative modeling, however they remain relatively challenging to train compared to standard deep neural networks. In this paper, we propose new visualization techniques for the optimization landscapes of GANs that enable us to study the game vector field resulting from the concatenation of the gradient of both players. Using these visualization techniques we try to bridge the gap between theory and practice by showing empirically that the training of GANs exhibits significant rotations around LSSP, similar to the one predicted by theory on toy examples. Moreover, we provide empirical evidence that GAN training seems to converge to a stable stationary point which is a... | Amjad Almahairi, Gauthier Gidel, Hugo Berard, Pascal Vincent, Simon LacosteJulien |  |
| 288 |  |  [On the Global Convergence of Training Deep Linear ResNets](https://openreview.net/forum?id=HJxEhREKDH) |  | 0 | We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training $L$-hidden-layer linear residual networks (ResNets). We prove that for training deep residual networks with certain linear transformations at input and output layers, which are fixed throughout training, both GD and SGD with zero initialization on all hidden weights can converge to the global minimum of the training loss. Moreover, when specializing to appropriate Gaussian random linear transformations, GD and SGD provably optimize wide enough deep linear ResNets. Compared with the global convergence result of GD for training standard deep linear networks \citep{du2019width}, our condition on the neural network width is sharper by a... | Difan Zou, Philip M. Long, Quanquan Gu |  |
| 289 |  |  [Towards a Deep Network Architecture for Structured Smoothness](https://openreview.net/forum?id=Hklr204Fvr) |  | 0 | We propose the Fixed Grouping Layer (FGL); a novel feedforward layer designed to incorporate the inductive bias of structured smoothness into a deep learning model. FGL achieves this goal by connecting nodes across layers based on spatial similarity. The use of structured smoothness, as implemented by FGL, is motivated by applications to structured spatial data, which is, in turn, motivated by domain knowledge. The proposed model architecture outperforms conventional neural network architectures across a variety of simulated and real datasets with structured smoothness. | Haroun Habeeb, Oluwasanmi Koyejo |  |
| 290 |  |  [Revisiting Self-Training for Neural Sequence Generation](https://openreview.net/forum?id=SJgdnAVKDH) |  | 0 | Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for... | Jiajun Shen, Jiatao Gu, Junxian He, Marc'Aurelio Ranzato |  |
| 291 |  |  [Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators](https://openreview.net/forum?id=HJeqhA4YDS) |  | 0 | Convolutional Neural Networks (CNNs) have emerged as highly successful tools for image generation, recovery, and restoration. A major contributing factor to this success is that convolutional networks impose strong prior assumptions about natural images. A surprising experiment that highlights this architectural bias towards natural images is that one can remove noise and corruptions from a natural image without using any training data, by simply fitting (via gradient descent) a randomly initialized, over-parameterized convolutional generator to the corrupted image. While this over-parameterized network can fit the corrupted image perfectly, surprisingly after a few iterations of gradient descent it generates an almost uncorrupted... | Mahdi Soltanolkotabi, Reinhard Heckel |  |
| 292 |  |  [Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities](https://openreview.net/forum?id=B1lj20NFDS) |  | 0 | Multivariate spatial point process models can describe heterotopic data over space. However, highly multivariate intensities are computationally challenging due to the curse of dimensionality. To bridge this gap, we introduce a declustering based hidden variable model that leads to an efficient inference procedure via a variational autoencoder (VAE). We also prove that this model is a generalization of the VAE-based model for collaborative filtering. This leads to an interesting application of spatial point process models to recommender systems. Experimental results show the method's utility on both synthetic data and real-world data sets. | Andrea L. Bertozzi, Baichuan Yuan, Chang Zhou, Hongxia Yang, Jianxin Ma, Xiaowei Wang |  |
| 293 |  |  [Model-Augmented Actor-Critic: Backpropagating through Paths](https://openreview.net/forum?id=Skln2A4YDB) |  | 0 | Current model-based reinforcement learning approaches use the model simply as a learned black-box simulator to augment the data for policy optimization or value function learning. In this paper, we show how to make more effective use of the model by exploiting its differentiability. We construct a policy optimization algorithm that uses the pathwise derivative of the learned model and policy across future timesteps. Instabilities of learning across many timesteps are prevented by using a terminal value function, learning the policy in an actor-critic fashion. Furthermore, we present a derivation on the monotonic improvement of our objective in terms of the gradient error in the model and value function. We show that our approach... | Ignasi Clavera, Pieter Abbeel, Yao Fu |  |
| 294 |  |  [LambdaNet: Probabilistic Type Inference using Graph Neural Networks](https://openreview.net/forum?id=Hkx6hANtwH) |  | 0 | As gradual typing becomes increasingly popular in languages like Python and TypeScript, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully inferred by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for TypeScript based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate... | Greg Durrett, Isil Dillig, Jiayi Wei, Maruth Goyal |  |
| 295 |  |  [From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech](https://openreview.net/forum?id=H1guaREYPr) |  | 0 | This work seeks the possibility of generating the human face from voice solely based on the audio-visual data without any human-labeled annotations. To this end, we propose a multi-modal learning framework that links the inference stage and generation stage. First, the inference networks are trained to match the speaker identity between the two different modalities. Then the pre-trained inference networks cooperate with the generation network by giving conditional information about the voice. The proposed method exploits the recent development of GANs techniques and generates the human face directly from the speech waveform making our system fully end-to-end. We analyze the extent to which the network can naturally disentangle two... | Changdae Park, HyeongSeok Choi, Kyogu Lee |  |
| 296 |  |  [Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping](https://openreview.net/forum?id=BJxt60VtPr) |  | 0 | Predictive coding theories suggest that the brain learns by predicting observations at various levels of abstraction. One of the most basic prediction tasks is view prediction: how would a given scene look from an alternative viewpoint? Humans excel at this task. Our ability to imagine and fill in missing information is tightly coupled with perception: we feel as if we see the world in 3 dimensions, while in fact, information from only the front surface of the world hits our retinas. This paper explores the role of view prediction in the development of 3D visual recognition. We propose neural 3D mapping networks, which take as input 2.5D (color and depth) video streams captured by a moving camera, and lift them to stable 3D feature... | Adam W. Harley, Fangyu Li, HsiaoYu Fish Tung, Katerina Fragkiadaki, Shrinidhi Kowshika Lakshmikanth, Xian Zhou |  |
| 297 |  |  [Decoupling Representation and Classifier for Long-Tailed Recognition](https://openreview.net/forum?id=r1gRTCVFvB) |  | 0 | The long-tail distribution of the visual world poses great challenges for deep learning based classification models on how to handle the class imbalance problem. Existing solutions usually involve class-balancing strategies, e.g., by loss re-weighting, data re-sampling, or transfer learning from head- to tail-classes, but most of them adhere to the scheme of jointly learning representations and classifiers. In this work, we decouple the learning procedure into representation learning and classification, and systematically explore how different balancing strategies affect them for long-tailed recognition. The findings are surprising: (1) data imbalance might not be an issue in learning high-quality representations; (2) with... | Albert Gordo, Bingyi Kang, Jiashi Feng, Marcus Rohrbach, Saining Xie, Yannis Kalantidis, Zhicheng Yan |  |
| 298 |  |  [Robust Reinforcement Learning for Continuous Control with Model Misspecification](https://openreview.net/forum?id=HJgC60EtwB) |  | 0 | We provide a framework for incorporating robustness -- to perturbations in the transition dynamics which we refer to as model misspecification -- into continuous control Reinforcement Learning (RL) algorithms. We specifically focus on incorporating robustness into a state-of-the-art continuous control RL algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve this by learning a policy that optimizes for a worst case, entropy-regularized, expected return objective and derive a corresponding robust entropy-regularized Bellman contraction operator. In addition, we introduce a less conservative, soft-robust, entropy-regularized objective with a corresponding Bellman operator. We show that both, robust and... | Abbas Abdolmaleki, Daniel J. Mankowitz, Jackie Kay, Jost Tobias Springenberg, Martin A. Riedmiller, Nir Levine, Rae Jeong, Timothy A. Mann, Todd Hester, Yuanyuan Shi |  |
| 299 |  |  [Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](https://openreview.net/forum?id=S1l-C0NtwS) |  | 0 | Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these methods across diverse cross-lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task-dependent.... | Graham Neubig, Jaime G. Carbonell, Jiateng Xie, Ruochen Xu, Yiming Yang, Zirui Wang |  |
| 300 |  |  [Training Recurrent Neural Networks Online by Learning Explicit State Variables](https://openreview.net/forum?id=SJgmR0NKPr) |  | 0 | Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm's performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically computationally expensive and focus computation on computing gradients back in time. In this work, we reformulate the RNN training objective to explicitly learn state vectors; this breaks the dependence... | Adam White, Alan Chan, Martha White, Somjit Nath, Vincent Liu, Xin Li |  |
| 301 |  |  [Uncertainty-guided Continual Learning with Bayesian Neural Networks](https://openreview.net/forum?id=HklUCCVKDB) |  | 0 | Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' \textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify \textit{what to remember} and \textit{what to change} as we continually learn, and thus mitigate catastrophic... | Marcus Rohrbach, Mohamed Elhoseiny, Sayna Ebrahimi, Trevor Darrell |  |
| 302 |  |  [Curriculum Loss: Robust Learning and Generalization against Label Corruption](https://openreview.net/forum?id=rkgt0REKwS) |  | 0 | Deep neural networks (DNNs) have great expressive power, which can even memorize samples with wrong labels. It is vitally important to reiterate robustness and generalization in DNNs against label corruption. To this end, this paper studies the 0-1 loss, which has a monotonic relationship between empirical adversary (reweighted) risk (Hu et al. 2018). Although the 0-1 loss is robust to outliers, it is also difficult to optimize. To efficiently optimize the 0-1 loss while keeping its robust properties, we propose a very simple and efficient loss, i.e. curriculum loss (CL). Our CL is a tighter upper bound of the 0-1 loss compared with conventional summation based surrogate losses. Moreover, CL can adaptively select samples for... | Ivor W. Tsang, Yueming Lyu |  |
| 303 |  |  [Picking Winning Tickets Before Training by Preserving Gradient Flow](https://openreview.net/forum?id=SkgsACVKPH) |  | 0 | Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments... | Chaoqi Wang, Guodong Zhang, Roger B. Grosse |  |
| 304 |  |  [Generative Models for Effective ML on Private, Decentralized Datasets](https://openreview.net/forum?id=SJgaRA4FPH) |  | 0 | To improve real-world applications of machine learning, experienced modelers develop intuition about their datasets, their models, and how the two interact. Manual inspection of raw data—of representative samples, of outliers, of misclassifications—is an essential tool in a) identifying and fixing problems in the data, b) generating new modeling hypotheses, and c) assigning or refining human-provided labels. However, manual data inspection is risky for privacy-sensitive datasets, such as those representing the behavior of real-world individuals. Furthermore, manual data inspection is impossible in the increasingly important setting of federated learning, where raw examples are stored at the edge and the modeler may only access... | Blaise Agüera y Arcas, Daniel Ramage, H. Brendan McMahan, Mingqing Chen, Peter Kairouz, Rajiv Mathews, Sean Augenstein, Swaroop Ramaswamy |  |
| 305 |  |  [Inductive representation learning on temporal graphs](https://openreview.net/forum?id=rJeW1yHYwH) |  | 0 | Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block... | Chuanwei Ruan, Da Xu, Evren Körpeoglu, Kannan Achan, Sushant Kumar |  |
| 306 |  |  [BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning](https://openreview.net/forum?id=Sklf1yrYDr) |  | 0 | Ensembles, where multiple neural networks are trained individually and their predictions are averaged, have been shown to be widely successful for improving both the accuracy and predictive uncertainty of single neural networks. However, an ensemble’s cost for both training and testing increases linearly with the number of networks, which quickly becomes untenable. In this paper, we propose BatchEnsemble, an ensemble method whose computational and memory costs are significantly lower than typical ensembles. BatchEnsemble achieves this by defining each weight matrix to be the Hadamard product of a shared weight among all ensemble members and a rank-one matrix per member. Unlike ensembles, BatchEnsemble is not only parallelizable... | Dustin Tran, Jimmy Ba, Yeming Wen |  |
| 307 |  |  [Towards neural networks that provably know when they don't know](https://openreview.net/forum?id=ByxGkySKwH) |  | 0 | It has recently been shown that ReLU networks produce arbitrarily over-confident predictions far away from the training data. Thus, ReLU networks do not know when they don't know. However, this is a highly important property in safety critical applications. In the context of out-of-distribution detection (OOD) there have been a number of proposals to mitigate this problem but none of them are able to make any mathematical guarantees. In this paper we propose a new approach to OOD which overcomes both problems. Our approach can be used with ReLU networks and provides provably low confidence predictions far away from the training data as well as the first certificates for low confidence predictions in a neighborhood of an... | Alexander Meinke, Matthias Hein |  |
| 308 |  |  [Iterative energy-based projection on a normal data manifold for anomaly localization](https://openreview.net/forum?id=HJx81ySKwr) |  | 0 | Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder's loss function. This energy can be augmented... | David Dehaene, Oriel Frigo, Pierre Eline, Sébastien Combrexelle |  |
| 309 |  |  [Towards Stable and Efficient Training of Verifiably Robust Neural Networks](https://openreview.net/forum?id=Skxuk1rFwB) |  | 0 | Training neural networks with verifiable robustness guarantees is challenging. Several existing approaches utilize linear relaxation based neural network output bounds under perturbation, but they can slow down training by a factor of hundreds depending on the underlying network architectures. Meanwhile, interval bound propagation (IBP) based training is efficient and significantly outperforms linear relaxation based methods on many tasks, yet it may suffer from stability issues since the bounds are much looser especially at the beginning of training. In this paper, we propose a new certified adversarial training method, CROWN-IBP, by combining the fast IBP bounds in a forward bounding pass and a tight linear relaxation based... | Bo Li, Chaowei Xiao, ChoJui Hsieh, Duane S. Boning, Hongge Chen, Huan Zhang, Robert Stanforth, Sven Gowal |  |
| 310 |  |  [Frequency-based Search-control in Dyna](https://openreview.net/forum?id=B1gskyStwr) |  | 0 | Model-based reinforcement learning has been empirically demonstrated as a successful strategy to improve sample efficiency. In particular, Dyna is an elegant model-based architecture integrating learning and planning that provides huge flexibility of using a model. One of the most important components in Dyna is called search-control, which refers to the process of generating state or state-action pairs from which we query the model to acquire simulated experiences. Search-control is critical in improving learning efficiency. In this work, we propose a simple and novel search-control strategy by searching high frequency regions of the value function. Our main intuition is built on Shannon sampling theorem from signal processing,... | Amirmassoud Farahmand, Jincheng Mei, Yangchen Pan |  |
| 311 |  |  [Learning representations for binary-classification without backpropagation](https://openreview.net/forum?id=Bke61krFvS) |  | 0 | The family of feedback alignment (FA) algorithms aims to provide a more biologically motivated alternative to backpropagation (BP), by substituting the computations that are unrealistic to be implemented in physical brains. While FA algorithms have been shown to work well in practice, there is a lack of rigorous theory proofing their learning capabilities. Here we introduce the first feedback alignment algorithm with provable learning guarantees. In contrast to existing work, we do not require any assumption about the size or depth of the network except that it has a single output neuron, i.e., such as for binary classification tasks. We show that our FA algorithm can deliver its theoretical promises in practice, surpassing the... | Mathias Lechner |  |
| 312 |  |  [Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks](https://openreview.net/forum?id=HygegyrYwH) |  | 0 | Recent theoretical work has guaranteed that overparameterized networks trained by gradient descent achieve arbitrarily low training error, and sometimes even low test error. The required width, however, is always polynomial in at least one of the sample size $n$, the (inverse) target error $1/\epsilon$, and the (inverse) failure probability $1/\delta$. This work shows that $\widetilde{\Theta}(1/\epsilon)$ iterations of gradient descent with $\widetilde{\Omega}(1/\epsilon^2)$ training examples on two-layer ReLU networks of any width exceeding $\textrm{polylog}(n,1/\epsilon,1/\delta)$ suffice to achieve a test misclassification error of $\epsilon$. We also prove that stochastic gradient descent can achieve $\epsilon$ test error with... | Matus Telgarsky, Ziwei Ji |  |
| 313 |  |  [Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics](https://openreview.net/forum?id=r1gelyrtwH) |  | 0 | Sparsely available data points cause numerical error on finite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or defined on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn finite differences inspired by physics equations. PA-DGN leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential... | Chuizheng Meng, Sungyong Seo, Yan Liu |  |
| 314 |  |  [HiLLoC: lossless image compression with hierarchical latent variable models](https://openreview.net/forum?id=r1lZgyBYwS) |  | 0 | We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results. | David Barber, James Townsend, Julius Kunze, Thomas Bird |  |
| 315 |  |  [IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks](https://openreview.net/forum?id=BJeGlJStPr) |  | 0 | The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases sample throughput (i.e. increasing parallelization in IMPALA (Espeholt et al., 2018)), sample efficiency drops significantly. To address this, we propose a new distributed reinforcement learning... | Eric Liang, Ion Stoica, Jiahao Yao, Michael Luo, Richard Liaw |  |
| 316 |  |  [On Bonus Based Exploration Methods In The Arcade Learning Environment](https://openreview.net/forum?id=BJewlyStDr) |  | 0 | Research on exploration in reinforcement learning, as applied to Atari 2600 game-playing, has emphasized tackling difficult exploration problems such as Montezuma's Revenge (Bellemare et al., 2016). Recently, bonus-based exploration methods, which explore by augmenting the environment reward, have reached above-human average performance on such domains. In this paper we reassess popular bonus-based exploration methods within a common evaluation framework. We combine Rainbow (Hessel et al., 2018) with different exploration bonuses and evaluate its performance on Montezuma's Revenge, Bellemare et al.'s set of hard of exploration games with sparse rewards, and the whole Atari 2600 suite. We find that while exploration bonuses lead to... | Aaron C. Courville, Adrien Ali Taïga, Marc G. Bellemare, Marlos C. Machado, William Fedus |  |
| 317 |  |  [Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation](https://openreview.net/forum?id=r1lOgyrKDS) |  | 0 | Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. However, high gradient variance hinders the practical use of this method. To stabilize this method, we adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. We also demonstrate the use of correlated MC rollouts for binary-tree softmax models, which reduce the high generation cost in large vocabulary... | Mingyuan Zhou, Xinjie Fan, Yizhe Zhang, Zhendong Wang |  |
| 318 |  |  [Smoothness and Stability in GANs](https://openreview.net/forum?id=HJeOekHKwr) |  | 0 | Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for... | Casey Chu, Kenji Fukumizu, Kentaro Minami |  |
| 319 |  |  [SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks](https://openreview.net/forum?id=rJxtgJBKDr) |  | 0 | SNOW is an efficient learning method to improve training/serving throughput as well as accuracy for transfer and lifelong learning of convolutional neural networks based on knowledge subscription. SNOW selects the top-K useful intermediate feature maps for a target task from a pre-trained and frozen source model through a novel channel pooling scheme, and utilizes them in the task-specific delta model. The source model is responsible for generating a large number of generic feature maps. Meanwhile, the delta model selectively subscribes to those feature maps and fuses them with its local ones to deliver high accuracy for the target task. Since a source model takes part in both training and serving of all target tasks in an... | Bumsoo Kang, Chungkuk Yoo, Minsik Cho |  |
| 320 |  |  [Empirical Studies on the Properties of Linear Regions in Deep Neural Networks](https://openreview.net/forum?id=SkeFl1HKwr) |  | 0 | A deep neural networks (DNN) with piecewise linear activations can partition the input space into numerous small linear regions, where different linear functions are fitted. It is believed that the number of these regions represents the expressivity of a DNN. This paper provides a novel and meticulous perspective to look into DNNs: Instead of just counting the number of the linear regions, we study their local properties, such as the inspheres, the directions of the corresponding hyperplanes, the decision boundaries, and the relevance of the surrounding regions. We empirically observed that different optimization techniques lead to completely different linear regions, even though they result in similar classification accuracies. We... | Dongrui Wu, Xiao Zhang |  |
| 321 |  |  [Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning](https://openreview.net/forum?id=S1ltg1rFDS) |  | 0 | Off-policy estimation for long-horizon problems is important in many real-life applications such as healthcare and robotics, where high-fidelity simulators may not be available and on-policy evaluation is expensive or impossible. Recently, \citet{liu18breaking} proposed an approach that avoids the curse of horizon suffered by typical importance-sampling-based methods. While showing promising results, this approach is limited in practice as it requires data being collected by a known behavior policy. In this work, we propose a novel approach that eliminates such limitations. In particular, we formulate the problem as solving for the fixed point of a "backward flow" operator and show that the fixed point solution gives the desired... | Ali Mousavi, Denny Zhou, Lihong Li, Qiang Liu |  |
| 322 |  |  [PairNorm: Tackling Oversmoothing in GNNs](https://openreview.net/forum?id=rkecl1rtwB) |  | 0 | The performance of graph neural nets (GNNs) is known to gradually decrease with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to quantify oversmoothing. Our main contribution is PairNorm, a novel normalization layer that is based on a careful analysis of the graph convolution operator, which prevents all node embeddings from becoming too similar. What is more, PairNorm is fast, easy to implement without any change to network architecture nor any additional parameters, and is broadly applicable to any GNN. Experiments on real-world graphs demonstrate that... | Leman Akoglu, Lingxiao Zhao |  |
| 323 |  |  [Unsupervised Clustering using Pseudo-semi-supervised Learning](https://openreview.net/forum?id=rJlnxkSYPS) |  | 0 | In this paper, we propose a framework that leverages semi-supervised models to improve unsupervised clustering performance. To leverage semi-supervised models, we first need to automatically generate labels, called pseudo-labels. We find that prior approaches for generating pseudo-labels hurt clustering performance because of their low accuracy. Instead, we use an ensemble of deep networks to construct a similarity graph, from which we extract high accuracy pseudo-labels. The approach of finding high quality pseudo-labels using ensembles and training the semi-supervised model is iterated, yielding continued improvement. We show that our approach outperforms state of the art clustering results for multiple image and text datasets.... | Divam Gupta, Muthian Sivathanu, Nipun Kwatra, Ramachandran Ramjee |  |
| 324 |  |  [Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee](https://openreview.net/forum?id=Hke3gyHYwH) |  | 0 | Over-parameterized deep neural networks trained by simple first-order methods are known to be able to fit any labeling of data. Such over-fitting ability hinders generalization when mislabeled training examples are present. On the other hand, simple regularization methods like early-stopping can often achieve highly nontrivial performance on clean test data in these scenarios, a phenomenon not theoretically understood. This paper proposes and analyzes two simple and intuitive regularization methods: (i) regularization by the distance between the network parameters to initialization, and (ii) adding a trainable auxiliary variable to the network output for each training example. Theoretically, we prove that gradient descent training... | Dingli Yu, Wei Hu, Zhiyuan Li |  |
| 325 |  |  [Controlling generative models with continuous factors of variations](https://openreview.net/forum?id=H1laeJrKDB) |  | 0 | Recent deep generative models can provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent works have shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control... | Antoine Plumerault, Céline Hudelot, Hervé Le Borgne |  |
| 326 |  |  [Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control](https://openreview.net/forum?id=ryxmb1rKDS) |  | 0 | In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system, given by an ordinary differential equation (ODE), from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way, which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even... | Amit Chakraborty, Biswadip Dey, Yaofeng Desmond Zhong |  |
| 327 |  |  [Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness](https://openreview.net/forum?id=SJeY-1BKDS) |  | 0 | Recently, the $\ell^4$-norm maximization has been proposed to solve the sparse dictionary learning (SDL) problem. The simple MSP (matching, stretching, and projection) algorithm proposed by \cite{zhai2019a} has proved surprisingly efficient and effective. This paper aims to better understand this algorithm from its strong geometric and statistical connections with the classic PCA and ICA, as well as their associated fixed-point style algorithms. Such connections provide a unified way of viewing problems that pursue {\em principal}, {\em independent}, or {\em sparse} components of high-dimensional data. Our studies reveal additional good properties of $\ell^4$-maximization: not only is the MSP algorithm for sparse coding insensitive... | Hermish Mehta, Yi Ma, Yuexiang Zhai, Zhengyuan Zhou |  |
| 328 |  |  [Quantum Algorithms for Deep Convolutional Neural Networks](https://openreview.net/forum?id=Hygab1rKDS) |  | 0 | Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and... | Anupam Prakash, Iordanis Kerenidis, Jonas Landman |  |
| 329 |  |  [Self-Supervised Learning of Appliance Usage](https://openreview.net/forum?id=B1lJzyStvS) |  | 0 | Learning home appliance usage is important for understanding people's activities and optimizing energy consumption. The problem is modeled as an event detection task, where the objective is to learn when a user turns an appliance on, and which appliance it is (microwave, hair dryer, etc.). Ideally, we would like to solve the problem in an unsupervised way so that the method can be applied to new homes and new appliances without any labels. To this end, we introduce a new deep learning model that takes input from two home sensors: 1) a smart electricity meter that outputs the total energy consumed by the home as a function of time, and 2) a motion sensor that outputs the locations of the residents over time. The model learns the... | Abbas Zeitoun, ChenYu Hsu, Dina Katabi, GuangHe Lee, Tommi S. Jaakkola |  |
| 330 |  |  [Deep Graph Matching Consensus](https://openreview.net/forum?id=HyeJf1HKvS) |  | 0 | This work presents a two-stage neural architecture for learning and refining structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs... | Christopher Morris, Jan Eric Lenssen, Jonathan Masci, Matthias Fey, Nils M. Kriege |  |
| 331 |  |  [Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks](https://openreview.net/forum?id=rkllGyBFPH) |  | 0 | Recent theoretical work has established connections between over-parametrized neural networks and linearized models governed by the Neural Tangent Kernels (NTKs). NTK theory leads to concrete convergence and generalization results, yet the empirical performance of neural networks are observed to exceed their linearized models, suggesting insufficiency of this theory. Towards closing this gap, we investigate the training of over-parametrized neural networks that are beyond the NTK regime yet still governed by the Taylor expansion of the network. We bring forward the idea of randomizing the neural networks, which allows them to escape their NTK and couple with quadratic models. We show that the optimization landscape of randomized... | Jason D. Lee, Yu Bai |  |
| 332 |  |  [Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers](https://openreview.net/forum?id=SJlbGJrtDB) |  | 0 | We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly ﬁnd the optimal network parameters and sparse network structure in a uniﬁed optimization process with trainable pruning thresholds. These thresholds can have ﬁne-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the... | Hayden KwokHay So, Junjie Liu, Ray C. C. Cheung, Runbin Shi, Zhe Xu |  |
| 333 |  |  [Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference](https://openreview.net/forum?id=rJgzzJHtDB) |  | 0 | Deep networks were recently suggested to face the odds between accuracy (on clean natural images) and robustness (on adversarially perturbed images) (Tsipras et al., 2019). Such a dilemma is shown to be rooted in the inherently higher sample complexity (Schmidt et al., 2018) and/or model capacity (Nakkiran, 2019), for learning a high-accuracy and robust classifier. In view of that, give a classification task, growing the model capacity appears to help draw a win-win between accuracy and robustness, yet at the expense of model size and latency, therefore posing challenges for resource-constrained applications. Is it possible to co-design model accuracy, robustness and efficiency to achieve their triple wins? This paper studies... | Haotao Wang, Tianlong Chen, TingKuei Hu, Zhangyang Wang |  |
| 334 |  |  [Neural Policy Gradient Methods: Global Optimality and Rates of Convergence](https://openreview.net/forum?id=BJgQfkSYDS) |  | 0 | Policy gradient methods with actor-critic schemes demonstrate tremendous empirical successes, especially when the actors and critics are parameterized by neural networks. However, it remains less clear whether such "neural" policy gradient methods converge to globally optimal policies and whether they even converge at all. We answer both the questions affirmatively in the overparameterized regime. In detail, we prove that neural natural policy gradient converges to a globally optimal policy at a sublinear rate. Also, we show that neural vanilla policy gradient converges sublinearly to a stationary point. Meanwhile, by relating the suboptimality of the stationary points to the~representation power of neural actor and critic classes,... | Lingxiao Wang, Qi Cai, Zhaoran Wang, Zhuoran Yang |  |
| 335 |  |  [Double Neural Counterfactual Regret Minimization](https://openreview.net/forum?id=ByedzkrKvH) |  | 0 | Counterfactual regret minimization (CFR) is a fundamental and effective technique for solving Imperfect Information Games (IIG). However, the original CFR algorithm only works for discrete states and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games. In this paper, we propose a double neural representation for the IIGs, where one neural network represents the cumulative regret, and the other represents the average strategy. Such neural representations allow us to avoid manual game abstraction and carry out end-to-end optimization. To make the learning efficient, we also developed several novel techniques... | Hui Li, Kailiang Hu, Le Song, Shaohua Zhang, Yuan Qi |  |
| 336 |  |  [GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation](https://openreview.net/forum?id=S1esMkHYPr) |  | 0 | Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking.... | Chence Shi, Jian Tang, Ming Zhang, Minkai Xu, Weinan Zhang, Zhaocheng Zhu |  |
| 337 |  |  [The Gambler's Problem and Beyond](https://openreview.net/forum?id=HyxnMyBKwB) |  | 0 | We analyze the Gambler's problem, a simple reinforcement learning problem where the gambler has the chance to double or lose their bets until the target is reached. This is an early example introduced in the reinforcement learning textbook by Sutton and Barto (2018), where they mention an interesting pattern of the optimal value function with high-frequency components and repeating non-smooth points. It is however without further investigation. We provide the exact formula for the optimal value function for both the discrete and the continuous cases. Though simple as it might seem, the value function is pathological: fractal, self-similar, derivative taking either zero or infinity, not smooth on any interval, and not written as... | Baoxiang Wang, Jiajin Li, Shuai Li, Siu On Chan |  |
| 338 |  |  [Multilingual Alignment of Contextual Word Representations](https://openreview.net/forum?id=r1xCMyBtPS) |  | 0 | We propose procedures for evaluating and strengthening contextual embedding alignment and show that they are useful in analyzing and improving multilingual BERT. In particular, after our proposed alignment procedure, BERT exhibits significantly improved zero-shot performance on XNLI compared to the base model, remarkably matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. Further, to measure the degree of alignment, we introduce a contextual version of word retrieval and show that it correlates well with downstream zero-shot transfer. Using this word retrieval task, we also analyze BERT and find that it exhibits systematic deficiencies, e.g. worse alignment for open-class parts-of-speech and word pairs... | Dan Klein, Nikita Kitaev, Steven Cao |  |
| 339 |  |  [The Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=rygGQyrFvH) |  | 0 | Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text... | Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi |  |
| 340 |  |  [Graph Convolutional Reinforcement Learning](https://openreview.net/forum?id=HkxdQkSYDB) |  | 0 | Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn... | Chen Dun, Jiechuan Jiang, Tiejun Huang, Zongqing Lu |  |
| 341 |  |  [Meta-Learning Deep Energy-Based Memory Models](https://openreview.net/forum?id=SyljQyBFDH) |  | 0 | We study the problem of learning an associative memory model -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast. Thus, most research in energy-based memory has... | Jack W. Rae, Sergey Bartunov, Simon Osindero, Timothy P. Lillicrap |  |
| 342 |  |  [Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning](https://openreview.net/forum?id=rkl3m1BFDB) |  | 0 | Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsifiable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather... | Akanksha Atrey, David D. Jensen, Kaleigh Clary |  |
| 343 |  |  [Fast Neural Network Adaptation via Parameter Remapping and Architecture Search](https://openreview.net/forum?id=rklTmyBKPH) |  | 0 | Deep neural networks achieve remarkable performance in many computer vision tasks. Most state-of-the-art~(SOTA) semantic segmentation and object detection approaches reuse neural network architectures designed for image classification as the backbone, commonly pre-trained on ImageNet. However, performance gains can be achieved by designing network architectures specifically for detection and segmentation, as shown by recent neural architecture search (NAS) research for detection and segmentation. One major challenge though, is that ImageNet pre-training of the search space representation (a.k.a. super network) or the searched networks incurs huge computational cost. In this paper, we propose a Fast Neural Network Adaptation (FNA)... | Jiemin Fang, Kangjian Peng, Qian Zhang, Wenyu Liu, Xinggang Wang, Yuan Li, Yuzhu Sun |  |
| 344 |  |  [Guiding Program Synthesis by Learning to Generate Examples](https://openreview.net/forum?id=BJl07ySKvS) |  | 0 | A key challenge of existing program synthesizers is ensuring that the synthesized program generalizes well. This can be difficult to achieve as the specification provided by the end user is often limited, containing as few as one or two input-output examples. In this paper we address this challenge via an iterative approach that finds ambiguities in the provided specification and learns to resolve these by generating additional input-output examples. The main insight is to reduce the problem of selecting which program generalizes well to the simpler task of deciding which output is correct. As a result, to train our probabilistic models, we can take advantage of the large amounts of data in the form of program outputs, which are... | Larissa Laich, Martin T. Vechev, Pavol Bielik |  |
| 345 |  |  [SNODE: Spectral Discretization of Neural ODEs for System Identification](https://openreview.net/forum?id=Sye0XkBKvS) |  | 0 | This paper proposes the use of spectral element methods \citep{canuto_spectral_1988} for fast and accurate training of Neural Ordinary Differential Equations (ODE-Nets; \citealp{Chen2018NeuralOD}) for system identification. This is achieved by expressing their dynamics as a truncated series of Legendre polynomials. The series coefficients, as well as the network weights, are computed by minimizing the weighted sum of the loss function and the violation of the ODE-Net dynamics. The problem is solved by coordinate descent that alternately minimizes, with respect to the coefficients and the weights, two unconstrained sub-problems using standard backpropagation and gradient methods. The resulting optimization scheme is fully... | Alessio Quaglino, Jan Koutník, Jonathan Masci, Marco Gallieri |  |
| 346 |  |  [Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition](https://openreview.net/forum?id=H1lxVyStPH) |  | 0 | When constructing random forests, it is of prime importance to ensure high accuracy and low correlation of individual tree classifiers for good performance. Nevertheless, it is typically difficult for existing random forest methods to strike a good balance between these conflicting factors. In this work, we propose a generalized convolutional forest networks to learn a feature space to maximize the strength of individual tree classifiers while minimizing the respective correlation. The feature space is iteratively constructed by a probabilistic triplet sampling method based on the distribution obtained from the splits of the random forest. The sampling process is designed to pull the data of the same label together for higher... | Gitaek Kwon, Jongbin Ryu, Jongwoo Lim, MingHsuan Yang |  |
| 347 |  |  [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://openreview.net/forum?id=HylxE1HKwS) |  | 0 | We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel... | Chuang Gan, Han Cai, Song Han, Tianzhe Wang, Zhekai Zhang |  |
| 348 |  |  [Multi-Agent Interactions Modeling with Correlated Policies](https://openreview.net/forum?id=B1gZV1HYvS) |  | 0 | In multi-agent systems, complex interacting behaviors arise due to the high correlations among agents. However, previous work on modeling multi-agent interactions from demonstrations is primarily constrained by assuming the independence among policies and their reward structures. In this paper, we cast the multi-agent interactions modeling problem into a multi-agent imitation learning framework with explicit modeling of correlated policies by approximating opponents’ policies, which can recover agents' policies that can regenerate similar interactions. Consequently, we develop a Decentralized Adversarial Imitation Learning algorithm with Correlated policies (CoDAIL), which allows for decentralized training and execution. Various... | Jun Wang, Ming Zhou, Minghuan Liu, Weinan Zhang, Wulong Liu, Yong Yu, Yuzheng Zhuang |  |
| 349 |  |  [PCMC-Net: Feature-based Pairwise Choice Markov Chains](https://openreview.net/forum?id=BJgWE1SFwS) |  | 0 | Pairwise Choice Markov Chains (PCMC) have been recently introduced to overcome limitations of choice models based on traditional axioms unable to express empirical observations from modern behavior economics like context effects occurring when a choice between two options is altered by adding a third alternative. The inference approach that estimates the transition rates between each possible pair of alternatives via maximum likelihood suffers when the examples of each alternative are scarce and is inappropriate when new alternatives can be observed at test time. In this work, we propose an amortized inference approach for PCMC by embedding its definition into a neural network that represents transition rates as a function of the... | Alix Lhéritier |  |
| 350 |  |  [Implementing Inductive bias for different navigation tasks through diverse RNN attrractors](https://openreview.net/forum?id=Byx4NkrtDS) |  | 0 | Navigation is crucial for animal behavior and is assumed to require an internal representation of the external environment, termed a cognitive map. The precise form of this representation is often considered to be a metric representation of space. An internal representation, however, is judged by its contribution to performance on a given task, and may thus vary between different types of navigation tasks. Here we train a recurrent neural network that controls an agent performing several navigation tasks in a simple environment. To focus on internal representations, we split learning into a task-agnostic pre-training stage that modifies internal connectivity and a task-specific Q learning stage that controls the network's output.... | Omri Barak, Tie Xu |  |
| 351 |  |  [Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings](https://openreview.net/forum?id=BJgr4kSFDS) |  | 0 | Answering complex logical queries on large-scale incomplete knowledge graphs (KGs) is a fundamental yet challenging task. Recently, a promising approach to this problem has been to embed KG entities as well as the query into a vector space such that entities that answer the query are embedded close to the query. However, prior work models queries as single points in the vector space, which is problematic because a complex query represents a potentially large set of its answer entities, but it is unclear how such a set can be represented as a single point. Furthermore, prior work can only handle queries that use conjunctions ($\wedge$) and existential quantifiers ($\exists$). Handling queries with logical disjunctions ($\vee$)... | Hongyu Ren, Jure Leskovec, Weihua Hu |  |
| 352 |  |  [Rethinking the Hyperparameters for Fine-tuning](https://openreview.net/forum?id=B1g8VkHFPH) |  | 0 | Fine-tuning from pre-trained ImageNet models has become the de-facto standard for various computer vision tasks. Current practices for fine-tuning typically involve selecting an ad-hoc choice of hyperparameters and keeping them fixed to values normally used for training from scratch. This paper re-examines several common practices of setting hyperparameters for fine-tuning. Our findings are based on extensive empirical evaluation for fine-tuning on various transfer learning benchmarks. (1) While prior works have thoroughly investigated learning rate and batch size, momentum for fine-tuning is a relatively unexplored parameter. We find that the value of momentum also affects fine-tuning performance and connect it with previous... | Avinash Ravichandran, Hao Li, Hao Yang, Michael Lam, Pratik Chaudhari, Rahul Bhotika, Stefano Soatto |  |
| 353 |  |  [Plug and Play Language Models: A Simple Approach to Controlled Text Generation](https://openreview.net/forum?id=H1edEyBKDS) |  | 0 | Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of... | Andrea Madotto, Eric Frank, Jane Hung, Janice Lan, Jason Yosinski, Piero Molino, Rosanne Liu, Sumanth Dathathri |  |
| 354 |  |  [Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks](https://openreview.net/forum?id=rkgqN1SYvr) |  | 0 | The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence... | Jeffrey Pennington, Lechao Xiao, Wei Hu |  |
| 355 |  |  [RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis](https://openreview.net/forum?id=HyxjNyrtPr) |  | 0 | Understanding three-dimensional (3D) geometries from two-dimensional (2D) images without any labeled information is promising for understanding the real world without incurring annotation cost. We herein propose a novel generative model, RGBD-GAN, which achieves unsupervised 3D representation learning from 2D images. The proposed method enables camera parameter--conditional image generation and depth image generation without any 3D annotations, such as camera poses or depth. We use an explicit 3D consistency loss for two RGBD images generated from different camera parameters, in addition to the ordinal GAN objective. The loss is simple yet effective for any type of image generator such as DCGAN and StyleGAN to be conditioned on... | Atsuhiro Noguchi, Tatsuya Harada |  |
| 356 |  |  [Towards Verified Robustness under Text Deletion Interventions](https://openreview.net/forum?id=SyxhVkrYvr) |  | 0 | Neural networks are widely used in Natural Language Processing, yet despite their empirical successes, their behaviour is brittle: they are both over-sensitive to small input changes, and under-sensitive to deletions of large fractions of input text. This paper aims to tackle under-sensitivity in the context of natural language inference by ensuring that models do not become more confident in their predictions as arbitrary subsets of words from the input text are deleted. We develop a novel technique for formal verification of this specification for models based on the popular decomposable attention mechanism by employing the efficient yet effective interval bound propagation (IBP) approach. Using this method we can efficiently... | Johannes Welbl, Krishnamurthy (Dj) Dvijotham, Martin Szummer, PoSen Huang, Pushmeet Kohli, Robert Stanforth, Sven Gowal |  |
| 357 |  |  [Jacobian Adversarially Regularized Networks for Robustness](https://openreview.net/forum?id=Hke0V1rKPS) |  | 0 | Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially... | Alvin Chan, Jie Fu, YewSoon Ong, Yi Tay |  |
| 358 |  |  [Thinking While Moving: Deep Reinforcement Learning with Concurrent Control](https://openreview.net/forum?id=SJexHkSFPS) |  | 0 | We study reinforcement learning in settings where sampling an action from the policy must be done concurrently with the time evolution of the controlled system, such as when a robot must decide on the next action while still performing the previous action. Much like a person or an animal, the robot must think and move at the same time, deciding on its next action before the previous one has completed. In order to develop an algorithmic framework for such concurrent control problems, we start with a continuous-time formulation of the Bellman equations, and then discretize them in a way that is aware of system delays. We instantiate this new class of approximate dynamic programming methods via a simple architectural extension to... | Alexander Herzog, Dmitry Kalashnikov, Eric Jang, Julian Ibarz, Karol Hausman, Sergey Levine, Ted Xiao |  |
| 359 |  |  [Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=SJxbHkrKDH) |  | 0 | In multi-agent games, the complexity of the environment can grow exponentially as the number of agents increases, so it is particularly challenging to learn good policies when the agent population is large. In this paper, we introduce Evolutionary Population Curriculum (EPC), a curriculum learning paradigm that scales up Multi-Agent Reinforcement Learning (MARL) by progressively increasing the population of training agents in a stage-wise manner. Furthermore, EPC uses an evolutionary approach to fix an objective misalignment issue throughout the curriculum: agents successfully trained in an early stage with a small population are not necessarily the best candidates for adapting to later stages with scaled populations. Concretely,... | Abhinav Gupta, Fei Fang, Qian Long, Xiaolong Wang, Yi Wu, Zihan Zhou |  |
| 360 |  |  [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB) |  | 0 | Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the... | Christopher D. Manning, Kevin Clark, MinhThang Luong, Quoc V. Le |  |
| 361 |  |  [Environmental drivers of systematicity and generalization in a situated agent](https://openreview.net/forum?id=SklGryBtwr) |  | 0 | The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. We first describe a comparatively generic agent architecture that exhibits strong performance on these tests. We then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame... | Adam Santoro, Andrew K. Lampinen, Felix Hill, James L. McClelland, Matthew M. Botvinick, Rosalia Schneider, Stephen Clark |  |
| 362 |  |  [Abstract Diagrammatic Reasoning with Multiplex Graph Networks](https://openreview.net/forum?id=ByxQB1BKwH) |  | 0 | Abstract reasoning, particularly in the visual domain, is a complex human ability, but it remains a challenging problem for artificial neural learning systems. In this work we propose MXGNet, a multilayer graph neural network for multi-panel diagrammatic reasoning tasks. MXGNet combines three powerful concepts, namely, object-level representation, graph neural networks and multiplex graphs, for solving visual reasoning tasks. MXGNet first extracts object-level representations for each element in all panels of the diagrams, and then forms a multi-layer multiplex graph capturing multiple relations between objects across different diagram panels. MXGNet summarises the multiple graphs extracted from the diagrams of the task, and uses... | Duo Wang, Mateja Jamnik, Pietro Liò |  |
| 363 |  |  [A Baseline for Few-Shot Image Classification](https://openreview.net/forum?id=rylXBkrYDS) |  | 0 | Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We... | Avinash Ravichandran, Guneet Singh Dhillon, Pratik Chaudhari, Stefano Soatto |  |
| 364 |  |  [Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering](https://openreview.net/forum?id=SJgVHkrYDH) |  | 0 | Answering questions that require multi-hop reasoning at web-scale necessitates retrieving multiple evidence documents, one of which often has little lexical or semantic relationship to the question. This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. Our retriever model trains a recurrent neural network that learns to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on the previously retrieved documents. Our reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results show state-of-the-art results in three... | Akari Asai, Caiming Xiong, Hannaneh Hajishirzi, Kazuma Hashimoto, Richard Socher |  |
| 365 |  |  [Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks](https://openreview.net/forum?id=BJlBSkHtDS) |  | 0 | The performance of deep network learning strongly depends on the choice of the non-linear activation function associated with each neuron. However, deciding on the best activation is non-trivial and the choice depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation functions by using flexible parametric rational functions instead. The resulting Padé Activation Units (PAUs) can both approximate common activation functions and also learn new ones while providing compact representations. Our empirical evidence shows that end-to-end learning deep networks with PAUs can... | Alejandro Molina, Kristian Kersting, Patrick Schramowski |  |
| 366 |  |  [A Framework for robustness Certification of Smoothed Classifiers using F-Divergences](https://openreview.net/forum?id=SJlKrkSFPH) |  | 0 | Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far require knowledge of the architecture of the machine learning model and remain hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like... | András György, Borja Balle, Chongli Qin, J. Zico Kolter, Jamie Hayes, Kai Xiao, Krishnamurthy (Dj) Dvijotham, Pushmeet Kohli, Sven Gowal |  |
| 367 |  |  [Contrastive Representation Distillation](https://openreview.net/forum?id=SkgpBJrtvS) |  | 0 | Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive... | Dilip Krishnan, Phillip Isola, Yonglong Tian |  |
| 368 |  |  [Certified Defenses for Adversarial Patches](https://openreview.net/forum?id=HyeaSkrYPH) |  | 0 | Adversarial patch attacks are among one of the most practical threat models against real-world computer vision systems. This paper studies certified and empirical defenses against patch attacks. We begin with a set of experiments showing that most existing defenses, which work by pre-processing input images to mitigate adversarial patches, are easily broken by simple white-box adversaries. Motivated by this finding, we propose the first certified defense against patch attacks, and propose faster methods for its training. Furthermore, we experiment with different patch shapes for testing, obtaining surprisingly good robustness transfer across shapes, and present preliminary results on certified defense against sparse attacks. Our... | Ahmed Abdelkader, Chen Zhu, Christoph Studer, Pingyeh Chiang, Renkun Ni, Tom Goldstein |  |
| 369 |  |  [Sample Efficient Policy Gradient Methods with Recursive Variance Reduction](https://openreview.net/forum?id=HJlxIJBFDr) |  | 0 | Improving the sample efficiency in reinforcement learning has been a long-standing research problem. In this work, we aim to reduce the sample complexity of existing policy gradient methods. We propose a novel policy gradient algorithm called SRVR-PG, which only requires $O(1/\epsilon^{3/2})$\footnote{$O(\cdot)$ notation hides constant factors.} episodes to find an $\epsilon$-approximate stationary point of the nonconcave performance function $J(\boldsymbol{\theta})$ (i.e., $\boldsymbol{\theta}$ such that $\\|\nabla J(\boldsymbol{\theta})\\|_2^2\leq\epsilon$). This sample complexity improves the existing result $O(1/\epsilon^{5/3})$ for stochastic variance reduced policy gradient algorithms by a factor of $O(1/\epsilon^{1/6})$. In... | Felicia Gao, Pan Xu, Quanquan Gu |  |
| 370 |  |  [Deep Symbolic Superoptimization Without Human Knowledge](https://openreview.net/forum?id=r1egIyBFPS) |  | 0 | Deep symbolic superoptimization refers to the task of applying deep learning methods to simplify symbolic expressions. Existing approaches either perform supervised training on human-constructed datasets that defines equivalent expression pairs, or apply reinforcement learning with human-defined equivalent trans-formation actions. In short, almost all existing methods rely on human knowledge to define equivalence, which suffers from large labeling cost and learning bias, because it is almost impossible to define and comprehensive equivalent set. We thus propose HISS, a reinforcement learning framework for symbolic super-optimization that keeps human outside the loop. HISS introduces a tree-LSTM encoder-decoder network with... | Hui Shi, Jishen Zhao, Xinyun Chen, Yang Zhang, Yuandong Tian |  |
| 371 |  |  [Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution](https://openreview.net/forum?id=SJgzLkBKPB) |  | 0 | As deep reinforcement learning (RL) is applied to more tasks, there is a need to visualize and understand the behavior of learned agents. Saliency maps explain agent behavior by highlighting the features of the input state that are most relevant for the agent in taking an action. Existing perturbation-based approaches to compute saliency often highlight regions of the input that are not relevant to the action taken by the agent. Our proposed approach, SARFA (Specific and Relevant Feature Attribution), generates more focused saliency maps by balancing two aspects (specificity and relevance) that capture different desiderata of saliency. The first captures the impact of perturbation on the relative expected reward of the action to be... | Balaji Krishnamurthy, Dhruv Kayastha, Nikaash Puri, Piyush Gupta, Sameer Singh, Shripad V. Deshmukh, Sukriti Verma |  |
| 372 |  |  [Universal Approximation with Certified Networks](https://openreview.net/forum?id=B1gX8kBtPr) |  | 0 | Training neural networks to be certifiably robust is critical to ensure their safety against adversarial attacks. However, it is currently very difficult to train a neural network that is both accurate and certifiably robust. In this work we take a step towards addressing this challenge. We prove that for every continuous function $f$, there exists a network $n$ such that: (i) $n$ approximates $f$ arbitrarily close, and (ii) simple interval bound propagation of a region $B$ through $n$ yields a result that is arbitrarily close to the optimal output of $f$ on $B$. Our result can be seen as a Universal Approximation Theorem for interval-certified ReLU networks. To the best of our knowledge, this is the first work to prove the... | Martin T. Vechev, Matthew Mirman, Maximilian Baader |  |
| 373 |  |  [Measuring and Improving the Use of Graph Information in Graph Neural Networks](https://openreview.net/forum?id=rkeIIkHKvS) |  | 0 | Graph neural networks (GNNs) have been widely used for representation learning on graph data. However, there is limited understanding on how much performance GNNs actually gain from graph data. This paper introduces a context-surrounding GNN framework and proposes two smoothness metrics to measure the quantity and quality of information obtained from graph data. A new, improved GNN model, called CS-GNN, is then devised to improve the use of graph information based on the smoothness values of a graph. CS-GNN is shown to achieve better performance than existing methods in different types of real graphs. | Hongzhi Chen, James Cheng, Jie Zhang, Kaili Ma, MingChang Yang, Richard T. B. Ma, Yifan Hou |  |
| 374 |  |  [State-only Imitation with Transition Dynamics Mismatch](https://openreview.net/forum?id=HJgLLyrYwB) |  | 0 | Imitation Learning (IL) is a popular paradigm for training agents to achieve complicated goals by leveraging expert behavior, rather than dealing with the hardships of designing a correct reward function. With the environment modeled as a Markov Decision Process (MDP), most of the existing IL algorithms are contingent on the availability of expert demonstrations in the same MDP as the one in which a new imitator policy is to be learned. This is uncharacteristic of many real-life scenarios where discrepancies between the expert and the imitator MDPs are common, especially in the transition dynamics function. Furthermore, obtaining expert actions may be costly or infeasible, making the recent trend towards state-only IL (where expert... | Jian Peng, Tanmay Gangwani |  |
| 375 |  |  [Adversarial AutoAugment](https://openreview.net/forum?id=ByxdUySKvS) |  | 0 | Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment (Cubuk et al., 2019) can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation... | Jian Zhang, Qiang Wang, Xinyu Zhang, Zhao Zhong |  |
| 376 |  |  [Meta Dropout: Learning to Perturb Latent Features for Generalization](https://openreview.net/forum?id=BJgd81SYwr) |  | 0 | A machine learning model that generalizes well should obtain low errors on unseen test examples. Thus, if we know how to optimally perturb training examples to account for test examples, we may achieve better generalization performance. However, obtaining such perturbation is not possible in standard machine learning frameworks as the distribution of the test data is unknown. To tackle this challenge, we propose a novel regularization method, meta-dropout, which learns to perturb the latent features of training examples for generalization in a meta-learning framework. Specifically, we meta-learn a noise generator which outputs a multiplicative noise distribution for latent features, to obtain low errors on the test instances in an... | Eunho Yang, Haebeom Lee, Sung Ju Hwang, Taewook Nam |  |
| 377 |  |  [Rényi Fair Inference](https://openreview.net/forum?id=HkgsUJrtDB) |  | 0 | Machine learning algorithms have been increasingly deployed in critical automated decision-making systems that directly affect human lives. When these algorithms are solely trained to minimize the training/test error, they could suffer from systematic discrimination against individuals based on their sensitive attributes, such as gender or race. Recently, there has been a surge in machine learning society to develop algorithms for fair machine learning. In particular, several adversarial learning procedures have been proposed to impose fairness. Unfortunately, these algorithms either can only impose fairness up to linear dependence between the variables, or they lack computational convergence guarantees. In this paper, we use Rényi... | Ahmad Beirami, Maher Nouiehed, Meisam Razaviyayn, Sina Baharlouei |  |
| 378 |  |  [Learning transport cost from subset correspondence](https://openreview.net/forum?id=SJlRUkrFPS) |  | 0 | Learning to align multiple datasets is an important problem with many applications, and it is especially useful when we need to integrate multiple experiments or correct for confounding. Optimal transport (OT) is a principled approach to align datasets, but a key challenge in applying OT is that we need to specify a cost function that accurately captures how the two datasets are related. Reliable cost functions are typically not available and practitioners often resort to using hand-crafted or Euclidean cost even if it may not be appropriate. In this work, we investigate how to learn the cost function using a small amount of side information which is often available. The side information we consider captures subset... | Akshay Balsubramani, James Zou, Ruishan Liu |  |
| 379 |  |  [BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget](https://openreview.net/forum?id=SklkDkSFPB) |  | 0 | The desire to map neural networks to varying-capacity devices has led to the development of a wealth of compression techniques, many of which involve replacing standard convolutional blocks in a large network with cheap alternative blocks. However, not all blocks are created equally; for a required compute budget there may exist a potent combination of many different cheap blocks, though exhaustively searching for such a combination is prohibitively expensive. In this work, we develop BlockSwap: a fast algorithm for choosing networks with interleaved block types by passing a single minibatch of training data through randomly initialised networks and gauging their Fisher potential. These networks can then be used as students and... | Amos J. Storkey, Elliot J. Crowley, Gavin Gray, Jack Turner, Michael F. P. O'Boyle |  |
| 380 |  |  [Variance Reduction With Sparse Gradients](https://openreview.net/forum?id=Syx1DkSYwB) |  | 0 | Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm... | Lihua Lei, Melih Elibol, Michael I. Jordan |  |
| 381 |  |  [Abductive Commonsense Reasoning](https://openreview.net/forum?id=Byg1v1HKDB) |  | 0 | Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense... | Ari Holtzman, Chaitanya Malaviya, Chandra Bhagavatula, Doug Downey, Hannah Rashkin, Keisuke Sakaguchi, Ronan Le Bras, Wentau Yih, Yejin Choi |  |
| 382 |  |  [Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth](https://openreview.net/forum?id=Byg-wJSYDS) |  | 0 | In most machine learning tasks unambiguous ground truth labels can easily be acquired. However, this luxury is often not afforded to many high-stakes, real-world scenarios such as medical image interpretation, where even expert human annotators typically exhibit very high levels of disagreement with one another. While prior works have focused on overcoming noisy labels during training, the question of how to evaluate models when annotators disagree about ground truth has remained largely unexplored. To address this, we propose the discrepancy ratio: a novel, task-independent and principled framework for validating machine learning models in the presence of high label noise. Conceptually, our approach evaluates a model by comparing... | Alon Daks, Ardavan Saeedi, Ben Sternlieb, Igor Lovchinsky, Israel Malkin, Nathan Silberman, Patrick Maher, Pouya Samangouei, Swami Sankaranarayanan, Tomer Gafner, Yang Liu |  |
| 383 |  |  [Weakly Supervised Disentanglement with Guarantees](https://openreview.net/forum?id=HJgSwyBKvr) |  | 0 | Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and... | Abhishek Kumar, Ben Poole, Rui Shu, Stefano Ermon, Yining Chen |  |
| 384 |  |  [Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks](https://openreview.net/forum?id=SJlHwkBYDH) |  | 0 | Deep learning models are vulnerable to adversarial examples crafted by applying human-imperceptible perturbations on benign inputs. However, under the black-box setting, most existing adversaries often have a poor transferability to attack other defense models. In this work, from the perspective of regarding the adversarial example generation as an optimization process, we propose two new methods to improve the transferability of adversarial examples, namely Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM aims to adapt Nesterov accelerated gradient into the iterative attacks so as to effectively look ahead and improve the transferability of adversarial examples. While SIM is... | Chuanbiao Song, Jiadong Lin, John E. Hopcroft, Kun He, Liwei Wang |  |
| 385 |  |  [Fantastic Generalization Measures and Where to Find Them](https://openreview.net/forum?id=SJgIPJBFvH) |  | 0 | Generalization of deep networks has been intensely researched in recent years, resulting in a number of theoretical bounds and empirically motivated measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether these measures are truly useful in practice. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand CIFAR-10 networks with systematic changes in important hyper-parameters. We attempt to uncover potential causal relationships between each measure and generalization, by using rank correlation coefficient and its modified forms. We analyze the results and show that some of the studied measures are... | Behnam Neyshabur, Dilip Krishnan, Hossein Mobahi, Samy Bengio, Yiding Jiang |  |
| 386 |  |  [Robustness Verification for Transformers](https://openreview.net/forum?id=BJxwPJHFwS) |  | 0 | Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding model behavior and obtaining safety guarantees. However, previous methods can usually only handle neural networks with relatively simple architectures. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous works. We resolve these challenges and develop the first robustness verification algorithm for Transformers. The certified robustness bounds computed by our method are... | ChoJui Hsieh, Huan Zhang, KaiWei Chang, Minlie Huang, Zhouxing Shi |  |
| 387 |  |  [Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning](https://openreview.net/forum?id=HJgcvJBFvB) |  | 0 | Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across... | Honglak Lee, Jinwoo Shin, Kibok Lee, Kimin Lee |  |
| 388 |  |  [Tensor Decompositions for Temporal Knowledge Base Completion](https://openreview.net/forum?id=rke2P1BFwS) |  | 0 | Most algorithms for representation learning and link prediction in relational data have been designed for static data. However, the data they are applied to usually evolves with time, such as friend graphs in social networks or user interactions with items in recommender systems. This is also the case for knowledge bases, which contain facts such as (US, has president, B. Obama, [2009-2017]) that are valid only at certain points in time. For the problem of link prediction under temporal constraints, i.e., answering queries of the form (US, has president, ?, 2012), we propose a solution inspired by the canonical decomposition of tensors of order 4. We introduce new regularization schemes and present an extension of ComplEx that... | Guillaume Obozinski, Nicolas Usunier, Timothée Lacroix |  |
| 389 |  |  [On Universal Equivariant Set Networks](https://openreview.net/forum?id=HkxTwkrKDB) |  | 0 | Using deep neural networks that are either invariant or equivariant to permutations in order to learn functions on unordered sets has become prevalent. The most popular, basic models are DeepSets (Zaheer et al. 2017) and PointNet (Qi et al. 2017). While known to be universal for approximating invariant functions, DeepSets and PointNet are not known to be universal when approximating equivariant set functions. On the other hand, several recent equivariant set architectures have been proven equivariant universal (Sannai et al. 2019, Keriven and Peyre 2019), however these models either use layers that are not permutation equivariant (in the standard sense) and/or use higher order tensor variables which are less practical. There is,... | Nimrod Segol, Yaron Lipman |  |
| 390 |  |  [Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$](https://openreview.net/forum?id=rklk_ySYPB) |  | 0 | In recent years several adversarial attacks and defenses have been proposed. Often seemingly robust models turn out to be non-robust when more sophisticated attacks are used. One way out of this dilemma are provable robustness guarantees. While provably robust models for specific $l_p$-perturbation models have been developed, we show that they do not come with any guarantee against other $l_q$-perturbations. We propose a new regularization scheme, MMR-Universal, for ReLU networks which enforces robustness wrt $l_1$- \textit{and} $l_\infty$-perturbations and show how that leads to the first provably robust models wrt any $l_p$-norm for $p\geq 1$. | Francesco Croce, Matthias Hein |  |
| 391 |  |  [Don't Use Large Mini-batches, Use Local SGD](https://openreview.net/forum?id=B1eyO1BFPr) |  | 0 | Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well, i.e. they do not show good accuracy on new data. As a remedy, we propose a \emph{post-local} SGD and show that it significantly improves the generalization performance compared to large-batch training on standard benchmarks while enjoying the same efficiency (time-to-accuracy) and scalability. We further provide an extensive study of the communication efficiency vs. performance trade-offs associated... | Kumar Kshitij Patel, Martin Jaggi, Sebastian U. Stich, Tao Lin |  |
| 392 |  |  [Kernel of CycleGAN as a principal homogeneous space](https://openreview.net/forum?id=B1eWOJHKvB) |  | 0 | Unpaired image-to-image translation has attracted significant interest due to the invention of CycleGAN, a method which utilizes a combination of adversarial and cycle consistency losses to avoid the need for paired data. It is known that the CycleGAN problem might admit multiple solutions, and our goal in this paper is to analyze the space of exact solutions and to give perturbation bounds for approximate solutions. We show theoretically that the exact solution space is invariant with respect to automorphisms of the underlying probability spaces, and, furthermore, that the group of automorphisms acts freely and transitively on the space of exact solutions. We examine the case of zero pure CycleGAN loss first in its generality,... | Jonas Adler, Jonas Teuwen, Nikita Moriakov |  |
| 393 |  |  [Distributionally Robust Neural Networks](https://openreview.net/forum?id=ryxGuJrFvS) |  | 0 | Overparameterized neural networks can be highly accurate on average on an i.i.d. test set, yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group... | Pang Wei Koh, Percy Liang, Shiori Sagawa, Tatsunori B. Hashimoto |  |
| 394 |  |  [On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach](https://openreview.net/forum?id=Hkx7_1rKwS) |  | 0 | Many tasks in modern machine learning can be formulated as finding equilibria in sequential games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose Follow-the-Ridge (FR), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and... | Guodong Zhang, Jimmy Ba, Yuanhao Wang |  |
| 395 |  |  [A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning](https://openreview.net/forum?id=SJxSOJStPr) |  | 0 | Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture... | Dongsu Zhang, Gunhee Kim, Junsoo Ha, Soochan Lee |  |
| 396 |  |  [Hyper-SAGNN: a self-attention based graph neural network for hypergraphs](https://openreview.net/forum?id=ryeHuJBtPH) |  | 0 | Graph representation learning for hypergraphs can be utilized to extract patterns among higher-order interactions that are critically important in many real world problems. Current approaches designed for hypergraphs, however, are unable to handle different types of hypergraphs and are typically not generic for various learning tasks. Indeed, models that can predict variable-sized heterogeneous hyperedges have not been available. Here we develop a new self-attention based graph neural network called Hyper-SAGNN applicable to homogeneous and heterogeneous hypergraphs with variable hyperedge sizes. We perform extensive evaluations on multiple datasets, including four benchmark network datasets and two single-cell Hi-C datasets in... | Jian Ma, Ruochi Zhang, Yuesong Zou |  |
| 397 |  |  [Neural Epitome Search for Architecture-Agnostic Network Compression](https://openreview.net/forum?id=HyxjOyrKvr) |  | 0 | Traditional compression methods including network pruning, quantization, low rank factorization and knowledge distillation all assume that network architectures and parameters should be hardwired. In this work, we propose a new perspective on network compression, i.e., network parameters can be disentangled from the architectures. From this viewpoint, we present the Neural Epitome Search (NES), a new neural network compression approach that learns to find compact yet expressive epitomes for weight parameters of a specified network architecture end-to-end. The complete network to compress can be generated from the learned epitome via a novel transformation method that adaptively transforms the epitomes to match shapes of the given... | Daquan Zhou, Jianchao Yang, Jiashi Feng, Kaixin Wang, Qibin Hou, Xiaojie Jin |  |
| 398 |  |  [On the Equivalence between Positional Node Embeddings and Structural Graph Representations](https://openreview.net/forum?id=SJxzFySKwH) |  | 0 | This work provides the first unifying theoretical framework for node (positional) embeddings and structural graph representations, bridging methods like matrix factorization and graph neural networks. Using invariant theory, we show that relationship between structural representations and node embeddings is analogous to that of a distribution and its samples. We prove that all tasks that can be performed by node embeddings can also be performed by structural representations and vice-versa. We also show that the concept of transductive and inductive learning is unrelated to node embeddings and graph representations, clearing another source of confusion in the literature. Finally, we introduce new practical guidelines to generating... | Balasubramaniam Srinivasan, Bruno Ribeiro |  |
| 399 |  |  [Probability Calibration for Knowledge Graph Embedding Models](https://openreview.net/forum?id=S1g8K1BFwS) |  | 0 | Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression... | Luca Costabello, Pedro Tabacof |  |
| 400 |  |  [Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks](https://openreview.net/forum?id=BylsKkHYvH) |  | 0 | Handling missing data is one of the most fundamental problems in machine learning. Among many approaches, the simplest and most intuitive way is zero imputation, which treats the value of a missing entry simply as zero. However, many studies have experimentally confirmed that zero imputation results in suboptimal performances in training neural networks. Yet, none of the existing work has explained what brings such performance degradations. In this paper, we introduce the variable sparsity problem (VSP), which describes a phenomenon where the output of a predictive model largely varies with respect to the rate of missingness in the given input, and show that it adversarially affects the model performance. We first theoretically... | Eunho Yang, Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang |  |
| 401 |  |  [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) |  | 0 | Over-fitting and over-smoothing are two main obstacles of developing deep Graph Convolutional Networks (GCNs) for node classification. In particular, over-fitting weakens the generalization ability on small dataset, while over-smoothing impedes model training by isolating output representations from the input features with the increase in network depth. This paper proposes DropEdge, a novel and flexible technique to alleviate both issues. At its core, DropEdge randomly removes a certain number of edges from the input graph at each training epoch, acting like a data augmenter and also a message passing reducer. Furthermore, we theoretically demonstrate that DropEdge either reduces the convergence speed of over-smoothing or relieves... | Junzhou Huang, Tingyang Xu, Wenbing Huang, Yu Rong |  |
| 402 |  |  [Masked Based Unsupervised Content Transfer](https://openreview.net/forum?id=BJe-91BtvH) |  | 0 | We consider the problem of translating, in an unsupervised manner, between two domains where one contains some additional information compared to the other. The proposed method disentangles the common and separate parts of these domains and, through the generation of a mask, focuses the attention of the underlying network to the desired augmentation alone, without wastefully reconstructing the entire target. This enables state-of-the-art quality and variety of content translation, as demonstrated through extensive quantitative and qualitative evaluation. Our method is also capable of adding the separate content of different guide images and domains as well as remove existing separate content. Furthermore, our method enables... | Amit Bermano, Lior Wolf, Ron Mokady, Sagie Benaim |  |
| 403 |  |  [U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation](https://openreview.net/forum?id=BJlZ5ySKPH) |  | 0 | We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture... | Hyeonwoo Kang, Junho Kim, Kwanghee Lee, Minjae Kim |  |
| 404 |  |  [Inductive and Unsupervised Representation Learning on Graph Structured Objects](https://openreview.net/forum?id=rkem91rtDB) |  | 0 | Inductive and unsupervised graph learning is a critical technique for predictive or information retrieval tasks where label information is difficult to obtain. It is also challenging to make graph learning inductive and unsupervised at the same time, as learning processes guided by reconstruction error based loss functions inevitably demand graph similarity evaluation that is usually computationally intractable. In this paper, we propose a general framework SEED (Sampling, Encoding, and Embedding Distributions) for inductive and unsupervised representation learning on graph structured objects. Instead of directly dealing with the computational challenges raised by graph similarity evaluation, given an input graph, the SEED... | Bo Zong, Dongjin Song, Haifeng Chen, Jingchao Ni, Lichen Wang, Qianqian Ma, Wei Cheng, Wenchao Yu, Yanchi Liu, Yun Fu |  |
| 405 |  |  [Batch-shaping for learning conditional channel gated networks](https://openreview.net/forum?id=Bke89JBtvB) |  | 0 | We present a method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost. This is achieved by gating the deep-learning architecture on a fine-grained-level. Individual convolutional maps are turned on/off conditionally on features in the network. To achieve this, we introduce a new residual block architecture that gates convolutional channels in a fine-grained manner. We also introduce a generally applicable tool batch-shaping that matches the marginal aggregate posteriors of features in a neural network to a pre-specified prior distribution. We use this novel technique to force gates to be more conditional on the data. We present results on CIFAR-10 and ImageNet... | Babak Ehteshami Bejnordi, Max Welling, Tijmen Blankevoort |  |
| 406 |  |  [Learning Robust Representations via Multi-View Information Bottleneck](https://openreview.net/forum?id=B1xwcyHFDr) |  | 0 | The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on... | Anjan Dutta, Marco Federici, Nate Kushman, Patrick Forré, Zeynep Akata |  |
| 407 |  |  [Deep probabilistic subsampling for task-adaptive compressed sensing](https://openreview.net/forum?id=SJeq9JBFvH) |  | 0 | The field of deep learning is commonly concerned with optimizing predictive models using large pre-acquired datasets of densely sampled datapoints or signals. In this work, we demonstrate that the deep learning paradigm can be extended to incorporate a subsampling scheme that is jointly optimized under a desired minimum sample rate. We present Deep Probabilistic Subsampling (DPS), a widely applicable framework for task-adaptive compressed sensing that enables end-to end optimization of an optimal subset of signal samples with a subsequent model that performs a required task. We demonstrate strong performance on reconstruction and classification tasks of a toy dataset, MNIST, and CIFAR10 under stringent subsampling rates in both the... | Bastiaan S. Veeling, Iris A. M. Huijben, Ruud J. G. van Sloun |  |
| 408 |  |  [Robust anomaly detection and backdoor attack detection via differential privacy](https://openreview.net/forum?id=SJx0q1rtvS) |  | 0 | Outlier detection and novelty detection are two important topics for anomaly detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as “outliers” that are intentionally added by attackers. Differential privacy has been proposed to avoid leaking any individual’s information, when aggregated analysis is performed on a given dataset. It is... | Dawn Song, Min Du, Ruoxi Jia |  |
| 409 |  |  [Learning to Guide Random Search](https://openreview.net/forum?id=B1gHokBKwS) |  | 0 | We are interested in derivative-free optimization of high-dimensional functions. The sample complexity of existing methods is high and depends on problem dimensionality, unlike the dimensionality-independent rates of first-order methods. The recent success of deep learning suggests that many datasets lie on low-dimensional manifolds that can be represented by deep nonlinear models. We therefore consider derivative-free optimization of a high-dimensional function that lies on a latent low-dimensional manifold. We develop an online learning approach that learns this manifold while performing the optimization. In other words, we jointly learn the manifold and optimize the function. Our analysis suggests that the presented method... | Ozan Sener, Vladlen Koltun |  |
| 410 |  |  [Lagrangian Fluid Simulation with Continuous Convolutions](https://openreview.net/forum?id=B1lDoJSYDH) |  | 0 | We present an approach to Lagrangian fluid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe fluids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior... | Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, Vladlen Koltun |  |
| 411 |  |  [Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs](https://openreview.net/forum?id=rkxDoJBYPB) |  | 0 | We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical... | Aditya Paliwal, Felix Gimeno, Miles Lubin, Oriol Vinyals, Pushmeet Kohli, Vinod Nair, Yujia Li |  |
| 412 |  |  [Compressive Transformers for Long-Range Sequence Modelling](https://openreview.net/forum?id=SylKikSYDH) |  | 0 | We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19. | Anna Potapenko, Chloe Hillier, Jack W. Rae, Siddhant M. Jayakumar, Timothy P. Lillicrap |  |
| 413 |  |  [A Stochastic Derivative Free Optimization Method with Momentum](https://openreview.net/forum?id=HylAoJSKvH) |  | 0 | We consider the problem of unconstrained minimization of a smooth objective function in $\mathbb{R}^d$ in setting where only function evaluations are possible. We propose and analyze stochastic zeroth-order method with heavy ball momentum. In particular, we propose, SMTP, a momentum version of the stochastic three-point method (STP) Bergou et al. (2019). We show new complexity results for non-convex, convex and strongly convex functions. We test our method on a collection of learning to continuous control tasks on several MuJoCo Todorov et al. (2012) environments with varying difficulty and compare against STP, other state-of-the-art derivative-free optimization algorithms and against policy gradient methods. SMTP significantly... | Adel Bibi, Eduard Gorbunov, El Houcine Bergou, Ozan Sener, Peter Richtárik |  |
| 414 |  |  [Understanding and Improving Information Transfer in Multi-Task Learning](https://openreview.net/forum?id=SylzhkBtDB) |  | 0 | We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for... | Christopher Ré, Hongyang R. Zhang, Sen Wu |  |
| 415 |  |  [Learning To Explore Using Active Neural SLAM](https://openreview.net/forum?id=HklXn1BKDH) |  | 0 | This work presents a modular and hierarchical approach to learn policies for exploring 3D environments, called \`Active Neural SLAM'. Our approach leverages the strengths of both classical and learning-based methods, by using analytical path planners with learned SLAM module, and global and local policies. The use of learning provides flexibility with respect to input modalities (in the SLAM module), leverages structural regularities of the world (in global policies), and provides robustness to errors in state estimation (in local policies). Such use of learning within each module retains its benefits, while at the same time, hierarchical decomposition and modular training allow us to sidestep the high sample complexities... | Abhinav Gupta, Devendra Singh Chaplot, Dhiraj Gandhi, Ruslan Salakhutdinov, Saurabh Gupta |  |
| 416 |  |  [EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks](https://openreview.net/forum?id=HJem3yHKwH) |  | 0 | Ensuring robustness of Deep Neural Networks (DNNs) is crucial to their adoption in safety-critical applications such as self-driving cars, drones, and healthcare. Notably, DNNs are vulnerable to adversarial attacks in which small input perturbations can produce catastrophic misclassifications. In this work, we propose EMPIR, ensembles of quantized DNN models with different numerical precisions, as a new approach to increase robustness against adversarial attacks. EMPIR is based on the observation that quantized neural networks often demonstrate much higher robustness to adversarial attacks than full precision networks, but at the cost of a substantial loss in accuracy on the original (unperturbed) inputs. EMPIR overcomes this... | Anand Raghunathan, Balaraman Ravindran, Sanchari Sen |  |
| 417 |  |  [Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel](https://openreview.net/forum?id=rkxNh1Stvr) |  | 0 | Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty... | Elliot Meyerson, Risto Miikkulainen, Xin Qiu |  |
| 418 |  |  [B-Spline CNNs on Lie groups](https://openreview.net/forum?id=H1gBhkBFDH) |  | 0 | Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular... | Erik J. Bekkers |  |
| 419 |  |  [Neural Outlier Rejection for Self-Supervised Keypoint Learning](https://openreview.net/forum?id=Skx82ySYPH) |  | 0 | Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks. However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint... | Hanme Kim, Jiexiong Tang, Rares Ambrus, Sudeep Pillai, Vitor Guizilini |  |
| 420 |  |  [Reducing Transformer Depth on Demand with Structured Dropout](https://openreview.net/forum?id=SylO2yStDr) |  | 0 | Overparametrized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the... | Angela Fan, Armand Joulin, Edouard Grave |  |
| 421 |  |  [Cross-Lingual Ability of Multilingual BERT: An Empirical Study](https://openreview.net/forum?id=HJeT3yrtDr) |  | 0 | Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between... | Dan Roth, Karthikeyan K, Stephen Mayhew, Zihan Wang |  |
| 422 |  |  [SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition](https://openreview.net/forum?id=rkl03ySYDH) |  | 0 | The ability to decompose complex multi-object scenes into meaningful abstractions like objects is fundamental to achieve higher-level cognition. Previous approaches for unsupervised object-oriented scene representation learning are either based on spatial-attention or scene-mixture approaches and limited in scalability which is a main obstacle towards modeling real-world scenes. In this paper, we propose a generative latent variable model, called SPACE, that provides a uniﬁed probabilistic modeling framework that combines the best of spatial-attention and scene-mixture approaches. SPACE can explicitly provide factorized object representations for foreground objects while also decomposing background segments of complex morphology.... | Fei Deng, Gautam Singh, Jindong Jiang, Skand Vishwanath Peri, Sungjin Ahn, Weihao Sun, YiFu Wu, Zhixuan Lin |  |
| 423 |  |  [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://openreview.net/forum?id=rkg-TJBFPB) |  | 0 | Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our... | Roberta Raileanu, Tim Rocktäschel |  |
| 424 |  |  [Low-dimensional statistical manifold embedding of directed graphs](https://openreview.net/forum?id=SkxQp1StDH) |  | 0 | We propose a novel node embedding of directed graphs to statistical manifolds, which is based on a global minimization of pairwise relative entropy and graph geodesics in a non-linear way. Each node is encoded with a probability density function over a measurable space. Furthermore, we analyze the connection of the geometrical properties of such embedding and their efficient learning procedure. Extensive experiments show that our proposed embedding is better preserving the global geodesic information of graphs, as well as outperforming existing embedding models on directed graphs in a variety of evaluation metrics, in an unsupervised setting. | Alen Lancic, Nino AntulovFantulin, Thorben Funke, Tian Guo |  |
| 425 |  |  [Efficient Probabilistic Logic Reasoning with Graph Neural Networks](https://openreview.net/forum?id=rJg76kStwH) |  | 0 | Markov Logic Networks (MLNs), which elegantly combine logic rules and probabilistic graphical models, can be used to address many knowledge graph problems. However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, we explore the combination of MLNs and GNNs, and use graph neural networks for variational inference in MLN. We propose a GNN variant, named ExpressGNN, which strikes a nice balance... | Arun Ramamurthy, Bo Li, Le Song, Xinshi Chen, Yuan Qi, Yuan Yang, Yuyu Zhang |  |
| 426 |  |  [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://openreview.net/forum?id=BJe8pkHFwS) |  | 0 | Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the "neighbor explosion" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose... | Ajitesh Srivastava, Hanqing Zeng, Hongkuan Zhou, Rajgopal Kannan, Viktor K. Prasanna |  |
| 427 |  |  [You Only Train Once: Loss-Conditional Training of Deep Networks](https://openreview.net/forum?id=HyxY6JHKwr) |  | 0 | In many machine learning problems, loss functions are weighted sums of several terms. A typical approach to dealing with these is to train multiple separate models with different selections of weights and then either choose the best one according to some criterion or keep multiple models if it is desirable to maintain a diverse set of solutions. This is inefficient both at training and at inference time. We propose a method that allows replacing multiple models trained on one loss function each by a single model trained on a distribution of losses. At test time a model trained this way can be conditioned to generate outputs corresponding to any loss from the training distribution of losses. We demonstrate this approach on three... | Alexey Dosovitskiy, Josip Djolonga |  |
| 428 |  |  [Projection-Based Constrained Policy Optimization](https://openreview.net/forum?id=rke3TJrtPS) |  | 0 | We consider the problem of learning control policies that optimize a reward function while satisfying constraints due to considerations of safety, fairness, or other costs. We propose a new algorithm - Projection-Based Constrained Policy Optimization (PCPO), an iterative method for optimizing policies in a two-step process - the first step performs an unconstrained update while the second step reconciles the constraint violation by projecting the policy back onto the constraint set. We theoretically analyze PCPO and provide a lower bound on reward improvement, as well as an upper bound on constraint violation for each policy update. We further characterize the convergence of PCPO with projection based on two different metrics - L2... | Justinian Rosca, Karthik Narasimhan, Peter J. Ramadge, TsungYen Yang |  |
| 429 |  |  [Infinite-Horizon Differentiable Model Predictive Control](https://openreview.net/forum?id=ryxC6kSYPr) |  | 0 | This paper proposes a differentiable linear quadratic Model Predictive Control (MPC) framework for safe imitation learning. The infinite-horizon cost is enforced using a terminal cost function obtained from the discrete-time algebraic Riccati equation (DARE), so that the learned controller can be proven to be stabilizing in closed-loop. A central contribution is the derivation of the analytical derivative of the solution of the DARE, thereby allowing the use of differentiation-based learning methods. A further contribution is the structure of the MPC optimization problem: an augmented Lagrangian method ensures that the MPC optimization is feasible throughout training whilst enforcing hard constraints on state and input, and a... | Jan Koutník, Jonathan Masci, Marco Gallieri, Mark Cannon, Sebastian East |  |
| 430 |  |  [Combining Q-Learning and Search with Amortized Value Estimates](https://openreview.net/forum?id=SkeAaJrKDS) |  | 0 | We introduce "Search with Amortized Value Estimates" (SAVE), an approach for combining model-free Q-learning with model-based Monte-Carlo Tree Search (MCTS). In SAVE, a learned prior over state-action values is used to guide MCTS, which estimates an improved set of state-action values. The new Q-estimates are then used in combination with real experience to update the prior. This effectively amortizes the value computation performed by MCTS, resulting in a cooperative relationship between model-free learning and model-based search. SAVE can be implemented on top of any Q-learning agent with access to a model, which we demonstrate by incorporating it into agents that perform challenging physical reasoning tasks and Atari. SAVE... | Alvaro SanchezGonzalez, Jessica B. Hamrick, Lars Buesing, Peter W. Battaglia, Theophane Weber, Tobias Pfaff, Victor Bapst |  |
| 431 |  |  [Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators](https://openreview.net/forum?id=Hye1RJHKwB) |  | 0 | Generative adversarial networks (GANs) have shown great success in applications such as image generation and inpainting. However, they typically require large datasets, which are often not available, especially in the context of prediction tasks such as image segmentation that require labels. Therefore, methods such as the CycleGAN use more easily available unlabelled data, but do not offer a way to leverage additional labelled data for improved performance. To address this shortcoming, we show how to factorise the joint data distribution into a set of lower-dimensional distributions along with their dependencies. This allows splitting the discriminator in a GAN into multiple "sub-discriminators" that can be independently trained... | Daniel Stoller, Sebastian Ewert, Simon Dixon |  |
| 432 |  |  [Decentralized Deep Learning with Arbitrary Communication Compression](https://openreview.net/forum?id=SkgGCkrKvH) |  | 0 | Decentralized training of deep learning models is a key element for enabling data privacy and on-device learning over networks, as well as for efficient scaling to large compute clusters. As current approaches are limited by network bandwidth, we propose the use of communication compression in the decentralized training context. We show that Choco-SGD achieves linear speedup in the number of workers for arbitrary high compression ratios on general non-convex functions, and non-IID training data. We demonstrate the practical performance of the algorithm in two key scenarios: the training of deep learning models (i) over decentralized user devices, connected by a peer-to-peer network and (ii) in a datacenter. | Anastasia Koloskova, Martin Jaggi, Sebastian U. Stich, Tao Lin |  |
| 433 |  |  [Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control](https://openreview.net/forum?id=SylL0krYPS) |  | 0 | Deep reinforcement learning has achieved great success in many previously difficult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efficient than model-free based attacks... | Jonathan Uesato, Kai Xiao, Krishnamurthy (Dj) Dvijotham, Pushmeet Kohli, Robert Stanforth, Sven Gowal, TsuiWei Weng |  |
| 434 |  |  [Gradient $\ell_1$ Regularization for Quantization Robustness](https://openreview.net/forum?id=ryxK0JBtPr) |  | 0 | We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths as energy and memory requirements of the application change. Unlike quantization-aware training using the straight-through estimator that only targets a specific bit-width and requires access to training data and pipeline, our regularization-based method paves the way for \`\`on the fly'' post-training quantization to various bit-widths. We show that by modeling quantization as a... | Arash Behboodi, Christos Louizos, Mart van Baalen, Max Welling, Milad Alizadeh, Tijmen Blankevoort |  |
| 435 |  |  [SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes](https://openreview.net/forum?id=rkxs0yHFPH) |  | 0 | Event-based neuromorphic systems promise to reduce the energy consumption of deep neural networks by replacing expensive floating point operations on dense matrices by low energy, sparse operations on spike events. While these systems can be trained increasingly well using approximations of the backpropagation algorithm, this usually requires high precision errors and is therefore incompatible with the typical communication infrastructure of neuromorphic circuits. In this work, we analyze how the gradient can be discretized into spike events when training a spiking neural network. To accelerate our simulation, we show that using a special implementation of the integrate-and-fire neuron allows us to describe the accumulated... | Antoine Dupret, Johannes C. Thiele, Olivier Bichler |  |
| 436 |  |  [On the Relationship between Self-Attention and Convolutional Layers](https://openreview.net/forum?id=HJlnC1rKPB) |  | 0 | Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our... | Andreas Loukas, JeanBaptiste Cordonnier, Martin Jaggi |  |
| 437 |  |  [Learning-Augmented Data Stream Algorithms](https://openreview.net/forum?id=HyxJ1xBYDH) |  | 0 | The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Recently Hsu et al. (2019) incorporated machine learning techniques into the data stream model in order to learn relevant patterns in the input data. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper we explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first optimal bounds for such problems. Namely, we apply the oracle to counting distinct elements on the difference of streams, estimating frequency moments, estimating cascaded aggregates, and... | David P. Woodruff, Honghao Lin, Tanqiu Jiang, Yi Li, Yisong Ruan |  |
| 438 |  |  [Structured Object-Aware Physics Prediction for Video Modeling and Planning](https://openreview.net/forum?id=B1e-kxSKDH) |  | 0 | When humans observe a physical system, they can easily locate components, understand their interactions, and anticipate future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical... | Claas Voelcker, Jannik Kossen, Karl Stelzner, Kristian Kersting, Marcel Hussing |  |
| 439 |  |  [Incorporating BERT into Neural Machine Translation](https://openreview.net/forum?id=Hyl7ygStwB) |  | 0 | The recently proposed BERT (Devlin et al., 2019) has shown great power on a variety of natural language understanding tasks, such as text classification, reading comprehension, etc. However, how to effectively apply BERT to neural machine translation (NMT) lacks enough exploration. While BERT is more commonly used as fine-tuning instead of contextual embedding for downstream language understanding tasks, in NMT, our preliminary exploration of using BERT as contextual embedding is better than using for fine-tuning. This motivates us to think how to better leverage BERT for NMT along this direction. We propose a new algorithm named BERT-fused model, in which we first use BERT to extract representations for an input sequence, and then... | Di He, Houqiang Li, Jinhua Zhu, Lijun Wu, Tao Qin, TieYan Liu, Wengang Zhou, Yingce Xia |  |
| 440 |  |  [MMA Training: Direct Input Space Margin Maximization through Adversarial Training](https://openreview.net/forum?id=HkeryxBtPB) |  | 0 | We study adversarial robustness of neural networks from a margin maximization perspective, where margins are defined as the distances from inputs to a classifier's decision boundary. Our study shows that maximizing margins can be achieved by minimizing the adversarial loss on the decision boundary at the "shortest successful perturbation", demonstrating a close connection between adversarial losses and the margins. We propose Max-Margin Adversarial (MMA) training to directly maximize the margins to achieve adversarial robustness. Instead of adversarial training with a fixed $\epsilon$, MMA offers an improvement by enabling adaptive selection of the "correct" $\epsilon$ as the margin individually for each datapoint. In addition, we... | Gavin Weiguang Ding, Kry Yik Chau Lui, Ruitong Huang, Yash Sharma |  |
| 441 |  |  [Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies](https://openreview.net/forum?id=rkgU1gHtvr) |  | 0 | We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete... | Heng Ge, Hongyuan Zha, Lu Wang, Xinyun Chen, Yizhe Hang |  |
| 442 |  |  [vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations](https://openreview.net/forum?id=rylwJxrYDS) |  | 0 | We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorithm uses either a gumbel softmax or online k-means clustering to quantize the dense representations. Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. Experiments show that BERT pre-training achieves a new state of the art on TIMIT phoneme classification and WSJ speech recognition. | Alexei Baevski, Michael Auli, Steffen Schneider |  |
| 443 |  |  [Meta-learning curiosity algorithms](https://openreview.net/forum?id=BygdyxHFDS) |  | 0 | We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: an outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: pieces of... | Ferran Alet, Leslie Pack Kaelbling, Martin F. Schneider, Tomás LozanoPérez |  |
| 444 |  |  [Making Efficient Use of Demonstrations to Solve Hard Exploration Problems](https://openreview.net/forum?id=SygKyeHKDH) |  | 0 | This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration. | Bobak Shahriari, Duncan Williams, Gabriel BarthMaron, Hubert Soyer, Matt Hoffman, Misha Denil, Nando de Freitas, Neil C. Rabinowitz, Richard Tanburn, Steven Kapturowski, Tom Le Paine, Worlds Team, Ziyu Wang, Çaglar Gülçehre |  |
| 445 |  |  [VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning](https://openreview.net/forum?id=Hkl9JlBYvr) |  | 0 | Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent’s uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncer- tainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate... | Katja Hofmann, Kyriacos Shiarlis, Luisa M. Zintgraf, Maximilian Igl, Sebastian Schulze, Shimon Whiteson, Yarin Gal |  |
| 446 |  |  [Lookahead: A Far-sighted Alternative of Magnitude-based Pruning](https://openreview.net/forum?id=ryl3ygHYDB) |  | 0 | Magnitude-based pruning is one of the simplest methods for pruning neural networks. Despite its simplicity, magnitude-based pruning and its variants demonstrated remarkable performances for pruning modern architectures. Based on the observation that magnitude-based pruning indeed minimizes the Frobenius distortion of a linear operator corresponding to a single layer, we develop a simple pruning method, coined lookahead pruning, by extending the single layer optimization to a multi-layer optimization. Our experimental results demonstrate that the proposed method consistently outperforms magnitude-based pruning on various networks, including VGG and ResNet, particularly in the high-sparsity regime. See... | Jaeho Lee, Jinwoo Shin, Sangwoo Mo, Sejun Park |  |
| 447 |  |  [Spike-based causal inference for weight alignment](https://openreview.net/forum?id=rJxWxxSYvB) |  | 0 | In artificial neural networks trained with gradient descent, the weights used for processing stimuli are also used during backward passes to calculate gradients. For the real brain to approximate gradients, gradient information would have to be propagated separately, such that one set of synaptic weights is used for processing and another set is used for backward passes. This produces the so-called "weight transport problem" for biological models of learning, where the backward weights used to calculate gradients need to mirror the forward weights used to process stimuli. This weight transport problem has been considered so hard that popular proposals for biological learning assume that the backward weights are simply random, as in... | Blake A. Richards, Jordan Guerguiev, Konrad P. Körding |  |
| 448 |  |  [Empirical Bayes Transductive Meta-Learning with Synthetic Gradients](https://openreview.net/forum?id=Hkg-xgrYvH) |  | 0 | We propose a meta-learning approach that learns from multiple tasks in a transductive setting, by leveraging the unlabeled query set in addition to the support set to generate a more powerful model for each task. To develop our framework, we revisit the empirical Bayes formulation for multi-task learning. The evidence lower bound of the marginal log-likelihood of empirical Bayes decomposes as a sum of local KL divergences between the variational posterior and the true posterior on the query set of each task. We derive a novel amortized variational inference that couples all the variational posteriors via a meta-model, which consists of a synthetic gradient network and an initialization network. Each variational posterior is derived... | Andreas C. Damianou, Guillaume Obozinski, Neil D. Lawrence, Pablo Garcia Moreno, Shell Xu Hu, Xi Shen, Yang Xiao |  |
| 449 |  |  [Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning](https://openreview.net/forum?id=rke7geHtwH) |  | 0 | Off-policy reinforcement learning algorithms promise to be applicable in settings where only a fixed data-set (batch) of environment interactions is available and no new experience can be acquired. This property makes these algorithms appealing for real world problems such as robot control. In practice, however, standard off-policy algorithms fail in the batch setting for continuous control. In this paper, we propose a simple solution to this problem. It admits the use of data generated by arbitrary behavior policies and uses a learned prior -- the advantage-weighted behavior model (ABM) -- to bias the RL policy towards actions that have previously been executed and are likely to be successful on the new task. Our method can be... | Abbas Abdolmaleki, Felix Berkenkamp, Jost Tobias Springenberg, Martin A. Riedmiller, Michael Neunert, Nicolas Heess, Noah Y. Siegel, Roland Hafner, Thomas Lampe |  |
| 450 |  |  [Understanding the Limitations of Conditional Generative Models](https://openreview.net/forum?id=r1lPleBFvH) |  | 0 | Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled... | Ethan Fetaya, JörnHenrik Jacobsen, Richard S. Zemel, Will Grathwohl |  |
| 451 |  |  [Demystifying Inter-Class Disentanglement](https://openreview.net/forum?id=Hyl9xxHYPr) |  | 0 | Learning to disentangle the hidden factors of variations within a set of observations is a key task for artificial intelligence. We present a unified formulation for class and content disentanglement and use it to illustrate the limitations of current methods. We therefore introduce LORD, a novel method based on Latent Optimization for Representation Disentanglement. We find that latent optimization, along with an asymmetric noise regularization, is superior to amortized inference for achieving disentangled representations. In extensive experiments, our method is shown to achieve better disentanglement performance than both adversarial and non-adversarial methods that use the same level of supervision. We further introduce a... | Aviv Gabbay, Yedid Hoshen |  |
| 452 |  |  [Mixed-curvature Variational Autoencoders](https://openreview.net/forum?id=S1g6xeSKDS) |  | 0 | Euclidean space has historically been the typical workhorse geometry for machine learning applications due to its power and simplicity. However, it has recently been shown that geometric spaces with constant non-zero curvature improve representations and performance on a variety of data types and downstream tasks. Consequently, generative models like Variational Autoencoders (VAEs) have been successfully generalized to elliptical and hyperbolic latent spaces. While these approaches work well on data with particular kinds of biases e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying and leveraging all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE... | Gary Bécigneul, OctavianEugen Ganea, Ondrej Skopek |  |
| 453 |  |  [BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations](https://openreview.net/forum?id=r1x0lxrFPS) |  | 0 | Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the... | Hyungjun Kim, JaeJoon Kim, Jinseok Kim, Kyungsu Kim |  |
| 454 |  |  [Model-based reinforcement learning for biological sequence design](https://openreview.net/forum?id=HklxbgBKvr) |  | 0 | The ability to design biological structures such as DNA or proteins would have considerable medical and industrial impact. Doing so presents a challenging black-box optimization problem characterized by the large-batch, low round setting due to the need for labor-intensive wet lab evaluations. In response, we propose using reinforcement learning (RL) based on proximal-policy optimization (PPO) for biological sequence design. RL provides a flexible framework for optimization generative sequence models to achieve specific criteria, such as diversity among the high-quality sequences discovered. We propose a model-based variant of PPO, DyNA-PPO, to improve sample efficiency, where the policy for a new round is trained offline using a... | Christof Angermüller, David Belanger, David Dohan, Kevin Murphy, Lucy J. Colwell, Ramya Deshpande |  |
| 455 |  |  [BayesOpt Adversarial Attack](https://openreview.net/forum?id=Hkem-lrtvH) |  | 0 | Black-box adversarial attacks require a large number of attempts before finding successful adversarial examples that are visually indistinguishable from the original input. Current approaches relying on substitute model training, gradient estimation or genetic algorithms often require an excessive number of queries. Therefore, they are not suitable for real-world systems where the maximum query number is limited due to cost. We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction. We demonstrate empirically that our method can achieve comparable success rates... | Adam D. Cobb, Arno Blaas, Binxin Ru, Yarin Gal |  |
| 456 |  |  [Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies](https://openreview.net/forum?id=HkgsWxrtPB) |  | 0 | We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference (MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiment results on two grid-world domains... | Honglak Lee, Hyunjae Woo, Jongwook Choi, Sungryull Sohn |  |
| 457 |  |  [Hypermodels for Exploration](https://openreview.net/forum?id=ryx6WgStPB) |  | 0 | We study the use of hypermodels to represent epistemic uncertainty and guide exploration. This generalizes and extends the use of ensembles to approximate Thompson sampling. The computational cost of training an ensemble grows with its size, and as such, prior work has typically been limited to ensembles with tens of elements. We show that alternative hypermodels can enjoy dramatic efficiency gains, enabling behavior that would otherwise require hundreds or thousands of elements, and even succeed in situations where ensemble methods fail to learn regardless of size. This allows more accurate approximation of Thompson sampling as well as use of more sophisticated exploration schemes. In particular, we consider an approximate form of... | Benjamin Van Roy, Ian Osband, Morteza Ibrahimi, Vikranth Dwaracherla, Xiuyuan Lu, Zheng Wen |  |
| 458 |  |  [RaPP: Novelty Detection with Reconstruction along Projection Pathway](https://openreview.net/forum?id=HkgeGeBYDB) |  | 0 | We propose RaPP, a new methodology for novelty detection by utilizing hidden space activation values obtained from a deep autoencoder. Precisely, RaPP compares input and its autoencoder reconstruction not only in the input space but also in the hidden spaces. We show that if we feed a reconstructed input to the same autoencoder again, its activated values in a hidden space are equivalent to the corresponding reconstruction in that hidden space given the original input. In order to aggregate the hidden space activation values, we propose two metrics, which enhance the novelty detection performance. Through extensive experiments using diverse datasets, we validate that RaPP improves novelty detection performances of autoencoder-based... | Andre S. Yoon, Byungchan Kim, Jeongwoo Choi, Jongseob Jeon, Ki Hyun Kim, Sangwoo Shim, Yongsub Lim |  |
| 459 |  |  [Dynamics-Aware Embeddings](https://openreview.net/forum?id=BJgZGeHFPH) |  | 0 | In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and actions. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps. | Abhinav Gupta, Kyunghyun Cho, Rajat Agarwal, William F. Whitney |  |
| 460 |  |  [Functional Regularisation for Continual Learning with Gaussian Processes](https://openreview.net/forum?id=HkxCzeHFDB) |  | 0 | We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is... | Alexander G. de G. Matthews, Jonathan Schwarz, Michalis K. Titsias, Razvan Pascanu, Yee Whye Teh |  |
| 461 |  |  [You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings](https://openreview.net/forum?id=BkxSmlBFvr) |  | 0 | Knowledge graph embedding (KGE) models learn algebraic representations of the entities and relations in a knowledge graph. A vast number of KGE techniques for multi-relational link prediction have been proposed in the recent literature, often with state-of-the-art performance. These approaches differ along a number of dimensions, including different model architectures, different training strategies, and different approaches to hyperparameter optimization. In this paper, we take a step back and aim to summarize and quantify empirically the impact of each of these dimensions on model performance. We report on the results of an extensive experimental study with popular model architectures and training strategies across a wide range... | Daniel Ruffinelli, Rainer Gemulla, Samuel Broscheit |  |
| 462 |  |  [AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing](https://openreview.net/forum?id=H1eqQeHFDS) |  | 0 | This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimic the pipeline of... | Bo Zhu, Helen Lu Cao, Xingzhe He |  |
| 463 |  |  [Never Give Up: Learning Directed Exploration Strategies](https://openreview.net/forum?id=Sye57xStvB) |  | 0 | We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration... | Adrià Puigdomènech Badia, Alex Vitvitskyi, Alexander Pritzel, Andrew Bolt, Bilal Piot, Charles Blundell, Martín Arjovsky, Olivier Tieleman, Pablo Sprechmann, Steven Kapturowski, Zhaohan Daniel Guo |  |
| 464 |  |  [Fair Resource Allocation in Federated Learning](https://openreview.net/forum?id=ByexElSYDr) |  | 0 | Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a communication-efficient method, q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets with both convex and non-convex... | Ahmad Beirami, Maziar Sanjabi, Tian Li, Virginia Smith |  |
| 465 |  |  [Smooth markets: A basic mechanism for organizing gradient-based learners](https://openreview.net/forum?id=B1xMEerYvB) |  | 0 | With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets (SM-games), a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes some GANs, adversarial training, and other recent algorithms. We show that SM-games are amenable to analysis and optimization using first-order methods. | David Balduzzi, Edward Hughes, Georgios Piliouras, Ian Gemp, Joel Z. Leibo, Thore Graepel, Tom Anthony, Wojciech M. Czarnecki |  |
| 466 |  |  [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://openreview.net/forum?id=BJgQ4lSFPH) |  | 0 | Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is... | Bin Bi, Chen Wu, Jiangnan Xia, Liwei Peng, Luo Si, Ming Yan, Wei Wang, Zuyi Bao |  |
| 467 |  |  [Training binary neural networks with real-to-binary convolutions](https://openreview.net/forum?id=BJg4NgBKvH) |  | 0 | This paper shows how to train binary networks to within a few percent points (~3-5%) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea in two complementary ways: (1) with a loss function, during training, by matching the spatial attention maps computed at the output of the binary and real-valued convolutions, and (2) in a data-driven manner,... | Adrian Bulat, Brais Martínez, Georgios Tzimiropoulos, Jing Yang |  |
| 468 |  |  [Permutation Equivariant Models for Compositional Generalization in Language](https://openreview.net/forum?id=SylVNerFvr) |  | 0 | Humans understand novel sentences by composing meanings and roles of core language components. In contrast, neural network models for natural language modeling fail when such compositional generalization is required. The main contribution of this paper is to hypothesize that language compositionality is a form of group-equivariance. Based on this hypothesis, we propose a set of tools for constructing equivariant sequence-to-sequence models. Throughout a variety of experiments on the SCAN tasks, we analyze the behavior of existing models under the lens of equivariance, and demonstrate that our equivariant architecture is able to achieve the type compositional generalization required in human language understanding. | David LopezPaz, Diane Bouchacourt, Jonathan Gordon, Marco Baroni |  |
| 469 |  |  [Continual learning with hypernetworks](https://openreview.net/forum?id=SJgwNerKvB) |  | 0 | Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned hypernetworks, i.e., networks that generate the weights of a target model based on task identity. Continual learning (CL) is less difficult for this class of models thanks to a simple key feature: instead of recalling the input-output relations of all previously seen data, task-conditioned hypernetworks only require rehearsing task-specific weight realizations, which can be maintained in memory using a simple regularizer. Besides achieving state-of-the-art performance on standard CL benchmarks, additional experiments on long task... | Benjamin F. Grewe, Christian Henning, Johannes von Oswald, João Sacramento |  |
| 470 |  |  [Phase Transitions for the Information Bottleneck in Representation Learning](https://openreview.net/forum?id=HJloElBYvB) |  | 0 | In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: IB_β[p(z\|x)] = I(X; Z) − βI(Y; Z) defined on the encoding distribution p(z\|x) for input X, target Y and representation Z, where sudden jumps of dI(Y; Z)/dβ and prediction accuracy are observed with increasing β. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using... | Ian S. Fischer, Tailin Wu |  |
| 471 |  |  [Variational Template Machine for Data-to-Text Generation](https://openreview.net/forum?id=HkejNgBtPB) |  | 0 | How to generate descriptions from structured data organized in tables? Existing approaches using neural encoder-decoder models often suffer from lacking diversity. We claim that an open set of templates is crucial for enriching the phrase constructions and realizing varied generations.Learning such templates is prohibitive since it often requires a large paired <table,description>, which is seldom available. This paper explores the problem of automatically learning reusable "templates" from paired and non-paired data. We propose the variational template machine (VTM), a novel method to generate text descriptions from data tables. Our contributions include: a) we carefully devise a specific model architecture and losses to... | Hao Zhou, Lei Li, Rong Ye, Wenxian Shi, Zhongyu Wei |  |
| 472 |  |  [Memory-Based Graph Networks](https://openreview.net/forum?id=r1laNeBYPB) |  | 0 | Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data. | Amir Hosein Khas Ahmadi, Kaveh Hassani, Leo Lee, Parsa Moradi, Quaid Morris |  |
| 473 |  |  [AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://openreview.net/forum?id=S1gmrxHFvB) |  | 0 | Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging... | Balaji Lakshminarayanan, Barret Zoph, Dan Hendrycks, Ekin Dogus Cubuk, Justin Gilmer, Norman Mu |  |
| 474 |  |  [AtomNAS: Fine-Grained End-to-End Neural Architecture Search](https://openreview.net/forum?id=BylQSxHFwr) |  | 0 | Search space design is very critical to neural architecture search (NAS) algorithms. We propose a fine-grained search space comprised of atomic blocks, a minimal search unit that is much smaller than the ones used in recent NAS algorithms. This search space allows a mix of operations by composing different types of atomic blocks, while the search space in previous methods only allows homogeneous operations. Based on this search space, we propose a resource-aware architecture search framework which automatically assigns the computational resources (e.g., output channel numbers) for each operation by jointly considering the performance and the computational cost. In addition, to accelerate the search process, we propose a dynamic... | Alan L. Yuille, Jianchao Yang, Jieru Mei, Linjie Yang, Xiaochen Lian, Xiaojie Jin, Yingwei Li |  |
| 475 |  |  [Residual Energy-Based Models for Text Generation](https://openreview.net/forum?id=B1l4SgHKDH) |  | 0 | Text generation is ubiquitous in many NLP tasks, from summarization, to dialogue and machine translation. The dominant parametric approach is based on locally normalized models which predict one word at a time. While these work remarkably well, they are plagued by exposure bias due to the greedy nature of the generation process. In this work, we investigate un-normalized energy-based models (EBMs) which operate not at the token but at the sequence level. In order to make training tractable, we first work in the residual of a pretrained locally normalized language model and second we train using noise contrastive estimation. Furthermore, since the EBM works at the sequence level, we can leverage pretrained bi-directional contextual... | Anton Bakhtin, Arthur Szlam, Marc'Aurelio Ranzato, Myle Ott, Yuntian Deng |  |
| 476 |  |  [A closer look at the approximation capabilities of neural networks](https://openreview.net/forum?id=rkevSgrtPr) |  | 0 | The universal approximation theorem, in one of its most general versions, says that if we consider only continuous activation functions σ, then a standard feedforward neural network with one hidden layer is able to approximate any continuous multivariate function f to any given approximation threshold ε, if and only if σ is non-polynomial. In this paper, we give a direct algebraic proof of the theorem. Furthermore we shall explicitly quantify the number of hidden units required for approximation. Specifically, if X in R^n is compact, then a neural network with n input units, m output units, and a single hidden layer with {n+d choose d} hidden units (independent of m and ε), can uniformly approximate any polynomial function f:X ->... | Kai Fong Ernest Chong |  |
| 477 |  |  [Deep Audio Priors Emerge From Harmonic Convolutional Networks](https://openreview.net/forum?id=rygjHxrYDB) |  | 0 | Convolutional neural networks (CNNs) excel in image recognition and generation. Among many efforts to explain their effectiveness, experiments show that CNNs carry strong inductive biases that capture natural image priors. Do deep networks also have inductive biases for audio signals? In this paper, we empirically show that current network architectures for audio processing do not show strong evidence in capturing such priors. We propose Harmonic Convolution, an operation that helps deep networks distill priors in audio signals by explicitly utilizing the harmonic structure within. This is done by engineering the kernel to be supported by sets of harmonic series, instead of local neighborhoods for convolutional kernels. We show... | Antonio Torralba, Chuang Gan, Jiajun Wu, Joshua B. Tenenbaum, William T. Freeman, Yunyun Wang, Zhoutong Zhang |  |
| 478 |  |  [Expected Information Maximization: Using the I-Projection for Mixture Density Estimation](https://openreview.net/forum?id=ByglLlHFDS) |  | 0 | Modelling highly multi-modal data is a challenging problem in machine learning. Most algorithms are based on maximizing the likelihood, which corresponds to the M(oment)-projection of the data distribution to the model distribution. The M-projection forces the model to average over modes it cannot represent. In contrast, the I(nformation)-projection ignores such modes in the data and concentrates on the modes the model can represent. Such behavior is appealing whenever we deal with highly multi-modal data where modelling single modes correctly is more important than covering all the modes. Despite this advantage, the I-projection is rarely used in practice due to the lack of algorithms that can efficiently optimize it based on... | Gerhard Neumann, Oleg Arenz, Philipp Becker |  |
| 479 |  |  [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://openreview.net/forum?id=ryxWIgBFPS) |  | 0 | We propose to use a meta-learning objective that maximizes the speed of transfer on a modified distribution to learn how to modularize acquired knowledge. In particular, we focus on how to factor a joint distribution into appropriate conditionals, consistent with the causal directions. We explain when this can work, using the assumption that the changes in distributions are localized (e.g. to one of the marginals, for example due to an intervention on one of the variables). We prove that under this assumption of localized changes in causal mechanisms, the correct causal graph will tend to have only a few of its parameters with non-zero gradient, i.e. that need to be adapted (those of the modified variables). We argue and observe... | Anirudh Goyal, Christopher J. Pal, Nan Rosemary Ke, Nasim Rahaman, Olexa Bilaniuk, Sébastien Lachapelle, Tristan Deleu, Yoshua Bengio |  |
| 480 |  |  [On the interaction between supervision and self-play in emergent communication](https://openreview.net/forum?id=rJxGLlBtwH) |  | 0 | A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed... | Abhinav Gupta, Douwe Kiela, Jakob N. Foerster, Joelle Pineau, Ryan Lowe |  |
| 481 |  |  [Dynamic Model Pruning with Feedback](https://openreview.net/forum?id=SJem8lSFwB) |  | 0 | Deep neural networks often have millions of parameters. This can hinder their deployment to low-end devices, not only due to high memory requirements but also because of increased latency at inference. We propose a novel model compression method that generates a sparse trained model without additional overhead: by allowing (i) dynamic allocation of the sparsity pattern and (ii) incorporating feedback signal to reactivate prematurely pruned weights we obtain a performant sparse model in one single training pass (retraining is not needed, but can further improve the performance). We evaluate the method on CIFAR-10 and ImageNet, and show that the obtained sparse models can reach the state-of-the-art performance of dense models and... | Daniil Dmitriev, Luis Barba, Martin Jaggi, Sebastian U. Stich, Tao Lin |  |
| 482 |  |  [Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings](https://openreview.net/forum?id=SJxE8erKDH) |  | 0 | Learned joint representations of images and text form the backbone of several important cross-domain tasks such as image captioning. Prior work mostly maps both domains into a common latent representation in a purely supervised fashion. This is rather restrictive, however, as the two domains follow distinct generative processes. Therefore, we propose a novel semi-supervised framework, which models shared information between domains and domain-specific information separately. The information shared between the domains is aligned with an invertible neural network. Our model integrates normalizing flow-based priors for the domain-specific information, which allows us to learn diverse many-to-many mappings between the two domains. We... | Iryna Gurevych, Shweta Mahajan, Stefan Roth |  |
| 483 |  |  [Transferring Optimality Across Data Distributions via Homotopy Methods](https://openreview.net/forum?id=S1gEIerYwH) |  | 0 | Homotopy methods, also known as continuation methods, are a powerful mathematical tool to efficiently solve various problems in numerical analysis, including complex non-convex optimization problems where no or only little prior knowledge regarding the localization of the solutions is available. In this work, we propose a novel homotopy-based numerical method that can be used to transfer knowledge regarding the localization of an optimum across different task distributions in deep learning applications. We validate the proposed methodology with some empirical evaluations in the regression and classification scenarios, where it shows that superior numerical performance can be achieved in popular deep learning benchmarks, i.e.... | Andrea Zanelli, Frank Hutter, Matilde Gargiani, Moritz Diehl, Quoc TranDinh |  |
| 484 |  |  [Regularizing activations in neural networks via distribution matching with the Wasserstein metric](https://openreview.net/forum?id=rygwLgrYPB) |  | 0 | Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein... | Byunghoon Kim, Donggu Kang, Taejong Joo |  |
| 485 |  |  [Mutual Information Gradient Estimation for Representation Learning](https://openreview.net/forum?id=ByxaUgrFvH) |  | 0 | Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of... | Liangjian Wen, Lirong He, Mingyuan Zhou, Yiji Zhou, Zenglin Xu |  |
| 486 |  |  [Lite Transformer with Long-Short Range Attention](https://openreview.net/forum?id=ByeMPlHKPH) |  | 0 | Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent... | Ji Lin, Song Han, Yujun Lin, Zhanghao Wu, Zhijian Liu |  |
| 487 |  |  [A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case](https://openreview.net/forum?id=H1lNPxHKDH) |  | 0 | We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function $f:\mathbb{R}\rightarrow \mathbb{R}^d$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case. | Daniel Soudry, Greg Ongie, Nathan Srebro, Rebecca Willett |  |
| 488 |  |  [Adversarial Lipschitz Regularization](https://openreview.net/forum?id=Bke_DertPB) |  | 0 | Generative adversarial networks (GANs) are one of the most popular approaches when it comes to training generative models, among which variants of Wasserstein GANs are considered superior to the standard GAN formulation in terms of learning stability and sample quality. However, Wasserstein GANs require the critic to be 1-Lipschitz, which is often enforced implicitly by penalizing the norm of its gradient, or by globally restricting its Lipschitz constant via weight normalization techniques. Training with a regularization term penalizing the violation of the Lipschitz constraint explicitly, instead of through the norm of the gradient, was found to be practically infeasible in most situations. Inspired by Virtual Adversarial... | Dávid Terjék |  |
| 489 |  |  [Compositional Language Continual Learning](https://openreview.net/forum?id=rklnDgHtDS) |  | 0 | Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over... | Kenneth Church, Liang Zhao, Mohamed Elhoseiny, Yuanpeng Li |  |
| 490 |  |  [End to End Trainable Active Contours via Differentiable Rendering](https://openreview.net/forum?id=rkxawlHKDr) |  | 0 | We present an image segmentation method that iteratively evolves a polygon. At each iteration, the vertices of the polygon are displaced based on the local value of a 2D shift map that is inferred from the input image via an encoder-decoder architecture. The main training loss that is used is the difference between the polygon shape and the ground truth segmentation mask. The network employs a neural renderer to create the polygon from its vertices, making the process fully differentiable. We demonstrate that our method outperforms the state of the art segmentation networks and deep active contour solutions in a variety of benchmarks, including medical imaging and aerial images. | Lior Wolf, Shir Gur, Tal Shaharabany |  |
| 491 |  |  [Provable Filter Pruning for Efficient Neural Networks](https://openreview.net/forum?id=BJxkOlSYDH) |  | 0 | We present a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs) by identifying and removing redundant filters from an over-parameterized network. Our algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability. In contrast to existing filter pruning approaches, our method is simultaneously data-informed, exhibits provable guarantees on the size and performance of the pruned network, and is widely applicable to varying network architectures and data sets. Our analytical bounds bridge the notions of compressibility... | Cenk Baykal, Dan Feldman, Daniela Rus, Harry Lang, Lucas Liebenwein |  |
| 492 |  |  [Effect of Activation Functions on the Training of Overparametrized Neural Nets](https://openreview.net/forum?id=rkgfdeBYvH) |  | 0 | It is well-known that overparametrized neural networks trained using gradient based methods quickly achieve small training error with appropriate hyperparameter settings. Recent papers have proved this statement theoretically for highly overparametrized networks under reasonable assumptions. These results either assume that the activation function is ReLU or they depend on the minimum eigenvalue of a certain Gram matrix. In the latter case, existing works only prove that this minimum eigenvalue is non-zero and do not provide quantitative bounds which require that this eigenvalue be large. Empirically, a number of alternative activation functions have been proposed which tend to perform better than ReLU at least in some settings but... | Abhishek Panigrahi, Abhishek Shetty, Navin Goyal |  |
| 493 |  |  [Lipschitz constant estimation of Neural Networks via sparse polynomial optimization](https://openreview.net/forum?id=rJe4_xSFDB) |  | 0 | We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bound on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the $\ell_\infty$-Lipschitz constant, our approach yields superior estimates as compared to other baselines available in the literature. | Fabian Latorre, Paul Rolland, Volkan Cevher |  |
| 494 |  |  [State Alignment-based Imitation Learning](https://openreview.net/forum?id=rylrdxHFDr) |  | 0 | Consider an imitation learning problem that the imitator and the expert have different dynamics models. Most of existing imitation learning methods fail because they focus on the imitation of actions. We propose a novel state alignment-based imitation learning method to train the imitator by following the state sequences in the expert demonstrations as much as possible. The alignment of states comes from both local and global perspectives. We combine them into a reinforcement learning framework by a regularized policy update objective. We show the superiority of our method on standard imitation learning settings as well as the challenging settings in which the expert and the imitator have different dynamics models. | Fangchen Liu, Hao Su, Tongzhou Mu, Zhan Ling |  |
| 495 |  |  [Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories](https://openreview.net/forum?id=rkl8dlHYvB) |  | 0 | We address the problem of learning to discover 3D parts for objects in unseen categories. Being able to learn the geometry prior of parts and transfer this prior to unseen categories pose fundamental challenges on data-driven shape segmentation approaches. Formulated as a contextual bandit problem, we propose a learning-based iterative grouping framework which learns a grouping policy to progressively merge small part proposals into bigger ones in a bottom-up fashion. At the core of our approach is to restrict the local context for extracting part-level features, which encourages the generalizability to novel categories. On a recently proposed large-scale fine-grained 3D part dataset, PartNet, we demonstrate that our method can... | Hao Su, Jiarui Xu, Kaichun Mo, Liwei Wang, Siyu Hu, Tiange Luo, Zhiao Huang |  |
| 496 |  |  [Discriminative Particle Filter Reinforcement Learning for Complex Partial observations](https://openreview.net/forum?id=HJl8_eHYvS) |  | 0 | Deep reinforcement learning is successful in decision making for sophisticated games, such as Atari, Go, etc. However, real-world decision making often requires reasoning with partial information extracted from complex visual observations. This paper presents Discriminative Particle Filter Reinforcement Learning (DPFRL), a new reinforcement learning framework for complex partial observations. DPFRL encodes a differentiable particle filter in the neural network policy for explicit reasoning with partial observations over time. The particle filter maintains a belief using learned discriminative update, which is trained end-to-end for decision making. We show that using the discriminative update instead of standard generative models... | David Hsu, Nan Ye, Péter Karkus, Wee Sun Lee, Xiao Ma |  |
| 497 |  |  [Unrestricted Adversarial Examples via Semantic Manipulation](https://openreview.net/forum?id=Sye_OgHFwH) |  | 0 | Machine learning models, especially deep neural networks (DNNs), have been shown to be vulnerable against adversarial examples which are carefully crafted samples with a small magnitude of the perturbation. Such adversarial perturbations are usually restricted by bounding their $\mathcal{L}_p$ norm such that they are imperceptible, and thus many current defenses can exploit this property to reduce their adversarial impact. In this paper, we instead introduce "unrestricted" perturbations that manipulate semantically meaningful image-based visual descriptors - color and texture - in order to generate effective and photorealistic adversarial examples. We show that these semantically aware perturbations are effective against JPEG... | Anand Bhattad, Bo Li, David A. Forsyth, Kaizhao Liang, Min Jin Chong |  |
| 498 |  |  [Classification-Based Anomaly Detection for General Data](https://openreview.net/forum?id=H1lK_lBtvS) |  | 0 | Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains. | Liron Bergman, Yedid Hoshen |  |
| 499 |  |  [Scale-Equivariant Steerable Networks](https://openreview.net/forum?id=HJgpugrKPS) |  | 0 | The effectiveness of Convolutional Neural Networks (CNNs) has been substantially attributed to their built-in property of translation equivariance. However, CNNs do not have embedded mechanisms to handle other types of transformations. In this work, we pay attention to scale changes, which regularly appear in various tasks due to the changing distances between the objects and the camera. First, we introduce the general theory for building scale-equivariant convolutional networks with steerable filters. We develop scale-convolution and generalize other common blocks to be scale-equivariant. We demonstrate the computational efficiency and numerical stability of the proposed method. We compare the proposed models to the previously... | Arnold W. M. Smeulders, Ivan Sosnovik, Michal Szmaja |  |
| 500 |  |  [On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning](https://openreview.net/forum?id=SkxxtgHKPS) |  | 0 | Generalization error (also known as the out-of-sample error) measures how well the hypothesis learned from training data generalizes to previously unseen data. Proving tight generalization error bounds is a central question in statistical learning theory. In this paper, we obtain generalization error bounds for learning general non-convex objectives, which has attracted significant attention in recent years. We develop a new framework, termed Bayes-Stability, for proving algorithm-dependent generalization error bounds. The new framework combines ideas from both the PAC-Bayesian theory and the notion of algorithmic stability. Applying the Bayes-Stability method, we obtain new data-dependent generalization bounds for stochastic... | Jian Li, Mingda Qiao, Xuanyuan Luo |  |
| 501 |  |  [Consistency Regularization for Generative Adversarial Networks](https://openreview.net/forum?id=S1lxKlSKPH) |  | 0 | Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization—a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively... | Augustus Odena, Han Zhang, Honglak Lee, Zizhao Zhang |  |
| 502 |  |  [Differentiable learning of numerical rules in knowledge graphs](https://openreview.net/forum?id=rJleKgrKwS) |  | 0 | Rules over a knowledge graph (KG) capture interpretable patterns in data and can be used for KG cleaning and completion. Inspired by the TensorLog differentiable logic framework, which compiles rule inference into a sequence of differentiable operations, recently a method called Neural LP has been proposed for learning the parameters as well as the structure of rules. However, it is limited with respect to the treatment of numerical features like age, weight or scientific measurements. We address this limitation by extending Neural LP to learn rules with numerical values, e.g., ”People younger than 18 typically live with their parents“. We demonstrate how dynamic programming and cumulative sum operations can be exploited to ensure... | Csaba Domokos, Daria Stepanova, J. Zico Kolter, PoWei Wang |  |
| 503 |  |  [Learning to Move with Affordance Maps](https://openreview.net/forum?id=BJgMFxrYPB) |  | 0 | The ability to autonomously explore and navigate a physical space is a fundamental requirement for virtually any mobile autonomous agent, from household robotic vacuums to autonomous vehicles. Traditional SLAM-based approaches for exploration and navigation largely focus on leveraging scene geometry, but fail to model dynamic objects (such as other agents) or semantic constraints (such as wet floors or doorways). Learning-based RL agents are an attractive alternative because they can incorporate both semantic and geometric information, but are notoriously sample inefficient, difficult to generalize to novel settings, and are difficult to interpret. In this paper, we combine the best of both worlds with a modular approach that {\em... | Deva Ramanan, Ravi Teja Mullapudi, Saurabh Gupta, William Qi |  |
| 504 |  |  [Neural tangent kernels, transportation mappings, and universal approximation](https://openreview.net/forum?id=HklQYxBKwS) |  | 0 | This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network weights are only allowed microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings between the initial weights and their desired values, and the construction of transport mappings via Fourier transforms. Regarding the first contribution, the proof scheme provides another perspective on how the NTK regime arises from rescaling: redundancy in the weights due to resampling allows... | Matus Telgarsky, Ruicheng Xian, Ziwei Ji |  |
| 505 |  |  [SCALOR: Generative World Models with Scalable Object Representations](https://openreview.net/forum?id=SJxrKgStDH) |  | 0 | Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that... | Gerard de Melo, Jindong Jiang, Sepehr Janghorbani, Sungjin Ahn |  |
| 506 |  |  [Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks](https://openreview.net/forum?id=SyevYxHtDB) |  | 0 | High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging... | Bernt Schiele, Mario Fritz, Tribhuvanesh Orekondy |  |
| 507 |  |  [Domain Adaptive Multibranch Networks](https://openreview.net/forum?id=rJxycxHKDS) |  | 0 | We tackle unsupervised domain adaptation by accounting for the fact that different domains may need to be processed differently to arrive to a common feature representation effective for recognition. To this end, we introduce a deep learning framework where each domain undergoes a different sequence of operations, allowing some, possibly more complex, domains to go through more computations than others. This contrasts with state-of-the-art domain adaptation techniques that force all domains to be processed with the same series of operations, even when using multi-stream architectures whose parameters are not shared. As evidenced by our experiments, the greater flexibility of our method translates to higher accuracy. Furthermore, it... | Mathieu Salzmann, Pascal Fua, Róger BermúdezChacón |  |
| 508 |  |  [DiffTaichi: Differentiable Programming for Physical Simulation](https://openreview.net/forum?id=B1eB5xSFvr) |  | 0 | We present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using source code transformations that preserve arithmetic intensity and parallelism. A light-weight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation. We demonstrate the performance and productivity of our language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in our language is 4.2x shorter than the... | Frédo Durand, Jonathan RaganKelley, Luke Anderson, Nathan Carr, Qi Sun, TzuMao Li, Yuanming Hu |  |
| 509 |  |  [Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning](https://openreview.net/forum?id=BJxI5gHKDr) |  | 0 | Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classification. We explore the standards for its quantification and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more insight in this study, we introduce the deep ensemble equivalent score (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of only few independently trained networks in... | Alexander Lyzhov, Arsenii Ashukha, Dmitry Molchanov, Dmitry P. Vetrov |  |
| 510 |  |  [Episodic Reinforcement Learning with Associative Memory](https://openreview.net/forum?id=HkxjqxBYDB) |  | 0 | Sample efficiency has been one of the major challenges for deep reinforcement learning. Non-parametric episodic control has been proposed to speed up parametric reinforcement learning by rapidly latching on previously successful policies. However, previous work on episodic reinforcement learning neglects the relationship between states and only stored the experiences as unrelated items. To improve sample efficiency of reinforcement learning, we propose a novel framework, called Episodic Reinforcement Learning with Associative Memory (ERLAM), which associates related experience trajectories to enable reasoning effective strategies. We build a graph on top of states in memory based on state transitions and develop a... | Chongjie Zhang, Guangwen Yang, Guangxiang Zhu, Zichuan Lin |  |
| 511 |  |  [Sub-policy Adaptation for Hierarchical Reinforcement Learning](https://openreview.net/forum?id=ByeWogStDS) |  | 0 | Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions are two-fold. First, we derive a new hierarchical policy gradient with an unbiased latent-dependent baseline, and we introduce Hierarchical Proximal Policy Optimization (HiPPO),... | Alexander C. Li, Carlos Florensa, Ignasi Clavera, Pieter Abbeel |  |
| 512 |  |  [Critical initialisation in continuous approximations of binary neural networks](https://openreview.net/forum?id=rylmoxrFDH) |  | 0 | The training of stochastic neural network models with binary ($\pm1$) weights and activations via continuous surrogate networks is investigated. We derive new surrogates using a novel derivation based on writing the stochastic neural network as a Markov chain. This derivation also encompasses existing variants of the surrogates presented in the literature. Following this, we theoretically study the surrogates at initialisation. We derive, using mean field theory, a set of scalar equations describing how input signals propagate through the randomly initialised networks. The equations reveal whether so-called critical initialisations exist for each surrogate network, where the network can be trained to arbitrary depth. Moreover, we... | Carlo Lucibello, Federica Gerace, George Stamatescu, Ian G. Fuss, Langford B. White |  |
| 513 |  |  [Deep Orientation Uncertainty Learning based on a Bingham Loss](https://openreview.net/forum?id=ryloogSKDS) |  | 0 | Reasoning about uncertain orientations is one of the core problems in many perception tasks such as object pose estimation or motion estimation. In these scenarios, poor illumination conditions, sensor limitations, or appearance invariance may result in highly uncertain estimates. In this work, we propose a novel learning-based representation for orientation uncertainty. By characterizing uncertainty over unit quaternions with the Bingham distribution, we formulate a loss that naturally captures the antipodal symmetry of the representation. We discuss the interpretability of the learned distribution parameters and demonstrate the feasibility of our approach on several challenging real-world pose estimation tasks involving uncertain... | Alexander Amini, Daniela Rus, Igor Gilitschenski, Roshni Sahoo, Sertac Karaman, Wilko Schwarting |  |
| 514 |  |  [Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data](https://openreview.net/forum?id=r1g6ogrtDr) |  | 0 | Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. an upright face with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment and utilizes this information to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to... | David W. Romero, Mark Hoogendoorn |  |
| 515 |  |  [Mixed Precision DNNs: All you need is a good parametrization](https://openreview.net/forum?id=Hyx0slrFvH) |  | 0 | Efficient deep neural network (DNN) inference on mobile or embedded devices typically involves quantization of the network parameters and activations. In particular, mixed precision networks achieve better performance than networks with homogeneous bitwidth for the same size constraint. Since choosing the optimal bitwidths is not straight forward, training methods, which can learn them, are desirable. Differentiable quantization with straight-through gradients allows to learn the quantizer's parameters using gradient methods. We show that a suited parametrization of the quantizer is the key to achieve a stable training and a good final performance. Specifically, we propose to parametrize the quantizer with the step size and dynamic... | Akira Nakamura, Fabien Cardinaux, Javier Alonso García, Kazuki Yoshiyama, Lukas Mauch, Stefan Uhlich, Stephen Tiedemann, Thomas Kemp |  |
| 516 |  |  [Information Geometry of Orthogonal Initializations and Training](https://openreview.net/forum?id=rkg1ngrFPr) |  | 0 | Recently mean field theory has been successfully used to analyze properties of wide, random neural networks. It gave rise to a prescriptive theory for initializing feed-forward neural networks with orthogonal weights, which ensures that both the forward propagated activations and the backpropagated gradients are near \(\ell_2\) isometries and as a consequence training is orders of magnitude faster. Despite strong empirical performance, the mechanisms by which critical initializations confer an advantage in the optimization of deep neural networks are poorly understood. Here we show a novel connection between the maximum curvature of the optimization landscape (gradient smoothness) as measured by the Fisher information matrix (FIM)... | Il Memming Park, Piotr Aleksander Sokól |  |
| 517 |  |  [Extreme Classification via Adversarial Softmax Approximation](https://openreview.net/forum?id=rJxe3xSYDS) |  | 0 | Training a classifier over a large number of classes, known as 'extreme classification', has become a topic of major interest with applications in technology, science, and e-commerce. Traditional softmax regression induces a gradient cost proportional to the number of classes C, which often is prohibitively expensive. A popular scalable softmax approximation relies on uniform negative sampling, which suffers from slow convergence due a poor signal-to-noise ratio. In this paper, we propose a simple training method for drastically enhancing the gradient signal by drawing negative samples from an adversarial model that mimics the data distribution. Our contributions are three-fold: (i) an adversarial sampling mechanism that produces... | Robert Bamler, Stephan Mandt |  |
| 518 |  |  [Learning Nearly Decomposable Value Functions Via Communication Minimization](https://openreview.net/forum?id=HJx-3grYDB) |  | 0 | Reinforcement learning encounters major challenges in multi-agent settings, such as scalability and non-stationarity. Recently, value function factorization learning emerges as a promising way to address these challenges in collaborative multi-agent systems. However, existing methods have been focusing on learning fully decentralized value functions, which are not efficient for tasks requiring communication. To address this limitation, this paper presents a novel framework for learning nearly decomposable Q-functions (NDQ) via communication minimization, with which agents act on their own most of the time but occasionally send messages to other agents in order for effective coordination. This framework hybridizes value function... | Chongjie Zhang, Chongyi Zheng, Jianhao Wang, Tonghan Wang |  |
| 519 |  |  [Robust Subspace Recovery Layer for Unsupervised Anomaly Detection](https://openreview.net/forum?id=rylb3eBtwr) |  | 0 | We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a \`\`manifold" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets... | ChiehHsin Lai, Dongmian Zou, Gilad Lerman |  |
| 520 |  |  [Learning to Coordinate Manipulation Skills via Skill Behavior Diversification](https://openreview.net/forum?id=ryxB2lBtvH) |  | 0 | When mastering a complex manipulation task, humans often decompose the task into sub-skills of their body parts, practice the sub-skills independently, and then execute the sub-skills together. Similarly, a robot with multiple end-effectors can perform complex tasks by coordinating sub-skills of each end-effector. To realize temporal and behavioral coordination of skills, we propose a modular framework that first individually trains sub-skills of each end-effector with skill behavior diversification, and then learns to coordinate end-effectors using diverse behaviors of the skills. We demonstrate that our proposed framework is able to efficiently coordinate skills to solve challenging collaborative control tasks such as picking up... | Jingyun Yang, Joseph J. Lim, Youngwoon Lee |  |
| 521 |  |  [NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search](https://openreview.net/forum?id=SJx9ngStPH) |  | 0 | One-shot neural architecture search (NAS) has played a crucial role in making NAS methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientific study of these components, we introduce a general framework for one-shot NAS that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that draws on the recent large-scale tabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS methods. To showcase the framework, we compare several state-of-the-art one-shot NAS methods, examine how... | Arber Zela, Frank Hutter, Julien Siems |  |
| 522 |  |  [Conservative Uncertainty Estimation By Fitting Prior Networks](https://openreview.net/forum?id=BJlahxHYDS) |  | 0 | Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for estimating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never underestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard supervised learning pipelines. We provide experimental evaluation of random priors on calibration and... | Kamil Ciosek, Katja Hofmann, Richard E. Turner, Ryota Tomioka, Vincent Fortuin |  |
| 523 |  |  [Understanding Generalization in Recurrent Neural Networks](https://openreview.net/forum?id=rkgg6xBYDH) |  | 0 | In this work, we develop the theory for analyzing the generalization performance of recurrent neural networks. We first present a new generalization bound for recurrent neural networks based on matrix 1-norm and Fisher-Rao norm. The definition of Fisher-Rao norm relies on a structural lemma about the gradient of RNNs. This new generalization bound assumes that the covariance matrix of the input data is positive definite, which might limit its use in practice. To address this issue, we propose to add random noise to the input data and prove a generalization bound for training with random noise, which is an extension of the former one. Compared with existing results, our generalization bounds have no explicit dependency on the size... | Dacheng Tao, Fengxiang He, Zhuozhuo Tu |  |
| 524 |  |  [The Shape of Data: Intrinsic Distance for Data Distributions](https://openreview.net/forum?id=HyebplHYwB) |  | 0 | The ability to represent and compare machine learning models is crucial in order to quantify subtle model changes, evaluate generative models, and gather insights on neural network architectures. Existing techniques for comparing data distributions focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. We develop a first-of-its-kind intrinsic and multi-scale method for characterizing and comparing data manifolds, using a lower-bound of the spectral variant of the Gromov-Wasserstein inter-manifold distance, which compares all data moments. In a thorough experimental study, we demonstrate that our method effectively discerns the structure of data manifolds even on unaligned data... | Alexander M. Bronstein, Anton Tsitsulin, Davide Mottin, Emmanuel Müller, Ivan V. Oseledets, Marina Munkhoeva, Panagiotis Karras |  |
| 525 |  |  [How to 0wn the NAS in Your Spare Time](https://openreview.net/forum?id=S1erpeBFPB) |  | 0 | New data processing pipelines and novel network architectures increasingly drive the success of deep learning. In consequence, the industry considers top-performing architectures as intellectual property and devotes considerable computational resources to discovering such architectures through neural architecture search (NAS). This provides an incentive for adversaries to steal these novel architectures; when used in the cloud, to provide Machine Learning as a Service (MLaaS), the adversaries also have an opportunity to reconstruct the architectures by exploiting a range of hardware side-channels. However, it is challenging to reconstruct novel architectures and pipelines without knowing the computational graph (e.g., the layers,... | Dana DachmanSoled, Michael Davinroy, Sanghyun Hong, Tudor Dumitras, Yigitcan Kaya |  |
| 526 |  |  [Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation](https://openreview.net/forum?id=B1xSperKvH) |  | 0 | Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a... | Gopalakrishnan Srinivasan, Kaushik Roy, Nitin Rathi, Priyadarshini Panda |  |
| 527 |  |  [Breaking Certified Defenses: Semantic Adversarial Examples with Spoofed robustness Certificates](https://openreview.net/forum?id=HJxdTxHYvB) |  | 0 | Defenses against adversarial attacks can be classified into certified and non-certified. Certifiable defenses make networks robust within a certain $\ell_p$-bounded radius, so that it is impossible for the adversary to make adversarial examples in the certificate bound. We present an attack that maintains the imperceptibility property of adversarial examples while being outside of the certified radius. Furthermore, the proposed "Shadow Attack" can fool certifiably robust networks by producing an imperceptible adversarial example that gets misclassified and produces a strong \`\`spoofed'' certificate. | Ali Shafahi, Amin Ghiasi, Tom Goldstein |  |
| 528 |  |  [Query-efficient Meta Attack to Deep Neural Networks](https://openreview.net/forum?id=Skxd6gSYDS) |  | 0 | Black-box attack methods aim to infer suitable attack patterns to targeted DNN models by only using output feedback of the models and the corresponding input queries. However, due to lack of prior and inefficiency in leveraging the query and feedback information, existing methods are mostly query-intensive for obtaining effective attack patterns. In this work, we propose a meta attack approach that is capable of attacking a targeted model with much fewer queries. Its high query-efficiency stems from effective utilization of meta learning approaches in learning generalizable prior abstraction from the previously observed attack patterns and exploiting such prior to help infer attack patterns from only a few queries and outputs.... | Hu Zhang, Jiashi Feng, Jiawei Du, Joey Tianyi Zhou, Yi Yang |  |
| 529 |  |  [Massively Multilingual Sparse Word Representations](https://openreview.net/forum?id=HyeYTgrFPB) |  | 0 | In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative... | Gábor Berend |  |
| 530 |  |  [Monotonic Multihead Attention](https://openreview.net/forum?id=Hyg96gBKPS) |  | 0 | Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approach for this task either apply a fixed policy on transformer, or a learnable monotonic attention on a weaker recurrent neural network based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which introduced the monotonic attention mechanism to multihead attention. We also introduced two novel interpretable approaches for latency control that are specifically designed for multiple attentions. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art... | James Cross, Jiatao Gu, Juan Miguel Pino, Liezl Puzon, Xutai Ma |  |
| 531 |  |  [Gradients as Features for Deep Representation Learning](https://openreview.net/forum?id=BkeoaeHKDS) |  | 0 | We address the challenging problem of deep representation learning -- the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We demonstrate that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradients. Our method is evaluated... | Fangzhou Mu, Yin Li, Yingyu Liang |  |
| 532 |  |  [Pay Attention to Features, Transfer Learn Faster CNNs](https://openreview.net/forum?id=ryxyCeHtPB) |  | 0 | Deep convolutional neural networks are now widely deployed in vision applications, but a limited size of training data can restrict their task performance. Transfer learning offers the chance for CNNs to learn with limited data samples by transferring knowledge from models pretrained on large datasets. Blindly transferring all learned features from the source dataset, however, brings unnecessary computation to CNNs on the target task. In this paper, we propose attentive feature distillation and selection (AFDS), which not only adjusts the strength of transfer learning regularization but also dynamically determines the important features to transfer. By deploying AFDS on ResNet-101, we achieved a state-of-the-art computation... | ChengZhong Xu, Dejing Dou, Kafeng Wang, Xingjian Li, Xitong Gao, Yiren Zhao |  |
| 533 |  |  [Program Guided Agent](https://openreview.net/forum?id=BkxUvnEYDH) |  | 0 | Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only... | Joseph J. Lim, ShaoHua Sun, TeLin Wu |  |
| 534 |  |  [Sparse Coding with Gated Learned ISTA](https://openreview.net/forum?id=BygPO2VKPH) |  | 0 | In this paper, we study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems. Following assumptions made by prior works, we first discover that the code components in its estimations may be lower than expected, i.e., require gains, and to address this problem, a gated mechanism amenable to theoretical analysis is then introduced. Specific design of the gates is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain gates, we further introduce overshoot gates for compensating insufficient step size in LISTA. Extensive empirical results confirm our theoretical findings and verify the effectiveness of our method. | Changshui Zhang, Kailun Wu, Yiwen Guo, Ziang Li |  |
| 535 |  |  [Graph Neural Networks Exponentially Lose Expressive Power for Node Classification](https://openreview.net/forum?id=S1ldO2EFPr) |  | 0 | Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the... | Kenta Oono, Taiji Suzuki |  |
| 536 |  |  [Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells](https://openreview.net/forum?id=rJljdh4KDH) |  | 0 | Unsupervised text encoding models have recently fueled substantial progress in NLP. The key idea is to use neural networks to convert words in texts to vector space representations (embeddings) based on word positions in a sentence and their contexts, which are suitable for end-to-end training of downstream tasks. We see a strikingly similar situation in spatial analysis, which focuses on incorporating both absolute positions and spatial contexts of geographic objects such as POIs into models. A general-purpose representation model for space is valuable for a multitude of tasks. However, no such general model exists to date beyond simply applying discretization or feed-forward nets to coordinates, and little effort has been put... | Bo Yan, Gengchen Mai, Krzysztof Janowicz, Ling Cai, Ni Lao, Rui Zhu |  |
| 537 |  |  [InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization](https://openreview.net/forum?id=r1lfF2NYvH) |  | 0 | This paper studies learning the representations of whole graphs in both unsupervised and semi-supervised scenarios. Graph-level representations are critical in a variety of real-world applications such as predicting the properties of molecules and community analysis in social networks. Traditional graph kernel based methods are simple, yet effective for obtaining fixed-length representations for graphs but they suffer from poor generalization due to hand-crafted designs. There are also some recent methods based on language models (e.g. graph2vec) but they tend to only consider certain substructures (e.g. subtrees) as graph representatives. Inspired by recent progress of unsupervised representation learning, in this paper we... | FanYun Sun, Jian Tang, Jordan Hoffmann, Vikas Verma |  |
| 538 |  |  [On Robustness of Neural Ordinary Differential Equations](https://openreview.net/forum?id=B1e9Y2NYvS) |  | 0 | Neural ordinary differential equations (ODEs) have been attracting increasing attention in various research domains recently. There have been some works studying optimization issues and approximation capabilities of neural ODEs, but their robustness is still yet unclear. In this work, we fill this important gap by exploring robustness properties of neural ODEs both empirically and theoretically. We first present an empirical study on the robustness of the neural ODE-based networks (ODENets) by exposing them to inputs with various types of perturbations and subsequently investigating the changes of the corresponding outputs. In contrast to conventional convolutional neural networks (CNNs), we find that the ODENets are more robust... | Hanshu Yan, Jiashi Feng, Jiawei Du, Vincent Y. F. Tan |  |
| 539 |  |  [Defending Against Physically Realizable Attacks on Image Classification](https://openreview.net/forum?id=H1xscnEKDr) |  | 0 | We study the problem of defending deep neural network approaches for image classification from physically realizable attacks. First, we demonstrate that the two most scalable and effective methods for learning robust models, adversarial training with PGD attacks and randomized smoothing, exhibit very limited effectiveness against three of the highest profile physical attacks. Next, we propose a new abstract adversarial model, rectangular occlusion attacks, in which an adversary places a small adversarially crafted rectangle in an image, and develop two approaches for efficiently computing the resulting adversarial examples. Finally, we demonstrate that adversarial training using our new attack yields image classification models... | Liang Tong, Tong Wu, Yevgeniy Vorobeychik |  |
| 540 |  |  [Estimating Gradients for Discrete Random Variables by Sampling without Replacement](https://openreview.net/forum?id=rklEj2EFvB) |  | 0 | We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators... | Herke van Hoof, Max Welling, Wouter Kool |  |
| 541 |  |  [Learning to Control PDEs with Differentiable Physics](https://openreview.net/forum?id=HyeSin4FPB) |  | 0 | Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We present a novel hierarchical predictor-corrector scheme which enables neural networks to learn to understand and control complex nonlinear physical systems over long time frames. We propose to split the problem into two distinct tasks: planning and control. To this end, we introduce a predictor... | Nils Thuerey, Philipp Holl, Vladlen Koltun |  |
| 542 |  |  [Intensity-Free Learning of Temporal Point Processes](https://openreview.net/forum?id=HygOjhEYDH) |  | 0 | Temporal point processes are the dominant paradigm for modeling sequences of events happening at irregular intervals. The standard way of learning in such models is by estimating the conditional intensity function. However, parameterizing the intensity function usually incurs several trade-offs. We show how to overcome the limitations of intensity-based approaches by directly modeling the conditional distribution of inter-event times. We draw on the literature on normalizing flows to design models that are flexible and efficient. We additionally propose a simple mixture model that matches the flexibility of flow-based models, but also permits sampling and computing moments in closed form. The proposed models achieve... | Marin Bilos, Oleksandr Shchur, Stephan Günnemann |  |
| 543 |  |  [A Signal Propagation Perspective for Pruning Neural Networks at Initialization](https://openreview.net/forum?id=HJeTo2VFwH) |  | 0 | Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, by noting connection sensitivity as a form of gradient, we formally characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. Moreover, we analyze... | Namhoon Lee, Philip H. S. Torr, Stephen Gould, Thalaiyasingam Ajanthan |  |
| 544 |  |  [Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets](https://openreview.net/forum?id=BJlRs34Fvr) |  | 0 | Skip connections are an essential component of current state-of-the-art deep neural networks (DNNs) such as ResNet, WideResNet, DenseNet, and ResNeXt. Despite their huge success in building deeper and more powerful DNNs, we identify a surprising \emph{security weakness} of skip connections in this paper. Use of skip connections \textit{allows easier generation of highly transferable adversarial examples}. Specifically, in ResNet-like (with skip connections) neural networks, gradients can backpropagate through either skip connections or residual modules. We find that using more gradients from the skip connections rather than the residual modules according to a decay factor, allows one to craft adversarial examples with high... | Dongxian Wu, James Bailey, ShuTao Xia, Xingjun Ma, Yisen Wang |  |
| 545 |  |  [White Noise Analysis of Neural Networks](https://openreview.net/forum?id=H1ebhnEYDH) |  | 0 | A white noise analysis of modern deep neural networks is presented to unveil their biases at the whole network level or the single neuron level. Our analysis is based on two popular and related methods in psychophysics and neurophysiology namely classification images and spike triggered analysis. These methods have been widely used to understand the underlying mechanisms of sensory systems in humans and monkeys. We leverage them to investigate the inherent biases of deep neural networks and to obtain a first-order approximation of their functionality. We emphasize on CNNs since they are currently the state of the art methods in computer vision and are a decent model of human visual processing. In addition, we study multi-layer... | Ali Borji, Sikun Lin |  |
| 546 |  |  [Neural Machine Translation with Universal Visual Representation](https://openreview.net/forum?id=Byl8hhNYPS) |  | 0 | Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with... | Eiichiro Sumita, Hai Zhao, Kehai Chen, Masao Utiyama, Rui Wang, Zhuosheng Zhang, Zuchao Li |  |
| 547 |  |  [Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds](https://openreview.net/forum?id=BJeKh3VYDH) |  | 0 | Point clouds, as a form of Lagrangian representation, allow for powerful and flexible applications in a large number of computational disciplines. We propose a novel deep-learning method to learn stable and temporally coherent feature spaces for points clouds that change over time. We identify a set of inherent problems with these approaches: without knowledge of the time dimension, the inferred solutions can exhibit strong flickering, and easy solutions to suppress this flickering can result in undesirable local minima that manifest themselves as halo structures. We propose a novel temporal loss function that takes into account higher time derivatives of the point positions, and encourages mingling, i.e., to prevent the... | Lukas Prantl, Nils Thuerey, Nuttapong Chentanez, Stefan Jeschke |  |
| 548 |  |  [PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search](https://openreview.net/forum?id=BJlS634tPr) |  | 0 | Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-net and searching for an optimal architecture. In this paper, we present a novel approach, namely Partially-Connected DARTS, by sampling a small part of super-net to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels.... | GuoJun Qi, Hongkai Xiong, Lingxi Xie, Qi Tian, Xiaopeng Zhang, Xin Chen, Yuhui Xu |  |
| 549 |  |  [Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach](https://openreview.net/forum?id=rkxZyaNtwB) |  | 0 | Motivated by applications to machine learning and imaging science, we study a class of online and stochastic optimization problems with loss functions that are not Lipschitz continuous; in particular, the loss functions encountered by the optimizer could exhibit gradient singularities or be singular themselves. Drawing on tools and techniques from Riemannian geometry, we examine a Riemann–Lipschitz (RL) continuity condition which is tailored to the singularity landscape of the problem’s loss functions. In this way, we are able to tackle cases beyond the Lipschitz framework provided by a global norm, and we derive optimal regret bounds and last iterate convergence results through the use of regularized learning methods (such as... | Elena Veronica Belmega, Kimon Antonakopoulos, Panayotis Mertikopoulos |  |
| 550 |  |  [Enhancing Adversarial Defense by k-Winners-Take-All](https://openreview.net/forum?id=Skgvy64tvr) |  | 0 | We propose a simple change to existing neural network structures for better defending against gradient-based adversarial attacks. Instead of using popular activation functions (such as ReLU), we advocate the use of k-Winners-Take-All (k-WTA) activation, a C0 discontinuous function that purposely invalidates the neural network model’s gradient at densely distributed input data points. The proposed k-WTA activation can be readily used in nearly all existing networks and training methods with no significant overhead. Our proposal is theoretically rationalized. We analyze why the discontinuities in k-WTA networks can largely prevent gradient-based search of adversarial examples and why they at the same time remain innocuous to the... | Chang Xiao, Changxi Zheng, Peilin Zhong |  |
| 551 |  |  [Encoding word order in complex embeddings](https://openreview.net/forum?id=Hke-WTVtwr) |  | 0 | Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions.... | Benyou Wang, Christina Lioma, Donghao Zhao, Jakob Grue Simonsen, Peng Zhang, Qiuchi Li |  |
| 552 |  |  [DDSP: Differentiable Digital Signal Processing](https://openreview.net/forum?id=B1x1ma4tDr) |  | 0 | Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not utilize existing knowledge of how sound is generated and perceived. A third approach (vocoders/synthesizers) successfully incorporates strong domain knowledge of signal processing and perception, but has been less actively researched due to limited expressivity and difficulty integrating with modern auto-differentiation-based machine learning methods. In this paper, we introduce the Differentiable Digital Signal Processing (DDSP) library, which enables direct integration of classic signal processing elements with deep learning methods.... | Adam Roberts, Chenjie Gu, Jesse H. Engel, Lamtharn Hantrakul |  |
| 553 |  |  [Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation](https://openreview.net/forum?id=SJl5Np4tPr) |  | 0 | Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms predict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate... | HsinYing Lee, HungYu Tseng, JiaBin Huang, MingHsuan Yang |  |
| 554 |  |  [Ridge Regression: Structure, Cross-Validation, and Sketching](https://openreview.net/forum?id=HklRwaEKwB) |  | 0 | We study the following three fundamental problems about ridge regression: (1) what is the structure of the estimator? (2) how to correctly use cross-validation to choose the regularization parameter? and (3) how to accelerate computation without losing too much accuracy? We consider the three problems in a unified large-data linear model. We give a precise representation of ridge regression as a covariance matrix-dependent linear combination of the true parameter and the noise. We study the bias of $K$-fold cross-validation for choosing the regularization parameter, and propose a simple bias-correction. We analyze the accuracy of primal and dual sketching for ridge regression, showing they are surprisingly accurate. Our results are... | Edgar Dobriban, Sifan Liu |  |
| 555 |  |  [Finite Depth and Width Corrections to the Neural Tangent Kernel](https://openreview.net/forum?id=SJgndT4KwB) |  | 0 | We prove the precise scaling, at finite depth and width, for the mean and variance of the neural tangent kernel (NTK) in a randomly initialized ReLU network. The standard deviation is exponential in the ratio of network depth to width. Thus, even in the limit of infinite overparameterization, the NTK is not deterministic if depth and width simultaneously tend to infinity. Moreover, we prove that for such deep and wide networks, the NTK has a non-trivial evolution during training by showing that the mean of its first SGD update is also exponential in the ratio of network depth to width. This is sharp contrast to the regime where depth is fixed and network width is very large. Our results suggest that, unlike relatively shallow and... | Boris Hanin, Mihai Nica |  |
| 556 |  |  [Meta-Learning without Memorization](https://openreview.net/forum?id=BklEFpEYwS) |  | 0 | The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of... | Chelsea Finn, George Tucker, Mingyuan Zhou, Mingzhang Yin, Sergey Levine |  |
| 557 |  |  [Influence-Based Multi-Agent Exploration](https://openreview.net/forum?id=BJgy96EYvr) |  | 0 | Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one... | Chongjie Zhang, Jianhao Wang, Tonghan Wang, Yi Wu |  |
| 558 |  |  [Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs](https://openreview.net/forum?id=SJeqs6EFvB) |  | 0 | We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy program modeled by a graph structure, our model makes a sequence of predictions including the position of bug nodes and corresponding graph edits to produce a fix. Unlike previous works that use deep neural networks, our approach targets bugs that are more complex and semantic in nature (i.e.~bugs that require adding or deleting statements to fix). We have realized our approach in a tool called HOPPITY. By training on 290,715 Javascript code change commits on Github, HOPPITY correctly detects and fixes bugs in 9,490 out of 36,361 programs in... | Elizabeth Dinella, Hanjun Dai, Ke Wang, Le Song, Mayur Naik, Ziyang Li |  |
| 559 |  |  [Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations](https://openreview.net/forum?id=BJge3TNKwH) |  | 0 | Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training. Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS). We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of... | Andrea Soltoggio, Nicholas A. Ketz, Praveen K. Pilly, Soheil Kolouri |  |
| 560 |  |  [How much Position Information Do Convolutional Neural Networks Encode?](https://openreview.net/forum?id=rJeB36NKvB) |  | 0 | In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where... | Md. Amirul Islam, Neil D. B. Bruce, Sen Jia |  |
| 561 |  |  [Hamiltonian Generative Networks](https://openreview.net/forum?id=HJenn6VFvB) |  | 0 | The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images)... | Aleksandar Botev, Andrew Jaegle, Danilo J. Rezende, Irina Higgins, Peter Toth, Sébastien Racanière |  |
| 562 |  |  [CoPhy: Counterfactual Learning of Physical Dynamics](https://openreview.net/forum?id=SkeyppEFvS) |  | 0 | Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene.... | Christian Wolf, Fabien Baradel, Greg Mori, Julien Mille, Natalia Neverova |  |
| 563 |  |  [Estimating counterfactual treatment outcomes over time through adversarially balanced representations](https://openreview.net/forum?id=BJg866NFvB) |  | 0 | Identifying when to give treatments to patients and how to select among multiple treatments over time are important medical problems with a few existing solutions. In this paper, we introduce the Counterfactual Recurrent Network (CRN), a novel sequence-to-sequence model that leverages the increasingly available patient observational data to estimate treatment effects over time and answer such medical questions. To handle the bias from time-varying confounders, covariates affecting the treatment assignment policy in the observational data, CRN uses domain adversarial training to build balancing representations of the patient history. At each timestep, CRN constructs a treatment invariant representation which removes the association... | Ahmed M. Alaa, Ioana Bica, James Jordon, Mihaela van der Schaar |  |
| 564 |  |  [Gradientless Descent: High-Dimensional Zeroth-Order Optimization](https://openreview.net/forum?id=Skep6TVYDB) |  | 0 | Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for {\it any monotone transform} of a smooth and strongly convex objective with latent dimension $k \ge n$, we present a novel analysis that shows convergence within an $\epsilon$-ball of the optimum in $O(kQ\log(n)\log(R/\epsilon))$ evaluations, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number.... | Chansoo Lee, Daniel Golovin, Greg Kochanski, John Karro, Qiuyi (Richard) Zhang, Xingyou Song |  |
| 565 |  |  [Conditional Learning of Fair Representations](https://openreview.net/forum?id=Hkekl0NFPr) |  | 0 | We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations... | Amanda Coston, Geoffrey J. Gordon, Han Zhao, Tameem Adel |  |
| 566 |  |  [Inductive Matrix Completion Based on Graph Neural Networks](https://openreview.net/forum?id=ByxxgCEYDS) |  | 0 | We propose an inductive matrix completion model without using side information. By factorizing the (rating) matrix into the product of low-dimensional latent embeddings of rows (users) and columns (items), a majority of existing matrix completion methods are transductive, since the learned embeddings cannot generalize to unseen rows/columns or to new matrices. To make matrix completion inductive, most previous works use content (side information), such as user's age or movie's genre, to make predictions. However, high-quality content is not always available, and can be hard to extract. Under the extreme setting where not any side information is available other than the matrix to complete, can we still learn an inductive matrix... | Muhan Zhang, Yixin Chen |  |
| 567 |  |  [Duration-of-Stay Storage Assignment under Uncertainty](https://openreview.net/forum?id=Hkx7xRVYDr) |  | 0 | Storage assignment, the act of choosing what goods are placed in what locations in a warehouse, is a central problem of supply chain logistics. Past literature has shown that the optimal method to assign pallets is to arrange them in increasing duration of stay in the warehouse (the Duration-of-Stay, or DoS, method), but the methodology requires perfect prior knowledge of DoS for each pallet, which is unknown and uncertain under realistic conditions. Attempts to predict DoS have largely been unfruitful due to the multi-valuedness nature (every shipment contains multiple identical pallets with different DoS) and data sparsity induced by lack of matching historical conditions. In this paper, we introduce a new framework for storage... | Daniel Wintz, Elliott Wolf, Michael Lingzhi Li |  |
| 568 |  |  [Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks](https://openreview.net/forum?id=HklSeREtPB) |  | 0 | Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, also the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuit of the rodent and fruit fly. We trained recurrent neural networks (RNNs) to estimate head direction through integration of... | Christopher J. Cueva, Matthew Chin, Peter Y. Wang, XueXin Wei |  |
| 569 |  |  [Deep neuroethology of a virtual rodent](https://openreview.net/forum?id=SyxrxR4KPS) |  | 0 | Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize... | Bence Olveczky, Diego Aldarondo, Greg Wayne, Jesse Marshall, Josh Merel, Yuval Tassa |  |
| 570 |  |  [Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation](https://openreview.net/forum?id=S1glGANtDr) |  | 0 | Infinite horizon off-policy policy evaluation is a highly challenging task due to the excessively large variance of typical importance sampling (IS) estimators. Recently, Liu et al. (2018) proposed an approach that significantly reduces the variance of infinite-horizon off-policy evaluation by estimating the stationary density ratio, but at the cost of introducing potentially high risks due to the error in density ratio estimation. In this paper, we develop a bias-reduced augmentation of their method, which can take advantage of a learned value function to obtain higher accuracy. Our method is doubly robust in that the bias vanishes when either the density ratio or value function estimation is perfect. In general, when either of... | Dengyong Zhou, Lihong Li, Qiang Liu, Yihao Feng, Ziyang Tang |  |
| 571 |  |  [Learning Compositional Koopman Operators for Model-Based Control](https://openreview.net/forum?id=H1ldzA4tPr) |  | 0 | Finding an embedding space for a linear approximation of a nonlinear dynamical system enables efficient system identification and control synthesis. The Koopman operator theory lays the foundation for identifying the nonlinear-to-linear coordinate transformations with data-driven methods. Recently, researchers have proposed to use deep neural networks as a more expressive class of basis functions for calculating the Koopman operators. These approaches, however, assume a fixed dimensional state space; they are therefore not applicable to scenarios with a variable number of objects. In this paper, we propose to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and... | Antonio Torralba, Dina Katabi, Hao He, Jiajun Wu, Yunzhu Li |  |
| 572 |  |  [CLEVRER: Collision Events for Video Representation and Reasoning](https://openreview.net/forum?id=HkxYzANYDB) |  | 0 | The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER) dataset, a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of question: descriptive (e.g., ‘what color’), explanatory... | Antonio Torralba, Chuang Gan, Jiajun Wu, Joshua B. Tenenbaum, Kexin Yi, Pushmeet Kohli, Yunzhu Li |  |
| 573 |  |  [The Logical Expressiveness of Graph Neural Networks](https://openreview.net/forum?id=r1lZ7AEKvB) |  | 0 | The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or false) can be expressed by GNNs. We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms... | Egor V. Kostylev, Jorge Pérez, Juan L. Reutter, Juan Pablo Silva, Mikaël Monet, Pablo Barceló |  |
| 574 |  |  [The Break-Even Point on Optimization Trajectories of Deep Neural Networks](https://openreview.net/forum?id=r1g87C4KwB) |  | 0 | The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the "\`\`break-even" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and... | Devansh Arpit, Jacek Tabor, Krzysztof J. Geras, Kyunghyun Cho, Maciej Szymczak, Stanislav Fort, Stanislaw Jastrzebski |  |
| 575 |  |  [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS) |  | 0 | Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT~\citep{devlin2018bert}. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model... | Kevin Gimpel, Mingda Chen, Piyush Sharma, Radu Soricut, Sebastian Goodman, Zhenzhong Lan |  |
| 576 |  |  [Disentangling neural mechanisms for perceptual grouping](https://openreview.net/forum?id=HJxrVA4FDS) |  | 0 | Forming perceptual groups and individuating objects in visual scenes is an essential step towards visual intelligence. This ability is thought to arise in the brain from computations implemented by bottom-up, horizontal, and top-down connections between neurons. However, the relative contributions of these connections to perceptual grouping are poorly understood. We address this question by systematically evaluating neural network architectures featuring combinations bottom-up, horizontal, and top-down connections on two synthetic visual tasks, which stress low-level "Gestalt" vs. high-level object cues for perceptual grouping. We show that increasing the difficulty of either task strains learning for networks that rely solely on... | Drew Linsley, Junkyung Kim, Kalpit Thakkar, Thomas Serre |  |
| 577 |  |  [Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees](https://openreview.net/forum?id=rJgJDAVKvB) |  | 0 | We propose a meta path planning algorithm named \emph{Neural Exploration-Exploitation Trees~(NEXT)} for learning from prior experience for solving new path planning problems in high dimensional continuous state and action spaces. Compared to more classical sampling-based methods like RRT, our approach achieves much better sample efficiency in high-dimensions and can benefit from prior experience of planning in similar environments. More specifically, NEXT exploits a novel neural architecture which can learn promising search directions from problem structures. The learned prior is then integrated into a UCB-type algorithm to achieve an online balance between \emph{exploration} and \emph{exploitation} when solving a new problem. We... | Binghong Chen, Bo Dai, Guo Ye, Han Liu, Le Song, Qinjie Lin |  |
| 578 |  |  [Symplectic Recurrent Neural Networks](https://openreview.net/forum?id=BkgYPREtPr) |  | 0 | We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. SRNNs model the Hamiltonian function of the system by a neural networks, and leverage symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show SRNNs succeed reliably on complex and noisy Hamiltonian systems. Finally, we show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards. | Jianyu Zhang, Léon Bottou, Martín Arjovsky, Zhengdao Chen |  |
| 579 |  |  [Asymptotics of Wide Networks from Feynman Diagrams](https://openreview.net/forum?id=S1gFvANKDS) |  | 0 | Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically. | Ethan Dyer, Guy GurAri |  |
| 580 |  |  [Learning The Difference That Makes A Difference With Counterfactually-Augmented Data](https://openreview.net/forum?id=Sklgs0NFvr) |  | 0 | Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural... | Divyansh Kaushik, Eduard H. Hovy, Zachary Chase Lipton |  |
| 581 |  |  [Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?](https://openreview.net/forum?id=r1genAVKPB) |  | 0 | Modern deep learning methods provide effective means to learn good representations. However, is a good representation itself sufficient for sample efficient reinforcement learning? This question has largely been studied only with respect to (worst-case) approximation error, in the more classical approximate dynamic programming literature. With regards to the statistical viewpoint, this question is largely unexplored, and the extant body of literature mainly focuses on conditions which \emph{permit} sample efficient reinforcement learning with little understanding of what are \emph{necessary} conditions for efficient reinforcement learning. This work shows that, from the statistical viewpoint, the situation is far subtler than... | Lin F. Yang, Ruosong Wang, Sham M. Kakade, Simon S. Du |  |
| 582 |  |  [Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=B1xm3RVtwB) |  | 0 | In recent years we have seen fast progress on a number of benchmark problems in AI, with modern methods achieving near or super human performance in Go, Poker and Dota. One common aspect of all of these challenges is that they are by design adversarial or, technically speaking, zero-sum. In contrast to these settings, success in the real world commonly requires humans to collaborate and communicate with others, in settings that are, at least partially, cooperative. In the last year, the card game Hanabi has been established as a new benchmark environment for AI to fill this gap. In particular, Hanabi is interesting to humans since it is entirely focused on theory of mind, i.e. the ability to effectively reason over the intentions,... | Hengyuan Hu, Jakob N. Foerster |  |
| 583 |  |  [Network Deconvolution](https://openreview.net/forum?id=rkeu30EtvS) |  | 0 | Convolution is a central operation in Convolutional Neural Networks (CNNs), which applies a kernel to overlapping regions shifted across the image. However, because of the strong correlations in real-world image data, convolutional kernels are in effect re-learning redundant data. In this work, we show that this redundancy has made neural network training challenging, and propose network deconvolution, a procedure which optimally removes pixel-wise and channel-wise correlations before the data is fed into each layer. Network deconvolution can be efficiently calculated at a fraction of the computational cost of a convolution layer. We also show that the deconvolution filters in the first layer of the network resemble the... | Anton Mitrokhin, Chengxi Ye, Cornelia Fermüller, Hua He, James A. Yorke, Matthew Evanusa, Tom Goldstein, Yiannis Aloimonos |  |
| 584 |  |  [Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension](https://openreview.net/forum?id=ryxjnREFwH) |  | 0 | Integrating distributed representations with symbolic operations is essential for reading comprehension requiring complex reasoning, such as counting, sorting and arithmetics, but most existing approaches are hard to scale to more domains or more complex reasoning. In this work, we propose the Neural Symbolic Reader (NeRd), which includes a reader, e.g., BERT, to encode the passage and question, and a programmer, e.g., LSTM, to generate a program that is executed to produce the answer. Compared to previous works, NeRd is more scalable in two aspects: (1) domain-agnostic, i.e., the same neural architecture works for different domains; (2) compositional, i.e., when needed, complex programs can be generated by recursively applying the... | Adams Wei Yu, Chen Liang, Dawn Song, Denny Zhou, Quoc V. Le, Xinyun Chen |  |
| 585 |  |  [Real or Not Real, that is the Question](https://openreview.net/forum?id=B1lPaCNtPB) |  | 0 | While generative adversarial networks (GAN) have been widely adopted in various topics, in this paper we generalize the standard GAN to a new perspective by treating realness as a random variable that can be estimated from multiple angles. In this generalized framework, referred to as RealnessGAN, the discriminator outputs a distribution as the measure of realness. While RealnessGAN shares similar theoretical guarantees with the standard GAN, it provides more insights on adversarial learning. More importantly, compared to multiple baselines, RealnessGAN provides stronger guidance for the generator, achieving improvements on both synthetic and real-world datasets. Moreover, it enables the basic DCGAN architecture to generate... | Bo Dai, Chen Change Loy, Dahua Lin, Yuanbo Xiangli, Yubin Deng |  |
| 586 |  |  [Dream to Control: Learning Behaviors by Latent Imagination](https://openreview.net/forum?id=S1lOTC4tDS) |  | 0 | Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance. | Danijar Hafner, Jimmy Ba, Mohammad Norouzi, Timothy P. Lillicrap |  |
| 587 |  |  [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://openreview.net/forum?id=HJlA0C4tPS) |  | 0 | We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model... | Graham Neubig, Junxian He, Taylor BergKirkpatrick, Xinyi Wang |  |
| 588 |  |  [Emergent Tool Use From Multi-Agent Autocurricula](https://openreview.net/forum?id=SkxpxJBKwS) |  | 0 | Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity... | Bob McGrew, Bowen Baker, Glenn Powell, Igor Mordatch, Ingmar Kanitscheider, Todor M. Markov, Yi Wu |  |
| 589 |  |  [NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search](https://openreview.net/forum?id=HJxyZkBKDr) |  | 0 | Neural architecture search (NAS) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the field of NAS. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various NAS algorithms. NAS-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to NAS-Bench-101: NAS-Bench-201 with a different search space, results on multiple datasets, and more diagnostic information.... | Xuanyi Dong, Yi Yang |  |
| 590 |  |  [Strategies for Pre-training Graph Neural Networks](https://openreview.net/forum?id=HJlWWJSFDH) |  | 0 | Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of... | Bowen Liu, Joseph Gomes, Jure Leskovec, Marinka Zitnik, Percy Liang, Vijay S. Pande, Weihua Hu |  |
| 591 |  |  [Behaviour Suite for Reinforcement Learning](https://openreview.net/forum?id=rygf-kSYwH) |  | 0 | This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source this http URL, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms.... | Andre Saraiva, Benjamin Van Roy, Csaba Szepesvári, David Silver, Eren Sezener, Hado van Hasselt, Ian Osband, John Aslanides, Katrina McKinney, Matteo Hessel, Richard S. Sutton, Satinder Singh, Tor Lattimore, Yotam Doron |  |
| 592 |  |  [FreeLB: Enhanced Adversarial Training for Natural Language Understanding](https://openreview.net/forum?id=BygzbyHFvB) |  | 0 | Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall... | Chen Zhu, Jingjing Liu, Siqi Sun, Tom Goldstein, Yu Cheng, Zhe Gan |  |
| 593 |  |  [Kernelized Wasserstein Natural Gradient](https://openreview.net/forum?id=Hklz71rYvS) |  | 0 | Many machine learning problems can be expressed as the optimization of some cost functional over a parametric family of probability distributions. It is often beneficial to solve such optimization problems using natural gradient methods. These methods are invariant to the parametrization of the family, and thus can yield more effective optimization. Unfortunately, computing the natural gradient is challenging as it requires inverting a high dimensional matrix at each iteration. We propose a general framework to approximate the natural gradient for the Wasserstein metric, by leveraging a dual formulation of the metric restricted to a Reproducing Kernel Hilbert Space. Our approach leads to an estimator for gradient direction that can... | Arthur Gretton, Guido Montúfar, Michael Arbel, Wuchen Li |  |
| 594 |  |  [And the Bit Goes Down: Revisiting the Quantization of Neural Networks](https://openreview.net/forum?id=rJehVyrKwH) |  | 0 | In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification... | Armand Joulin, Benjamin Graham, Hervé Jégou, Pierre Stock, Rémi Gribonval |  |
| 595 |  |  [A Latent Morphology Model for Open-Vocabulary Neural Machine Translation](https://openreview.net/forum?id=BJxSI1SKDH) |  | 0 | Translation into morphologically-rich languages challenges neural machine translation (NMT) models with extremely sparse vocabularies where atomic treatment of surface forms is unrealistic. This problem is typically addressed by either pre-processing words into subword units or performing translation directly at the level of characters. The former is based on word segmentation algorithms optimized using corpus-level statistics with no regard to the translation task. The latter learns directly from translation data but requires rather deep architectures. In this paper, we propose to translate words by modeling word formation through a hierarchical latent variable model which mimics the process of morphological inflection. Our model... | Alexandra Birch, Duygu Ataman, Wilker Aziz |  |
| 596 |  |  [Understanding Why Neural Networks Generalize Well Through GSNR of Parameters](https://openreview.net/forum?id=HyevIJStwH) |  | 0 | As deep neural networks (DNNs) achieve tremendous success across many application domains, researchers tried to explore in many aspects on why they generalize well. In this paper, we provide a novel perspective on these issues using the gradient signal to noise ratio (GSNR) of parameters during training process of DNNs. The GSNR of a parameter is simply defined as the ratio between its gradient's squared mean and variance, over the data distribution. Based on several approximations, we establish a quantitative relationship between model parameters' GSNR and the generalization gap. This relationship indicates that larger GSNR during training process leads to better generalization performance. Futher, we show that, different from... | Guoqing Jiang, Huayan Wang, Jinlong Liu, Ting Chen, Yunzhi Bai |  |
| 597 |  |  [Model Based Reinforcement Learning for Atari](https://openreview.net/forum?id=S1xCPJHtDB) |  | 0 | Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present... | Afroz Mohiuddin, Blazej Osinski, Chelsea Finn, Dumitru Erhan, George Tucker, Henryk Michalewski, Konrad Czechowski, Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Kozakowski, Piotr Milos, Roy H. Campbell, Ryan Sepassi, Sergey Levine |  |
| 598 |  |  [Disagreement-Regularized Imitation Learning](https://openreview.net/forum?id=rkgbYyHtwB) |  | 0 | We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that... | Kianté Brantley, Mikael Henaff, Wen Sun |  |
| 599 |  |  [Stable Rank Normalization for Improved Generalization in Neural Networks and GANs](https://openreview.net/forum?id=H1enKkrFDB) |  | 0 | Exciting new work on generalization bounds for neural networks (NN) given by Bartlett et al. (2017); Neyshabur et al. (2018) closely depend on two parameter- dependant quantities: the Lipschitz constant upper bound and the stable rank (a softer version of rank). Even though these bounds typically have minimal practical utility, they facilitate questions on whether controlling such quantities together could improve the generalization behaviour of NNs in practice. To this end, we propose stable rank normalization (SRN), a novel, provably optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, despite being non-convex, can be shown to have... | Amartya Sanyal, Philip H. S. Torr, Puneet K. Dokania |  |
| 600 |  |  [Measuring the Reliability of Reinforcement Learning Algorithms](https://openreview.net/forum?id=SJlpYJBKvH) |  | 0 | Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the... | Anoop Korattikara, John F. Canny, Samuel Fishman, Sergio Guadarrama, Stephanie C. Y. Chan |  |
| 601 |  |  [Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue](https://openreview.net/forum?id=Hke0K1HKwr) |  | 0 | Knowledge-grounded dialogue is a task of generating an informative response based on both discourse context and external knowledge. As we focus on better modeling the knowledge selection in the multi-turn knowledge-grounded dialogue, we propose a sequential latent variable model as the first approach to this matter. The model named sequential knowledge transformer (SKT) can keep track of the prior and posterior distribution over knowledge; as a result, it can not only reduce the ambiguity caused from the diversity in knowledge selection of conversation but also better leverage the response information for proper choice of knowledge. Our experimental results show that the proposed model improves the knowledge selection accuracy and... | Byeongchang Kim, Gunhee Kim, Jaewoo Ahn |  |
| 602 |  |  [Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://openreview.net/forum?id=SklD9yrFPS) |  | 0 | Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space. The entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with... | Alexander A. Alemi, Jaehoon Lee, Jascha SohlDickstein, Jiri Hron, Lechao Xiao, Roman Novak, Samuel S. Schoenholz |  |
| 603 |  |  [Self-labelling via simultaneous clustering and representation learning](https://openreview.net/forum?id=Hyx-jyBFPr) |  | 0 | Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly... | Andrea Vedaldi, Christian Rupprecht, Yuki Markus Asano |  |
| 604 |  |  [The intriguing role of module criticality in the generalization of deep networks](https://openreview.net/forum?id=S1e4jkSKvB) |  | 0 | We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier... | Behnam Neyshabur, Hanie Sedghi, Niladri S. Chatterji |  |
| 605 |  |  [Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks](https://openreview.net/forum?id=rkl8sJBYvH) |  | 0 | Recent research shows that the following two models are equivalent: (a) infinitely wide neural networks (NNs) trained under l2 loss by gradient descent with infinitesimally small learning rate (b) kernel regression with respect to so-called Neural Tangent Kernels (NTKs) (Jacot et al., 2018). An efficient algorithm to compute the NTK, as well as its convolutional counterparts, appears in Arora et al. (2019a), which allowed studying performance of infinitely wide nets on datasets like CIFAR-10. However, super-quadratic running time of kernel methods makes them best suited for small-data tasks. We report results suggesting neural tangent kernels perform strongly on low-data tasks. 1. On a standard testbed of classification/regression... | Dingli Yu, Ruosong Wang, Ruslan Salakhutdinov, Sanjeev Arora, Simon S. Du, Zhiyuan Li |  |
| 606 |  |  [Differentiation of Blackbox Combinatorial Solvers](https://openreview.net/forum?id=BkevoJSYPB) |  | 0 | Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into... | Anselm Paulus, Georg Martius, Marin Vlastelica Pogancic, Michal Rolínek, Vít Musil |  |
| 607 |  |  [Scaling Autoregressive Video Models](https://openreview.net/forum?id=rJgsskrFwH) |  | 0 | Due to the statistical complexity of video, the high degree of inherent stochasticity, and the sheer amount of data, generating natural video remains a challenging task. State-of-the-art video generation models attempt to address these issues by combining sometimes complex, often video-specific neural network architectures, latent variable models, adversarial training and a range of other methods. Despite their often high complexity, these approaches still fall short of generating high quality video continuations outside of narrow domains and often struggle with fidelity. In contrast, we show that conceptually simple, autoregressive video generation models based on a three-dimensional self-attention mechanism achieve highly... | Dirk Weissenborn, Jakob Uszkoreit, Oscar Täckström |  |
| 608 |  |  [The Ingredients of Real World Robotic Reinforcement Learning](https://openreview.net/forum?id=rJe2syrtvS) |  | 0 | The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and... | Abhishek Gupta, Avi Singh, Dhruv Shah, Henry Zhu, Justin Yu, Kristian Hartikainen, Sergey Levine, Vikash Kumar |  |
| 609 |  |  [Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization](https://openreview.net/forum?id=ryeYpJSKwr) |  | 0 | Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present... | Andreas Doerr, Christian Daniel, Frank Hutter, Kirsten Fischer, Lukas P. Fröhlich, Michael Volpp, Stefan Falkner |  |
| 610 |  |  [Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning](https://openreview.net/forum?id=BJliakStvH) |  | 0 | While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent’s policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent’s behavior. Our approach is... | Dexter R. R. Scobee, S. Shankar Sastry |  |
| 611 |  |  [Spectral Embedding of Regularized Block Models](https://openreview.net/forum?id=H1l_0JBYwS) |  | 0 | Spectral embedding is a popular technique for the representation of graph data. Several regularization techniques have been proposed to improve the quality of the embedding with respect to downstream tasks like clustering. In this paper, we explain on a simple block model the impact of the complete graph regularization, whereby a constant is added to all entries of the adjacency matrix. Specifically, we show that the regularization forces the spectral embedding to focus on the largest blocks, making the representation less sensitive to noise or outliers. We illustrate these results on both on both synthetic and real data, showing how regularization improves standard clustering scores. | Nathan de Lara, Thomas Bonald |  |
| 612 |  |  [Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models](https://openreview.net/forum?id=BkxRRkSKwr) |  | 0 | The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the... | Junyi Du, Xiang Ren, Xiangyang Xue, Xisen Jin, Zhongyu Wei |  |
| 613 |  |  [word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement](https://openreview.net/forum?id=HkxARkrFwB) |  | 0 | Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, word2ket and word2ketXS, for storing word embedding matrix during... | Aliakbar Panahi, Seyran Saeedi, Tomasz Arodz |  |
| 614 |  |  [What Can Neural Networks Reason About?](https://openreview.net/forum?id=rJxbJeHFPS) |  | 0 | Neural networks have succeeded in many reasoning tasks. Empirically, these tasks require specialized network structures, e.g., Graph Neural Networks (GNNs) perform well on many such tasks, but less structured networks fail. Theoretically, there is limited understanding of why and when a network structure generalizes better than others, although they have equal expressive power. In this paper, we develop a framework to characterize which reasoning tasks a network can learn well, by studying how well its computation structure aligns with the algorithmic structure of the relevant reasoning process. We formally define this algorithmic alignment and derive a sample complexity bound that decreases with better alignment. This framework... | Jingling Li, Kenichi Kawarabayashi, Keyulu Xu, Mozhi Zhang, Simon S. Du, Stefanie Jegelka |  |
| 615 |  |  [Training individually fair ML models with sensitive subspace robustness](https://openreview.net/forum?id=B1gdkxHFDH) |  | 0 | We consider training machine learning models that are fair in the sense that their performance is invariant under certain sensitive perturbations to the inputs. For example, the performance of a resume screening system should be invariant under changes to the gender and/or ethnicity of the applicant. We formalize this notion of algorithmic fairness as a variant of individual fairness and develop a distributionally robust optimization approach to enforce it during training. We also demonstrate the effectiveness of the approach on two ML tasks that are susceptible to gender and racial biases. | Amanda Bower, Mikhail Yurochkin, Yuekai Sun |  |
| 616 |  |  [Learning from Rules Generalizing Labeled Exemplars](https://openreview.net/forum?id=SkeuexBtDr) |  | 0 | In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of... | Abhijeet Awasthi, Rasna Goyal, Sabyasachi Ghosh, Sunita Sarawagi |  |
| 617 |  |  [Directional Message Passing for Molecular Graphs](https://openreview.net/forum?id=B1eWbxStPH) |  | 0 | Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with... | Janek Groß, Johannes Klicpera, Stephan Günnemann |  |
| 618 |  |  [Explanation by Progressive Exaggeration](https://openreview.net/forum?id=H1xFWgrFPS) |  | 0 | As machine learning methods see greater adoption and implementation in high stakes applications such as medical image diagnosis, the need for model interpretability and explanation has become more critical. Classical approaches that assess feature importance (eg saliency maps) do not explain how and why a particular region of an image is relevant to the prediction. We propose a method that explains the outcome of a classification black-box by gradually exaggerating the semantic effect of a given class. Given a query input to a classifier, our method produces a progressive set of plausible variations of that query, which gradually change the posterior probability from its original class to its negation. These counter-factually... | Brian Pollack, Junxiang Chen, Kayhan Batmanghelich, Sumedha Singla |  |
| 619 |  |  [Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network](https://openreview.net/forum?id=ByeGzlrKwH) |  | 0 | One of the biggest issues in deep learning theory is the generalization ability of networks with huge model size. The classical learning theory suggests that overparameterized models cause overfitting. However, practically used large deep models avoid overfitting, which is not well explained by the classical approaches. To resolve this issue, several attempts have been made. Among them, the compression based bound is one of the promising approaches. However, the compression based bound can be applied only to a compressed network, and it is not applicable to the non-compressed original network. In this paper, we give a unified frame-work that can convert compression based bounds to those for non-compressed original networks. The... | Hiroshi Abe, Taiji Suzuki, Tomoaki Nishimura |  |
| 620 |  |  [At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?](https://openreview.net/forum?id=Bkeb7lHtvH) |  | 0 | Background: Recent developments have made it possible to accelerate neural networks training significantly using large batch sizes and data parallelism. Training in an asynchronous fashion, where delay occurs, can make training even more scalable. However, asynchronous training has its pitfalls, mainly a degradation in generalization, even after convergence of the algorithm. This gap remains not well understood, as theoretical analysis so far mainly focused on the convergence rate of asynchronous methods. Contributions: We examine asynchronous training from the perspective of dynamical stability. We find that the degree of delay interacts with the learning rate, to change the set of minima accessible by an asynchronous stochastic... | Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Niv Giladi |  |
| 621 |  |  [Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)](https://openreview.net/forum?id=rygeHgSFDH) |  | 0 | A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one... | Carsten Rother, Peter Sorrenson, Ullrich Köthe |  |
| 622 |  |  [Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps](https://openreview.net/forum?id=BkgrBgSYDS) |  | 0 | Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within... | Albert Gu, Amit Blonder, Atri Rudra, Christopher Ré, Matthew Eichhorn, Megan Leszczynski, Nimit Sharad Sohoni, Tri Dao |  |
| 623 |  |  [Improving Generalization in Meta Reinforcement Learning using Learned Objectives](https://openreview.net/forum?id=S1evHerYPr) |  | 0 | Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency. | Jürgen Schmidhuber, Louis Kirsch, Sjoerd van Steenkiste |  |
| 624 |  |  [Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks](https://openreview.net/forum?id=BJxsrgStvr) |  | 0 | (Frankle & Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as Early-Bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that... | Chaojian Li, Haoran You, Pengfei Xu, Richard G. Baraniuk, Xiaohan Chen, Yingyan Lin, Yonggan Fu, Yue Wang, Zhangyang Wang |  |
| 625 |  |  [Truth or backpropaganda? An empirical investigation of deep learning theory](https://openreview.net/forum?id=HyxyIgHFvr) |  | 0 | We empirically evaluate common assumptions about neural networks that are widely held by practitioners and theorists alike. In this work, we: (1) prove the widespread existence of suboptimal local minima in the loss landscape of neural networks, and we use our theory to find examples; (2) show that small-norm parameters are not optimal for generalization; (3) demonstrate that ResNets do not conform to wide-network theories, such as the neural tangent kernel, and that the interaction between skip connections and batch normalization plays a role; (4) find that rank does not correlate with generalization or robustness in a practical setting. | Avi Schwarzschild, Jonas Geiping, Micah Goldblum, Michael Moeller, Tom Goldstein |  |
| 626 |  |  [Neural Arithmetic Units](https://openreview.net/forum?id=H1gNOeHKPS) |  | 0 | Neural networks can approximate complex functions, but they struggle to perform exact arithmetic operations over real numbers. The lack of inductive bias for arithmetic operations leaves neural networks without the underlying logic necessary to extrapolate on tasks such as addition, subtraction, and multiplication. We present two new neural network components: the Neural Addition Unit (NAU), which can learn exact addition and subtraction; and the Neural Multiplication Unit (NMU) that can multiply subsets of a vector. The NMU is, to our knowledge, the first arithmetic neural network component that can learn to multiply elements from a vector, when the hidden size is large. The two new components draw inspiration from a theoretical... | Alexander Rosenberg Johansen, Andreas Madsen |  |
| 627 |  |  [DeepSphere: a graph-based spherical CNN](https://openreview.net/forum?id=B1e3OlStPB) |  | 0 | Designing a convolution for a spherical neural network requires a delicate tradeoff between efficiency and rotation equivariance. DeepSphere, a method based on a graph representation of the discretized sphere, strikes a controllable balance between these two desiderata. This contribution is twofold. First, we study both theoretically and empirically how equivariance is affected by the underlying graph with respect to the number of pixels and neighbors. Second, we evaluate DeepSphere on relevant problems. Experiments show state-of-the-art performance and demonstrates the efficiency and flexibility of this formulation. Perhaps surprisingly, comparison with previous work suggests that anisotropic filters might be an unnecessary price... | Frédérick Gusset, Martino Milani, Michaël Defferrard, Nathanaël Perraudin |  |
| 628 |  |  [SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models](https://openreview.net/forum?id=SylkYeHtwr) |  | 0 | Standard variational lower bounds used to train latent variable models produce biased estimates of most quantities of interest. We introduce an unbiased estimator of the log marginal likelihood and its gradients for latent variable models based on randomized truncation of infinite series. If parameterized by an encoder-decoder architecture, the parameters of the encoder can be optimized to minimize its variance of this estimator. We show that models trained using our estimator give better test-set likelihoods than a standard importance-sampling based approach for the same average computational cost. This estimator also allows use of latent variable models for tasks where unbiased estimators, rather than marginal likelihood lower... | Alex Beatson, David Duvenaud, Jun Zhu, Mohammad Norouzi, Ricky T. Q. Chen, Ryan P. Adams, Yucen Luo |  |
| 629 |  |  [Deep Learning For Symbolic Mathematics](https://openreview.net/forum?id=S1eZYeHFDS) |  | 0 | Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing these mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica. | François Charton, Guillaume Lample |  |
| 630 |  |  [Making Sense of Reinforcement Learning and Probabilistic Inference](https://openreview.net/forum?id=S1xitgHtvS) |  | 0 | Reinforcement learning (RL) combines a control problem with statistical estimation: The system dynamics are not known to the agent, but can be learned through experience. A recent line of research casts ‘RL as inference’ and suggests a particular framework to generalize the RL problem as probabilistic inference. Our paper surfaces a key shortcoming in that approach, and clarifies the sense in which RL can be coherently cast as an inference problem. In particular, an RL agent must consider the effects of its actions upon future rewards and observations: The exploration-exploitation tradeoff. In all but the most simple settings, the resulting inference is computationally intractable so that practical RL algorithms must resort to... | Brendan O'Donoghue, Catalin Ionescu, Ian Osband |  |
| 631 |  |  [Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models](https://openreview.net/forum?id=r1eyceSYPr) |  | 0 | The contrastive divergence algorithm is a popular approach to training energy-based latent variable models, which has been widely used in many machine learning models such as the restricted Boltzmann machines and deep belief nets. Despite its empirical success, the contrastive divergence algorithm is also known to have biases that severely affect its convergence. In this article we propose an unbiased version of the contrastive divergence algorithm that completely removes its bias in stochastic gradient methods, based on recent advances on unbiased Markov chain Monte Carlo methods. Rigorous theoretical analysis is developed to justify the proposed algorithm, and numerical experiments show that it significantly improves the existing... | Lingsong Zhang, Xiao Wang, Yixuan Qiu |  |
| 632 |  |  [A Mutual Information Maximization Perspective of Language Representation Learning](https://openreview.net/forum?id=Syx79eBKwr) |  | 0 | We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple... | Cyprien de Masson d'Autume, Dani Yogatama, Lei Yu, Lingpeng Kong, Wang Ling, Zihang Dai |  |
| 633 |  |  [Energy-based models for atomic-resolution protein conformations](https://openreview.net/forum?id=S1e_9xrFvS) |  | 0 | We propose an energy-based model (EBM) of protein conformations that operates at atomic scale. The model is trained solely on crystallized protein data. By contrast, existing approaches for scoring conformations use energy functions that incorporate knowledge of physical principles and features that are the complex product of several decades of research and tuning. To evaluate the model, we benchmark on the rotamer recovery task, the problem of predicting the conformation of a side chain from its context within a protein structure, which has been used to evaluate energy functions for protein design. The model achieves performance close to that of the Rosetta energy function, a state-of-the-art method widely used in protein... | Alexander Rives, Jerry Ma, Joshua Meier, Rob Fergus, Yilun Du |  |
| 634 |  |  [Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem](https://openreview.net/forum?id=BJe55gBtvH) |  | 0 | Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky’s work reveals the limitations of shallow neural networks, it doesn’t inform us on why these functions are difficult to represent and in fact he states it as a... | Ioannis Panageas, Sai Ganesh Nagarajan, Vaggos Chatziafratis, Xiao Wang |  |
| 635 |  |  [Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint](https://openreview.net/forum?id=H1gBsgBYwH) |  | 0 | This paper investigates the generalization properties of two-layer neural networks in high-dimensions, i.e. when the number of samples $n$, features $d$, and neurons $h$ tend to infinity at the same rate. Specifically, we derive the exact population risk of the unregularized least squares regression problem with two-layer neural networks when either the first or the second layer is trained using a gradient flow under different initialization setups. When only the second layer coefficients are optimized, we recover the \textit{double descent} phenomenon: a cusp in the population risk appears at $h\approx n$ and further overparameterization decreases the risk. In contrast, when the first layer weights are optimized, we highlight how... | Denny Wu, Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Tianzong Zhang |  |
| 636 |  |  [Reconstructing continuous distributions of 3D protein structure from cryo-EM images](https://openreview.net/forum?id=SJxUjlBtwB) |  | 0 | Cryo-electron microscopy (cryo-EM) is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the 3D structure of a macromolecule from $10^{4-7}$ noisy and randomly oriented 2D projection images. However, the imaged protein complexes may exhibit structural variability, which complicates reconstruction and is typically addressed using discrete clustering approaches that fail to capture the full range of protein dynamics. Here, we introduce a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. This method encodes... | Bonnie Berger, Ellen D. Zhong, Joseph H. Davis, Tristan Bepler |  |
| 637 |  |  [Progressive Learning and Disentanglement of Hierarchical Representations](https://openreview.net/forum?id=SJxpsxrYPS) |  | 0 | Learning rich representation from data is an important task for deep generative models such as variational auto-encoder (VAE). However, by extracting high-level abstractions in the bottom-up inference process, the goal of preserving all factors of variations for top-down generation is compromised. Motivated by the concept of “starting small”, we present a strategy to progressively learn independent hierarchical representations from high- to low-levels of abstractions. The model starts with learning the most abstract representation, and then progressively grow the network architecture to introduce new representations at different levels of abstraction. We quantitatively demonstrate the ability of the presented model to improve... | Jaideep Vitthal Murkute, Linwei Wang, Prashnna Kumar Gyawali, Zhiyuan Li |  |
| 638 |  |  [An Exponential Learning Rate Schedule for Deep Learning](https://openreview.net/forum?id=rJg8TeSFDH) |  | 0 | Intriguing empirical evidence exists that deep learning can work well with exotic schedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN(Ioffe & Szegedy, 2015), which is ubiq- uitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier theoretical analyses of stand-alone BN (Ioffe & Szegedy, 2015; Santurkar et al., 2018; Arora et al., 2018) • Training can be done using SGD with momentum and an exponentially in- creasing learning rate schedule, i.e., learning rate increases by some... | Sanjeev Arora, Zhiyuan Li |  |
| 639 |  |  [Geom-GCN: Geometric Graph Convolutional Networks](https://openreview.net/forum?id=S1e2agrFvS) |  | 0 | Message-passing neural networks (MPNNs) have been successfully applied in a wide variety of applications in the real world. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph.... | Bingzhe Wei, Bo Yang, Hongbin Pei, Kevin ChenChuan Chang, Yu Lei |  |
| 640 |  |  [CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning](https://openreview.net/forum?id=HJgzt2VKPB) |  | 0 | Computer vision has undergone a dramatic revolution in performance, driven in large part through deep features trained on large-scale supervised datasets. However, much of these improvements have focused on static image analysis; video understanding has seen rather modest improvements. Even though new datasets and spatiotemporal models have been proposed, simple frame-by-frame classification methods often still remain competitive. We posit that current video datasets are plagued with implicit biases over scene and object structure that can dwarf variations in temporal structure. In this work, we build a video dataset with fully observable and controllable object and scene bias, and which truly requires spatiotemporal understanding... | Deva Ramanan, Rohit Girdhar |  |
| 641 |  |  [BackPACK: Packing more into Backprop](https://openreview.net/forum?id=BJlrF24twB) |  | 0 | Automatic differentiation frameworks are optimized for exactly one thing: computing the average mini-batch gradient. Yet, other quantities such as the variance of the mini-batch gradients or many approximations to the Hessian can, in theory, be computed efficiently, and at the same time as the gradient. While these quantities are of great interest to researchers and practitioners, current deep learning software does not support their automatic calculation. Manually implementing them is burdensome, inefficient if done naively, and the resulting code is rarely shared. This hampers progress in deep learning, and unnecessarily narrows research to focus on gradient descent and its variants; it also complicates replication studies and... | Felix Dangel, Frederik Kunstner, Philipp Hennig |  |
| 642 |  |  [GenDICE: Generalized Offline Estimation of Stationary Values](https://openreview.net/forum?id=HkxlcnVFwB) |  | 0 | An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting... | Bo Dai, Dale Schuurmans, Lihong Li, Ruiyi Zhang |  |
| 643 |  |  [Principled Weight Initialization for Hypernetworks](https://openreview.net/forum?id=H1lma24tPB) |  | 0 | Hypernetworks are meta neural networks that generate weights for a main neural network in an end-to-end differentiable manner. Despite extensive applications ranging from multi-task learning to Bayesian deep learning, the problem of optimizing hypernetworks has not been studied to date. We observe that classical weight initialization methods like Glorot & Bengio (2010) and He et al. (2015), when applied directly on a hypernet, fail to produce weights for the mainnet in the correct scale. We develop principled techniques for weight initialization in hypernets, and show that they lead to more stable mainnet weights, lower training loss, and faster convergence. | Hod Lipson, Lampros Flokas, Oscar Chang |  |
| 644 |  |  [On the Convergence of FedAvg on Non-IID Data](https://openreview.net/forum?id=HJxNAnVtDS) |  | 0 | Federated learning enables a large amount of edge computing devices to jointly learn a model without data sharing. As a leading algorithm in this setting, Federated Averaging (\texttt{FedAvg}) runs Stochastic Gradient Descent (SGD) in parallel on a small subset of the total devices and averages the sequences only once in a while. Despite its simplicity, it lacks theoretical guarantees under realistic settings. In this paper, we analyze the convergence of \texttt{FedAvg} on non-iid data and establish a convergence rate of $\mathcal{O}(\frac{1}{T})$ for strongly convex and smooth problems, where $T$ is the number of SGDs. Importantly, our bound demonstrates a trade-off between communication-efficiency and convergence rate. As user... | Kaixuan Huang, Shusen Wang, Wenhao Yang, Xiang Li, Zhihua Zhang |  |
| 645 |  |  [Data-dependent Gaussian Prior Objective for Language Generation](https://openreview.net/forum?id=S1efxTVYDr) |  | 0 | For typical sequence prediction problems such as language generation, maximum likelihood estimation (MLE) has commonly been adopted as it encourages the predicted sequence most consistent with the ground-truth sequence to have the highest probability of occurring. However, MLE focuses on once-to-all matching between the predicted sequence and gold-standard, consequently treating all incorrect predictions as being equally incorrect. We refer to this drawback as {\it negative diversity ignorance} in this paper. Treating all incorrect predictions as equal unfairly downplays the nuance of these sequences' detailed token-wise structure. To counteract this, we augment the MLE loss by introducing an extra Kullback--Leibler divergence term... | Eiichiro Sumita, Hai Zhao, Kehai Chen, Masao Utiyama, Rui Wang, Zhuosheng Zhang, Zuchao Li |  |
| 646 |  |  [Contrastive Learning of Structured World Models](https://openreview.net/forum?id=H1gax6VtDB) |  | 0 | A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving... | Elise van der Pol, Max Welling, Thomas N. Kipf |  |
| 647 |  |  [Neural Network Branching for Neural Network Verification](https://openreview.net/forum?id=B1evfa4tPB) |  | 0 | Formal verification of neural networks is essential for their deployment in safety-critical areas. Many available formal verification methods have been shown to be instances of a unified Branch and Bound (BaB) formulation. We propose a novel framework for designing an effective branching strategy for BaB. Specifically, we learn a graph neural network (GNN) to imitate the strong branching heuristic behaviour. Our framework differs from previous methods for learning to branch in two main aspects. Firstly, our framework directly treats the neural network we want to verify as a graph input for the GNN. Secondly, we develop an intuitive forward and backward embedding update schedule. Empirically, our framework achieves roughly $50\%$... | Jingyue Lu, M. Pawan Kumar |  |
| 648 |  |  [Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity](https://openreview.net/forum?id=BJgnXpVYwS) |  | 0 | We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely... | Ali Jadbabaie, Jingzhao Zhang, Suvrit Sra, Tianxing He |  |
| 649 |  |  [Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information](https://openreview.net/forum?id=Syg-ET4FPS) |  | 0 | Posterior sampling for reinforcement learning (PSRL) is a useful framework for making decisions in an unknown environment. PSRL maintains a posterior distribution of the environment and then makes planning on the environment sampled from the posterior distribution. Though PSRL works well on single-agent reinforcement learning problems, how to apply PSRL to multi-agent reinforcement learning problems is relatively unexplored. In this work, we extend PSRL to two-player zero-sum extensive-games with imperfect information (TEGI), which is a class of multi-agent systems. More specifically, we combine PSRL with counterfactual regret minimization (CFR), which is the leading algorithm for TEGI with a known environment. Our main... | Jialian Li, Jun Zhu, Yichi Zhou |  |
| 650 |  |  [Mogrifier LSTM](https://openreview.net/forum?id=SJe5P6EYvS) |  | 0 | Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language... | Gábor Melis, Phil Blunsom, Tomás Kociský |  |
| 651 |  |  [Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech](https://openreview.net/forum?id=B1elCp4KwH) |  | 0 | In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3% reduction in ABX error rate over the top-performing submission, while keeping the... | David Harwath, James R. Glass, WeiNing Hsu |  |
| 652 |  |  [Mirror-Generative Neural Machine Translation](https://openreview.net/forum?id=HkxQRTNYPH) |  | 0 | Training neural machine translation models (NMT) requires a large amount of parallel corpus, which is scarce for many language pairs. However, raw non-parallel corpora are often easy to obtain. Existing approaches have not exploited the full potential of non-parallel bilingual data either in training or decoding. In this paper, we propose the mirror-generative NMT (MGNMT), a single unified architecture that simultaneously integrates the source to target translation model, the target to source translation model, and two language models. Both translation models and language models share the same latent semantic space, therefore both translation directions can learn from non-parallel data more effectively. Besides, the translation... | Hao Zhou, Jiajun Chen, Lei Li, Shujian Huang, XinYu Dai, Zaixiang Zheng |  |
| 653 |  |  [Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning](https://openreview.net/forum?id=rkeS1RVtPS) |  | 0 | The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We prove non-asymptotic convergence theory of our proposed algorithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks. | Andrew Gordon Wilson, Changyou Chen, Chunyuan Li, Jianyi Zhang, Ruqi Zhang |  |
| 654 |  |  [Your classifier is secretly an energy based model and you should treat it like one](https://openreview.net/forum?id=Hkxzx0NtDB) |  | 0 | We propose to reinterpret a standard discriminative classifier of p(y\|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x\|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach... | David Duvenaud, JörnHenrik Jacobsen, Kevin Swersky, KuanChieh Wang, Mohammad Norouzi, Will Grathwohl |  |
| 655 |  |  [Dynamics-Aware Unsupervised Discovery of Skills](https://openreview.net/forum?id=HJgLZR4KvH) |  | 0 | Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of... | Archit Sharma, Karol Hausman, Sergey Levine, Shixiang Gu, Vikash Kumar |  |
| 656 |  |  [Optimal Strategies Against Generative Attacks](https://openreview.net/forum?id=BkgzMCVtPB) |  | 0 | Generative neural models have improved dramatically recently. With this progress comes the risk that such models will be used to attack systems that rely on sensor data for authentication and anomaly detection. Many such learning systems are installed worldwide, protecting critical infrastructure or private data against malfunction and cyber attacks. We formulate the scenario of such an authentication system facing generative impersonation attacks, characterize it from a theoretical perspective and explore its practical implications. In particular, we ask fundamental theoretical questions in learning, statistics and information theory: How hard is it to detect a "fake reality"? How much data does the attacker need to collect before... | Amir Globerson, Erez Peterfreund, Matan Gavish, Roy Mor |  |
| 657 |  |  [GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding](https://openreview.net/forum?id=r1lGO0EKDH) |  | 0 | Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose GraphZoom, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. GraphZoom first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information.... | Chenhui Deng, Yongyu Wang, Zhiqiang Zhao, Zhiru Zhang, Zhuo Feng |  |
| 658 |  |  [Harnessing Structures for Value-Based Planning and Reinforcement Learning](https://openreview.net/forum?id=rklHqRVKvH) |  | 0 | Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low-rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general... | Dina Katabi, Guo Zhang, Yuzhe Yang, Zhi Xu |  |
| 659 |  |  [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://openreview.net/forum?id=S1gSj0NKvB) |  | 0 | Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding (as proposed by Frankle et al., (2019)), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding (which we propose) trains the unpruned weights from their final values using the same learning... | Alex Renda, Jonathan Frankle, Michael Carbin |  |
| 660 |  |  [Meta-Q-Learning](https://openreview.net/forum?id=SJeD3CEFPH) |  | 0 | This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state-of-the-art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard... | Alexander J. Smola, Pratik Chaudhari, Rasool Fakoor, Stefano Soatto |  |
| 661 |  |  [Mathematical Reasoning in Latent Space](https://openreview.net/forum?id=Ske31kBtPr) |  | 0 | We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate... | Christian Szegedy, Dennis Lee, Kshitij Bansal, Markus N. Rabe, Sarah M. Loos |  |
| 662 |  |  [A Theory of Usable Information under Computational Constraints](https://openreview.net/forum?id=r1eBeyHFDH) |  | 0 | We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon’s information theory that takes into account the modeling power and computational constraints of the observer. The resulting predictive V-information encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon’s mutual information and in violation of the data processing inequality, V-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational... | Jiaming Song, Russell Stewart, Shengjia Zhao, Stefano Ermon, Yilun Xu |  |
| 663 |  |  [Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning](https://openreview.net/forum?id=rygixkHKDH) |  | 0 | Learning overcomplete representations finds many applications in machine learning and data analytics. In the past decade, despite the empirical success of heuristic methods, theoretical understandings and explanations of these algorithms are still far from satisfactory. In this work, we provide new theoretical insights for several important representation learning problems: learning (i) sparsely used overcomplete dictionaries and (ii) convolutional dictionaries. We formulate these problems as $\ell^4$-norm optimization problems over the sphere and study the geometric properties of their nonconvex optimization landscapes. For both problems, we show the nonconvex objective has benign (global) geometric structures, which enable the... | Qing Qu, Xiao Li, Yuexiang Zhai, Yuqian Zhang, Zhihui Zhu |  |
| 664 |  |  [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://openreview.net/forum?id=ryghZJBKPS) |  | 0 | We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems. | Akshay Krishnamurthy, Alekh Agarwal, Chicheng Zhang, John Langford, Jordan T. Ash |  |
| 665 |  |  [Understanding and Robustifying Differentiable Architecture Search](https://openreview.net/forum?id=H1gDNyrKDS) |  | 0 | Differentiable Architecture Search (DARTS) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, DARTS does not work robustly for new problems: we identify a wide range of search spaces for which DARTS yields degenerate architectures with very poor test performance. We study this failure mode and show that, while DARTS successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify DARTS to find solutions with less curvature... | Arber Zela, Frank Hutter, Thomas Brox, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi |  |
| 666 |  |  [A Closer Look at Deep Policy Gradients](https://openreview.net/forum?id=ryxdEkHtPS) |  | 0 | We study how the behavior of deep policy gradient algorithms reflects the conceptual framework motivating their development. To this end, we propose a fine-grained analysis of state-of-the-art methods based on key elements of this framework: gradient estimation, value prediction, and optimization landscapes. Our results show that the behavior of deep policy gradient algorithms often deviates from what their motivating framework would predict: surrogate rewards do not match the true reward landscape, learned value estimators fail to fit the true value function, and gradient estimates poorly correlate with the "true" gradient. The mismatch between predicted and empirical behavior we uncover highlights our poor understanding of... | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Logan Engstrom, Shibani Santurkar |  |
| 667 |  |  [Implementation Matters in Deep RL: A Case Study on PPO and TRPO](https://openreview.net/forum?id=r1etN1rtPB) |  | 0 | We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of "code-level optimizations:" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty, and importance, of attributing performance gains... | Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Logan Engstrom, Shibani Santurkar |  |
| 668 |  |  [Fast Task Inference with Variational Intrinsic Successor Features](https://openreview.net/forum?id=BJeAHkrYDS) |  | 0 | It has been established that diverse behaviors spanning the controllable subspace of a Markov decision process can be trained by rewarding a policy for being distinguishable from other policies. However, one limitation of this formulation is the difficulty to generalize beyond the finite set of behaviors being explicitly learned, as may be needed in subsequent tasks. Successor features provide an appealing solution to this generalization problem, but require defining the reward function as linear in some grounded feature space. In this paper, we show that these two techniques can be combined, and that each method solves the other's primary limitation. To do so we introduce Variational Intrinsic Successor FeatuRes (VISR), a novel... | André Barreto, David WardeFarley, Steven Hansen, Tom Van de Wiele, Volodymyr Mnih, Will Dabney |  |
| 669 |  |  [Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks](https://openreview.net/forum?id=rkeZIJBYvr) |  | 0 | While tasks could come with varying the number of instances and classes in realistic settings, the existing meta-learning approaches for few-shot classification assume that number of instances per task and class is fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks, on which the meta-knowledge may have less usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning within each task. Through the learning... | Donghyun Na, Eunho Yang, Haebeom Lee, Hayeon Lee, Minseop Park, Saehoon Kim, Sung Ju Hwang |  |
| 670 |  |  [RNA Secondary Structure Prediction By Learning Unrolled Algorithms](https://openreview.net/forum?id=S1eALyrYDH) |  | 0 | In this paper, we propose an end-to-end deep learning model, called E2Efold, for RNA secondary structure prediction which can effectively take into account the inherent constraints in the problem. The key idea of E2Efold is to directly predict the RNA base-pairing matrix, and use an unrolled algorithm for constrained programming as the template for deep architectures to enforce constraints. With comprehensive experiments on benchmark datasets, we demonstrate the superior performance of E2Efold: it predicts significantly better structures compared to previous SOTA (especially for pseudoknotted structures), while being as efficient as the fastest algorithms in terms of inference time. | Le Song, Ramzan Umarov, Xin Gao, Xinshi Chen, Yu Li |  |
| 671 |  |  [Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search](https://openreview.net/forum?id=BJlQtJSKDB) |  | 0 | Monte Carlo Tree Search (MCTS) algorithms have achieved great success on many challenging benchmarks (e.g., Computer Go). However, they generally require a large number of rollouts, making their applications costly. Furthermore, it is also extremely challenging to parallelize MCTS due to its inherent sequential nature: each rollout heavily relies on the statistics (e.g., node visitation counts) estimated from previous simulations to achieve an effective exploration-exploitation tradeoff. In spite of these difficulties, we develop an algorithm, WU-UCT, to effectively parallelize MCTS, which achieves linear speedup and exhibits only limited performance loss with an increasing number of workers. The key idea in WU-UCT is a set of... | Anji Liu, Ji Liu, Jianshu Chen, Mingze Yu, Xuewen Zhou, Yu Zhai |  |
| 672 |  |  [Target-Embedding Autoencoders for Supervised Representation Learning](https://openreview.net/forum?id=BygXFkSYDH) |  | 0 | Autoencoder-based learning has emerged as a staple for disciplining representations in unsupervised and semi-supervised settings. This paper analyzes a framework for improving generalization in a purely supervised setting, where the target space is high-dimensional. We motivate and formalize the general framework of target-embedding autoencoders (TEA) for supervised prediction, learning intermediate latent representations jointly optimized to be both predictable from features as well as predictive of targets---encoding the prior that variations in targets are driven by a compact set of underlying factors. As our theoretical contribution, we provide a guarantee of generalization for linear TEAs by demonstrating uniform stability,... | Daniel Jarrett, Mihaela van der Schaar |  |
| 673 |  |  [Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB) |  | 0 | Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L \log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more... | Anselm Levskaya, Lukasz Kaiser, Nikita Kitaev |  |
| 674 |  |  [Rotation-invariant clustering of neuronal responses in primary visual cortex](https://openreview.net/forum?id=rklr9kHFDB) |  | 0 | Similar to a convolutional neural network (CNN), the mammalian retina encodes visual information into several dozen nonlinear feature maps, each formed by one ganglion cell type that tiles the visual space in an approximately shift-equivariant manner. Whether such organization into distinct cell types is maintained at the level of cortical image processing is an open question. Predictive models building upon convolutional features have been shown to provide state-of-the-art performance, and have recently been extended to include rotation equivariance in order to account for the orientation selectivity of V1 neurons. However, generally no direct correspondence between CNN feature maps and groups of individual neurons emerges in... | Alexander S. Ecker, Andreas S. Tolias, Edgar Y. Walker, Emmanouil Froudarakis, Erick Cobos, Fabian H. Sinz, Ivan Ustyuzhaninov, Jacob Reimer, Matthias Bethge, Paul G. Fahey, Santiago A. Cadena |  |
| 675 |  |  [Causal Discovery with Reinforcement Learning](https://openreview.net/forum?id=S1g2skStPB) |  | 0 | Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates... | Ignavier Ng, Shengyu Zhu, Zhitang Chen |  |
| 676 |  |  [Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems](https://openreview.net/forum?id=rkg6sJHYDr) |  | 0 | In many complex dynamical systems, artificial or natural, one can observe self-organization of patterns emerging from local rules. Cellular automata, like the Game of Life (GOL), have been widely used as abstract models enabling the study of various aspects of self-organization and morphogenesis, such as the emergence of spatially localized patterns. However, findings of self-organized patterns in such models have so far relied on manual tuning of parameters and initial states, and on the human eye to identify interesting patterns. In this paper, we formulate the problem of automated discovery of diverse self-organized patterns in such high-dimensional complex dynamical systems, as well as a framework for experimentation and... | Chris Reinke, Mayalen Etcheverry, PierreYves Oudeyer |  |
| 677 |  |  [Restricting the Flow: Information Bottlenecks for Attribution](https://openreview.net/forum?id=S1xWh1rYwB) |  | 0 | Attribution methods provide insights into the decision-making of machine learning models like artificial neural networks. For a given input sample, they assign a relevance score to each individual input variable, such as the pixels of an image. In this work, we adopt the information bottleneck concept for attribution. By adding noise to intermediate feature maps, we restrict the flow of information and can quantify (in bits) how much information image regions provide. We compare our method against ten baselines using three different metrics on VGG-16 and ResNet-50, and find that our methods outperform all baselines in five out of six settings. The method’s information-theoretic foundation provides an absolute frame of reference for... | Federico Tombari, Karl Schulz, Leon Sixt, Tim Landgraf |  |
| 678 |  |  [Building Deep Equivariant Capsule Networks](https://openreview.net/forum?id=BJgNJgSFPS) |  | 0 | Capsule networks are constrained by the parameter-expensive nature of their layers, and the general lack of provable equivariance guarantees. We present a variation of capsule networks that aims to remedy this. We identify that learning all pair-wise part-whole relationships between capsules of successive layers is inefficient. Further, we also realise that the choice of prediction networks and the routing mechanism are both key to equivariance. Based on these, we propose an alternative framework for capsule networks that learns to projectively encode the manifold of pose-variations, termed the space-of-variation (SOV), for every capsule-type of each layer. This is done using a trainable, equivariant function defined over a grid of... | R. Raghunatha Sarma, S. Balasubramanian, Sai Raam Venkataraman |  |
| 679 |  |  [A Generalized Training Approach for Multiagent Learning](https://openreview.net/forum?id=Bkl5kxrKDr) |  | 0 | This paper investigates a population-based training regime based on game-theoretic principles called Policy-Spaced Response Oracles (PSRO). PSRO is general in the sense that it (1) encompasses well-known algorithms such as fictitious play and double oracle as special cases, and (2) in principle applies to general-sum, many-player games. Despite this, prior studies of PSRO have been focused on two-player zero-sum games, a regime where in Nash equilibria are tractably computable. In moving from two-player zero-sum games to more general settings, computation of Nash equilibria quickly becomes infeasible. Here, we extend the theoretical underpinnings of PSRO by considering an alternative solution concept, α-Rank, which is unique (thus... | Daniel Hennes, Edward Hughes, Guy Lever, Julien Pérolat, Karl Tuyls, Luke Marris, Marc Lanctot, Mark Rowland, Nicolas Heess, Paul Muller, Rémi Munos, Shayegan Omidshafiei, Siqi Liu, Thore Graepel, Zhe Wang |  |
| 680 |  |  [High Fidelity Speech Synthesis with Adversarial Networks](https://openreview.net/forum?id=r1gfQgSFDr) |  | 0 | Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention, and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well... | Aidan Clark, Erich Elsen, Jeff Donahue, Karen Simonyan, Luis C. Cobo, Mikolaj Binkowski, Norman Casagrande, Sander Dieleman |  |
| 681 |  |  [SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference](https://openreview.net/forum?id=rkgvXlrKwH) |  | 0 | We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost. of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 twice as fast in wall-time. For the... | Ke Wang, Lasse Espeholt, Marcin Michalski, Piotr Stanczyk, Raphaël Marinier |  |
| 682 |  |  [Meta-Learning with Warped Gradient Descent](https://openreview.net/forum?id=rkeiQlBFPB) |  | 0 | Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading... | Andrei A. Rusu, Francesco Visin, Hujun Yin, Raia Hadsell, Razvan Pascanu, Sebastian Flennerhag |  |
| 683 |  |  [Convolutional Conditional Neural Processes](https://openreview.net/forum?id=Skey4eBYPS) |  | 0 | We introduce the Convolutional Conditional Neural Process (ConvCNP), a new member of the Neural Process family that models translation equivariance in the data. Translation equivariance is an important inductive bias for many learning problems including time series modelling, spatial data, and images. The model embeds data sets into an infinite-dimensional function space, as opposed to finite-dimensional vector spaces. To formalize this notion, we extend the theory of neural representations of sets to include functional representations, and demonstrate that any translation-equivariant embedding can be represented using a convolutional deep-set. We evaluate ConvCNPs in several settings, demonstrating that they achieve... | Andrew Y. K. Foong, James Requeima, Jonathan Gordon, Richard E. Turner, Wessel P. Bruinsma, Yann Dubois |  |
| 684 |  |  [Gradient Descent Maximizes the Margin of Homogeneous Neural Networks](https://openreview.net/forum?id=SJeLIgBKPS) |  | 0 | In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its... | Jian Li, Kaifeng Lyu |  |
| 685 |  |  [Adversarial Training and Provable Defenses: Bridging the Gap](https://openreview.net/forum?id=SJxSDxrKDr) |  | 0 | We present COLT, a new method to train neural networks based on a novel combination of adversarial training and provable defenses. The key idea is to model neural network training as a procedure which includes both, the verifier and the adversary. In every iteration, the verifier aims to certify the network using convex relaxation while the adversary tries to find inputs inside that convex relaxation which cause verification to fail. We experimentally show that this training method, named convex layerwise adversarial training (COLT), is promising and achieves the best of both worlds -- it produces a state-of-the-art neural network with certified robustness of 60.5% and accuracy of 78.4% on the challenging CIFAR-10 dataset with a... | Martin T. Vechev, Mislav Balunovic |  |
| 686 |  |  [Differentiable Reasoning over a Virtual Knowledge Base](https://openreview.net/forum?id=SJxstlHFPH) |  | 0 | We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a KB, softly following paths of relations between mentions of entities in the corpus. At each step the module uses a combination of sparse-matrix TFIDF indices and a maximum inner product search (MIPS) on a special index of contextual representations of the mentions. This module is differentiable, so the full system can be trained end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the contextual representation encoder by generating hard negative examples using existing... | Bhuwan Dhingra, Graham Neubig, Manzil Zaheer, Ruslan Salakhutdinov, Vidhisha Balachandran, William W. Cohen |  |
| 687 |  |  [Federated Learning with Matched Averaging](https://openreview.net/forum?id=BkluqlSFDS) |  | 0 | Federated learning allows edge devices to collaboratively learn a shared model while keeping the training data on device, decoupling the ability to do model training from the need to store the data in the cloud. We propose Federated matched averaging (FedMA) algorithm designed for federated learning of modern neural network architectures e.g. convolutional neural networks (CNNs) and LSTMs. FedMA constructs the shared global model in a layer-wise manner by matching and averaging hidden elements (i.e. channels for convolution layers; hidden states for LSTM; neurons for fully connected layers) with similar feature extraction signatures. Our experiments indicate that FedMA not only outperforms popular state-of-the-art federated... | Dimitris S. Papailiopoulos, Hongyi Wang, Mikhail Yurochkin, Yasaman Khazaeni, Yuekai Sun |  |
