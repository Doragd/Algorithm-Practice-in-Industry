# ICLR2020

## 会议论文列表

本会议共有 687 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [Large Batch Optimization for Deep Learning: Training BERT in 76 minutes](https://openreview.net/forum?id=Syx4wnEtvH) |  | 0 | Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes. | Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, ChoJui Hsieh |  |
| 2 |  |  [SELF: Learning to Filter Noisy Labels with Self-Ensembling](https://openreview.net/forum?id=HkgsPhNYPS) |  | 0 | Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures. | Duc Tam Nguyen, Chaithanya Kumar Mummadi, ThiPhuongNhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, Thomas Brox |  |
| 3 |  |  [Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation](https://openreview.net/forum?id=HygnDhEtvr) |  | 0 | Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark. | Yu Chen, Lingfei Wu, Mohammed J. Zaki |  |
| 4 |  |  [Sharing Knowledge in Multi-Task Deep Reinforcement Learning](https://openreview.net/forum?id=rkgpv2VFvr) |  | 0 | We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in terms of sample efficiency and performance. | Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters |  |
| 5 |  |  [On the Weaknesses of Reinforcement Learning for Neural Machine Translation](https://openreview.net/forum?id=H1eCw3EKvH) |  | 0 | Reinforcement learning (RL) is frequently used to increase performance in text generation tasks, including machine translation (MT), notably through the use of Minimum Risk Training (MRT) and Generative Adversarial Networks (GAN). However, little is known about what and how these methods learn in the context of MT. We prove that one of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. In fact, our results suggest that RL practices in MT are likely to improve performance only where the pre-trained parameters are already close to yielding the correct translation. Our findings further suggest that observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve. | Leshem Choshen, Lior Fox, Zohar Aizenbud, Omri Abend |  |
| 6 |  |  [StructPool: Structured Graph Pooling via Conditional Random Fields](https://openreview.net/forum?id=BJxg_hVtwH) |  | 0 | Learning high-level representations for graphs is of great importance for graph analysis tasks. In addition to graph convolution, graph pooling is an important but less explored research area. In particular, most of existing graph pooling techniques do not consider the graph structural information explicitly. We argue that such information is important and develop a novel graph pooling technique, know as the StructPool, in this work. We consider the graph pooling as a node clustering problem, which requires the learning of a cluster assignment matrix. We propose to formulate it as a structured prediction problem and employ conditional random fields to capture the relationships among assignments of different nodes. We also generalize our method to incorporate graph topological information in designing the Gibbs energy function. Experimental results on multiple datasets demonstrate the effectiveness of our proposed StructPool. | Hao Yuan, Shuiwang Ji |  |
| 7 |  |  [Learning deep graph matching with channel-independent embedding and Hungarian attention](https://openreview.net/forum?id=rJgBd2NYPH) |  | 0 | Graph matching aims to establishing node-wise correspondence between two graphs, which is a classic combinatorial problem and in general NP-complete. Until very recently, deep graph matching methods start to resort to deep networks to achieve unprecedented matching accuracy. Along this direction, this paper makes two complementary contributions which can also be reused as plugin in existing works: i) a novel node and edge embedding strategy which stimulates the multi-head strategy in attention models and allows the information in each channel to be merged independently. In contrast, only node embedding is accounted in previous works; ii) a general masking mechanism over the loss function is devised to improve the smoothness of objective learning for graph matching. Using Hungarian algorithm, it dynamically constructs a structured and sparsely connected layer, taking into account the most contributing matching pairs as hard attention. Our approach performs competitively, and can also improve state-of-the-art methods as plugin, regarding with matching accuracy on three public benchmarks. | Tianshu Yu, Runzhong Wang, Junchi Yan, Baoxin Li |  |
| 8 |  |  [Graph inference learning for semi-supervised classification](https://openreview.net/forum?id=r1evOhEKvH) |  | 0 | In this work, we address the semi-supervised classification of graph data, where the categories of those unlabeled nodes are inferred from labeled nodes as well as graph structures. Recent works often solve this problem with the advanced graph convolution in a conventional supervised manner, but the performance could be heavily affected when labeled data is scarce. Here we propose a Graph Inference Learning (GIL) framework to boost the performance of node classification by learning the inference of node labels on graph topology. To bridge the connection of two nodes, we formally define a structure relation by encapsulating node attributes, between-node paths and local topological structures together, which can make inference conveniently deduced from one node to another node. For learning the inference process, we further introduce meta-optimization on structure relations from training nodes to validation nodes, such that the learnt graph inference capability can be better self-adapted into test nodes. Comprehensive evaluations on four benchmark datasets (including Cora, Citeseer, Pubmed and NELL) demonstrate the superiority of our GIL when compared with other state-of-the-art methods in the semi-supervised node classification task. | Chunyan Xu, Zhen Cui, Xiaobin Hong, Tong Zhang, Jian Yang, Wei Liu |  |
| 9 |  |  [SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards](https://openreview.net/forum?id=S1xKd24twB) |  | 0 | Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo. This paper is a proof of concept that illustrates how a simple imitation method based on RL with constant rewards can be as effective as more complex methods that use learned rewards. | Siddharth Reddy, Anca D. Dragan, Sergey Levine |  |
| 10 |  |  [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data](https://openreview.net/forum?id=r1eiu2VtwH) |  | 0 | Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data. | Sergei Popov, Stanislav Morozov, Artem Babenko |  |
| 11 |  |  [Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification](https://openreview.net/forum?id=rJlnOhVYPS) |  | 0 | Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner. In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks. | Yixiao Ge, Dapeng Chen, Hongsheng Li |  |
| 12 |  |  [Automatically Discovering and Learning New Visual Categories with Ranking Statistics](https://openreview.net/forum?id=BJl2_nVFPB) |  | 0 | We tackle the problem of discovering novel classes in an image collection given labelled examples of other classes. This setting is similar to semi-supervised learning, but significantly harder because there are no labelled examples for the new classes. The challenge, then, is to leverage the information contained in the labelled images in order to learn a general-purpose clustering model and use the latter to identify the new classes in the unlabelled data. In this work we address this problem by combining three ideas: (1) we suggest that the common approach of bootstrapping an image representation using the labeled data only introduces an unwanted bias, and that this can be avoided by using self-supervised learning to train the representation from scratch on the union of labelled and unlabelled data; (2) we use rank statistics to transfer the model's knowledge of the labelled classes to the problem of clustering the unlabelled images; and, (3) we train the data representation by optimizing a joint objective function on the labelled and unlabelled subsets of the data, improving both the supervised classification of the labelled data, and the clustering of the unlabelled data. We evaluate our approach on standard classification benchmarks and outperform current methods for novel category discovery by a significant margin. | Kai Han, SylvestreAlvise Rebuffi, Sébastien Ehrhardt, Andrea Vedaldi, Andrew Zisserman |  |
| 13 |  |  [Maxmin Q-learning: Controlling the Estimation Bias of Q-learning](https://openreview.net/forum?id=Bkg0u3Etwr) |  | 0 | Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems. | Qingfeng Lan, Yangchen Pan, Alona Fyshe, Martha White |  |
| 14 |  |  [Federated Adversarial Domain Adaptation](https://openreview.net/forum?id=HJezF3VYPB) |  | 0 | Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting. | Xingchao Peng, Zijun Huang, Yizhe Zhu, Kate Saenko |  |
| 15 |  |  [Depth-Adaptive Transformer](https://openreview.net/forum?id=SJg7KhVKPH) |  | 0 | State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less than a quarter of the decoder layers. | Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli |  |
| 16 |  |  [DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures](https://openreview.net/forum?id=rylBK34FDS) |  | 0 | In seeking for sparse and efficient neural network models, many previous works investigated on enforcing L1 or L0 regularizers to encourage weight sparsity during training. The L0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The L1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between L1 and L2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning. | Huanrui Yang, Wei Wen, Hai Li |  |
| 17 |  |  [Evaluating The Search Phase of Neural Architecture Search](https://openreview.net/forum?id=H1loF2NFwr) |  | 0 | Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. NAS algorithms are currently compared solely based on their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we propose to evaluate the NAS search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the state-of-the-art NAS algorithms perform similarly to the random policy; (ii) the widely-used weight sharing strategy degrades the ranking of the NAS candidates to the point of not reflecting their true performance, thus reducing the effectiveness of the search process. We believe that our evaluation framework will be key to designing NAS strategies that consistently discover architectures superior to random ones. | Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, Mathieu Salzmann |  |
| 18 |  |  [Diverse Trajectory Forecasting with Determinantal Point Processes](https://openreview.net/forum?id=ryxnY3NYPS) |  | 0 | The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data. | Ye Yuan, Kris M. Kitani |  |
| 19 |  |  [ProxSGD: Training Structured Neural Networks under Regularization and Constraints](https://openreview.net/forum?id=HygpthEtvr) |  | 0 |  | Yang Yang, Yaxiong Yuan, Avraam Chatzimichailidis, Ruud J. G. van Sloun, Lei Lei, Symeon Chatzinotas |  |
| 20 |  |  [LAMOL: LAnguage MOdeling for Lifelong Language Learning](https://openreview.net/forum?id=Skgxcn4YDS) |  | 0 | Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling. LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity. Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples. When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task. The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. Overall, LAMOL outperforms previous methods by a considerable margin and is only 2-3% worse than multitasking, which is usually considered the LLL upper bound. The source code is available at https://github.com/jojotenya/LAMOL. | FanKeng Sun, ChengHao Ho, HungYi Lee |  |
| 21 |  |  [Learning Expensive Coordination: An Event-Based Deep RL Approach](https://openreview.net/forum?id=ryeG924twB) |  | 0 | Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly focus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordination, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader's long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers' behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers' decision space and thus accelerate the training process of followers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically. | Zhenyu Shi, Runsheng Yu, Xinrun Wang, Rundong Wang, Youzhi Zhang, Hanjiang Lai, Bo An |  |
| 22 |  |  [Curvature Graph Network](https://openreview.net/forum?id=BylEqnVFDB) |  | 0 | Graph-structured data is prevalent in many domains. Despite the widely celebrated success of deep neural networks, their power in graph-structured data is yet to be fully explored. We propose a novel network architecture that incorporates advanced graph structural features. In particular, we leverage discrete graph curvature, which measures how the neighborhoods of a pair of nodes are structurally related. The curvature of an edge (x, y) defines the distance taken to travel from neighbors of x to neighbors of y, compared with the length of edge (x, y). It is a much more descriptive feature compared to previously used features that only focus on node specific attributes or limited topological information such as degree. Our curvature graph convolution network outperforms state-of-the-art on various synthetic and real-world graphs, especially the larger and denser ones. | Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, Chao Chen |  |
| 23 |  |  [Distance-Based Learning from Errors for Confidence Calibration](https://openreview.net/forum?id=BJeB5hVtvB) |  | 0 |  | Chen Xing, Sercan Ömer Arik, Zizhao Zhang, Tomas Pfister |  |
| 24 |  |  [Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient](https://openreview.net/forum?id=rkeIq2VYPr) |  | 0 | Determinantal point processes (DPPs) is an effective tool to deliver diversity on multiple machine learning and computer vision tasks. Under deep learning framework, DPP is typically optimized via approximation, which is not straightforward and has some conflict with diversity requirement. We note, however, there has been no deep learning paradigms to optimize DPP directly since it involves matrix inversion which may result in highly computational instability. This fact greatly hinders the wide use of DPP on some specific objectives where DPP serves as a term to measure the feature diversity. In this paper, we devise a simple but effective algorithm to address this issue to optimize DPP term directly expressed with L-ensemble in spectral domain over gram matrix, which is more flexible than learning on parametric kernels. By further taking into account some geometric constraints, our algorithm seeks to generate valid sub-gradients of DPP term in case when the DPP gram matrix is not invertible (no gradients exist in this case). In this sense, our algorithm can be easily incorporated with multiple deep learning tasks. Experiments show the effectiveness of our algorithm, indicating promising performance for practical learning problems. | Tianshu Yu, Yikang Li, Baoxin Li |  |
| 25 |  |  [N-BEATS: Neural basis expansion analysis for interpretable time series forecasting](https://openreview.net/forum?id=r1ecqn4YwB) |  | 0 | We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy. | Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio |  |
| 26 |  |  [Automated Relational Meta-learning](https://openreview.net/forum?id=rklp93EtwH) |  | 0 | In order to efficiently learn with small amount of data on new tasks, meta-learning transfers knowledge learned from previous tasks to the new ones. However, a critical challenge in meta-learning is the task heterogeneity which cannot be well handled by traditional globally shared meta-learning methods. In addition, current task-specific meta-learning methods may either suffer from hand-crafted structure design or lack the capability to capture complex relations between tasks. In this paper, motivated by the way of knowledge organization in knowledge bases, we propose an automated relational meta-learning (ARML) framework that automatically extracts the cross-task relations and constructs the meta-knowledge graph. When a new task arrives, it can quickly find the most relevant structure and tailor the learned structure knowledge to the meta-learner. As a result, the proposed framework not only addresses the challenge of task heterogeneity by a learned meta-knowledge graph, but also increases the model interpretability. We conduct extensive experiments on 2D toy regression and few-shot image classification and the results demonstrate the superiority of ARML over state-of-the-art baselines. | Huaxiu Yao, Xian Wu, Zhiqiang Tao, Yaliang Li, Bolin Ding, Ruirui Li, Zhenhui Li |  |
| 27 |  |  [To Relieve Your Headache of Training an MRF, Take AdVIL](https://openreview.net/forum?id=Sylgsn4Fvr) |  | 0 | We propose a black-box algorithm called {\it Adversarial Variational Inference and Learning} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results. | Chongxuan Li, Chao Du, Kun Xu, Max Welling, Jun Zhu, Bo Zhang |  |
| 28 |  |  [Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware](https://openreview.net/forum?id=H1lBj2VFPS) |  | 0 | With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware coordination at design-phase. In this paper, we propose a learned linear symmetric quantizer for integer neural network processors, which not only quantizes neural parameters and activations to low-bit integer but also accelerates hardware inference by using batch normalization fusion and low-precision accumulators (e.g., 16-bit) and multipliers (e.g., 4-bit). We use a unified way to quantize weights and activations, and the results outperform many previous approaches for various networks such as AlexNet, ResNet, and lightweight models like MobileNet while keeping friendly to the accelerator architecture. Additional, we also apply the method to object detection models and witness high performance and accuracy in YOLO-v2. Finally, we deploy the quantized models on our specialized integer-arithmetic-only DNN accelerator to show the effectiveness of the proposed quantizer. We show that even with linear symmetric quantization, the results can be better than asymmetric or non-linear methods in 4-bit networks. In evaluation, the proposed quantizer induces less than 0.4\% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the whole network as required by the integer processors. | Xiandong Zhao, Ying Wang, Xuyi Cai, Cheng Liu, Lei Zhang |  |
| 29 |  |  [Weakly Supervised Clustering by Exploiting Unique Class Count](https://openreview.net/forum?id=B1xIj3VYvr) |  | 0 | A weakly supervised learning based clustering framework is proposed in this paper. As the core of this framework, we introduce a novel multiple instance learning task based on a bag level label called unique class count (ucc), which is the number of unique classes among all instances inside the bag. In this task, no annotations on individual instances inside the bag are needed during training of the models. We mathematically prove that with a perfect ucc classifier, perfect clustering of individual instances inside the bags is possible even when no annotations on individual instances are given during training. We have constructed a neural network based ucc classifier and experimentally shown that the clustering performance of our framework with our weakly supervised ucc classifier is comparable to that of fully supervised learning models where labels for all instances are known. Furthermore, we have tested the applicability of our framework to a real world task of semantic segmentation of breast cancer metastases in histological lymph node sections and shown that the performance of our weakly supervised framework is comparable to the performance of a fully supervised Unet model. | Mustafa Umit Oner, Hwee Kuan Lee, WingKin Sung |  |
| 30 |  |  [Scalable and Order-robust Continual Learning with Additive Parameter Decomposition](https://openreview.net/forum?id=r1gdj2EKPB) |  | 0 | While recent continual learning methods largely alleviate the catastrophic problem on toy-sized datasets, there are issues that remain to be tackled in order to apply them to real-world problem domains. First, a continual learning model should effectively handle catastrophic forgetting and be efficient to train even with a large number of tasks. Secondly, it needs to tackle the problem of order-sensitivity, where the performance of the tasks largely varies based on the order of the task arrival sequence, as it may cause serious problems where fairness plays a critical role (e.g. medical diagnosis). To tackle these practical challenges, we propose a novel continual learning method that is scalable as well as order-robust, which instead of learning a completely shared set of weights, represents the parameters for each task as a sum of task-shared and sparse task-adaptive parameters. With our Additive Parameter Decomposition (APD), the task-adaptive parameters for earlier tasks remain mostly unaffected, where we update them only to reflect the changes made to the task-shared parameters. This decomposition of parameters effectively prevents catastrophic forgetting and order-sensitivity, while being computation- and memory-efficient. Further, we can achieve even better scalability with APD using hierarchical knowledge consolidation, which clusters the task-adaptive parameters to obtain hierarchically shared parameters. We validate our network with APD, APD-Net, on multiple benchmark datasets against state-of-the-art continual learning methods, which it largely outperforms in accuracy, scalability, and order-robustness. | Jaehong Yoon, Saehoon Kim, Eunho Yang, Sung Ju Hwang |  |
| 31 |  |  [Continual Learning with Adaptive Weights (CLAW)](https://openreview.net/forum?id=Hklso24Kwr) |  | 0 | Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. | Tameem Adel, Han Zhao, Richard E. Turner |  |
| 32 |  |  [Transferable Perturbations of Deep Feature Distributions](https://openreview.net/forum?id=rJxAo2VYwr) |  | 0 |  | Nathan Inkawhich, Kevin J. Liang, Lawrence Carin, Yiran Chen |  |
| 33 |  |  [A Learning-based Iterative Method for Solving Vehicle Routing Problems](https://openreview.net/forum?id=BJe1334YDH) |  | 0 | This paper is concerned with solving combinatorial optimization problems, in particular, the capacitated vehicle routing problems (CVRP). Classical Operations Research (OR) algorithms such as LKH3 \citep{helsgaun2017extension} are inefficient and difficult to scale to larger-size problems. Machine learning based approaches have recently shown to be promising, partly because of their efficiency (once trained, they can perform solving within minutes or even seconds). However, there is still a considerable gap between the quality of a machine learned solution and what OR methods can offer (e.g., on CVRP-100, the best result of learned solutions is between 16.10-16.80, significantly worse than LKH3's 15.65). In this paper, we present \`\`Learn to Improve'' (L2I), the first learning based approach for CVRP that is efficient in solving speed and at the same time outperforms OR methods. Starting with a random initial solution, L2I learns to iteratively refine the solution with an improvement operator, selected by a reinforcement learning based controller. The improvement operator is selected from a pool of powerful operators that are customized for routing problems. By combining the strengths of the two worlds, our approach achieves the new state-of-the-art results on CVRP, e.g., an average cost of 15.57 on CVRP-100. | Hao Lu, Xingwen Zhang, Shuang Yang |  |
| 34 |  |  [Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring](https://openreview.net/forum?id=SkxgnnNFvH) |  | 0 | The use of deep pre-trained transformers has led to remarkable progress in a number of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common: Cross-encoders performing full self-attention over the pair and Bi-encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-encoder, that learns global rather than token level self-attention features. We perform a detailed comparison of all three approaches, including what pre-training and fine-tuning strategies work best. We show our models achieve state-of-the-art results on four tasks; that Poly-encoders are faster than Cross-encoders and more accurate than Bi-encoders; and that the best results are obtained by pre-training on large datasets similar to the downstream tasks. | Samuel Humeau, Kurt Shuster, MarieAnne Lachaux, Jason Weston |  |
| 35 |  |  [AutoQ: Automated Kernel-Wise Neural Network Quantization](https://openreview.net/forum?id=rygfnn4twS) |  | 0 | Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) DDPG-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06%, and decrease the inference energy consumption by 50.69%, while achieving the same inference accuracy. | Qian Lou, Feng Guo, Minje Kim, Lantao Liu, Lei Jiang |  |
| 36 |  |  [Understanding Architectures Learnt by Cell-based Neural Architecture Search](https://openreview.net/forum?id=BJxH22EKPS) |  | 0 | Neural architecture search (NAS) searches architectures automatically for given tasks, e.g., image classification and language modeling. Improving the search efficiency and effectiveness has attracted increasing attention in recent years. However, few efforts have been devoted to understanding the generated architectures. In this paper, we first reveal that existing NAS algorithms (e.g., DARTS, ENAS) tend to favor architectures with wide and shallow cell structures. These favorable architectures consistently achieve fast convergence and are consequently selected by NAS algorithms. Our empirical and theoretical study further confirms that their fast convergence derives from their smooth loss landscape and accurate gradient information. Nonetheless, these architectures may not necessarily lead to better generalization performance compared with other candidate architectures in the same search space, and therefore further improvement is possible by revising existing NAS algorithms. | Yao Shu, Wei Wang, Shaofeng Cai |  |
| 37 |  |  [SVQN: Sequential Variational Soft Q-Learning Networks](https://openreview.net/forum?id=r1xPh2VtPB) |  | 0 | Partially Observable Markov Decision Processes (POMDPs) are popular and flexible models for real-world decision-making applications that demand the information from past observations to make optimal decisions. Standard reinforcement learning algorithms for solving Markov Decision Processes (MDP) tasks are not applicable, as they cannot infer the unobserved states. In this paper, we propose a novel algorithm for POMDPs, named sequential variational soft Q-learning networks (SVQNs), which formalizes the inference of hidden states and maximum entropy reinforcement learning (MERL) under a unified graphical model and optimizes the two modules jointly. We further design a deep recurrent neural network to reduce the computational complexity of the algorithm. Experimental results show that SVQNs can utilize past information to help decision making for efficient inference, and outperforms other baselines on several challenging tasks. Our ablation study shows that SVQNs have the generalization ability over time and are robust to the disturbance of the observation. | Shiyu Huang, Hang Su, Jun Zhu, Ting Chen |  |
| 38 |  |  [Ranking Policy Gradient](https://openreview.net/forum?id=rJld3hEYvS) |  | 0 | Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art. | Kaixiang Lin, Jiayu Zhou |  |
| 39 |  |  [On Mutual Information Maximization for Representation Learning](https://openreview.net/forum?id=rkxoh24FPH) |  | 0 | Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods. | Michael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, Mario Lucic |  |
| 40 |  |  [Observational Overfitting in Reinforcement Learning](https://openreview.net/forum?id=HJli2hNKDH) |  | 0 | A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL). | Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur |  |
| 41 |  |  [Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier](https://openreview.net/forum?id=BkgWahEFvr) |  | 0 | Adversarial attacks on convolutional neural networks (CNN) have gained significant attention and there have been active research efforts on defense mechanisms. Stochastic input transformation methods have been proposed, where the idea is to recover the image from adversarial attack by random transformation, and to take the majority vote as consensus among the random samples. However, the transformation improves the accuracy on adversarial images at the expense of the accuracy on clean images. While it is intuitive that the accuracy on clean images would deteriorate, the exact mechanism in which how this occurs is unclear. In this paper, we study the distribution of softmax induced by stochastic transformations. We observe that with random transformations on the clean images, although the mass of the softmax distribution could shift to the wrong class, the resulting distribution of softmax could be used to correct the prediction. Furthermore, on the adversarial counterparts, with the image transformation, the resulting shapes of the distribution of softmax are similar to the distributions from the clean images. With these observations, we propose a method to improve existing transformation-based defenses. We train a separate lightweight distribution classifier to recognize distinct features in the distributions of softmax outputs of transformed images. Our empirical studies show that our distribution classifier, by training on distributions obtained from clean images only, outperforms majority voting for both clean and adversarial images. Our method is generic and can be integrated with existing transformation-based defenses. | Connie Kou, Hwee Kuan Lee, EeChien Chang, Teck Khim Ng |  |
| 42 |  |  [Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks](https://openreview.net/forum?id=BkgXT24tDS) |  | 0 | We propose Additive Powers-of-Two~(APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model reduces 22% computational cost compared with the uniformly quantized counterpart. | Yuhang Li, Xin Dong, Wei Wang |  |
| 43 |  |  [Lazy-CFR: fast and near-optimal regret minimization for extensive games with imperfect information](https://openreview.net/forum?id=rJx4p3NYDB) |  | 0 | Counterfactual regret minimization (CFR) methods are effective for solving two-player zero-sum extensive games with imperfect information with state-of-the-art results. However, the vanilla CFR has to traverse the whole game tree in each round, which is time-consuming in large-scale games. In this paper, we present Lazy-CFR, a CFR algorithm that adopts a lazy update strategy to avoid traversing the whole game tree in each round. We prove that the regret of Lazy-CFR is almost the same to the regret of the vanilla CFR and only needs to visit a small portion of the game tree. Thus, Lazy-CFR is provably faster than CFR. Empirical results consistently show that Lazy-CFR is significantly faster than the vanilla CFR. | Yichi Zhou, Tongzheng Ren, Jialian Li, Dong Yan, Jun Zhu |  |
| 44 |  |  [Knowledge Consistency between Neural Networks and Beyond](https://openreview.net/forum?id=BJeS62EtwH) |  | 0 | This paper aims to analyze knowledge consistency between pre-trained deep neural networks. We propose a generic definition for knowledge consistency between neural networks at different fuzziness levels. A task-agnostic method is designed to disentangle feature components, which represent the consistent knowledge, from raw intermediate-layer features of each neural network. As a generic tool, our method can be broadly used for different applications. In preliminary experiments, we have used knowledge consistency as a tool to diagnose representations of neural networks. Knowledge consistency provides new insights to explain the success of existing deep-learning techniques, such as knowledge distillation and network compression. More crucially, knowledge consistency can also be used to refine pre-trained networks and boost performance. | Ruofan Liang, Tianlin Li, Longfei Li, Jing Wang, Quanshi Zhang |  |
| 45 |  |  [Image-guided Neural Object Rendering](https://openreview.net/forum?id=Hyg9anEFPS) |  | 0 | We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours and sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on \`\`remembering'' object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data. | Justus Thies, Michael Zollhöfer, Christian Theobalt, Marc Stamminger, Matthias Nießner |  |
| 46 |  |  [Implicit Bias of Gradient Descent based Adversarial Training on Separable Data](https://openreview.net/forum?id=HkgTTh4FDH) |  | 0 | Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this paper, we provide new theoretical insights of gradient descent based adversarial training by studying its computational properties, specifically on its implicit bias. We take the binary classification task on linearly separable data as an illustrative example, where the loss asymptotically attains its infimum as the parameter diverges to infinity along certain directions. Specifically, we show that for any fixed iteration $T$, when the adversarial perturbation during training has proper bounded L2 norm, the classifier learned by gradient descent based adversarial training converges in direction to the maximum L2 norm margin classifier at the rate of $O(1/\sqrt{T})$, significantly faster than the rate $O(1/\log T}$ of training with clean data. In addition, when the adversarial perturbation during training has bounded Lq norm, the resulting classifier converges in direction to a maximum mixed-norm margin classifier, which has a natural interpretation of robustness, as being the maximum L2 norm margin classifier under worst-case bounded Lq norm perturbation to the data. Our findings provide theoretical backups for adversarial training that it indeed promotes robustness against adversarial perturbation. | Yan Li, Ethan X. Fang, Huan Xu, Tuo Zhao |  |
| 47 |  |  [TabFact: A Large-scale Dataset for Table-based Fact Verification](https://openreview.net/forum?id=rkeJRhNYDH) |  | 0 |  | Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, William Yang Wang |  |
| 48 |  |  [ES-MAML: Simple Hessian-Free Meta Learning](https://openreview.net/forum?id=S1exA2NtDB) |  | 0 | We introduce ES-MAML, a new framework for solving the model agnostic meta learning (MAML) problem based on Evolution Strategies (ES). Existing algorithms for MAML are based on policy gradients, and incur significant difficulties when attempting to estimate second derivatives using backpropagation on stochastic policies. We show how ES can be applied to MAML to obtain an algorithm which avoids the problem of estimating second derivatives, and is also conceptually simple and easy to implement. Moreover, ES-MAML can handle new types of nonsmooth adaptation operators, and other techniques for improving performance and estimation of ES methods become applicable. We show empirically that ES-MAML is competitive with existing methods and often yields better adaptation with fewer queries. | Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, Yunhao Tang |  |
| 49 |  |  [Neural Stored-program Memory](https://openreview.net/forum?id=rkxxA24FDr) |  | 0 | Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks. | Hung Le, Truyen Tran, Svetha Venkatesh |  |
| 50 |  |  [Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation](https://openreview.net/forum?id=H1gzR2VKDH) |  | 0 | Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves more than 20% absolute performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. | Suraj Nair, Chelsea Finn |  |
| 51 |  |  [Multi-agent Reinforcement Learning for Networked System Control](https://openreview.net/forum?id=Syx7A3NFvH) |  | 0 | This paper considers multi-agent reinforcement learning (MARL) in networked system control. Specifically, each agent learns a decentralized control policy based on local observations and messages from connected neighbors. We formulate such a networked MARL (NMARL) problem as a spatiotemporal Markov decision process and introduce a spatial discount factor to stabilize the training of each local agent. Further, we propose a new differentiable communication protocol, called NeurComm, to reduce information loss and non-stationarity in NMARL. Based on experiments in realistic NMARL scenarios of adaptive traffic signal control and cooperative adaptive cruise control, an appropriate spatial discount factor effectively enhances the learning curves of non-communicative MARL algorithms, while NeurComm outperforms existing communication protocols in both learning efficiency and control performance. | Tianshu Chu, Sandeep Chinchali, Sachin Katti |  |
| 52 |  |  [FSPool: Learning Set Representations with Featurewise Sort Pooling](https://openreview.net/forum?id=HJgBA2VYwH) |  | 0 | Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on a variety of datasets. | Yan Zhang, Jonathon S. Hare, Adam PrügelBennett |  |
| 53 |  |  [Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction](https://openreview.net/forum?id=H1xPR3NtPB) |  | 0 |  | Taeuk Kim, Jihun Choi, Daniel Edmiston, Sanggoo Lee |  |
| 54 |  |  [Dynamically Pruned Message Passing Networks for Large-scale Knowledge Graph Reasoning](https://openreview.net/forum?id=rkeuAhVKvB) |  | 0 | We propose Dynamically Pruned Message Passing Networks (DPMPN) for large-scale knowledge graph reasoning. In contrast to existing models, embedding-based or path-based, we learn an input-dependent subgraph to explicitly model a sequential reasoning process. Each subgraph is dynamically constructed, expanding itself selectively under a flow-style attention mechanism. In this way, we can not only construct graphical explanations to interpret prediction, but also prune message passing in Graph Neural Networks (GNNs) to scale with the size of graphs. We take the inspiration from the consciousness prior proposed by Bengio to design a two-GNN framework to encode global input-invariant graph-structured representation and learn local input-dependent one coordinated by an attention module. Experiments show the reasoning capability in our model that is providing a clear graphical explanation as well as predicting results accurately, outperforming most state-of-the-art methods in knowledge base completion tasks. | Xiaoran Xu, Wei Feng, Yunsheng Jiang, Xiaohui Xie, Zhiqing Sun, ZhiHong Deng |  |
| 55 |  |  [Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks](https://openreview.net/forum?id=ByxtC2VtPB) |  | 0 | It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants. | Tianyu Pang, Kun Xu, Jun Zhu |  |
| 56 |  |  [Theory and Evaluation Metrics for Learning Disentangled Representations](https://openreview.net/forum?id=HJgK0h4Ywr) |  | 0 | We make two theoretical contributions to disentanglement learning by (a) defining precise semantics of disentangled representations, and (b) establishing robust metrics for evaluation. First, we characterize the concept “disentangled representations” used in supervised and unsupervised methods along three dimensions–informativeness, separability and interpretability–which can be expressed and quantified explicitly using information-theoretic constructs. This helps explain the behaviors of several well-known disentanglement learning models. We then propose robust metrics for measuring informativeness, separability and interpretability. Through a comprehensive suite of experiments, we show that our metrics correctly characterize the representations learned by different methods and are consistent with qualitative (visual) results. Thus, the metrics allow disentanglement learning methods to be compared on a fair ground. We also empirically uncovered new interesting properties of VAE-based methods and interpreted them with our formulation. These findings are promising and hopefully will encourage the design of more theoretically driven models for learning disentangled representations. | Kien Do, Truyen Tran |  |
| 57 |  |  [Measuring Compositional Generalization: A Comprehensive Method on Realistic Data](https://openreview.net/forum?id=SygcCnNKwr) |  | 0 | State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings. | Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, Olivier Bousquet |  |
| 58 |  |  [Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness](https://openreview.net/forum?id=Byg9A24tvB) |  | 0 | Previous work shows that adversarially robust generalization requires larger sample complexity, and the same dataset, e.g., CIFAR-10, which enables good standard accuracy may not suffice to train robust models. Since collecting new training data could be costly, we focus on better utilizing the given data by inducing the regions with high sample density in the feature space, which could lead to locally sufficient samples for robust learning. We first formally show that the softmax cross-entropy (SCE) loss and its variants convey inappropriate supervisory signals, which encourage the learned feature points to spread over the space sparsely in training. This inspires us to propose the Max-Mahalanobis center (MMC) loss to explicitly induce dense feature regions in order to benefit robustness. Namely, the MMC loss encourages the model to concentrate on learning ordered and compact representations, which gather around the preset optimal centers for different classes. We empirically demonstrate that applying the MMC loss can significantly improve robustness even under strong adaptive attacks, while keeping state-of-the-art accuracy on clean inputs with little extra computation compared to the SCE loss. | Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, Jun Zhu |  |
| 59 |  |  [The Implicit Bias of Depth: How Incremental Learning Drives Generalization](https://openreview.net/forum?id=H1lj0nNFwB) |  | 0 | A leading hypothesis for the surprising generalization of neural networks is that the dynamics of gradient descent bias the model towards simple solutions, by searching through the solution space in an incremental order of complexity. We formally define the notion of incremental learning dynamics and derive the conditions on depth and initialization for which this phenomenon arises in deep linear models. Our main theoretical contribution is a dynamical depth separation result, proving that while shallow models can exhibit incremental learning dynamics, they require the initialization to be exponentially small for these dynamics to present themselves. However, once the model becomes deeper, the dependence becomes polynomial and incremental learning can arise in more natural settings. We complement our theoretical findings by experimenting with deep matrix sensing, quadratic neural networks and with binary classification using diagonal and convolutional linear networks, showing all of these models exhibit incremental learning. | Daniel Gissin, Shai ShalevShwartz, Amit Daniely |  |
| 60 |  |  [The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget](https://openreview.net/forum?id=Hye1kTVFDS) |  | 0 | In many applications, it is desirable to extract only the relevant information from complex input data, which involves making a decision about which input features are relevant. The information bottleneck method formalizes this as an information-theoretic optimization problem by maintaining an optimal tradeoff between compression (throwing away irrelevant input information), and predicting the target. In many problem settings, including the reinforcement learning problems we consider in this work, we might prefer to compress only part of the input. This is typically the case when we have a standard conditioning input, such as a state observation, and a \`\`privileged'' input, which might correspond to the goal of a task, the output of a costly planning algorithm, or communication with another agent. In such cases, we might prefer to compress the privileged input, either to achieve better generalization (e.g., with respect to goals) or to minimize access to costly information (e.g., in the case of communication). Practical implementations of the information bottleneck based on variational inference require access to the privileged input in order to compute the bottleneck variable, so although they perform compression, this compression operation itself needs unrestricted, lossless access. In this work, we propose the variational bandwidth bottleneck, which decides for each example on the estimated value of the privileged information before seeing it, i.e., only based on the standard input, and then accordingly chooses stochastically, whether to access the privileged input or not. We formulate a tractable approximation to this framework and demonstrate in a series of reinforcement learning experiments that it can improve generalization and reduce access to computationally costly information. | Anirudh Goyal, Yoshua Bengio, Matthew M. Botvinick, Sergey Levine |  |
| 61 |  |  [Learning the Arrow of Time for Problems in Reinforcement Learning](https://openreview.net/forum?id=rylJkpEtwS) |  | 0 | We humans have an innate understanding of the asymmetric progression of time, which we use to efficiently and safely perceive and manipulate our environment. Drawing inspiration from that, we approach the problem of learning an arrow of time in a Markov (Decision) Process. We illustrate how a learned arrow of time can capture salient information about the environment, which in turn can be used to measure reachability, detect side-effects and to obtain an intrinsic reward signal. Finally, we propose a simple yet effective algorithm to parameterize the problem at hand and learn an arrow of time with a function approximator (here, a deep neural network). Our empirical results span a selection of discrete and continuous environments, and demonstrate for a class of stochastic processes that the learned arrow of time agrees reasonably well with a well known notion of an arrow of time due to Jordan, Kinderlehrer and Otto (1998). | Nasim Rahaman, Steffen Wolf, Anirudh Goyal, Roman Remme, Yoshua Bengio |  |
| 62 |  |  [Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives](https://openreview.net/forum?id=ryxgJTEYDr) |  | 0 |  | Anirudh Goyal, Shagun Sodhani, Jonathan Binas, Xue Bin Peng, Sergey Levine, Yoshua Bengio |  |
| 63 |  |  [Robust Local Features for Improving the Generalization of Adversarial Training](https://openreview.net/forum?id=H1lZJpVFvr) |  | 0 | Adversarial training has been demonstrated as one of the most effective methods for training robust models to defend against adversarial examples. However, adversarially trained models often lack adversarially robust generalization on unseen testing data. Recent works show that adversarially trained models are more biased towards global structure features. Instead, in this work, we would like to investigate the relationship between the generalization of adversarial training and the robust local features, as the robust local features generalize well for unseen shape variation. To learn the robust local features, we develop a Random Block Shuffle (RBS) transformation to break up the global structure features on normal adversarial examples. We continue to propose a new approach called Robust Local Features for Adversarial Training (RLFAT), which first learns the robust local features by adversarial training on the RBS-transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples. To demonstrate the generality of our argument, we implement RLFAT in currently state-of-the-art adversarial training frameworks. Extensive experiments on STL-10, CIFAR-10 and CIFAR-100 show that RLFAT significantly improves both the adversarially robust generalization and the standard generalization of adversarial training. Additionally, we demonstrate that our models capture more local features of the object on the images, aligning better with human perception. | Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, John E. Hopcroft |  |
| 64 |  |  [Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification](https://openreview.net/forum?id=rJgQkT4twH) |  | 0 | Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs. | Bennet Breier, Arno Onken |  |
| 65 |  |  [Learning Disentangled Representations for CounterFactual Regression](https://openreview.net/forum?id=HkxBJT4YvB) |  | 0 | We consider the challenge of estimating treatment effects from observational data; and point out that, in general, only some factors based on the observed covariates X contribute to selection of the treatment T, and only some to determining the outcomes Y. We model this by considering three underlying sources of {X, T, Y} and show that explicitly modeling these sources offers great insight to guide designing models that better handle selection bias. This paper is an attempt to conceptualize this line of thought and provide a path to explore it further. In this work, we propose an algorithm to (1) identify disentangled representations of the above-mentioned underlying factors from any given observational dataset D and (2) leverage this knowledge to reduce, as well as account for, the negative impact of selection bias on estimating the treatment effects from D. Our empirical results show that the proposed method achieves state-of-the-art performance in both individual and population based evaluation measures. | Negar Hassanpour, Russell Greiner |  |
| 66 |  |  [Exploration in Reinforcement Learning with Deep Covering Options](https://openreview.net/forum?id=SkeIyaVtwB) |  | 0 | While many option discovery methods have been proposed to accelerate exploration in reinforcement learning, they are often heuristic. Recently, covering options was proposed to discover a set of options that provably reduce the upper bound of the environment's cover time, a measure of the difficulty of exploration. Covering options are computed using the eigenvectors of the graph Laplacian, but they are constrained to tabular tasks and are not applicable to tasks with large or continuous state-spaces. We introduce deep covering options, an online method that extends covering options to large state spaces, automatically discovering task-agnostic options that encourage exploration. We evaluate our method in several challenging sparse-reward domains and we show that our approach identifies less explored regions of the state-space and successfully generates options to visit these regions, substantially improving both the exploration and the total accumulated reward. | Yuu Jinnai, Jee Won Park, Marlos C. Machado, George Dimitri Konidaris |  |
| 67 |  |  [Ae-OT: a New Generative Model based on Extended Semi-discrete Optimal transport](https://openreview.net/forum?id=HkldyTNYwH) |  | 0 | Generative adversarial networks (GANs) have attracted huge attention due to its capability to generate visual realistic images. However, most of the existing models suffer from the mode collapse or mode mixture problems. In this work, we give a theoretic explanation of the both problems by Figalli’s regularity theory of optimal transportation maps. Basically, the generator compute the transportation maps between the white noise distributions and the data distributions, which are in general discontinuous. However, DNNs can only represent continuous maps. This intrinsic conflict induces mode collapse and mode mixture. In order to tackle the both problems, we explicitly separate the manifold embedding and the optimal transportation; the first part is carried out using an autoencoder to map the images onto the latent space; the second part is accomplished using a GPU-based convex optimization to find the discontinuous transportation maps. Composing the extended OT map and the decoder, we can finally generate new images from the white noise. This AE-OT model avoids representing discontinuous maps by DNNs, therefore effectively prevents mode collapse and mode mixture. | Dongsheng An, Yang Guo, Na Lei, Zhongxuan Luo, ShingTung Yau, Xianfeng Gu |  |
| 68 |  |  [Logic and the 2-Simplicial Transformer](https://openreview.net/forum?id=rkecJ6VFvr) |  | 0 | We introduce the 2-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning. | James Clift, Dmitry Doryn, Daniel Murfet, James Wallbridge |  |
| 69 |  |  [Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards](https://openreview.net/forum?id=SJg5J6NtDr) |  | 0 |  | Allan Zhou, Eric Jang, Daniel Kappler, Alexander Herzog, Mohi Khansari, Paul Wohlhart, Yunfei Bai, Mrinal Kalakrishnan, Sergey Levine, Chelsea Finn |  |
| 70 |  |  [Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking](https://openreview.net/forum?id=rJl31TNYPr) |  | 0 | Recent work in adversarial machine learning started to focus on the visual perception in autonomous driving and studied Adversarial Examples (AEs) for object detection models. However, in such visual perception pipeline the detected objects must also be tracked, in a process called Multiple Object Tracking (MOT), to build the moving trajectories of surrounding obstacles. Since MOT is designed to be robust against errors in object detection, it poses a general challenge to existing attack techniques that blindly target objection detection: we find that a success rate of over 98% is needed for them to actually affect the tracking results, a requirement that no existing attack technique can satisfy. In this paper, we are the first to study adversarial machine learning attacks against the complete visual perception pipeline in autonomous driving, and discover a novel attack technique, tracker hijacking, that can effectively fool MOT using AEs on object detection. Using our technique, successful AEs on as few as one single frame can move an existing object in to or out of the headway of an autonomous vehicle to cause potential safety hazards. We perform evaluation using the Berkeley Deep Drive dataset and find that on average when 3 frames are attacked, our attack can have a nearly 100% success rate while attacks that blindly target object detection only have up to 25%. | Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chan, Zhenyu Zhong, Tao Wei |  |
| 71 |  |  [DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://openreview.net/forum?id=HJgExaVtwr) |  | 0 | Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix . | Junnan Li, Richard Socher, Steven C. H. Hoi |  |
| 72 |  |  [Improving Adversarial Robustness Requires Revisiting Misclassified Examples](https://openreview.net/forum?id=rklOg6EFwS) |  | 0 | Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, we investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. Specifically, we find that misclassified examples indeed have a significant impact on the final robustness. More surprisingly, we find that different maximization techniques on misclassified examples may have a negligible influence on the final robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called {\em Misclassification Aware adveRsarial Training} (MART), which explicitly differentiates the misclassified and correctly classified examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could significantly improve the state-of-the-art adversarial robustness. | Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, Quanquan Gu |  |
| 73 |  |  [V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control](https://openreview.net/forum?id=SylOlp4FvH) |  | 0 | Some of the most successful applications of deep reinforcement learning to challenging domains in discrete and continuous control have used policy gradient methods in the on-policy setting. However, policy gradients can suffer from large variance that may limit performance, and in practice require carefully tuned entropy regularization to prevent policy collapse. As an alternative to policy gradient algorithms, we introduce V-MPO, an on-policy adaptation of Maximum a Posteriori Policy Optimization (MPO) that performs policy iteration based on a learned state-value function. We show that V-MPO surpasses previously reported scores for both the Atari-57 and DMLab-30 benchmark suites in the multi-task setting, and does so reliably without importance weighting, entropy regularization, or population-based tuning of hyperparameters. On individual DMLab and Atari levels, the proposed algorithm can achieve scores that are substantially higher than has previously been reported. V-MPO is also applicable to problems with high-dimensional, continuous action spaces, which we demonstrate in the context of learning to control simulated humanoids with 22 degrees of freedom from full state observations and 56 degrees of freedom from pixel observations, as well as example OpenAI Gym tasks where V-MPO achieves substantially higher asymptotic scores than previously reported. | H. Francis Song, Abbas Abdolmaleki, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin A. Riedmiller, Matthew M. Botvinick |  |
| 74 |  |  [Interpretable Complex-Valued Neural Networks for Privacy Protection](https://openreview.net/forum?id=S1xFl64tDr) |  | 0 | Previous studies have found that an adversary attacker can often infer unintended input information from intermediate-layer features. We study the possibility of preventing such adversarial inference, yet without too much accuracy degradation. We propose a generic method to revise the neural network to boost the challenge of inferring input attributes from features, while maintaining highly accurate outputs. In particular, the method transforms real-valued features into complex-valued ones, in which the input is hidden in a randomized phase of the transformed features. The knowledge of the phase acts like a key, with which any party can easily recover the output from the processing result, but without which the party can neither recover the output nor distinguish the original input. Preliminary experiments on various datasets and network structures have shown that our method significantly diminishes the adversary's ability in inferring about the input while largely preserves the resulting accuracy. | Liyao Xiang, Hao Zhang, Haotian Ma, Yifan Zhang, Jie Ren, Quanshi Zhang |  |
| 75 |  |  [Accelerating SGD with momentum for over-parameterized learning](https://openreview.net/forum?id=r1gixp4FPH) |  | 0 |  | Chaoyue Liu, Mikhail Belkin |  |
| 76 |  |  [A critical analysis of self-supervision, or what we can learn from a single image](https://openreview.net/forum?id=B1esx6EYvr) |  | 0 | We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset. | Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi |  |
| 77 |  |  [Disentangling Factors of Variations Using Few Labels](https://openreview.net/forum?id=SygagpEKwB) |  | 0 | Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of (some) factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples (0.01--0.5% of the data set), with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations. | Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem |  |
| 78 |  |  [Functional vs. parametric equivalence of ReLU networks](https://openreview.net/forum?id=Bylx-TNKvH) |  | 0 | We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest. | Mary Phuong, Christoph H. Lampert |  |
| 79 |  |  [Input Complexity and Out-of-distribution Detection with Likelihood-based Generative Models](https://openreview.net/forum?id=SyxIWpVYvr) |  | 0 |  | Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F. Núñez, Jordi Luque |  |
| 80 |  |  [RTFM: Generalising to New Environment Dynamics via Reading](https://openreview.net/forum?id=SJgob6NKvH) |  | 0 | Obtaining policies that can generalise to new environments in reinforcement learning is challenging. In this work, we demonstrate that language understanding via a reading policy learner is a promising vehicle for generalisation to new environments. We propose a grounded policy learning problem, Read to Fight Monsters (RTFM), in which the agent must jointly reason over a language goal, relevant dynamics described in a document, and environment observations. We procedurally generate environment dynamics and corresponding language descriptions of the dynamics, such that agents must read to understand new environment dynamics instead of memorising any particular information. In addition, we propose txt2π, a model that captures three-way interactions between the goal, document, and observations. On RTFM, txt2π generalises to new environments with dynamics not seen during training via reading. Furthermore, our model outperforms baselines such as FiLM and language-conditioned CNNs on RTFM. Through curriculum learning, txt2π produces policies that excel on complex RTFM tasks requiring several reasoning and coreference steps. | Victor Zhong, Tim Rocktäschel, Edward Grefenstette |  |
| 81 |  |  [What graph neural networks cannot learn: depth vs width](https://openreview.net/forum?id=B1l2bp4YwS) |  | 0 | This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp's depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation. | Andreas Loukas |  |
| 82 |  |  [Progressive Memory Banks for Incremental Domain Adaptation](https://openreview.net/forum?id=BkepbpNFwr) |  | 0 | This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in the current domain. The goal of IDA is to build a unified model performing well on all the domains that we have encountered. We adopt the recurrent neural network (RNN) widely used in NLP, but augment it with a directly parameterized memory bank, which is retrieved by an attention mechanism at each step of RNN transition. The memory bank provides a natural way of IDA: when adapting our model to a new domain, we progressively add new slots to the memory bank, which increases the number of parameters, and thus the model capacity. We learn the new memory slots and fine-tune existing parameters by back-propagation. Experimental results show that our approach achieves significantly better performance than fine-tuning alone. Compared with expanding hidden states, our approach is more robust for old domains, shown by both empirical and theoretical results. Our model also outperforms previous work of IDA including elastic weight consolidation and progressive neural networks in the experiments. | Nabiha Asghar, Lili Mou, Kira A. Selby, Kevin D. Pantasdo, Pascal Poupart, Xin Jiang |  |
| 83 |  |  [Automated curriculum generation through setter-solver interactions](https://openreview.net/forum?id=H1e0Wp4KvH) |  | 0 | Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula –the breakdown of tasks into simpler, static challenges with dense rewards– to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich,dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes. | Sébastien Racanière, Andrew K. Lampinen, Adam Santoro, David P. Reichert, Vlad Firoiu, Timothy P. Lillicrap |  |
| 84 |  |  [On Identifiability in Transformers](https://openreview.net/forum?id=BJg1f6EFDB) |  | 0 | In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models. | Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, Roger Wattenhofer |  |
| 85 |  |  [Exploring Model-based Planning with Policy Networks](https://openreview.net/forum?id=H1exf64KwH) |  | 0 | Model-based reinforcement learning (MBRL) with model-predictive control or online planning has shown great potential for locomotion control tasks in both sample efficiency and asymptotic performance. Despite the successes, the existing planning methods search from candidate sequences randomly generated in the action space, which is inefficient in complex high-dimensional environments. In this paper, we propose a novel MBRL algorithm, model-based policy planning (POPLIN), that combines policy networks with online planning. More specifically, we formulate action planning at each time-step as an optimization problem using neural networks. We experiment with both optimization w.r.t. the action sequences initialized from the policy network, and also online optimization directly w.r.t. the parameters of the policy network. We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments, being about 3x more sample efficient than the state-of-the-art algorithms, such as PETS, TD3 and SAC. To explain the effectiveness of our algorithm, we show that the optimization surface in parameter space is smoother than in action space. Further more, we found the distilled policy network can be effectively applied without the expansive model predictive control during test time for some environments such as Cheetah. Code is released. | Tingwu Wang, Jimmy Ba |  |
| 86 |  |  [Learning Self-Correctable Policies and Value Functions from Demonstrations with Negative Sampling](https://openreview.net/forum?id=rke-f6NKvS) |  | 0 |  | Yuping Luo, Huazhe Xu, Tengyu Ma |  |
| 87 |  |  [Geometric Insights into the Convergence of Nonlinear TD Learning](https://openreview.net/forum?id=SJezGp4YPr) |  | 0 | While there are convergence guarantees for temporal difference (TD) learning when using linear function approximators, the situation for nonlinear models is far less understood, and divergent examples are known. Here we take a first step towards extending theoretical convergence guarantees to TD learning with nonlinear function approximation. More precisely, we consider the expected learning dynamics of the TD(0) algorithm for value estimation. As the step-size converges to zero, these dynamics are defined by a nonlinear ODE which depends on the geometry of the space of function approximators, the structure of the underlying Markov chain, and their interaction. We find a set of function approximators that includes ReLU networks and has geometry amenable to TD learning regardless of environment, so that the solution performs about as well as linear TD in the worst case. Then, we show how environments that are more reversible induce dynamics that are better for TD learning and prove global convergence to the true value function for well-conditioned function approximators. Finally, we generalize a divergent counterexample to a family of divergent problems to demonstrate how the interaction between approximator and environment can go wrong and to motivate the assumptions needed to prove convergence. | David Brandfonbrener, Joan Bruna |  |
| 88 |  |  [Few-shot Text Classification with Distributional Signatures](https://openreview.net/forum?id=H1emfT4twB) |  | 0 | In this paper, we explore meta-learning for few-shot text classification. Meta-learning has shown strong performance in computer vision, where low-level patterns are transferable across learning tasks. However, directly applying this approach to text is challenging--lexical features highly informative for one task may be insignificant for another. Thus, rather than learning solely from words, our model also leverages their distributional signatures, which encode pertinent word occurrence patterns. Our model is trained within a meta-learning framework to map these signatures into attention scores, which are then used to weight the lexical representations of words. We demonstrate that our model consistently outperforms prototypical networks learned on lexical knowledge (Snell et al., 2017) in both few-shot text classification and relation classification by a significant margin across six benchmark datasets (20.0% on average in 1-shot classification). | Yujia Bao, Menghua Wu, Shiyu Chang, Regina Barzilay |  |
| 89 |  |  [Escaping Saddle Points Faster with Stochastic Momentum](https://openreview.net/forum?id=rkeNfp4tPr) |  | 0 | Stochastic gradient descent (SGD) with stochastic momentum is popular in nonconvex stochastic optimization and particularly for the training of deep neural networks. In standard SGD, parameters are updated by improving along the path of the gradient at the current iterate on a batch of examples, where the addition of a \`\`momentum'' term biases the update in the direction of the previous change in parameters. In non-stochastic convex optimization one can show that a momentum adjustment provably reduces convergence time in many settings, yet such results have been elusive in the stochastic and non-convex settings. At the same time, a widely-observed empirical phenomenon is that in training deep networks stochastic momentum appears to significantly improve convergence time, variants of it have flourished in the development of other popular update methods, e.g. ADAM, AMSGrad, etc. Yet theoretical justification for the use of stochastic momentum has remained a significant open question. In this paper we propose an answer: stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster and, consequently, to more quickly find a second order stationary point. Our theoretical results also shed light on the related question of how to choose the ideal momentum parameter--our analysis suggests that $\beta \in [0,1)$ should be large (close to 1), which comports with empirical findings. We also provide experimental findings that further validate these conclusions. | JunKun Wang, ChiHeng Lin, Jacob D. Abernethy |  |
| 90 |  |  [Adversarial Policies: Attacking Deep Reinforcement Learning](https://openreview.net/forum?id=HJgEMpVFwB) |  | 0 | Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/. | Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell |  |
| 91 |  |  [VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation](https://openreview.net/forum?id=rJgUfTEYvH) |  | 0 | Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video. | Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh, Durk Kingma |  |
| 92 |  |  [GLAD: Learning Sparse Graph Recovery](https://openreview.net/forum?id=BkxpMTEtPB) |  | 0 | Recovering sparse conditional independence graphs from data is a fundamental problem in machine learning with wide applications. A popular formulation of the problem is an $\ell_1$ regularized maximum likelihood estimation. Many convex optimization algorithms have been designed to solve this formulation to recover the graph structure. Recently, there is a surge of interest to learn algorithms directly based on data, and in this case, learn to map empirical covariance to the sparse precision matrix. However, it is a challenging task in this case, since the symmetric positive definiteness (SPD) and sparsity of the matrix are not easy to enforce in learned algorithms, and a direct mapping from data to precision matrix may contain many parameters. We propose a deep learning architecture, GLAD, which uses an Alternating Minimization (AM) algorithm as our model inductive bias, and learns the model parameters via supervised learning. We show that GLAD learns a very compact and effective model for recovering sparse graphs from data. | Harsh Shrivastava, Xinshi Chen, Binghong Chen, Guanghui Lan, Srinivas Aluru, Han Liu, Le Song |  |
| 93 |  |  [Pruned Graph Scattering Transforms](https://openreview.net/forum?id=rJeg7TEYwB) |  | 0 | Graph convolutional networks (GCNs) have achieved remarkable performance in a variety of network science learning tasks. However, theoretical analysis of such approaches is still at its infancy. Graph scattering transforms (GSTs) are non-trainable deep GCN models that are amenable to generalization and stability analyses. The present work addresses some limitations of GSTs by introducing a novel so-termed pruned (p)GST approach. The resultant pruning algorithm is guided by a graph-spectrum-inspired criterion, and retains informative scattering features on-the-fly while bypassing the exponential complexity associated with GSTs. It is further established that pGSTs are stable to perturbations of the input graph signals with bounded energy. Experiments showcase that i) pGST performs comparably to the baseline GST that uses all scattering features, while achieving significant computational savings; ii) pGST achieves comparable performance to state-of-the-art GCNs; and iii) Graph data from various domains lead to different scattering patterns, suggesting domain-adaptive pGST network architectures. | Vassilis N. Ioannidis, Siheng Chen, Georgios B. Giannakis |  |
| 94 |  |  [Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model](https://openreview.net/forum?id=BJlzm64tDH) |  | 0 | Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models trained with our new objective yield significant improvements on the fact completion task. When applied to downstream tasks, our model consistently outperforms BERT on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains. | Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov |  |
| 95 |  |  [Can gradient clipping mitigate label noise?](https://openreview.net/forum?id=rklB76EKPr) |  | 0 | Gradient clipping is a widely-used technique in the training of deep networks, and is generally motivated from an optimisation lens: informally, it controls the dynamics of iterates, thus enhancing the rate of convergence to a local minimum. This intuition has been made precise in a line of recent works, which show that suitable clipping can yield significantly faster convergence than vanilla gradient descent. In this paper, we propose a new lens for studying gradient clipping, namely, robustness: informally, one expects clipping to provide robustness to noise, since one does not overly trust any single sample. Surprisingly, we prove that for the common problem of label noise in classification, standard gradient clipping does not in general provide robustness. On the other hand, we show that a simple variant of gradient clipping is provably robust, and corresponds to suitably modifying the underlying loss function. This yields a simple, noise-robust alternative to the standard cross-entropy loss which performs well empirically. | Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar |  |
| 96 |  |  [Editable Neural Networks](https://openreview.net/forum?id=HJedXaEtvS) |  | 0 |  | Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V. Pyrkin, Sergei Popov, Artem Babenko |  |
| 97 |  |  [Learning Execution through Neural Code fusion](https://openreview.net/forum?id=SJetQpEYvB) |  | 0 |  | Zhan Shi, Kevin Swersky, Daniel Tarlow, Parthasarathy Ranganathan, Milad Hashemi |  |
| 98 |  |  [FasterSeg: Searching for Faster Real-time Semantic Segmentation](https://openreview.net/forum?id=BJgqQ6NYvB) |  | 0 | We present FasterSeg, an automatically designed semantic segmentation network with not only state-of-the-art performance but also faster speed than current methods. Utilizing neural architecture search (NAS), FasterSeg is discovered from a novel and broader search space integrating multi-resolution branches, that has been recently found to be vital in manually designed segmentation models. To better calibrate the balance between the goals of high accuracy and low latency, we propose a decoupled and fine-grained latency regularization, that effectively overcomes our observed phenomenons that the searched networks are prone to "collapsing" to low-latency yet poor-accuracy models. Moreover, we seamlessly extend FasterSeg to a new collaborative search (co-searching) framework, simultaneously searching for a teacher and a student network in the same single run. The teacher-student distillation further boosts the student model’s accuracy. Experiments on popular segmentation benchmarks demonstrate the competency of FasterSeg. For example, FasterSeg can run over 30% faster than the closest manually designed competitor on Cityscapes, while maintaining comparable accuracy. | Wuyang Chen, Xinyu Gong, Xianming Liu, Qian Zhang, Yuan Li, Zhangyang Wang |  |
| 99 |  |  [Difference-Seeking Generative Adversarial Network-Unseen Sample Generation](https://openreview.net/forum?id=rygjmpVFvB) |  | 0 |  | Yi Lin Sung, SungHsien Hsieh, SooChang Pei, ChunShien Lu |  |
| 100 |  |  [Stochastic AUC Maximization with Deep Neural Networks](https://openreview.net/forum?id=HJepXaVYDr) |  | 0 | Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms. | Mingrui Liu, Zhuoning Yuan, Yiming Ying, Tianbao Yang |  |
| 101 |  |  [Semantically-Guided Representation Learning for Self-Supervised Monocular Depth](https://openreview.net/forum?id=ByxT7TNFvH) |  | 0 | Self-supervised learning is showing great promise for monocular depth estimation, using geometry as the only source of supervision. Depth networks are indeed capable of learning representations that relate visual appearance to 3D properties by implicitly leveraging category-level patterns. In this work we investigate how to leverage more directly this semantic structure to guide geometric representation learning, while remaining in the self-supervised regime. Instead of using semantic labels and proxy losses in a multi-task approach, we propose a new architecture leveraging fixed pretrained semantic segmentation networks to guide self-supervised representation learning via pixel-adaptive convolutions. Furthermore, we propose a two-stage training process to overcome a common semantic bias on dynamic objects via resampling. Our method improves upon the state of the art for self-supervised monocular depth prediction over all pixels, fine-grained details, and per semantic categories. | Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, Adrien Gaidon |  |
| 102 |  |  [MACER: Attack-free and Scalable Robust Training via Maximizing Certified Radius](https://openreview.net/forum?id=rJx1Na4Fwr) |  | 0 | Adversarial training is one of the most popular ways to learn robust models but is usually attack-dependent and time costly. In this paper, we propose the MACER algorithm, which learns robust models without using adversarial training but performs better than all existing provable l2-defenses. Recent work shows that randomized smoothing can be used to provide a certified l2 radius to smoothed classifiers, and our algorithm trains provably robust smoothed classifiers via MAximizing the CErtified Radius (MACER). The attack-free characteristic makes MACER faster to train and easier to optimize. In our experiments, we show that our method can be applied to modern deep neural networks on a wide range of datasets, including Cifar-10, ImageNet, MNIST, and SVHN. For all tasks, MACER spends less training time than state-of-the-art adversarial training algorithms, and the learned models achieve larger average certified radius. | Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, ChoJui Hsieh, Liwei Wang |  |
| 103 |  |  [Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions](https://openreview.net/forum?id=Skgy464Kvr) |  | 0 | Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples. | Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison W. Cottrell, Geoffrey E. Hinton |  |
| 104 |  |  [GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification](https://openreview.net/forum?id=SJeQEp4YDH) |  | 0 |  | Xuwang Yin, Soheil Kolouri, Gustavo K. Rohde |  |
| 105 |  |  [Variational Recurrent Models for Solving Partially Observable Control Tasks](https://openreview.net/forum?id=r1lL4a4tDB) |  | 0 |  | Dongqi Han, Kenji Doya, Jun Tani |  |
| 106 |  |  [Population-Guided Parallel Policy Search for Reinforcement Learning](https://openreview.net/forum?id=rJeINp4KwH) |  | 0 |  | Whiyoung Jung, Giseung Park, Youngchul Sung |  |
| 107 |  |  [Compositional languages emerge in a neural iterated learning model](https://openreview.net/forum?id=HkePNpVKPB) |  | 0 |  | Yi Ren, Shangmin Guo, Matthieu Labeau, Shay B. Cohen, Simon Kirby |  |
| 108 |  |  [Black-Box Adversarial Attack with Transferable Model-based Embedding](https://openreview.net/forum?id=SJxhNTNYwB) |  | 0 | We present a new method for black-box adversarial attack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and Google Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate. | Zhichao Huang, Tong Zhang |  |
| 109 |  |  [I Am Going MAD: Maximum Discrepancy Competition for Comparing Classifiers Adaptively](https://openreview.net/forum?id=rJehNT4YPr) |  | 0 |  | Haotao Wang, Tianlong Chen, Zhangyang Wang, Kede Ma |  |
| 110 |  |  [Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models](https://openreview.net/forum?id=HkgaETNtDB) |  | 0 |  | Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang |  |
| 111 |  |  [Q-learning with UCB Exploration is Sample Efficient for Infinite-Horizon MDP](https://openreview.net/forum?id=BkglSTNFDB) |  | 0 | A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. (2018) proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \emph{without} accessing a generative model. We show that the \textit{sample complexity of exploration} of our algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$. This improves the previously best known result of $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in this setting achieved by delayed Q-learning (Strehlet al., 2006),, and matches the lower bound in terms of $\epsilon$ as well as $S$ and $A$ up to logarithmic factors. | Yuanhao Wang, Kefan Dong, Xiaoyu Chen, Liwei Wang |  |
| 112 |  |  [Deep Network Classification by Scattering and Homotopy Dictionary Learning](https://openreview.net/forum?id=SJxWS64FwH) |  | 0 | We introduce a sparse scattering deep convolutional neural network, which provides a simple model to analyze properties of deep representation learning for classification. Learning a single dictionary matrix with a classifier yields a higher classification accuracy than AlexNet over the ImageNet 2012 dataset. The network first applies a scattering transform that linearizes variabilities due to geometric transformations such as translations and small deformations. A sparse $\ell^1$ dictionary coding reduces intra-class variability while preserving class separation through projections over unions of linear spaces. It is implemented in a deep convolutional network with a homotopy algorithm having an exponential convergence. A convergence proof is given in a general framework that includes ALISTA. Classification results are analyzed on ImageNet. | John Zarka, Louis Thiry, Tomás Angles, Stéphane Mallat |  |
| 113 |  |  [Data-Independent Neural Pruning via Coresets](https://openreview.net/forum?id=H1gmHaEKwB) |  | 0 | Previous work showed empirically that large neural networks can be significantly reduced in size while preserving their accuracy. Model compression became a central research topic, as it is crucial for deployment of neural networks on devices with limited computational and memory resources. The majority of the compression methods are based on heuristics and offer no worst-case guarantees on the trade-off between the compression rate and the approximation error for an arbitrarily new sample. We propose the first efficient, data-independent neural pruning algorithm with a provable trade-off between its compression rate and the approximation error for any future test sample. Our method is based on the coreset framework, which finds a small weighted subset of points that provably approximates the original inputs. Specifically, we approximate the output of a layer of neurons by a coreset of neurons in the previous layer and discard the rest. We apply this framework in a layer-by-layer fashion from the top to the bottom. Unlike previous works, our coreset is data independent, meaning that it provably guarantees the accuracy of the function for any input $x\in \mathbb{R}^d$, including an adversarial one. We demonstrate the effectiveness of our method on popular network architectures. In particular, our coresets yield 90% compression of the LeNet-300-100 architecture on MNIST while improving the accuracy. | Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, Dan Feldman |  |
| 114 |  |  [Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks](https://openreview.net/forum?id=BkgXHTNtvS) |  | 0 |  | Arsalan SharifNassab, Saber Salehkaleybar, S. Jamaloddin Golestani |  |
| 115 |  |  [Novelty Detection Via Blurring](https://openreview.net/forum?id=ByeNra4FDB) |  | 0 |  | SungIk Choi, SaeYoung Chung |  |
| 116 |  |  [Piecewise linear activations substantially shape the loss surfaces of neural networks](https://openreview.net/forum?id=B1x6BTEKwr) |  | 0 |  | Fengxiang He, Bohan Wang, Dacheng Tao |  |
| 117 |  |  [Relational State-Space Model for Stochastic Multi-Object Systems](https://openreview.net/forum?id=B1lGU64tDr) |  | 0 |  | Fan Yang, Ling Chen, Fan Zhou, Yusong Gao, Wei Cao |  |
| 118 |  |  [Learning Efficient Parameter Server Synchronization Policies for Distributed SGD](https://openreview.net/forum?id=rJxX8T4Kvr) |  | 0 |  | Rong Zhu, Sheng Yang, Andreas Pfadler, Zhengping Qian, Jingren Zhou |  |
| 119 |  |  [Action Semantics Network: Considering the Effects of Actions in Multiagent Systems](https://openreview.net/forum?id=ryg48p4tPH) |  | 0 |  | Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao |  |
| 120 |  |  [Vid2Game: Controllable Characters Extracted from Real-World Videos](https://openreview.net/forum?id=SkxBUpEKwH) |  | 0 |  | Oran Gafni, Lior Wolf, Yaniv Taigman |  |
| 121 |  |  [Self-Adversarial Learning with Comparative Discrimination for Text Generation](https://openreview.net/forum?id=B1l8L6EtDS) |  | 0 |  | Wangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, Ming Zhou |  |
| 122 |  |  [Robust training with ensemble consensus](https://openreview.net/forum?id=ryxOUTVYDH) |  | 0 |  | Jisoo Lee, SaeYoung Chung |  |
| 123 |  |  [Identifying through Flows for Recovering Latent Representations](https://openreview.net/forum?id=SklOUpEYvB) |  | 0 |  | Shen Li, Bryan Hooi, Gim Hee Lee |  |
| 124 |  |  [Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing](https://openreview.net/forum?id=BkeWw6VFwr) |  | 0 |  | Jinyuan Jia, Xiaoyu Cao, Binghui Wang, Neil Zhenqiang Gong |  |
| 125 |  |  [Optimistic Exploration even with a Pessimistic Initialisation](https://openreview.net/forum?id=r1xGP6VYwH) |  | 0 |  | Tabish Rashid, Bei Peng, Wendelin Boehmer, Shimon Whiteson |  |
| 126 |  |  [VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://openreview.net/forum?id=SygXPaEYvH) |  | 0 |  | Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai |  |
| 127 |  |  [Deformable Kernels: Adapting Effective Receptive Fields for Object Deformation](https://openreview.net/forum?id=SkxSv6VFvS) |  | 0 |  | Hang Gao, Xizhou Zhu, Stephen Lin, Jifeng Dai |  |
| 128 |  |  [Ensemble Distribution Distillation](https://openreview.net/forum?id=BygSP6Vtvr) |  | 0 |  | Andrey Malinin, Bruno Mlodozeniec, Mark J. F. Gales |  |
| 129 |  |  [Gap-Aware Mitigation of Gradient Staleness](https://openreview.net/forum?id=B1lLw6EYwB) |  | 0 |  | Saar Barkai, Ido Hakimi, Assaf Schuster |  |
| 130 |  |  [Counterfactuals uncover the modular structure of deep generative models](https://openreview.net/forum?id=SJxDDpEKvH) |  | 0 |  | Michel Besserve, Arash Mehrjou, Rémy Sun, Bernhard Schölkopf |  |
| 131 |  |  [Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video](https://openreview.net/forum?id=BJeKwTNFvB) |  | 0 |  | Miguel Jaques, Michael Burke, Timothy M. Hospedales |  |
| 132 |  |  [An Inductive Bias for Distances: Neural Nets that Respect the Triangle Inequality](https://openreview.net/forum?id=HJeiDpVFPr) |  | 0 |  | Silviu Pitis, Harris Chan, Kiarash Jamali, Jimmy Ba |  |
| 133 |  |  [A Constructive Prediction of the Generalization Error Across Scales](https://openreview.net/forum?id=ryenvpEKDr) |  | 0 |  | Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, Nir Shavit |  |
| 134 |  |  [Scalable Neural Methods for Reasoning With a Symbolic Knowledge Base](https://openreview.net/forum?id=BJlguT4YPr) |  | 0 |  | William W. Cohen, Haitian Sun, R. Alex Hofer, Matthew Siegler |  |
| 135 |  |  [CLN2INV: Learning Loop Invariants with Continuous Logic Networks](https://openreview.net/forum?id=HJlfuTEtvB) |  | 0 |  | Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, Suman Jana |  |
| 136 |  |  [NAS evaluation is frustratingly hard](https://openreview.net/forum?id=HygrdpVKvr) |  | 0 |  | Antoine Yang, Pedro M. Esperança, Fabio Maria Carlucci |  |
| 137 |  |  [Efficient and Information-Preserving Future Frame Prediction and Beyond](https://openreview.net/forum?id=B1eY_pVYvB) |  | 0 |  | Wei Yu, Yichao Lu, Steve Easterbrook, Sanja Fidler |  |
| 138 |  |  [Order Learning and Its Application to Age Estimation](https://openreview.net/forum?id=HygsuaNFwr) |  | 0 |  | Kyungsun Lim, NyeongHo Shin, YoungYoon Lee, ChangSu Kim |  |
| 139 |  |  [ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning](https://openreview.net/forum?id=HJgJtT4tvB) |  | 0 |  | Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng |  |
| 140 |  |  [AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures](https://openreview.net/forum?id=SJgMK64Ywr) |  | 0 |  | Michael S. Ryoo, A. J. Piergiovanni, Mingxing Tan, Anelia Angelova |  |
| 141 |  |  [Adversarially Robust Representations with Smooth Encoders](https://openreview.net/forum?id=H1gfFaEYDS) |  | 0 |  | A. Taylan Cemgil, Sumedh Ghaisas, Krishnamurthy (Dj) Dvijotham, Pushmeet Kohli |  |
| 142 |  |  [From Variational to Deterministic Autoencoders](https://openreview.net/forum?id=S1g7tpEYDS) |  | 0 |  | Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael J. Black, Bernhard Schölkopf |  |
| 143 |  |  [Computation Reallocation for Object Detection](https://openreview.net/forum?id=SkxLFaNKwB) |  | 0 |  | Feng Liang, Chen Lin, Ronghao Guo, Ming Sun, Wei Wu, Junjie Yan, Wanli Ouyang |  |
| 144 |  |  [Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents](https://openreview.net/forum?id=rylvYaNYDH) |  | 0 |  | Christian Rupprecht, Cyril Ibrahim, Christopher J. Pal |  |
| 145 |  |  [A Fair Comparison of Graph Neural Networks for Graph Classification](https://openreview.net/forum?id=HygDF6NFPB) |  | 0 |  | Federico Errica, Marco Podda, Davide Bacciu, Alessio Micheli |  |
| 146 |  |  [Generalization bounds for deep convolutional neural networks](https://openreview.net/forum?id=r1e_FpNFDr) |  | 0 |  | Philip M. Long, Hanie Sedghi |  |
| 147 |  |  [SAdam: A Variant of Adam for Strongly Convex Functions](https://openreview.net/forum?id=rye5YaEtPr) |  | 0 |  | Guanghui Wang, Shiyin Lu, Quan Cheng, Weiwei Tu, Lijun Zhang |  |
| 148 |  |  [Continual Learning with Bayesian Neural Networks for Non-Stationary Data](https://openreview.net/forum?id=SJlsFpVtDB) |  | 0 |  | Richard Kurle, Botond Cseke, Alexej Klushyn, Patrick van der Smagt, Stephan Günnemann |  |
| 149 |  |  [Multiplicative Interactions and Where to Find Them](https://openreview.net/forum?id=rylnK6VtDH) |  | 0 |  | Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack W. Rae, Simon Osindero, Yee Whye Teh, Tim Harley, Razvan Pascanu |  |
| 150 |  |  [Few-Shot Learning on graphs via super-Classes based on Graph spectral Measures](https://openreview.net/forum?id=Bkeeca4Kvr) |  | 0 |  | Jatin Chauhan, Deepak Nathani, Manohar Kaul |  |
| 151 |  |  [On Computation and Generalization of Generative Adversarial Imitation Learning](https://openreview.net/forum?id=BJl-5pNKDB) |  | 0 |  | Minshuo Chen, Yizhou Wang, Tianyi Liu, Zhuoran Yang, Xingguo Li, Zhaoran Wang, Tuo Zhao |  |
| 152 |  |  [A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning](https://openreview.net/forum?id=BylVcTNtDS) |  | 0 |  | Shahbaz Rezaei, Xin Liu |  |
| 153 |  |  [Low-Resource Knowledge-Grounded Dialogue Generation](https://openreview.net/forum?id=rJeIcTNtvS) |  | 0 |  | Xueliang Zhao, Wei Wu, Chongyang Tao, Can Xu, Dongyan Zhao, Rui Yan |  |
| 154 |  |  [Deep 3D Pan via local adaptive "t-shaped" convolutions with global and local adaptive dilations](https://openreview.net/forum?id=B1gF56VYPH) |  | 0 |  | Juan Luis Gonzalez Bello, Munchurl Kim |  |
| 155 |  |  [Tree-Structured Attention with Hierarchical Accumulation](https://openreview.net/forum?id=HJxK5pEYvr) |  | 0 |  | XuanPhi Nguyen, Shafiq R. Joty, Steven C. H. Hoi, Richard Socher |  |
| 156 |  |  [The asymptotic spectrum of the Hessian of DNN throughout training](https://openreview.net/forum?id=SkgscaNYPS) |  | 0 |  | Arthur Jacot, Franck Gabriel, Clément Hongler |  |
| 157 |  |  [Actor-Critic Provably Finds Nash Equilibria of Linear-Quadratic Mean-Field Games](https://openreview.net/forum?id=H1lhqpEYPr) |  | 0 |  | Zuyue Fu, Zhuoran Yang, Yongxin Chen, Zhaoran Wang |  |
| 158 |  |  [In Search for a SAT-friendly Binarized Neural Network Architecture](https://openreview.net/forum?id=SJx-j64FDr) |  | 0 |  | Nina Narodytska, Hongce Zhang, Aarti Gupta, Toby Walsh |  |
| 159 |  |  [Generative Ratio Matching Networks](https://openreview.net/forum?id=SJg7spEYDS) |  | 0 |  | Akash Srivastava, Kai Xu, Michael U. Gutmann, Charles Sutton |  |
| 160 |  |  [Learning to Represent Programs with Property Signatures](https://openreview.net/forum?id=rylHspEKPr) |  | 0 |  | Augustus Odena, Charles Sutton |  |
| 161 |  |  [V4D: 4D Convolutional Neural Networks for Video-level Representation Learning](https://openreview.net/forum?id=SJeLopEYDH) |  | 0 |  | Shiwen Zhang, Sheng Guo, Weilin Huang, Matthew R. Scott, Limin Wang |  |
| 162 |  |  [Option Discovery using Deep Skill Chaining](https://openreview.net/forum?id=B1gqipNYwH) |  | 0 |  | Akhil Bagaria, George Konidaris |  |
| 163 |  |  [Quantifying the Cost of Reliable Photo Authentication via High-Performance Learned Lossy Representations](https://openreview.net/forum?id=HyxG3p4twS) |  | 0 |  | Pawel Korus, Nasir D. Memon |  |
| 164 |  |  [On the Variance of the Adaptive Learning Rate and Beyond](https://openreview.net/forum?id=rkgz2aEKDr) |  | 0 |  | Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han |  |
| 165 |  |  [Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery](https://openreview.net/forum?id=H1lmhaVtvr) |  | 0 |  | Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine |  |
| 166 |  |  [A Theoretical Analysis of the Number of Shots in Few-Shot Learning](https://openreview.net/forum?id=HkgB2TNYPS) |  | 0 |  | Tianshi Cao, Marc T. Law, Sanja Fidler |  |
| 167 |  |  [Unsupervised Model Selection for Variational Disentangled Representation Learning](https://openreview.net/forum?id=SyxL2TNtvr) |  | 0 |  | Sunny Duan, Loic Matthey, Andre Saraiva, Nick Watters, Chris Burgess, Alexander Lerchner, Irina Higgins |  |
| 168 |  |  [Feature Interaction Interpretability: A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection](https://openreview.net/forum?id=BkgnhTEtDS) |  | 0 |  | Michael Tsang, Dehua Cheng, Hanpeng Liu, Xue Feng, Eric Zhou, Yan Liu |  |
| 169 |  |  [Understanding the Limitations of Variational Mutual Information Estimators](https://openreview.net/forum?id=B1x62TNtDS) |  | 0 |  | Jiaming Song, Stefano Ermon |  |
| 170 |  |  [GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations](https://openreview.net/forum?id=BkxfaTVFwH) |  | 0 |  | Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner |  |
| 171 |  |  [Language GANs Falling Short](https://openreview.net/forum?id=BJgza6VtPB) |  | 0 |  | Massimo Caccia, Lucas Caccia, William Fedus, Hugo Larochelle, Joelle Pineau, Laurent Charlin |  |
| 172 |  |  [Stochastic Conditional Generative Networks with Basis Decomposition](https://openreview.net/forum?id=S1lSapVtwS) |  | 0 |  | Ze Wang, Xiuyuan Cheng, Guillermo Sapiro, Qiang Qiu |  |
| 173 |  |  [Learned Step Size quantization](https://openreview.net/forum?id=rkgO66VKDS) |  | 0 |  | Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S. Modha |  |
| 174 |  |  [On the "steerability" of generative adversarial networks](https://openreview.net/forum?id=HylsTT4FvB) |  | 0 |  | Ali Jahanian, Lucy Chai, Phillip Isola |  |
| 175 |  |  [Reinforced active learning for image segmentation](https://openreview.net/forum?id=SkgC6TNFvr) |  | 0 |  | Arantxa Casanova, Pedro O. Pinheiro, Negar Rostamzadeh, Christopher J. Pal |  |
| 176 |  |  [Sign Bits Are All You Need for Black-Box Attacks](https://openreview.net/forum?id=SygW0TEFwH) |  | 0 |  | Abdullah AlDujaili, UnaMay O'Reilly |  |
| 177 |  |  [Deep Semi-Supervised Anomaly Detection](https://openreview.net/forum?id=HkgH0TEYwH) |  | 0 |  | Lukas Ruff, Robert A. Vandermeulen, Nico Görnitz, Alexander Binder, Emmanuel Müller, KlausRobert Müller, Marius Kloft |  |
| 178 |  |  [Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints](https://openreview.net/forum?id=HyxLRTVKPH) |  | 0 |  | Mengtian Li, Ersin Yumer, Deva Ramanan |  |
| 179 |  |  [Minimizing FLOPs to Learn Efficient Sparse Representations](https://openreview.net/forum?id=SygpC6Ntvr) |  | 0 |  | Biswajit Paria, ChihKuan Yeh, Ian EnHsu Yen, Ning Xu, Pradeep Ravikumar, Barnabás Póczos |  |
| 180 |  |  [Reanalysis of Variance Reduced Temporal Difference Learning](https://openreview.net/forum?id=S1ly10EKDS) |  | 0 |  | Tengyu Xu, Zhe Wang, Yi Zhou, Yingbin Liang |  |
| 181 |  |  [Imitation Learning via Off-Policy Distribution Matching](https://openreview.net/forum?id=Hyg-JC4FDr) |  | 0 |  | Ilya Kostrikov, Ofir Nachum, Jonathan Tompson |  |
| 182 |  |  [Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML](https://openreview.net/forum?id=rkgMkCEtPB) |  | 0 |  | Aniruddh Raghu, Maithra Raghu, Samy Bengio, Oriol Vinyals |  |
| 183 |  |  [Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space](https://openreview.net/forum?id=H1lmyRNFvr) |  | 0 |  | AkshatKumar Nigam, Pascal Friederich, Mario Krenn, Alán AspuruGuzik |  |
| 184 |  |  [Improved Sample Complexities for Deep Neural Networks and Robust Classification via an All-Layer Margin](https://openreview.net/forum?id=HJe_yR4Fwr) |  | 0 |  | Colin Wei, Tengyu Ma |  |
| 185 |  |  [Identity Crisis: Memorization and Generalization Under Extreme Overparameterization](https://openreview.net/forum?id=B1l6y0VFPr) |  | 0 |  | Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C. Mozer, Yoram Singer |  |
| 186 |  |  [ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring](https://openreview.net/forum?id=HklkeR4KPB) |  | 0 |  | David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, Colin Raffel |  |
| 187 |  |  [Adaptive Structural Fingerprints for Graph Attention Networks](https://openreview.net/forum?id=BJxWx0NYPr) |  | 0 |  | Kai Zhang, Yaokang Zhu, Jun Wang, Jie Zhang |  |
| 188 |  |  [CAQL: Continuous Action Q-Learning](https://openreview.net/forum?id=BkxXe0Etwr) |  | 0 |  | Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, Craig Boutilier |  |
| 189 |  |  [Learning Heuristics for Quantified Boolean Formulas through Reinforcement Learning](https://openreview.net/forum?id=BJluxREKDB) |  | 0 |  | Gil Lederman, Markus N. Rabe, Sanjit Seshia, Edward A. Lee |  |
| 190 |  |  [Pure and Spurious Critical Points: a Geometric Study of Linear Networks](https://openreview.net/forum?id=rkgOlCVYvB) |  | 0 |  | Matthew Trager, Kathlén Kohn, Joan Bruna |  |
| 191 |  |  [Neural Text Generation With Unlikelihood Training](https://openreview.net/forum?id=SJeYe0NtvH) |  | 0 |  | Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston |  |
| 192 |  |  [Semi-Supervised Generative Modeling for Controllable Speech Synthesis](https://openreview.net/forum?id=rJeqeCEtvH) |  | 0 |  | Raza Habib, Soroosh Mariooryad, Matt Shannon, Eric Battenberg, R. J. SkerryRyan, Daisy Stanton, David Kao, Tom Bagby |  |
| 193 |  |  [Dynamic Time Lag Regression: Predicting What & When](https://openreview.net/forum?id=SkxybANtDB) |  | 0 |  | Mandar Chandorkar, Cyril Furtlehner, Bala Poduval, Enrico Camporeale, Michèle Sebag |  |
| 194 |  |  [Scalable Model Compression by Entropy Penalized Reparameterization](https://openreview.net/forum?id=HkgxW0EYDS) |  | 0 |  | Deniz Oktay, Johannes Ballé, Saurabh Singh, Abhinav Shrivastava |  |
| 195 |  |  [AMRL: Aggregated Memory For Reinforcement Learning](https://openreview.net/forum?id=Bkl7bREtDr) |  | 0 |  | Jacob Beck, Kamil Ciosek, Sam Devlin, Sebastian Tschiatschek, Cheng Zhang, Katja Hofmann |  |
| 196 |  |  [Efficient Riemannian Optimization on the Stiefel Manifold via the Cayley Transform](https://openreview.net/forum?id=HJxV-ANKDH) |  | 0 |  | Jun Li, Fuxin Li, Sinisa Todorovic |  |
| 197 |  |  [Unpaired Point Cloud Completion on Real Scans using Adversarial Training](https://openreview.net/forum?id=HkgrZ0EYwB) |  | 0 |  | Xuelin Chen, Baoquan Chen, Niloy J. Mitra |  |
| 198 |  |  [Adjustable Real-time Style Transfer](https://openreview.net/forum?id=HJe_Z04Yvr) |  | 0 |  | Mohammad Babaeizadeh, Golnaz Ghiasi |  |
| 199 |  |  [Stochastic Weight Averaging in Parallel: Large-Batch Training That Generalizes Well](https://openreview.net/forum?id=rygFWAEFwS) |  | 0 |  | Vipul Gupta, Santiago Akle Serrano, Dennis DeCoste |  |
| 200 |  |  [Short and Sparse Deconvolution - A Geometric Approach](https://openreview.net/forum?id=Byg5ZANtvH) |  | 0 |  | Yenson Lau, Qing Qu, HanWen Kuo, Pengcheng Zhou, Yuqian Zhang, John Wright |  |
| 201 |  |  [Selection via Proxy: Efficient Data Selection for Deep Learning](https://openreview.net/forum?id=HJg2b0VYDr) |  | 0 |  | Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, Matei Zaharia |  |
| 202 |  |  [Global Relational Models of Source Code](https://openreview.net/forum?id=B1lnbRNtwr) |  | 0 |  | Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, David Bieber |  |
| 203 |  |  [Detecting Extrapolation with Local Ensembles](https://openreview.net/forum?id=BJl6bANtwH) |  | 0 |  | David Madras, James Atwood, Alexander D'Amour |  |
| 204 |  |  [Learning to Link](https://openreview.net/forum?id=S1eRbANtDB) |  | 0 |  | MariaFlorina Balcan, Travis Dick, Manuel Lang |  |
| 205 |  |  [Adversarially robust transfer learning](https://openreview.net/forum?id=ryebG04YvB) |  | 0 |  | Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David W. Jacobs, Tom Goldstein |  |
| 206 |  |  [Overlearning Reveals Sensitive Attributes](https://openreview.net/forum?id=SJeNz04tDS) |  | 0 |  | Congzheng Song, Vitaly Shmatikov |  |
| 207 |  |  [Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness](https://openreview.net/forum?id=SJgwzCEKwH) |  | 0 |  | Pu Zhao, PinYu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue Lin |  |
| 208 |  |  [Differentially Private Meta-Learning](https://openreview.net/forum?id=rJgqMRVYvr) |  | 0 |  | Jeffrey Li, Mikhail Khodak, Sebastian Caldas, Ameet Talwalkar |  |
| 209 |  |  [One-Shot Pruning of Recurrent Neural Networks by Jacobian Spectrum Evaluation](https://openreview.net/forum?id=r1e9GCNKvH) |  | 0 |  | Matthew Shunshi Zhang, Bradly C. Stadie |  |
| 210 |  |  [Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples](https://openreview.net/forum?id=rkgAGAVKPr) |  | 0 |  | Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, PierreAntoine Manzagol, Hugo Larochelle |  |
| 211 |  |  [Are Transformers universal approximators of sequence-to-sequence functions?](https://openreview.net/forum?id=ByxRM0Ntvr) |  | 0 |  | Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar |  |
| 212 |  |  [Pre-training Tasks for Embedding-based Large-scale Retrieval](https://openreview.net/forum?id=rkg-mA4FDr) |  | 0 |  | WeiCheng Chang, Felix X. Yu, YinWen Chang, Yiming Yang, Sanjiv Kumar |  |
| 213 |  |  [Deep Imitative Models for Flexible Inference, Planning, and Control](https://openreview.net/forum?id=Skl4mRNYDr) |  | 0 |  | Nicholas Rhinehart, Rowan McAllister, Sergey Levine |  |
| 214 |  |  [CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning](https://openreview.net/forum?id=S1lEX04tPr) |  | 0 |  | Jiachen Yang, Alireza Nakhaei, David Isele, Kikuo Fujimura, Hongyuan Zha |  |
| 215 |  |  [Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks](https://openreview.net/forum?id=HJlSmC4FPS) |  | 0 |  | Sreyas Mohan, Zahra Kadkhodaie, Eero P. Simoncelli, Carlos FernandezGranda |  |
| 216 |  |  [Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets](https://openreview.net/forum?id=SJxIm0VtwH) |  | 0 |  | Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, Tianbao Yang |  |
| 217 |  |  [DeepV2D: Video to Depth with Differentiable Structure from Motion](https://openreview.net/forum?id=HJeO7RNKPr) |  | 0 |  | Zachary Teed, Jia Deng |  |
| 218 |  |  [Learning Space Partitions for Nearest Neighbor Search](https://openreview.net/forum?id=rkenmREFDr) |  | 0 |  | Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, Tal Wagner |  |
| 219 |  |  [Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP](https://openreview.net/forum?id=S1xnXRVFwH) |  | 0 |  | Haonan Yu, Sergey Edunov, Yuandong Tian, Ari S. Morcos |  |
| 220 |  |  [Sign-OPT: A Query-Efficient Hard-label Adversarial Attack](https://openreview.net/forum?id=SklTQCNtvS) |  | 0 |  | Minhao Cheng, Simranjit Singh, Patrick H. Chen, PinYu Chen, Sijia Liu, ChoJui Hsieh |  |
| 221 |  |  [RaCT: Toward Amortized Ranking-Critical Training For Collaborative Filtering](https://openreview.net/forum?id=HJxR7R4FvS) |  | 0 |  | Sam Lobel, Chunyuan Li, Jianfeng Gao, Lawrence Carin |  |
| 222 |  |  [Intrinsic Motivation for Encouraging Synergistic Behavior](https://openreview.net/forum?id=SJleNCNtDH) |  | 0 |  | Rohan Chitnis, Shubham Tulsiani, Saurabh Gupta, Abhinav Gupta |  |
| 223 |  |  [Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation](https://openreview.net/forum?id=rygG4AVFvH) |  | 0 |  | Byung Hoon Ahn, Prannoy Pilligundla, Amir Yazdanbakhsh, Hadi Esmaeilzadeh |  |
| 224 |  |  [Recurrent neural circuits for contour detection](https://openreview.net/forum?id=H1gB4RVKvB) |  | 0 |  | Drew Linsley, Junkyung Kim, Alekh Ashok, Thomas Serre |  |
| 225 |  |  [Locality and Compositionality in Zero-Shot Learning](https://openreview.net/forum?id=Hye_V0NKwr) |  | 0 |  | Tristan Sylvain, Linda Petrini, R. Devon Hjelm |  |
| 226 |  |  [Understanding Knowledge Distillation in Non-autoregressive Machine Translation](https://openreview.net/forum?id=BygFVAEKDH) |  | 0 |  | Chunting Zhou, Jiatao Gu, Graham Neubig |  |
| 227 |  |  [Thieves on Sesame Street! Model Extraction of BERT-based APIs](https://openreview.net/forum?id=Byl5NREFDr) |  | 0 |  | Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, Mohit Iyyer |  |
| 228 |  |  [Fast is better than free: Revisiting adversarial training](https://openreview.net/forum?id=BJx040EFvH) |  | 0 |  | Eric Wong, Leslie Rice, J. Zico Kolter |  |
| 229 |  |  [DBA: Distributed Backdoor Attacks against Federated Learning](https://openreview.net/forum?id=rkgyS0VFvr) |  | 0 |  | Chulin Xie, Keli Huang, PinYu Chen, Bo Li |  |
| 230 |  |  [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH) |  | 0 |  | Sachin Mehta, Rik KoncelKedziorski, Mohammad Rastegari, Hannaneh Hajishirzi |  |
| 231 |  |  [Sampling-Free Learning of Bayesian Quantized Neural Networks](https://openreview.net/forum?id=rylVHR4FPB) |  | 0 |  | Jiahao Su, Milan Cvitkovic, Furong Huang |  |
| 232 |  |  [Learning to solve the credit assignment problem](https://openreview.net/forum?id=ByeUBANtvB) |  | 0 |  | Benjamin James Lansdell, Prashanth Ravi Prakash, Konrad Paul Körding |  |
| 233 |  |  [Four Things Everyone Should Know to Improve Batch Normalization](https://openreview.net/forum?id=HJx8HANFDH) |  | 0 |  | Cecilia Summers, Michael J. Dinneen |  |
| 234 |  |  [Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving](https://openreview.net/forum?id=BJedHRVtPB) |  | 0 |  | Yurong You, Yan Wang, WeiLun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark E. Campbell, Kilian Q. Weinberger |  |
| 235 |  |  [SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum](https://openreview.net/forum?id=SkxJ8REYPH) |  | 0 |  | Jianyu Wang, Vinayak Tantia, Nicolas Ballas, Michael G. Rabbat |  |
| 236 |  |  [MetaPix: Few-Shot Video Retargeting](https://openreview.net/forum?id=SJx1URNKwH) |  | 0 |  | Jessica Lee, Deva Ramanan, Rohit Girdhar |  |
| 237 |  |  [Learning to Learn by Zeroth-Order Oracle](https://openreview.net/forum?id=ryxz8CVYDH) |  | 0 |  | Yangjun Ruan, Yuanhao Xiong, Sashank J. Reddi, Sanjiv Kumar, ChoJui Hsieh |  |
| 238 |  |  [DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames](https://openreview.net/forum?id=H1gX8C4YPr) |  | 0 |  | Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra |  |
| 239 |  |  [PAC Confidence Sets for Deep Neural Networks via Calibrated Prediction](https://openreview.net/forum?id=BJxVI04YvB) |  | 0 |  | Sangdon Park, Osbert Bastani, Nikolai Matni, Insup Lee |  |
| 240 |  |  [Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations](https://openreview.net/forum?id=SJgVU0EKwS) |  | 0 |  | Yichi Zhang, Ritchie Zhao, Weizhe Hua, Nayun Xu, G. Edward Suh, Zhiru Zhang |  |
| 241 |  |  [Oblique Decision Trees from Derivatives of ReLU Networks](https://openreview.net/forum?id=Bke8UR4FPB) |  | 0 |  | GuangHe Lee, Tommi S. Jaakkola |  |
| 242 |  |  [Span Recovery for Deep Neural Networks with Applications to Input Obfuscation](https://openreview.net/forum?id=B1guLAVFDB) |  | 0 |  | Rajesh Jayaram, David P. Woodruff, Qiuyi Zhang |  |
| 243 |  |  [Improving Neural Language Generation with Spectrum Control](https://openreview.net/forum?id=ByxY8CNtvr) |  | 0 |  | Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu |  |
| 244 |  |  [Learn to Explain Efficiently via Neural Logic Inductive Learning](https://openreview.net/forum?id=SJlh8CEYDB) |  | 0 |  | Yuan Yang, Le Song |  |
| 245 |  |  [Improved memory in recurrent neural networks with sequential non-normal dynamics](https://openreview.net/forum?id=ryx1wRNFvB) |  | 0 |  | A. Emin Orhan, Xaq Pitkow |  |
| 246 |  |  [Neural Module Networks for Reasoning over Text](https://openreview.net/forum?id=SygWvAVFPr) |  | 0 |  | Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner |  |
| 247 |  |  [Higher-Order Function Networks for Learning Composable 3D Object Representations](https://openreview.net/forum?id=HJgfDREKDB) |  | 0 |  | Eric Mitchell, Selim Engin, Volkan Isler, Daniel D. Lee |  |
| 248 |  |  [Variational Hetero-Encoder Randomized GANs for Joint Image-Text Modeling](https://openreview.net/forum?id=H1x5wRVtvS) |  | 0 |  | Hao Zhang, Bo Chen, Long Tian, Zhengjue Wang, Mingyuan Zhou |  |
| 249 |  |  [Towards Fast Adaptation of Neural Architectures with Meta Learning](https://openreview.net/forum?id=r1eowANFvr) |  | 0 |  | Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, Shenghua Gao |  |
| 250 |  |  [Graph Constrained Reinforcement Learning for Natural Language Action Spaces](https://openreview.net/forum?id=B1x6w0EtwH) |  | 0 |  | Prithviraj Ammanabrolu, Matthew J. Hausknecht |  |
| 251 |  |  [Prediction, Consistency, Curvature: Representation Learning for Locally-Linear Control](https://openreview.net/forum?id=BJxG_0EtDS) |  | 0 |  | Nir Levine, Yinlam Chow, Rui Shu, Ang Li, Mohammad Ghavamzadeh, Hung Bui |  |
| 252 |  |  [Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History](https://openreview.net/forum?id=ryxQuANKPB) |  | 0 |  | Yiheng Zhou, Yulia Tsvetkov, Alan W. Black, Zhou Yu |  |
| 253 |  |  [BERTScore: Evaluating Text Generation with BERT](https://openreview.net/forum?id=SkeHuCVFDr) |  | 0 |  | Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi |  |
| 254 |  |  [Neural Execution of Graph Algorithms](https://openreview.net/forum?id=SkgKO0EtvS) |  | 0 |  | Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, Charles Blundell |  |
| 255 |  |  [On the Need for Topology-Aware Generative Models for Manifold-Based Defenses](https://openreview.net/forum?id=r1lF_CEYwS) |  | 0 |  | Uyeong Jang, Susmit Jha, Somesh Jha |  |
| 256 |  |  [FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary](https://openreview.net/forum?id=S1xtORNFwH) |  | 0 |  | Yingzhen Yang, Jiahui Yu, Nebojsa Jojic, Jun Huan, Thomas S. Huang |  |
| 257 |  |  [Capsules with Inverted Dot-Product Attention Routing](https://openreview.net/forum?id=HJe6uANtwH) |  | 0 |  | YaoHung Hubert Tsai, Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov |  |
| 258 |  |  [Composition-based Multi-Relational Graph Convolutional Networks](https://openreview.net/forum?id=BylA_C4tPr) |  | 0 |  | Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, Partha P. Talukdar |  |
| 259 |  |  [Gradient-Based Neural DAG Learning](https://openreview.net/forum?id=rklbKA4YDS) |  | 0 |  | Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu, Simon LacosteJulien |  |
| 260 |  |  [The Local Elasticity of Neural Networks](https://openreview.net/forum?id=HJxMYANtPH) |  | 0 |  | Hangfeng He, Weijie J. Su |  |
| 261 |  |  [Composing Task-Agnostic Policies with Deep Reinforcement Learning](https://openreview.net/forum?id=H1ezFREtwH) |  | 0 |  | Ahmed Hussain Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, Michael C. Yip |  |
| 262 |  |  [Convergence of Gradient Methods on Bilinear Zero-Sum Games](https://openreview.net/forum?id=SJlVY04FwH) |  | 0 |  | Guojun Zhang, Yaoliang Yu |  |
| 263 |  |  [Discovering Motor Programs by Recomposing Demonstrations](https://openreview.net/forum?id=rkgHY0NYwr) |  | 0 |  | Tanmay Shankar, Shubham Tulsiani, Lerrel Pinto, Abhinav Gupta |  |
| 264 |  |  [Learning from Explanations with Neural Execution Tree](https://openreview.net/forum?id=rJlUt0EYwS) |  | 0 |  | Ziqi Wang, Yujia Qin, Wenxuan Zhou, Jun Yan, Qinyuan Ye, Leonardo Neves, Zhiyuan Liu, Xiang Ren |  |
| 265 |  |  [Jelly Bean World: A Testbed for Never-Ending Learning](https://openreview.net/forum?id=Byx_YAVYPH) |  | 0 |  | Emmanouil Antonios Platanios, Abulhair Saparov, Tom M. Mitchell |  |
| 266 |  |  [Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization](https://openreview.net/forum?id=ryeFY0EFwS) |  | 0 |  | Satrajit Chatterjee |  |
| 267 |  |  [Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks](https://openreview.net/forum?id=HJgCF0VFwr) |  | 0 |  | Xin Xing, Long Sha, Pengyu Hong, Zuofeng Shang, Jun S. Liu |  |
| 268 |  |  [MEMO: A Deep Network for Flexible Combination of Episodic Memories](https://openreview.net/forum?id=rJxlc0EtDr) |  | 0 |  | Andrea Banino, Adrià Puigdomènech Badia, Raphael Köster, Martin J. Chadwick, Vinícius Flores Zambaldi, Demis Hassabis, Caswell Barry, Matthew M. Botvinick, Dharshan Kumaran, Charles Blundell |  |
| 269 |  |  [Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality](https://openreview.net/forum?id=SyxV9ANFDH) |  | 0 |  | Saurabh Khanna, Vincent Y. F. Tan |  |
| 270 |  |  [Bayesian Meta Sampling for Fast Uncertainty Adaptation](https://openreview.net/forum?id=Bkxv90EKPB) |  | 0 |  | Zhenyi Wang, Yang Zhao, Ping Yu, Ruiyi Zhang, Changyou Chen |  |
| 271 |  |  [Non-Autoregressive Dialog State Tracking](https://openreview.net/forum?id=H1e_cC4twS) |  | 0 |  | Hung Le, Richard Socher, Steven C. H. Hoi |  |
| 272 |  |  [Extreme Tensoring for Low-Memory Preconditioning](https://openreview.net/forum?id=SklKcRNYDH) |  | 0 |  | Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, Yi Zhang |  |
| 273 |  |  [RNNs Incrementally Evolving on an Equilibrium Manifold: A Panacea for Vanishing and Exploding Gradients?](https://openreview.net/forum?id=HylpqA4FwS) |  | 0 |  | Anil Kag, Ziming Zhang, Venkatesh Saligrama |  |
| 274 |  |  [The Early Phase of Neural Network Training](https://openreview.net/forum?id=Hkl1iRNFwS) |  | 0 |  | Jonathan Frankle, David J. Schwab, Ari S. Morcos |  |
| 275 |  |  [NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension](https://openreview.net/forum?id=ryxgsCVYPr) |  | 0 |  | Seohyun Back, Sai Chetan Chinthakindi, Akhil Kedia, Haejun Lee, Jaegul Choo |  |
| 276 |  |  [Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization](https://openreview.net/forum?id=SkgGjRVKDS) |  | 0 |  | Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, Jian Sun |  |
| 277 |  |  [Single Episode Policy Transfer in Reinforcement Learning](https://openreview.net/forum?id=rJeQoCNYDS) |  | 0 |  | Jiachen Yang, Brenden K. Petersen, Hongyuan Zha, Daniel M. Faissol |  |
| 278 |  |  [Generalization through Memorization: Nearest Neighbor Language Models](https://openreview.net/forum?id=HklBjCEKvH) |  | 0 |  | Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis |  |
| 279 |  |  [Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention](https://openreview.net/forum?id=r1eIiCNYwS) |  | 0 |  | Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul N. Bennett, Saurabh Tiwary |  |
| 280 |  |  [Synthesizing Programmatic Policies that Inductively Generalize](https://openreview.net/forum?id=S1l8oANFDH) |  | 0 |  | Jeevana Priya Inala, Osbert Bastani, Zenna Tavares, Armando SolarLezama |  |
| 281 |  |  [Decoding As Dynamic Programming For Recurrent Autoregressive Models](https://openreview.net/forum?id=HklOo0VFDH) |  | 0 |  | Najam Zaidi, Trevor Cohn, Gholamreza Haffari |  |
| 282 |  |  [Deep Double Descent: Where Bigger Models and More Data Hurt](https://openreview.net/forum?id=B1g5sA4twr) |  | 0 |  | Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever |  |
| 283 |  |  [Intriguing Properties of Adversarial Training at Scale](https://openreview.net/forum?id=HyxJhCEFDS) |  | 0 |  | Cihang Xie, Alan L. Yuille |  |
| 284 |  |  [Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks](https://openreview.net/forum?id=Bkxe2AVtPS) |  | 0 |  | Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Oguz H. Elibol, Mehran Nekuii, Hanlin Tang |  |
| 285 |  |  [Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication](https://openreview.net/forum?id=SJxZnR4YvB) |  | 0 |  | Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, Liwei Wang |  |
| 286 |  |  [Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks](https://openreview.net/forum?id=r1xGnA4Kvr) |  | 0 |  | Timothy Tadros, Giri P. Krishnan, Ramyaa Ramyaa, Maxim Bazhenov |  |
| 287 |  |  [A Closer Look at the Optimization Landscapes of Generative Adversarial Networks](https://openreview.net/forum?id=HJeVnCEKwH) |  | 0 |  | Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, Simon LacosteJulien |  |
| 288 |  |  [On the Global Convergence of Training Deep Linear ResNets](https://openreview.net/forum?id=HJxEhREKDH) |  | 0 |  | Difan Zou, Philip M. Long, Quanquan Gu |  |
| 289 |  |  [Towards a Deep Network Architecture for Structured Smoothness](https://openreview.net/forum?id=Hklr204Fvr) |  | 0 |  | Haroun Habeeb, Oluwasanmi Koyejo |  |
| 290 |  |  [Revisiting Self-Training for Neural Sequence Generation](https://openreview.net/forum?id=SJgdnAVKDH) |  | 0 |  | Junxian He, Jiatao Gu, Jiajun Shen, Marc'Aurelio Ranzato |  |
| 291 |  |  [Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators](https://openreview.net/forum?id=HJeqhA4YDS) |  | 0 |  | Reinhard Heckel, Mahdi Soltanolkotabi |  |
| 292 |  |  [Variational Autoencoders for Highly Multivariate Spatial Point Processes Intensities](https://openreview.net/forum?id=B1lj20NFDS) |  | 0 |  | Baichuan Yuan, Xiaowei Wang, Jianxin Ma, Chang Zhou, Andrea L. Bertozzi, Hongxia Yang |  |
| 293 |  |  [Model-Augmented Actor-Critic: Backpropagating through Paths](https://openreview.net/forum?id=Skln2A4YDB) |  | 0 |  | Ignasi Clavera, Yao Fu, Pieter Abbeel |  |
| 294 |  |  [LambdaNet: Probabilistic Type Inference using Graph Neural Networks](https://openreview.net/forum?id=Hkx6hANtwH) |  | 0 |  | Jiayi Wei, Maruth Goyal, Greg Durrett, Isil Dillig |  |
| 295 |  |  [From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech](https://openreview.net/forum?id=H1guaREYPr) |  | 0 |  | HyeongSeok Choi, Changdae Park, Kyogu Lee |  |
| 296 |  |  [Learning from Unlabelled Videos Using Contrastive Predictive Neural 3D Mapping](https://openreview.net/forum?id=BJxt60VtPr) |  | 0 |  | Adam W. Harley, Shrinidhi Kowshika Lakshmikanth, Fangyu Li, Xian Zhou, HsiaoYu Fish Tung, Katerina Fragkiadaki |  |
| 297 |  |  [Decoupling Representation and Classifier for Long-Tailed Recognition](https://openreview.net/forum?id=r1gRTCVFvB) |  | 0 |  | Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis |  |
| 298 |  |  [Robust Reinforcement Learning for Continuous Control with Model Misspecification](https://openreview.net/forum?id=HJgC60EtwB) |  | 0 |  | Daniel J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy A. Mann, Martin A. Riedmiller |  |
| 299 |  |  [Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework](https://openreview.net/forum?id=S1l-C0NtwS) |  | 0 |  | Zirui Wang, Jiateng Xie, Ruochen Xu, Yiming Yang, Graham Neubig, Jaime G. Carbonell |  |
| 300 |  |  [Training Recurrent Neural Networks Online by Learning Explicit State Variables](https://openreview.net/forum?id=SJgmR0NKPr) |  | 0 |  | Somjit Nath, Vincent Liu, Alan Chan, Xin Li, Adam White, Martha White |  |
| 301 |  |  [Uncertainty-guided Continual Learning with Bayesian Neural Networks](https://openreview.net/forum?id=HklUCCVKDB) |  | 0 |  | Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, Marcus Rohrbach |  |
| 302 |  |  [Curriculum Loss: Robust Learning and Generalization against Label Corruption](https://openreview.net/forum?id=rkgt0REKwS) |  | 0 |  | Yueming Lyu, Ivor W. Tsang |  |
| 303 |  |  [Picking Winning Tickets Before Training by Preserving Gradient Flow](https://openreview.net/forum?id=SkgsACVKPH) |  | 0 |  | Chaoqi Wang, Guodong Zhang, Roger B. Grosse |  |
| 304 |  |  [Generative Models for Effective ML on Private, Decentralized Datasets](https://openreview.net/forum?id=SJgaRA4FPH) |  | 0 |  | Sean Augenstein, H. Brendan McMahan, Daniel Ramage, Swaroop Ramaswamy, Peter Kairouz, Mingqing Chen, Rajiv Mathews, Blaise Agüera y Arcas |  |
| 305 |  |  [Inductive representation learning on temporal graphs](https://openreview.net/forum?id=rJeW1yHYwH) |  | 0 |  | Da Xu, Chuanwei Ruan, Evren Körpeoglu, Sushant Kumar, Kannan Achan |  |
| 306 |  |  [BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning](https://openreview.net/forum?id=Sklf1yrYDr) |  | 0 |  | Yeming Wen, Dustin Tran, Jimmy Ba |  |
| 307 |  |  [Towards neural networks that provably know when they don't know](https://openreview.net/forum?id=ByxGkySKwH) |  | 0 |  | Alexander Meinke, Matthias Hein |  |
| 308 |  |  [Iterative energy-based projection on a normal data manifold for anomaly localization](https://openreview.net/forum?id=HJx81ySKwr) |  | 0 |  | David Dehaene, Oriel Frigo, Sébastien Combrexelle, Pierre Eline |  |
| 309 |  |  [Towards Stable and Efficient Training of Verifiably Robust Neural Networks](https://openreview.net/forum?id=Skxuk1rFwB) |  | 0 |  | Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane S. Boning, ChoJui Hsieh |  |
| 310 |  |  [Frequency-based Search-control in Dyna](https://openreview.net/forum?id=B1gskyStwr) |  | 0 |  | Yangchen Pan, Jincheng Mei, Amirmassoud Farahmand |  |
| 311 |  |  [Learning representations for binary-classification without backpropagation](https://openreview.net/forum?id=Bke61krFvS) |  | 0 |  | Mathias Lechner |  |
| 312 |  |  [Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks](https://openreview.net/forum?id=HygegyrYwH) |  | 0 |  | Ziwei Ji, Matus Telgarsky |  |
| 313 |  |  [Physics-aware Difference Graph Networks for Sparsely-Observed Dynamics](https://openreview.net/forum?id=r1gelyrtwH) |  | 0 |  | Sungyong Seo, Chuizheng Meng, Yan Liu |  |
| 314 |  |  [HiLLoC: lossless image compression with hierarchical latent variable models](https://openreview.net/forum?id=r1lZgyBYwS) |  | 0 |  | James Townsend, Thomas Bird, Julius Kunze, David Barber |  |
| 315 |  |  [IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks](https://openreview.net/forum?id=BJeGlJStPr) |  | 0 |  | Michael Luo, Jiahao Yao, Richard Liaw, Eric Liang, Ion Stoica |  |
| 316 |  |  [On Bonus Based Exploration Methods In The Arcade Learning Environment](https://openreview.net/forum?id=BJewlyStDr) |  | 0 |  | Adrien Ali Taïga, William Fedus, Marlos C. Machado, Aaron C. Courville, Marc G. Bellemare |  |
| 317 |  |  [Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation](https://openreview.net/forum?id=r1lOgyrKDS) |  | 0 |  | Xinjie Fan, Yizhe Zhang, Zhendong Wang, Mingyuan Zhou |  |
| 318 |  |  [Smoothness and Stability in GANs](https://openreview.net/forum?id=HJeOekHKwr) |  | 0 |  | Casey Chu, Kentaro Minami, Kenji Fukumizu |  |
| 319 |  |  [SNOW: Subscribing to Knowledge via Channel Pooling for Transfer & Lifelong Learning of Convolutional Neural Networks](https://openreview.net/forum?id=rJxtgJBKDr) |  | 0 |  | Chungkuk Yoo, Bumsoo Kang, Minsik Cho |  |
| 320 |  |  [Empirical Studies on the Properties of Linear Regions in Deep Neural Networks](https://openreview.net/forum?id=SkeFl1HKwr) |  | 0 |  | Xiao Zhang, Dongrui Wu |  |
| 321 |  |  [Black-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning](https://openreview.net/forum?id=S1ltg1rFDS) |  | 0 |  | Ali Mousavi, Lihong Li, Qiang Liu, Denny Zhou |  |
| 322 |  |  [PairNorm: Tackling Oversmoothing in GNNs](https://openreview.net/forum?id=rkecl1rtwB) |  | 0 |  | Lingxiao Zhao, Leman Akoglu |  |
| 323 |  |  [Unsupervised Clustering using Pseudo-semi-supervised Learning](https://openreview.net/forum?id=rJlnxkSYPS) |  | 0 |  | Divam Gupta, Ramachandran Ramjee, Nipun Kwatra, Muthian Sivathanu |  |
| 324 |  |  [Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee](https://openreview.net/forum?id=Hke3gyHYwH) |  | 0 |  | Wei Hu, Zhiyuan Li, Dingli Yu |  |
| 325 |  |  [Controlling generative models with continuous factors of variations](https://openreview.net/forum?id=H1laeJrKDB) |  | 0 |  | Antoine Plumerault, Hervé Le Borgne, Céline Hudelot |  |
| 326 |  |  [Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control](https://openreview.net/forum?id=ryxmb1rKDS) |  | 0 |  | Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty |  |
| 327 |  |  [Understanding l4-based Dictionary Learning: Interpretation, Stability, and Robustness](https://openreview.net/forum?id=SJeY-1BKDS) |  | 0 |  | Yuexiang Zhai, Hermish Mehta, Zhengyuan Zhou, Yi Ma |  |
| 328 |  |  [Quantum Algorithms for Deep Convolutional Neural Networks](https://openreview.net/forum?id=Hygab1rKDS) |  | 0 |  | Iordanis Kerenidis, Jonas Landman, Anupam Prakash |  |
| 329 |  |  [Self-Supervised Learning of Appliance Usage](https://openreview.net/forum?id=B1lJzyStvS) |  | 0 |  | ChenYu Hsu, Abbas Zeitoun, GuangHe Lee, Dina Katabi, Tommi S. Jaakkola |  |
| 330 |  |  [Deep Graph Matching Consensus](https://openreview.net/forum?id=HyeJf1HKvS) |  | 0 |  | Matthias Fey, Jan Eric Lenssen, Christopher Morris, Jonathan Masci, Nils M. Kriege |  |
| 331 |  |  [Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks](https://openreview.net/forum?id=rkllGyBFPH) |  | 0 |  | Yu Bai, Jason D. Lee |  |
| 332 |  |  [Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers](https://openreview.net/forum?id=SJlbGJrtDB) |  | 0 |  | Junjie Liu, Zhe Xu, Runbin Shi, Ray C. C. Cheung, Hayden KwokHay So |  |
| 333 |  |  [Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference](https://openreview.net/forum?id=rJgzzJHtDB) |  | 0 |  | TingKuei Hu, Tianlong Chen, Haotao Wang, Zhangyang Wang |  |
| 334 |  |  [Neural Policy Gradient Methods: Global Optimality and Rates of Convergence](https://openreview.net/forum?id=BJgQfkSYDS) |  | 0 |  | Lingxiao Wang, Qi Cai, Zhuoran Yang, Zhaoran Wang |  |
| 335 |  |  [Double Neural Counterfactual Regret Minimization](https://openreview.net/forum?id=ByedzkrKvH) |  | 0 |  | Hui Li, Kailiang Hu, Shaohua Zhang, Yuan Qi, Le Song |  |
| 336 |  |  [GraphAF: a Flow-based Autoregressive Model for Molecular Graph Generation](https://openreview.net/forum?id=S1esMkHYPr) |  | 0 |  | Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, Jian Tang |  |
| 337 |  |  [The Gambler's Problem and Beyond](https://openreview.net/forum?id=HyxnMyBKwB) |  | 0 |  | Baoxiang Wang, Shuai Li, Jiajin Li, Siu On Chan |  |
| 338 |  |  [Multilingual Alignment of Contextual Word Representations](https://openreview.net/forum?id=r1xCMyBtPS) |  | 0 |  | Steven Cao, Nikita Kitaev, Dan Klein |  |
| 339 |  |  [The Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=rygGQyrFvH) |  | 0 |  | Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi |  |
| 340 |  |  [Graph Convolutional Reinforcement Learning](https://openreview.net/forum?id=HkxdQkSYDB) |  | 0 |  | Jiechuan Jiang, Chen Dun, Tiejun Huang, Zongqing Lu |  |
| 341 |  |  [Meta-Learning Deep Energy-Based Memory Models](https://openreview.net/forum?id=SyljQyBFDH) |  | 0 |  | Sergey Bartunov, Jack W. Rae, Simon Osindero, Timothy P. Lillicrap |  |
| 342 |  |  [Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning](https://openreview.net/forum?id=rkl3m1BFDB) |  | 0 |  | Akanksha Atrey, Kaleigh Clary, David D. Jensen |  |
| 343 |  |  [Fast Neural Network Adaptation via Parameter Remapping and Architecture Search](https://openreview.net/forum?id=rklTmyBKPH) |  | 0 |  | Jiemin Fang, Yuzhu Sun, Kangjian Peng, Qian Zhang, Yuan Li, Wenyu Liu, Xinggang Wang |  |
| 344 |  |  [Guiding Program Synthesis by Learning to Generate Examples](https://openreview.net/forum?id=BJl07ySKvS) |  | 0 |  | Larissa Laich, Pavol Bielik, Martin T. Vechev |  |
| 345 |  |  [SNODE: Spectral Discretization of Neural ODEs for System Identification](https://openreview.net/forum?id=Sye0XkBKvS) |  | 0 |  | Alessio Quaglino, Marco Gallieri, Jonathan Masci, Jan Koutník |  |
| 346 |  |  [Generalized Convolutional Forest Networks for Domain Generalization and Visual Recognition](https://openreview.net/forum?id=H1lxVyStPH) |  | 0 |  | Jongbin Ryu, Gitaek Kwon, MingHsuan Yang, Jongwoo Lim |  |
| 347 |  |  [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://openreview.net/forum?id=HylxE1HKwS) |  | 0 |  | Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han |  |
| 348 |  |  [Multi-Agent Interactions Modeling with Correlated Policies](https://openreview.net/forum?id=B1gZV1HYvS) |  | 0 |  | Minghuan Liu, Ming Zhou, Weinan Zhang, Yuzheng Zhuang, Jun Wang, Wulong Liu, Yong Yu |  |
| 349 |  |  [PCMC-Net: Feature-based Pairwise Choice Markov Chains](https://openreview.net/forum?id=BJgWE1SFwS) |  | 0 |  | Alix Lhéritier |  |
| 350 |  |  [Implementing Inductive bias for different navigation tasks through diverse RNN attrractors](https://openreview.net/forum?id=Byx4NkrtDS) |  | 0 |  | Tie Xu, Omri Barak |  |
| 351 |  |  [Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings](https://openreview.net/forum?id=BJgr4kSFDS) |  | 0 |  | Hongyu Ren, Weihua Hu, Jure Leskovec |  |
| 352 |  |  [Rethinking the Hyperparameters for Fine-tuning](https://openreview.net/forum?id=B1g8VkHFPH) |  | 0 |  | Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto |  |
| 353 |  |  [Plug and Play Language Models: A Simple Approach to Controlled Text Generation](https://openreview.net/forum?id=H1edEyBKDS) |  | 0 |  | Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu |  |
| 354 |  |  [Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks](https://openreview.net/forum?id=rkgqN1SYvr) |  | 0 |  | Wei Hu, Lechao Xiao, Jeffrey Pennington |  |
| 355 |  |  [RGBD-GAN: Unsupervised 3D Representation Learning From Natural Image Datasets via RGBD Image Synthesis](https://openreview.net/forum?id=HyxjNyrtPr) |  | 0 |  | Atsuhiro Noguchi, Tatsuya Harada |  |
| 356 |  |  [Towards Verified Robustness under Text Deletion Interventions](https://openreview.net/forum?id=SyxhVkrYvr) |  | 0 |  | Johannes Welbl, PoSen Huang, Robert Stanforth, Sven Gowal, Krishnamurthy (Dj) Dvijotham, Martin Szummer, Pushmeet Kohli |  |
| 357 |  |  [Jacobian Adversarially Regularized Networks for Robustness](https://openreview.net/forum?id=Hke0V1rKPS) |  | 0 |  | Alvin Chan, Yi Tay, YewSoon Ong, Jie Fu |  |
| 358 |  |  [Thinking While Moving: Deep Reinforcement Learning with Concurrent Control](https://openreview.net/forum?id=SJexHkSFPS) |  | 0 |  | Ted Xiao, Eric Jang, Dmitry Kalashnikov, Sergey Levine, Julian Ibarz, Karol Hausman, Alexander Herzog |  |
| 359 |  |  [Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=SJxbHkrKDH) |  | 0 |  | Qian Long, Zihan Zhou, Abhinav Gupta, Fei Fang, Yi Wu, Xiaolong Wang |  |
| 360 |  |  [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/forum?id=r1xMH1BtvB) |  | 0 |  | Kevin Clark, MinhThang Luong, Quoc V. Le, Christopher D. Manning |  |
| 361 |  |  [Environmental drivers of systematicity and generalization in a situated agent](https://openreview.net/forum?id=SklGryBtwr) |  | 0 |  | Felix Hill, Andrew K. Lampinen, Rosalia Schneider, Stephen Clark, Matthew M. Botvinick, James L. McClelland, Adam Santoro |  |
| 362 |  |  [Abstract Diagrammatic Reasoning with Multiplex Graph Networks](https://openreview.net/forum?id=ByxQB1BKwH) |  | 0 |  | Duo Wang, Mateja Jamnik, Pietro Liò |  |
| 363 |  |  [A Baseline for Few-Shot Image Classification](https://openreview.net/forum?id=rylXBkrYDS) |  | 0 |  | Guneet Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, Stefano Soatto |  |
| 364 |  |  [Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering](https://openreview.net/forum?id=SJgVHkrYDH) |  | 0 |  | Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, Caiming Xiong |  |
| 365 |  |  [Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks](https://openreview.net/forum?id=BJlBSkHtDS) |  | 0 |  | Alejandro Molina, Patrick Schramowski, Kristian Kersting |  |
| 366 |  |  [A Framework for robustness Certification of Smoothed Classifiers using F-Divergences](https://openreview.net/forum?id=SJlKrkSFPH) |  | 0 |  | Krishnamurthy (Dj) Dvijotham, Jamie Hayes, Borja Balle, J. Zico Kolter, Chongli Qin, András György, Kai Xiao, Sven Gowal, Pushmeet Kohli |  |
| 367 |  |  [Contrastive Representation Distillation](https://openreview.net/forum?id=SkgpBJrtvS) |  | 0 |  | Yonglong Tian, Dilip Krishnan, Phillip Isola |  |
| 368 |  |  [Certified Defenses for Adversarial Patches](https://openreview.net/forum?id=HyeaSkrYPH) |  | 0 |  | Pingyeh Chiang, Renkun Ni, Ahmed Abdelkader, Chen Zhu, Christoph Studer, Tom Goldstein |  |
| 369 |  |  [Sample Efficient Policy Gradient Methods with Recursive Variance Reduction](https://openreview.net/forum?id=HJlxIJBFDr) |  | 0 |  | Pan Xu, Felicia Gao, Quanquan Gu |  |
| 370 |  |  [Deep Symbolic Superoptimization Without Human Knowledge](https://openreview.net/forum?id=r1egIyBFPS) |  | 0 |  | Hui Shi, Yang Zhang, Xinyun Chen, Yuandong Tian, Jishen Zhao |  |
| 371 |  |  [Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution](https://openreview.net/forum?id=SJgzLkBKPB) |  | 0 |  | Nikaash Puri, Sukriti Verma, Piyush Gupta, Dhruv Kayastha, Shripad V. Deshmukh, Balaji Krishnamurthy, Sameer Singh |  |
| 372 |  |  [Universal Approximation with Certified Networks](https://openreview.net/forum?id=B1gX8kBtPr) |  | 0 |  | Maximilian Baader, Matthew Mirman, Martin T. Vechev |  |
| 373 |  |  [Measuring and Improving the Use of Graph Information in Graph Neural Networks](https://openreview.net/forum?id=rkeIIkHKvS) |  | 0 |  | Yifan Hou, Jie Zhang, James Cheng, Kaili Ma, Richard T. B. Ma, Hongzhi Chen, MingChang Yang |  |
| 374 |  |  [State-only Imitation with Transition Dynamics Mismatch](https://openreview.net/forum?id=HJgLLyrYwB) |  | 0 |  | Tanmay Gangwani, Jian Peng |  |
| 375 |  |  [Adversarial AutoAugment](https://openreview.net/forum?id=ByxdUySKvS) |  | 0 |  | Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong |  |
| 376 |  |  [Meta Dropout: Learning to Perturb Latent Features for Generalization](https://openreview.net/forum?id=BJgd81SYwr) |  | 0 |  | Haebeom Lee, Taewook Nam, Eunho Yang, Sung Ju Hwang |  |
| 377 |  |  [Rényi Fair Inference](https://openreview.net/forum?id=HkgsUJrtDB) |  | 0 |  | Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, Meisam Razaviyayn |  |
| 378 |  |  [Learning transport cost from subset correspondence](https://openreview.net/forum?id=SJlRUkrFPS) |  | 0 |  | Ruishan Liu, Akshay Balsubramani, James Zou |  |
| 379 |  |  [BlockSwap: Fisher-guided Block Substitution for Network Compression on a Budget](https://openreview.net/forum?id=SklkDkSFPB) |  | 0 |  | Jack Turner, Elliot J. Crowley, Michael F. P. O'Boyle, Amos J. Storkey, Gavin Gray |  |
| 380 |  |  [Variance Reduction With Sparse Gradients](https://openreview.net/forum?id=Syx1DkSYwB) |  | 0 |  | Melih Elibol, Lihua Lei, Michael I. Jordan |  |
| 381 |  |  [Abductive Commonsense Reasoning](https://openreview.net/forum?id=Byg1v1HKDB) |  | 0 |  | Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wentau Yih, Yejin Choi |  |
| 382 |  |  [Discrepancy Ratio: Evaluating Model Performance When Even Experts Disagree on the Truth](https://openreview.net/forum?id=Byg-wJSYDS) |  | 0 |  | Igor Lovchinsky, Alon Daks, Israel Malkin, Pouya Samangouei, Ardavan Saeedi, Yang Liu, Swami Sankaranarayanan, Tomer Gafner, Ben Sternlieb, Patrick Maher, Nathan Silberman |  |
| 383 |  |  [Weakly Supervised Disentanglement with Guarantees](https://openreview.net/forum?id=HJgSwyBKvr) |  | 0 |  | Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, Ben Poole |  |
| 384 |  |  [Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks](https://openreview.net/forum?id=SJlHwkBYDH) |  | 0 |  | Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, John E. Hopcroft |  |
| 385 |  |  [Fantastic Generalization Measures and Where to Find Them](https://openreview.net/forum?id=SJgIPJBFvH) |  | 0 |  | Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, Samy Bengio |  |
| 386 |  |  [Robustness Verification for Transformers](https://openreview.net/forum?id=BJxwPJHFwS) |  | 0 |  | Zhouxing Shi, Huan Zhang, KaiWei Chang, Minlie Huang, ChoJui Hsieh |  |
| 387 |  |  [Network Randomization: A Simple Technique for Generalization in Deep Reinforcement Learning](https://openreview.net/forum?id=HJgcvJBFvB) |  | 0 |  | Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee |  |
| 388 |  |  [Tensor Decompositions for Temporal Knowledge Base Completion](https://openreview.net/forum?id=rke2P1BFwS) |  | 0 |  | Timothée Lacroix, Guillaume Obozinski, Nicolas Usunier |  |
| 389 |  |  [On Universal Equivariant Set Networks](https://openreview.net/forum?id=HkxTwkrKDB) |  | 0 |  | Nimrod Segol, Yaron Lipman |  |
| 390 |  |  [Provable robustness against all adversarial $l_p$-perturbations for $p\geq 1$](https://openreview.net/forum?id=rklk_ySYPB) |  | 0 |  | Francesco Croce, Matthias Hein |  |
| 391 |  |  [Don't Use Large Mini-batches, Use Local SGD](https://openreview.net/forum?id=B1eyO1BFPr) |  | 0 |  | Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, Martin Jaggi |  |
| 392 |  |  [Kernel of CycleGAN as a principal homogeneous space](https://openreview.net/forum?id=B1eWOJHKvB) |  | 0 |  | Nikita Moriakov, Jonas Adler, Jonas Teuwen |  |
| 393 |  |  [Distributionally Robust Neural Networks](https://openreview.net/forum?id=ryxGuJrFvS) |  | 0 |  | Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, Percy Liang |  |
| 394 |  |  [On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach](https://openreview.net/forum?id=Hkx7_1rKwS) |  | 0 |  | Yuanhao Wang, Guodong Zhang, Jimmy Ba |  |
| 395 |  |  [A Neural Dirichlet Process Mixture Model for Task-Free Continual Learning](https://openreview.net/forum?id=SJxSOJStPr) |  | 0 |  | Soochan Lee, Junsoo Ha, Dongsu Zhang, Gunhee Kim |  |
| 396 |  |  [Hyper-SAGNN: a self-attention based graph neural network for hypergraphs](https://openreview.net/forum?id=ryeHuJBtPH) |  | 0 |  | Ruochi Zhang, Yuesong Zou, Jian Ma |  |
| 397 |  |  [Neural Epitome Search for Architecture-Agnostic Network Compression](https://openreview.net/forum?id=HyxjOyrKvr) |  | 0 |  | Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang, Jiashi Feng |  |
| 398 |  |  [On the Equivalence between Positional Node Embeddings and Structural Graph Representations](https://openreview.net/forum?id=SJxzFySKwH) |  | 0 |  | Balasubramaniam Srinivasan, Bruno Ribeiro |  |
| 399 |  |  [Probability Calibration for Knowledge Graph Embedding Models](https://openreview.net/forum?id=S1g8K1BFwS) |  | 0 |  | Pedro Tabacof, Luca Costabello |  |
| 400 |  |  [Why Not to Use Zero Imputation? Correcting Sparsity Bias in Training Neural Networks](https://openreview.net/forum?id=BylsKkHYvH) |  | 0 |  | Joonyoung Yi, Juhyuk Lee, Kwang Joon Kim, Sung Ju Hwang, Eunho Yang |  |
| 401 |  |  [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) |  | 0 |  | Yu Rong, Wenbing Huang, Tingyang Xu, Junzhou Huang |  |
| 402 |  |  [Masked Based Unsupervised Content Transfer](https://openreview.net/forum?id=BJe-91BtvH) |  | 0 |  | Ron Mokady, Sagie Benaim, Lior Wolf, Amit Bermano |  |
| 403 |  |  [U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation](https://openreview.net/forum?id=BJlZ5ySKPH) |  | 0 |  | Junho Kim, Minjae Kim, Hyeonwoo Kang, Kwanghee Lee |  |
| 404 |  |  [Inductive and Unsupervised Representation Learning on Graph Structured Objects](https://openreview.net/forum?id=rkem91rtDB) |  | 0 |  | Lichen Wang, Bo Zong, Qianqian Ma, Wei Cheng, Jingchao Ni, Wenchao Yu, Yanchi Liu, Dongjin Song, Haifeng Chen, Yun Fu |  |
| 405 |  |  [Batch-shaping for learning conditional channel gated networks](https://openreview.net/forum?id=Bke89JBtvB) |  | 0 |  | Babak Ehteshami Bejnordi, Tijmen Blankevoort, Max Welling |  |
| 406 |  |  [Learning Robust Representations via Multi-View Information Bottleneck](https://openreview.net/forum?id=B1xwcyHFDr) |  | 0 |  | Marco Federici, Anjan Dutta, Patrick Forré, Nate Kushman, Zeynep Akata |  |
| 407 |  |  [Deep probabilistic subsampling for task-adaptive compressed sensing](https://openreview.net/forum?id=SJeq9JBFvH) |  | 0 |  | Iris A. M. Huijben, Bastiaan S. Veeling, Ruud J. G. van Sloun |  |
| 408 |  |  [Robust anomaly detection and backdoor attack detection via differential privacy](https://openreview.net/forum?id=SJx0q1rtvS) |  | 0 |  | Min Du, Ruoxi Jia, Dawn Song |  |
| 409 |  |  [Learning to Guide Random Search](https://openreview.net/forum?id=B1gHokBKwS) |  | 0 |  | Ozan Sener, Vladlen Koltun |  |
| 410 |  |  [Lagrangian Fluid Simulation with Continuous Convolutions](https://openreview.net/forum?id=B1lDoJSYDH) |  | 0 |  | Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, Vladlen Koltun |  |
| 411 |  |  [Reinforced Genetic Algorithm Learning for Optimizing Computation Graphs](https://openreview.net/forum?id=rkxDoJBYPB) |  | 0 |  | Aditya Paliwal, Felix Gimeno, Vinod Nair, Yujia Li, Miles Lubin, Pushmeet Kohli, Oriol Vinyals |  |
| 412 |  |  [Compressive Transformers for Long-Range Sequence Modelling](https://openreview.net/forum?id=SylKikSYDH) |  | 0 |  | Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, Timothy P. Lillicrap |  |
| 413 |  |  [A Stochastic Derivative Free Optimization Method with Momentum](https://openreview.net/forum?id=HylAoJSKvH) |  | 0 |  | Eduard Gorbunov, Adel Bibi, Ozan Sener, El Houcine Bergou, Peter Richtárik |  |
| 414 |  |  [Understanding and Improving Information Transfer in Multi-Task Learning](https://openreview.net/forum?id=SylzhkBtDB) |  | 0 |  | Sen Wu, Hongyang R. Zhang, Christopher Ré |  |
| 415 |  |  [Learning To Explore Using Active Neural SLAM](https://openreview.net/forum?id=HklXn1BKDH) |  | 0 |  | Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov |  |
| 416 |  |  [EMPIR: Ensembles of Mixed Precision Deep Networks for Increased Robustness Against Adversarial Attacks](https://openreview.net/forum?id=HJem3yHKwH) |  | 0 |  | Sanchari Sen, Balaraman Ravindran, Anand Raghunathan |  |
| 417 |  |  [Quantifying Point-Prediction Uncertainty in Neural Networks via Residual Estimation with an I/O Kernel](https://openreview.net/forum?id=rkxNh1Stvr) |  | 0 |  | Xin Qiu, Elliot Meyerson, Risto Miikkulainen |  |
| 418 |  |  [B-Spline CNNs on Lie groups](https://openreview.net/forum?id=H1gBhkBFDH) |  | 0 |  | Erik J. Bekkers |  |
| 419 |  |  [Neural Outlier Rejection for Self-Supervised Keypoint Learning](https://openreview.net/forum?id=Skx82ySYPH) |  | 0 |  | Jiexiong Tang, Hanme Kim, Vitor Guizilini, Sudeep Pillai, Rares Ambrus |  |
| 420 |  |  [Reducing Transformer Depth on Demand with Structured Dropout](https://openreview.net/forum?id=SylO2yStDr) |  | 0 |  | Angela Fan, Edouard Grave, Armand Joulin |  |
| 421 |  |  [Cross-Lingual Ability of Multilingual BERT: An Empirical Study](https://openreview.net/forum?id=HJeT3yrtDr) |  | 0 |  | Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth |  |
| 422 |  |  [SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition](https://openreview.net/forum?id=rkl03ySYDH) |  | 0 |  | Zhixuan Lin, YiFu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, Sungjin Ahn |  |
| 423 |  |  [RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments](https://openreview.net/forum?id=rkg-TJBFPB) |  | 0 |  | Roberta Raileanu, Tim Rocktäschel |  |
| 424 |  |  [Low-dimensional statistical manifold embedding of directed graphs](https://openreview.net/forum?id=SkxQp1StDH) |  | 0 |  | Thorben Funke, Tian Guo, Alen Lancic, Nino AntulovFantulin |  |
| 425 |  |  [Efficient Probabilistic Logic Reasoning with Graph Neural Networks](https://openreview.net/forum?id=rJg76kStwH) |  | 0 |  | Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, Le Song |  |
| 426 |  |  [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://openreview.net/forum?id=BJe8pkHFwS) |  | 0 |  | Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, Viktor K. Prasanna |  |
| 427 |  |  [You Only Train Once: Loss-Conditional Training of Deep Networks](https://openreview.net/forum?id=HyxY6JHKwr) |  | 0 |  | Alexey Dosovitskiy, Josip Djolonga |  |
| 428 |  |  [Projection-Based Constrained Policy Optimization](https://openreview.net/forum?id=rke3TJrtPS) |  | 0 |  | TsungYen Yang, Justinian Rosca, Karthik Narasimhan, Peter J. Ramadge |  |
| 429 |  |  [Infinite-Horizon Differentiable Model Predictive Control](https://openreview.net/forum?id=ryxC6kSYPr) |  | 0 |  | Sebastian East, Marco Gallieri, Jonathan Masci, Jan Koutník, Mark Cannon |  |
| 430 |  |  [Combining Q-Learning and Search with Amortized Value Estimates](https://openreview.net/forum?id=SkeAaJrKDS) |  | 0 |  | Jessica B. Hamrick, Victor Bapst, Alvaro SanchezGonzalez, Tobias Pfaff, Theophane Weber, Lars Buesing, Peter W. Battaglia |  |
| 431 |  |  [Training Generative Adversarial Networks from Incomplete Observations using Factorised Discriminators](https://openreview.net/forum?id=Hye1RJHKwB) |  | 0 |  | Daniel Stoller, Sebastian Ewert, Simon Dixon |  |
| 432 |  |  [Decentralized Deep Learning with Arbitrary Communication Compression](https://openreview.net/forum?id=SkgGCkrKvH) |  | 0 |  | Anastasia Koloskova, Tao Lin, Sebastian U. Stich, Martin Jaggi |  |
| 433 |  |  [Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control](https://openreview.net/forum?id=SylL0krYPS) |  | 0 |  | TsuiWei Weng, Krishnamurthy (Dj) Dvijotham, Jonathan Uesato, Kai Xiao, Sven Gowal, Robert Stanforth, Pushmeet Kohli |  |
| 434 |  |  [Gradient $\ell_1$ Regularization for Quantization Robustness](https://openreview.net/forum?id=ryxK0JBtPr) |  | 0 |  | Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen Blankevoort, Max Welling |  |
| 435 |  |  [SpikeGrad: An ANN-equivalent Computation Model for Implementing Backpropagation with Spikes](https://openreview.net/forum?id=rkxs0yHFPH) |  | 0 |  | Johannes C. Thiele, Olivier Bichler, Antoine Dupret |  |
| 436 |  |  [On the Relationship between Self-Attention and Convolutional Layers](https://openreview.net/forum?id=HJlnC1rKPB) |  | 0 |  | JeanBaptiste Cordonnier, Andreas Loukas, Martin Jaggi |  |
| 437 |  |  [Learning-Augmented Data Stream Algorithms](https://openreview.net/forum?id=HyxJ1xBYDH) |  | 0 |  | Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, David P. Woodruff |  |
| 438 |  |  [Structured Object-Aware Physics Prediction for Video Modeling and Planning](https://openreview.net/forum?id=B1e-kxSKDH) |  | 0 |  | Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting |  |
| 439 |  |  [Incorporating BERT into Neural Machine Translation](https://openreview.net/forum?id=Hyl7ygStwB) |  | 0 |  | Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, TieYan Liu |  |
| 440 |  |  [MMA Training: Direct Input Space Margin Maximization through Adversarial Training](https://openreview.net/forum?id=HkeryxBtPB) |  | 0 |  | Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, Ruitong Huang |  |
| 441 |  |  [Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies](https://openreview.net/forum?id=rkgU1gHtvr) |  | 0 |  | Xinyun Chen, Lu Wang, Yizhe Hang, Heng Ge, Hongyuan Zha |  |
| 442 |  |  [vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations](https://openreview.net/forum?id=rylwJxrYDS) |  | 0 |  | Alexei Baevski, Steffen Schneider, Michael Auli |  |
| 443 |  |  [Meta-learning curiosity algorithms](https://openreview.net/forum?id=BygdyxHFDS) |  | 0 |  | Ferran Alet, Martin F. Schneider, Tomás LozanoPérez, Leslie Pack Kaelbling |  |
| 444 |  |  [Making Efficient Use of Demonstrations to Solve Hard Exploration Problems](https://openreview.net/forum?id=SygKyeHKDH) |  | 0 |  | Çaglar Gülçehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil C. Rabinowitz, Duncan Williams, Gabriel BarthMaron, Ziyu Wang, Nando de Freitas, Worlds Team |  |
| 445 |  |  [VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning](https://openreview.net/forum?id=Hkl9JlBYvr) |  | 0 |  | Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson |  |
| 446 |  |  [Lookahead: A Far-sighted Alternative of Magnitude-based Pruning](https://openreview.net/forum?id=ryl3ygHYDB) |  | 0 |  | Sejun Park, Jaeho Lee, Sangwoo Mo, Jinwoo Shin |  |
| 447 |  |  [Spike-based causal inference for weight alignment](https://openreview.net/forum?id=rJxWxxSYvB) |  | 0 |  | Jordan Guerguiev, Konrad P. Körding, Blake A. Richards |  |
| 448 |  |  [Empirical Bayes Transductive Meta-Learning with Synthetic Gradients](https://openreview.net/forum?id=Hkg-xgrYvH) |  | 0 |  | Shell Xu Hu, Pablo Garcia Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, Andreas C. Damianou |  |
| 449 |  |  [Keep Doing What Worked: Behavior Modelling Priors for Offline Reinforcement Learning](https://openreview.net/forum?id=rke7geHtwH) |  | 0 |  | Noah Y. Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, Martin A. Riedmiller |  |
| 450 |  |  [Understanding the Limitations of Conditional Generative Models](https://openreview.net/forum?id=r1lPleBFvH) |  | 0 |  | Ethan Fetaya, JörnHenrik Jacobsen, Will Grathwohl, Richard S. Zemel |  |
| 451 |  |  [Demystifying Inter-Class Disentanglement](https://openreview.net/forum?id=Hyl9xxHYPr) |  | 0 |  | Aviv Gabbay, Yedid Hoshen |  |
| 452 |  |  [Mixed-curvature Variational Autoencoders](https://openreview.net/forum?id=S1g6xeSKDS) |  | 0 |  | Ondrej Skopek, OctavianEugen Ganea, Gary Bécigneul |  |
| 453 |  |  [BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations](https://openreview.net/forum?id=r1x0lxrFPS) |  | 0 |  | Hyungjun Kim, Kyungsu Kim, Jinseok Kim, JaeJoon Kim |  |
| 454 |  |  [Model-based reinforcement learning for biological sequence design](https://openreview.net/forum?id=HklxbgBKvr) |  | 0 |  | Christof Angermüller, David Dohan, David Belanger, Ramya Deshpande, Kevin Murphy, Lucy J. Colwell |  |
| 455 |  |  [BayesOpt Adversarial Attack](https://openreview.net/forum?id=Hkem-lrtvH) |  | 0 |  | Binxin Ru, Adam D. Cobb, Arno Blaas, Yarin Gal |  |
| 456 |  |  [Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies](https://openreview.net/forum?id=HkgsWxrtPB) |  | 0 |  | Sungryull Sohn, Hyunjae Woo, Jongwook Choi, Honglak Lee |  |
| 457 |  |  [Hypermodels for Exploration](https://openreview.net/forum?id=ryx6WgStPB) |  | 0 |  | Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, Benjamin Van Roy |  |
| 458 |  |  [RaPP: Novelty Detection with Reconstruction along Projection Pathway](https://openreview.net/forum?id=HkgeGeBYDB) |  | 0 |  | Ki Hyun Kim, Sangwoo Shim, Yongsub Lim, Jongseob Jeon, Jeongwoo Choi, Byungchan Kim, Andre S. Yoon |  |
| 459 |  |  [Dynamics-Aware Embeddings](https://openreview.net/forum?id=BJgZGeHFPH) |  | 0 |  | William F. Whitney, Rajat Agarwal, Kyunghyun Cho, Abhinav Gupta |  |
| 460 |  |  [Functional Regularisation for Continual Learning with Gaussian Processes](https://openreview.net/forum?id=HkxCzeHFDB) |  | 0 |  | Michalis K. Titsias, Jonathan Schwarz, Alexander G. de G. Matthews, Razvan Pascanu, Yee Whye Teh |  |
| 461 |  |  [You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings](https://openreview.net/forum?id=BkxSmlBFvr) |  | 0 |  | Daniel Ruffinelli, Samuel Broscheit, Rainer Gemulla |  |
| 462 |  |  [AdvectiveNet: An Eulerian-Lagrangian Fluidic Reservoir for Point Cloud Processing](https://openreview.net/forum?id=H1eqQeHFDS) |  | 0 |  | Xingzhe He, Helen Lu Cao, Bo Zhu |  |
| 463 |  |  [Never Give Up: Learning Directed Exploration Strategies](https://openreview.net/forum?id=Sye57xStvB) |  | 0 |  | Adrià Puigdomènech Badia, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martín Arjovsky, Alexander Pritzel, Andrew Bolt, Charles Blundell |  |
| 464 |  |  [Fair Resource Allocation in Federated Learning](https://openreview.net/forum?id=ByexElSYDr) |  | 0 |  | Tian Li, Maziar Sanjabi, Ahmad Beirami, Virginia Smith |  |
| 465 |  |  [Smooth markets: A basic mechanism for organizing gradient-based learners](https://openreview.net/forum?id=B1xMEerYvB) |  | 0 |  | David Balduzzi, Wojciech M. Czarnecki, Tom Anthony, Ian Gemp, Edward Hughes, Joel Z. Leibo, Georgios Piliouras, Thore Graepel |  |
| 466 |  |  [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://openreview.net/forum?id=BJgQ4lSFPH) |  | 0 |  | Wei Wang, Bin Bi, Ming Yan, Chen Wu, Jiangnan Xia, Zuyi Bao, Liwei Peng, Luo Si |  |
| 467 |  |  [Training binary neural networks with real-to-binary convolutions](https://openreview.net/forum?id=BJg4NgBKvH) |  | 0 |  | Brais Martínez, Jing Yang, Adrian Bulat, Georgios Tzimiropoulos |  |
| 468 |  |  [Permutation Equivariant Models for Compositional Generalization in Language](https://openreview.net/forum?id=SylVNerFvr) |  | 0 |  | Jonathan Gordon, David LopezPaz, Marco Baroni, Diane Bouchacourt |  |
| 469 |  |  [Continual learning with hypernetworks](https://openreview.net/forum?id=SJgwNerKvB) |  | 0 |  | Johannes von Oswald, Christian Henning, João Sacramento, Benjamin F. Grewe |  |
| 470 |  |  [Phase Transitions for the Information Bottleneck in Representation Learning](https://openreview.net/forum?id=HJloElBYvB) |  | 0 |  | Tailin Wu, Ian S. Fischer |  |
| 471 |  |  [Variational Template Machine for Data-to-Text Generation](https://openreview.net/forum?id=HkejNgBtPB) |  | 0 |  | Rong Ye, Wenxian Shi, Hao Zhou, Zhongyu Wei, Lei Li |  |
| 472 |  |  [Memory-Based Graph Networks](https://openreview.net/forum?id=r1laNeBYPB) |  | 0 |  | Amir Hosein Khas Ahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee, Quaid Morris |  |
| 473 |  |  [AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty](https://openreview.net/forum?id=S1gmrxHFvB) |  | 0 |  | Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, Balaji Lakshminarayanan |  |
| 474 |  |  [AtomNAS: Fine-Grained End-to-End Neural Architecture Search](https://openreview.net/forum?id=BylQSxHFwr) |  | 0 |  | Jieru Mei, Yingwei Li, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan L. Yuille, Jianchao Yang |  |
| 475 |  |  [Residual Energy-Based Models for Text Generation](https://openreview.net/forum?id=B1l4SgHKDH) |  | 0 |  | Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, Marc'Aurelio Ranzato |  |
| 476 |  |  [A closer look at the approximation capabilities of neural networks](https://openreview.net/forum?id=rkevSgrtPr) |  | 0 |  | Kai Fong Ernest Chong |  |
| 477 |  |  [Deep Audio Priors Emerge From Harmonic Convolutional Networks](https://openreview.net/forum?id=rygjHxrYDB) |  | 0 |  | Zhoutong Zhang, Yunyun Wang, Chuang Gan, Jiajun Wu, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman |  |
| 478 |  |  [Expected Information Maximization: Using the I-Projection for Mixture Density Estimation](https://openreview.net/forum?id=ByglLlHFDS) |  | 0 |  | Philipp Becker, Oleg Arenz, Gerhard Neumann |  |
| 479 |  |  [A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms](https://openreview.net/forum?id=ryxWIgBFPS) |  | 0 |  | Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, Christopher J. Pal |  |
| 480 |  |  [On the interaction between supervision and self-play in emergent communication](https://openreview.net/forum?id=rJxGLlBtwH) |  | 0 |  | Ryan Lowe, Abhinav Gupta, Jakob N. Foerster, Douwe Kiela, Joelle Pineau |  |
| 481 |  |  [Dynamic Model Pruning with Feedback](https://openreview.net/forum?id=SJem8lSFwB) |  | 0 |  | Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, Martin Jaggi |  |
| 482 |  |  [Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings](https://openreview.net/forum?id=SJxE8erKDH) |  | 0 |  | Shweta Mahajan, Iryna Gurevych, Stefan Roth |  |
| 483 |  |  [Transferring Optimality Across Data Distributions via Homotopy Methods](https://openreview.net/forum?id=S1gEIerYwH) |  | 0 |  | Matilde Gargiani, Andrea Zanelli, Quoc TranDinh, Moritz Diehl, Frank Hutter |  |
| 484 |  |  [Regularizing activations in neural networks via distribution matching with the Wasserstein metric](https://openreview.net/forum?id=rygwLgrYPB) |  | 0 |  | Taejong Joo, Donggu Kang, Byunghoon Kim |  |
| 485 |  |  [Mutual Information Gradient Estimation for Representation Learning](https://openreview.net/forum?id=ByxaUgrFvH) |  | 0 |  | Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, Zenglin Xu |  |
| 486 |  |  [Lite Transformer with Long-Short Range Attention](https://openreview.net/forum?id=ByeMPlHKPH) |  | 0 |  | Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, Song Han |  |
| 487 |  |  [A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case](https://openreview.net/forum?id=H1lNPxHKDH) |  | 0 |  | Greg Ongie, Rebecca Willett, Daniel Soudry, Nathan Srebro |  |
| 488 |  |  [Adversarial Lipschitz Regularization](https://openreview.net/forum?id=Bke_DertPB) |  | 0 |  | Dávid Terjék |  |
| 489 |  |  [Compositional Language Continual Learning](https://openreview.net/forum?id=rklnDgHtDS) |  | 0 |  | Yuanpeng Li, Liang Zhao, Kenneth Church, Mohamed Elhoseiny |  |
| 490 |  |  [End to End Trainable Active Contours via Differentiable Rendering](https://openreview.net/forum?id=rkxawlHKDr) |  | 0 |  | Shir Gur, Tal Shaharabany, Lior Wolf |  |
| 491 |  |  [Provable Filter Pruning for Efficient Neural Networks](https://openreview.net/forum?id=BJxkOlSYDH) |  | 0 |  | Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, Daniela Rus |  |
| 492 |  |  [Effect of Activation Functions on the Training of Overparametrized Neural Nets](https://openreview.net/forum?id=rkgfdeBYvH) |  | 0 |  | Abhishek Panigrahi, Abhishek Shetty, Navin Goyal |  |
| 493 |  |  [Lipschitz constant estimation of Neural Networks via sparse polynomial optimization](https://openreview.net/forum?id=rJe4_xSFDB) |  | 0 |  | Fabian Latorre, Paul Rolland, Volkan Cevher |  |
| 494 |  |  [State Alignment-based Imitation Learning](https://openreview.net/forum?id=rylrdxHFDr) |  | 0 |  | Fangchen Liu, Zhan Ling, Tongzhou Mu, Hao Su |  |
| 495 |  |  [Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories](https://openreview.net/forum?id=rkl8dlHYvB) |  | 0 |  | Tiange Luo, Kaichun Mo, Zhiao Huang, Jiarui Xu, Siyu Hu, Liwei Wang, Hao Su |  |
| 496 |  |  [Discriminative Particle Filter Reinforcement Learning for Complex Partial observations](https://openreview.net/forum?id=HJl8_eHYvS) |  | 0 |  | Xiao Ma, Péter Karkus, David Hsu, Wee Sun Lee, Nan Ye |  |
| 497 |  |  [Unrestricted Adversarial Examples via Semantic Manipulation](https://openreview.net/forum?id=Sye_OgHFwH) |  | 0 |  | Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, David A. Forsyth |  |
| 498 |  |  [Classification-Based Anomaly Detection for General Data](https://openreview.net/forum?id=H1lK_lBtvS) |  | 0 |  | Liron Bergman, Yedid Hoshen |  |
| 499 |  |  [Scale-Equivariant Steerable Networks](https://openreview.net/forum?id=HJgpugrKPS) |  | 0 |  | Ivan Sosnovik, Michal Szmaja, Arnold W. M. Smeulders |  |
| 500 |  |  [On Generalization Error Bounds of Noisy Gradient Methods for Non-Convex Learning](https://openreview.net/forum?id=SkxxtgHKPS) |  | 0 |  | Jian Li, Xuanyuan Luo, Mingda Qiao |  |
| 501 |  |  [Consistency Regularization for Generative Adversarial Networks](https://openreview.net/forum?id=S1lxKlSKPH) |  | 0 |  | Han Zhang, Zizhao Zhang, Augustus Odena, Honglak Lee |  |
| 502 |  |  [Differentiable learning of numerical rules in knowledge graphs](https://openreview.net/forum?id=rJleKgrKwS) |  | 0 |  | PoWei Wang, Daria Stepanova, Csaba Domokos, J. Zico Kolter |  |
| 503 |  |  [Learning to Move with Affordance Maps](https://openreview.net/forum?id=BJgMFxrYPB) |  | 0 |  | William Qi, Ravi Teja Mullapudi, Saurabh Gupta, Deva Ramanan |  |
| 504 |  |  [Neural tangent kernels, transportation mappings, and universal approximation](https://openreview.net/forum?id=HklQYxBKwS) |  | 0 |  | Ziwei Ji, Matus Telgarsky, Ruicheng Xian |  |
| 505 |  |  [SCALOR: Generative World Models with Scalable Object Representations](https://openreview.net/forum?id=SJxrKgStDH) |  | 0 |  | Jindong Jiang, Sepehr Janghorbani, Gerard de Melo, Sungjin Ahn |  |
| 506 |  |  [Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks](https://openreview.net/forum?id=SyevYxHtDB) |  | 0 |  | Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz |  |
| 507 |  |  [Domain Adaptive Multibranch Networks](https://openreview.net/forum?id=rJxycxHKDS) |  | 0 |  | Róger BermúdezChacón, Mathieu Salzmann, Pascal Fua |  |
| 508 |  |  [DiffTaichi: Differentiable Programming for Physical Simulation](https://openreview.net/forum?id=B1eB5xSFvr) |  | 0 |  | Yuanming Hu, Luke Anderson, TzuMao Li, Qi Sun, Nathan Carr, Jonathan RaganKelley, Frédo Durand |  |
| 509 |  |  [Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning](https://openreview.net/forum?id=BJxI5gHKDr) |  | 0 |  | Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, Dmitry P. Vetrov |  |
| 510 |  |  [Episodic Reinforcement Learning with Associative Memory](https://openreview.net/forum?id=HkxjqxBYDB) |  | 0 |  | Guangxiang Zhu, Zichuan Lin, Guangwen Yang, Chongjie Zhang |  |
| 511 |  |  [Sub-policy Adaptation for Hierarchical Reinforcement Learning](https://openreview.net/forum?id=ByeWogStDS) |  | 0 |  | Alexander C. Li, Carlos Florensa, Ignasi Clavera, Pieter Abbeel |  |
| 512 |  |  [Critical initialisation in continuous approximations of binary neural networks](https://openreview.net/forum?id=rylmoxrFDH) |  | 0 |  | George Stamatescu, Federica Gerace, Carlo Lucibello, Ian G. Fuss, Langford B. White |  |
| 513 |  |  [Deep Orientation Uncertainty Learning based on a Bingham Loss](https://openreview.net/forum?id=ryloogSKDS) |  | 0 |  | Igor Gilitschenski, Roshni Sahoo, Wilko Schwarting, Alexander Amini, Sertac Karaman, Daniela Rus |  |
| 514 |  |  [Co-Attentive Equivariant Neural Networks: Focusing Equivariance On Transformations Co-Occurring in Data](https://openreview.net/forum?id=r1g6ogrtDr) |  | 0 |  | David W. Romero, Mark Hoogendoorn |  |
| 515 |  |  [Mixed Precision DNNs: All you need is a good parametrization](https://openreview.net/forum?id=Hyx0slrFvH) |  | 0 |  | Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso García, Stephen Tiedemann, Thomas Kemp, Akira Nakamura |  |
| 516 |  |  [Information Geometry of Orthogonal Initializations and Training](https://openreview.net/forum?id=rkg1ngrFPr) |  | 0 |  | Piotr Aleksander Sokól, Il Memming Park |  |
| 517 |  |  [Extreme Classification via Adversarial Softmax Approximation](https://openreview.net/forum?id=rJxe3xSYDS) |  | 0 |  | Robert Bamler, Stephan Mandt |  |
| 518 |  |  [Learning Nearly Decomposable Value Functions Via Communication Minimization](https://openreview.net/forum?id=HJx-3grYDB) |  | 0 |  | Tonghan Wang, Jianhao Wang, Chongyi Zheng, Chongjie Zhang |  |
| 519 |  |  [Robust Subspace Recovery Layer for Unsupervised Anomaly Detection](https://openreview.net/forum?id=rylb3eBtwr) |  | 0 |  | ChiehHsin Lai, Dongmian Zou, Gilad Lerman |  |
| 520 |  |  [Learning to Coordinate Manipulation Skills via Skill Behavior Diversification](https://openreview.net/forum?id=ryxB2lBtvH) |  | 0 |  | Youngwoon Lee, Jingyun Yang, Joseph J. Lim |  |
| 521 |  |  [NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search](https://openreview.net/forum?id=SJx9ngStPH) |  | 0 |  | Arber Zela, Julien Siems, Frank Hutter |  |
| 522 |  |  [Conservative Uncertainty Estimation By Fitting Prior Networks](https://openreview.net/forum?id=BJlahxHYDS) |  | 0 |  | Kamil Ciosek, Vincent Fortuin, Ryota Tomioka, Katja Hofmann, Richard E. Turner |  |
| 523 |  |  [Understanding Generalization in Recurrent Neural Networks](https://openreview.net/forum?id=rkgg6xBYDH) |  | 0 |  | Zhuozhuo Tu, Fengxiang He, Dacheng Tao |  |
| 524 |  |  [The Shape of Data: Intrinsic Distance for Data Distributions](https://openreview.net/forum?id=HyebplHYwB) |  | 0 |  | Anton Tsitsulin, Marina Munkhoeva, Davide Mottin, Panagiotis Karras, Alexander M. Bronstein, Ivan V. Oseledets, Emmanuel Müller |  |
| 525 |  |  [How to 0wn the NAS in Your Spare Time](https://openreview.net/forum?id=S1erpeBFPB) |  | 0 |  | Sanghyun Hong, Michael Davinroy, Yigitcan Kaya, Dana DachmanSoled, Tudor Dumitras |  |
| 526 |  |  [Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation](https://openreview.net/forum?id=B1xSperKvH) |  | 0 |  | Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, Kaushik Roy |  |
| 527 |  |  [Breaking Certified Defenses: Semantic Adversarial Examples with Spoofed robustness Certificates](https://openreview.net/forum?id=HJxdTxHYvB) |  | 0 |  | Amin Ghiasi, Ali Shafahi, Tom Goldstein |  |
| 528 |  |  [Query-efficient Meta Attack to Deep Neural Networks](https://openreview.net/forum?id=Skxd6gSYDS) |  | 0 |  | Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, Jiashi Feng |  |
| 529 |  |  [Massively Multilingual Sparse Word Representations](https://openreview.net/forum?id=HyeYTgrFPB) |  | 0 |  | Gábor Berend |  |
| 530 |  |  [Monotonic Multihead Attention](https://openreview.net/forum?id=Hyg96gBKPS) |  | 0 |  | Xutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon, Jiatao Gu |  |
| 531 |  |  [Gradients as Features for Deep Representation Learning](https://openreview.net/forum?id=BkeoaeHKDS) |  | 0 |  | Fangzhou Mu, Yingyu Liang, Yin Li |  |
| 532 |  |  [Pay Attention to Features, Transfer Learn Faster CNNs](https://openreview.net/forum?id=ryxyCeHtPB) |  | 0 |  | Kafeng Wang, Xitong Gao, Yiren Zhao, Xingjian Li, Dejing Dou, ChengZhong Xu |  |
| 533 |  |  [Program Guided Agent](https://openreview.net/forum?id=BkxUvnEYDH) |  | 0 |  | ShaoHua Sun, TeLin Wu, Joseph J. Lim |  |
| 534 |  |  [Sparse Coding with Gated Learned ISTA](https://openreview.net/forum?id=BygPO2VKPH) |  | 0 |  | Kailun Wu, Yiwen Guo, Ziang Li, Changshui Zhang |  |
| 535 |  |  [Graph Neural Networks Exponentially Lose Expressive Power for Node Classification](https://openreview.net/forum?id=S1ldO2EFPr) |  | 0 |  | Kenta Oono, Taiji Suzuki |  |
| 536 |  |  [Multi-Scale Representation Learning for Spatial Feature Distributions using Grid Cells](https://openreview.net/forum?id=rJljdh4KDH) |  | 0 |  | Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, Ni Lao |  |
| 537 |  |  [InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization](https://openreview.net/forum?id=r1lfF2NYvH) |  | 0 |  | FanYun Sun, Jordan Hoffmann, Vikas Verma, Jian Tang |  |
| 538 |  |  [On Robustness of Neural Ordinary Differential Equations](https://openreview.net/forum?id=B1e9Y2NYvS) |  | 0 |  | Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, Jiashi Feng |  |
| 539 |  |  [Defending Against Physically Realizable Attacks on Image Classification](https://openreview.net/forum?id=H1xscnEKDr) |  | 0 |  | Tong Wu, Liang Tong, Yevgeniy Vorobeychik |  |
| 540 |  |  [Estimating Gradients for Discrete Random Variables by Sampling without Replacement](https://openreview.net/forum?id=rklEj2EFvB) |  | 0 |  | Wouter Kool, Herke van Hoof, Max Welling |  |
| 541 |  |  [Learning to Control PDEs with Differentiable Physics](https://openreview.net/forum?id=HyeSin4FPB) |  | 0 |  | Philipp Holl, Nils Thuerey, Vladlen Koltun |  |
| 542 |  |  [Intensity-Free Learning of Temporal Point Processes](https://openreview.net/forum?id=HygOjhEYDH) |  | 0 |  | Oleksandr Shchur, Marin Bilos, Stephan Günnemann |  |
| 543 |  |  [A Signal Propagation Perspective for Pruning Neural Networks at Initialization](https://openreview.net/forum?id=HJeTo2VFwH) |  | 0 |  | Namhoon Lee, Thalaiyasingam Ajanthan, Stephen Gould, Philip H. S. Torr |  |
| 544 |  |  [Skip Connections Matter: On the Transferability of Adversarial Examples Generated with ResNets](https://openreview.net/forum?id=BJlRs34Fvr) |  | 0 |  | Dongxian Wu, Yisen Wang, ShuTao Xia, James Bailey, Xingjun Ma |  |
| 545 |  |  [White Noise Analysis of Neural Networks](https://openreview.net/forum?id=H1ebhnEYDH) |  | 0 |  | Ali Borji, Sikun Lin |  |
| 546 |  |  [Neural Machine Translation with Universal Visual Representation](https://openreview.net/forum?id=Byl8hhNYPS) |  | 0 |  | Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao |  |
| 547 |  |  [Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds](https://openreview.net/forum?id=BJeKh3VYDH) |  | 0 |  | Lukas Prantl, Nuttapong Chentanez, Stefan Jeschke, Nils Thuerey |  |
| 548 |  |  [PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search](https://openreview.net/forum?id=BJlS634tPr) |  | 0 |  | Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, GuoJun Qi, Qi Tian, Hongkai Xiong |  |
| 549 |  |  [Online and stochastic optimization beyond Lipschitz continuity: A Riemannian approach](https://openreview.net/forum?id=rkxZyaNtwB) |  | 0 |  | Kimon Antonakopoulos, Elena Veronica Belmega, Panayotis Mertikopoulos |  |
| 550 |  |  [Enhancing Adversarial Defense by k-Winners-Take-All](https://openreview.net/forum?id=Skgvy64tvr) |  | 0 |  | Chang Xiao, Peilin Zhong, Changxi Zheng |  |
| 551 |  |  [Encoding word order in complex embeddings](https://openreview.net/forum?id=Hke-WTVtwr) |  | 0 |  | Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, Jakob Grue Simonsen |  |
| 552 |  |  [DDSP: Differentiable Digital Signal Processing](https://openreview.net/forum?id=B1x1ma4tDr) |  | 0 |  | Jesse H. Engel, Lamtharn Hantrakul, Chenjie Gu, Adam Roberts |  |
| 553 |  |  [Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation](https://openreview.net/forum?id=SJl5Np4tPr) |  | 0 |  | HungYu Tseng, HsinYing Lee, JiaBin Huang, MingHsuan Yang |  |
| 554 |  |  [Ridge Regression: Structure, Cross-Validation, and Sketching](https://openreview.net/forum?id=HklRwaEKwB) |  | 0 |  | Sifan Liu, Edgar Dobriban |  |
| 555 |  |  [Finite Depth and Width Corrections to the Neural Tangent Kernel](https://openreview.net/forum?id=SJgndT4KwB) |  | 0 |  | Boris Hanin, Mihai Nica |  |
| 556 |  |  [Meta-Learning without Memorization](https://openreview.net/forum?id=BklEFpEYwS) |  | 0 |  | Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, Chelsea Finn |  |
| 557 |  |  [Influence-Based Multi-Agent Exploration](https://openreview.net/forum?id=BJgy96EYvr) |  | 0 |  | Tonghan Wang, Jianhao Wang, Yi Wu, Chongjie Zhang |  |
| 558 |  |  [Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs](https://openreview.net/forum?id=SJeqs6EFvB) |  | 0 |  | Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, Ke Wang |  |
| 559 |  |  [Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations](https://openreview.net/forum?id=BJge3TNKwH) |  | 0 |  | Soheil Kolouri, Nicholas A. Ketz, Andrea Soltoggio, Praveen K. Pilly |  |
| 560 |  |  [How much Position Information Do Convolutional Neural Networks Encode?](https://openreview.net/forum?id=rJeB36NKvB) |  | 0 |  | Md. Amirul Islam, Sen Jia, Neil D. B. Bruce |  |
| 561 |  |  [Hamiltonian Generative Networks](https://openreview.net/forum?id=HJenn6VFvB) |  | 0 |  | Peter Toth, Danilo J. Rezende, Andrew Jaegle, Sébastien Racanière, Aleksandar Botev, Irina Higgins |  |
| 562 |  |  [CoPhy: Counterfactual Learning of Physical Dynamics](https://openreview.net/forum?id=SkeyppEFvS) |  | 0 |  | Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, Christian Wolf |  |
| 563 |  |  [Estimating counterfactual treatment outcomes over time through adversarially balanced representations](https://openreview.net/forum?id=BJg866NFvB) |  | 0 |  | Ioana Bica, Ahmed M. Alaa, James Jordon, Mihaela van der Schaar |  |
| 564 |  |  [Gradientless Descent: High-Dimensional Zeroth-Order Optimization](https://openreview.net/forum?id=Skep6TVYDB) |  | 0 |  | Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, Qiuyi (Richard) Zhang |  |
| 565 |  |  [Conditional Learning of Fair Representations](https://openreview.net/forum?id=Hkekl0NFPr) |  | 0 |  | Han Zhao, Amanda Coston, Tameem Adel, Geoffrey J. Gordon |  |
| 566 |  |  [Inductive Matrix Completion Based on Graph Neural Networks](https://openreview.net/forum?id=ByxxgCEYDS) |  | 0 |  | Muhan Zhang, Yixin Chen |  |
| 567 |  |  [Duration-of-Stay Storage Assignment under Uncertainty](https://openreview.net/forum?id=Hkx7xRVYDr) |  | 0 |  | Michael Lingzhi Li, Elliott Wolf, Daniel Wintz |  |
| 568 |  |  [Emergence of functional and structural properties of the head direction system by optimization of recurrent neural networks](https://openreview.net/forum?id=HklSeREtPB) |  | 0 |  | Christopher J. Cueva, Peter Y. Wang, Matthew Chin, XueXin Wei |  |
| 569 |  |  [Deep neuroethology of a virtual rodent](https://openreview.net/forum?id=SyxrxR4KPS) |  | 0 |  | Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Olveczky |  |
| 570 |  |  [Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation](https://openreview.net/forum?id=S1glGANtDr) |  | 0 |  | Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, Qiang Liu |  |
| 571 |  |  [Learning Compositional Koopman Operators for Model-Based Control](https://openreview.net/forum?id=H1ldzA4tPr) |  | 0 |  | Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, Antonio Torralba |  |
| 572 |  |  [CLEVRER: Collision Events for Video Representation and Reasoning](https://openreview.net/forum?id=HkxYzANYDB) |  | 0 |  | Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum |  |
| 573 |  |  [The Logical Expressiveness of Graph Neural Networks](https://openreview.net/forum?id=r1lZ7AEKvB) |  | 0 |  | Pablo Barceló, Egor V. Kostylev, Mikaël Monet, Jorge Pérez, Juan L. Reutter, Juan Pablo Silva |  |
| 574 |  |  [The Break-Even Point on Optimization Trajectories of Deep Neural Networks](https://openreview.net/forum?id=r1g87C4KwB) |  | 0 |  | Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, Krzysztof J. Geras |  |
| 575 |  |  [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://openreview.net/forum?id=H1eA7AEtvS) |  | 0 |  | Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut |  |
| 576 |  |  [Disentangling neural mechanisms for perceptual grouping](https://openreview.net/forum?id=HJxrVA4FDS) |  | 0 |  | Junkyung Kim, Drew Linsley, Kalpit Thakkar, Thomas Serre |  |
| 577 |  |  [Learning to Plan in High Dimensions via Neural Exploration-Exploitation Trees](https://openreview.net/forum?id=rJgJDAVKvB) |  | 0 |  | Binghong Chen, Bo Dai, Qinjie Lin, Guo Ye, Han Liu, Le Song |  |
| 578 |  |  [Symplectic Recurrent Neural Networks](https://openreview.net/forum?id=BkgYPREtPr) |  | 0 |  | Zhengdao Chen, Jianyu Zhang, Martín Arjovsky, Léon Bottou |  |
| 579 |  |  [Asymptotics of Wide Networks from Feynman Diagrams](https://openreview.net/forum?id=S1gFvANKDS) |  | 0 |  | Ethan Dyer, Guy GurAri |  |
| 580 |  |  [Learning The Difference That Makes A Difference With Counterfactually-Augmented Data](https://openreview.net/forum?id=Sklgs0NFvr) |  | 0 |  | Divyansh Kaushik, Eduard H. Hovy, Zachary Chase Lipton |  |
| 581 |  |  [Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?](https://openreview.net/forum?id=r1genAVKPB) |  | 0 |  | Simon S. Du, Sham M. Kakade, Ruosong Wang, Lin F. Yang |  |
| 582 |  |  [Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning](https://openreview.net/forum?id=B1xm3RVtwB) |  | 0 |  | Hengyuan Hu, Jakob N. Foerster |  |
| 583 |  |  [Network Deconvolution](https://openreview.net/forum?id=rkeu30EtvS) |  | 0 |  | Chengxi Ye, Matthew Evanusa, Hua He, Anton Mitrokhin, Tom Goldstein, James A. Yorke, Cornelia Fermüller, Yiannis Aloimonos |  |
| 584 |  |  [Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension](https://openreview.net/forum?id=ryxjnREFwH) |  | 0 |  | Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, Quoc V. Le |  |
| 585 |  |  [Real or Not Real, that is the Question](https://openreview.net/forum?id=B1lPaCNtPB) |  | 0 |  | Yuanbo Xiangli, Yubin Deng, Bo Dai, Chen Change Loy, Dahua Lin |  |
| 586 |  |  [Dream to Control: Learning Behaviors by Latent Imagination](https://openreview.net/forum?id=S1lOTC4tDS) |  | 0 |  | Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, Mohammad Norouzi |  |
| 587 |  |  [A Probabilistic Formulation of Unsupervised Text Style Transfer](https://openreview.net/forum?id=HJlA0C4tPS) |  | 0 |  | Junxian He, Xinyi Wang, Graham Neubig, Taylor BergKirkpatrick |  |
| 588 |  |  [Emergent Tool Use From Multi-Agent Autocurricula](https://openreview.net/forum?id=SkxpxJBKwS) |  | 0 |  | Bowen Baker, Ingmar Kanitscheider, Todor M. Markov, Yi Wu, Glenn Powell, Bob McGrew, Igor Mordatch |  |
| 589 |  |  [NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search](https://openreview.net/forum?id=HJxyZkBKDr) |  | 0 |  | Xuanyi Dong, Yi Yang |  |
| 590 |  |  [Strategies for Pre-training Graph Neural Networks](https://openreview.net/forum?id=HJlWWJSFDH) |  | 0 |  | Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, Jure Leskovec |  |
| 591 |  |  [Behaviour Suite for Reinforcement Learning](https://openreview.net/forum?id=rygf-kSYwH) |  | 0 |  | Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvári, Satinder Singh, Benjamin Van Roy, Richard S. Sutton, David Silver, Hado van Hasselt |  |
| 592 |  |  [FreeLB: Enhanced Adversarial Training for Natural Language Understanding](https://openreview.net/forum?id=BygzbyHFvB) |  | 0 |  | Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, Jingjing Liu |  |
| 593 |  |  [Kernelized Wasserstein Natural Gradient](https://openreview.net/forum?id=Hklz71rYvS) |  | 0 |  | Michael Arbel, Arthur Gretton, Wuchen Li, Guido Montúfar |  |
| 594 |  |  [And the Bit Goes Down: Revisiting the Quantization of Neural Networks](https://openreview.net/forum?id=rJehVyrKwH) |  | 0 |  | Pierre Stock, Armand Joulin, Rémi Gribonval, Benjamin Graham, Hervé Jégou |  |
| 595 |  |  [A Latent Morphology Model for Open-Vocabulary Neural Machine Translation](https://openreview.net/forum?id=BJxSI1SKDH) |  | 0 |  | Duygu Ataman, Wilker Aziz, Alexandra Birch |  |
| 596 |  |  [Understanding Why Neural Networks Generalize Well Through GSNR of Parameters](https://openreview.net/forum?id=HyevIJStwH) |  | 0 |  | Jinlong Liu, Yunzhi Bai, Guoqing Jiang, Ting Chen, Huayan Wang |  |
| 597 |  |  [Model Based Reinforcement Learning for Atari](https://openreview.net/forum?id=S1xCPJHtDB) |  | 0 |  | Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk Michalewski |  |
| 598 |  |  [Disagreement-Regularized Imitation Learning](https://openreview.net/forum?id=rkgbYyHtwB) |  | 0 |  | Kianté Brantley, Wen Sun, Mikael Henaff |  |
| 599 |  |  [Stable Rank Normalization for Improved Generalization in Neural Networks and GANs](https://openreview.net/forum?id=H1enKkrFDB) |  | 0 |  | Amartya Sanyal, Philip H. S. Torr, Puneet K. Dokania |  |
| 600 |  |  [Measuring the Reliability of Reinforcement Learning Algorithms](https://openreview.net/forum?id=SJlpYJBKvH) |  | 0 |  | Stephanie C. Y. Chan, Samuel Fishman, Anoop Korattikara, John F. Canny, Sergio Guadarrama |  |
| 601 |  |  [Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue](https://openreview.net/forum?id=Hke0K1HKwr) |  | 0 |  | Byeongchang Kim, Jaewoo Ahn, Gunhee Kim |  |
| 602 |  |  [Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://openreview.net/forum?id=SklD9yrFPS) |  | 0 |  | Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha SohlDickstein, Samuel S. Schoenholz |  |
| 603 |  |  [Self-labelling via simultaneous clustering and representation learning](https://openreview.net/forum?id=Hyx-jyBFPr) |  | 0 |  | Yuki Markus Asano, Christian Rupprecht, Andrea Vedaldi |  |
| 604 |  |  [The intriguing role of module criticality in the generalization of deep networks](https://openreview.net/forum?id=S1e4jkSKvB) |  | 0 |  | Niladri S. Chatterji, Behnam Neyshabur, Hanie Sedghi |  |
| 605 |  |  [Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks](https://openreview.net/forum?id=rkl8sJBYvH) |  | 0 |  | Sanjeev Arora, Simon S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, Dingli Yu |  |
| 606 |  |  [Differentiation of Blackbox Combinatorial Solvers](https://openreview.net/forum?id=BkevoJSYPB) |  | 0 |  | Marin Vlastelica Pogancic, Anselm Paulus, Vít Musil, Georg Martius, Michal Rolínek |  |
| 607 |  |  [Scaling Autoregressive Video Models](https://openreview.net/forum?id=rJgsskrFwH) |  | 0 |  | Dirk Weissenborn, Oscar Täckström, Jakob Uszkoreit |  |
| 608 |  |  [The Ingredients of Real World Robotic Reinforcement Learning](https://openreview.net/forum?id=rJe2syrtvS) |  | 0 |  | Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Kumar, Sergey Levine |  |
| 609 |  |  [Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization](https://openreview.net/forum?id=ryeYpJSKwr) |  | 0 |  | Michael Volpp, Lukas P. Fröhlich, Kirsten Fischer, Andreas Doerr, Stefan Falkner, Frank Hutter, Christian Daniel |  |
| 610 |  |  [Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning](https://openreview.net/forum?id=BJliakStvH) |  | 0 |  | Dexter R. R. Scobee, S. Shankar Sastry |  |
| 611 |  |  [Spectral Embedding of Regularized Block Models](https://openreview.net/forum?id=H1l_0JBYwS) |  | 0 |  | Nathan de Lara, Thomas Bonald |  |
| 612 |  |  [Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models](https://openreview.net/forum?id=BkxRRkSKwr) |  | 0 |  | Xisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, Xiang Ren |  |
| 613 |  |  [word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement](https://openreview.net/forum?id=HkxARkrFwB) |  | 0 |  | Aliakbar Panahi, Seyran Saeedi, Tomasz Arodz |  |
| 614 |  |  [What Can Neural Networks Reason About?](https://openreview.net/forum?id=rJxbJeHFPS) |  | 0 |  | Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Kenichi Kawarabayashi, Stefanie Jegelka |  |
| 615 |  |  [Training individually fair ML models with sensitive subspace robustness](https://openreview.net/forum?id=B1gdkxHFDH) |  | 0 |  | Mikhail Yurochkin, Amanda Bower, Yuekai Sun |  |
| 616 |  |  [Learning from Rules Generalizing Labeled Exemplars](https://openreview.net/forum?id=SkeuexBtDr) |  | 0 |  | Abhijeet Awasthi, Sabyasachi Ghosh, Rasna Goyal, Sunita Sarawagi |  |
| 617 |  |  [Directional Message Passing for Molecular Graphs](https://openreview.net/forum?id=B1eWbxStPH) |  | 0 |  | Johannes Klicpera, Janek Groß, Stephan Günnemann |  |
| 618 |  |  [Explanation by Progressive Exaggeration](https://openreview.net/forum?id=H1xFWgrFPS) |  | 0 |  | Sumedha Singla, Brian Pollack, Junxiang Chen, Kayhan Batmanghelich |  |
| 619 |  |  [Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network](https://openreview.net/forum?id=ByeGzlrKwH) |  | 0 |  | Taiji Suzuki, Hiroshi Abe, Tomoaki Nishimura |  |
| 620 |  |  [At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?](https://openreview.net/forum?id=Bkeb7lHtvH) |  | 0 |  | Niv Giladi, Mor Shpigel Nacson, Elad Hoffer, Daniel Soudry |  |
| 621 |  |  [Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)](https://openreview.net/forum?id=rygeHgSFDH) |  | 0 |  | Peter Sorrenson, Carsten Rother, Ullrich Köthe |  |
| 622 |  |  [Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps](https://openreview.net/forum?id=BkgrBgSYDS) |  | 0 |  | Tri Dao, Nimit Sharad Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, Christopher Ré |  |
| 623 |  |  [Improving Generalization in Meta Reinforcement Learning using Learned Objectives](https://openreview.net/forum?id=S1evHerYPr) |  | 0 |  | Louis Kirsch, Sjoerd van Steenkiste, Jürgen Schmidhuber |  |
| 624 |  |  [Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks](https://openreview.net/forum?id=BJxsrgStvr) |  | 0 |  | Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen, Richard G. Baraniuk, Zhangyang Wang, Yingyan Lin |  |
| 625 |  |  [Truth or backpropaganda? An empirical investigation of deep learning theory](https://openreview.net/forum?id=HyxyIgHFvr) |  | 0 |  | Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, Tom Goldstein |  |
| 626 |  |  [Neural Arithmetic Units](https://openreview.net/forum?id=H1gNOeHKPS) |  | 0 |  | Andreas Madsen, Alexander Rosenberg Johansen |  |
| 627 |  |  [DeepSphere: a graph-based spherical CNN](https://openreview.net/forum?id=B1e3OlStPB) |  | 0 |  | Michaël Defferrard, Martino Milani, Frédérick Gusset, Nathanaël Perraudin |  |
| 628 |  |  [SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models](https://openreview.net/forum?id=SylkYeHtwr) |  | 0 |  | Yucen Luo, Alex Beatson, Mohammad Norouzi, Jun Zhu, David Duvenaud, Ryan P. Adams, Ricky T. Q. Chen |  |
| 629 |  |  [Deep Learning For Symbolic Mathematics](https://openreview.net/forum?id=S1eZYeHFDS) |  | 0 |  | Guillaume Lample, François Charton |  |
| 630 |  |  [Making Sense of Reinforcement Learning and Probabilistic Inference](https://openreview.net/forum?id=S1xitgHtvS) |  | 0 |  | Brendan O'Donoghue, Ian Osband, Catalin Ionescu |  |
| 631 |  |  [Unbiased Contrastive Divergence Algorithm for Training Energy-Based Latent Variable Models](https://openreview.net/forum?id=r1eyceSYPr) |  | 0 |  | Yixuan Qiu, Lingsong Zhang, Xiao Wang |  |
| 632 |  |  [A Mutual Information Maximization Perspective of Language Representation Learning](https://openreview.net/forum?id=Syx79eBKwr) |  | 0 |  | Lingpeng Kong, Cyprien de Masson d'Autume, Lei Yu, Wang Ling, Zihang Dai, Dani Yogatama |  |
| 633 |  |  [Energy-based models for atomic-resolution protein conformations](https://openreview.net/forum?id=S1e_9xrFvS) |  | 0 |  | Yilun Du, Joshua Meier, Jerry Ma, Rob Fergus, Alexander Rives |  |
| 634 |  |  [Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem](https://openreview.net/forum?id=BJe55gBtvH) |  | 0 |  | Vaggos Chatziafratis, Sai Ganesh Nagarajan, Ioannis Panageas, Xiao Wang |  |
| 635 |  |  [Generalization of Two-layer Neural Networks: An Asymptotic Viewpoint](https://openreview.net/forum?id=H1gBsgBYwH) |  | 0 |  | Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Denny Wu, Tianzong Zhang |  |
| 636 |  |  [Reconstructing continuous distributions of 3D protein structure from cryo-EM images](https://openreview.net/forum?id=SJxUjlBtwB) |  | 0 |  | Ellen D. Zhong, Tristan Bepler, Joseph H. Davis, Bonnie Berger |  |
| 637 |  |  [Progressive Learning and Disentanglement of Hierarchical Representations](https://openreview.net/forum?id=SJxpsxrYPS) |  | 0 |  | Zhiyuan Li, Jaideep Vitthal Murkute, Prashnna Kumar Gyawali, Linwei Wang |  |
| 638 |  |  [An Exponential Learning Rate Schedule for Deep Learning](https://openreview.net/forum?id=rJg8TeSFDH) |  | 0 |  | Zhiyuan Li, Sanjeev Arora |  |
| 639 |  |  [Geom-GCN: Geometric Graph Convolutional Networks](https://openreview.net/forum?id=S1e2agrFvS) |  | 0 |  | Hongbin Pei, Bingzhe Wei, Kevin ChenChuan Chang, Yu Lei, Bo Yang |  |
| 640 |  |  [CATER: A diagnostic dataset for Compositional Actions & TEmporal Reasoning](https://openreview.net/forum?id=HJgzt2VKPB) |  | 0 |  | Rohit Girdhar, Deva Ramanan |  |
| 641 |  |  [BackPACK: Packing more into Backprop](https://openreview.net/forum?id=BJlrF24twB) |  | 0 |  | Felix Dangel, Frederik Kunstner, Philipp Hennig |  |
| 642 |  |  [GenDICE: Generalized Offline Estimation of Stationary Values](https://openreview.net/forum?id=HkxlcnVFwB) |  | 0 |  | Ruiyi Zhang, Bo Dai, Lihong Li, Dale Schuurmans |  |
| 643 |  |  [Principled Weight Initialization for Hypernetworks](https://openreview.net/forum?id=H1lma24tPB) |  | 0 |  | Oscar Chang, Lampros Flokas, Hod Lipson |  |
| 644 |  |  [On the Convergence of FedAvg on Non-IID Data](https://openreview.net/forum?id=HJxNAnVtDS) |  | 0 |  | Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, Zhihua Zhang |  |
| 645 |  |  [Data-dependent Gaussian Prior Objective for Language Generation](https://openreview.net/forum?id=S1efxTVYDr) |  | 0 |  | Zuchao Li, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Zhuosheng Zhang, Hai Zhao |  |
| 646 |  |  [Contrastive Learning of Structured World Models](https://openreview.net/forum?id=H1gax6VtDB) |  | 0 |  | Thomas N. Kipf, Elise van der Pol, Max Welling |  |
| 647 |  |  [Neural Network Branching for Neural Network Verification](https://openreview.net/forum?id=B1evfa4tPB) |  | 0 |  | Jingyue Lu, M. Pawan Kumar |  |
| 648 |  |  [Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity](https://openreview.net/forum?id=BJgnXpVYwS) |  | 0 |  | Jingzhao Zhang, Tianxing He, Suvrit Sra, Ali Jadbabaie |  |
| 649 |  |  [Posterior sampling for multi-agent reinforcement learning: solving extensive games with imperfect information](https://openreview.net/forum?id=Syg-ET4FPS) |  | 0 |  | Yichi Zhou, Jialian Li, Jun Zhu |  |
| 650 |  |  [Mogrifier LSTM](https://openreview.net/forum?id=SJe5P6EYvS) |  | 0 |  | Gábor Melis, Tomás Kociský, Phil Blunsom |  |
| 651 |  |  [Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech](https://openreview.net/forum?id=B1elCp4KwH) |  | 0 |  | David Harwath, WeiNing Hsu, James R. Glass |  |
| 652 |  |  [Mirror-Generative Neural Machine Translation](https://openreview.net/forum?id=HkxQRTNYPH) |  | 0 |  | Zaixiang Zheng, Hao Zhou, Shujian Huang, Lei Li, XinYu Dai, Jiajun Chen |  |
| 653 |  |  [Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning](https://openreview.net/forum?id=rkeS1RVtPS) |  | 0 |  | Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, Andrew Gordon Wilson |  |
| 654 |  |  [Your classifier is secretly an energy based model and you should treat it like one](https://openreview.net/forum?id=Hkxzx0NtDB) |  | 0 |  | Will Grathwohl, KuanChieh Wang, JörnHenrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky |  |
| 655 |  |  [Dynamics-Aware Unsupervised Discovery of Skills](https://openreview.net/forum?id=HJgLZR4KvH) |  | 0 |  | Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman |  |
| 656 |  |  [Optimal Strategies Against Generative Attacks](https://openreview.net/forum?id=BkgzMCVtPB) |  | 0 |  | Roy Mor, Erez Peterfreund, Matan Gavish, Amir Globerson |  |
| 657 |  |  [GraphZoom: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding](https://openreview.net/forum?id=r1lGO0EKDH) |  | 0 |  | Chenhui Deng, Zhiqiang Zhao, Yongyu Wang, Zhiru Zhang, Zhuo Feng |  |
| 658 |  |  [Harnessing Structures for Value-Based Planning and Reinforcement Learning](https://openreview.net/forum?id=rklHqRVKvH) |  | 0 |  | Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi |  |
| 659 |  |  [Comparing Rewinding and Fine-tuning in Neural Network Pruning](https://openreview.net/forum?id=S1gSj0NKvB) |  | 0 |  | Alex Renda, Jonathan Frankle, Michael Carbin |  |
| 660 |  |  [Meta-Q-Learning](https://openreview.net/forum?id=SJeD3CEFPH) |  | 0 |  | Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, Alexander J. Smola |  |
| 661 |  |  [Mathematical Reasoning in Latent Space](https://openreview.net/forum?id=Ske31kBtPr) |  | 0 |  | Dennis Lee, Christian Szegedy, Markus N. Rabe, Sarah M. Loos, Kshitij Bansal |  |
| 662 |  |  [A Theory of Usable Information under Computational Constraints](https://openreview.net/forum?id=r1eBeyHFDH) |  | 0 |  | Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, Stefano Ermon |  |
| 663 |  |  [Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning](https://openreview.net/forum?id=rygixkHKDH) |  | 0 |  | Qing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, Zhihui Zhu |  |
| 664 |  |  [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://openreview.net/forum?id=ryghZJBKPS) |  | 0 |  | Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, Alekh Agarwal |  |
| 665 |  |  [Understanding and Robustifying Differentiable Architecture Search](https://openreview.net/forum?id=H1gDNyrKDS) |  | 0 |  | Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, Frank Hutter |  |
| 666 |  |  [A Closer Look at Deep Policy Gradients](https://openreview.net/forum?id=ryxdEkHtPS) |  | 0 |  | Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry |  |
| 667 |  |  [Implementation Matters in Deep RL: A Case Study on PPO and TRPO](https://openreview.net/forum?id=r1etN1rtPB) |  | 0 |  | Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, Aleksander Madry |  |
| 668 |  |  [Fast Task Inference with Variational Intrinsic Successor Features](https://openreview.net/forum?id=BJeAHkrYDS) |  | 0 |  | Steven Hansen, Will Dabney, André Barreto, David WardeFarley, Tom Van de Wiele, Volodymyr Mnih |  |
| 669 |  |  [Learning to Balance: Bayesian Meta-Learning for Imbalanced and Out-of-distribution Tasks](https://openreview.net/forum?id=rkeZIJBYvr) |  | 0 |  | Haebeom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, Sung Ju Hwang |  |
| 670 |  |  [RNA Secondary Structure Prediction By Learning Unrolled Algorithms](https://openreview.net/forum?id=S1eALyrYDH) |  | 0 |  | Xinshi Chen, Yu Li, Ramzan Umarov, Xin Gao, Le Song |  |
| 671 |  |  [Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search](https://openreview.net/forum?id=BJlQtJSKDB) |  | 0 |  | Anji Liu, Jianshu Chen, Mingze Yu, Yu Zhai, Xuewen Zhou, Ji Liu |  |
| 672 |  |  [Target-Embedding Autoencoders for Supervised Representation Learning](https://openreview.net/forum?id=BygXFkSYDH) |  | 0 |  | Daniel Jarrett, Mihaela van der Schaar |  |
| 673 |  |  [Reformer: The Efficient Transformer](https://openreview.net/forum?id=rkgNKkHtvB) |  | 0 |  | Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya |  |
| 674 |  |  [Rotation-invariant clustering of neuronal responses in primary visual cortex](https://openreview.net/forum?id=rklr9kHFDB) |  | 0 |  | Ivan Ustyuzhaninov, Santiago A. Cadena, Emmanouil Froudarakis, Paul G. Fahey, Edgar Y. Walker, Erick Cobos, Jacob Reimer, Fabian H. Sinz, Andreas S. Tolias, Matthias Bethge, Alexander S. Ecker |  |
| 675 |  |  [Causal Discovery with Reinforcement Learning](https://openreview.net/forum?id=S1g2skStPB) |  | 0 |  | Shengyu Zhu, Ignavier Ng, Zhitang Chen |  |
| 676 |  |  [Intrinsically Motivated Discovery of Diverse Patterns in Self-Organizing Systems](https://openreview.net/forum?id=rkg6sJHYDr) |  | 0 |  | Chris Reinke, Mayalen Etcheverry, PierreYves Oudeyer |  |
| 677 |  |  [Restricting the Flow: Information Bottlenecks for Attribution](https://openreview.net/forum?id=S1xWh1rYwB) |  | 0 |  | Karl Schulz, Leon Sixt, Federico Tombari, Tim Landgraf |  |
| 678 |  |  [Building Deep Equivariant Capsule Networks](https://openreview.net/forum?id=BJgNJgSFPS) |  | 0 |  | Sai Raam Venkataraman, S. Balasubramanian, R. Raghunatha Sarma |  |
| 679 |  |  [A Generalized Training Approach for Multiagent Learning](https://openreview.net/forum?id=Bkl5kxrKDr) |  | 0 |  | Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Pérolat, Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes, Zhe Wang, Guy Lever, Nicolas Heess, Thore Graepel, Rémi Munos |  |
| 680 |  |  [High Fidelity Speech Synthesis with Adversarial Networks](https://openreview.net/forum?id=r1gfQgSFDr) |  | 0 |  | Mikolaj Binkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C. Cobo, Karen Simonyan |  |
| 681 |  |  [SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference](https://openreview.net/forum?id=rkgvXlrKwH) |  | 0 |  | Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, Marcin Michalski |  |
| 682 |  |  [Meta-Learning with Warped Gradient Descent](https://openreview.net/forum?id=rkeiQlBFPB) |  | 0 |  | Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, Raia Hadsell |  |
| 683 |  |  [Convolutional Conditional Neural Processes](https://openreview.net/forum?id=Skey4eBYPS) |  | 0 |  | Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, Richard E. Turner |  |
| 684 |  |  [Gradient Descent Maximizes the Margin of Homogeneous Neural Networks](https://openreview.net/forum?id=SJeLIgBKPS) |  | 0 |  | Kaifeng Lyu, Jian Li |  |
| 685 |  |  [Adversarial Training and Provable Defenses: Bridging the Gap](https://openreview.net/forum?id=SJxSDxrKDr) |  | 0 |  | Mislav Balunovic, Martin T. Vechev |  |
| 686 |  |  [Differentiable Reasoning over a Virtual Knowledge Base](https://openreview.net/forum?id=SJxstlHFPH) |  | 0 |  | Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov, William W. Cohen |  |
| 687 |  |  [Federated Learning with Matched Averaging](https://openreview.net/forum?id=BkluqlSFDS) |  | 0 |  | Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S. Papailiopoulos, Yasaman Khazaeni |  |
