# ICLR2023

## 会议论文列表

本会议共有 220 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023](https://openreview.net/group?id=ICLR.cc/2023/TinyPapers) |  | 0 |  | Krystal Maughan, Rosanne Liu, Thomas F. Burns |  |
| 2 |  |  [Statistical Property Testing for Generative Models](https://openreview.net/forum?id=xmY_plRB15j) |  | 0 | Generative models that produce images, text, or other types of data are recently be equipped with more powerful capabilities. Nevertheless, in some use cases of the generated data (e.g., using it for model training), one must ensure that the synthetic data points satisfy some properties that make them suitable for the intended use. Towards this goal, we present a simple framework to statistically check if the data produced by a generative model satisfy some property with a given confidence level. We apply our methodology to standard image and text-to-image generative models. | Emmanouil Seferis, Simon Burton, ChihHong Cheng |  |
| 3 |  |  [Can Conformal Prediction Obtain Meaningful Safety Guarantees for ML Models?](https://openreview.net/forum?id=5aO1lsEJGu) |  | 0 | Conformal Prediction (CP) has been recently proposed as a methodology to calibrate the predictions of Machine Learning (ML) models so that they can output rigorous quantification of their uncertainties. For example, one can calibrate the predictions of an ML model into prediction sets, that guarantee to cover the ground truth class with a probability larger than a specified threshold. In this paper, we study whether CP can provide strong statistical guarantees that would be required in safety-critical applications. Our evaluation on the ImageNet demonstrates that using CP over state-of-the-art models fails to deliver the required guarantees. We corroborate our results by deriving a simple connection between the CP prediction sets and top-k accuracy. | Emmanouil Seferis, Simon Burton, ChihHong Cheng |  |
| 4 |  |  [A two-parameter learnable Logmoid Activation Unit](https://openreview.net/forum?id=LcXWYmA8Ek) |  | 0 | A novel learnable Logmoid Activation Unit (LAU) is proposed as, $f(x)=x\ln(1+\alpha\textrm{sigmoid}(\beta x))$, by parameterizing Sigmoid with two hyper-parameters $\alpha$ and $\beta$ that are optimized by the back-propagation algorithm. The end-to-end deep neural networks with learnable LAUs can increase the predictive performances beyond well-known activation functions for different tasks. | Xuemei Zhou, Lingfang Li, Xingzhou Zheng, Mingxing Luo |  |
| 5 |  |  [Cross Domain Vulnerability Detection using Graph Contrastive Learning](https://openreview.net/forum?id=rrZtzI7xj2b) |  | 0 | To overcome the difficulty of finding good-quality labeled data in domains such as vulnerability detection, Self--Supervised Learning (SSL) methods such as Contrastive Learning (CL) algorithms were developed. We evaluate the performance of one such state-of-the-art CL method, GraphCL, that trains on our graph dataset generated from code repositories of six widely used C/C++ applications. We also propose a custom graph type having a new structure that combines both code-level and binary-level CPG graphs. This is because, even though existing graph types such as AST, CFG and CPG are effective in detecting vulnerabilities in the source code, it is ineffective in detecting the ones that only occur in the binary-level. Hence, to detect those vulnerabilities, we propose a new graph type, Cross Domain Control Property Graph (CDCPG). We perform extensive experiments, using different augmentation techniques and loss functions to show that our custom graph type, CDCPG, performs better than other graph types in many scenarios. | Mahmoud Zamani, Saquib Irtiza, Shamila Wickramasuriya, Latifur Khan, Kevin W. Hamlen |  |
| 6 |  |  [Tiny Attention: A Simple yet Effective Method for Learning Contextual Word Embeddings](https://openreview.net/forum?id=BWWrDHaP29) |  | 0 | Contextual Word Embedding (CWE) obtained via the Attention Mechanism in Transformer (AMT) models is one of the key drivers of the current revolution in Natural Language Processing. Previous techniques for learning CWEs are not only inferior to AMT but also are largely subpar to the simple bag-of-words baseline. Though there have been many variants of the Transformer model, the attention mechanism itself remains unchanged and is largely opaque. We introduce a new method for leaning CWEs that uses a simple and transparent attention mechanism. Our method is derived from the SVD based Syntagmatic Word Embeddings, which capture word associations. We test our model on the Word-in-Context dataset, and show that it outperforms the simple but tough-to-beat baseline by a substantial margin. | Renjith P. Ravindran, Narayana Murthy Kavi |  |
| 7 |  |  [Training Data Eigenvector Dynamics in the EigenPro Implementation of the Neural Tangent Kernel and Recursive Feature Machines](https://openreview.net/forum?id=8WiNDyXgj6) |  | 0 | There has been much recent work on kernel methods as a viable alternative to deep neural networks (DNNs). The advent of the $\textit{Neural Tangent Kernel}$ (NTK) has brought on renewed interest in these methods and their application to typical deep learning tasks. Recently, kernels have been shown to be capable of feature learning similar to that of DNNs, termed $\textit{Recursive Feature Machines}$ (RFMs). In accordance with the growing scale of kernel models, the EigenPro 3 algorithm was proposed to facilitate large-scale training based on preconditioned gradient descent. We propose an accessible framework for observing the eigenvector dynamics of EigenPro's training data in its implementation of these kernel methods, and find empirically that significant change ceases early in training along with apparent bias towards equilibrium. In the case of RFMs, we find that significant change in the training data eigenvectors typically curtails before five iterations, in accordance with findings that RFMs achieve optimal performance in five iterations. This represents a path forward in gaining intuition for the inner workings of large-scale kernel training methods. We provide an easy to use Python implementation of our framework at https://github.com/cgorlla/ep3dynamics. | Cyril Gorlla |  |
| 8 |  |  [Metric Transform: Exploring beyond Affine Transform for Neural Networks](https://openreview.net/forum?id=fVuTIiTBky) |  | 0 | Artificial Neural Networks(ANN) of varying architectures are generally paired with linear transformation at the core. However, we find dot product neurons with global influence less interpretable as compared to a more local influence of euclidean distance (as used in RBF). In this work, we explore the generalization of dot product neurons to lp-norm, metrics, and beyond. We find such metrics as transform performs similarly to affine transform when used in MLP or CNN. Furthermore, we use distance/similarity measuring neurons to interpret and explain input data, overfitting and Residual MLP. We share our code in github. | Suman Sapkota, Binod Bhattarai |  |
| 9 |  |  [MaskedFusion360: Reconstruct LiDAR Data by Querying Camera Features](https://openreview.net/forum?id=mIEMVZ47aNA) |  | 0 | In self-driving applications, LiDAR data provides accurate information about distances in 3D but lacks the semantic richness of camera data. Therefore, state-of-the-art methods for perception in urban scenes fuse data from both sensor types. In this work, we introduce a novel self-supervised method to fuse LiDAR and camera data for self-driving applications. We build upon masked autoencoders (MAEs) and train deep learning models to reconstruct masked LiDAR data from fused LiDAR and camera features. In contrast to related methods that use birds-eye-view representations, we fuse features from dense spherical LiDAR projections and features from fish-eye camera crops with a similar field of view. Therefore, we reduce the learned spatial transformations to moderate perspective transformations and do not require additional modules to generate dense LiDAR representations. Code is available at: https://github.com/KIT-MRT/masked-fusion-360 | Royden Wagner, Marvin Klemp, Carlos Fernández Lopez |  |
| 10 |  |  [Meta-Learning for Subject Adaptation in Low-Data Environments for EEG-Based Motor Imagery Brain-Computer Interfaces](https://openreview.net/forum?id=7QqlQW9hJ8J) |  | 0 | Motor imagery classification from Electroencephalogram (EEG) signals involves decoding information during the imagination of specific movements. However, learning representations for EEG-based motor imagery classification is challenging due to inter-subject variability and differences in mental imagery, resulting in poor generalization of deep learning models to new subjects. While pre-trained deep learning models achieve high accuracy on subjects with similar domains, they fail on subjects with dissimilar domains. Optimization-based meta-learning algorithms can address this limitation by learning a good initialization for the model, enabling quick adaptation to new subjects with limited fine-tuning examples. We demonstrate that our Meta Learning approach consistently outperforms Transfer Learning on the BCI Competition IV 2a dataset. Although accuracy varies depending on domain similarity, meta-learning demonstrates efficient adaption to unseen subjects with limited data. By improving generalization across subjects with different domains under low-data environments, we can enhance the reliability and practicality of brain-computer interfaces for real-world applications. | Arnav Pati, Deepak Mewada, Debasis Samanta |  |
| 11 |  |  [Fostering Effective Communication Between Humans and Machines](https://openreview.net/forum?id=AHnLJBD7xKx) |  | 0 | With the growing usage of smartphones, digital communication has become significant. This paper describes the primary research conducted to study user interaction patterns on smartphones. Results show that the time between two touches, or the editorial context duration, is just 5 to 10 seconds for a vast majority of smartphone interactions. So, the paper introduces the novel concept of MicroStimuli, which can generate a response in mere milliseconds, specifically tailored for smartphones. The construct of MicroStimuli is formulated by leveraging the neuroscience of decision-making in response to visual stimuli. | Karthika Kamath, Tanya Upadhyay, Jieya Rawal, Kirtana Sunil Phatnani, Biju Dominic |  |
| 12 |  |  [Transfer Learning on Kinyarwanda Tweets Sentiment Analysis](https://openreview.net/forum?id=7lyEQXHkGpl) |  | 0 | Pretrained models available on platform such as Hugging Face have become a valuable resource for machine learning community, particularly for natural language processing task. In this study, we evaluated the performance of Kinyarwanda and English pretrained models for sentiment analysis of Kinyarwanda tweets through transfer learning using Hugging Face pretrained models and Trainer for implementation. We have found that English pretrained models for translated Kinyarwanda tweets dataset using Google translate out performed Kinyarwanda pretrained models. | Roger Byakunda |  |
| 13 |  |  [Symbolic Regression in Financial Economics](https://openreview.net/forum?id=RuCQRXk7a7G) |  | 0 | We apply symbolic regression, the machine learning approach of recovering models from data, in financial economics. Specifically, we present a data set consisting of equations that cover a broad range of topics in financial economics. These equations are built off a common set of mathematical symbols but importantly have new variations in functional forms. We test the joint performance of deep learning and genetic programming symbolic regression systems in recovering these non-physical equations. | Jiacheng Liu, Siqi Guo |  |
| 14 |  |  [FRESCO: Federated Reinforcement Energy System for Cooperative Optimization](https://openreview.net/forum?id=75mWq5j4iso) |  | 0 | The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in creating the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results. | Nicolas M. Cuadrado, Roberto Alejandro Gutiérrez Guillén, Martin Takác |  |
| 15 |  |  [Learning Rotation-Agnostic Representations via Group Equivariant VAEs](https://openreview.net/forum?id=jbNqgEJf0EI) |  | 0 | An emerging field in representation learning involves the study of group-equivariant neural networks, that leverage concepts from group representation theory to design neural architectures that can exploit discrete and continuous symmetries to produce more general representations. Following this direction, in this work we demonstrate how an image embedding agnostic to rotations can be naturally obtained by training a variational autoencoder (S-GVAE) equipped with a Group equivariant Convolutional Neural Network (G-CNN) encoder. | Ahmedeo Shokry, Antonio Norelli |  |
| 16 |  |  [Can Text Encoders be Deceived by Length Attack?](https://openreview.net/forum?id=KPHbtTtCDw) |  | 0 | Albeit \textit{de facto} to use in training dense retrieval models, we observe that contrastive learning is prone to length overfitting, making it vulnerable to adversarial length attacks. We examine the behaviour of this phenomenon and propose an editing method to mitigate this problem. We find that our method can effectively improve the robustness of models against length attacks. Its effectiveness can be attributed to reduced length information in the embeddings, more robust intra-document token interaction, and enhanced isotropy at trained length range. | Chenghao Xiao, Zihuiwen Ye, G. Thomas Hudson, Zhongtian Sun, Phil Blunsom, Noura Al Moubayed |  |
| 17 |  |  [Whispering Across the Continent: Collecting and Analyzing African Culture Using Community Radios](https://openreview.net/forum?id=kidk_E11QQ) |  | 0 | African culture is rich and diverse, but much of its knowledge is held by the elders of the community and passed down through oral traditions. With globalization, young Africans are becoming increasingly disconnected from their roots, making it essential to collect and preserve this knowledge. However, the lack of accessible data on African culture presents a significant challenge. This research aims to address this problem by exploring new ways to collect and preserve African cultural data. Specifically, we have developed a device to perform continuous recording of cultural radio programs in local languages, which has enabled us to collect over 1500 hours of audio data. We are also exploring the use of a whisper model, which has proven to be effective in outperforming human transcription and being multilingual. This research project's final goal is to build a language model that understands African culture, providing an effective approach to store this knowledge for future generations to learn about their culture. | Joseph Domguia, Guy Adingono Nkama |  |
| 18 |  |  [Handling unstructured data for operator learning using implicit neural representations](https://openreview.net/forum?id=e2gSQqH3V10) |  | 0 | Operator learning methods are too often constrained by a fixed sampling of both the input and output functions. We propose a novel method to allow current operator learning methods to learn on any sampling. We show that our method can perform inference on unseen samplings, and that it allows returning outputs as continuous functions. | Thomas X. Wang, Patrick Gallinari |  |
| 19 |  |  [Model Extraction Attacks on DistilBERT](https://openreview.net/forum?id=njpSzZ6mCU) |  | 0 | This paper investigates model extraction attacks, where an adversary can train a substitute model by collecting data through query access to a victim model and stealing its functionality. We use DistilBERT as the victim model due to its smaller size and faster processing speed. The results demonstrate the effectiveness of the model extraction attack and show that fine-tuning more powerful language models can improve accuracy. The study provides important insights into the security of machine learning models. | Amro Salman, Ayman Saeed, Khalid N. Elmadani, Sharief Babiker |  |
| 20 |  |  [One Important Thing To Do Before Federated Training](https://openreview.net/forum?id=qq-bA-VLUN) |  | 0 | Previous research in Federated learning (FL) have emphasized privacy protection, model optimization, and so on, meanwhile, they overlooked how to choose the appropriate FL algorithm for a new federation with preserving data privacy. In our study, we provide a formal problem formulation for algorithm selection in FL and present a novel approach that involves leveraging trained federations to aid with algorithm selection. Empirical results prove the effectiveness of our method. | Yichu Xu, Wenqian Li, Yinchuan Li, Yunfeng Shao, Yan Pang, DeChuan Zhan |  |
| 21 |  |  [Insights into the mechanism behind reusing Teacher's classifier in Knowledge Distillation](https://openreview.net/forum?id=_cL7Uj4LXAJ) |  | 0 | Knowledge distillation (KD) has emerged as an effective approach to compress deep neural networks by transferring knowledge from a powerful yet cumbersome teacher model to a lightweight student model. Recent research has suggested that re-using the teacher's final layer (i.e., the classifier) can be a straightforward and effective method for knowledge distillation. The underlying mechanism for this method's success remains unclear. Our study aims to shed light on how the knowledge distillation loss affects the alignment between the weights of the student classifier and the teacher classifier. Specifically, we compare the $L^2$ norm of the difference between the weights of the student and the teacher classifier during the training process. Our experiments demonstrate that the knowledge distillation loss encourages alignment between the student and teacher classifiers, as indicated by a strong positive correlation ($>0.97$) between the $L^2$ norm and the loss during training. We also observe that as temperature increases, this alignment decreases and the $L^2$ norm behaves similar to normal (non-KD) training. Our analysis aims to provide to a better understanding of knowledge distillation provide a starting point for the development of new KD frameworks. | Kinshuk Dua |  |
| 22 |  |  [When will federated learning transfer from generalization to personalization?](https://openreview.net/forum?id=iCqvQSvar5V) |  | 0 | The timing of personalization refers to determining when to train and update personalized models for the participants in Personalized federated learning. Determining the timing for personalization contributes to improving the overall efficiency of federated learning. We propose that training transfers to personalization when the accuracy of the global model reaches a predefined threshold. Experimental results show that this method can effectively improve the accuracy of personalized models in a non-IID scenario. | Wenxuan Liu, Lukun Wang, Jiaming Pei |  |
| 23 |  |  [A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion](https://openreview.net/forum?id=S9NTReFikL2) |  | 0 | Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities. | Zeinab Sadat Taghavi, Ali Satvaty, Hossein Sameti |  |
| 24 |  |  [Fairness Under Partial Observability](https://openreview.net/forum?id=if1Mmrxf-pq) |  | 0 | The purpose of this article is to discuss an important challenge faced in \`\`real life'' when trying to implement \emph{group} fairness-aware models and algorithms. Here, we focus specifically on the role that uncertainty and ambiguity play and revisit the case where protected attributes are only partially observable. | Islam Utyagulov, Francois BuetGolfouse, Peter Hill |  |
| 25 |  |  [Geodesic Mode Connectivity](https://openreview.net/forum?id=cFtt9fU7YB6) |  | 0 | Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity. | Charlie Tan, Theodore Long, Sarah Zhao, Rudolf Laine |  |
| 26 |  |  [Pursuit Policies in Dynamic Environments](https://openreview.net/forum?id=_cZLvP7LAt7) |  | 0 | Cooperative pursuit is a popular multi-agent reinforcement learning (MARL) game where a team of predators target prey while avoiding obstacles. Previous literature has largely considered the impact of different predator, prey abilities on learning. Here, we investigate the impact of dynamic environments on learning predator pursuit policies from partial observations with deep Q-learning. Interestingly, we find predators are able to learn cooperative pursuit strategies that leverage moving obstacles. | Joseph L. Briones, Andréa W. Richa |  |
| 27 |  |  [Resource-efficient image inpainting](https://openreview.net/forum?id=OJILbuOodvm) |  | 0 | Image inpainting refers to the synthesis of missing regions in an image, which can help restore occluded or degraded areas and also serve as a precursor task for self-supervision. The current state-of-the-art models for image inpainting are computationally heavy as they are based on vision transformer backbones in adversarial or diffusion settings. This paper diverges from vision transformers by using a computationally-efficient WaveMix-based fully convolutional architecture, which uses a 2D-discrete wavelet transform (DWT) for spatial and multi-resolution token-mixing along with convolutional layers. The proposed model outperforms the current-state-of-the-art models for large mask inpainting on reconstruction quality while also using less than half the parameter count and considerably lower training and evaluation times. | Dharshan Sampath Kumar, Pranav Jeevan, Amit Sethi |  |
| 28 |  |  [Recursive Reasoning with Neural Networks](https://openreview.net/forum?id=TS8l4VS7_BK) |  | 0 | Many problems can naturally be thought about recursively. However, neural networks fundamentally cannot reason this way on arbitrarily large problems. This is because they do not have the memory to maintain state for the maximum recursion depth required. Solving this issue would enable neural networks to reason like a wide range of classical recursive algorithms (e.g., tree search in model-based RL). To address this, we propose a neural architecture augmented with a stack that learns to save and recall state as needed. We empirically demonstrate the utility of this method on a recursive neural algorithmic reasoning task (learning depth-first search) and show that our architecture leads to improved generalization. | Jonas Jürß, Dulhan Hansaja Jayalath |  |
| 29 |  |  [SUDANESE ARABIC DIALECT ENCODING USING XLM-RoBERTa LANGUAGE MODEL: Zol-ROBERTA](https://openreview.net/forum?id=wUEY2CdQGi1) |  | 0 | XLM-RoBERTa has proven to be very efficient at Natural Language Understanding (NLU), as it allows to achieve state-of-the-art results in most NLU tasks. In this work we aim to utilize the power of XLM-RoBERTa in Sudanese Arabic dialect. We collected over 6 million sentences in Sudanese dialect and used them to resume training of the pre-trained XLM-RoBERTa, as it was trained on 2.5T of data across 100 languages filtered from Common Crawl. Our model -Zol-RoBERTa- is expected to achieve better performance on Sudanese Sentiment Analysis, this clarifies that Zol-RoBERTa will work better in understanding Sudanese Dialectic, which is the domain we are targeting. | Duaa Badradein Alshareif, Taiseer Abdulateef Fadlalla, Muhammed Yahya Saeed, Hiba Hassan S. M. Ali |  |
| 30 |  |  [Reducing the Effect of Incomplete Annotations in Object Detection for Histopathology](https://openreview.net/forum?id=PIfJnq9kpdw) |  | 0 | Training neural networks for object detection usually requires decent amounts of data to produce great results. Apart from the image variety, the number of annotated objects is a crucial factor for success. In histopathology, the average annotation density is very high, resulting in resource-consuming data preparation for neural network training. We explore the effect of incomplete annotations in object detection. We show that modern object detectors, such as YOLO-v5, can effectively learn from histopathology datasets that lack up to 90% of annotations. Additionally, we suggest an easy model tuning setup to reduce the impact of incomplete annotations and enhance learning capability overall. We publish our code at https://github.com/DenysKaliuzhnyi/yolov5. | Denys Kaliuzhnyi, Dmytro Fishman, Mikhail Papkov |  |
| 31 |  |  [Experimenting with Multimodal AutoML: Detection and Evaluation of Alzheimer's Disease](https://openreview.net/forum?id=nSqrgBKBGkv) |  | 0 | This paper describes an experiment using AutoML, AutoGluon Tabular, to discover multimodal models for MMSE regression and AD detection. Using the ADReSSo dataset, this paper reports enhanced performance in classification models and comparable performance in regression models to the baseline, achieving a significant improvement of up to 82\% accuracy on the test dataset. In contrast, the test RMSE has a marginal difference of only 0.28 compared to the baseline. | Ujjawal Shah, Saurav Keshari Aryal |  |
| 32 |  |  [GFlowNets with Human Feedback](https://openreview.net/forum?id=2KxH_4US0ZH) |  | 0 | We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration of training language models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF, and thus is more suitable for large-scale language model tasks. | Yinchuan Li, Shuang Luo, Yunfeng Shao, Jianye Hao |  |
| 33 |  |  [Attention-likelihood relationship in Transformers](https://openreview.net/forum?id=R82eeIF4rP_) |  | 0 | We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at [url]. | Valeria Ruscio, Valentino Maiorca, Fabrizio Silvestri |  |
| 34 |  |  [Truly Generative Data Augmentation for Image Segmentation - Case of Cloud Images](https://openreview.net/forum?id=cQ_eEMsc6p) |  | 0 | Supervised learning frameworks frequently rely on semantic image segmentation, which necessitates a substantial amount of annotated data. Existing methodologies for data augmentation either employ image transformations that are limited by the cardinality of the original dataset or employ generative augmentation techniques that introduce pixel categorization errors. This paper presents an innovative approach for "truly" generative data augmentation for image segmentation, specifically in the context of sky/cloud images. The proposed method involves separate generation of the background clear sky image and the foreground cloud masks using two separate DCGANs, which are subsequently merged to produce augmented images. This organic approach enhances the quality of generated images while preserving accurate pixel categorization. The proposed approach is finally noted to improve the robustness of the sky/cloud image segmentation models. | Mayank Jain, Soumyabrata Dev |  |
| 35 |  |  [Fast Fourier Convolutions in Self-Supervised Neural Networks for Image Denoising](https://openreview.net/forum?id=ghfL1e2rOd) |  | 0 | Recently, denoising convolutional neural networks (CNN) have started to outperform classical denoising algorithms. However, CNNs performance could be constrained by the limited receptive field of regular convolution. To mitigate this problem, a new modification for CNNs was proposed: Fast Fourier Convolution (FFC). Here, a global receptive field is achieved by using Fourier Transform and convolving spectral representation. The global perception field can help CNNs to better capture dependencies in image regions that are far apart. In this work, we design multiple approaches for incorporating FFC into self-supervised neural networks for image denoising. We evaluate these approaches on three benchmark datasets and compare them with supervised and self-supervised methods. We empirically show that an FFC-enhanced denoising network achieves the state-of-the- art results on the character dataset and shows a comparable level of performance for both grayscale and color natural images. | Joonas Ariva, Mikhail Papkov |  |
| 36 |  |  [Personalized Federated Learning for Medical Segmentation using Hypernetworks](https://openreview.net/forum?id=lpcO1957Tv) |  | 0 | In federated learning (FL), several clients jointly train a shared model without sharing their data, maintaining data privacy and reducing communication costs. In personalized federated learning (PFL), each client has their own model, and models are trained jointly. Hypernetworks have been shown to be useful for PFL in classification problems, but it is still not clear how to apply them to problems like segmentation. There, models are very large, and it is not known what parts of models should be personalized, and what parts should be shared across clients. Here, we explore HNs for PFL for solving a problem of image segmentation in the context of medical imaging diagnosis. Using MRI scans for prostate segmentation, we demonstrate that using a hypernetwork to personalize a single convolution layer and the batch-norm layer outperforms local and FL baselines. | Hilit Segev, Gal Chechik |  |
| 37 |  |  [Career Path Modeling and Recommendations with Linkedin Career Data and Predicted Salary Estimations](https://openreview.net/forum?id=R5NNAThG0i) |  | 0 | Career planning involves devising a sequence of steps that build up an ideal career path for a person. However, career planning has become more complex in recent years, demanding the need for better models and systems for recommending Career Paths. With that in mind, we explored new variables and techniques that could help in predicting better career paths. We built a Long-Short Term Memory Network with Self Attention Layers called LSTM-ATT, that predicts a person’s possible career path using Linkedin Career History and new variables such as salary estimations and social networks. We measured the model’s performance in terms of Mean Percentile Rank and Precision at 50 and 100. We found that LSTM and self-attention layers were able to show good predictive performance for multi-class classification even with over 6000 classes for companies and skills, effectively beating a multi-channel CNN for all metrics. However, by checking the versions of either model with added features, they did not yield any major increase in predictive accuracy against the models without it. This leads us to conclude that the added variables did not help in predicting better career paths. | Micaela Tayoto Cerilla, Aaron Santillan, Carl John Vinas, Michael B. Dela Fuente |  |
| 38 |  |  [Sleep Deprivation in the Forward-Forward Algorithm](https://openreview.net/forum?id=q_lJooPbX_) |  | 0 | This paper aims to explore the separation of the two forward passes in the Forward-Forward algorithm from a biological perspective in the context of sleep. We show the size of the gap between the sleep and awake phase influences the learning capabilities of the algorithm and highlight the importance of negative data in diminishing the devastating effects of sleep deprivation. | Mircea Tudor Lica, David DinucuJianu |  |
| 39 |  |  [MetaXLR - Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit](https://openreview.net/forum?id=nF70Sl-HUZ) |  | 0 | Transfer learning for extremely low-resource languages is a challenging task as there is no large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning. We follow the work of (Xia et al., 2021) which suggests using meta learning for transfer learning from a single source language to an extremely low resource one. We propose an enhanced approach which uses multiple source languages chosen in a data-driven manner. In addition, we introduce a sample selection strategy for utilizing the languages in training by using a multi armed bandit algorithm. Using both of these improvements we managed to achieve state-of-the-art results on the NER task for the extremely low resource languages even with the same amount of data, making the representations better generalized. Also, due to the method’s ability to use multiple languages it allows the framework to use much larger amounts of data, while still having superior results over the former MetaXL method even with the same amounts of data. | Liat Bezalel, Eyal Orgad |  |
| 40 |  |  [Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models](https://openreview.net/forum?id=cKlgcx7nSZ) |  | 0 | Massive language models with billions of parameters have significant compute expenses and thus can benefit from pruning. Pruning techniques for massive models are typically iterative and require extensive weight retraining after pruning. SparseGPT, a recently introduced one-shot technique for pruning such models, enables pruning without retraining. We improve upon SparseGPT by fine-tuning during pruning with minimal training steps, and we perform experiments against magnitude pruning and find that our iteratively fine-tuned SparseGPT models significantly outperform their magnitude pruning counterparts at high sparsity. | Aaquib Syed, Phillip Guo, Vijaykaarti Sundarapandiyan |  |
| 41 |  |  [Model Extraction Attacks on Arabic BERT-Based APIs](https://openreview.net/forum?id=XptW6NuULJ) |  | 0 | In this paper, we study the feasibility of performing Model Extraction attacks on Arabic BERT-based APIs. In our experiments, we try to perform these attacks under different scenarios and observe the accuracy of the extracted model against the victim model. We then propose a method for protecting against these types of attacks by introducing noise, in the form of pre-training the victim model for more epochs on non-public generic data. Our results show that this strategy better secures the victim model from such attacks. | Hassan Abbelkarim, Mohammed Eltahir, Khalid N. Elmadani, Anas Showk |  |
| 42 |  |  [IMITATION LEARNING USING THE FORWARD-FORWARD ALGORITHM](https://openreview.net/forum?id=baF9FqIdTY) |  | 0 | The forward-forward (FF) algorithm has been recently introduced as a novel approach to training neural networks in a way that approximates the behavior of real neurons. Nevertheless, its application has been limited to visual domains and has not been investigated in the context of imitation learning or reinforcement learning. In this study, we evaluate the FF algorithm in the context of imitation learning. We implement a straightforward imitation model based on the FF algorithm and present a comparative analysis with the backpropagation (BP) model. Our findings indicate that the FF-based model exhibits comparable performance to the BP-based model on larger datasets but demonstrates inferior performance on smaller datasets. | Insik Chung, Isaac Han, KyungJoong Kim |  |
| 43 |  |  [The Geometry of Multilingual Language Models: An Equality Lens](https://openreview.net/forum?id=dGuMR8tLDs) |  | 0 | Understanding the representations of different languages in multilingual language models is essential for comprehending their cross-lingual properties, predicting their performance on downstream tasks, and identifying any biases across languages. In our study, we analyze the geometry of three multilingual language models in Euclidean space and find that all languages are represented by unique geometries. Although languages tend to be closer according to their linguistic family, they are almost separable with languages from other families. We also introduce a Cross-Lingual Similarity Index to study the semantic similarity across languages. Our findings indicate that the representation of low-resource languages is low compared to high-resource languages. | Cheril Shah, Yashashree Chandak, Manan Suri |  |
| 44 |  |  [Dynamic Human AI Collaboration](https://openreview.net/forum?id=Muwb2KohnX) |  | 0 | Domain experts possess valuable knowledge and insights that can help improve the accuracy and relevance of the machine learning (ML) models. By incorporating expert opinions, the models can capture important nuances and factors that may not be captured by data-driven methods alone. The integration of machine learning models with human experts has become increasingly common in real-world applications. In this paper, we propose a Bayesian framework for human-in-the-loop pipelines. We consider the scenario where the final decision is an amalgamation of algorithm and expert opinions and deferral systems are a special case. We finally show that updating expert opinion priors with information sharing between experts is key to achieving superior performance. | Parth Pahwa, Kabir Thakur, Francois BuetGolfouse |  |
| 45 |  |  [The Responsibility Problem in Neural Networks with Unordered Targets](https://openreview.net/forum?id=jd7Hy1jRiv4) |  | 0 | We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data. | Ben Hayes, Charalampos Saitis, György Fazekas |  |
| 46 |  |  [Concept Understanding in Large Language Models: An Empirical Study](https://openreview.net/forum?id=losgEaOWIL7) |  | 0 | Large Language Models (LLMs) have demonstrated their superior comprehension and expressiveness across a wide range of tasks, and exhibited remarkable capabilities in real-world applications. Hence, it is crucial to investigate their potential and limitations for trustworthy performance in both academia and industry. In this paper, we focus on exploring LLMs' ability to understand concepts, especially abstract and concrete ones. To this end, we construct a WordNet-based dataset containing a subset for abstract concepts and a subset for concrete concepts. We select six pre-trained LLMs and conduct a classic NLP task, hypernym discovery, as evidence of LLMs' comprehension ability in understanding concepts. The experimental results suggest that the LLM's understanding of abstract concepts is significantly weaker than that of concrete concepts. | Jiayi Liao, Xu Chen, Lun Du |  |
| 47 |  |  [Adaptive Distance Message Passing From the Multi-Relational Edge View](https://openreview.net/forum?id=rAT51tL04I2) |  | 0 | Message-passing graph neural networks (MP-GNNs) excel in deep learning on graphs. Despite their success in various studies, they are limited by passing information to the fixed length $k$ distance neighbouring nodes, where $k$ is the number of layers. In reality, different types of edges (alternatively relations) may influence nodes at varying distance and should not be uniformly treated. This paper proposes an adaptive distance message-passing method that considers the unique roles of edge types, addressing this issue. Experiments on real-world datasets validate the effectiveness of our approach. | Zhongtian Sun, Alexandra I. Cristea, Pietro Lio, Jialin Yu |  |
| 48 |  |  [Sustainable Resource Management](https://openreview.net/forum?id=DLwlmWwmJBi) |  | 0 | Given finite resources and growing demand, a supply-side balance must be struck between maximising profit and sustainable resource management. This paper combines the two techniques in a stochastic setting to create a sustainable profit model and uses Gaussian processes to estimate and bound resource dynamics. | Nicholas William David Martin, Peter Hill, Tingsheng Tan, Francois BuetGolfouse |  |
| 49 |  |  [Chain Of Thought Prompting Under Streaming Batch: A Case Study](https://openreview.net/forum?id=n5aZMLXVndP) |  | 0 | Recently, Large Language Models (LLMs) have demonstrated remarkable capa- bilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However, developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to au- tomatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption. In this paper, we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings. | Yuxin Tang |  |
| 50 |  |  [Language Models can do Zero-Shot Visual Referring Expression Comprehension](https://openreview.net/forum?id=F7mdgA7c2zD) |  | 0 | The use of visual referring expressions is an important aspect of human-robot in- teractions. Comprehending referring expressions (ReC) like “the brown cookie near the cup” requires to understand both self-referential expressions, “brown cookie”, and relational referential expressions, “near the cup”. Large pretrained Vision-Language models like CLIP excel at handling self-referential expressions, while struggle with the latter. In this work, we reframe ReC as a language reasoning task and explore whether it can be addressed using large pretrained language models (LLMs), including GPT-3.5 and GPT-4. Given the textual attribute (object category, color, center location, size), GPT-3.5 performs unstably on understanding spatial relationships even with heavy prompt engineering, while GPT-4 shows strong and stable zero-shot relation reasoning. Evaluation on RefCOCO/g datasets and scenarios of interactive robot grasping shows that LLMs can do ReC with decent performance. It suggests a vast potential of using LLMs to enhance the reasoning in vision tasks. The code can be accessed at https://github.com/xiuchao/LLM4ReC. | Xiuchao Sui, Shaohua Li, Hong Yang, Hongyuan Zhu, Yan Wu |  |
| 51 |  |  [Simple Parameter-free Self-attention Approximation](https://openreview.net/forum?id=isodM5jTA7h) |  | 0 | The hybrid model of self-attention and convolution is one of the methods to lighten ViT. The quadratic computational complexity of self-attention with respect to token length limits the efficiency of ViT on edge devices. We propose a self-attention approximation without training parameters, called SPSA, which captures global spatial features with linear complexity. To verify the effectiveness of SPSA combined with convolution, we conduct extensive experiments on image classification and object detection tasks. The source code will be available. | Yuwen Zhai, Jing Hao, Liang Gao, Xinyu Li, Yiping Gao, Shumin Han |  |
| 52 |  |  [Neuromodulation Gated Transformer](https://openreview.net/forum?id=cYKtDg5JnxV) |  | 0 | We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets. | Kobe Knowles, Joshua Bensemann, Diana Benavides Prado, Vithya Yogarajan, Michael Witbrock, Gillian Dobbie, Yang Chen |  |
| 53 |  |  [Decomposing Causality and Fairness](https://openreview.net/forum?id=Lm7z2vYergk) |  | 0 | It is often informative to decompose key quantities of interest into smaller components, in order to develop a better understanding of the key quantity. In this paper, we focus causality and fairness, where bias attribution can be particularly useful. We show how quantities can be broken down based on independence, or conditional independence criteria, and show how such a decomposition can be used as a diagnosis tool. | Peter Hill, Francois BuetGolfouse |  |
| 54 |  |  [Self-Supervised Image Denoising with Swin Transformer](https://openreview.net/forum?id=EARgl3EH-nq) |  | 0 | Self-supervised image denoising aims to reconstruct signal from a noisy image with no additional information. Typically, this is accomplished by means of specific frameworks built upon fully-convolutional neural networks. In two such frameworks, Noise2Self and Noise2Same, we replaced conventional convolutional backbones with a state-of-the-art Swin Transformer-based model. In this paper, we summarize the results of experiments on a range of datasets and examine the advantages and limitations of transformers in self-supervised denoising. | Pavel Chizhov, Mikhail Papkov |  |
| 55 |  |  [SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data](https://openreview.net/forum?id=1wtUadpmVzu) |  | 0 | Training sophisticated machine learning (ML) models requires large datasets that are difficult or expensive to collect for many applications. If prior knowledge about system dynamics is available, mechanistic representations can be used to supplement real-world data. We present SimbaML (Simulation-Based ML), an open-source tool that unifies realistic synthetic dataset generation from ordinary differential equation-based models and the direct analysis and inclusion in ML pipelines. SimbaML conveniently enables investigating transfer learning from synthetic to real-world data, data augmentation, identifying needs for data collection, and benchmarking physics-informed ML approaches. SimbaML is available from https://pypi.org/project/simba-ml/. | Maximilian Kleissl, Lukas Drews, Benedict B. Heyder, Julian Zabbarov, Pascal Iversen, Simon Witzke, Bernhard Y. Renard, Katharina Baum |  |
| 56 |  |  [GeValDi: Generative Validation of Discriminative Models](https://openreview.net/forum?id=zwywBS3GyFs) |  | 0 | Evaluation of machine learning (ML) models is critically important for reliable use. Though typically done via unseen data, such validation datasets often need to be large and hard to procure; additionally, mutliple models may perform equally well on such datasets. To address these challenges, we offer GeValdi: a data-efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such \`\`maximally different samples'' can be constructed and leveraged to probe the failure modes of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. | Vivek Palaniappan, Matthew Ashman, Katherine M. Collins, Juyeon Heo, Adrian Weller, Umang Bhatt |  |
| 57 |  |  [Regularized Offline GFlowNets](https://openreview.net/forum?id=kbhUUAMZmQT) |  | 0 | We propose the regularized offline generative flow networks (RO-GFlowNets) that does not rely on online sampling. Since offline datasets usually cannot cover the entire state space, traditional GFlowNets cannot accurately predict the action sampling probability for each state. To address this problem, RO-GFlowNet aims to minimize the flow matching loss while regularizing the distribution distance of policy and offline datasets. Experimental results show that RO-GFlowNets perform well on offline datasets. | Haozhi Wang, Yunfeng Shao, Jianye Hao, Yinchuan Li |  |
| 58 |  |  [Secure Communication Model for Quantum Federated Learning: A Proof of Concept](https://openreview.net/forum?id=xZGPLvRpf4N) |  | 0 | We design a model of Post Quantum Cryptography (PQC) Quantum Federated Learning (QFL). We develop a proof of concept with a dynamic server selection and study convergence and security conditions. We develop a preliminary study with a proof of concept model of post-quantum secure QFL. | Dev Gurung, Shiva Raj Pokhrel, Gang Li |  |
| 59 |  |  [Learn to Select: Efficient Cross-device Federated Learning via Reinforcement Learning](https://openreview.net/forum?id=wecTsVkrjit) |  | 0 | Federated Learning (FL) is a collaborative training method that provides data privacy in the age of big data. However, it is often ineffective on edge devices due to their heterogeneous and constrained resources. The primary challenge is to identify devices with useful training data and available compute capability, which are both time-varying. In this paper, we propose FedRank, a novel federated learning approach based on reinforcement learning. Our approach addresses the device selection problem by casting it as a ranking problem and employing a pairwise training scheme. Furthermore, we leverage imitation learning from state-of-the-art algorithms to eliminate the cold start phenomenon that offsets the benefits of previous learning-based approaches. Experimental results show that FedRank improves model accuracy by 2.59\%-12.75\%, and accelerates the training process by up to 2.25$\times$ and 1.76$\times$ on average at the same time. | Chunlin Tian, Zhan Shi, Li Li |  |
| 60 |  |  [A Dynamic Prompt-tuning Method for Data Augmentation with Associated Knowledge](https://openreview.net/forum?id=hli7A0ioiS_) |  | 0 | Transformer-based pretrained language models (PLMs) have shown to pre-learn rich prior knowledge. To assist data-to-text task, we propose a new dynamic prompt tuning method, DPTAK, to retrieve knowledge from a PLM that is associated with individual data-text pairs. Our method increases the diversity of the training examples without the need to manually collecting and labelling data. When applied on GPT-2, DPTAK outperforms baseline models in several well-studied data-to-text and text-to-data datasets such as E2E, WebNLG, DART. | Qianqian Qi, Qiming Bao, Alex Yuxuan Peng, Jiamou Liu, Michael Witbrock |  |
| 61 |  |  [MARKOVIAN EMBEDDINGS FOR COALITIONAL BARGAINING GAMES](https://openreview.net/forum?id=1LeLyB6T0JM) |  | 0 | We examine the Markovian properties of coalition bargaining games, in particular, the case where past rejected proposals cannot be repeated. We propose a Markovian embedding with filtrations to render the sates Markovian and thus, fit into the framework of stochastic games. | Lucia CipolinaKun |  |
| 62 |  |  [The Point to Which Soft Actor-Critic Converges](https://openreview.net/forum?id=1_PEwKmTepo) |  | 0 | Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence. | Jianfei Ma |  |
| 63 |  |  [No Double Descent in Self-Supervised Learning](https://openreview.net/forum?id=qNJRvdKDGYg) |  | 0 | Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically in two additional previously unstudied settings using a standard and linear autoencoder. We observe the test loss has either a classical U-shape or monotonically decreases, rather than exhibiting the double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon. | Dulhan Hansaja Jayalath, Alisia Maria Lupidi, Yonatan Gideoni |  |
| 64 |  |  [Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers](https://openreview.net/forum?id=kBkou5ucR_d) |  | 0 | Annealed Importance Sampling (AIS) moves particles along a Markov chain from a tractable initial distribution to an intractable target distribution. The recently proposed Differentiable AIS (DAIS) (Geffner & Domke, 2021; Zhang et al., 2021) enables efficient optimization of the transition kernels of AIS and of the distributions. However, we observe a low effective sample size in DAIS, indicating degenerate distributions. We thus propose to extend DAIS by a resampling step inspired by Sequential Monte Carlo. Surprisingly, we find empirically—and can explain theoretically—that it is not necessary to differentiate through the resampling step which avoids gradient variance issues observed in similar approaches for Particle Filters (Maddison et al., 2017; Naesseth et al., 2018; Le et al., 2018). | Johannes Zenn, Robert Bamler |  |
| 65 |  |  [Generalised Lookahead Optimiser](https://openreview.net/forum?id=uNrSvEr9Lqc) |  | 0 | The vast majority of deep learning models are trained using SGD or one of its variants. Zhang et al. (2019) suggested the Lookahead optimiser as an alternative which enjoys remarkable test performance on many established datasets and mod- els. In this work we investigate a generalisation of this optimisation method. We validate the method empirically, generally demonstrating better results and faster convergence relative to the baselines of SGD and Lookahead | CostinAndrei Oncescu, Jack Valmadre, João F. Henriques |  |
| 66 |  |  [Revisiting CounteRGAN for Counterfactual Explainability of Graphs](https://openreview.net/forum?id=d0m0Rl15q3g) |  | 0 | Counterfactual explainability (CE) has been widely explored in various domains ranging from medical image diagnosis to self-driving cars. Graph CE (GCE), on the other hand, and especially, generative-based GCE has yet to be explored. Here, we adapt CounteRGAN, an image-based generative approach, to consider graph adjacency matrices as special black-and-white images and sample valid counterfactuals directly from the learnt latent space probabilistic distribution. | Mario Alfonso PradoRomero, Bardh Prenkaj, Giovanni Stilo |  |
| 67 |  |  [Language Models Inversely Scale on Piecewise Function Evaluation with Biased Examples](https://openreview.net/forum?id=GJhsHNKm7kj) |  | 0 | We investigate whether pretrained language models (LMs) can be misled by providing them with factually correct, but unrepresentative/biased examples, in the context of integer-to-integer piecewise functions. Given the definition of a piecewise function and several examples of the function’s evaluation, we instruct LMs to apply the function to a new input. We assess LMs on two variants of this task: one where the example function evaluations are evenly distributed across both branches of the function, and one where all of the examples exercise one branch of the input and the target input exercises the other branch. We observe that model performance positively scales with model size only when examples are balanced, and that performance inversely scales with size when the examples are misrepresentative. | Bradley C. A. Brown, Jordan Juravsky, Atif Mahmud, Wais Shahbaz, Ryan Ehrlich |  |
| 68 |  |  [RETHINKING POSITIONAL EMBEDDING: A CASE STUDY IN TEMPORAL EVENT SEQUENCE MODELLING](https://openreview.net/forum?id=iF2w_kqmYmw) |  | 0 | In this paper, we present a time-decaying encoding as an alternative to sinusoidal positional encoding in the transformer architecture. We evaluate our approach in the context of an educational domain involving 14,043 question-solving interactions from 1,260 students. We argue that including time-based attention can be beneficial for event sequence modeling applications where the inter-event time intervals and the interpretation of the model's prediction both are crucial. | Effat Farhana |  |
| 69 |  |  [Exploring Efficient and Simple Initialization Strategies for Bayesian Optimization with SETUP-BO](https://openreview.net/forum?id=TFrzVBZk05g) |  | 0 | This paper studies the effectiveness of random and grid initialization strategies in SETUP-BO, a self-tuning Bayesian optimization algorithm. Our experiments on benchmark functions compare the performance of these initialization strategies to deterministic initialization. The results show that random initialization outperforms other methods, indicating that it can enhance the performance of BO. | Seyed Ali YaghoubNejad, Mohammad Taghi Manzuri |  |
| 70 |  |  [Quota Constraints for Diversity Interventions in Subset Selection](https://openreview.net/forum?id=x4MuUFPKEIj) |  | 0 | The combinatorial optimization problem of subset selection is often modeled as maximizing a set function that captures inter-element dependencies under some capacity/matroid constraints. In this paper, we examine this problem under “quota constraints” where the selected subset must meet some minimum group-wise quotas. We provide algorithms with guarantees for two popular scenarios extended to the quota-constrained setting and make an empirical case for their applicability to fair subset selection. | Neeraja Abhyankar |  |
| 71 |  |  [Mitigating Metastable Failures in Distributed Systems with Offline Reinforcement Learning](https://openreview.net/forum?id=zYF6NLJl6LM) |  | 0 | This paper introduces a load-shedding mechanism that mitigates metastable failures through offline reinforcement learning (RL). Previous studies have heavily focused on heuristics that are reactive and limited in generalization, while online RL algorithms face challenges in accurately simulating system dynamics and acquiring data with sufficient coverage. In contrast, our algorithm leverages offline RL to learn directly from existing log data. Through extensive empirical experiments, we demonstrate that our algorithm outperforms rule-based methods and supervised learning algorithms in a proactive, adaptive, generalizable, and safe manner. Deployed in a Java compute service with diverse execution times and configurations, our algorithm exhibits faster reaction time and attains the Pareto frontier between throughput and tail latency. | Yueying Li, Daochen Zha, Tianjun Zhang, G. Edward Suh, Christina Delimitrou, Francis Y. Yan |  |
| 72 |  |  [Knowledge Distillation of BERT Language Model on the Arabic Language](https://openreview.net/forum?id=-bMH1Sk8SSF) |  | 0 | The absence of good Arabic language models led to significant setbacks in the Arabic language related tasks and lag with respect to robustness and accuracy. While a pre-trained version of BERT on Arabic language is available, a smaller distilled version could be proven to be highly scalable. In this research paper, we propose the development of a smaller and more efficient version of BERT, known as DistilBERT for the Arabic language for the pursuit of achieving comparable results with significantly less computational resources. Employing knowledge distillation to create a compact model allows for wider implementation, even in areas with limited computational resources. Ultimately, this project aims to break down language barriers, bring greater inclusivity and improve the accessibility of the Arabic language in NLP applications worldwide. This project serves as a starting point for further research and investigation of the performance of the Arabic DistilBERT model across various NLP tasks. | Hager Adil, Abrar Elidrisi, Muhammed Saeed |  |
| 73 |  |  [Offensiveness as an Opinion: Dissecting population-level Label Distributions](https://openreview.net/forum?id=DoOiwBcRir3) |  | 0 |  | Tharindu Cyril Weerasooriya, Sarah Luger, Yu Liang, Christopher M. Homan |  |
| 74 |  |  [How do ConvNets Understand Image Intensity?](https://openreview.net/forum?id=z2Gr8YqsimF) |  | 0 | Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization. | Jackson Kaunismaa, Michael Guerzhoy |  |
| 75 |  |  [Learning Weight Sensitivity from Entropy](https://openreview.net/forum?id=x_adzmY6PQ5) |  | 0 | Multiple network pruning methods have used connection sensitivity of each weight to prune their network. This paper proposes a meta-learning approach to learn sensitivity of weights based on their entropy, or change, during the training phase. We have experimentally shown the validity of such an approach. | Novena Agnes |  |
| 76 |  |  [A Study on Sample Diversity in Generative Models: GANs vs. Diffusion Models](https://openreview.net/forum?id=BQpCuJoMykZ) |  | 0 | In this project, we compare the sample diversity of two generative models: Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs). GANs have achieved impressive results in generating high-quality samples, but have been known to suffer from the issue of mode collapse, which can result in a lack of sample diversity. Mode collapse occurs when the generator network in a GAN becomes stuck in a local minimum, causing it to produce samples that are similar to each other rather than sampling from the full range of possibilities in the target distribution. This can lead to a lack of sample diversity, as the generator is unable to explore and represent the full range of features in the data. DDPMs, on the other hand, have demonstrated improved sample diversity compared to GANs. We conducted experiments using both synthetic and image data to explore the connection between mode collapse and sample diversity in these two frameworks. Our findings indicate that by addressing the mode collapse problem, DDPM preserves a comprehensive representation of the distribution. | Reza Bayat |  |
| 77 |  |  [Artificial Intelligent Life: A New Perspective on Artificial General Intelligence](https://openreview.net/forum?id=XCbe51-arJt) |  | 0 | We propose artificial intelligent life (AILife) as a new perspective to approach artificial general intelligence, similar to a living organism in reality. Unlike machine learning approaches that focus on reward functions and mathematical optimizations, AILife seeks to develop an artificial organism that learns by the mechanism of biological neurons. AILife is composed of a biological-like neuron system to learn from interactions with the world, a sensory system to feel the world, and actuators to perform activities. We show a toy example to explain AILife. | Borui Cai, Yong Xiang, Yao Zhao |  |
| 78 |  |  [Generative AI for Therapy? Opportunities and Barriers for ChatGPT in Speech-Language Therapy](https://openreview.net/forum?id=cRZSr6Tpr1S) |  | 0 | Speech-language pathologists (SLPs) are health professionals who work with children and adults with various communication disorders in areas such as speech, language, hearing, and voice. The rise of voice assistants and chatbots brings new opportunities for SLPs and points to new opportunities and barriers when adopted during clinical service delivery. This paper explores the potential adoption of ChatGPT in speech-language therapy for individuals with receptive and expressive language disorders. By offering SLP critique for ChatGPT's responses to multiple therapeutic use cases, limitations and solutions for improving generative AI tools for speech-language therapy are also discussed. | Yao Du, Felix JuefeiXu |  |
| 79 |  |  [The Art of Embedding Fusion: Optimizing Hate Speech Detection](https://openreview.net/forum?id=1yXbt6_o6av) |  | 0 | Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. | Mohammad Aflah Khan, Neemesh Yadav, Mohit Jain, Sanyam Goyal |  |
| 80 |  |  [COLLABORATIVE CONCEPT DRIFT DETECTION](https://openreview.net/forum?id=STpRX-XCO6t) |  | 0 | Collaborative Concept Drift Detection (C2D2) combines Fast Correlated Based Filtering (FCBF) and Singular Value Decomposition (SVD) to detect concept drifts in 5 synthetic datasets. We compare our results against 6 diveregence tests and introduce Performance Gain Update Cost Ratio (PGUCR). Post-hoc Tukey HSD test confirmed that C2D2 outperformed the other tests in terms of PGUCR. Much of C2D2’s improvement is based on its conservative signals for updates. | Beverly Abadines Quon, JeanLuc Gaudiot |  |
| 81 |  |  [End-to-End Learnable Masks With Differentiable Indexing](https://openreview.net/forum?id=EyliiBqhFz) |  | 0 | An essential step towards developing efficient learning algorithms involves being able to work with as little data as possible to achieve good performance. For this reason, sparse representation learning is a crucial avenue of computer vision research. However, sparsity-inducing methods like importance sampling rely on non-differentiable operators like masking or top-K selection. While several tricks have been proposed for getting gradients to flow ‘through’ the pixels selected by the operators, the actual indices for which pixels are masked or selected are non-differentiable and thus cannot be learned end-to-end. We propose three methods for making operations like masking and top-k selection fully differentiable by allowing gradients to flow through the operator indices and showing how they can be optimized end-to-end using backpropagation. As a result, all three methods can be used as simple layers or submodules in existing neural network libraries. | Dibyanshu Shekhar, Sree Harsha Nelaturu, Ashwath Shetty, Ilia Sucholutsky |  |
| 82 |  |  [FigGen: Text to Scientific Figure Generation](https://openreview.net/forum?id=Hx_iTXnCR5) |  | 0 | The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion | Juan A. Rodríguez, David Vázquez, Issam H. Laradji, Marco Pedersoli, Pau Rodríguez |  |
| 83 |  |  [Evaluating Impact of Emoticons and Pre-processing on Sentiment Classification of Translated African Tweets](https://openreview.net/forum?id=OMARmh02Ruk) |  | 0 | This paper examines the impact of emoticons and pre-processing on sentiment classification for English translations of 11 African languages. Using AfriSenti-SemEval datasets, Roberta and Twitter-Roberta models are fine-tuned, and standard classification metrics are used to assess performance. The study concludes no significant performance differences with emoticons and pre-processing and no distinction between standard Roberta and domain-specific Twitter-Roberta. | Saurav Keshari Aryal, Gaurav Adhikari |  |
| 84 |  |  [SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels](https://openreview.net/forum?id=OiSbJbVWBJT) |  | 0 | Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility. | Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, YoungBin Kim |  |
| 85 |  |  [MatPropXtractor: Generate to Extract](https://openreview.net/forum?id=5CdkvFyatt2) |  | 0 | The field of materials science has amassed a wealth of information about materials in text publications, however, such information is often confined within the publication. A lack of standardized structure and naming consistency preclude the information from being effectively utilized for research and discovery. We introduce MatPropXtractor, an extraction system that uses pre-trained large language models (LLMs) in a generative setting to extract materials and their properties as reported in the materials science literature. MatPropXtractor consists of a three-step pipeline that includes 1) a document selection tool to identify related articles, 2) a paragraph classifier to identify passages containing important materials properties, and 3) a property extractor exploiting in-context learning in GPT-3. MatPropXtractor extracted 154 material-property pairs from five materials science papers. The extracted pairs were analyzed by an expert and obtained an average precision of 72.73% on paragraph classification and an average precision of 56.7% precision on material-property identification. | Aswathy Ajith, Marcus Schwarting, Zhi Hong, Kyle Chard, Ian T. Foster |  |
| 86 |  |  [Zero-Shot Classification Reveals Potential Positive Sentiment Bias in African Languages Translations](https://openreview.net/forum?id=-AIukSeLAz9) |  | 0 | Natural Language Processing research into African languages has been limited, with over 2000 languages still needing to be studied. We employ the AfriSenti-SemEval dataset, a recently released resource that provides annotated tweets across 13 African languages, for sentiment analysis to address this. However, given the persistent data limitations for specific languages, we translate each language to English and conduct zero-shot classification using a large BART model trained with three candidate labels: positive, neutral, and negative. Intriguingly, our findings indicate that all tweets are classified as positive. Further investigation into prediction probabilities reveals that translation technologies may exhibit a bias in translating African languages toward positive sentiments. This observation highlights the potential impact of translation tools on sentiment analysis and warrants further examination. | Hrishav Sapkota, Saurav Keshari Aryal, Howard Prioleau |  |
| 87 |  |  [Lesion Search with Self-supervised Learning](https://openreview.net/forum?id=c-YTzVUkfAW) |  | 0 | Content-based image retrieval (CBIR) with self-supervised learning (SSL) accelerates clinicians’ interpretation of similar images without manual annotations. We develop a CBIR from the contrastive learning SimCLR and incorporate a generalized-mean (GeM) pooling followed by L2 normalization to classify lesion types and retrieve similar images before clinicians' analysis. Results have shown improved performance. We additionally build an open-source application for image analysis and retrieval. The application is easy to integrate, relieving manual efforts and suggesting the potential to support clinicians’ everyday activities. | Kristin Qi, Jiali Cheng, Daniel Haehn |  |
| 88 |  |  [Feature Importance Analysis for Mini Mental Status Score Prediction in Alzheimer's Disease](https://openreview.net/forum?id=GPA-BPLwYHf) |  | 0 | This research article proposes developing predictive models to forecast Mini-Mental State Exam (MMSE) scores using the 54 most important features identified from the current state-of-the-art model. The study employs the SHapley Additive exPlanations (SHAP) method to explore feature importance and interpret model performance. The analysis shows that the Automated Readability Index (ARI) is the most influential feature in predicting MMSE scores. This finding suggests that ARI's capability to capture language impairment and morphosyntax is valuable in predicting cognitive decline in dementia patients. Although the analysis could not evaluate all features, this study provides a foundation for future investigations into features that may assist in predicting MMSE scores and the onset of Dementia. | Howard Prioleau, Saurav Keshari Aryal |  |
| 89 |  |  [On a Relation Between the Rate-Distortion Function and Optimal Transport](https://openreview.net/forum?id=1F8pPnUinbU) |  | 0 | We discuss a relationship between rate-distortion and optimal transport (OT) theory, even though they seem to be unrelated at first glance. In particular, we show that a function defined via an extremal entropic OT distance is equivalent to the rate-distortion function. We numerically verify this result as well as previous results that connect the Monge and Kantorovich problems to optimal scalar quantization. Thus, we unify solving scalar quantization and rate-distortion functions in an alternative fashion by using their respective optimal transport solvers. | Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti |  |
| 90 |  |  [Is CLIP Fooled by Optical Illusions?](https://openreview.net/forum?id=YdGkE4Ugg2C) |  | 0 | Recent large machine learning models such as CLIP have shown impressive generalization performance for various perception tasks. In this work, we explore to what extent they model the human cognitive process. We focus our attention on how these models perceive optical illusions. We present a simple way to assess the effect by presenting illusions in the form of image and text prompts while observing the changes in models’ output under different illusory strengths. Our results show that CLIP can indeed be fooled by different types of illusions relating to lightness and geometry. | Jerry Ngo, Swami Sankaranarayanan, Phillip Isola |  |
| 91 |  |  [Seeing in Words: Learning to Classify through Language Bottlenecks](https://openreview.net/forum?id=_QreMdMNIz-) |  | 0 | Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks. In contrast, humans can explain their predictions using succinct and intuitive descriptions. To incorporate explainability into neural networks, we train a vision model whose feature representations are text. We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it. | Khalid Saifullah, Yuxin Wen, Jonas Geiping, Micah Goldblum, Tom Goldstein |  |
| 92 |  |  [Towards Stochastic Gradient Variance Reduction by Solving a Filtering Problem](https://openreview.net/forum?id=0sxmoci9Ma) |  | 0 | Stochastic gradient descent is commonly used to optimize deep neural networks, but it often produces noisy and unreliable gradient estimates that hinder convergence. To address this issue, we introduce \textbf{Filter Gradient Descent} (FGD), a family of stochastic optimization algorithms that consistently estimate the local gradient by solving an adaptive filtering problem. By incorporating historical states, FGD reduces the variance in stochastic gradient descent and improves the current estimation. We demonstrate the efficacy of FGD in numerical optimization and neural network training, where it outperforms traditional momentum-based methods in terms of robustness and performance. Code is available at \url{https://github.com/Adamdad/Filter-Gradient-Decent}. | Xingyi Yang |  |
| 93 |  |  [MACHINE TRANSLATION BASELINES FOR ARABIC - SWAHILI](https://openreview.net/forum?id=aVepdnlRb5) |  | 0 | Building neural machine translation (NMT) systems for low-resource languages poses several challenges, mainly due to the lack of parallel data. In this research, we propose a baseline NMT system for translating between Arabic and Swahili. Despite being spoken by nearly 300 million individuals worldwide, the parallel corpus between these two languages is severely underrepresented. To address this, we scraped and processed the largest high-quality parallel corpus of Swahili and Arabic to our knowledge. We then used state-of-the-art NMT models, including Transformers and multilingual variants of Transformers, to build a baseline for bidirectional Arabic-Swahili NMT. Finally, we report an increase in the performance of our NMT system using the back-translation technique. | Asim Awad Osman, Ahmed Emadeldin Almahady, Muhammed Saeed, Hiba Hassan Sayed |  |
| 94 |  |  [Accuracy of white box and black box adversarial attacks on a sign activation 01 loss neural network ensemble](https://openreview.net/forum?id=QimsmhYvsf) |  | 0 | In this work we ask the question: is an ensemble of single hidden layer sign activation 01 loss networks more robust to white box and black box adversarial attacks than an ensemble of its differentiable counterpart of cross-entropy loss with relu activations and an ensemble of the approximate differentiable counterpart of cross-entropy loss with sign activations? We consider a simple experimental setting of attacking models trained for binary classification on pairwise CIFAR10 datasets - altogether a total of 45 datasets. We study ensembles of {\bf bcebp}: binary cross-entropy loss with relu activations trained with back-propagation, {\bf bceban}: binary cross-entropy loss with sign activations trained with back-propagation with the straight through estimator gradient, {\bf 01scd}: 01-loss with sign activations trained with gradient-free stochastic coordinate descent, and {\bf bcescd}: binary cross-entropy loss with relu activation trained with gradient-free stochastic coordinate descent (to isolate the effect of 01 loss from gradient-free training). We train each model in an ensemble with a different random number generator seed. Our four models have similar mean test accuracies in the mid to high 80s on pairwise CIFAR10 datasets but under powerful PGD white-box attacks they each drop to near 0\% except for our 01 loss network ensemble that has 31\% accuracy. Even training with the gradient-free stochastic coordinate descent can be attacked thus suggesting that the defense lies in 01 loss. In a black-box transfer attack we find adversaries produced from the bcebp model fully transfer to bceban but much less to 01scd - we see the same transferability pattern from bceban to bcebp and 01scd. We also find that adversaries from 01scd barely transfer to bcebp and bceban. While our results are far from those of multi-class and convolutional networks, they suggest that 01 loss models are hard to attack naturally without any adversarial training. All models, data, and code to reproduce results here are available from \url{https://github.com/xyzacademic/mlp01example}. | Yunzhe Xue, Usman Roshan |  |
| 95 |  |  [TopEx: Topic-based Explanations for Model Comparison](https://openreview.net/forum?id=AidIUjh__t) |  | 0 | Meaningfully comparing language models is challenging with current explanation methods. Current explanations are overwhelming for humans due to large vocabularies or incomparable across models. We present TopEx, an explanation method that enables a level playing field for comparing language models via model-agnostic topics. We demonstrate how TopEx can identify similarities and differences between DistilRoBERTa and GPT-2 on a variety of NLP tasks. | Shreya Havaldar, Adam Stein, Eric Wong, Lyle H. Ungar |  |
| 96 |  |  [GeneDAE: A Sparse Denoising Autoencoder for Deriving Interpretable Gene Embeddings](https://openreview.net/forum?id=cxmjk8O3Yn) |  | 0 | A challenge in genomics research involves identifying functionally relevant genes associated with diseases. We present GeneDAE, a sparse denoising autoencoder that extracts gene representations from large-scale population-level genotype data, which can then be used to identify gene-to-disease associations. The GeneDAE encoder and decoder connections are modeled on a bipartite biological knowledge graph that connects individual variants (single nucleotide polymorphisms; SNPs) to their nearby genes, enabling each node in the hidden layer to be used as an interpretable, multi-purpose gene embedding derived using information only from variants in close proximity that are most likely to impact gene function. We use the UK Biobank dataset and focus on the major histone compatibility complex (MHC) region of the genome, which is critical to immune function and autoimmune disease pathophysiology. Using GeneDAE, we extracted 239 MHC gene embeddings and identified novel gene-to-disease associations. | Monica Isgut, Neha Jain, Andrew Hornback, Karan Samel, May Dongmei Wang |  |
| 97 |  |  [Improving Hyperspectral Adversarial Robustness Under Multiple Attacks](https://openreview.net/forum?id=XHfWgU2IiP) |  | 0 | Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network. | Nicholas Soucy, Salimeh Yasaei Sekeh |  |
| 98 |  |  [Prompt Programming for the Visual Domain](https://openreview.net/forum?id=hBz5h3C9Sq) |  | 0 | In this work, we ask how text-to-image synthesis via large language models can effectively probe imagery that embodies fidelity and imagination. We investigate this question in the context of prompts (writing to language models) in a novel probing mechanism known as prompt programming, or programming in natural language. We start by refining existing techniques to characterize the effect of templates on visual fidelity then hone in on approaches to capture holistic nuances within the visual domain. We present a systematic analysis of prompt engineering for visual image generation. | Alayt Issak, Lav R. Varshney |  |
| 99 |  |  [Mapping the Typographic Latent Space of Digits](https://openreview.net/forum?id=ufA2FuCGyz) |  | 0 | Since the advancement of handwritten text to typefaces on a computer, the human mind has evolved towards corresponding various typefaces as norms of comprehension. Current-day typefaces, much like those written by hand, exist in disparities and are governed by consensus reached among Typographers. Currently, the PANOSE system, developed in 1998, is the most widely used and accepted method for classifying typefaces based on 10 visual attributes. In this work, we employ Disentangled Beta-VAE's, in an unsupervised learning approach, to map the latent feature space with a dataset of MNIST Style Typographic Images (TMNIST-Digit) of 0-9 digits across 2990 unique font styles. We expose the learning representation across a variety of font styles to enable typographers to contemplate and identify new attributes to their classification system. | Alayt Issak, Sarthak Kakkar, Sair Goetz, Nik Bear Brown, Casper Harteveld |  |
| 100 |  |  [Hyperbolic Deep Reinforcement Learning for Continuous Control](https://openreview.net/forum?id=Mrz9PgP3sT) |  | 0 | Integrating hyperbolic representations with Deep Reinforcement Learning (DRL) has recently been proposed as a promising approach for enhancing generalization and sample-efficiency in discrete control tasks. In this work, we extend hyperbolic RL to continuous control by introducing a novel hyperbolic actor-critic model. Empirically, our simple implementation outperforms its Euclidean counterpart, with significant gains on 16/24 tasks from the DeepMind Control Suite with pixel inputs. Notably, in the low-data regime, our method even outperforms several pre-trained unsupervised RL agents. Our findings show that hyperbolic representations provide a valuable inductive bias for continuous control. | Omar Salemohamed, Edoardo Cetin, Sai Rajeswar, Arnab Kumar Mondal |  |
| 101 |  |  [Incorporating Expert Prior Knowledge for Oral Lesion Recognition](https://openreview.net/forum?id=XXsjViheWZ) |  | 0 | External information may improve predictive accuracy and uncertainty in medical image recognition. For example in oral lesion recognition, some lesion types are implausible to occur at certain anatomical locations. We propose a strategy to induce the prior knowledge about such correlations using an additional loss term that optimizes for plausible lesion types given an anatomical location. Our results suggests an improvement in model calibration, reduction in predicted number of implausible classes and improved uncertainty estimation for implausible predictions. | Camille Besombes, Adeetya Patel, Sreenath Arekunnath Madathil |  |
| 102 |  |  [LEARNING LIGHTWEIGHT STRUCTURE-AWARE EMBEDDINGS FOR PROTEIN SEQUENCES](https://openreview.net/forum?id=2M8dEAJcG5) |  | 0 | Machine learning models, such as AlphaFold, have recently demonstrated remarkable accuracy in predicting the structures of protein sequences. This capability enables their use as oracles for providing structure-based information to aid other learning tasks. In this study, we investigate the use of deep learning embeddings and explore the feasibility of developing a structure-aware protein sequence embedding. To accomplish this, we employ S4PRED and ESMFold, two models that predict protein secondary (2D) and tertiary structures (3D) respectively, directly from single sequences. These models act as oracles to form structure-aware embeddings through an autoencoder. We then compare this approach to purely sequence-based embeddings in a Protein-Protein Interaction (PPI)-prediction task. Our findings highlight the potential advantages of employing structure embeddings and provide grounds for future research directions. | Sidharth Lakshmanan, Philip J. Y. Leung, Jeffrey Nivala |  |
| 103 |  |  [Proactive policing as reinforcement learning](https://openreview.net/forum?id=lmcPpHDa0B) |  | 0 | Recent analyses of predictive policing have shown the inherent biases in such systems. We show that the models considered in fact apply to proactive policing in general, which can be also viewed as a reinforcement learning system, and thus may also lead to over-policing. | Dawson Kinsman, Tian An Wong |  |
| 104 |  |  [The Small Batch Size Anomaly in Multistep Deep Reinforcement Learning](https://openreview.net/forum?id=G0heahVv5Y) |  | 0 | We present a surprising discovery: in deep reinforcement learning, decreasing the batch size during training can dramatically improve the agent's performance when combined with multi-step learning. Both reducing batch sizes and increasing the update horizon increase the variance of the gradients, so it is quite surprising that increased variance on two fronts yields improved performance. We perform a wide range of experiments to gain a better understanding of this phenomenon, which we denote variance double-down. | Johan S. ObandoCeron, Marc G. Bellemare, Pablo Samuel Castro |  |
| 105 |  |  [Bootstrapping Parallel Anchors for Relative Representations](https://openreview.net/forum?id=VBuUL2IWlq) |  | 0 | The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks. | Irene Cannistraci, Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Emanuele Rodolà |  |
| 106 |  |  [A Brief History of the Speculative Measures for Autonomy](https://openreview.net/forum?id=qqKO_rrg9y) |  | 0 | This paper presents a novel summary of the history of the evolution of the measures for autonomy (i.e. self-legislating systems), from Creation myths to the study of technological autonomy, progressing through five interrelated phases. First, the original legislator of the laws of nature is considered as the singular measure for autonomous existence. Second, a set of hierarchical governing systems are conceived as a sort of supplement to the limitations of the original monopolistic autonomy. Third, this hierarchy of governors is inverted by transcendental philosophy, putting emphasis on the immediate conscious “self” - the auto - in self-legislation. The fourth stage emerges in existential philosophy, which notices the seemingly inescapable paradoxes of this introverted autonomy, as all justice and justification becomes circularly self-justified. The fifth and most contemporary measure of autonomy universalizes this existential insight and measures each being’s autonomy simply in proportion to its own alienated independence. The paper concludes with an analysis on the potential limits of this universalized autonomy and suggests a route for future research which may ultimately separate the measure of autonomy from the measure of responsibility for justice. | Micah Tewers |  |
| 107 |  |  [Semantic feature verification in FLAN-T5](https://openreview.net/forum?id=_1z2Bqte5L) |  | 0 | This study evaluates the potential of a large language model for aiding in generation of semantic feature norms–a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines. | Siddharth Suresh, Kushin Mukherjee, Timothy T. Rogers |  |
| 108 |  |  [Augmenting Collective Intelligence through Belbin's Team Roles](https://openreview.net/forum?id=XiZOalwf_U) |  | 0 | Augmented Collective Intelligence (ACI) allows organizations to improve performance by combining human and artificial intelligence in teams. We use Belbin’s Team Roles, a popular framework that identifies nine clusters of behavioral attributes, to explore how roles might be augmented by AI tools. Each role is rated as having low, medium, or high potential for augmentation at current system capabilities. The paper concludes that developing an ACI strategy requires leaders to understand human and AI capabilities and how each evolves over time. | Abhishek Gupta, Emily Dardaman |  |
| 109 |  |  [Text2Face: 3D Morphable Faces From Text](https://openreview.net/forum?id=7Zyv70nGl_g) |  | 0 | We present the first 3D morphable modelling approach, whereby 3D face shape can be directly and completely defined using a textual prompt. Building on work in multi-modal learning, we extend the FLAME head model to a common image-and-text latent space. This allows for direct 3D Morphable Model (3DMM) parameter generation and therefore shape manipulation from textual descriptions. Our method, Text2Face, has many applications; for example: generating police photofits where the input is already in natural language. It further enables multi- modal 3DMM image fitting to sketches and sculptures, as well as images. | Will Rowan, Patrik Huber, Nick E. Pears, Andrew Keeling |  |
| 110 |  |  [Towards Parametric Robust Activation Functions in Adversarial Machine Learning](https://openreview.net/forum?id=oKa5_mxHBV) |  | 0 | Machine learning's vulnerability to adversarial perturbations has been argued to stem from a learning model's non-local generalization over complex input data. Given the incomplete information in a complex dataset, a learning model captures non-linear patterns between data points with volatility in the loss surface and exploitable areas of low-confidence knowledge. It is the responsibility of activation functions to capture the non-linearity in data and, thus, has inspired disjointed research efforts to create robust activation functions. This work unifies the properties of activation functions that contribute to robust generalization with the generalized gamma distribution function. We show that combining the disjointed characteristics presented in the literature provides more effective robustness than the individual characteristics alone. | Sheila Alemany, Alberto Luis Dominguez, Ilan Grapel, Niki Pissinou |  |
| 111 |  |  [Understanding Label Bias in Single Positive Multi-Label Learning](https://openreview.net/forum?id=iWiwox99aJ) |  | 0 | Annotating data for multi-label classification is prohibitively expensive because every category of interest must be confirmed to be either present or absent. Recent work on single positive multi-label (SPML) learning has shown that it is possible to train effective multi-label classifiers using only one positive label per image. The standard benchmarks for SPML are derived from traditional multi-label classification datasets by retaining one positive label for each training example (chosen uniformly at random) and discarding all other labels. However, in realistic annotation settings it is not likely that positive labels are chosen uniformly at random. In this work, we explore the effect of label bias in SPML. | Julio Arroyo, Pietro Perona, Elijah Cole |  |
| 112 |  |  [Knowledge and Attitude of Medical Students and Doctors towards Artificial Intelligence: A study of University of Ilorin](https://openreview.net/forum?id=5lZaexgIey) |  | 0 | This study assesses the knowledge and attitudes of medical students and doctors in University of Ilorin toward Artificial Intelligence (AI) in medical education. It involved a cross-sectional study using an online survey consisting of close-ended questions. The survey targeted medical students at all medical levels and doctors in their postgraduate training. total of 481 medical students and doctors responded. When assessing AI knowledge sources (86.5%) got their information from the media as compared to (13.5%) from medical school curriculum. However, students who learned the basics of AI while being in medical school were more knowledge about AI than their peers who did not and were more interested in the applications of AI in healthcare. The advancements in AI affected the choice of specialty of around a quarter of the students (26.8%). Finally, less than a quarter of students (22.1%) want to be assessed by AI, even though about half (57.7%) reported that assessment by AI is more objective. In conclusion, Although most physicians and medical students do not sufficiently understand AI and its significance in the medical field, they have favorable views regarding using AI in the medical field and a willing to learn the fundamentals. Nigerians medical authorities and international organizations should suggest including artificial intelligence in the medical field, particularly when training residents and fellowship physicians. | Abdulhameed Abiola Dere |  |
| 113 |  |  [Human-machine cooperation for semantic feature listing](https://openreview.net/forum?id=K-SVVOIcsP) |  | 0 | Semantic feature norms — lists of features that concepts do and do not possess — have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms. | Kushin Mukherjee, Siddharth Suresh, Timothy T. Rogers |  |
| 114 |  |  [Improving generalization by loss modification](https://openreview.net/forum?id=vHOO1lxggJ) |  | 0 | What data points from available data set should be used for training? For all subsets of available data it will generally make different solutions. We show that a simple loss modification allows to find a single solution that represents data set properties and not particular selections of data points thus improving the generalization performance. | Michael Tetelman |  |
| 115 |  |  [A Rate-Distortion View on Model Updates](https://openreview.net/forum?id=6ry6ibTKOx) |  | 0 | Compressing model updates is critical for reducing communication costs in federated learning. We examine the problem using rate--distortion theory to present a compression method that is near-optimal in many use cases. We empirically show that common transforms applied to model updates in standard compression algorithms, normalization in QSGD and random rotation in DRIVE, yield sub-optimal compressed representations in practice. | Nicole Mitchell, Johannes Ballé, Zachary Charles, Jakub Konecný |  |
| 116 |  |  [Text-Based Games as a Challenging Benchmark for Large Language Models](https://openreview.net/forum?id=2g4m5S_knF) |  | 0 | Text-based games (TBG) are puzzle-solving, interactive dialogue language tasks that have the potential to become a challenging intelligence benchmark for large language models (LLMs). TBGs are similar to interactive dialogue, as they require the capability for bidirectional communication in natural language, while at the same time being straightforward to evaluate in terms of performance, as a score clearly indicates progress in TBGs. We conduct preliminary experiments on FLAN-T5, Turing, and OPT language models to test their puzzle-solving abilities using an \textit{easy} TBG called \`\`Detective''. Our results suggest that LLMs underperform in comparison with state-of-the-art and human performance. We discuss the potential reasons behind the performance gap, such as the complexity of turning TBGs into prompts, LLMs not learning from past trials, their lack of memory, and LLMs relying on statistical prediction instead of goal orientation. | Qinyue Tan, Ashkan Kazemi, Rada Mihalcea |  |
| 117 |  |  [Learned Learning Rate Schedules for Deep Neural Network Training Using Reinforcement Learning](https://openreview.net/forum?id=0Zhwu1VaOs) |  | 0 | We present a novel strategy to generate learned learning rate schedules for any optimizer using reinforcement learning (RL). Our approach trains a Proximal Policy Optimization (PPO) agent to predict optimal learning rate schedules for SGD, which we compare with other optimizer-scheduler combinations and full grid search. Our experiments show that the agent learns to generate dynamic schedules that result in stable, non-divergent loss histories, and can be more useful in practice than equally-expensive Hyperparameter Optimization and fixed optimizer-scheduler combinations. | Shreyas Subramanian, Vignesh Ganapathiraman, Aly El Gamal |  |
| 118 |  |  [Speaker-Invariant Speech Recognition through Fine-Tuning on Individual-Specific Data with Voice Conversion](https://openreview.net/forum?id=CTZigc9V69) |  | 0 | In this paper, we propose a speaker-invariant speech recognition method that fine-tunes a pre-trained model (Obtained by a self-supervised learning method) on a selected subset of data containing speech from a specific individual. This fine-tuning changes the network's behavior, allowing it to focus on information that is important for tasks such as ASR and phoneme recognition while reducing sensitivity to speaker-specific vocal characteristics. In the test time, we recommend employing voice conversion techniques to transform the voices of diverse individuals to match that of the individual used for training. | Samin Heydarian, Tohid Abedini, Alireza Morsali, Moein Heidari |  |
| 119 |  |  [Effect of training fragment length on Transformers in text complexity prediction](https://openreview.net/forum?id=SBqbPjVFfm) |  | 0 | With the myriad practical applications of text complexity classification, it is important to optimize the training text fragment size for performance. We experiment with fine-tuning pre-trained BERT models to classify the complexity of Russian school text using different fragment sizes for training. | Rafik Hachana, Vladimir V. Ivanov |  |
| 120 |  |  [Revisiting Bisimulation: A Sampling-Based State Similarity Pseudo-metric](https://openreview.net/forum?id=lkWvTn2IzA) |  | 0 | In reinforcement learning (RL), we typically deal with systems with large or continuous states encoded in an unstructured way. Because it is not possible to represent the value of each state, it is necessary to learn a structured representation from limited state samples to express the value function in a more meaningful way. One approach to do so is to endow the set of states with a behavioral metric, such that two states that are close in the metric space are also close in the space of value functions. While there exists some notions of state similarity, they are either not amenable to sample-based algorithms \citep{ferns2004metrics, ferns05metrics}, need additional assumptions \citep{castro2020scalable, zhang2020learning, agarwal2021pse} or yield limited theoretical guarantees \citep{castro2021mico}. In this paper, we present a new behavioural pseudo-metric, PMiCo, to overcome these shortcomings. PMiCo is based on a recent sampling-based behavioural distance, MICo \citep[Matching under Independent Couplings;][]{castro2021mico}, but enjoys more interesting theoretical properties, which we also illustrate empirically. | Charline Le Lan, Rishabh Agarwal |  |
| 121 |  |  [Efficient Learning rate schedules for Stochastic Non-negative Matrix Factorization via Reinforcement Learning](https://openreview.net/forum?id=AAu_WuIiwi) |  | 0 | For deep learning training, learning rate schedules are often picked through trial and error, or hand-crafted optimization algorithms that focus mostly on maintaining stability and convergence without systemic incorporation of higher order derivative information to optimize the convergence slope. In this paper, we consider a stochastic version of Non-negative Matrix Factorization (NMF) where only a noisy gradient is known, and calculate a theoretical upper bound for SGD learning rate (LR) schedule that guarantees convergence, thereby providing a clean example where stability and convergence is not a challenge. We then use a Reinforcement Learning agent to demonstrate how efficient LR schedules, superior to those found by traditional algorithms, can be found for this NMF problem. | Shreyas Subramanian, Vignesh Ganapathiraman, Aly El Gamal |  |
| 122 |  |  [Chaotic Transformers for Deep Reinforcement Learning in Algorithmic Trading](https://openreview.net/forum?id=H2mbtfasD4K) |  | 0 | Chaotic Transformers for Deep Reinforcement Learning can be applied in algorithmic trading to improve the efficiency and effectiveness of trading strategies. In algorithmic trading, deep reinforcement learning can be used to learn trading policies that maximize the expected reward. However, due to the highly complex and nonlinear nature of financial markets, it can be challenging to identify profitable trading opportunities and avoid overfitting to historical data. | Tohid Abedini, Samin Heydarian, Moein Heidari, Alireza Morsali |  |
| 123 |  |  [Effects of Single-Attribute Control on the Music Generated by FIGARO](https://openreview.net/forum?id=G_MpqMHYo-) |  | 0 | We have experimented with controlling the musical attributes of the music generated by the FIGARO model, while evaluating the success of control and the overall musical quality. The results suggest non-trivial correlations between the musical attributes and the musical quality metrics. | Rafik Hachana, Adil Khan |  |
| 124 |  |  [Iterative weakly supervised learning for novel class object detection](https://openreview.net/forum?id=FWohKbMhlo) |  | 0 |  | Dejana Mandic, Wieland Brendel, Claudio Michaelis |  |
| 125 |  |  [BANDIT SAMPLING FOR FASTER NEURAL NETWORK TRAINING WITH SGD](https://openreview.net/forum?id=LV33sOiYCEP) |  | 0 | Importance sampling is a valuable technique in deep learning that involves sampling useful training examples more frequently to improve learning algorithms. However, obtaining reliable sample importance estimates early on in training can be challenging, as existing importance sampling methods can be computationally expensive and slow to converge. In this work, we propose a novel sampling schemed based on Multi-arm bandits (MAB). The proposed sampler is able to achieve higher validation accuracies significantly earlier in the training, compared to baselines. | Vignesh Ganapathiraman, Francisco Javier Calderon, Anila Joshi |  |
| 126 |  |  [Train Monolingual, Infer Bilingual](https://openreview.net/forum?id=MjVdwBGkys) |  | 0 | Cross-lingual transfer learning has been studied at depth. While many methods have been developed for pretraining or fine-tuning on monolingual, multilingual and parallel corpora with the purpose of predicting on a low-resource monolingual test set; in this paper we investigate the feasibility of training a text classifier on a monolingual training set and predicting on a parallel test set, jointly utilizing both languages at inference time only. | Alaeddin Selçuk Gürel, Aydin Gerek |  |
| 127 |  |  [A Simple, Fast Algorithm for Continual Learning from High-Dimensional Data](https://openreview.net/forum?id=TPTbHxeR6U) |  | 0 | As an alternative to resource-intensive deep learning approaches to the continual learning problem, we propose a simple, fast algorithm inspired by adaptive resonance theory (ART). To cope with the curse of dimensionality and avoid catastrophic forgetting, we apply incremental principal component analysis (IPCA) to the model's previously learned weights. Experiments show that this approach approximates the performance achieved using static PCA and is competitive with continual deep learning methods. Our implementation is available on https://github.com/neil-ash/ART-IPCA | Neil Ashtekar, Vasant G. Honavar |  |
| 128 |  |  [Discerning Self-Supervised Learning and Weakly Supervised Learning](https://openreview.net/forum?id=H9BGkFz-Sm) |  | 0 | The AI community has been a very rapidly growing community producing a vast amount of research in a very short span of time. These researches generate a lot of new methods and terminologies. With this scale of developments, it is very difficult to keep a track of terminologies, and under such conditions even more difficult to standardize the definitions. In the wake of such scenarios, we try to perform a detailed study of some terminologies in representation learning and form standard definitions of them. With this work, we establish a clear distinction between the concepts of Self-supervised learning and Weakly supervised learning. | Chandan Kumar, Matthew J. Darr, Ali Jannesari |  |
| 129 |  |  [DiffGANPaint: Fast Inpainting Using Denoising Diffusion GANs](https://openreview.net/forum?id=x2XNoPdXF8J) |  | 0 | Free-form image inpainting is the task of reconstructing parts of an image specified by an arbitrary binary mask. In this task, it is typically desired to generalize model capabilities to unseen mask types, rather than learning certain mask distributions. Capitalizing on the advances in diffusion models, in this paper, we propose a Denoising Diffusion Probabilistic Model (DDPM) based model capable of filling missing pixels fast as it models the backward diffusion process using the generator of a generative adversarial network (GAN) network to reduce sampling cost in diffusion models. Experiments on general-purpose image inpainting datasets verify that our approach performs superior or on par with most contemporary works. | Moein Heidari, Alireza Morsali, Tohid Abedini, Samin Heydarian |  |
| 130 |  |  [Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis](https://openreview.net/forum?id=dJfdug9aGd8) |  | 0 |  | Vithya Yogarajan, Gillian Dobbie, Henry Gouk |  |
| 131 |  |  [An Analysis of Transferability in Network Intrusion Detection using Distributed Deep Learning](https://openreview.net/forum?id=FPzByCI0yz1) |  | 0 | In this paper, we utilize a distributed deep learning framework to investigate transferability of network intrusion detection between federated nodes. Transferable learning makes intrusion detection systems more robust to rare attacks and enables them to adapt to real life scenarios. We analyze symmetric and asymmetric transferability relationships. We propose and investigate the impact of feature pre-processing to improve transferability. The code for this work is available at https://github.com/ghosh64/fedlearn. | Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal |  |
| 132 |  |  [MLP-Attention: Improving Transformer Architecture with MLP Attention Weights](https://openreview.net/forum?id=99XvUeDFYTD) |  | 0 | The Transformer architecture has revolutionized natural language processing (NLP) and has achieved state-of-the-art results in various tasks. The attention mechanism is one of the key components of the Transformer architecture, which allows the model to focus on relevant parts of the input. In the standard Transformer, the attention weights are computed by the dot product of query and key vectors followed by a softmax function. However, in this paper, we propose to replace the dot product of query and key vectors with a multi-layer perceptron (MLP) to compute attention weights directly from the embeddings. The proposed modification is simple and can be easily implemented in existing Transformer-based models to improve their performance as shown in this paper for an NLP task. We provide the implementation code at https://github.com/AlirezaMorsali/MLP-Attention for reproducibility and ease of adoption. | Alireza Morsali, Moein Heidari, Samin Heydarian, Tohid Abedini |  |
| 133 |  |  [Averager Student: Distillation from Undistillable Teacher](https://openreview.net/forum?id=4isz71_aZN) |  | 0 | Today, some companies release their black-box model as a service for users, where users can see the model’s output corresponding to their input. However, these models can be stolen via knowledge distillation by malicious users. Recently, undistillable teacher (Ma et al., 2021) is introduced in order to prevent the knowledge leakage. In this study, with the aim of contributing to solutions for model intellectual property (IP) protection, we propose a novel method which improves the distillation from an undistillable teacher whose goal is make the distillation difficult for students, with the purpose of model protection. The codes are released at https://github.com/rkevser/AveragerStudent. | Reyhan Kevser Keser, Behçet Ugur Töreyin |  |
| 134 |  |  [Inducing Document Representations from Graphs: A Blueprint](https://openreview.net/forum?id=2rp3guEM3A) |  | 0 | Representing textual documents in continuous numerical spaces is a crucial task in NLP. Early practitioners of NLP built their approach around capturing statistical patterns within documents and utilizing them as features in rich feature spaces. In contrast, contemporary state-of-the-art techniques leverage large neural networks and learn the document representations self-supervised. However, while these approaches excel at learning contextual word representations, they often overlook implicit document-to-document relations that can arise in real-world settings. We propose a blueprint method for constructing document representations that explicitly accounts for implicit document-to-document relations to address this issue. | Boshko Koloski, Marko Pranjic, Nada Lavrac, Blaz Skrlj, Senja Pollak |  |
| 135 |  |  [Uncertainty-Aware Test-Time Augmented Ensemble of BERTs for Classification of Common Mental Illnesses on Social Media Posts](https://openreview.net/forum?id=a9VgV-hywP) |  | 0 | Given the current state of the world, because of existing situations around the world, millions of people suffering from mental illnesses feel isolated and unable to receive help in person. Psychological studies have shown that our state of mind can manifest itself in the linguistic features we use to communicate. People have increasingly turned to online platforms to express themselves and seek help with their conditions. Deep learning methods have been commonly used to identify and analyze mental health conditions from various sources of information, including social media. Still, they face challenges, including a lack of reliability and overconfidence in predictions resulting in the poor calibration of the models. To solve these issues, We propose UATTA-EB: Uncertainty-Aware Test-Time Augmented Ensembling of BERTs for producing reliable and well-calibrated predictions to classify six possible types of mental illnesses. | Pratinav Seth, Mihir Agarwal |  |
| 136 |  |  [Predicting Targets with Data from Non-Conforming Sources](https://openreview.net/forum?id=uMlLT_xuiE) |  | 0 | Machine learning applications to real-world settings are often tasked with making predictions on data generated by multiple sources. There are many methods for understanding when data is Out-Of-Distribution (OOD). A less explored area of importance is where OOD data can be considered In-Distribution (ID) when conditioned by its generating data source. Within this preliminary research, we focus on this issue and propose methods for building classification models capable of making predictions on data in which labels can depend on their source. | Alexander Capstick, Payam M. Barnaghi |  |
| 137 |  |  [One Explanation Does Not Fit XIL](https://openreview.net/forum?id=o7uGWBK6Uo) |  | 0 | Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning. To address such flaws, the XIL framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL. | Felix Friedrich, David Steinmann, Kristian Kersting |  |
| 138 |  |  [Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics](https://openreview.net/forum?id=xKf-LSD2-Jg) |  | 0 |  | Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy S. Vatolin |  |
| 139 |  |  [GraphEx: A User-Centric Model-Level Explainer for Graph Neural Networks](https://openreview.net/forum?id=CuE1F1M0_yR) |  | 0 | With the increasing application of Graph Neural Networks (GNNs) in real-world domains, there is a growing need to understand the decision-making process of these models. To address this, we propose GraphEx, a model-level explainer that learns a graph generative model to approximate the distribution of graphs classified into a target class by the GNN model. Unlike existing methods, GraphEx does not require another black box deep model to explain the GNN and can generate a diverse set of explanation graphs with different node and edge features in one shot. Moreover, GraphEx does not need white box access to the GNN model, making it more accessible to end-users. Experiments on both synthetic and real datasets demonstrate that GraphEx can consistently produce explanations aligned with the class identity and can also identify potential limitations of the GNN model. | Sayan Saha, Monidipa Das, Sanghamitra Bandyopadhyay |  |
| 140 |  |  [Theta sequences as eligibility traces: A biological solution to credit assignment](https://openreview.net/forum?id=vd16AYbem3Z) |  | 0 | Credit assignment problems, for example policy evaluation in RL, often require bootstrapping prediction errors through preceding states or maintaining temporally extended memory traces; solutions which are unfavourable or implausible for biological networks of neurons. We propose theta sequences -- chains of neural activity during theta oscillations in the hippocampus, thought to represent rapid playthroughs of awake behviour -- as a solution. By analysing and simulating a model for theta sequences we show they compress behaviour such that existing but short $\mathsf{O}(10)$ ms neuronal memory traces are effectively extended allowing for bootstrap-free credit assignment without long memory traces, equivalent to the use of eligibility traces in TD($\lambda$). | Tom George |  |
| 141 |  |  [Federated Learning with Variational Autoencoders](https://openreview.net/forum?id=mvo72yTjhTl) |  | 0 | In this work we investigate the feasibility of using federated learning to train a variational autoencoder capable of generated handwritten digits when trained on the MNIST dataset. It was found that using federated learning we were able to train a model that produced comparable results to a centralised model, both in image reconstructions and image generations. | Hugo Dugdale |  |
| 142 |  |  [TRACTABLE LARGE SCALE CALIBRATION WITH RL](https://openreview.net/forum?id=AXxBPw5zdl4) |  | 0 | In this work we show that Reinforcement Learning (RL) is an effective algorithm for calibration problems at a scale which traditionally applied Bayesian approaches struggle. This work uses synthetic data, so has access to ground truth parameters and it can be seen that RL learns different, arguably better information for different parts of the learning process. These exciting results set the foundation for deeper consideration of RL in this space. | Fadel Thior, Rose Bandolo, Sekou Remy |  |
| 143 |  |  [A Variational Condition for Minimal-Residual Latent Representations](https://openreview.net/forum?id=A2VfgYliIT) |  | 0 | Autoencoders are a useful unsupervised-learning architecture that can be used to build surrogate models of systems governed by partial differential equations, enabling a more cost-effective route to study complex phenomena across science and engineering. In this article, we address two key questions underpinning this procedure: whether the reconstructed output satisfies the partial differential equation, and whether other latent vectors not corresponding to the encoding of any training data satisfy the same equation. Our results spell out some relevant conditions, and clarify the different impact of three main design decisions (architecture, training criterion, and choice of training solutions) on the final result. | Eloisa Bentivegna |  |
| 144 |  |  [Attention Based Variational Graph Auto-Encoder (AVGAE)](https://openreview.net/forum?id=j1gj0ndrk1) |  | 0 | Recently techniques such as VGAEs (Variational Graph Autoencoder) are quite popular in the unsupervised task setting and in generative modeling. Unlike conventional autoencoders, which typically use fully-connected layers to learn a latent representation of input data, VGAEs operate on graph-structured data. We propose to incorporate attention in VGAEs (AVGAE) for capturing the relationships better thereby increasing the robustness and generalisability. In a VAE, the encoder network learns to map input data to a lower-dimensional latent space, while the decoder network learns to map latent space vectors back to the original input data. Unlike traditional autoencoders, which typically use a fixed encoding function, VAEs use a probabilistic encoding function that maps input data to a probability distribution over the latent space. They have been shown to improve the quality of the generated output, particularly for tasks where the input data is complex and high-dimensional. | Nevasini Sasikumar, Krishna Sri Ipsit Mantri |  |
| 145 |  |  [Generative STResnet for Crime Prediction](https://openreview.net/forum?id=-IH_dcPGWM) |  | 0 | In this work, we combine STResnet with VAE to generate crime distribution. The outputs can be used for downstream tasks such as patrol deployment planning. | Tran Phong, Hoong Chuin Lau |  |
| 146 |  |  [A Light Spectrometer Device for Crop Disease Monitoring](https://openreview.net/forum?id=KtVonyo4AS) |  | 0 | Portable devices for the early detection of crop diseases are needed to support the farmers working in the field. Spectrometers showed their potential in the detection of crop diseases. However, high interpretation skills are needed to use the currently available spectrometers. In this project, we propose a portable device that obtains a spectrum wavelength of 700 nanometers describing the information of the crop. The output of this tool is integrated into a smartphone in the form of an app, making it accessible for use in the field in real applications. | Joshua Jeremy Dhikusooka, Ephraim Nuwamanya, Estefania Talavera Martínez, Godliver Owomugisha |  |
| 147 |  |  [Can Arterial Blood Pressure Predict Age? A ConvNet Classification Task](https://openreview.net/forum?id=jbBPBUGk-4) |  | 0 | Blood pressure (BP) increases throughout life, and is controlled by several feedback mechanisms in mammals. Therefore, high resolution BP data may contain information related to the health and functionality of those systems, and the organism as a whole. We used beat-to-beat BP data at different sampling rates collected from a heterogeneous population of rats to predict their age, and achieved 90%+ test accuracy, AUROC, and AUPRC across all sampling rates. Higher frequency components appeared to contribute less to 100 Hz sample rate BP data classification, but nonetheless could point to previously unknown Spectro-temporal patterns that the model identified. | Abdalrhman Mostafa, NourMounira Z. Bakkar, Mohamed Abdelhack, Ahmed F. ElYazbi |  |
| 148 |  |  [Pay Attention to Multi-Channel for Improving Graph Neural Networks](https://openreview.net/forum?id=IkHVGw_Ipu) |  | 0 | We propose Multi-channel Graph Attention (MGAT) to efficiently handle channel-specific representations encoded by convolutional kernels, enhancing the incorporation of attention with graph convolutional network (GCN)-based architectures. Our experiments demonstrate the effectiveness of integrating our proposed MGAT with various spatial-temporal GCN models for improving prediction performance. | ChungYi Lin, ShenLung Tung, Winston H. Hsu |  |
| 149 |  |  [pGS-CAM: Interpretable LiDAR Point Cloud Semantic Segmentation via Gradient Based Localization](https://openreview.net/forum?id=8kX0btdpAU5) |  | 0 | To extract the local information required for effective semantic segmentation of point clouds, a number of deep learning architectures typically make use of sophisticated feature extractors. Unfortunately, there has not been a lot of discussion on how to interpret their forecasts, which is essential if deployed in real-world settings. To that end, we propose pGS-CAM (point cloud Grad-Seg-CAM), a quick and effective gradient-based method for class activation mapping in point cloud semantic segmentation architectures. To gain insight into what each intermediate layer of the architecture does, our technique provides a heatmap for the corresponding layer. We use the popular semantic segmentation architecture (RandLA-Net) and a commonly used MLS dataset (SemanticKITTI) for our experimentation. | Abhishek Kuriyal, Vaibhav Kumar |  |
| 150 |  |  [Large Language Models Perform Diagnostic Reasoning](https://openreview.net/forum?id=N0lQfjeNWOE) |  | 0 | We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15% comparing to standard prompting. Moreover, the gap reaches a pronounced 18% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings. | ChengKuang Wu, WeiLin Chen, HsinHsi Chen |  |
| 151 |  |  [A Simple Loss Function for Convergent Algorithm Synthesis using RNNs](https://openreview.net/forum?id=WaAJ883AqiY) |  | 0 | Running a Recurrent Neural Network (RNN) over the same input multiple times, or iterative reasoning, enables logical extrapolation, where a model can be run on problems larger than the models were trained on. The loss function used to train these networks has a profound impact on their extrapolation ability. In this paper, we propose using a simple loss function called the Delta Loss (Salle & Prates, 2019). We show that the Delta Loss, like the state-of-the-art Progressive Loss (Bansal et al., 2022), leads to convergent algorithm synthesis, but with a simpler formulation, increased training efficiency, and greater robustness. | Alexandre Salle, Shervin Malmasi |  |
| 152 |  |  [The Obscure Limitation of Modular Multilingual Language Models](https://openreview.net/forum?id=zEGstYVHBt) |  | 0 | We expose the limitation of modular multilingual language models (MLMs) in multilingual inference scenarios with unknown languages. Existing evaluations of modular MLMs exclude the involvement of language identification (LID) modules, which obscures the performance of real-case multilingual scenarios of modular MLMs. In this work, we showcase the effect of adding LID on the multilingual evaluation of modular MLMs and provide discussions for closing the performance gap of caused by the pipelined approach of LID and modular MLMs. | Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Ayu Purwarianti |  |
| 153 |  |  [Exploratory Analysis of Scholarly Publications on Artificial Intelligence (AI) in Colonoscopy using Litstudy](https://openreview.net/forum?id=zdhCATDn_Q) |  | 0 | Due to the large number of scholarly papers on AI and colonoscopy and the short research period, it can be difficult to answer general questions about the research area, such as who the key authors are and what the key issues or insights are. We use Litstudy, a Python library, to study colonoscopy AI research. ”AI” and ”colonoscopy” keywords were used as search results. 3865 IEEE Xplore and 2007 Springer bibliographies were downloaded. Scopus found 5083 citation papers, excluding 789 unavailable citations. Topic clusters were created using the NMF model with a 0.85 threshold. Topic clouds showed that ”Patient” occurred most in four topics: 2, 3, 7, and 10. Despite querying IEEE, Springer, and Scopus databases with the ”Artificial Intelligence” keyword, subject 5 with AI has the lowest topic phrase weight in topic clouds. Topic 10 words cluster on colon cancer rehabilitation in colonoscopy showed weak topic clusters. The project selects scientific articles, analyses and visualises their scholarly contribution using natural language processing (NLP), bibliographic network analysis, and, most importantly, reveals word clusters in AI for colonoscopy publications. | Mary Adewunmi |  |
| 154 |  |  [Propagate Deeper and Adaptive Graph Convolutional Networks](https://openreview.net/forum?id=RR_w2fbYmV) |  | 0 | Graph Convolutional Networks (GCNs) are the basic architecture for handling graph-structured data. Deeper GCNs are required for large and sparse graph data. As the number of layers increases, the performance of GCNs degrades, which is commonly attributed to over-smoothing but is constantly debated. In this paper, we eliminate the equivalence between model degradation and over-smoothing or gradient vanishing and propose a systematic solution, an Adaptive DeepGCN (ADGCN) architecture, which makes the model the potential to address all issues. We place learnable parameters at the appropriate locations to make adaptive adjustments to different graph-structured data. We conduct experiments on real-world datasets to verify the stability and adaptability of our architecture. | Sisi Zhang, Lun Du, Fan Li, Ge Yu, Mengyuan Chen |  |
| 155 |  |  [L2 Norm Guided Adaptive Computation](https://openreview.net/forum?id=qW_GZYyn7C) |  | 0 | Although the human brain can adjust the amount of time and energy it uses to solve problems of varying complexity, many standard neural networks require a fixed computation budget regardless of the problem’s complexity. This work introduces L2 Adaptive Computation (LAC), a new algorithm that adjusts the computation budget, by tracking changes in the L2 norm of a neural network’s hidden state as layers are applied to the input. Unlike previous methods, LAC does not require additional trainable modules or auxiliary loss terms to make halting decisions. LAC matches the results of best-performing methods on a complex synthetic task and improves image classification accuracy while also increasing efficiency. | Mani Shemiranifar, Mostafa Dehghani |  |
| 156 |  |  [Efficient Temporal Denoising for Improved Depth Map Applications](https://openreview.net/forum?id=nazr0QFvHR) |  | 0 | Depth estimation involves acquiring three-dimensional information from images, which has numerous applications in downstream tasks. Although several effective monocular depth estimation algorithms have been developed, directly applying frame-by-frame depth estimation can result in flickering, which hinders many video-related applications. Previous video-based approaches have primarily been post-processing methods that utilize spatial information about camera poses to reduce flicker, but they come with a considerable computational cost. In this paper, we introduce the concept of depth map noise to better understand flicker in depth maps and propose a depth noise smoothing network to eliminate visual flicker in depth maps. Our approach can be applied to different depth estimation models and run in real-time for screen-based applications, such as video bokeh. | Pengzhi Li, Zhiheng Li |  |
| 157 |  |  [AI-based opportunistic analysis of the CT images during COVID (2021): Does living in a metropolitan area affect the vertebral body mineral density in older people?](https://openreview.net/forum?id=QRKKFN7FLm) |  | 0 | The aim of the study is to reveal the problems of using information on several territorial units (districts) of an integral urban agglomeration for an identification of interdimensional peculiarities of living in a particular area by estimating vertebral bone mineral density in Moscow residents aged 50 and older. The results of this study will provide hypothesis testing for a pilot project to create a model of spatial exposure to the urban environment (Computer Vision Experiment; Clinical Trial: NCT04489992), which will form the basis of identifying individuals (groups of individuals) at risk of "accelerated" occupational aging and musculoskeletal diseases. | Andrey V. Vlasov, Alexei V. Petraikin |  |
| 158 |  |  [One Student Knows All Experts Know: From Sparse to Dense](https://openreview.net/forum?id=1PW_txDkX7) |  | 0 | Human education system trains one student by multiple experts. Mixture-of-experts (MoE) is a powerful sparse architecture including multiple experts. However, sparse MoE model is easy to overfit, hard to deploy, and not hardware-friendly for practitioners. In this work, inspired by the human education model, we propose a novel task, knowledge integration, to obtain a dense student model (OneS) as knowledgeable as one sparse MoE. We investigate this task by exploring 4 different ways to gather knowledge from MoE to initialize a dense student model, and we then refine the dense student by knowledge distillation. We evaluate our model on both vision and language tasks. Experimental results show, with $3.7 \times$ inference speedup, the dense student can still preserve $88.2\%$ benefits from MoE counterpart. | Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, Yang You |  |
| 159 |  |  [AN ENSEMBLE LEARNING FRAMEWORK FOR VISIBILITY PREDICTION IN INDO-GANGETIC REGION](https://openreview.net/forum?id=WkDqZD3VRo) |  | 0 | Visibility of an area affects all forms of transportation such as sea, surface and aviation which can further affect the economy of that area. Thus it is very important to accurately estimate the visibility of an area for the upcoming days based on different parameters of the past meteorological data, so that we can take precautions in case of poor visibility. Several machine learning techniques have been already applied on different kinds of data sets to estimate the visibility. However, most of these methods could not perform reasonably well and none of them were applied on the meteorological data of the Indo-Gangetic plane in India, which witnesses widespread fog primarily during winter that badly impacts visibility and therefore transportation. In this spirit, a Extreme Gradient Boosting (XGboost) based regression framework is developed here to estimate the visibility of the Indo-Gangetic plane. The method identifies significant parameters of the data following different standard feature engineering schemes and subsequently implement the XGboost regression model. The experimental results show that the proposed framework outperforms the state of the arts in terms of mean absolute error (MAE) and root mean squared error (RMSE). In future, the aim is to explore the performance of this framework to estimate the visibility of other crucial areas across globe | Arkapal Panda, Tanmay Basu, Vaibhav Kumar |  |
| 160 |  |  [Evolutionary Federated Learning Using Particle Swarm Optimization](https://openreview.net/forum?id=fcQFbluDTX) |  | 0 |  | Ender Minyard, Steven Kolawole, Nayan Saxena |  |
| 161 |  |  [Understanding the Effectiveness of Cross-Domain Contrastive Unsupervised Domain Adaptation](https://openreview.net/forum?id=0GpMf9UeI3G) |  | 0 | Unsupervised domain adaptation helps to transfer learned tasks from a source to a target domain in the lack of labeled data. Recently, contrastive learning showed promising results on this setup. However, there are limitations on the performance due to unbalanced objectives between the self-representation and the adaptation tasks. We show that pre-training choices and hard negative mining provide consistent improvements to successfully pair contrastive learning and unsupervised domain adaptation. | Viacheslav Sinii, Adín Ramírez Rivera, Adil Khan |  |
| 162 |  |  [Performance Evaluation of Enhanced ConvNeXtTiny-based Fire Detection System in Real-world Scenarios](https://openreview.net/forum?id=A-E41oZCfrf) |  | 0 | Timely detection of fires is crucial for saving human lives and minimizing the economic and ecological impact of such incidents. Although numerous attempts have been made to identify a fire in its early stage, significant challenges remain in achieving accurate and reliable detection. Therefore, we proposed a modified pre-trained ConvNeXtTiny architecture for detecting fire, offering high detection accuracy and fast inference time compared to other alternatives over benchmarks. Our source code of the paper will be publicly available at https://github.com/TaimoorKhan561/ICLR_Source. | Taimoor Khan, Haci Ismail Aslan, Chang Choi |  |
| 163 |  |  [Robustness Evaluation of Multi-Agent Reinforcement Learning Algorithms using GNAs](https://openreview.net/forum?id=zZjPRz0EX5T) |  | 0 | Recently, multi-agent reinforcement learning (MARL) has shown its ability in solving sequential decision-making problems in complicated multi-agent environments. However, uncertainties from observations and executions undermine its performance when MARL methods are deployed in real-world applications. While crucial for deployment, a systematic robustness evaluation for MARL algorithms is not present. In this work, we utilize Gaussian noise attacks (GNAs) to examine the robustness of a benchmark MARL algorithm: multi-agent deep deterministic policy gradient (MADDPG). To the best of our knowledge, our work is the first to investigate the robustness of MADDPG to GNAs to observation and execution information. Our experiments show that GNA has totally different patterns in observation-wise attacks and execution-wise attacks. Furthermore, there are counter-intuitive insights from the experimental results which could guide researchers in future MARL methods development. | Xusheng Zhang, Wei Zhang, Yishu Gong, Liangliang Yang, Jianyu Zhang, Zhengyu Chen, Sihong He |  |
| 164 |  |  [State Advantage Weighting for Offline RL](https://openreview.net/forum?id=PjypHLTo29v) |  | 0 |  | Jiafei Lyu, Aicheng Gong, Le Wan, Zongqing Lu, Xiu Li |  |
| 165 |  |  [Towards Robust Feature Learning with t-vFM Similarity for Continual Learning](https://openreview.net/forum?id=6I5i0Ytnlul) |  | 0 |  | Bilan Gao, YoungBin Kim |  |
| 166 |  |  [When Biology has Chemistry: Solubility And Drug Subcategory Prediction using SMILES Strings](https://openreview.net/forum?id=28si4RXwDt1) |  | 0 | Drug discovery is a complex process that requires extensive research and development. One important aspect of drug discovery is the prediction of drug properties, such as solubility. In recent years, sequence-based embedding methods, such as SMILES strings, have gained popularity in the drug discovery community due to their ability to encode chemical structures. SMILES strings are text-based representations of chemical structures that can be easily processed by machine learning models. This research paper presents a study on predicting (i) the solubility ALOGPS (Ghose-Crippen-Viswanadhan octanol-water partition coefficient) and (ii) drug subcategories using traditional molecular fingerprints and sequence-based embedding methods (from the bioinformatics domain) of SMILES strings. The study investigates five types of embeddings: Morgan fingerprint, MACCS fingerprint, $k$-mers, and minimizer-based spectrum. Additionally, a weighted version of $k$-mers that employs inverse document frequency is used to assign weights to each $k$-mer within the spectrum. For the classification task (\ie, drug subcategory prediction), we use the same embedding methods as input to several classifiers and report classification goodness using several evaluation metrics. For the regression task (\ie, solubility ALOGPS prediction), we use several popular models \eg, linear regression, and evaluate the performance using multiple evaluation metrics such as RMSE MAE, MSE, etc. The classification results indicate that the weighted $k$-mers method outperforms the baselines for predictive performance. The regression results indicate that the MACCS fingerprint with random forest regression model outperforms all other embedding methods and regression models. Overall, this study provides insights into the effectiveness of different embeddings, regression models, and classification models for solubility and drug subcategory prediction, which can be helpful for future tasks such as drug discovery. | Sarwan Ali, Prakash Chourasia, Murray Patterson |  |
| 167 |  |  [Answering Questions Over Knowledge Graphs Using Logic Programming Along with Language Models](https://openreview.net/forum?id=D2lo4toTUTo) |  | 0 | Question Answering over Knowledge Graphs (KGQA) is the task of answering natural language questions over a knowledge graph (KG). This task requires a model to reason over multiple edges of the KG to reach the right answer. In this work, we present a method to equip large language models (LLMs) with classic logical programming languages to provide an explainable solution to the problem. Our goal is to extract the representation of the question in the form of a Prolog query, which can then be used to answer the query programmatically. To demonstrate the effectiveness of this approach, we use the MetaQA dataset and show that our method finds the correct answer entities for all the questions in the test dataset. | Navid Madani, Kenneth Joseph |  |
| 168 |  |  [Contrastive Training with more data](https://openreview.net/forum?id=ZTp85mW5nFy) |  | 0 | This paper proposes a new method of contrastive training over multiple data points, focusing on the scaling issue present when using in-batch negatives. Our approach compares transformer training with dual encoders vs training with multiple encoders. Our method can provide a feasible approach to improve loss modelling as encoders scale. | Stephen Mander, Scott Piao, Hossein Rahmani |  |
| 169 |  |  [Statistical Methods for Auditing the Quality of Manual Content Reviews](https://openreview.net/forum?id=GSlYBJ3aOpC) |  | 0 | Large technology firms face the problem of moderating content on their online platforms for compliance with laws and policies. To accomplish this at the scale of billions of pieces of content per day, a combination of human and machine review are necessary to label content. Subjective judgement and bias are of concern to both human annotated content as well as to auditors who may be employed to evaluate the quality of such annotations in conformance with law and/or policy. To address this concern, this paper presents a novel application of statistical analysis methods to identify human error and these sources of audit risk. | Xuan Yang, Andrew Smart, Daniel Theron |  |
| 170 |  |  [ARTIFICIAL PSYCHOLOGY](https://openreview.net/forum?id=TqkzImZ92t8) |  | 0 | Most Deep Learning based Model Compression methods draw their inspiration from the human brain. This is an example of how powerful but overlooked abstractions from the human mind are. Hoping to benefit the discipline of one from the other, this paper aims to draw attention to a line of research at the intersection of Artificial Intelligence and Psychology, dubbing the area Artificial Psychology | Mubarek Mohammed |  |
| 171 |  |  [Automated Mapping of Healthcare Concepts to a Standardized Healthcare Taxonomy](https://openreview.net/forum?id=87oCobKKS6x) |  | 0 | SNOMED CT presents an opportunity for numerous research prospects to learn medical terminologies and effectively assist the diagnosis process. In this work, we propose mapping the information in medical records to paths extracted from the SNOMED CT knowledge graph. To achieve this, we have leveraged language models to extract feature embeddings from EHR and compare them against the paths found in SNOMED CT. Our motivation is that this method can significantly assist the diagnosis process, ultimately leading to better healthcare outcomes. We have used several strategies to score the paths and evaluated them with experts in this field who have provided encouraging feedback on the outcomes of this approach. | Sabbir Mollah, Mohammed Rakib, Nabeel Mohammed, Mashrur Wasek, AKM Shahariar Azad Rabby, Fuad Rahman |  |
| 172 |  |  [RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping](https://openreview.net/forum?id=frB4MiYGoD_) |  | 0 |  | Kamil Sabbagh, Patrik Joslin Kenfack, Adín Ramírez Rivera, Adil Khan |  |
| 173 |  |  [Prior knowledge meets Neural ODEs: a two-stage training method for improved explainability](https://openreview.net/forum?id=p7sHcNt_tqo) |  | 0 | Neural Ordinary Differential Equations (ODEs) have been used extensively to model physical systems because they represent a continuous-time function that can make predictions over the entire time domain. However, most of the time, the parameters of these physical systems are subject to strict laws/constraints. But there is no guarantee that the Neural ODE model satisfies these constraints. Therefore, we propose a two-stage training for Neural ODE. The first stage aims at finding feasible parameters by minimizing a loss function defined by the constraints violation. The second stage aims at improving the feasible solution by minimizing the distance between the predicted and ground-truth values. By training the Neural ODE in two stages, we ensure that the governing laws of the system are satisfied and the model fits the data. | C. Coelho, M. Fernanda P. Costa, Luís L. Ferrás |  |
| 174 |  |  [Self-Supervised Continual Learning](https://openreview.net/forum?id=udl9OobOxZu) |  | 0 | This paper proposes \textit{Self-Supervised Continual Learning (SCL)} for regularization-based class incremental learning. The novel pretext task in SCL utilizes randomly-transformed labels without depending on data-augmented transforms. \textit{SCL} trained with a novel incremental task-regularizer and an orthogonal weight modifications backbone shows promising performance on three datasets. | Kartik Thakral, Surbhi Mittal, Utkarsh Uppal, Bharat Giddwani, Mayank Vatsa, Richa Singh |  |
| 175 |  |  [When Spiking Neural Networks Meet Temporal Attention Image Decoding and Adaptive Spiking Neuron](https://openreview.net/forum?id=MuOFB0LQKcy) |  | 0 | Spiking Neural Networks (SNNs) are capable of encoding and processing temporal information in a biologically plausible way. However, most existing SNN-based methods for image tasks do not fully exploit this feature. Moreover, they often overlook the role of adaptive threshold in spiking neurons, which can enhance their dynamic behavior and learning ability. To address these issues, we propose a novel method for image decoding based on temporal attention (TAID) and an adaptive Leaky-Integrate-and-Fire (ALIF) neuron model. Our method leverages the temporal information of SNN outputs to generate high-quality images that surpass the state-of-the-art (SOTA) in terms of Inception score, Fréchet Inception Distance, and Fréchet Autoencoder Distance. Furthermore, our ALIF neuron model achieves remarkable classification accuracy on MNIST (99.78%) and CIFAR-10 (93.89%) datasets, demonstrating the effectiveness of learning adaptive thresholds for spiking neurons. | Xuerui Qiu, Zheng Luan, Zhaorui Wang, RuiJie Zhu |  |
| 176 |  |  [Characters Are Like Faces](https://openreview.net/forum?id=HM_jOWEYL7y) |  | 0 | There are over 100,000 characters in Chinese, though only four thousand of them are used in our daily life. However for cultural researchers, they interact with those Rarely Used Characters (RUCs) frequently. It would facilitate using these RUCs for them with Optical Character Recognition (OCR) technology. Nevertheless, the current OCR methods, no matter regression based or classification based, are difficult to recognize such a huge amount of characters. In this work, we simply treat characters like human faces and adopt the MobileFaceNetV3 to recognize over 74,000 Chinese characters included in Unicode. A demo can be seen at http://risingentropy.top/OCR.html. All source code:https://github.com/RisingEntropy/Characters-Are-Like-Faces | Haoyu Deng, Zhaoteng Ye, Yule Duan |  |
| 177 |  |  [A Scalable Self-supervised Learner for Hyperspectral Image Classification](https://openreview.net/forum?id=uwbyW92Sonu) |  | 0 | Learning-based Hyperspectral image classification methods have achieved fantastic performance due to their superior ability to represent features at the cost that these methods are complex, inflexible and weak to generalize. Thus, we propose a simple and scalable pretrained model which can greatly accelerate convergence rate and improve the performance in downstream classification task. Experiments shows the performance of our method achieved state-of-the-art performance with limited training samples. | Weili Kong, Lin Qi, Baisen Liu, Jiaming Pei |  |
| 178 |  |  [Optimizing MPJPE promotes miscalibration in multi-hypothesis human pose lifting](https://openreview.net/forum?id=B5riBS9HZGn) |  | 0 |  | Pawel A. Pierzchlewicz, Mohammad Bashiri, R. James Cotton, Fabian H. Sinz |  |
| 179 |  |  [Unsupervised Detection of Cell Assemblies with Graph Neural Networks](https://openreview.net/forum?id=Tbzv_BbjjO8) |  | 0 |  | Roman Koshkin, Tomoki Fukai |  |
| 180 |  |  [Heat Up The Sentiment Learning With ICE](https://openreview.net/forum?id=wFxjFCHUkS) |  | 0 |  | Yao Yao, Zuchao Li, Hai Zhao |  |
| 181 |  |  [Almost Sure Last Iterate Convergence of Sharpness-Aware Minimization](https://openreview.net/forum?id=IcDTYTI0Nx) |  | 0 |  | Kyunghun Nam, Jinseok Chung, Namhoon Lee |  |
| 182 |  |  [On the application and impact of ε-DP and fairness in ambulance engagement time prediction](https://openreview.net/forum?id=WKVH54a1W4) |  | 0 | This study elaborates on a complete pipeline for the development of a private and fair Machine Learning (ML) model to predict ambulance engagement time. It was shown that sensitive variables reduced their impact on model building with Random Forest as the differential privacy budget (ε) decreased with the GRR and Geometric mechanisms. Also, the application of the Reweighing fairness mechanism negatively affected fairness in private models. Finally, it is possible to keep firefighters' and victims' privacy, recovering an ML model with good performance. | Selene Cerna, Catuscia Palamidessi |  |
| 183 |  |  [Adversarial Policy Gradient for Learning Graph-Based Representation in Human Visual Processing](https://openreview.net/forum?id=5-ROmmBJKV) |  | 0 | This article discusses the challenges in modeling the neural mechanisms underlying human visual processing and the use of graph-based representations to capture inter-region relationships in visual processing. While graphs have shown promise in analyzing neural responses, learning an optimal graph representation from limited data is challenging, as there is no ground truth to learn from. To address this, the authors propose a novel approach to graph-based representation using Adversarial Policy Gradient, which involves an adversarial game between the Policy Network and the Reward Network to generate a better graph representation. | Subhrasankar Chatterjee, Subrata Pain, Debasis Samanta |  |
| 184 |  |  [Federated Learning for Local and Global Data Distribution](https://openreview.net/forum?id=qX8cGLnfAd) |  | 0 | Existing research in Federated Learning focuses on synthetic or small-scale datasets, with in-house distribution posing challenges for long-term real-world use cases. We propose a novel approach that maximizes in-house (local) distribution gains while focusing on generalization. Experimental results on several datasets demonstrate the efficacy of the proposed approach. | Gaurav Goswami, Akshay Agarwal, Nalini K. Ratha, Richa Singh, Mayank Vatsa |  |
| 185 |  |  [Is DFR for Soft Biometrics Prediction in Unconstrained Images Fair and Effective?](https://openreview.net/forum?id=rLqN6XLbON) |  | 0 | Face being a nonintrusive recognition modality, is an ideal candidate for identifying criminals. The modality is not only related to identity but can also extract several other important features such as age, race, and gender. In this preliminary research, we have collected a novel unconstrained face recognition dataset using mobile phones. On the collected dataset, we have evaluated the current state-of-the-art (SOTA) deep face recognition (DFR) algorithms for face attribute identification. This research aims to identify whether current algorithms are effective for the task or biased towards their training set. The results suggest that the current technology is not effective in identifying face attributes when the images are captured in unconstrained environments. For example, deep face networks yield the best F-1 score of $\mathbf{0.43}$ when asked to predict gender on the collected dataset. | Udaybhan Rathore, Akshay Agarwal |  |
| 186 |  |  [Integrating Information from Natural Language Parse Tree to Code Generation](https://openreview.net/forum?id=1WEPXTIjAd) |  | 0 | While more and more research works have considered Natural Language artifacts as the inputs of software engineering research, such as code generation, information about their graph/tree representations needs to be carefully considered. In this work, we propose an approach for integrating information on NLPT on multiple problems in SE tasks. The preliminary experiment shows that augmenting information of NLPT can improve the code generation from pseudocode. | Hung Phan, Ali Jannesari |  |
| 187 |  |  [Pseudo Labels for Single Positive Multi-Label Learning](https://openreview.net/forum?id=-CH1C-aQ5pk) |  | 0 | The cost of data annotation is a substantial impediment for multi-label image classification: in every image, every category must be labeled as present or absent. Single positive multi-label (SPML) learning is a cost-effective solution, where models are trained on a single positive label per image. Thus, SPML is a more challenging domain, since it requires dealing with missing labels. In this work, we propose a method to turn single positive data into fully-labeled data: “Pseudo Multi-Labels”. Basically, a “teacher” network is trained on single positive labels. Then, we treat the “teacher” model's predictions on the training data as ground-truth labels to train a “student” network on fully-labeled images. With this simple approach, we show that the performance achieved by the “student” model approaches that of a model trained on the actual fully-labeled images. | Julio Arroyo |  |
| 188 |  |  [Drowning Detection based on YOLOv8 improved by GP-GAN Augmentation](https://openreview.net/forum?id=osqgjMNm4_) |  | 0 | Drowning is a significant safety issue worldwide, and a robust computer vision-based alert system can easily prevent such tragedies in swimming pools. However, due to domain shift caused by the visual gap (potentially due to lighting, indoor scene change, pool floor color etc.) between the training swimming pool and the test swimming pool, the robustness of such algorithms has been questionable. To address this issue, we propose a domain-aware data augmentation pipeline based on Gaussian Poisson Generative Adversarial Network (GP-GAN). Combined with YOLOv8, we demonstrate that such a domain adaptation technique can significantly improve the model performance (from 0.24 mAP to 0.82 mAP) on new test scenes. | En Wei, Simiao Ren |  |
| 189 |  |  [Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection](https://openreview.net/forum?id=eaKoBpxCPe) |  | 0 | Health experts assert that hope plays a crucial role in enhancing individuals' physical and mental well-being, facilitating their recovery, and promoting restoration. Hope speech refers to \`\`YouTube comments/posts that offer support, reassurance, suggestions, inspiration, and insight.". The detection of hope speech involves the analysis of such textual content, with the aim of identifying messages that invoke positive emotions in people. Our study aims to find computationally efficient yet comparable/superior methods for hope speech detection. Our codes for replicating our findings will be made available in the near future. | Neemesh Yadav, Mohammad Aflah Khan, Diksha Sethi, Raghav Sahni |  |
| 190 |  |  [Group Equivariant Convolutional Networks](https://openreview.net/forum?id=niyvAOOnwPM) |  | 0 | Convolutional Neural Networks (CNN) are using symmetry priors to make the best out of the properties of the data, in particular translation invariance in images. Group Equivariant CNN (Cohen & Welling, 2016) extend CNN by using invari- ances from other groups of symmetry. After exploring their mathematical proper- ties, we confirm their performances on Rotated MNIST and CIFAR10, introduce some extensions with GAN and propose applications in High Energy Physics. | Maya Janvier |  |
| 191 |  |  [Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization](https://openreview.net/forum?id=2Z-dQTRezZ) |  | 0 | Controlled data generation with GANs is desirable but challenging due to the nonlinearity and high dimensionality of their latent spaces. In this work, we explore image manipulations learned by GANSpace, a state-of-the-art method based on PCA. Through quantitative and qualitative assessments we show: (a) GANSpace produces a wide range of high-quality image manipulations, but they can be highly entangled, limiting potential use cases; (b) Replacing PCA with ICA improves the quality and disentanglement of manipulations; (c) The quality of the generated images can be sensitive to the size of GANs, but regardless of their complexity, fundamental controlling directions can be observed in their latent spaces. | Andrey Palaev, Rustam A. Lukmanov, Adil Khan |  |
| 192 |  |  [The Polarised Regime of identifiable Variational Autoencoders](https://openreview.net/forum?id=iSkcAjBqUHU) |  | 0 |  | Lisa Bonheme, Marek Grzes |  |
| 193 |  |  [Uni-Match: A Semantic Unified Model for Query-Product Retrieval](https://openreview.net/forum?id=91Bcj6sgcxt) |  | 0 |  | Zhenyang Zhu, RuiJie Zhu, Yunrui Ge, Qihang Zhao |  |
| 194 |  |  [Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention](https://openreview.net/forum?id=Peb3QdR8zzP) |  | 0 | Compared with standard text, understanding dialogue is more challenging for machines as the dynamic and unexpected semantic changes in each turn. To model such inconsistent semantics, we propose a simple but effective Hierarchical Dialogue Understanding model, HiDialog. Specifically, we first insert multiple special tokens into a dialogue and propose the turn-level attention to learn turn embeddings hierarchically. Then, a heterogeneous graph module is leveraged to polish the learned embeddings. We evaluate our model on various dialogue understanding tasks including dialogue relation extraction, dialogue emotion recognition, and dialogue act classification. Results show that our simple approach achieves state-of-the-art performance on all three tasks above. All our source code is publicly available at https://github.com/ShawX825/HiDialog. | Xiao Liu, Jian Zhang, Heng Zhang, Fuzhao Xue, Yang You |  |
| 195 |  |  [Large Sparse Kernels for Federated Learning](https://openreview.net/forum?id=ZCv4E1unfJP) |  | 0 |  | Feilong Zhang, Yinchuan Li, Shiyi Lin, Yunfeng Shao, Junjun Jiang, Xianming Liu |  |
| 196 |  |  [Using vision transformer-based GANs against Vision Transformers](https://openreview.net/forum?id=WFatA9XIQ0m) |  | 0 | Vision transformers have become one of the best architectures for image classification tasks. In this paper, we introduce a novel method for creating adversarial attacks in a black box environment without using surrogate models. Specifically, we introduce a single encoder and a three-encoder Transformer based GAN that creates a perturbation with a successful attack rate higher than state of the art methods. | Andrei Florin Pamînt, Sergiu Adrian Darabant |  |
| 197 |  |  [Pivot Pre-finetuning for Low Resource MT: A Case Study in Kikamba](https://openreview.net/forum?id=PaHmtktx86H) |  | 0 | Current approaches to performant machine translation often require large amounts of data (Koehn et al., 2022). However for a majority of 7000+ languages in the world, these languages often have a relative lack of digitized/organized text available, and are considered low-resource. In practical terms, this often means that there is a substantial drop in quality between translation performance between high and low-resource language pairs. We look to explore the intersection of rapid NMT adaptation techniques and pre-trained sequence to sequence models to better leverage multilingual models, performing a case study on Kikamba. | Stephen Ngumbi Kiilu, Machel Reid |  |
| 198 |  |  [Unsupervised Learning for Anomaly Detection: A Comparison of Deep Generative Models](https://openreview.net/forum?id=WU3veNUvvU) |  | 0 |  | Kitgak Simon |  |
| 199 |  |  [Bayes classifier cannot be learned from noisy responses with unknown noise rates](https://openreview.net/forum?id=U4o5iSWSaD) |  | 0 | Training a classifier with noisy labels typically requires the learner to specify the distribution of label noise, which is often unknown in practice. Although there have been some recent attempts to relax that requirement, we show that the Bayes decision rule is unidentified in most classification problems with noisy labels. This suggests it is generally not possible to bypass/relax the requirement. In the special cases in which the Bayes decision rule is identified, we develop a simple algorithm to learn the Bayes decision rule, that does not require knowledge of the noise distribution. | Soham Bakshi, Subha Maity |  |
| 200 |  |  [Contrastive Learning with 3D Shapes](https://openreview.net/forum?id=ChW0YYRIni) |  | 0 | In fields such as Computer Vision or NLP, there is a large amount of data available which, however, cannot be labeled, as it would be very expensive. A possible solution to this problem is Contrastive Learning, a Self-Supervised technique. This work aims to implement a contrastive learning regime for a non-Euclidean data type, more precisely 3D Point Cloud Shape. | Andrea Bernini |  |
| 201 |  |  [Adaptive-saturated RNN: Remember more with less instability](https://openreview.net/forum?id=Ihzsru2bw2) |  | 0 | Orthogonal parameterization is a compelling solution to the vanishing gradient problem (VGP) in recurrent neural networks (RNNs). With orthogonal parameters and non-saturated activation functions, gradients in such models are constrained to unit norms. On the other hand, although the traditional vanilla RNNs are seen to have higher memory capacity, they suffer from the VGP and perform badly in many applications. This work proposes Adaptive-Saturated RNNs (asRNN), a variant that dynamically adjusts its saturation level between the two mentioned approaches. Consequently, asRNN enjoys both the capacity of a vanilla RNN and the training stability of orthogonal RNNs. Our experiments show encouraging results of asRNN on challenging sequence learning benchmarks compared to several strong competitors. The research code is accessible at https://github.com/ndminhkhoi46/asRNN/. | Khoi Minh NguyenDuy, Quang Pham, Binh T. Nguyen |  |
| 202 |  |  [Stratospheric Aerosols: Establishing a Novel Optical Thickness Benchmark for Effective Climate Change Mitigation](https://openreview.net/forum?id=pKd6q-FrprW) |  | 0 | Global Warming has been a problem at the heart of Earth’s environmental issues for nearly 5 decades, with the potential to affect a significant portion of the global population and cause catastrophic irreversible damage to the planet's future. Changes in Earth’s climate due to the rise in global temperatures will have an enormous impact on communities around the world, along with a drastic displacement of humans and an extreme loss in natural biodiversity. Current methods of combating this issue have proven to be ineffective, requiring a more comprehensive and innovative approach. This project aims to propose a potential solution to mitigate the effects of global warming and limit temperatures to sustainable levels through the use of stratospheric aerosols. Through a process of data collection, experimentation, and modeling, I was able to correlate the presence of aerosols in the stratosphere to a consequent drop in temperatures and utilize regression prediction to forecast a 16 percent drop in global temperatures after examining the effects of volcanic ash in the stratosphere. I was also able to compare monthly aerosol concentration levels to declines in the growth of temperatures and conclude that by keeping aerosol optical thickness over 0.185, we can stabilize global temperatures and achieve climate change goals set to protect the Earth. By implementing the changes to Earth’s atmosphere, we can reflect heat from the Sun and create a cooling effect for the planet, potentially stopping climate change and saving billions of people. | Mihir Garimella |  |
| 203 |  |  [CausalStructCodec: Causally-aware observational and interventional data generator](https://openreview.net/forum?id=cKLmwCTFiI) |  | 0 |  | Louis Hernandez, Matthieu Boussard |  |
| 204 |  |  [Semantic Similarity Based Label Augmentation for Visual Classification](https://openreview.net/forum?id=bRI_3OFg4o) |  | 0 |  | Yu Cao |  |
| 205 |  |  [Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning](https://openreview.net/forum?id=3EfxJTp_-Cj) |  | 0 |  | Chenkai Ma |  |
| 206 |  |  [Fidelity of Interpretability Methods and Perturbation Artifacts in Neural Networks](https://openreview.net/forum?id=nbqO93YTz-) |  | 0 | Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Using the ResNet-50 trained on the ImageNet, we demonstrate the proposed fidelity estimation of four popular post-hoc interpretability methods. | Lennart Brocki, Neo Christopher Chung |  |
| 207 |  |  [EDCDE - Extended Discovery of Closed-Form Differential Equations](https://openreview.net/forum?id=EVz_vcZQvvg) |  | 0 |  | Robert Joseph George |  |
| 208 |  |  [Synthetic Controls as Balancing Scores](https://openreview.net/forum?id=AFLNyWMg4D2) |  | 0 |  | Harsh Parikh |  |
| 209 |  |  [Clustered Federated Learning with Slightly Skewed Labels](https://openreview.net/forum?id=qPwZouq5sY_) |  | 0 | Clustered federated learning methods are proposed to realize the personalization of federated learning. The core of these methods is KMeans while it cannot identify sample from which cluster under slight non-IID data distribution. This paper proposed Gaussian mixture cluster (GMCFL) to measure the probability and uncertainty of that samples belongs to which clusters. This method efficiently aggregated the model parameters between clusters and were robust to non-IID data distribution as well. The empirical results demonstrated our method had better performance than other state-of-the art clustered federated learning methods. | Jiaming Pei, Wei Li |  |
| 210 |  |  [Fusing 3D-CNN and lightweight Swin Transformer networks for HSI](https://openreview.net/forum?id=Jx44OPxLZ2-) |  | 0 |  | Baisen Liu, Yuanjia Liu, Wulin Zhang, Yiran Tian |  |
| 211 |  |  [Compound Tokens: Channel Fusion for Vision-Language Representation Learning](https://openreview.net/forum?id=_3_VZtMkvMB) |  | 0 | We present an effective method for fusing visual-and-language representations for several question answering tasks including visual question answering and visual entailment. In contrast to prior works that concatenate unimodal representations or use only cross-attention, we compose multimodal representations via channel fusion. By fusing on the channels, the model is able to more effectively align the tokens compared to standard methods. These multimodal representations, which we call compound tokens are generated with cross-attention transformer layers. We demonstrate the effectiveness of compound tokens using an encoder-decoder vision-language model trained end-to-end in the open-vocabulary setting. Compound Tokens achieve highly competitive performance across a range of question answering tasks including GQA, VQA2.0, and SNLI-VE. | Maxwell Mbabilla Aladago, A. J. Piergiovanni |  |
| 212 |  |  [Message-passing Selection: Towards Interpretable GNNs for Graph Classification](https://openreview.net/forum?id=99Go96dla5y) |  | 0 |  | Wenda Li, Kaixuan Chen, Shunyu Liu, Wenjie Huang, Haofei Zhang, Mingli Song, Yingjie Tian, Yun Su |  |
| 213 |  |  [Multi-Agent Reinforcement Learning for Coalitional Bargaining Games](https://openreview.net/forum?id=OaZktJBVpUy) |  | 0 | In recent years, there has been growing attention to the application of MARL to coalition formation problems, in particular, on coalitional bargaining games as a means of negotiation. However, the lack of theoretical principles for using MARL in coalitional bargaining games remain less explored. This paper aims to address this gap by providing an examination of the theoretical link between coalition formation, coalitional bargaining games, and MARL through the link of stochastic games. Through this analysis, the paper seeks to shed light on the underlying principles that support the use of MARL in coalitional bargaining and to explore the contributions of this approach and its limitations in comparison to traditional game theoretical methods. | Lucia CipolinaKun, Stephen Mak, Ignacio Carlucho, Vahid Yazdanpanah, Sebastian Stein, Enrico H. Gerding, Kalesha Bullard |  |
| 214 |  |  [Astroformer: More Data might not be all you need for Classification](https://openreview.net/forum?id=ChqP6ORFYK6) |  | 0 |  | Rishit Dagli |  |
| 215 |  |  [Error Analysis of Fitted Q-iteration with ReLU-activated Deep Neural Networks](https://openreview.net/forum?id=EVwbNcRa6Yf) |  | 0 |  | Lican Kang, Han Yuan, Chang Zhu |  |
| 216 |  |  [General Purpose Artificial Intelligence Systems as Group Agents](https://openreview.net/forum?id=ddFJsnpZtTX) |  | 0 |  | Matija Franklin |  |
| 217 |  |  [An Empirical Study of the Effect of Background Data Size on the Stability of SHapley Additive exPlanations (SHAP) for Deep Learning Models](https://openreview.net/forum?id=L38bbHmRKx) |  | 0 |  | Han Yuan, Mingxuan Liu, Lican Kang, Chenkui Miao, Ying Wu |  |
| 218 |  |  [Interpretable Machine Learning-Based Risk Scoring with Individual and Ensemble Model Selection for Clinical Decision Making](https://openreview.net/forum?id=RNlfw6KXJey) |  | 0 |  | Han Yuan, Jin Wee Lee, Mingxuan Liu, Siqi Li, Chenglin Niu, Jun Wen, Feng Xie |  |
| 219 |  |  [Analytical solutions for a family of single layer neural network regression problems](https://openreview.net/forum?id=g6ZFp73_T7) |  | 0 |  | Siddharth Krishna Kumar |  |
| 220 |  |  [Pretrained Vision Models for Predicting High-Risk Breast Cancer Stage](https://openreview.net/forum?id=Idalad_7wG) |  | 0 |  | Bonaventure F. P. Dossou, Yenoukoume S. K. Gbenou, Miglanche Ghomsi Nono |  |
