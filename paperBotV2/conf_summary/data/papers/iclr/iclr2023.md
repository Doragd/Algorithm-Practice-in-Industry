# ICLR2023

## 会议论文列表

本会议共有 220 篇论文

| 序号 | 标题 | 链接 | 推荐理由 | 推荐度 | 摘要 | 作者 | 组织 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 |  |  [The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023](https://openreview.net/group?id=ICLR.cc/2023/TinyPapers) |  | 0 |  | Krystal Maughan, Rosanne Liu, Thomas F. Burns |  |
| 2 |  |  [Statistical Property Testing for Generative Models](https://openreview.net/forum?id=xmY_plRB15j) |  | 0 | Generative models that produce images, text, or other types of data are recently be equipped with more powerful capabilities. Nevertheless, in some use cases of the generated data (e.g., using it for model training), one must ensure that the synthetic data points satisfy some properties that make them suitable for the intended use. Towards this goal, we present a simple framework to statistically check if the data produced by a generative model satisfy some property with a given confidence level. We apply our methodology to standard image and text-to-image generative models. | Emmanouil Seferis, Simon Burton, ChihHong Cheng |  |
| 3 |  |  [Can Conformal Prediction Obtain Meaningful Safety Guarantees for ML Models?](https://openreview.net/forum?id=5aO1lsEJGu) |  | 0 | Conformal Prediction (CP) has been recently proposed as a methodology to calibrate the predictions of Machine Learning (ML) models so that they can output rigorous quantification of their uncertainties. For example, one can calibrate the predictions of an ML model into prediction sets, that guarantee to cover the ground truth class with a probability larger than a specified threshold. In this paper, we study whether CP can provide strong statistical guarantees that would be required in safety-critical applications. Our evaluation on the ImageNet demonstrates that using CP over state-of-the-art models fails to deliver the required guarantees. We corroborate our results by deriving a simple connection between the CP prediction sets and top-k accuracy. | Emmanouil Seferis, Simon Burton, ChihHong Cheng |  |
| 4 |  |  [A two-parameter learnable Logmoid Activation Unit](https://openreview.net/forum?id=LcXWYmA8Ek) |  | 0 | A novel learnable Logmoid Activation Unit (LAU) is proposed as, $f(x)=x\ln(1+\alpha\textrm{sigmoid}(\beta x))$, by parameterizing Sigmoid with two hyper-parameters $\alpha$ and $\beta$ that are optimized by the back-propagation algorithm. The end-to-end deep neural networks with learnable LAUs can increase the predictive performances beyond well-known activation functions for different tasks. | Xuemei Zhou, Lingfang Li, Xingzhou Zheng, Mingxing Luo |  |
| 5 |  |  [Cross Domain Vulnerability Detection using Graph Contrastive Learning](https://openreview.net/forum?id=rrZtzI7xj2b) |  | 0 | To overcome the difficulty of finding good-quality labeled data in domains such as vulnerability detection, Self--Supervised Learning (SSL) methods such as Contrastive Learning (CL) algorithms were developed. We evaluate the performance of one such state-of-the-art CL method, GraphCL, that trains on our graph dataset generated from code repositories of six widely used C/C++ applications. We also propose a custom graph type having a new structure that combines both code-level and binary-level CPG graphs. This is because, even though existing graph types such as AST, CFG and CPG are effective in detecting vulnerabilities in the source code, it is ineffective in detecting the ones that only occur in the binary-level. Hence, to detect those vulnerabilities, we propose a new graph type, Cross Domain Control Property Graph (CDCPG). We perform extensive experiments, using different augmentation techniques and loss functions to show that our custom graph type, CDCPG, performs better than other graph types in many scenarios. | Mahmoud Zamani, Saquib Irtiza, Shamila Wickramasuriya, Latifur Khan, Kevin W. Hamlen |  |
| 6 |  |  [Tiny Attention: A Simple yet Effective Method for Learning Contextual Word Embeddings](https://openreview.net/forum?id=BWWrDHaP29) |  | 0 | Contextual Word Embedding (CWE) obtained via the Attention Mechanism in Transformer (AMT) models is one of the key drivers of the current revolution in Natural Language Processing. Previous techniques for learning CWEs are not only inferior to AMT but also are largely subpar to the simple bag-of-words baseline. Though there have been many variants of the Transformer model, the attention mechanism itself remains unchanged and is largely opaque. We introduce a new method for leaning CWEs that uses a simple and transparent attention mechanism. Our method is derived from the SVD based Syntagmatic Word Embeddings, which capture word associations. We test our model on the Word-in-Context dataset, and show that it outperforms the simple but tough-to-beat baseline by a substantial margin. | Renjith P. Ravindran, Narayana Murthy Kavi |  |
| 7 |  |  [Training Data Eigenvector Dynamics in the EigenPro Implementation of the Neural Tangent Kernel and Recursive Feature Machines](https://openreview.net/forum?id=8WiNDyXgj6) |  | 0 | There has been much recent work on kernel methods as a viable alternative to deep neural networks (DNNs). The advent of the $\textit{Neural Tangent Kernel}$ (NTK) has brought on renewed interest in these methods and their application to typical deep learning tasks. Recently, kernels have been shown to be capable of feature learning similar to that of DNNs, termed $\textit{Recursive Feature Machines}$ (RFMs). In accordance with the growing scale of kernel models, the EigenPro 3 algorithm was proposed to facilitate large-scale training based on preconditioned gradient descent. We propose an accessible framework for observing the eigenvector dynamics of EigenPro's training data in its implementation of these kernel methods, and find empirically that significant change ceases early in training along with apparent bias towards equilibrium. In the case of RFMs, we find that significant change in the training data eigenvectors typically curtails before five iterations, in accordance with findings that RFMs achieve optimal performance in five iterations. This represents a path forward in gaining intuition for the inner workings of large-scale kernel training methods. We provide an easy to use Python implementation of our framework at https://github.com/cgorlla/ep3dynamics. | Cyril Gorlla |  |
| 8 |  |  [Metric Transform: Exploring beyond Affine Transform for Neural Networks](https://openreview.net/forum?id=fVuTIiTBky) |  | 0 | Artificial Neural Networks(ANN) of varying architectures are generally paired with linear transformation at the core. However, we find dot product neurons with global influence less interpretable as compared to a more local influence of euclidean distance (as used in RBF). In this work, we explore the generalization of dot product neurons to lp-norm, metrics, and beyond. We find such metrics as transform performs similarly to affine transform when used in MLP or CNN. Furthermore, we use distance/similarity measuring neurons to interpret and explain input data, overfitting and Residual MLP. We share our code in github. | Suman Sapkota, Binod Bhattarai |  |
| 9 |  |  [MaskedFusion360: Reconstruct LiDAR Data by Querying Camera Features](https://openreview.net/forum?id=mIEMVZ47aNA) |  | 0 | In self-driving applications, LiDAR data provides accurate information about distances in 3D but lacks the semantic richness of camera data. Therefore, state-of-the-art methods for perception in urban scenes fuse data from both sensor types. In this work, we introduce a novel self-supervised method to fuse LiDAR and camera data for self-driving applications. We build upon masked autoencoders (MAEs) and train deep learning models to reconstruct masked LiDAR data from fused LiDAR and camera features. In contrast to related methods that use birds-eye-view representations, we fuse features from dense spherical LiDAR projections and features from fish-eye camera crops with a similar field of view. Therefore, we reduce the learned spatial transformations to moderate perspective transformations and do not require additional modules to generate dense LiDAR representations. Code is available at: https://github.com/KIT-MRT/masked-fusion-360 | Royden Wagner, Marvin Klemp, Carlos Fernández Lopez |  |
| 10 |  |  [Meta-Learning for Subject Adaptation in Low-Data Environments for EEG-Based Motor Imagery Brain-Computer Interfaces](https://openreview.net/forum?id=7QqlQW9hJ8J) |  | 0 | Motor imagery classification from Electroencephalogram (EEG) signals involves decoding information during the imagination of specific movements. However, learning representations for EEG-based motor imagery classification is challenging due to inter-subject variability and differences in mental imagery, resulting in poor generalization of deep learning models to new subjects. While pre-trained deep learning models achieve high accuracy on subjects with similar domains, they fail on subjects with dissimilar domains. Optimization-based meta-learning algorithms can address this limitation by learning a good initialization for the model, enabling quick adaptation to new subjects with limited fine-tuning examples. We demonstrate that our Meta Learning approach consistently outperforms Transfer Learning on the BCI Competition IV 2a dataset. Although accuracy varies depending on domain similarity, meta-learning demonstrates efficient adaption to unseen subjects with limited data. By improving generalization across subjects with different domains under low-data environments, we can enhance the reliability and practicality of brain-computer interfaces for real-world applications. | Arnav Pati, Deepak Mewada, Debasis Samanta |  |
| 11 |  |  [Fostering Effective Communication Between Humans and Machines](https://openreview.net/forum?id=AHnLJBD7xKx) |  | 0 | With the growing usage of smartphones, digital communication has become significant. This paper describes the primary research conducted to study user interaction patterns on smartphones. Results show that the time between two touches, or the editorial context duration, is just 5 to 10 seconds for a vast majority of smartphone interactions. So, the paper introduces the novel concept of MicroStimuli, which can generate a response in mere milliseconds, specifically tailored for smartphones. The construct of MicroStimuli is formulated by leveraging the neuroscience of decision-making in response to visual stimuli. | Karthika Kamath, Tanya Upadhyay, Jieya Rawal, Kirtana Sunil Phatnani, Biju Dominic |  |
| 12 |  |  [Transfer Learning on Kinyarwanda Tweets Sentiment Analysis](https://openreview.net/forum?id=7lyEQXHkGpl) |  | 0 | Pretrained models available on platform such as Hugging Face have become a valuable resource for machine learning community, particularly for natural language processing task. In this study, we evaluated the performance of Kinyarwanda and English pretrained models for sentiment analysis of Kinyarwanda tweets through transfer learning using Hugging Face pretrained models and Trainer for implementation. We have found that English pretrained models for translated Kinyarwanda tweets dataset using Google translate out performed Kinyarwanda pretrained models. | Roger Byakunda |  |
| 13 |  |  [Symbolic Regression in Financial Economics](https://openreview.net/forum?id=RuCQRXk7a7G) |  | 0 | We apply symbolic regression, the machine learning approach of recovering models from data, in financial economics. Specifically, we present a data set consisting of equations that cover a broad range of topics in financial economics. These equations are built off a common set of mathematical symbols but importantly have new variations in functional forms. We test the joint performance of deep learning and genetic programming symbolic regression systems in recovering these non-physical equations. | Jiacheng Liu, Siqi Guo |  |
| 14 |  |  [FRESCO: Federated Reinforcement Energy System for Cooperative Optimization](https://openreview.net/forum?id=75mWq5j4iso) |  | 0 | The rise in renewable energy is creating new dynamics in the energy grid that promise to create a cleaner and more participative energy grid, where technology plays a crucial part in creating the required flexibility to achieve the vision of the next-generation grid. This work presents FRESCO, a framework that aims to ease the implementation of energy markets using a hierarchical control architecture of reinforcement learning agents trained using federated learning. The core concept we are proving is that having greedy agents subject to changing conditions from a higher level agent creates a cooperative setup that will allow for fulfilling all the individual objectives. This paper presents a general overview of the framework, the current progress, and some insights we obtained from the recent results. | Nicolas M. Cuadrado, Roberto Alejandro Gutiérrez Guillén, Martin Takác |  |
| 15 |  |  [Learning Rotation-Agnostic Representations via Group Equivariant VAEs](https://openreview.net/forum?id=jbNqgEJf0EI) |  | 0 | An emerging field in representation learning involves the study of group-equivariant neural networks, that leverage concepts from group representation theory to design neural architectures that can exploit discrete and continuous symmetries to produce more general representations. Following this direction, in this work we demonstrate how an image embedding agnostic to rotations can be naturally obtained by training a variational autoencoder (S-GVAE) equipped with a Group equivariant Convolutional Neural Network (G-CNN) encoder. | Ahmedeo Shokry, Antonio Norelli |  |
| 16 |  |  [Can Text Encoders be Deceived by Length Attack?](https://openreview.net/forum?id=KPHbtTtCDw) |  | 0 | Albeit \textit{de facto} to use in training dense retrieval models, we observe that contrastive learning is prone to length overfitting, making it vulnerable to adversarial length attacks. We examine the behaviour of this phenomenon and propose an editing method to mitigate this problem. We find that our method can effectively improve the robustness of models against length attacks. Its effectiveness can be attributed to reduced length information in the embeddings, more robust intra-document token interaction, and enhanced isotropy at trained length range. | Chenghao Xiao, Zihuiwen Ye, G. Thomas Hudson, Zhongtian Sun, Phil Blunsom, Noura Al Moubayed |  |
| 17 |  |  [Whispering Across the Continent: Collecting and Analyzing African Culture Using Community Radios](https://openreview.net/forum?id=kidk_E11QQ) |  | 0 | African culture is rich and diverse, but much of its knowledge is held by the elders of the community and passed down through oral traditions. With globalization, young Africans are becoming increasingly disconnected from their roots, making it essential to collect and preserve this knowledge. However, the lack of accessible data on African culture presents a significant challenge. This research aims to address this problem by exploring new ways to collect and preserve African cultural data. Specifically, we have developed a device to perform continuous recording of cultural radio programs in local languages, which has enabled us to collect over 1500 hours of audio data. We are also exploring the use of a whisper model, which has proven to be effective in outperforming human transcription and being multilingual. This research project's final goal is to build a language model that understands African culture, providing an effective approach to store this knowledge for future generations to learn about their culture. | Joseph Domguia, Guy Adingono Nkama |  |
| 18 |  |  [Handling unstructured data for operator learning using implicit neural representations](https://openreview.net/forum?id=e2gSQqH3V10) |  | 0 | Operator learning methods are too often constrained by a fixed sampling of both the input and output functions. We propose a novel method to allow current operator learning methods to learn on any sampling. We show that our method can perform inference on unseen samplings, and that it allows returning outputs as continuous functions. | Thomas X. Wang, Patrick Gallinari |  |
| 19 |  |  [Model Extraction Attacks on DistilBERT](https://openreview.net/forum?id=njpSzZ6mCU) |  | 0 | This paper investigates model extraction attacks, where an adversary can train a substitute model by collecting data through query access to a victim model and stealing its functionality. We use DistilBERT as the victim model due to its smaller size and faster processing speed. The results demonstrate the effectiveness of the model extraction attack and show that fine-tuning more powerful language models can improve accuracy. The study provides important insights into the security of machine learning models. | Amro Salman, Ayman Saeed, Khalid N. Elmadani, Sharief Babiker |  |
| 20 |  |  [One Important Thing To Do Before Federated Training](https://openreview.net/forum?id=qq-bA-VLUN) |  | 0 | Previous research in Federated learning (FL) have emphasized privacy protection, model optimization, and so on, meanwhile, they overlooked how to choose the appropriate FL algorithm for a new federation with preserving data privacy. In our study, we provide a formal problem formulation for algorithm selection in FL and present a novel approach that involves leveraging trained federations to aid with algorithm selection. Empirical results prove the effectiveness of our method. | Yichu Xu, Wenqian Li, Yinchuan Li, Yunfeng Shao, Yan Pang, DeChuan Zhan |  |
| 21 |  |  [Insights into the mechanism behind reusing Teacher's classifier in Knowledge Distillation](https://openreview.net/forum?id=_cL7Uj4LXAJ) |  | 0 | Knowledge distillation (KD) has emerged as an effective approach to compress deep neural networks by transferring knowledge from a powerful yet cumbersome teacher model to a lightweight student model. Recent research has suggested that re-using the teacher's final layer (i.e., the classifier) can be a straightforward and effective method for knowledge distillation. The underlying mechanism for this method's success remains unclear. Our study aims to shed light on how the knowledge distillation loss affects the alignment between the weights of the student classifier and the teacher classifier. Specifically, we compare the $L^2$ norm of the difference between the weights of the student and the teacher classifier during the training process. Our experiments demonstrate that the knowledge distillation loss encourages alignment between the student and teacher classifiers, as indicated by a strong positive correlation ($>0.97$) between the $L^2$ norm and the loss during training. We also observe that as temperature increases, this alignment decreases and the $L^2$ norm behaves similar to normal (non-KD) training. Our analysis aims to provide to a better understanding of knowledge distillation provide a starting point for the development of new KD frameworks. | Kinshuk Dua |  |
| 22 |  |  [When will federated learning transfer from generalization to personalization?](https://openreview.net/forum?id=iCqvQSvar5V) |  | 0 | The timing of personalization refers to determining when to train and update personalized models for the participants in Personalized federated learning. Determining the timing for personalization contributes to improving the overall efficiency of federated learning. We propose that training transfers to personalization when the accuracy of the global model reaches a predefined threshold. Experimental results show that this method can effectively improve the accuracy of personalized models in a non-IID scenario. | Wenxuan Liu, Lukun Wang, Jiaming Pei |  |
| 23 |  |  [A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion](https://openreview.net/forum?id=S9NTReFikL2) |  | 0 | Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities. | Zeinab Sadat Taghavi, Ali Satvaty, Hossein Sameti |  |
| 24 |  |  [Fairness Under Partial Observability](https://openreview.net/forum?id=if1Mmrxf-pq) |  | 0 | The purpose of this article is to discuss an important challenge faced in \`\`real life'' when trying to implement \emph{group} fairness-aware models and algorithms. Here, we focus specifically on the role that uncertainty and ambiguity play and revisit the case where protected attributes are only partially observable. | Islam Utyagulov, Francois BuetGolfouse, Peter Hill |  |
| 25 |  |  [Geodesic Mode Connectivity](https://openreview.net/forum?id=cFtt9fU7YB6) |  | 0 | Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity. | Charlie Tan, Theodore Long, Sarah Zhao, Rudolf Laine |  |
| 26 |  |  [Pursuit Policies in Dynamic Environments](https://openreview.net/forum?id=_cZLvP7LAt7) |  | 0 | Cooperative pursuit is a popular multi-agent reinforcement learning (MARL) game where a team of predators target prey while avoiding obstacles. Previous literature has largely considered the impact of different predator, prey abilities on learning. Here, we investigate the impact of dynamic environments on learning predator pursuit policies from partial observations with deep Q-learning. Interestingly, we find predators are able to learn cooperative pursuit strategies that leverage moving obstacles. | Joseph L. Briones, Andréa W. Richa |  |
| 27 |  |  [Resource-efficient image inpainting](https://openreview.net/forum?id=OJILbuOodvm) |  | 0 | Image inpainting refers to the synthesis of missing regions in an image, which can help restore occluded or degraded areas and also serve as a precursor task for self-supervision. The current state-of-the-art models for image inpainting are computationally heavy as they are based on vision transformer backbones in adversarial or diffusion settings. This paper diverges from vision transformers by using a computationally-efficient WaveMix-based fully convolutional architecture, which uses a 2D-discrete wavelet transform (DWT) for spatial and multi-resolution token-mixing along with convolutional layers. The proposed model outperforms the current-state-of-the-art models for large mask inpainting on reconstruction quality while also using less than half the parameter count and considerably lower training and evaluation times. | Dharshan Sampath Kumar, Pranav Jeevan, Amit Sethi |  |
| 28 |  |  [Recursive Reasoning with Neural Networks](https://openreview.net/forum?id=TS8l4VS7_BK) |  | 0 | Many problems can naturally be thought about recursively. However, neural networks fundamentally cannot reason this way on arbitrarily large problems. This is because they do not have the memory to maintain state for the maximum recursion depth required. Solving this issue would enable neural networks to reason like a wide range of classical recursive algorithms (e.g., tree search in model-based RL). To address this, we propose a neural architecture augmented with a stack that learns to save and recall state as needed. We empirically demonstrate the utility of this method on a recursive neural algorithmic reasoning task (learning depth-first search) and show that our architecture leads to improved generalization. | Jonas Jürß, Dulhan Hansaja Jayalath |  |
| 29 |  |  [SUDANESE ARABIC DIALECT ENCODING USING XLM-RoBERTa LANGUAGE MODEL: Zol-ROBERTA](https://openreview.net/forum?id=wUEY2CdQGi1) |  | 0 |  | Duaa Badradein Alshareif, Taiseer Abdulateef Fadlalla, Muhammed Yahya Saeed, Hiba Hassan S. M. Ali |  |
| 30 |  |  [Reducing the Effect of Incomplete Annotations in Object Detection for Histopathology](https://openreview.net/forum?id=PIfJnq9kpdw) |  | 0 | Training neural networks for object detection usually requires decent amounts of data to produce great results. Apart from the image variety, the number of annotated objects is a crucial factor for success. In histopathology, the average annotation density is very high, resulting in resource-consuming data preparation for neural network training. We explore the effect of incomplete annotations in object detection. We show that modern object detectors, such as YOLO-v5, can effectively learn from histopathology datasets that lack up to 90% of annotations. Additionally, we suggest an easy model tuning setup to reduce the impact of incomplete annotations and enhance learning capability overall. We publish our code at https://github.com/DenysKaliuzhnyi/yolov5. | Denys Kaliuzhnyi, Dmytro Fishman, Mikhail Papkov |  |
| 31 |  |  [Experimenting with Multimodal AutoML: Detection and Evaluation of Alzheimer's Disease](https://openreview.net/forum?id=nSqrgBKBGkv) |  | 0 | This paper describes an experiment using AutoML, AutoGluon Tabular, to discover multimodal models for MMSE regression and AD detection. Using the ADReSSo dataset, this paper reports enhanced performance in classification models and comparable performance in regression models to the baseline, achieving a significant improvement of up to 82\% accuracy on the test dataset. In contrast, the test RMSE has a marginal difference of only 0.28 compared to the baseline. | Ujjawal Shah, Saurav Keshari Aryal |  |
| 32 |  |  [GFlowNets with Human Feedback](https://openreview.net/forum?id=2KxH_4US0ZH) |  | 0 | We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration of training language models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF, and thus is more suitable for large-scale language model tasks. | Yinchuan Li, Shuang Luo, Yunfeng Shao, Jianye Hao |  |
| 33 |  |  [Attention-likelihood relationship in Transformers](https://openreview.net/forum?id=R82eeIF4rP_) |  | 0 | We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at [url]. | Valeria Ruscio, Valentino Maiorca, Fabrizio Silvestri |  |
| 34 |  |  [Truly Generative Data Augmentation for Image Segmentation - Case of Cloud Images](https://openreview.net/forum?id=cQ_eEMsc6p) |  | 0 | Supervised learning frameworks frequently rely on semantic image segmentation, which necessitates a substantial amount of annotated data. Existing methodologies for data augmentation either employ image transformations that are limited by the cardinality of the original dataset or employ generative augmentation techniques that introduce pixel categorization errors. This paper presents an innovative approach for "truly" generative data augmentation for image segmentation, specifically in the context of sky/cloud images. The proposed method involves separate generation of the background clear sky image and the foreground cloud masks using two separate DCGANs, which are subsequently merged to produce augmented images. This organic approach enhances the quality of generated images while preserving accurate pixel categorization. The proposed approach is finally noted to improve the robustness of the sky/cloud image segmentation models. | Mayank Jain, Soumyabrata Dev |  |
| 35 |  |  [Fast Fourier Convolutions in Self-Supervised Neural Networks for Image Denoising](https://openreview.net/forum?id=ghfL1e2rOd) |  | 0 | Recently, denoising convolutional neural networks (CNN) have started to outperform classical denoising algorithms. However, CNNs performance could be constrained by the limited receptive field of regular convolution. To mitigate this problem, a new modification for CNNs was proposed: Fast Fourier Convolution (FFC). Here, a global receptive field is achieved by using Fourier Transform and convolving spectral representation. The global perception field can help CNNs to better capture dependencies in image regions that are far apart. In this work, we design multiple approaches for incorporating FFC into self-supervised neural networks for image denoising. We evaluate these approaches on three benchmark datasets and compare them with supervised and self-supervised methods. We empirically show that an FFC-enhanced denoising network achieves the state-of-the- art results on the character dataset and shows a comparable level of performance for both grayscale and color natural images. | Joonas Ariva, Mikhail Papkov |  |
| 36 |  |  [Personalized Federated Learning for Medical Segmentation using Hypernetworks](https://openreview.net/forum?id=lpcO1957Tv) |  | 0 | In federated learning (FL), several clients jointly train a shared model without sharing their data, maintaining data privacy and reducing communication costs. In personalized federated learning (PFL), each client has their own model, and models are trained jointly. Hypernetworks have been shown to be useful for PFL in classification problems, but it is still not clear how to apply them to problems like segmentation. There, models are very large, and it is not known what parts of models should be personalized, and what parts should be shared across clients. Here, we explore HNs for PFL for solving a problem of image segmentation in the context of medical imaging diagnosis. Using MRI scans for prostate segmentation, we demonstrate that using a hypernetwork to personalize a single convolution layer and the batch-norm layer outperforms local and FL baselines. | Hilit Segev, Gal Chechik |  |
| 37 |  |  [Career Path Modeling and Recommendations with Linkedin Career Data and Predicted Salary Estimations](https://openreview.net/forum?id=R5NNAThG0i) |  | 0 | Career planning involves devising a sequence of steps that build up an ideal career path for a person. However, career planning has become more complex in recent years, demanding the need for better models and systems for recommending Career Paths. With that in mind, we explored new variables and techniques that could help in predicting better career paths. We built a Long-Short Term Memory Network with Self Attention Layers called LSTM-ATT, that predicts a person’s possible career path using Linkedin Career History and new variables such as salary estimations and social networks. We measured the model’s performance in terms of Mean Percentile Rank and Precision at 50 and 100. We found that LSTM and self-attention layers were able to show good predictive performance for multi-class classification even with over 6000 classes for companies and skills, effectively beating a multi-channel CNN for all metrics. However, by checking the versions of either model with added features, they did not yield any major increase in predictive accuracy against the models without it. This leads us to conclude that the added variables did not help in predicting better career paths. | Micaela Tayoto Cerilla, Aaron Santillan, Carl John Vinas, Michael B. Dela Fuente |  |
| 38 |  |  [Sleep Deprivation in the Forward-Forward Algorithm](https://openreview.net/forum?id=q_lJooPbX_) |  | 0 | This paper aims to explore the separation of the two forward passes in the Forward-Forward algorithm from a biological perspective in the context of sleep. We show the size of the gap between the sleep and awake phase influences the learning capabilities of the algorithm and highlight the importance of negative data in diminishing the devastating effects of sleep deprivation. | Mircea Tudor Lica, David DinucuJianu |  |
| 39 |  |  [MetaXLR - Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit](https://openreview.net/forum?id=nF70Sl-HUZ) |  | 0 | Transfer learning for extremely low-resource languages is a challenging task as there is no large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning. We follow the work of (Xia et al., 2021) which suggests using meta learning for transfer learning from a single source language to an extremely low resource one. We propose an enhanced approach which uses multiple source languages chosen in a data-driven manner. In addition, we introduce a sample selection strategy for utilizing the languages in training by using a multi armed bandit algorithm. Using both of these improvements we managed to achieve state-of-the-art results on the NER task for the extremely low resource languages even with the same amount of data, making the representations better generalized. Also, due to the method’s ability to use multiple languages it allows the framework to use much larger amounts of data, while still having superior results over the former MetaXL method even with the same amounts of data. | Liat Bezalel, Eyal Orgad |  |
| 40 |  |  [Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models](https://openreview.net/forum?id=cKlgcx7nSZ) |  | 0 | Massive language models with billions of parameters have significant compute expenses and thus can benefit from pruning. Pruning techniques for massive models are typically iterative and require extensive weight retraining after pruning. SparseGPT, a recently introduced one-shot technique for pruning such models, enables pruning without retraining. We improve upon SparseGPT by fine-tuning during pruning with minimal training steps, and we perform experiments against magnitude pruning and find that our iteratively fine-tuned SparseGPT models significantly outperform their magnitude pruning counterparts at high sparsity. | Aaquib Syed, Phillip Guo, Vijaykaarti Sundarapandiyan |  |
| 41 |  |  [Model Extraction Attacks on Arabic BERT-Based APIs](https://openreview.net/forum?id=XptW6NuULJ) |  | 0 | In this paper, we study the feasibility of performing Model Extraction attacks on Arabic BERT-based APIs. In our experiments, we try to perform these attacks under different scenarios and observe the accuracy of the extracted model against the victim model. We then propose a method for protecting against these types of attacks by introducing noise, in the form of pre-training the victim model for more epochs on non-public generic data. Our results show that this strategy better secures the victim model from such attacks. | Hassan Abbelkarim, Mohammed Eltahir, Khalid N. Elmadani, Anas Showk |  |
| 42 |  |  [IMITATION LEARNING USING THE FORWARD-FORWARD ALGORITHM](https://openreview.net/forum?id=baF9FqIdTY) |  | 0 | The forward-forward (FF) algorithm has been recently introduced as a novel approach to training neural networks in a way that approximates the behavior of real neurons. Nevertheless, its application has been limited to visual domains and has not been investigated in the context of imitation learning or reinforcement learning. In this study, we evaluate the FF algorithm in the context of imitation learning. We implement a straightforward imitation model based on the FF algorithm and present a comparative analysis with the backpropagation (BP) model. Our findings indicate that the FF-based model exhibits comparable performance to the BP-based model on larger datasets but demonstrates inferior performance on smaller datasets. | Insik Chung, Isaac Han, KyungJoong Kim |  |
| 43 |  |  [The Geometry of Multilingual Language Models: An Equality Lens](https://openreview.net/forum?id=dGuMR8tLDs) |  | 0 | Understanding the representations of different languages in multilingual language models is essential for comprehending their cross-lingual properties, predicting their performance on downstream tasks, and identifying any biases across languages. In our study, we analyze the geometry of three multilingual language models in Euclidean space and find that all languages are represented by unique geometries. Although languages tend to be closer according to their linguistic family, they are almost separable with languages from other families. We also introduce a Cross-Lingual Similarity Index to study the semantic similarity across languages. Our findings indicate that the representation of low-resource languages is low compared to high-resource languages. | Cheril Shah, Yashashree Chandak, Manan Suri |  |
| 44 |  |  [Dynamic Human AI Collaboration](https://openreview.net/forum?id=Muwb2KohnX) |  | 0 | Domain experts possess valuable knowledge and insights that can help improve the accuracy and relevance of the machine learning (ML) models. By incorporating expert opinions, the models can capture important nuances and factors that may not be captured by data-driven methods alone. The integration of machine learning models with human experts has become increasingly common in real-world applications. In this paper, we propose a Bayesian framework for human-in-the-loop pipelines. We consider the scenario where the final decision is an amalgamation of algorithm and expert opinions and deferral systems are a special case. We finally show that updating expert opinion priors with information sharing between experts is key to achieving superior performance. | Parth Pahwa, Kabir Thakur, Francois BuetGolfouse |  |
| 45 |  |  [The Responsibility Problem in Neural Networks with Unordered Targets](https://openreview.net/forum?id=jd7Hy1jRiv4) |  | 0 | We discuss the discontinuities that arise when mapping unordered objects to neural network outputs of fixed permutation, referred to as the responsibility problem. Prior work has proved the existence of the issue by identifying a single discontinuity. Here, we show that discontinuities under such models are uncountably infinite, motivating further research into neural networks for unordered data. | Ben Hayes, Charalampos Saitis, György Fazekas |  |
| 46 |  |  [Concept Understanding in Large Language Models: An Empirical Study](https://openreview.net/forum?id=losgEaOWIL7) |  | 0 | Large Language Models (LLMs) have demonstrated their superior comprehension and expressiveness across a wide range of tasks, and exhibited remarkable capabilities in real-world applications. Hence, it is crucial to investigate their potential and limitations for trustworthy performance in both academia and industry. In this paper, we focus on exploring LLMs' ability to understand concepts, especially abstract and concrete ones. To this end, we construct a WordNet-based dataset containing a subset for abstract concepts and a subset for concrete concepts. We select six pre-trained LLMs and conduct a classic NLP task, hypernym discovery, as evidence of LLMs' comprehension ability in understanding concepts. The experimental results suggest that the LLM's understanding of abstract concepts is significantly weaker than that of concrete concepts. | Jiayi Liao, Xu Chen, Lun Du |  |
| 47 |  |  [Adaptive Distance Message Passing From the Multi-Relational Edge View](https://openreview.net/forum?id=rAT51tL04I2) |  | 0 | Message-passing graph neural networks (MP-GNNs) excel in deep learning on graphs. Despite their success in various studies, they are limited by passing information to the fixed length $k$ distance neighbouring nodes, where $k$ is the number of layers. In reality, different types of edges (alternatively relations) may influence nodes at varying distance and should not be uniformly treated. This paper proposes an adaptive distance message-passing method that considers the unique roles of edge types, addressing this issue. Experiments on real-world datasets validate the effectiveness of our approach. | Zhongtian Sun, Alexandra I. Cristea, Pietro Lio, Jialin Yu |  |
| 48 |  |  [Sustainable Resource Management](https://openreview.net/forum?id=DLwlmWwmJBi) |  | 0 | Given finite resources and growing demand, a supply-side balance must be struck between maximising profit and sustainable resource management. This paper combines the two techniques in a stochastic setting to create a sustainable profit model and uses Gaussian processes to estimate and bound resource dynamics. | Nicholas William David Martin, Peter Hill, Tingsheng Tan, Francois BuetGolfouse |  |
| 49 |  |  [Chain Of Thought Prompting Under Streaming Batch: A Case Study](https://openreview.net/forum?id=n5aZMLXVndP) |  | 0 | Recently, Large Language Models (LLMs) have demonstrated remarkable capa- bilities. Chain-of-Thought (CoT) has been proposed as a way of assisting LLMs in performing complex reasoning. However, developing effective prompts can be a challenging and labor-intensive task. Many studies come out of some way to au- tomatically construct CoT from test data. Most of them assume that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption. In this paper, we present a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings. | Yuxin Tang |  |
| 50 |  |  [Language Models can do Zero-Shot Visual Referring Expression Comprehension](https://openreview.net/forum?id=F7mdgA7c2zD) |  | 0 | The use of visual referring expressions is an important aspect of human-robot in- teractions. Comprehending referring expressions (ReC) like “the brown cookie near the cup” requires to understand both self-referential expressions, “brown cookie”, and relational referential expressions, “near the cup”. Large pretrained Vision-Language models like CLIP excel at handling self-referential expressions, while struggle with the latter. In this work, we reframe ReC as a language reasoning task and explore whether it can be addressed using large pretrained language models (LLMs), including GPT-3.5 and GPT-4. Given the textual attribute (object category, color, center location, size), GPT-3.5 performs unstably on understanding spatial relationships even with heavy prompt engineering, while GPT-4 shows strong and stable zero-shot relation reasoning. Evaluation on RefCOCO/g datasets and scenarios of interactive robot grasping shows that LLMs can do ReC with decent performance. It suggests a vast potential of using LLMs to enhance the reasoning in vision tasks. The code can be accessed at https://github.com/xiuchao/LLM4ReC. | Xiuchao Sui, Shaohua Li, Hong Yang, Hongyuan Zhu, Yan Wu |  |
| 51 |  |  [Simple Parameter-free Self-attention Approximation](https://openreview.net/forum?id=isodM5jTA7h) |  | 0 | The hybrid model of self-attention and convolution is one of the methods to lighten ViT. The quadratic computational complexity of self-attention with respect to token length limits the efficiency of ViT on edge devices. We propose a self-attention approximation without training parameters, called SPSA, which captures global spatial features with linear complexity. To verify the effectiveness of SPSA combined with convolution, we conduct extensive experiments on image classification and object detection tasks. The source code will be available. | Yuwen Zhai, Jing Hao, Liang Gao, Xinyu Li, Yiping Gao, Shumin Han |  |
| 52 |  |  [Neuromodulation Gated Transformer](https://openreview.net/forum?id=cYKtDg5JnxV) |  | 0 | We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets. | Kobe Knowles, Joshua Bensemann, Diana Benavides Prado, Vithya Yogarajan, Michael Witbrock, Gillian Dobbie, Yang Chen |  |
| 53 |  |  [Decomposing Causality and Fairness](https://openreview.net/forum?id=Lm7z2vYergk) |  | 0 | It is often informative to decompose key quantities of interest into smaller components, in order to develop a better understanding of the key quantity. In this paper, we focus causality and fairness, where bias attribution can be particularly useful. We show how quantities can be broken down based on independence, or conditional independence criteria, and show how such a decomposition can be used as a diagnosis tool. | Peter Hill, Francois BuetGolfouse |  |
| 54 |  |  [Self-Supervised Image Denoising with Swin Transformer](https://openreview.net/forum?id=EARgl3EH-nq) |  | 0 | Self-supervised image denoising aims to reconstruct signal from a noisy image with no additional information. Typically, this is accomplished by means of specific frameworks built upon fully-convolutional neural networks. In two such frameworks, Noise2Self and Noise2Same, we replaced conventional convolutional backbones with a state-of-the-art Swin Transformer-based model. In this paper, we summarize the results of experiments on a range of datasets and examine the advantages and limitations of transformers in self-supervised denoising. | Pavel Chizhov, Mikhail Papkov |  |
| 55 |  |  [SimbaML: Connecting Mechanistic Models and Machine Learning with Augmented Data](https://openreview.net/forum?id=1wtUadpmVzu) |  | 0 | Training sophisticated machine learning (ML) models requires large datasets that are difficult or expensive to collect for many applications. If prior knowledge about system dynamics is available, mechanistic representations can be used to supplement real-world data. We present SimbaML (Simulation-Based ML), an open-source tool that unifies realistic synthetic dataset generation from ordinary differential equation-based models and the direct analysis and inclusion in ML pipelines. SimbaML conveniently enables investigating transfer learning from synthetic to real-world data, data augmentation, identifying needs for data collection, and benchmarking physics-informed ML approaches. SimbaML is available from https://pypi.org/project/simba-ml/. | Maximilian Kleissl, Lukas Drews, Benedict B. Heyder, Julian Zabbarov, Pascal Iversen, Simon Witzke, Bernhard Y. Renard, Katharina Baum |  |
| 56 |  |  [GeValDi: Generative Validation of Discriminative Models](https://openreview.net/forum?id=zwywBS3GyFs) |  | 0 | Evaluation of machine learning (ML) models is critically important for reliable use. Though typically done via unseen data, such validation datasets often need to be large and hard to procure; additionally, mutliple models may perform equally well on such datasets. To address these challenges, we offer GeValdi: a data-efficient method to validate discriminative classifiers by creating samples where such classifiers maximally differ. We demonstrate how such \`\`maximally different samples'' can be constructed and leveraged to probe the failure modes of classifiers and offer a hierarchically-aware metric to further support fine-grained, comparative model evaluation. | Vivek Palaniappan, Matthew Ashman, Katherine M. Collins, Juyeon Heo, Adrian Weller, Umang Bhatt |  |
| 57 |  |  [Regularized Offline GFlowNets](https://openreview.net/forum?id=kbhUUAMZmQT) |  | 0 |  | Haozhi Wang, Yunfeng Shao, Jianye Hao, Yinchuan Li |  |
| 58 |  |  [Secure Communication Model for Quantum Federated Learning: A Proof of Concept](https://openreview.net/forum?id=xZGPLvRpf4N) |  | 0 | We design a model of Post Quantum Cryptography (PQC) Quantum Federated Learning (QFL). We develop a proof of concept with a dynamic server selection and study convergence and security conditions. We develop a preliminary study with a proof of concept model of post-quantum secure QFL. | Dev Gurung, Shiva Raj Pokhrel, Gang Li |  |
| 59 |  |  [Learn to Select: Efficient Cross-device Federated Learning via Reinforcement Learning](https://openreview.net/forum?id=wecTsVkrjit) |  | 0 | Federated Learning (FL) is a collaborative training method that provides data privacy in the age of big data. However, it is often ineffective on edge devices due to their heterogeneous and constrained resources. The primary challenge is to identify devices with useful training data and available compute capability, which are both time-varying. In this paper, we propose FedRank, a novel federated learning approach based on reinforcement learning. Our approach addresses the device selection problem by casting it as a ranking problem and employing a pairwise training scheme. Furthermore, we leverage imitation learning from state-of-the-art algorithms to eliminate the cold start phenomenon that offsets the benefits of previous learning-based approaches. Experimental results show that FedRank improves model accuracy by 2.59\%-12.75\%, and accelerates the training process by up to 2.25$\times$ and 1.76$\times$ on average at the same time. | Chunlin Tian, Zhan Shi, Li Li |  |
| 60 |  |  [A Dynamic Prompt-tuning Method for Data Augmentation with Associated Knowledge](https://openreview.net/forum?id=hli7A0ioiS_) |  | 0 | Transformer-based pretrained language models (PLMs) have shown to pre-learn rich prior knowledge. To assist data-to-text task, we propose a new dynamic prompt tuning method, DPTAK, to retrieve knowledge from a PLM that is associated with individual data-text pairs. Our method increases the diversity of the training examples without the need to manually collecting and labelling data. When applied on GPT-2, DPTAK outperforms baseline models in several well-studied data-to-text and text-to-data datasets such as E2E, WebNLG, DART. | Qianqian Qi, Qiming Bao, Alex Yuxuan Peng, Jiamou Liu, Michael Witbrock |  |
| 61 |  |  [MARKOVIAN EMBEDDINGS FOR COALITIONAL BARGAINING GAMES](https://openreview.net/forum?id=1LeLyB6T0JM) |  | 0 | We examine the Markovian properties of coalition bargaining games, in particular, the case where past rejected proposals cannot be repeated. We propose a Markovian embedding with filtrations to render the sates Markovian and thus, fit into the framework of stochastic games. | Lucia CipolinaKun |  |
| 62 |  |  [The Point to Which Soft Actor-Critic Converges](https://openreview.net/forum?id=1_PEwKmTepo) |  | 0 | Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence. | Jianfei Ma |  |
| 63 |  |  [No Double Descent in Self-Supervised Learning](https://openreview.net/forum?id=qNJRvdKDGYg) |  | 0 | Most investigations into double descent have focused on supervised models while the few works studying self-supervised settings find a surprising lack of the phenomenon. These results imply that double descent may not exist in self-supervised models. We show this empirically in two additional previously unstudied settings using a standard and linear autoencoder. We observe the test loss has either a classical U-shape or monotonically decreases, rather than exhibiting the double-descent curve. We hope that further work on this will help elucidate the theoretical underpinnings of this phenomenon. | Dulhan Hansaja Jayalath, Alisia Maria Lupidi, Yonatan Gideoni |  |
| 64 |  |  [Resampling Gradients Vanish in Differentiable Sequential Monte Carlo Samplers](https://openreview.net/forum?id=kBkou5ucR_d) |  | 0 | Annealed Importance Sampling (AIS) moves particles along a Markov chain from a tractable initial distribution to an intractable target distribution. The recently proposed Differentiable AIS (DAIS) (Geffner & Domke, 2021; Zhang et al., 2021) enables efficient optimization of the transition kernels of AIS and of the distributions. However, we observe a low effective sample size in DAIS, indicating degenerate distributions. We thus propose to extend DAIS by a resampling step inspired by Sequential Monte Carlo. Surprisingly, we find empirically—and can explain theoretically—that it is not necessary to differentiate through the resampling step which avoids gradient variance issues observed in similar approaches for Particle Filters (Maddison et al., 2017; Naesseth et al., 2018; Le et al., 2018). | Johannes Zenn, Robert Bamler |  |
| 65 |  |  [Generalised Lookahead Optimiser](https://openreview.net/forum?id=uNrSvEr9Lqc) |  | 0 | The vast majority of deep learning models are trained using SGD or one of its variants. Zhang et al. (2019) suggested the Lookahead optimiser as an alternative which enjoys remarkable test performance on many established datasets and mod- els. In this work we investigate a generalisation of this optimisation method. We validate the method empirically, generally demonstrating better results and faster convergence relative to the baselines of SGD and Lookahead | CostinAndrei Oncescu, Jack Valmadre, João F. Henriques |  |
| 66 |  |  [Revisiting CounteRGAN for Counterfactual Explainability of Graphs](https://openreview.net/forum?id=d0m0Rl15q3g) |  | 0 | Counterfactual explainability (CE) has been widely explored in various domains ranging from medical image diagnosis to self-driving cars. Graph CE (GCE), on the other hand, and especially, generative-based GCE has yet to be explored. Here, we adapt CounteRGAN, an image-based generative approach, to consider graph adjacency matrices as special black-and-white images and sample valid counterfactuals directly from the learnt latent space probabilistic distribution. | Mario Alfonso PradoRomero, Bardh Prenkaj, Giovanni Stilo |  |
| 67 |  |  [Language Models Inversely Scale on Piecewise Function Evaluation with Biased Examples](https://openreview.net/forum?id=GJhsHNKm7kj) |  | 0 | We investigate whether pretrained language models (LMs) can be misled by providing them with factually correct, but unrepresentative/biased examples, in the context of integer-to-integer piecewise functions. Given the definition of a piecewise function and several examples of the function’s evaluation, we instruct LMs to apply the function to a new input. We assess LMs on two variants of this task: one where the example function evaluations are evenly distributed across both branches of the function, and one where all of the examples exercise one branch of the input and the target input exercises the other branch. We observe that model performance positively scales with model size only when examples are balanced, and that performance inversely scales with size when the examples are misrepresentative. | Bradley C. A. Brown, Jordan Juravsky, Atif Mahmud, Wais Shahbaz, Ryan Ehrlich |  |
| 68 |  |  [RETHINKING POSITIONAL EMBEDDING: A CASE STUDY IN TEMPORAL EVENT SEQUENCE MODELLING](https://openreview.net/forum?id=iF2w_kqmYmw) |  | 0 | In this paper, we present a time-decaying encoding as an alternative to sinusoidal positional encoding in the transformer architecture. We evaluate our approach in the context of an educational domain involving 14,043 question-solving interactions from 1,260 students. We argue that including time-based attention can be beneficial for event sequence modeling applications where the inter-event time intervals and the interpretation of the model's prediction both are crucial. | Effat Farhana |  |
| 69 |  |  [Exploring Efficient and Simple Initialization Strategies for Bayesian Optimization with SETUP-BO](https://openreview.net/forum?id=TFrzVBZk05g) |  | 0 | This paper studies the effectiveness of random and grid initialization strategies in SETUP-BO, a self-tuning Bayesian optimization algorithm. Our experiments on benchmark functions compare the performance of these initialization strategies to deterministic initialization. The results show that random initialization outperforms other methods, indicating that it can enhance the performance of BO. | Seyed Ali YaghoubNejad, Mohammad Taghi Manzuri |  |
| 70 |  |  [Quota Constraints for Diversity Interventions in Subset Selection](https://openreview.net/forum?id=x4MuUFPKEIj) |  | 0 | The combinatorial optimization problem of subset selection is often modeled as maximizing a set function that captures inter-element dependencies under some capacity/matroid constraints. In this paper, we examine this problem under “quota constraints” where the selected subset must meet some minimum group-wise quotas. We provide algorithms with guarantees for two popular scenarios extended to the quota-constrained setting and make an empirical case for their applicability to fair subset selection. | Neeraja Abhyankar |  |
| 71 |  |  [Mitigating Metastable Failures in Distributed Systems with Offline Reinforcement Learning](https://openreview.net/forum?id=zYF6NLJl6LM) |  | 0 | This paper introduces a load-shedding mechanism that mitigates metastable failures through offline reinforcement learning (RL). Previous studies have heavily focused on heuristics that are reactive and limited in generalization, while online RL algorithms face challenges in accurately simulating system dynamics and acquiring data with sufficient coverage. In contrast, our algorithm leverages offline RL to learn directly from existing log data. Through extensive empirical experiments, we demonstrate that our algorithm outperforms rule-based methods and supervised learning algorithms in a proactive, adaptive, generalizable, and safe manner. Deployed in a Java compute service with diverse execution times and configurations, our algorithm exhibits faster reaction time and attains the Pareto frontier between throughput and tail latency. | Yueying Li, Daochen Zha, Tianjun Zhang, G. Edward Suh, Christina Delimitrou, Francis Y. Yan |  |
| 72 |  |  [Knowledge Distillation of BERT Language Model on the Arabic Language](https://openreview.net/forum?id=-bMH1Sk8SSF) |  | 0 | The absence of good Arabic language models led to significant setbacks in the Arabic language related tasks and lag with respect to robustness and accuracy. While a pre-trained version of BERT on Arabic language is available, a smaller distilled version could be proven to be highly scalable. In this research paper, we propose the development of a smaller and more efficient version of BERT, known as DistilBERT for the Arabic language for the pursuit of achieving comparable results with significantly less computational resources. Employing knowledge distillation to create a compact model allows for wider implementation, even in areas with limited computational resources. Ultimately, this project aims to break down language barriers, bring greater inclusivity and improve the accessibility of the Arabic language in NLP applications worldwide. This project serves as a starting point for further research and investigation of the performance of the Arabic DistilBERT model across various NLP tasks. | Hager Adil, Abrar Elidrisi, Muhammed Saeed |  |
| 73 |  |  [Offensiveness as an Opinion: Dissecting population-level Label Distributions](https://openreview.net/forum?id=DoOiwBcRir3) |  | 0 |  | Tharindu Cyril Weerasooriya, Sarah Luger, Yu Liang, Christopher M. Homan |  |
| 74 |  |  [How do ConvNets Understand Image Intensity?](https://openreview.net/forum?id=z2Gr8YqsimF) |  | 0 | Convolutional Neural Networks (ConvNets) usually rely on edge/shape information to classify images. Visualization methods developed over the last decade confirm that ConvNets rely on edge information. We investigate situations where the ConvNet needs to rely on image intensity in addition to shape. We show that the ConvNet relies on image intensity information using visualization. | Jackson Kaunismaa, Michael Guerzhoy |  |
| 75 |  |  [Learning Weight Sensitivity from Entropy](https://openreview.net/forum?id=x_adzmY6PQ5) |  | 0 | Multiple network pruning methods have used connection sensitivity of each weight to prune their network. This paper proposes a meta-learning approach to learn sensitivity of weights based on their entropy, or change, during the training phase. We have experimentally shown the validity of such an approach. | Novena Agnes |  |
| 76 |  |  [A Study on Sample Diversity in Generative Models: GANs vs. Diffusion Models](https://openreview.net/forum?id=BQpCuJoMykZ) |  | 0 | In this project, we compare the sample diversity of two generative models: Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs). GANs have achieved impressive results in generating high-quality samples, but have been known to suffer from the issue of mode collapse, which can result in a lack of sample diversity. Mode collapse occurs when the generator network in a GAN becomes stuck in a local minimum, causing it to produce samples that are similar to each other rather than sampling from the full range of possibilities in the target distribution. This can lead to a lack of sample diversity, as the generator is unable to explore and represent the full range of features in the data. DDPMs, on the other hand, have demonstrated improved sample diversity compared to GANs. We conducted experiments using both synthetic and image data to explore the connection between mode collapse and sample diversity in these two frameworks. Our findings indicate that by addressing the mode collapse problem, DDPM preserves a comprehensive representation of the distribution. | Reza Bayat |  |
| 77 |  |  [Artificial Intelligent Life: A New Perspective on Artificial General Intelligence](https://openreview.net/forum?id=XCbe51-arJt) |  | 0 |  | Borui Cai, Yong Xiang, Yao Zhao |  |
| 78 |  |  [Generative AI for Therapy? Opportunities and Barriers for ChatGPT in Speech-Language Therapy](https://openreview.net/forum?id=cRZSr6Tpr1S) |  | 0 |  | Yao Du, Felix JuefeiXu |  |
| 79 |  |  [The Art of Embedding Fusion: Optimizing Hate Speech Detection](https://openreview.net/forum?id=1yXbt6_o6av) |  | 0 | Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. | Mohammad Aflah Khan, Neemesh Yadav, Mohit Jain, Sanyam Goyal |  |
| 80 |  |  [COLLABORATIVE CONCEPT DRIFT DETECTION](https://openreview.net/forum?id=STpRX-XCO6t) |  | 0 | Collaborative Concept Drift Detection (C2D2) combines Fast Correlated Based Filtering (FCBF) and Singular Value Decomposition (SVD) to detect concept drifts in 5 synthetic datasets. We compare our results against 6 diveregence tests and introduce Performance Gain Update Cost Ratio (PGUCR). Post-hoc Tukey HSD test confirmed that C2D2 outperformed the other tests in terms of PGUCR. Much of C2D2’s improvement is based on its conservative signals for updates. | Beverly Abadines Quon, JeanLuc Gaudiot |  |
| 81 |  |  [End-to-End Learnable Masks With Differentiable Indexing](https://openreview.net/forum?id=EyliiBqhFz) |  | 0 | An essential step towards developing efficient learning algorithms involves being able to work with as little data as possible to achieve good performance. For this reason, sparse representation learning is a crucial avenue of computer vision research. However, sparsity-inducing methods like importance sampling rely on non-differentiable operators like masking or top-K selection. While several tricks have been proposed for getting gradients to flow ‘through’ the pixels selected by the operators, the actual indices for which pixels are masked or selected are non-differentiable and thus cannot be learned end-to-end. We propose three methods for making operations like masking and top-k selection fully differentiable by allowing gradients to flow through the operator indices and showing how they can be optimized end-to-end using backpropagation. As a result, all three methods can be used as simple layers or submodules in existing neural network libraries. | Dibyanshu Shekhar, Sree Harsha Nelaturu, Ashwath Shetty, Ilia Sucholutsky |  |
| 82 |  |  [FigGen: Text to Scientific Figure Generation](https://openreview.net/forum?id=Hx_iTXnCR5) |  | 0 | The generative modeling landscape has experienced tremendous growth in recent years, particularly in generating natural images and art. Recent techniques have shown impressive potential in creating complex visual compositions while delivering impressive realism and quality. However, state-of-the-art methods have been focusing on the narrow domain of natural images, while other distributions remain unexplored. In this paper, we introduce the problem of text-to-figure generation, that is creating scientific figures of papers from text descriptions. We present FigGen, a diffusion-based approach for text-to-figure as well as the main challenges of the proposed task. Code and models are available at https://github.com/joanrod/figure-diffusion | Juan A. Rodríguez, David Vázquez, Issam H. Laradji, Marco Pedersoli, Pau Rodríguez |  |
| 83 |  |  [Evaluating Impact of Emoticons and Pre-processing on Sentiment Classification of Translated African Tweets](https://openreview.net/forum?id=OMARmh02Ruk) |  | 0 | This paper examines the impact of emoticons and pre-processing on sentiment classification for English translations of 11 African languages. Using AfriSenti-SemEval datasets, Roberta and Twitter-Roberta models are fine-tuned, and standard classification metrics are used to assess performance. The study concludes no significant performance differences with emoticons and pre-processing and no distinction between standard Roberta and domain-specific Twitter-Roberta. | Saurav Keshari Aryal, Gaurav Adhikari |  |
| 84 |  |  [SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels](https://openreview.net/forum?id=OiSbJbVWBJT) |  | 0 | Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility. | Juhwan Choi, Kyohoon Jin, Junho Lee, Sangmin Song, YoungBin Kim |  |
| 85 |  |  [MatPropXtractor: Generate to Extract](https://openreview.net/forum?id=5CdkvFyatt2) |  | 0 | The field of materials science has amassed a wealth of information about materials in text publications, however, such information is often confined within the publication. A lack of standardized structure and naming consistency preclude the information from being effectively utilized for research and discovery. We introduce MatPropXtractor, an extraction system that uses pre-trained large language models (LLMs) in a generative setting to extract materials and their properties as reported in the materials science literature. MatPropXtractor consists of a three-step pipeline that includes 1) a document selection tool to identify related articles, 2) a paragraph classifier to identify passages containing important materials properties, and 3) a property extractor exploiting in-context learning in GPT-3. MatPropXtractor extracted 154 material-property pairs from five materials science papers. The extracted pairs were analyzed by an expert and obtained an average precision of 72.73% on paragraph classification and an average precision of 56.7% precision on material-property identification. | Aswathy Ajith, Marcus Schwarting, Zhi Hong, Kyle Chard, Ian T. Foster |  |
| 86 |  |  [Zero-Shot Classification Reveals Potential Positive Sentiment Bias in African Languages Translations](https://openreview.net/forum?id=-AIukSeLAz9) |  | 0 | Natural Language Processing research into African languages has been limited, with over 2000 languages still needing to be studied. We employ the AfriSenti-SemEval dataset, a recently released resource that provides annotated tweets across 13 African languages, for sentiment analysis to address this. However, given the persistent data limitations for specific languages, we translate each language to English and conduct zero-shot classification using a large BART model trained with three candidate labels: positive, neutral, and negative. Intriguingly, our findings indicate that all tweets are classified as positive. Further investigation into prediction probabilities reveals that translation technologies may exhibit a bias in translating African languages toward positive sentiments. This observation highlights the potential impact of translation tools on sentiment analysis and warrants further examination. | Hrishav Sapkota, Saurav Keshari Aryal, Howard Prioleau |  |
| 87 |  |  [Lesion Search with Self-supervised Learning](https://openreview.net/forum?id=c-YTzVUkfAW) |  | 0 | Content-based image retrieval (CBIR) with self-supervised learning (SSL) accelerates clinicians’ interpretation of similar images without manual annotations. We develop a CBIR from the contrastive learning SimCLR and incorporate a generalized-mean (GeM) pooling followed by L2 normalization to classify lesion types and retrieve similar images before clinicians' analysis. Results have shown improved performance. We additionally build an open-source application for image analysis and retrieval. The application is easy to integrate, relieving manual efforts and suggesting the potential to support clinicians’ everyday activities. | Kristin Qi, Jiali Cheng, Daniel Haehn |  |
| 88 |  |  [Feature Importance Analysis for Mini Mental Status Score Prediction in Alzheimer's Disease](https://openreview.net/forum?id=GPA-BPLwYHf) |  | 0 | This research article proposes developing predictive models to forecast Mini-Mental State Exam (MMSE) scores using the 54 most important features identified from the current state-of-the-art model. The study employs the SHapley Additive exPlanations (SHAP) method to explore feature importance and interpret model performance. The analysis shows that the Automated Readability Index (ARI) is the most influential feature in predicting MMSE scores. This finding suggests that ARI's capability to capture language impairment and morphosyntax is valuable in predicting cognitive decline in dementia patients. Although the analysis could not evaluate all features, this study provides a foundation for future investigations into features that may assist in predicting MMSE scores and the onset of Dementia. | Howard Prioleau, Saurav Keshari Aryal |  |
| 89 |  |  [On a Relation Between the Rate-Distortion Function and Optimal Transport](https://openreview.net/forum?id=1F8pPnUinbU) |  | 0 |  | Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti |  |
| 90 |  |  [Is CLIP Fooled by Optical Illusions?](https://openreview.net/forum?id=YdGkE4Ugg2C) |  | 0 | Recent large machine learning models such as CLIP have shown impressive generalization performance for various perception tasks. In this work, we explore to what extent they model the human cognitive process. We focus our attention on how these models perceive optical illusions. We present a simple way to assess the effect by presenting illusions in the form of image and text prompts while observing the changes in models’ output under different illusory strengths. Our results show that CLIP can indeed be fooled by different types of illusions relating to lightness and geometry. | Jerry Ngo, Swami Sankaranarayanan, Phillip Isola |  |
| 91 |  |  [Seeing in Words: Learning to Classify through Language Bottlenecks](https://openreview.net/forum?id=_QreMdMNIz-) |  | 0 |  | Khalid Saifullah, Yuxin Wen, Jonas Geiping, Micah Goldblum, Tom Goldstein |  |
| 92 |  |  [Towards Stochastic Gradient Variance Reduction by Solving a Filtering Problem](https://openreview.net/forum?id=0sxmoci9Ma) |  | 0 | Stochastic gradient descent is commonly used to optimize deep neural networks, but it often produces noisy and unreliable gradient estimates that hinder convergence. To address this issue, we introduce \textbf{Filter Gradient Descent} (FGD), a family of stochastic optimization algorithms that consistently estimate the local gradient by solving an adaptive filtering problem. By incorporating historical states, FGD reduces the variance in stochastic gradient descent and improves the current estimation. We demonstrate the efficacy of FGD in numerical optimization and neural network training, where it outperforms traditional momentum-based methods in terms of robustness and performance. Code is available at \url{https://github.com/Adamdad/Filter-Gradient-Decent}. | Xingyi Yang |  |
| 93 |  |  [MACHINE TRANSLATION BASELINES FOR ARABIC - SWAHILI](https://openreview.net/forum?id=aVepdnlRb5) |  | 0 | Building neural machine translation (NMT) systems for low-resource languages poses several challenges, mainly due to the lack of parallel data. In this research, we propose a baseline NMT system for translating between Arabic and Swahili. Despite being spoken by nearly 300 million individuals worldwide, the parallel corpus between these two languages is severely underrepresented. To address this, we scraped and processed the largest high-quality parallel corpus of Swahili and Arabic to our knowledge. We then used state-of-the-art NMT models, including Transformers and multilingual variants of Transformers, to build a baseline for bidirectional Arabic-Swahili NMT. Finally, we report an increase in the performance of our NMT system using the back-translation technique. | Asim Awad Osman, Ahmed Emadeldin Almahady, Muhammed Saeed, Hiba Hassan Sayed |  |
| 94 |  |  [Accuracy of white box and black box adversarial attacks on a sign activation 01 loss neural network ensemble](https://openreview.net/forum?id=QimsmhYvsf) |  | 0 | In this work we ask the question: is an ensemble of single hidden layer sign activation 01 loss networks more robust to white box and black box adversarial attacks than an ensemble of its differentiable counterpart of cross-entropy loss with relu activations and an ensemble of the approximate differentiable counterpart of cross-entropy loss with sign activations? We consider a simple experimental setting of attacking models trained for binary classification on pairwise CIFAR10 datasets - altogether a total of 45 datasets. We study ensembles of {\bf bcebp}: binary cross-entropy loss with relu activations trained with back-propagation, {\bf bceban}: binary cross-entropy loss with sign activations trained with back-propagation with the straight through estimator gradient, {\bf 01scd}: 01-loss with sign activations trained with gradient-free stochastic coordinate descent, and {\bf bcescd}: binary cross-entropy loss with relu activation trained with gradient-free stochastic coordinate descent (to isolate the effect of 01 loss from gradient-free training). We train each model in an ensemble with a different random number generator seed. Our four models have similar mean test accuracies in the mid to high 80s on pairwise CIFAR10 datasets but under powerful PGD white-box attacks they each drop to near 0\% except for our 01 loss network ensemble that has 31\% accuracy. Even training with the gradient-free stochastic coordinate descent can be attacked thus suggesting that the defense lies in 01 loss. In a black-box transfer attack we find adversaries produced from the bcebp model fully transfer to bceban but much less to 01scd - we see the same transferability pattern from bceban to bcebp and 01scd. We also find that adversaries from 01scd barely transfer to bcebp and bceban. While our results are far from those of multi-class and convolutional networks, they suggest that 01 loss models are hard to attack naturally without any adversarial training. All models, data, and code to reproduce results here are available from \url{https://github.com/xyzacademic/mlp01example}. | Yunzhe Xue, Usman Roshan |  |
| 95 |  |  [TopEx: Topic-based Explanations for Model Comparison](https://openreview.net/forum?id=AidIUjh__t) |  | 0 |  | Shreya Havaldar, Adam Stein, Eric Wong, Lyle H. Ungar |  |
| 96 |  |  [GeneDAE: A Sparse Denoising Autoencoder for Deriving Interpretable Gene Embeddings](https://openreview.net/forum?id=cxmjk8O3Yn) |  | 0 | A challenge in genomics research involves identifying functionally relevant genes associated with diseases. We present GeneDAE, a sparse denoising autoencoder that extracts gene representations from large-scale population-level genotype data, which can then be used to identify gene-to-disease associations. The GeneDAE encoder and decoder connections are modeled on a bipartite biological knowledge graph that connects individual variants (single nucleotide polymorphisms; SNPs) to their nearby genes, enabling each node in the hidden layer to be used as an interpretable, multi-purpose gene embedding derived using information only from variants in close proximity that are most likely to impact gene function. We use the UK Biobank dataset and focus on the major histone compatibility complex (MHC) region of the genome, which is critical to immune function and autoimmune disease pathophysiology. Using GeneDAE, we extracted 239 MHC gene embeddings and identified novel gene-to-disease associations. | Monica Isgut, Neha Jain, Andrew Hornback, Karan Samel, May Dongmei Wang |  |
| 97 |  |  [Improving Hyperspectral Adversarial Robustness Under Multiple Attacks](https://openreview.net/forum?id=XHfWgU2IiP) |  | 0 |  | Nicholas Soucy, Salimeh Yasaei Sekeh |  |
| 98 |  |  [Prompt Programming for the Visual Domain](https://openreview.net/forum?id=hBz5h3C9Sq) |  | 0 |  | Alayt Issak, Lav R. Varshney |  |
| 99 |  |  [Mapping the Typographic Latent Space of Digits](https://openreview.net/forum?id=ufA2FuCGyz) |  | 0 | Since the advancement of handwritten text to typefaces on a computer, the human mind has evolved towards corresponding various typefaces as norms of comprehension. Current-day typefaces, much like those written by hand, exist in disparities and are governed by consensus reached among Typographers. Currently, the PANOSE system, developed in 1998, is the most widely used and accepted method for classifying typefaces based on 10 visual attributes. In this work, we employ Disentangled Beta-VAE's, in an unsupervised learning approach, to map the latent feature space with a dataset of MNIST Style Typographic Images (TMNIST-Digit) of 0-9 digits across 2990 unique font styles. We expose the learning representation across a variety of font styles to enable typographers to contemplate and identify new attributes to their classification system. | Alayt Issak, Sarthak Kakkar, Sair Goetz, Nik Bear Brown, Casper Harteveld |  |
| 100 |  |  [Hyperbolic Deep Reinforcement Learning for Continuous Control](https://openreview.net/forum?id=Mrz9PgP3sT) |  | 0 |  | Omar Salemohamed, Edoardo Cetin, Sai Rajeswar, Arnab Kumar Mondal |  |
| 101 |  |  [Incorporating Expert Prior Knowledge for Oral Lesion Recognition](https://openreview.net/forum?id=XXsjViheWZ) |  | 0 | External information may improve predictive accuracy and uncertainty in medical image recognition. For example in oral lesion recognition, some lesion types are implausible to occur at certain anatomical locations. We propose a strategy to induce the prior knowledge about such correlations using an additional loss term that optimizes for plausible lesion types given an anatomical location. Our results suggests an improvement in model calibration, reduction in predicted number of implausible classes and improved uncertainty estimation for implausible predictions. | Camille Besombes, Adeetya Patel, Sreenath Arekunnath Madathil |  |
| 102 |  |  [LEARNING LIGHTWEIGHT STRUCTURE-AWARE EMBEDDINGS FOR PROTEIN SEQUENCES](https://openreview.net/forum?id=2M8dEAJcG5) |  | 0 | Machine learning models, such as AlphaFold, have recently demonstrated remarkable accuracy in predicting the structures of protein sequences. This capability enables their use as oracles for providing structure-based information to aid other learning tasks. In this study, we investigate the use of deep learning embeddings and explore the feasibility of developing a structure-aware protein sequence embedding. To accomplish this, we employ S4PRED and ESMFold, two models that predict protein secondary (2D) and tertiary structures (3D) respectively, directly from single sequences. These models act as oracles to form structure-aware embeddings through an autoencoder. We then compare this approach to purely sequence-based embeddings in a Protein-Protein Interaction (PPI)-prediction task. Our findings highlight the potential advantages of employing structure embeddings and provide grounds for future research directions. | Sidharth Lakshmanan, Philip J. Y. Leung, Jeffrey Nivala |  |
| 103 |  |  [Proactive policing as reinforcement learning](https://openreview.net/forum?id=lmcPpHDa0B) |  | 0 | Recent analyses of predictive policing have shown the inherent biases in such systems. We show that the models considered in fact apply to proactive policing in general, which can be also viewed as a reinforcement learning system, and thus may also lead to over-policing. | Dawson Kinsman, Tian An Wong |  |
| 104 |  |  [The Small Batch Size Anomaly in Multistep Deep Reinforcement Learning](https://openreview.net/forum?id=G0heahVv5Y) |  | 0 | We present a surprising discovery: in deep reinforcement learning, decreasing the batch size during training can dramatically improve the agent's performance when combined with multi-step learning. Both reducing batch sizes and increasing the update horizon increase the variance of the gradients, so it is quite surprising that increased variance on two fronts yields improved performance. We perform a wide range of experiments to gain a better understanding of this phenomenon, which we denote variance double-down. | Johan S. ObandoCeron, Marc G. Bellemare, Pablo Samuel Castro |  |
| 105 |  |  [Bootstrapping Parallel Anchors for Relative Representations](https://openreview.net/forum?id=VBuUL2IWlq) |  | 0 | The use of relative representations for latent embeddings has shown potential in enabling latent space communication and zero-shot model stitching across a wide range of applications. Nevertheless, relative representations rely on a certain amount of parallel anchors to be given as input, which can be impractical to obtain in certain scenarios. To overcome this limitation, we propose an optimization-based method to discover new parallel anchors from a limited known set (seed). Our approach can be used to find semantic correspondence between different domains, align their relative spaces, and achieve competitive results in several tasks. | Irene Cannistraci, Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Emanuele Rodolà |  |
| 106 |  |  [A Brief History of the Speculative Measures for Autonomy](https://openreview.net/forum?id=qqKO_rrg9y) |  | 0 | This paper presents a novel summary of the history of the evolution of the measures for autonomy (i.e. self-legislating systems), from Creation myths to the study of technological autonomy, progressing through five interrelated phases. First, the original legislator of the laws of nature is considered as the singular measure for autonomous existence. Second, a set of hierarchical governing systems are conceived as a sort of supplement to the limitations of the original monopolistic autonomy. Third, this hierarchy of governors is inverted by transcendental philosophy, putting emphasis on the immediate conscious “self” - the auto - in self-legislation. The fourth stage emerges in existential philosophy, which notices the seemingly inescapable paradoxes of this introverted autonomy, as all justice and justification becomes circularly self-justified. The fifth and most contemporary measure of autonomy universalizes this existential insight and measures each being’s autonomy simply in proportion to its own alienated independence. The paper concludes with an analysis on the potential limits of this universalized autonomy and suggests a route for future research which may ultimately separate the measure of autonomy from the measure of responsibility for justice. | Micah Tewers |  |
| 107 |  |  [Semantic feature verification in FLAN-T5](https://openreview.net/forum?id=_1z2Bqte5L) |  | 0 | This study evaluates the potential of a large language model for aiding in generation of semantic feature norms–a critical tool for evaluating conceptual structure in cognitive science. Building from an existing human-generated dataset, we show that machine-verified norms capture aspects of conceptual structure beyond what is expressed in human norms alone, and better explain human judgments of semantic similarity amongst items that are distally related. The results suggest that LLMs can greatly enhance traditional methods of semantic feature norm verification, with implications for our understanding of conceptual representation in humans and machines. | Siddharth Suresh, Kushin Mukherjee, Timothy T. Rogers |  |
| 108 |  |  [Augmenting Collective Intelligence through Belbin's Team Roles](https://openreview.net/forum?id=XiZOalwf_U) |  | 0 | Augmented Collective Intelligence (ACI) allows organizations to improve performance by combining human and artificial intelligence in teams. We use Belbin’s Team Roles, a popular framework that identifies nine clusters of behavioral attributes, to explore how roles might be augmented by AI tools. Each role is rated as having low, medium, or high potential for augmentation at current system capabilities. The paper concludes that developing an ACI strategy requires leaders to understand human and AI capabilities and how each evolves over time. | Abhishek Gupta, Emily Dardaman |  |
| 109 |  |  [Text2Face: 3D Morphable Faces From Text](https://openreview.net/forum?id=7Zyv70nGl_g) |  | 0 | We present the first 3D morphable modelling approach, whereby 3D face shape can be directly and completely defined using a textual prompt. Building on work in multi-modal learning, we extend the FLAME head model to a common image-and-text latent space. This allows for direct 3D Morphable Model (3DMM) parameter generation and therefore shape manipulation from textual descriptions. Our method, Text2Face, has many applications; for example: generating police photofits where the input is already in natural language. It further enables multi- modal 3DMM image fitting to sketches and sculptures, as well as images. | Will Rowan, Patrik Huber, Nick E. Pears, Andrew Keeling |  |
| 110 |  |  [Towards Parametric Robust Activation Functions in Adversarial Machine Learning](https://openreview.net/forum?id=oKa5_mxHBV) |  | 0 | Machine learning's vulnerability to adversarial perturbations has been argued to stem from a learning model's non-local generalization over complex input data. Given the incomplete information in a complex dataset, a learning model captures non-linear patterns between data points with volatility in the loss surface and exploitable areas of low-confidence knowledge. It is the responsibility of activation functions to capture the non-linearity in data and, thus, has inspired disjointed research efforts to create robust activation functions. This work unifies the properties of activation functions that contribute to robust generalization with the generalized gamma distribution function. We show that combining the disjointed characteristics presented in the literature provides more effective robustness than the individual characteristics alone. | Sheila Alemany, Alberto Luis Dominguez, Ilan Grapel, Niki Pissinou |  |
| 111 |  |  [Understanding Label Bias in Single Positive Multi-Label Learning](https://openreview.net/forum?id=iWiwox99aJ) |  | 0 | Annotating data for multi-label classification is prohibitively expensive because every category of interest must be confirmed to be either present or absent. Recent work on single positive multi-label (SPML) learning has shown that it is possible to train effective multi-label classifiers using only one positive label per image. The standard benchmarks for SPML are derived from traditional multi-label classification datasets by retaining one positive label for each training example (chosen uniformly at random) and discarding all other labels. However, in realistic annotation settings it is not likely that positive labels are chosen uniformly at random. In this work, we explore the effect of label bias in SPML. | Julio Arroyo, Pietro Perona, Elijah Cole |  |
| 112 |  |  [Knowledge and Attitude of Medical Students and Doctors towards Artificial Intelligence: A study of University of Ilorin](https://openreview.net/forum?id=5lZaexgIey) |  | 0 | This study assesses the knowledge and attitudes of medical students and doctors in University of Ilorin toward Artificial Intelligence (AI) in medical education. It involved a cross-sectional study using an online survey consisting of close-ended questions. The survey targeted medical students at all medical levels and doctors in their postgraduate training. total of 481 medical students and doctors responded. When assessing AI knowledge sources (86.5%) got their information from the media as compared to (13.5%) from medical school curriculum. However, students who learned the basics of AI while being in medical school were more knowledge about AI than their peers who did not and were more interested in the applications of AI in healthcare. The advancements in AI affected the choice of specialty of around a quarter of the students (26.8%). Finally, less than a quarter of students (22.1%) want to be assessed by AI, even though about half (57.7%) reported that assessment by AI is more objective. In conclusion, Although most physicians and medical students do not sufficiently understand AI and its significance in the medical field, they have favorable views regarding using AI in the medical field and a willing to learn the fundamentals. Nigerians medical authorities and international organizations should suggest including artificial intelligence in the medical field, particularly when training residents and fellowship physicians. | Abdulhameed Abiola Dere |  |
| 113 |  |  [Human-machine cooperation for semantic feature listing](https://openreview.net/forum?id=K-SVVOIcsP) |  | 0 | Semantic feature norms — lists of features that concepts do and do not possess — have played a central role in characterizing human conceptual knowledge, but require extensive human labor. Large language models (LLMs) offer a novel avenue for the automatic generation of such feature lists, but are prone to significant error. Here, we present a new method for combining a learned model of human lexical-semantics from limited data with LLM-generated data to efficiently generate high-quality feature norms. | Kushin Mukherjee, Siddharth Suresh, Timothy T. Rogers |  |
| 114 |  |  [Improving generalization by loss modification](https://openreview.net/forum?id=vHOO1lxggJ) |  | 0 | What data points from available data set should be used for training? For all subsets of available data it will generally make different solutions. We show that a simple loss modification allows to find a single solution that represents data set properties and not particular selections of data points thus improving the generalization performance. | Michael Tetelman |  |
| 115 |  |  [A Rate-Distortion View on Model Updates](https://openreview.net/forum?id=6ry6ibTKOx) |  | 0 | Compressing model updates is critical for reducing communication costs in federated learning. We examine the problem using rate--distortion theory to present a compression method that is near-optimal in many use cases. We empirically show that common transforms applied to model updates in standard compression algorithms, normalization in QSGD and random rotation in DRIVE, yield sub-optimal compressed representations in practice. | Nicole Mitchell, Johannes Ballé, Zachary Charles, Jakub Konecný |  |
| 116 |  |  [Text-Based Games as a Challenging Benchmark for Large Language Models](https://openreview.net/forum?id=2g4m5S_knF) |  | 0 | Text-based games (TBG) are puzzle-solving, interactive dialogue language tasks that have the potential to become a challenging intelligence benchmark for large language models (LLMs). TBGs are similar to interactive dialogue, as they require the capability for bidirectional communication in natural language, while at the same time being straightforward to evaluate in terms of performance, as a score clearly indicates progress in TBGs. We conduct preliminary experiments on FLAN-T5, Turing, and OPT language models to test their puzzle-solving abilities using an \textit{easy} TBG called \`\`Detective''. Our results suggest that LLMs underperform in comparison with state-of-the-art and human performance. We discuss the potential reasons behind the performance gap, such as the complexity of turning TBGs into prompts, LLMs not learning from past trials, their lack of memory, and LLMs relying on statistical prediction instead of goal orientation. | Qinyue Tan, Ashkan Kazemi, Rada Mihalcea |  |
| 117 |  |  [Learned Learning Rate Schedules for Deep Neural Network Training Using Reinforcement Learning](https://openreview.net/forum?id=0Zhwu1VaOs) |  | 0 |  | Shreyas Subramanian, Vignesh Ganapathiraman, Aly El Gamal |  |
| 118 |  |  [Speaker-Invariant Speech Recognition through Fine-Tuning on Individual-Specific Data with Voice Conversion](https://openreview.net/forum?id=CTZigc9V69) |  | 0 | In this paper, we propose a speaker-invariant speech recognition method that fine-tunes a pre-trained model (Obtained by a self-supervised learning method) on a selected subset of data containing speech from a specific individual. This fine-tuning changes the network's behavior, allowing it to focus on information that is important for tasks such as ASR and phoneme recognition while reducing sensitivity to speaker-specific vocal characteristics. In the test time, we recommend employing voice conversion techniques to transform the voices of diverse individuals to match that of the individual used for training. | Samin Heydarian, Tohid Abedini, Alireza Morsali, Moein Heidari |  |
| 119 |  |  [Effect of training fragment length on Transformers in text complexity prediction](https://openreview.net/forum?id=SBqbPjVFfm) |  | 0 | With the myriad practical applications of text complexity classification, it is important to optimize the training text fragment size for performance. We experiment with fine-tuning pre-trained BERT models to classify the complexity of Russian school text using different fragment sizes for training. | Rafik Hachana, Vladimir V. Ivanov |  |
| 120 |  |  [Revisiting Bisimulation: A Sampling-Based State Similarity Pseudo-metric](https://openreview.net/forum?id=lkWvTn2IzA) |  | 0 | In reinforcement learning (RL), we typically deal with systems with large or continuous states encoded in an unstructured way. Because it is not possible to represent the value of each state, it is necessary to learn a structured representation from limited state samples to express the value function in a more meaningful way. One approach to do so is to endow the set of states with a behavioral metric, such that two states that are close in the metric space are also close in the space of value functions. While there exists some notions of state similarity, they are either not amenable to sample-based algorithms \citep{ferns2004metrics, ferns05metrics}, need additional assumptions \citep{castro2020scalable, zhang2020learning, agarwal2021pse} or yield limited theoretical guarantees \citep{castro2021mico}. In this paper, we present a new behavioural pseudo-metric, PMiCo, to overcome these shortcomings. PMiCo is based on a recent sampling-based behavioural distance, MICo \citep[Matching under Independent Couplings;][]{castro2021mico}, but enjoys more interesting theoretical properties, which we also illustrate empirically. | Charline Le Lan, Rishabh Agarwal |  |
| 121 |  |  [Efficient Learning rate schedules for Stochastic Non-negative Matrix Factorization via Reinforcement Learning](https://openreview.net/forum?id=AAu_WuIiwi) |  | 0 | For deep learning training, learning rate schedules are often picked through trial and error, or hand-crafted optimization algorithms that focus mostly on maintaining stability and convergence without systemic incorporation of higher order derivative information to optimize the convergence slope. In this paper, we consider a stochastic version of Non-negative Matrix Factorization (NMF) where only a noisy gradient is known, and calculate a theoretical upper bound for SGD learning rate (LR) schedule that guarantees convergence, thereby providing a clean example where stability and convergence is not a challenge. We then use a Reinforcement Learning agent to demonstrate how efficient LR schedules, superior to those found by traditional algorithms, can be found for this NMF problem. | Shreyas Subramanian, Vignesh Ganapathiraman, Aly El Gamal |  |
| 122 |  |  [Chaotic Transformers for Deep Reinforcement Learning in Algorithmic Trading](https://openreview.net/forum?id=H2mbtfasD4K) |  | 0 | Chaotic Transformers for Deep Reinforcement Learning can be applied in algorithmic trading to improve the efficiency and effectiveness of trading strategies. In algorithmic trading, deep reinforcement learning can be used to learn trading policies that maximize the expected reward. However, due to the highly complex and nonlinear nature of financial markets, it can be challenging to identify profitable trading opportunities and avoid overfitting to historical data. | Tohid Abedini, Samin Heydarian, Moein Heidari, Alireza Morsali |  |
| 123 |  |  [Effects of Single-Attribute Control on the Music Generated by FIGARO](https://openreview.net/forum?id=G_MpqMHYo-) |  | 0 |  | Rafik Hachana, Adil Khan |  |
| 124 |  |  [Iterative weakly supervised learning for novel class object detection](https://openreview.net/forum?id=FWohKbMhlo) |  | 0 |  | Dejana Mandic, Wieland Brendel, Claudio Michaelis |  |
| 125 |  |  [BANDIT SAMPLING FOR FASTER NEURAL NETWORK TRAINING WITH SGD](https://openreview.net/forum?id=LV33sOiYCEP) |  | 0 |  | Vignesh Ganapathiraman, Francisco Javier Calderon, Anila Joshi |  |
| 126 |  |  [Train Monolingual, Infer Bilingual](https://openreview.net/forum?id=MjVdwBGkys) |  | 0 |  | Alaeddin Selçuk Gürel, Aydin Gerek |  |
| 127 |  |  [A Simple, Fast Algorithm for Continual Learning from High-Dimensional Data](https://openreview.net/forum?id=TPTbHxeR6U) |  | 0 | As an alternative to resource-intensive deep learning approaches to the continual learning problem, we propose a simple, fast algorithm inspired by adaptive resonance theory (ART). To cope with the curse of dimensionality and avoid catastrophic forgetting, we apply incremental principal component analysis (IPCA) to the model's previously learned weights. Experiments show that this approach approximates the performance achieved using static PCA and is competitive with continual deep learning methods. Our implementation is available on https://github.com/neil-ash/ART-IPCA | Neil Ashtekar, Vasant G. Honavar |  |
| 128 |  |  [Discerning Self-Supervised Learning and Weakly Supervised Learning](https://openreview.net/forum?id=H9BGkFz-Sm) |  | 0 |  | Chandan Kumar, Matthew J. Darr, Ali Jannesari |  |
| 129 |  |  [DiffGANPaint: Fast Inpainting Using Denoising Diffusion GANs](https://openreview.net/forum?id=x2XNoPdXF8J) |  | 0 | Free-form image inpainting is the task of reconstructing parts of an image specified by an arbitrary binary mask. In this task, it is typically desired to generalize model capabilities to unseen mask types, rather than learning certain mask distributions. Capitalizing on the advances in diffusion models, in this paper, we propose a Denoising Diffusion Probabilistic Model (DDPM) based model capable of filling missing pixels fast as it models the backward diffusion process using the generator of a generative adversarial network (GAN) network to reduce sampling cost in diffusion models. Experiments on general-purpose image inpainting datasets verify that our approach performs superior or on par with most contemporary works. | Moein Heidari, Alireza Morsali, Tohid Abedini, Samin Heydarian |  |
| 130 |  |  [Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis](https://openreview.net/forum?id=dJfdug9aGd8) |  | 0 |  | Vithya Yogarajan, Gillian Dobbie, Henry Gouk |  |
| 131 |  |  [An Analysis of Transferability in Network Intrusion Detection using Distributed Deep Learning](https://openreview.net/forum?id=FPzByCI0yz1) |  | 0 | In this paper, we utilize a distributed deep learning framework to investigate transferability of network intrusion detection between federated nodes. Transferable learning makes intrusion detection systems more robust to rare attacks and enables them to adapt to real life scenarios. We analyze symmetric and asymmetric transferability relationships. We propose and investigate the impact of feature pre-processing to improve transferability. The code for this work is available at https://github.com/ghosh64/fedlearn. | Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal |  |
| 132 |  |  [MLP-Attention: Improving Transformer Architecture with MLP Attention Weights](https://openreview.net/forum?id=99XvUeDFYTD) |  | 0 | The Transformer architecture has revolutionized natural language processing (NLP) and has achieved state-of-the-art results in various tasks. The attention mechanism is one of the key components of the Transformer architecture, which allows the model to focus on relevant parts of the input. In the standard Transformer, the attention weights are computed by the dot product of query and key vectors followed by a softmax function. However, in this paper, we propose to replace the dot product of query and key vectors with a multi-layer perceptron (MLP) to compute attention weights directly from the embeddings. The proposed modification is simple and can be easily implemented in existing Transformer-based models to improve their performance as shown in this paper for an NLP task. We provide the implementation code at https://github.com/AlirezaMorsali/MLP-Attention for reproducibility and ease of adoption. | Alireza Morsali, Moein Heidari, Samin Heydarian, Tohid Abedini |  |
| 133 |  |  [Averager Student: Distillation from Undistillable Teacher](https://openreview.net/forum?id=4isz71_aZN) |  | 0 | Today, some companies release their black-box model as a service for users, where users can see the model’s output corresponding to their input. However, these models can be stolen via knowledge distillation by malicious users. Recently, undistillable teacher (Ma et al., 2021) is introduced in order to prevent the knowledge leakage. In this study, with the aim of contributing to solutions for model intellectual property (IP) protection, we propose a novel method which improves the distillation from an undistillable teacher whose goal is make the distillation difficult for students, with the purpose of model protection. The codes are released at https://github.com/rkevser/AveragerStudent. | Reyhan Kevser Keser, Behçet Ugur Töreyin |  |
| 134 |  |  [Inducing Document Representations from Graphs: A Blueprint](https://openreview.net/forum?id=2rp3guEM3A) |  | 0 | Representing textual documents in continuous numerical spaces is a crucial task in NLP. Early practitioners of NLP built their approach around capturing statistical patterns within documents and utilizing them as features in rich feature spaces. In contrast, contemporary state-of-the-art techniques leverage large neural networks and learn the document representations self-supervised. However, while these approaches excel at learning contextual word representations, they often overlook implicit document-to-document relations that can arise in real-world settings. We propose a blueprint method for constructing document representations that explicitly accounts for implicit document-to-document relations to address this issue. | Boshko Koloski, Marko Pranjic, Nada Lavrac, Blaz Skrlj, Senja Pollak |  |
| 135 |  |  [Uncertainty-Aware Test-Time Augmented Ensemble of BERTs for Classification of Common Mental Illnesses on Social Media Posts](https://openreview.net/forum?id=a9VgV-hywP) |  | 0 |  | Pratinav Seth, Mihir Agarwal |  |
| 136 |  |  [Predicting Targets with Data from Non-Conforming Sources](https://openreview.net/forum?id=uMlLT_xuiE) |  | 0 |  | Alexander Capstick, Payam M. Barnaghi |  |
| 137 |  |  [One Explanation Does Not Fit XIL](https://openreview.net/forum?id=o7uGWBK6Uo) |  | 0 | Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning. To address such flaws, the XIL framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL. | Felix Friedrich, David Steinmann, Kristian Kersting |  |
| 138 |  |  [Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics](https://openreview.net/forum?id=xKf-LSD2-Jg) |  | 0 |  | Ekaterina Shumitskaya, Anastasia Antsiferova, Dmitriy S. Vatolin |  |
| 139 |  |  [GraphEx: A User-Centric Model-Level Explainer for Graph Neural Networks](https://openreview.net/forum?id=CuE1F1M0_yR) |  | 0 | With the increasing application of Graph Neural Networks (GNNs) in real-world domains, there is a growing need to understand the decision-making process of these models. To address this, we propose GraphEx, a model-level explainer that learns a graph generative model to approximate the distribution of graphs classified into a target class by the GNN model. Unlike existing methods, GraphEx does not require another black box deep model to explain the GNN and can generate a diverse set of explanation graphs with different node and edge features in one shot. Moreover, GraphEx does not need white box access to the GNN model, making it more accessible to end-users. Experiments on both synthetic and real datasets demonstrate that GraphEx can consistently produce explanations aligned with the class identity and can also identify potential limitations of the GNN model. | Sayan Saha, Monidipa Das, Sanghamitra Bandyopadhyay |  |
| 140 |  |  [Theta sequences as eligibility traces: A biological solution to credit assignment](https://openreview.net/forum?id=vd16AYbem3Z) |  | 0 |  | Tom George |  |
| 141 |  |  [Federated Learning with Variational Autoencoders](https://openreview.net/forum?id=mvo72yTjhTl) |  | 0 | In this work we investigate the feasibility of using federated learning to train a variational autoencoder capable of generated handwritten digits when trained on the MNIST dataset. It was found that using federated learning we were able to train a model that produced comparable results to a centralised model, both in image reconstructions and image generations. | Hugo Dugdale |  |
| 142 |  |  [TRACTABLE LARGE SCALE CALIBRATION WITH RL](https://openreview.net/forum?id=AXxBPw5zdl4) |  | 0 |  | Fadel Thior, Rose Bandolo, Sekou Remy |  |
| 143 |  |  [A Variational Condition for Minimal-Residual Latent Representations](https://openreview.net/forum?id=A2VfgYliIT) |  | 0 |  | Eloisa Bentivegna |  |
| 144 |  |  [Attention Based Variational Graph Auto-Encoder (AVGAE)](https://openreview.net/forum?id=j1gj0ndrk1) |  | 0 | Recently techniques such as VGAEs (Variational Graph Autoencoder) are quite popular in the unsupervised task setting and in generative modeling. Unlike conventional autoencoders, which typically use fully-connected layers to learn a latent representation of input data, VGAEs operate on graph-structured data. We propose to incorporate attention in VGAEs (AVGAE) for capturing the relationships better thereby increasing the robustness and generalisability. In a VAE, the encoder network learns to map input data to a lower-dimensional latent space, while the decoder network learns to map latent space vectors back to the original input data. Unlike traditional autoencoders, which typically use a fixed encoding function, VAEs use a probabilistic encoding function that maps input data to a probability distribution over the latent space. They have been shown to improve the quality of the generated output, particularly for tasks where the input data is complex and high-dimensional. | Nevasini Sasikumar, Krishna Sri Ipsit Mantri |  |
| 145 |  |  [Generative STResnet for Crime Prediction](https://openreview.net/forum?id=-IH_dcPGWM) |  | 0 | In this work, we combine STResnet with VAE to generate crime distribution. The outputs can be used for downstream tasks such as patrol deployment planning. | Tran Phong, Hoong Chuin Lau |  |
| 146 |  |  [A Light Spectrometer Device for Crop Disease Monitoring](https://openreview.net/forum?id=KtVonyo4AS) |  | 0 |  | Joshua Jeremy Dhikusooka, Ephraim Nuwamanya, Estefania Talavera Martínez, Godliver Owomugisha |  |
| 147 |  |  [Can Arterial Blood Pressure Predict Age? A ConvNet Classification Task](https://openreview.net/forum?id=jbBPBUGk-4) |  | 0 |  | Abdalrhman Mostafa, NourMounira Z. Bakkar, Mohamed Abdelhack, Ahmed F. ElYazbi |  |
| 148 |  |  [Pay Attention to Multi-Channel for Improving Graph Neural Networks](https://openreview.net/forum?id=IkHVGw_Ipu) |  | 0 |  | ChungYi Lin, ShenLung Tung, Winston H. Hsu |  |
| 149 |  |  [pGS-CAM: Interpretable LiDAR Point Cloud Semantic Segmentation via Gradient Based Localization](https://openreview.net/forum?id=8kX0btdpAU5) |  | 0 |  | Abhishek Kuriyal, Vaibhav Kumar |  |
| 150 |  |  [Large Language Models Perform Diagnostic Reasoning](https://openreview.net/forum?id=N0lQfjeNWOE) |  | 0 |  | ChengKuang Wu, WeiLin Chen, HsinHsi Chen |  |
| 151 |  |  [A Simple Loss Function for Convergent Algorithm Synthesis using RNNs](https://openreview.net/forum?id=WaAJ883AqiY) |  | 0 |  | Alexandre Salle, Shervin Malmasi |  |
| 152 |  |  [The Obscure Limitation of Modular Multilingual Language Models](https://openreview.net/forum?id=zEGstYVHBt) |  | 0 |  | Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Ayu Purwarianti |  |
| 153 |  |  [Exploratory Analysis of Scholarly Publications on Artificial Intelligence (AI) in Colonoscopy using Litstudy](https://openreview.net/forum?id=zdhCATDn_Q) |  | 0 |  | Mary Adewunmi |  |
| 154 |  |  [Propagate Deeper and Adaptive Graph Convolutional Networks](https://openreview.net/forum?id=RR_w2fbYmV) |  | 0 |  | Sisi Zhang, Lun Du, Fan Li, Ge Yu, Mengyuan Chen |  |
| 155 |  |  [L2 Norm Guided Adaptive Computation](https://openreview.net/forum?id=qW_GZYyn7C) |  | 0 |  | Mani Shemiranifar, Mostafa Dehghani |  |
| 156 |  |  [Efficient Temporal Denoising for Improved Depth Map Applications](https://openreview.net/forum?id=nazr0QFvHR) |  | 0 |  | Pengzhi Li, Zhiheng Li |  |
| 157 |  |  [AI-based opportunistic analysis of the CT images during COVID (2021): Does living in a metropolitan area affect the vertebral body mineral density in older people?](https://openreview.net/forum?id=QRKKFN7FLm) |  | 0 |  | Andrey V. Vlasov, Alexei V. Petraikin |  |
| 158 |  |  [One Student Knows All Experts Know: From Sparse to Dense](https://openreview.net/forum?id=1PW_txDkX7) |  | 0 |  | Fuzhao Xue, Xiaoxin He, Xiaozhe Ren, Yuxuan Lou, Yang You |  |
| 159 |  |  [AN ENSEMBLE LEARNING FRAMEWORK FOR VISIBILITY PREDICTION IN INDO-GANGETIC REGION](https://openreview.net/forum?id=WkDqZD3VRo) |  | 0 |  | Arkapal Panda, Tanmay Basu, Vaibhav Kumar |  |
| 160 |  |  [Evolutionary Federated Learning Using Particle Swarm Optimization](https://openreview.net/forum?id=fcQFbluDTX) |  | 0 |  | Ender Minyard, Steven Kolawole, Nayan Saxena |  |
| 161 |  |  [Understanding the Effectiveness of Cross-Domain Contrastive Unsupervised Domain Adaptation](https://openreview.net/forum?id=0GpMf9UeI3G) |  | 0 |  | Viacheslav Sinii, Adín Ramírez Rivera, Adil Khan |  |
| 162 |  |  [Performance Evaluation of Enhanced ConvNeXtTiny-based Fire Detection System in Real-world Scenarios](https://openreview.net/forum?id=A-E41oZCfrf) |  | 0 |  | Taimoor Khan, Haci Ismail Aslan, Chang Choi |  |
| 163 |  |  [Robustness Evaluation of Multi-Agent Reinforcement Learning Algorithms using GNAs](https://openreview.net/forum?id=zZjPRz0EX5T) |  | 0 |  | Xusheng Zhang, Wei Zhang, Yishu Gong, Liangliang Yang, Jianyu Zhang, Zhengyu Chen, Sihong He |  |
| 164 |  |  [State Advantage Weighting for Offline RL](https://openreview.net/forum?id=PjypHLTo29v) |  | 0 |  | Jiafei Lyu, Aicheng Gong, Le Wan, Zongqing Lu, Xiu Li |  |
| 165 |  |  [Towards Robust Feature Learning with t-vFM Similarity for Continual Learning](https://openreview.net/forum?id=6I5i0Ytnlul) |  | 0 |  | Bilan Gao, YoungBin Kim |  |
| 166 |  |  [When Biology has Chemistry: Solubility And Drug Subcategory Prediction using SMILES Strings](https://openreview.net/forum?id=28si4RXwDt1) |  | 0 |  | Sarwan Ali, Prakash Chourasia, Murray Patterson |  |
| 167 |  |  [Answering Questions Over Knowledge Graphs Using Logic Programming Along with Language Models](https://openreview.net/forum?id=D2lo4toTUTo) |  | 0 |  | Navid Madani, Kenneth Joseph |  |
| 168 |  |  [Contrastive Training with more data](https://openreview.net/forum?id=ZTp85mW5nFy) |  | 0 |  | Stephen Mander, Scott Piao, Hossein Rahmani |  |
| 169 |  |  [Statistical Methods for Auditing the Quality of Manual Content Reviews](https://openreview.net/forum?id=GSlYBJ3aOpC) |  | 0 |  | Xuan Yang, Andrew Smart, Daniel Theron |  |
| 170 |  |  [ARTIFICIAL PSYCHOLOGY](https://openreview.net/forum?id=TqkzImZ92t8) |  | 0 |  | Mubarek Mohammed |  |
| 171 |  |  [Automated Mapping of Healthcare Concepts to a Standardized Healthcare Taxonomy](https://openreview.net/forum?id=87oCobKKS6x) |  | 0 |  | Sabbir Mollah, Mohammed Rakib, Nabeel Mohammed, Mashrur Wasek, AKM Shahariar Azad Rabby, Fuad Rahman |  |
| 172 |  |  [RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping](https://openreview.net/forum?id=frB4MiYGoD_) |  | 0 |  | Kamil Sabbagh, Patrik Joslin Kenfack, Adín Ramírez Rivera, Adil Khan |  |
| 173 |  |  [Prior knowledge meets Neural ODEs: a two-stage training method for improved explainability](https://openreview.net/forum?id=p7sHcNt_tqo) |  | 0 |  | C. Coelho, M. Fernanda P. Costa, Luís L. Ferrás |  |
| 174 |  |  [Self-Supervised Continual Learning](https://openreview.net/forum?id=udl9OobOxZu) |  | 0 |  | Kartik Thakral, Surbhi Mittal, Utkarsh Uppal, Bharat Giddwani, Mayank Vatsa, Richa Singh |  |
| 175 |  |  [When Spiking Neural Networks Meet Temporal Attention Image Decoding and Adaptive Spiking Neuron](https://openreview.net/forum?id=MuOFB0LQKcy) |  | 0 |  | Xuerui Qiu, Zheng Luan, Zhaorui Wang, RuiJie Zhu |  |
| 176 |  |  [Characters Are Like Faces](https://openreview.net/forum?id=HM_jOWEYL7y) |  | 0 |  | Haoyu Deng, Zhaoteng Ye, Yule Duan |  |
| 177 |  |  [A Scalable Self-supervised Learner for Hyperspectral Image Classification](https://openreview.net/forum?id=uwbyW92Sonu) |  | 0 |  | Weili Kong, Lin Qi, Baisen Liu, Jiaming Pei |  |
| 178 |  |  [Optimizing MPJPE promotes miscalibration in multi-hypothesis human pose lifting](https://openreview.net/forum?id=B5riBS9HZGn) |  | 0 |  | Pawel A. Pierzchlewicz, Mohammad Bashiri, R. James Cotton, Fabian H. Sinz |  |
| 179 |  |  [Unsupervised Detection of Cell Assemblies with Graph Neural Networks](https://openreview.net/forum?id=Tbzv_BbjjO8) |  | 0 |  | Roman Koshkin, Tomoki Fukai |  |
| 180 |  |  [Heat Up The Sentiment Learning With ICE](https://openreview.net/forum?id=wFxjFCHUkS) |  | 0 |  | Yao Yao, Zuchao Li, Hai Zhao |  |
| 181 |  |  [Almost Sure Last Iterate Convergence of Sharpness-Aware Minimization](https://openreview.net/forum?id=IcDTYTI0Nx) |  | 0 |  | Kyunghun Nam, Jinseok Chung, Namhoon Lee |  |
| 182 |  |  [On the application and impact of ε-DP and fairness in ambulance engagement time prediction](https://openreview.net/forum?id=WKVH54a1W4) |  | 0 |  | Selene Cerna, Catuscia Palamidessi |  |
| 183 |  |  [Adversarial Policy Gradient for Learning Graph-Based Representation in Human Visual Processing](https://openreview.net/forum?id=5-ROmmBJKV) |  | 0 |  | Subhrasankar Chatterjee, Subrata Pain, Debasis Samanta |  |
| 184 |  |  [Federated Learning for Local and Global Data Distribution](https://openreview.net/forum?id=qX8cGLnfAd) |  | 0 |  | Gaurav Goswami, Akshay Agarwal, Nalini K. Ratha, Richa Singh, Mayank Vatsa |  |
| 185 |  |  [Is DFR for Soft Biometrics Prediction in Unconstrained Images Fair and Effective?](https://openreview.net/forum?id=rLqN6XLbON) |  | 0 |  | Udaybhan Rathore, Akshay Agarwal |  |
| 186 |  |  [Integrating Information from Natural Language Parse Tree to Code Generation](https://openreview.net/forum?id=1WEPXTIjAd) |  | 0 |  | Hung Phan, Ali Jannesari |  |
| 187 |  |  [Pseudo Labels for Single Positive Multi-Label Learning](https://openreview.net/forum?id=-CH1C-aQ5pk) |  | 0 |  | Julio Arroyo |  |
| 188 |  |  [Drowning Detection based on YOLOv8 improved by GP-GAN Augmentation](https://openreview.net/forum?id=osqgjMNm4_) |  | 0 |  | En Wei, Simiao Ren |  |
| 189 |  |  [Beyond Negativity: Re-Analysis and Follow-Up Experiments on Hope Speech Detection](https://openreview.net/forum?id=eaKoBpxCPe) |  | 0 |  | Neemesh Yadav, Mohammad Aflah Khan, Diksha Sethi, Raghav Sahni |  |
| 190 |  |  [Group Equivariant Convolutional Networks](https://openreview.net/forum?id=niyvAOOnwPM) |  | 0 |  | Maya Janvier |  |
| 191 |  |  [Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization](https://openreview.net/forum?id=2Z-dQTRezZ) |  | 0 |  | Andrey Palaev, Rustam A. Lukmanov, Adil Khan |  |
| 192 |  |  [The Polarised Regime of identifiable Variational Autoencoders](https://openreview.net/forum?id=iSkcAjBqUHU) |  | 0 |  | Lisa Bonheme, Marek Grzes |  |
| 193 |  |  [Uni-Match: A Semantic Unified Model for Query-Product Retrieval](https://openreview.net/forum?id=91Bcj6sgcxt) |  | 0 |  | Zhenyang Zhu, RuiJie Zhu, Yunrui Ge, Qihang Zhao |  |
| 194 |  |  [Hierarchical Dialogue Understanding with Special Tokens and Turn-level Attention](https://openreview.net/forum?id=Peb3QdR8zzP) |  | 0 |  | Xiao Liu, Jian Zhang, Heng Zhang, Fuzhao Xue, Yang You |  |
| 195 |  |  [Large Sparse Kernels for Federated Learning](https://openreview.net/forum?id=ZCv4E1unfJP) |  | 0 |  | Feilong Zhang, Yinchuan Li, Shiyi Lin, Yunfeng Shao, Junjun Jiang, Xianming Liu |  |
| 196 |  |  [Using vision transformer-based GANs against Vision Transformers](https://openreview.net/forum?id=WFatA9XIQ0m) |  | 0 |  | Andrei Florin Pamînt, Sergiu Adrian Darabant |  |
| 197 |  |  [Pivot Pre-finetuning for Low Resource MT: A Case Study in Kikamba](https://openreview.net/forum?id=PaHmtktx86H) |  | 0 |  | Stephen Ngumbi Kiilu, Machel Reid |  |
| 198 |  |  [Unsupervised Learning for Anomaly Detection: A Comparison of Deep Generative Models](https://openreview.net/forum?id=WU3veNUvvU) |  | 0 |  | Kitgak Simon |  |
| 199 |  |  [Bayes classifier cannot be learned from noisy responses with unknown noise rates](https://openreview.net/forum?id=U4o5iSWSaD) |  | 0 |  | Soham Bakshi, Subha Maity |  |
| 200 |  |  [Contrastive Learning with 3D Shapes](https://openreview.net/forum?id=ChW0YYRIni) |  | 0 |  | Andrea Bernini |  |
| 201 |  |  [Adaptive-saturated RNN: Remember more with less instability](https://openreview.net/forum?id=Ihzsru2bw2) |  | 0 |  | Khoi Minh NguyenDuy, Quang Pham, Binh T. Nguyen |  |
| 202 |  |  [Stratospheric Aerosols: Establishing a Novel Optical Thickness Benchmark for Effective Climate Change Mitigation](https://openreview.net/forum?id=pKd6q-FrprW) |  | 0 |  | Mihir Garimella |  |
| 203 |  |  [CausalStructCodec: Causally-aware observational and interventional data generator](https://openreview.net/forum?id=cKLmwCTFiI) |  | 0 |  | Louis Hernandez, Matthieu Boussard |  |
| 204 |  |  [Semantic Similarity Based Label Augmentation for Visual Classification](https://openreview.net/forum?id=bRI_3OFg4o) |  | 0 |  | Yu Cao |  |
| 205 |  |  [Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning](https://openreview.net/forum?id=3EfxJTp_-Cj) |  | 0 |  | Chenkai Ma |  |
| 206 |  |  [Fidelity of Interpretability Methods and Perturbation Artifacts in Neural Networks](https://openreview.net/forum?id=nbqO93YTz-) |  | 0 |  | Lennart Brocki, Neo Christopher Chung |  |
| 207 |  |  [EDCDE - Extended Discovery of Closed-Form Differential Equations](https://openreview.net/forum?id=EVz_vcZQvvg) |  | 0 |  | Robert Joseph George |  |
| 208 |  |  [Synthetic Controls as Balancing Scores](https://openreview.net/forum?id=AFLNyWMg4D2) |  | 0 |  | Harsh Parikh |  |
| 209 |  |  [Clustered Federated Learning with Slightly Skewed Labels](https://openreview.net/forum?id=qPwZouq5sY_) |  | 0 |  | Jiaming Pei, Wei Li |  |
| 210 |  |  [Fusing 3D-CNN and lightweight Swin Transformer networks for HSI](https://openreview.net/forum?id=Jx44OPxLZ2-) |  | 0 |  | Baisen Liu, Yuanjia Liu, Wulin Zhang, Yiran Tian |  |
| 211 |  |  [Compound Tokens: Channel Fusion for Vision-Language Representation Learning](https://openreview.net/forum?id=_3_VZtMkvMB) |  | 0 |  | Maxwell Mbabilla Aladago, A. J. Piergiovanni |  |
| 212 |  |  [Message-passing Selection: Towards Interpretable GNNs for Graph Classification](https://openreview.net/forum?id=99Go96dla5y) |  | 0 |  | Wenda Li, Kaixuan Chen, Shunyu Liu, Wenjie Huang, Haofei Zhang, Mingli Song, Yingjie Tian, Yun Su |  |
| 213 |  |  [Multi-Agent Reinforcement Learning for Coalitional Bargaining Games](https://openreview.net/forum?id=OaZktJBVpUy) |  | 0 |  | Lucia CipolinaKun, Stephen Mak, Ignacio Carlucho, Vahid Yazdanpanah, Sebastian Stein, Enrico H. Gerding, Kalesha Bullard |  |
| 214 |  |  [Astroformer: More Data might not be all you need for Classification](https://openreview.net/forum?id=ChqP6ORFYK6) |  | 0 |  | Rishit Dagli |  |
| 215 |  |  [Error Analysis of Fitted Q-iteration with ReLU-activated Deep Neural Networks](https://openreview.net/forum?id=EVwbNcRa6Yf) |  | 0 |  | Lican Kang, Han Yuan, Chang Zhu |  |
| 216 |  |  [General Purpose Artificial Intelligence Systems as Group Agents](https://openreview.net/forum?id=ddFJsnpZtTX) |  | 0 |  | Matija Franklin |  |
| 217 |  |  [An Empirical Study of the Effect of Background Data Size on the Stability of SHapley Additive exPlanations (SHAP) for Deep Learning Models](https://openreview.net/forum?id=L38bbHmRKx) |  | 0 |  | Han Yuan, Mingxuan Liu, Lican Kang, Chenkui Miao, Ying Wu |  |
| 218 |  |  [Interpretable Machine Learning-Based Risk Scoring with Individual and Ensemble Model Selection for Clinical Decision Making](https://openreview.net/forum?id=RNlfw6KXJey) |  | 0 |  | Han Yuan, Jin Wee Lee, Mingxuan Liu, Siqi Li, Chenglin Niu, Jun Wen, Feng Xie |  |
| 219 |  |  [Analytical solutions for a family of single layer neural network regression problems](https://openreview.net/forum?id=g6ZFp73_T7) |  | 0 |  | Siddharth Krishna Kumar |  |
| 220 |  |  [Pretrained Vision Models for Predicting High-Risk Breast Cancer Stage](https://openreview.net/forum?id=Idalad_7wG) |  | 0 |  | Bonaventure F. P. Dossou, Yenoukoume S. K. Gbenou, Miglanche Ghomsi Nono |  |
